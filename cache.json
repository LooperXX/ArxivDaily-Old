{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.06922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Shih-Hsuan Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_T/0/1/0/all/0/1\">Tien-Hong Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>",
          "description": "An important research direction in automatic speech recognition (ASR) has\ncentered around the development of effective methods to rerank the output\nhypotheses of an ASR system with more sophisticated language models (LMs) for\nfurther gains. A current mainstream school of thoughts for ASR N-best\nhypothesis reranking is to employ a recurrent neural network (RNN)-based LM or\nits variants, with performance superiority over the conventional n-gram LMs\nacross a range of ASR tasks. In real scenarios such as a long conversation, a\nsequence of consecutive sentences may jointly contain ample cues of\nconversation-level information such as topical coherence, lexical entrainment\nand adjacency pairs, which however remains to be underexplored. In view of\nthis, we first formulate ASR N-best reranking as a prediction problem, putting\nforward an effective cross-sentence neural LM approach that reranks the ASR\nN-best hypotheses of an upcoming sentence by taking into consideration the word\nusage in its precedent sentences. Furthermore, we also explore to extract\ntask-specific global topical information of the cross-sentence history in an\nunsupervised manner for better ASR performance. Extensive experiments conducted\non the AMI conversational benchmark corpus indicate the effectiveness and\nfeasibility of our methods in comparison to several state-of-the-art reranking\nmethods.",
          "link": "http://arxiv.org/abs/2106.06922",
          "publishedOn": "2021-07-09T01:58:26.931Z",
          "wordCount": 681,
          "title": "Cross-sentence Neural Language Models for Conversational Speech Recognition. (arXiv:2106.06922v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Le Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1\">Tao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chaochun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Liefeng Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Wen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>",
          "description": "We investigate large-scale latent variable models (LVMs) for neural story\ngeneration -- an under-explored application for open-domain long text -- with\nobjectives in two threads: generation effectiveness and controllability. LVMs,\nespecially the variational autoencoder (VAE), have achieved both effective and\ncontrollable generation through exploiting flexible distributional latent\nrepresentations. Recently, Transformers and its variants have achieved\nremarkable effectiveness without explicit latent representation learning, thus\nlack satisfying controllability in generation. In this paper, we advocate to\nrevive latent variable modeling, essentially the power of representation\nlearning, in the era of Transformers to enhance controllability without hurting\nstate-of-the-art generation effectiveness. Specifically, we integrate latent\nrepresentation vectors with a Transformer-based pre-trained architecture to\nbuild conditional variational autoencoder (CVAE). Model components such as\nencoder, decoder and the variational posterior are all built on top of\npre-trained language models -- GPT2 specifically in this paper. Experiments\ndemonstrate state-of-the-art conditional generation ability of our model, as\nwell as its excellent representation learning capability and controllability.",
          "link": "http://arxiv.org/abs/2101.00828",
          "publishedOn": "2021-07-09T01:58:26.805Z",
          "wordCount": 632,
          "title": "Transformer-based Conditional Variational Autoencoder for Controllable Story Generation. (arXiv:2101.00828v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DeLucia_A/0/1/0/all/0/1\">Alexandra DeLucia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1\">Aaron Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lisa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>",
          "description": "Narrative generation is an open-ended NLP task in which a model generates a\nstory given a prompt. The task is similar to neural response generation for\nchatbots; however, innovations in response generation are often not applied to\nnarrative generation, despite the similarity between these tasks. We aim to\nbridge this gap by applying and evaluating advances in decoding methods for\nneural response generation to neural narrative generation. In particular, we\nemploy GPT-2 and perform ablations across nucleus sampling thresholds and\ndiverse decoding hyperparameters -- specifically, maximum mutual information --\nanalyzing results over multiple criteria with automatic and human evaluation.\nWe find that (1) nucleus sampling is generally best with thresholds between 0.7\nand 0.9; (2) a maximum mutual information objective can improve the quality of\ngenerated stories; and (3) established automatic metrics do not correlate well\nwith human judgments of narrative quality on any qualitative metric.",
          "link": "http://arxiv.org/abs/2010.07375",
          "publishedOn": "2021-07-09T01:58:26.695Z",
          "wordCount": 622,
          "title": "Decoding Methods for Neural Narrative Generation. (arXiv:2010.07375v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.05556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1\">Pedro Henrique Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1\">Vlad Niculae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinho_Z/0/1/0/all/0/1\">Zita Marinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; Martins</a>",
          "description": "Visual attention mechanisms are widely used in multimodal tasks, as visual\nquestion answering (VQA). One drawback of softmax-based attention mechanisms is\nthat they assign some probability mass to all image regions, regardless of\ntheir adjacency structure and of their relevance to the text. In this paper, to\nbetter link the image structure with the text, we replace the traditional\nsoftmax attention mechanism with two alternative sparsity-promoting\ntransformations: sparsemax, which is able to select only the relevant regions\n(assigning zero weight to the rest), and a newly proposed Total-Variation\nSparse Attention (TVmax), which further encourages the joint selection of\nadjacent spatial locations. Experiments in VQA show gains in accuracy as well\nas higher similarity to human attention, which suggests better\ninterpretability.",
          "link": "http://arxiv.org/abs/2002.05556",
          "publishedOn": "2021-07-09T01:58:26.488Z",
          "wordCount": 587,
          "title": "Sparse and Structured Visual Attention. (arXiv:2002.05556v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.08964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1\">Matthew Stevenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mues_C/0/1/0/all/0/1\">Christophe Mues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1\">Cristi&#xe1;n Bravo</a>",
          "description": "Compared to consumer lending, Micro, Small and Medium Enterprise (mSME)\ncredit risk modelling is particularly challenging, as, often, the same sources\nof information are not available. Therefore, it is standard policy for a loan\nofficer to provide a textual loan assessment to mitigate limited data\navailability. In turn, this statement is analysed by a credit expert alongside\nany available standard credit data. In our paper, we exploit recent advances\nfrom the field of Deep Learning and Natural Language Processing (NLP),\nincluding the BERT (Bidirectional Encoder Representations from Transformers)\nmodel, to extract information from 60 000 textual assessments provided by a\nlender. We consider the performance in terms of the AUC (Area Under the\nreceiver operating characteristic Curve) and Brier Score metrics and find that\nthe text alone is surprisingly effective for predicting default. However, when\ncombined with traditional data, it yields no additional predictive capability,\nwith performance dependent on the text's length. Our proposed deep learning\nmodel does, however, appear to be robust to the quality of the text and\ntherefore suitable for partly automating the mSME lending process. We also\ndemonstrate how the content of loan assessments influences performance, leading\nus to a series of recommendations on a new strategy for collecting future mSME\nloan assessments.",
          "link": "http://arxiv.org/abs/2003.08964",
          "publishedOn": "2021-07-09T01:58:26.416Z",
          "wordCount": 712,
          "title": "The value of text for small business default prediction: A deep learning approach. (arXiv:2003.08964v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AzariJafari_M/0/1/0/all/0/1\">Mohammad AzariJafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourjafari_Z/0/1/0/all/0/1\">Zohreh Pourjafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faridan_Jahromi_S/0/1/0/all/0/1\">Soroush Faridan-Jahromi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouhkan_Z/0/1/0/all/0/1\">Zeinab Kouhkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirak_A/0/1/0/all/0/1\">Azadeh Amirak</a>",
          "description": "Natural language inference (NLI) is known as one of the central tasks in\nnatural language processing (NLP) which encapsulates many fundamental aspects\nof language understanding. With the considerable achievements of data-hungry\ndeep learning methods in NLP tasks, a great amount of e ort has been devoted to\ndevelop more diverse datasets for di erent languages. In this paper, we present\na new dataset for the NLI task in the Persian language, also known as Farsi,\nwhich is one of the dominant languages in the Middle East. This dataset, named\nFarsTail, includes 10,367 samples which are provided in both the Persian\nlanguage as well as the indexed format to be useful for non-Persian\nresearchers. The samples are generated from 3,539 multiple-choice questions\nwith the least amount of annotator interventions in a way similar to the\nSciTail dataset. A carefully designed multi-step process is adopted to ensure\nthe quality of the dataset. We also present the results of traditional and\nstate-of-the-art methods on FarsTail including di erent embedding methods such\nas word2vec, fastText, ELMo, BERT, and LASER, as well as di erent modeling\napproaches such as DecompAtt, ESIM, HBMP, and ULMFiT to provide a solid\nbaseline for the future research. The best obtained test accuracy is 83.38%\nwhich shows that there is a big room for improving the current methods to be\nuseful for real-world NLP applications in di erent languages. We also\ninvestigate the extent to which the models exploit super cial clues, also known\nas dataset biases, in FarsTail, and partition the test set into easy and hard\nsubsets according to the success of biased models. The dataset is available at\nhttps://github.com/dml-qom/ FarsTail.",
          "link": "http://arxiv.org/abs/2009.08820",
          "publishedOn": "2021-07-09T01:58:26.389Z",
          "wordCount": 733,
          "title": "FarsTail: A Persian Natural Language Inference Dataset. (arXiv:2009.08820v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1\">Maxine Eskenazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1\">Shikib Mehri</a>",
          "description": "Automatic evaluation metrics are a crucial component of dialog systems\nresearch. Standard language evaluation metrics are known to be ineffective for\nevaluating dialog. As such, recent research has proposed a number of novel,\ndialog-specific metrics that correlate better with human judgements. Due to the\nfast pace of research, many of these metrics have been assessed on different\ndatasets and there has as yet been no time for a systematic comparison between\nthem. To this end, this paper provides a comprehensive assessment of recently\nproposed dialog evaluation metrics on a number of datasets. In this paper, 23\ndifferent automatic evaluation metrics are evaluated on 10 different datasets.\nFurthermore, the metrics are assessed in different settings, to better qualify\ntheir respective strengths and weaknesses. Metrics are assessed (1) on both the\nturn level and the dialog level, (2) for different dialog lengths, (3) for\ndifferent dialog qualities (e.g., coherence, engaging), (4) for different types\nof response generation models (i.e., generative, retrieval, simple models and\nstate-of-the-art models), (5) taking into account the similarity of different\nmetrics and (6) exploring combinations of different metrics. This comprehensive\nassessment offers several takeaways pertaining to dialog evaluation metrics in\ngeneral. It also suggests how to best assess evaluation metrics and indicates\npromising directions for future work.",
          "link": "http://arxiv.org/abs/2106.03706",
          "publishedOn": "2021-07-09T01:58:26.360Z",
          "wordCount": 686,
          "title": "A Comprehensive Assessment of Dialog Evaluation Metrics. (arXiv:2106.03706v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.07648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lynch_C/0/1/0/all/0/1\">Corey Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1\">Pierre Sermanet</a>",
          "description": "Natural language is perhaps the most flexible and intuitive way for humans to\ncommunicate tasks to a robot. Prior work in imitation learning typically\nrequires each task be specified with a task id or goal image -- something that\nis often impractical in open-world environments. On the other hand, previous\napproaches in instruction following allow agent behavior to be guided by\nlanguage, but typically assume structure in the observations, actuators, or\nlanguage that limit their applicability to complex settings like robotics. In\nthis work, we present a method for incorporating free-form natural language\nconditioning into imitation learning. Our approach learns perception from\npixels, natural language understanding, and multitask continuous control\nend-to-end as a single neural network. Unlike prior work in imitation learning,\nour method is able to incorporate unlabeled and unstructured demonstration data\n(i.e. no task or language labels). We show this dramatically improves language\nconditioned performance, while reducing the cost of language annotation to less\nthan 1% of total data. At test time, a single language conditioned visuomotor\npolicy trained with our method can perform a wide variety of robotic\nmanipulation skills in a 3D environment, specified only with natural language\ndescriptions of each task (e.g. \"open the drawer...now pick up the block...now\npress the green button...\"). To scale up the number of instructions an agent\ncan follow, we propose combining text conditioned policies with large\npretrained neural language models. We find this allows a policy to be robust to\nmany out-of-distribution synonym instructions, without requiring new\ndemonstrations. See videos of a human typing live text commands to our agent at\nlanguage-play.github.io",
          "link": "http://arxiv.org/abs/2005.07648",
          "publishedOn": "2021-07-09T01:58:25.724Z",
          "wordCount": 735,
          "title": "Language Conditioned Imitation Learning over Unstructured Data. (arXiv:2005.07648v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1\">Gautam Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1\">Milind Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dheram_P/0/1/0/all/0/1\">Pranav Dheram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Bryan Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_B/0/1/0/all/0/1\">Bach Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>",
          "description": "We propose an end-to-end trained spoken language understanding (SLU) system\nthat extracts transcripts, intents and slots from an input speech utterance. It\nconsists of a streaming recurrent neural network transducer (RNNT) based\nautomatic speech recognition (ASR) model connected to a neural natural language\nunderstanding (NLU) model through a neural interface. This interface allows for\nend-to-end training using multi-task RNNT and NLU losses. Additionally, we\nintroduce semantic sequence loss training for the joint RNNT-NLU system that\nallows direct optimization of non-differentiable SLU metrics. This end-to-end\nSLU model paradigm can leverage state-of-the-art advancements and pretrained\nmodels in both ASR and NLU research communities, outperforming recently\nproposed direct speech-to-semantics models, and conventional pipelined ASR and\nNLU systems. We show that this method improves both ASR and NLU metrics on both\npublic SLU datasets and large proprietary datasets.",
          "link": "http://arxiv.org/abs/2106.15919",
          "publishedOn": "2021-07-09T01:58:25.658Z",
          "wordCount": 599,
          "title": "End-to-End Spoken Language Understanding using RNN-Transducer ASR. (arXiv:2106.15919v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roemmele_M/0/1/0/all/0/1\">Melissa Roemmele</a>",
          "description": "Getting machines to generate text perceived as creative is a long-pursued\ngoal. A growing body of research directs this goal towards augmenting the\ncreative writing abilities of human authors. In this paper, we pursue this\nobjective by analyzing how observing examples of automatically generated text\ninfluences writing. In particular, we examine a task referred to as sentence\ninfilling, which involves transforming a list of words into a complete\nsentence. We emphasize \"storiability\" as a desirable feature of the resulting\nsentences, where \"storiable\" sentences are those that suggest a story a reader\nwould be curious to hear about. Both humans and an automated system (based on a\nneural language model) performed this sentence infilling task. In one setting,\npeople wrote sentences on their own; in a different setting, people observed\nthe sentences produced by the model while writing their own sentences. Readers\nthen assigned storiability preferences to the resulting sentences in a\nsubsequent evaluation. We find that human-authored sentences were judged as\nmore storiable when authors observed the generated examples, and that\nstoriability increased as authors derived more semantic content from the\nexamples. This result gives evidence of an \"inspiration through observation\"\nparadigm for human-computer collaborative writing, through which human writing\ncan be enhanced by text generation models without directly copying their\noutput.",
          "link": "http://arxiv.org/abs/2107.04007",
          "publishedOn": "2021-07-09T01:58:25.614Z",
          "wordCount": 660,
          "title": "Inspiration through Observation: Demonstrating the Influence of Automatically Generated Text on Creative Writing. (arXiv:2107.04007v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.09828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schumacher_E/0/1/0/all/0/1\">Elliot Schumacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayfield_J/0/1/0/all/0/1\">James Mayfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>",
          "description": "Cross-language entity linking grounds mentions in multiple languages to a\nsingle-language knowledge base. We propose a neural ranking architecture for\nthis task that uses multilingual BERT representations of the mention and the\ncontext in a neural network. We find that the multilingual ability of BERT\nleads to robust performance in monolingual and multilingual settings.\nFurthermore, we explore zero-shot language transfer and find surprisingly\nrobust performance. We investigate the zero-shot degradation and find that it\ncan be partially mitigated by a proposed auxiliary training objective, but that\nthe remaining error can best be attributed to domain shift rather than language\ntransfer.",
          "link": "http://arxiv.org/abs/2010.09828",
          "publishedOn": "2021-07-09T01:58:25.599Z",
          "wordCount": 564,
          "title": "Cross-Lingual Transfer in Zero-Shot Cross-Language Entity Linking. (arXiv:2010.09828v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haqbeen_J/0/1/0/all/0/1\">J. Haqbeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_T/0/1/0/all/0/1\">T. Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahab_S/0/1/0/all/0/1\">S. Sahab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfi_R/0/1/0/all/0/1\">R. Hadfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1\">T. Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okuhara_S/0/1/0/all/0/1\">S. Okuhara</a>",
          "description": "In this paper, we report about a large-scale online discussion with 1099\ncitizens on the Afghanistan Sustainable Development Goals.",
          "link": "http://arxiv.org/abs/2107.04011",
          "publishedOn": "2021-07-09T01:58:25.216Z",
          "wordCount": 490,
          "title": "Meeting the SDGs : Enabling the Goals by Cooperation with Crowd using a Conversational AI Platform. (arXiv:2107.04011v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yu-Ying Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Mihi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xuefeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1\">R. Harald Baayen</a>",
          "description": "This paper presents three case studies of modeling aspects of lexical\nprocessing with Linear Discriminative Learning (LDL), the computational engine\nof the Discriminative Lexicon model (Baayen et al., 2019). With numeric\nrepresentations of word forms and meanings, LDL learns to map one vector space\nonto the other, without being informed about any morphological structure or\ninflectional classes. The modeling results demonstrated that LDL not only\nperforms well for understanding and producing morphologically complex words,\nbut also generates quantitative measures that are predictive for human\nbehavioral data. LDL models are straightforward to implement with the JudiLing\npackage (Luo et al., 2021). Worked examples are provided for three modeling\nchallenges: producing and understanding Korean verb inflection, predicting\nprimed Dutch lexical decision latencies, and predicting the acoustic duration\nof Mandarin words.",
          "link": "http://arxiv.org/abs/2107.03950",
          "publishedOn": "2021-07-09T01:58:25.209Z",
          "wordCount": 558,
          "title": "Vector Space Morphology with Linear Discriminative Learning. (arXiv:2107.03950v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mozes_M/0/1/0/all/0/1\">Maximilian Mozes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vegt_I/0/1/0/all/0/1\">Isabelle van der Vegt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1\">Bennett Kleinberg</a>",
          "description": "The introduction of COVID-19 lockdown measures and an outlook on return to\nnormality are demanding societal changes. Among the most pressing questions is\nhow individuals adjust to the pandemic. This paper examines the emotional\nresponses to the pandemic in a repeated-measures design. Data (n=1698) were\ncollected in April 2020 (during strict lockdown measures) and in April 2021\n(when vaccination programmes gained traction). We asked participants to report\ntheir emotions and express these in text data. Statistical tests revealed an\naverage trend towards better adjustment to the pandemic. However, clustering\nanalyses suggested a more complex heterogeneous pattern with a well-coping and\na resigning subgroup of participants. Linguistic computational analyses\nuncovered that topics and n-gram frequencies shifted towards attention to the\nvaccination programme and away from general worrying. Implications for public\nmental health efforts in identifying people at heightened risk are discussed.\nThe dataset is made publicly available.",
          "link": "http://arxiv.org/abs/2107.03466",
          "publishedOn": "2021-07-09T01:58:25.183Z",
          "wordCount": 647,
          "title": "Worry, coping and resignation -- A repeated-measures study on emotional responses after a year in the pandemic. (arXiv:2107.03466v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saglam_R/0/1/0/all/0/1\">Rahime Belen Saglam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurse_J/0/1/0/all/0/1\">Jason R.C. Nurse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodges_D/0/1/0/all/0/1\">Duncan Hodges</a>",
          "description": "Through advances in their conversational abilities, chatbots have started to\nrequest and process an increasing variety of sensitive personal information.\nThe accurate disclosure of sensitive information is essential where it is used\nto provide advice and support to users in the healthcare and finance sectors.\nIn this study, we explore users' concerns regarding factors associated with the\nuse of sensitive data by chatbot providers. We surveyed a representative sample\nof 491 British citizens. Our results show that the user concerns focus on\ndeleting personal information and concerns about their data's inappropriate\nuse. We also identified that individuals were concerned about losing control\nover their data after a conversation with conversational agents. We found no\neffect from a user's gender or education but did find an effect from the user's\nage, with those over 45 being more concerned than those under 45. We also\nconsidered the factors that engender trust in a chatbot. Our respondents'\nprimary focus was on the chatbot's technical elements, with factors such as the\nresponse quality being identified as the most critical factor. We again found\nno effect from the user's gender or education level; however, when we\nconsidered some social factors (e.g. avatars or perceived 'friendliness'), we\nfound those under 45 years old rated these as more important than those over\n45. The paper concludes with a discussion of these results within the context\nof designing inclusive, digital systems that support a wide range of users.",
          "link": "http://arxiv.org/abs/2107.03959",
          "publishedOn": "2021-07-09T01:58:25.157Z",
          "wordCount": 707,
          "title": "Privacy Concerns in Chatbot Interactions: When to Trust and When to Worry. (arXiv:2107.03959v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aadesh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1\">Kaustubh D.Dhole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarway_R/0/1/0/all/0/1\">Rahul Tarway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakar_S/0/1/0/all/0/1\">Swetha Prabhakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ashish Shrivastava</a>",
          "description": "Domain-specific dialogue systems generally determine user intents by relying\non sentence-level classifiers which mainly focus on single action sentences.\nSuch classifiers are not designed to effectively handle complex queries\ncomposed of conditional and sequential clauses that represent multiple actions.\nWe attempt to decompose such queries into smaller single-action sub-queries\nthat are reasonable for intent classifiers to understand in a dialogue\npipeline. We release CANDLE (Conditional & AND type Expressions), a dataset\nconsisting of 3124 utterances manually tagged with conditional and sequential\nlabels and demonstrates this decomposition by training two baseline taggers.",
          "link": "http://arxiv.org/abs/2107.03884",
          "publishedOn": "2021-07-09T01:58:25.149Z",
          "wordCount": 532,
          "title": "CANDLE: Decomposing Conditional and Conjunctive Queries for Task-Oriented Dialogue Systems. (arXiv:2107.03884v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1\">Yves Bestgen</a>",
          "description": "A comparison of formulaic sequences in human and neural machine translation\nof quality newspaper articles shows that neural machine translations contain\nless lower-frequency, but strongly-associated formulaic sequences, and more\nhigh-frequency formulaic sequences. These differences were statistically\nsignificant and the effect sizes were almost always medium or large. These\nobservations can be related to the differences between second language learners\nof various levels and between translated and untranslated texts. The comparison\nbetween the neural machine translation systems indicates that some systems\nproduce more formulaic sequences of both types than other systems.",
          "link": "http://arxiv.org/abs/2107.03625",
          "publishedOn": "2021-07-09T01:58:25.139Z",
          "wordCount": 535,
          "title": "Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation. (arXiv:2107.03625v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvir Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1\">Janntatul Tajrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naira Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>",
          "description": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.",
          "link": "http://arxiv.org/abs/2107.03844",
          "publishedOn": "2021-07-09T01:58:25.107Z",
          "wordCount": 691,
          "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Camps_J/0/1/0/all/0/1\">Jean-Baptiste Camps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_Gorene_C/0/1/0/all/0/1\">Chahan Vidal-Gor&#xe8;ne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vernet_M/0/1/0/all/0/1\">Marguerite Vernet</a>",
          "description": "Although abbreviations are fairly common in handwritten sources, particularly\nin medieval and modern Western manuscripts, previous research dealing with\ncomputational approaches to their expansion is scarce. Yet abbreviations\npresent particular challenges to computational approaches such as handwritten\ntext recognition and natural language processing tasks. Often, pre-processing\nultimately aims to lead from a digitised image of the source to a normalised\ntext, which includes expansion of the abbreviations. We explore different\nsetups to obtain such a normalised text, either directly, by training HTR\nengines on normalised (i.e., expanded, disabbreviated) text, or by decomposing\nthe process into discrete steps, each making use of specialist models for\nrecognition, word segmentation and normalisation. The case studies considered\nhere are drawn from the medieval Latin tradition.",
          "link": "http://arxiv.org/abs/2107.03450",
          "publishedOn": "2021-07-09T01:58:25.098Z",
          "wordCount": 561,
          "title": "Handling Heavily Abbreviated Manuscripts: HTR engines vs text normalisation approaches. (arXiv:2107.03450v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huayun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Ke Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>",
          "description": "Speech evaluation is an essential component in computer-assisted language\nlearning (CALL). While speech evaluation on English has been popular, automatic\nspeech scoring on low resource languages remains challenging. Work in this area\nhas focused on monolingual specific designs and handcrafted features stemming\nfrom resource-rich languages like English. Such approaches are often difficult\nto generalize to other languages, especially if we also want to consider\nsuprasegmental qualities such as rhythm. In this work, we examine three\ndifferent languages that possess distinct rhythm patterns: English\n(stress-timed), Malay (syllable-timed), and Tamil (mora-timed). We exploit\nrobust feature representations inspired by music processing and vector\nrepresentation learning. Empirical validations show consistent gains for all\nthree languages when predicting pronunciation, rhythm and intonation\nperformance.",
          "link": "http://arxiv.org/abs/2107.03675",
          "publishedOn": "2021-07-09T01:58:25.090Z",
          "wordCount": 568,
          "title": "Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil. (arXiv:2107.03675v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klimaszewski_M/0/1/0/all/0/1\">Mateusz Klimaszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Alina Wr&#xf3;blewska</a>",
          "description": "We introduce the COMBO-based approach for EUD parsing and its implementation,\nwhich took part in the IWPT 2021 EUD shared task. The goal of this task is to\nparse raw texts in 17 languages into Enhanced Universal Dependencies (EUD). The\nproposed approach uses COMBO to predict UD trees and EUD graphs. These\nstructures are then merged into the final EUD graphs. Some EUD edge labels are\nextended with case information using a single language-independent expansion\nrule. In the official evaluation, the solution ranked fourth, achieving an\naverage ELAS of 83.79%. The source code is available at\nhttps://gitlab.clarin-pl.eu/syntactic-tools/combo.",
          "link": "http://arxiv.org/abs/2107.03809",
          "publishedOn": "2021-07-09T01:58:25.082Z",
          "wordCount": 530,
          "title": "COMBO: a new module for EUD parsing. (arXiv:2107.03809v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1\">Emily Dinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abercrombie_G/0/1/0/all/0/1\">Gavin Abercrombie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1\">A. Stevie Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spruit_S/0/1/0/all/0/1\">Shannon Spruit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>",
          "description": "Over the last several years, end-to-end neural conversational agents have\nvastly improved in their ability to carry a chit-chat conversation with humans.\nHowever, these models are often trained on large datasets from the internet,\nand as a result, may learn undesirable behaviors from this data, such as toxic\nor otherwise harmful language. Researchers must thus wrestle with the issue of\nhow and when to release these models. In this paper, we survey the problem\nlandscape for safety for end-to-end conversational AI and discuss recent and\nrelated work. We highlight tensions between values, potential positive impact\nand potential harms, and provide a framework for making decisions about whether\nand how to release these models, following the tenets of value-sensitive\ndesign. We additionally provide a suite of tools to enable researchers to make\nbetter-informed decisions about training and releasing end-to-end\nconversational AI models.",
          "link": "http://arxiv.org/abs/2107.03451",
          "publishedOn": "2021-07-09T01:58:25.069Z",
          "wordCount": 588,
          "title": "Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling. (arXiv:2107.03451v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abeysinghe_B/0/1/0/all/0/1\">Bhashithe Abeysinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Dhara Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freas_C/0/1/0/all/0/1\">Chris Freas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_R/0/1/0/all/0/1\">Robert Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunderraman_R/0/1/0/all/0/1\">Rajshekhar Sunderraman</a>",
          "description": "Most online message threads inherently will be cluttered and any new user or\nan existing user visiting after a hiatus will have a difficult time\nunderstanding whats being discussed in the thread. Similarly cluttered\nresponses in a message thread makes analyzing the messages a difficult problem.\nThe need for disentangling the clutter is much higher when the platform where\nthe discussion is taking place does not provide functions to retrieve reply\nrelations of the messages. This introduces an interesting problem to which\n\\cite{wang2011learning} phrases as a structural learning problem. We create\nvector embeddings for posts in a thread so that it captures both linguistic and\npositional features in relation to a context of where a given message is in.\nUsing these embeddings for posts we compute a similarity based connectivity\nmatrix which then converted into a graph. After employing a pruning mechanisms\nthe resultant graph can be used to discover the reply relation for the posts in\nthe thread. The process of discovering or disentangling chat is kept as an\nunsupervised mechanism. We present our experimental results on a data set\nobtained from Telegram with limited meta data.",
          "link": "http://arxiv.org/abs/2107.03529",
          "publishedOn": "2021-07-09T01:58:25.046Z",
          "wordCount": 625,
          "title": "POSLAN: Disentangling Chat with Positional and Language encoded Post Embeddings. (arXiv:2107.03529v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1\">Vivek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "Text generation is a highly active area of research in the computational\nlinguistic community. The evaluation of the generated text is a challenging\ntask and multiple theories and metrics have been proposed over the years.\nUnfortunately, text generation and evaluation are relatively understudied due\nto the scarcity of high-quality resources in code-mixed languages where the\nwords and phrases from multiple languages are mixed in a single utterance of\ntext and speech. To address this challenge, we present a corpus (HinGE) for a\nwidely popular code-mixed language Hinglish (code-mixing of Hindi and English\nlanguages). HinGE has Hinglish sentences generated by humans as well as two\nrule-based algorithms corresponding to the parallel Hindi-English sentences. In\naddition, we demonstrate the inefficacy of widely-used evaluation metrics on\nthe code-mixed data. The HinGE dataset will facilitate the progress of natural\nlanguage generation research in code-mixed languages.",
          "link": "http://arxiv.org/abs/2107.03760",
          "publishedOn": "2021-07-09T01:58:24.979Z",
          "wordCount": 574,
          "title": "HinGE: A Dataset for Generation and Evaluation of Code-Mixed Hinglish Text. (arXiv:2107.03760v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_T/0/1/0/all/0/1\">Tobias Schnabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1\">Marti A. Hearst</a>",
          "description": "This work presents Keep it Simple (KiS), a new approach to unsupervised text\nsimplification which learns to balance a reward across three properties:\nfluency, salience and simplicity. We train the model with a novel algorithm to\noptimize the reward (k-SCST), in which the model proposes several candidate\nsimplifications, computes each candidate's reward, and encourages candidates\nthat outperform the mean reward. Finally, we propose a realistic text\ncomprehension task as an evaluation method for text simplification. When tested\non the English news domain, the KiS model outperforms strong supervised\nbaselines by more than 4 SARI points, and can help people complete a\ncomprehension task an average of 18% faster while retaining accuracy, when\ncompared to the original text. Code available:\nhttps://github.com/tingofurro/keep_it_simple",
          "link": "http://arxiv.org/abs/2107.03444",
          "publishedOn": "2021-07-09T01:58:24.879Z",
          "wordCount": 571,
          "title": "Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text. (arXiv:2107.03444v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Luke Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandarkar_L/0/1/0/all/0/1\">Lucas Bandarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1\">Marti A. Hearst</a>",
          "description": "The Shuffle Test is the most common task to evaluate whether NLP models can\nmeasure coherence in text. Most recent work uses direct supervision on the\ntask; we show that by simply finetuning a RoBERTa model, we can achieve a near\nperfect accuracy of 97.8%, a state-of-the-art. We argue that this outstanding\nperformance is unlikely to lead to a good model of text coherence, and suggest\nthat the Shuffle Test should be approached in a Zero-Shot setting: models\nshould be evaluated without being trained on the task itself. We evaluate\ncommon models in this setting, such as Generative and Bi-directional\nTransformers, and find that larger architectures achieve high-performance\nout-of-the-box. Finally, we suggest the k-Block Shuffle Test, a modification of\nthe original by increasing the size of blocks shuffled. Even though human\nreader performance remains high (around 95% accuracy), model performance drops\nfrom 94% to 78% as block size increases, creating a conceptually simple\nchallenge to benchmark NLP models. Code available:\nhttps://github.com/tingofurro/shuffle_test/",
          "link": "http://arxiv.org/abs/2107.03448",
          "publishedOn": "2021-07-09T01:58:24.700Z",
          "wordCount": 620,
          "title": "Can Transformer Models Measure Coherence In Text? Re-Thinking the Shuffle Test. (arXiv:2107.03448v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1\">Junha Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1\">Karthik Desingh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "To realize robots that can understand human instructions and perform\nmeaningful tasks in the near future, it is important to develop learned models\nthat can understand referential language to identify common objects in\nreal-world 3D scenes. In this paper, we develop a spatial-language model for a\n3D visual grounding problem. Specifically, given a reconstructed 3D scene in\nthe form of a point cloud with 3D bounding boxes of potential object\ncandidates, and a language utterance referring to a target object in the scene,\nour model identifies the target object from a set of potential candidates. Our\nspatial-language model uses a transformer-based architecture that combines\nspatial embedding from bounding-box with a finetuned language embedding from\nDistilBert and reasons among the objects in the 3D scene to find the target\nobject. We show that our model performs competitively on visio-linguistic\ndatasets proposed by ReferIt3D. We provide additional analysis of performance\nin spatial reasoning tasks decoupled from perception noise, the effect of\nview-dependent utterances in terms of accuracy, and view-point annotations for\npotential robotics applications.",
          "link": "http://arxiv.org/abs/2107.03438",
          "publishedOn": "2021-07-09T01:58:24.521Z",
          "wordCount": 614,
          "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding. (arXiv:2107.03438v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hyunmin Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gary Geunbae Lee</a>",
          "description": "Recently, reinforcement learning (RL) has been applied to task-oriented\ndialogue systems by using latent actions to solve shortcomings of supervised\nlearning (SL). In this paper, we propose a multi-domain task-oriented dialogue\nsystem, called Dialogue System with Optimizing a Recurrent Action Policy using\nEfficient Context (DORA), that uses SL, with subsequently applied RL to\noptimize dialogue systems using a recurrent dialogue policy. This dialogue\npolicy recurrently generates explicit system actions as a both word-level and\nhigh-level policy. As a result, DORA is clearly optimized during both SL and RL\nsteps by using an explicit system action policy that considers an efficient\ncontext instead of the entire dialogue history. The system actions are both\ninterpretable and controllable, whereas the latent actions are not. DORA\nimproved the success rate by 6.6 points on MultiWOZ 2.0 and by 10.9 points on\nMultiWOZ 2.1.",
          "link": "http://arxiv.org/abs/2107.03286",
          "publishedOn": "2021-07-08T01:57:57.316Z",
          "wordCount": 585,
          "title": "DORA: Toward Policy Optimization for Task-oriented Dialogue System with Efficient Context. (arXiv:2107.03286v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2007.15823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garbacea_C/0/1/0/all/0/1\">Cristina Garbacea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mengtian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carton_S/0/1/0/all/0/1\">Samuel Carton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1\">Qiaozhu Mei</a>",
          "description": "Text simplification reduces the language complexity of professional content\nfor accessibility purposes. End-to-end neural network models have been widely\nadopted to directly generate the simplified version of input text, usually\nfunctioning as a blackbox. We show that text simplification can be decomposed\ninto a compact pipeline of tasks to ensure the transparency and explainability\nof the process. The first two steps in this pipeline are often neglected: 1) to\npredict whether a given piece of text needs to be simplified, and 2) if yes, to\nidentify complex parts of the text. The two tasks can be solved separately\nusing either lexical or deep learning methods, or solved jointly. Simply\napplying explainable complexity prediction as a preliminary step, the\nout-of-sample text simplification performance of the state-of-the-art,\nblack-box simplification models can be improved by a large margin.",
          "link": "http://arxiv.org/abs/2007.15823",
          "publishedOn": "2021-07-08T01:57:57.267Z",
          "wordCount": 612,
          "title": "Explainable Prediction of Text Complexity: The Missing Preliminaries for Text Simplification. (arXiv:2007.15823v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1\">Guoshun Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1\">Rui Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_S/0/1/0/all/0/1\">Sicong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>",
          "description": "Video grounding aims to localize a moment from an untrimmed video for a given\ntextual query. Existing approaches focus more on the alignment of visual and\nlanguage stimuli with various likelihood-based matching or regression\nstrategies, i.e., P(Y|X). Consequently, these models may suffer from spurious\ncorrelations between the language and video features due to the selection bias\nof the dataset. 1) To uncover the causality behind the model and data, we first\npropose a novel paradigm from the perspective of the causal inference, i.e.,\ninterventional video grounding (IVG) that leverages backdoor adjustment to\ndeconfound the selection bias based on structured causal model (SCM) and\ndo-calculus P(Y|do(X)). Then, we present a simple yet effective method to\napproximate the unobserved confounder as it cannot be directly sampled from the\ndataset. 2) Meanwhile, we introduce a dual contrastive learning approach (DCL)\nto better align the text and video by maximizing the mutual information (MI)\nbetween query and video clips, and the MI between start/end frames of a target\nmoment and the others within a video to learn more informative visual\nrepresentations. Experiments on three standard benchmarks show the\neffectiveness of our approaches. Our code is available on GitHub:\nhttps://github.com/nanguoshun/IVG.",
          "link": "http://arxiv.org/abs/2106.11013",
          "publishedOn": "2021-07-08T01:57:57.249Z",
          "wordCount": 677,
          "title": "Interventional Video Grounding with Dual Contrastive Learning. (arXiv:2106.11013v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayyeri_M/0/1/0/all/0/1\">Mojtaba Nayyeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cil_G/0/1/0/all/0/1\">Gokce Muge Cil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdati_S/0/1/0/all/0/1\">Sahar Vahdati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osborne_F/0/1/0/all/0/1\">Francesco Osborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mahfuzur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angioni_S/0/1/0/all/0/1\">Simone Angioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salatino_A/0/1/0/all/0/1\">Angelo Salatino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1\">Diego Reforgiato Recupero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassilyeva_N/0/1/0/all/0/1\">Nadezhda Vassilyeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motta_E/0/1/0/all/0/1\">Enrico Motta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>",
          "description": "The incompleteness of Knowledge Graphs (KGs) is a crucial issue affecting the\nquality of AI-based services. In the scholarly domain, KGs describing research\npublications typically lack important information, hindering our ability to\nanalyse and predict research dynamics. In recent years, link prediction\napproaches based on Knowledge Graph Embedding models became the first aid for\nthis issue. In this work, we present Trans4E, a novel embedding model that is\nparticularly fit for KGs which include N to M relations with N$\\gg$M. This is\ntypical for KGs that categorize a large number of entities (e.g., research\narticles, patents, persons) according to a relatively small set of categories.\nTrans4E was applied on two large-scale knowledge graphs, the Academia/Industry\nDynAmics (AIDA) and Microsoft Academic Graph (MAG), for completing the\ninformation about Fields of Study (e.g., 'neural networks', 'machine learning',\n'artificial intelligence'), and affiliation types (e.g., 'education',\n'company', 'government'), improving the scope and accuracy of the resulting\ndata. We evaluated our approach against alternative solutions on AIDA, MAG, and\nfour other benchmarks (FB15k, FB15k-237, WN18, and WN18RR). Trans4E outperforms\nthe other models when using low embedding dimensions and obtains competitive\nresults in high dimensions.",
          "link": "http://arxiv.org/abs/2107.03297",
          "publishedOn": "2021-07-08T01:57:56.870Z",
          "wordCount": 648,
          "title": "Trans4E: Link Prediction on Scholarly Knowledge Graphs. (arXiv:2107.03297v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Masum Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrab_K/0/1/0/all/0/1\">Kazi Sajeed Mehrab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>",
          "description": "We present Text2App -- a framework that allows users to create functional\nAndroid applications from natural language specifications. The conventional\nmethod of source code generation tries to generate source code directly, which\nis impractical for creating complex software. We overcome this limitation by\ntransforming natural language into an abstract intermediate formal language\nrepresenting an application with a substantially smaller number of tokens. The\nintermediate formal representation is then compiled into target source codes.\nThis abstraction of programming details allows seq2seq networks to learn\ncomplex application structures with less overhead. In order to train sequence\nmodels, we introduce a data synthesis method grounded in a human survey. We\ndemonstrate that Text2App generalizes well to unseen combination of app\ncomponents and it is capable of handling noisy natural language instructions.\nWe explore the possibility of creating applications from highly abstract\ninstructions by coupling our system with GPT-3 -- a large pretrained language\nmodel. We perform an extensive human evaluation and identify the capabilities\nand limitations of our system. The source code, a ready-to-run demo notebook,\nand a demo video are publicly available at\n\\url{https://github.com/text2app/Text2App}.",
          "link": "http://arxiv.org/abs/2104.08301",
          "publishedOn": "2021-07-08T01:57:56.835Z",
          "wordCount": 661,
          "title": "Text2App: A Framework for Creating Android Apps from Text Descriptions. (arXiv:2104.08301v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cortis_K/0/1/0/all/0/1\">Keith Cortis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_B/0/1/0/all/0/1\">Brian Davis</a>",
          "description": "Social media popularity and importance is on the increase due to people using\nit for various types of social interaction across multiple channels. This\nsystematic review focuses on the evolving research area of Social Opinion\nMining, tasked with the identification of multiple opinion dimensions, such as\nsubjectivity, sentiment polarity, emotion, affect, sarcasm and irony, from\nuser-generated content represented across multiple social media platforms and\nin various media formats, like text, image, video and audio. Through Social\nOpinion Mining, natural language can be understood in terms of the different\nopinion dimensions, as expressed by humans. This contributes towards the\nevolution of Artificial Intelligence which in turn helps the advancement of\nseveral real-world use cases, such as customer service and decision making. A\nthorough systematic review was carried out on Social Opinion Mining research\nwhich totals 485 published studies and spans a period of twelve years between\n2007 and 2018. The in-depth analysis focuses on the social media platforms,\ntechniques, social datasets, language, modality, tools and technologies, and\nother aspects derived. Social Opinion Mining can be utilised in many\napplication areas, ranging from marketing, advertising and sales for\nproduct/service management, and in multiple domains and industries, such as\npolitics, technology, finance, healthcare, sports and government. The latest\ndevelopments in Social Opinion Mining beyond 2018 are also presented together\nwith future research directions, with the aim of leaving a wider academic and\nsocietal impact in several real-world applications.",
          "link": "http://arxiv.org/abs/2012.03091",
          "publishedOn": "2021-07-08T01:57:56.799Z",
          "wordCount": 717,
          "title": "Over a Decade of Social Opinion Mining: A Systematic Review. (arXiv:2012.03091v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wenjing Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xin Lei</a>",
          "description": "The unified streaming and non-streaming two-pass (U2) end-to-end model for\nspeech recognition has shown great performance in terms of streaming\ncapability, accuracy, real-time factor (RTF), and latency. In this paper, we\npresent U2++, an enhanced version of U2 to further improve the accuracy. The\ncore idea of U2++ is to use the forward and the backward information of the\nlabeling sequences at the same time at training to learn richer information,\nand combine the forward and backward prediction at decoding to give more\naccurate recognition results. We also proposed a new data augmentation method\ncalled SpecSub to help the U2++ model to be more accurate and robust. Our\nexperiments show that, compared with U2, U2++ shows faster convergence at\ntraining, better robustness to the decoding method, as well as consistent 5\\% -\n8\\% word error rate reduction gain over U2. On the experiment of AISHELL-1, we\nachieve a 4.63\\% character error rate (CER) with a non-streaming setup and\n5.05\\% with a streaming setup with 320ms latency by U2++. To the best of our\nknowledge, 5.05\\% is the best-published streaming result on the AISHELL-1 test\nset.",
          "link": "http://arxiv.org/abs/2106.05642",
          "publishedOn": "2021-07-08T01:57:56.753Z",
          "wordCount": 662,
          "title": "U2++: Unified Two-pass Bidirectional End-to-end Model for Speech Recognition. (arXiv:2106.05642v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandle_S/0/1/0/all/0/1\">Sebastian Br&#xe4;ndle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanussek_M/0/1/0/all/0/1\">Marc Hanussek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blohm_M/0/1/0/all/0/1\">Matthias Blohm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1\">Maximilien Kintz</a>",
          "description": "Automated Machine Learning (AutoML) has gained increasing success on tabular\ndata in recent years. However, processing unstructured data like text is a\nchallenge and not widely supported by open-source AutoML tools. This work\ncompares three manually created text representations and text embeddings\nautomatically created by AutoML tools. Our benchmark includes four popular\nopen-source AutoML tools and eight datasets for text classification purposes.\nThe results show that straightforward text representations perform better than\nAutoML tools with automatically created text embeddings.",
          "link": "http://arxiv.org/abs/2106.12798",
          "publishedOn": "2021-07-08T01:57:56.723Z",
          "wordCount": 552,
          "title": "Evaluation of Representation Models for Text Classification with AutoML Tools. (arXiv:2106.12798v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.00694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lau_W/0/1/0/all/0/1\">Wilson Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aaltonen_L/0/1/0/all/0/1\">Laura Aaltonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunn_M/0/1/0/all/0/1\">Martin Gunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>",
          "description": "Selecting radiology examination protocol is a repetitive, and time-consuming\nprocess. In this paper, we present a deep learning approach to automatically\nassign protocols to computer tomography examinations, by pre-training a\ndomain-specific BERT model ($BERT_{rad}$). To handle the high data imbalance\nacross exam protocols, we used a knowledge distillation approach that\nup-sampled the minority classes through data augmentation. We compared\nclassification performance of the described approach with the statistical\nn-gram models using Support Vector Machine (SVM), Gradient Boosting Machine\n(GBM), and Random Forest (RF) classifiers, as well as the Google's\n$BERT_{base}$ model. SVM, GBM and RF achieved macro-averaged F1 scores of 0.45,\n0.45, and 0.6 while $BERT_{base}$ and $BERT_{rad}$ achieved 0.61 and 0.63.\nKnowledge distillation improved overall performance on the minority classes,\nachieving a F1 score of 0.66.",
          "link": "http://arxiv.org/abs/2009.00694",
          "publishedOn": "2021-07-08T01:57:56.707Z",
          "wordCount": 613,
          "title": "Automatic Assignment of Radiology Examination Protocols Using Pre-trained Language Models with Knowledge Distillation. (arXiv:2009.00694v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seung-won Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Sang-eun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dohyeon Lee</a>",
          "description": "This paper studies the bias problem of multi-hop question answering models,\nof answering correctly without correct reasoning. One way to robustify these\nmodels is by supervising to not only answer right, but also with right\nreasoning chains. An existing direction is to annotate reasoning chains to\ntrain models, requiring expensive additional annotations. In contrast, we\npropose a new approach to learn evidentiality, deciding whether the answer\nprediction is supported by correct evidences, without such annotations.\nInstead, we compare counterfactual changes in answer confidence with and\nwithout evidence sentences, to generate \"pseudo-evidentiality\" annotations. We\nvalidate our proposed model on an original set and challenge set in HotpotQA,\nshowing that our method is accurate and robust in multi-hop reasoning.",
          "link": "http://arxiv.org/abs/2107.03242",
          "publishedOn": "2021-07-08T01:57:56.668Z",
          "wordCount": 551,
          "title": "Robustifying Multi-hop QA through Pseudo-Evidentiality Training. (arXiv:2107.03242v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chi-Yang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yun-Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1\">Lun-Wei Ku</a>",
          "description": "Writing a coherent and engaging story is not easy. Creative writers use their\nknowledge and worldview to put disjointed elements together to form a coherent\nstoryline, and work and rework iteratively toward perfection. Automated visual\nstorytelling (VIST) models, however, make poor use of external knowledge and\niterative generation when attempting to create stories. This paper introduces\nPR-VIST, a framework that represents the input image sequence as a story graph\nin which it finds the best path to form a storyline. PR-VIST then takes this\npath and learns to generate the final story via an iterative training process.\nThis framework produces stories that are superior in terms of diversity,\ncoherence, and humanness, per both automatic and human evaluations. An ablation\nstudy shows that both plotting and reworking contribute to the model's\nsuperiority.",
          "link": "http://arxiv.org/abs/2105.06950",
          "publishedOn": "2021-07-08T01:57:56.654Z",
          "wordCount": 610,
          "title": "Plot and Rework: Modeling Storylines for Visual Storytelling. (arXiv:2105.06950v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kori_M/0/1/0/all/0/1\">Mayuko Kori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasuo_I/0/1/0/all/0/1\">Ichiro Hasuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsumata_S/0/1/0/all/0/1\">Shin-ya Katsumata</a>",
          "description": "The coincidence between initial algebras (IAs) and final coalgebras (FCs) is\na phenomenon that underpins various important results in theoretical computer\nscience. In this paper, we identify a general fibrational condition for the\nIA-FC coincidence, namely in the fiber over an initial algebra in the base\ncategory. Identifying (co)algebras in a fiber as (co)inductive predicates, our\nfibrational IA-FC coincidence allows one to use coinductive witnesses (such as\ninvariants) for verifying inductive properties (such as liveness). Our general\nfibrational theory features the technical condition of stability of chain\ncolimits; we extend the framework to the presence of a monadic effect, too,\nrestricting to fibrations of complete lattice-valued predicates. Practical\nbenefits of our categorical theory are exemplified by new \"upside-down\" witness\nnotions for three verification problems: probabilistic liveness, and acceptance\nand model-checking with respect to bottom-up tree automata.",
          "link": "http://arxiv.org/abs/2105.04817",
          "publishedOn": "2021-07-08T01:57:56.639Z",
          "wordCount": 613,
          "title": "Fibrational Initial Algebra-Final Coalgebra Coincidence over Initial Algebras: Turning Verification Witnesses Upside Down. (arXiv:2105.04817v2 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>",
          "description": "The syntactic structure of a sentence is often represented using syntactic\ndependency trees. The sum of the distances between syntactically related words\nhas been in the limelight for the past decades. Research on dependency\ndistances led to the formulation of the principle of dependency distance\nminimization whereby words in sentences are ordered so as to minimize that sum.\nNumerous random baselines have been defined to carry out related quantitative\nstudies on languages. The simplest random baseline is the expected value of the\nsum in unconstrained random permutations of the words in the sentence, namely\nwhen all the shufflings of the words of a sentence are allowed and equally\nlikely. Here we focus on a popular baseline: random projective permutations of\nthe words of the sentence, that is, permutations where the syntactic dependency\nstructure is projective, a formal constraint that sentences satisfy often in\nlanguages. Thus far, the expectation of the sum of dependency distances in\nrandom projective shufflings of a sentence has been estimated approximately\nwith a Monte Carlo procedure whose cost is of the order of $Zn$, where $n$ is\nthe number of words of the sentence and $Z$ is the number of samples; the\nlarger $Z$, the lower the error of the estimation but the larger the time cost.\nHere we present formulae to compute that expectation without error in time of\nthe order of $n$. Furthermore, we show that star trees maximize it, and devise\na dynamic programming algorithm to retrieve the trees that minimize it.",
          "link": "http://arxiv.org/abs/2107.03277",
          "publishedOn": "2021-07-08T01:57:56.565Z",
          "wordCount": 695,
          "title": "Linear-time calculation of the expected sum of edge lengths in random projective linearizations of trees. (arXiv:2107.03277v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chun Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zaixiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "The choice of token vocabulary affects the performance of machine\ntranslation. This paper aims to figure out what is a good vocabulary and\nwhether one can find the optimal vocabulary without trial training. To answer\nthese questions, we first provide an alternative understanding of the role of\nvocabulary from the perspective of information theory. Motivated by this, we\nformulate the quest of vocabularization -- finding the best token dictionary\nwith a proper size -- as an optimal transport (OT) problem. We propose VOLT, a\nsimple and efficient solution without trial training. Empirical results show\nthat VOLT outperforms widely-used vocabularies in diverse scenarios, including\nWMT-14 English-German and TED's 52 translation directions. For example, VOLT\nachieves almost 70% vocabulary size reduction and 0.5 BLEU gain on\nEnglish-German translation. Also, compared to BPE-search, VOLT reduces the\nsearch time from 384 GPU hours to 30 GPU hours on English-German translation.\nCodes are available at https://github.com/Jingjing-NLP/VOLT .",
          "link": "http://arxiv.org/abs/2012.15671",
          "publishedOn": "2021-07-08T01:57:56.556Z",
          "wordCount": 631,
          "title": "Vocabulary Learning via Optimal Transport for Machine Translation. (arXiv:2012.15671v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ernie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_H/0/1/0/all/0/1\">Hui-Syuan Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>",
          "description": "Large-scale pretrained language models have led to dramatic improvements in\ntext generation. Impressive performance can be achieved by finetuning only on a\nsmall number of instances (few-shot setting). Nonetheless, almost all previous\nwork simply applies random sampling to select the few-shot training instances.\nLittle to no attention has been paid to the selection strategies and how they\nwould affect model performance. In this work, we present a study on training\ninstance selection in few-shot neural text generation. The selection decision\nis made based only on the unlabeled data so as to identify the most worthwhile\ndata points that should be annotated under some budget of labeling cost. Based\non the intuition that the few-shot training instances should be diverse and\nrepresentative of the entire data distribution, we propose a simple selection\nstrategy with K-means clustering. We show that even with the naive\nclustering-based approach, the generation models consistently outperform random\nsampling on three text generation tasks: data-to-text generation, document\nsummarization and question generation. We hope that this work will call for\nmore attention on this largely unexplored area.",
          "link": "http://arxiv.org/abs/2107.03176",
          "publishedOn": "2021-07-08T01:57:56.484Z",
          "wordCount": 620,
          "title": "On Training Instance Selection for Few-Shot Neural Text Generation. (arXiv:2107.03176v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Partanen_N/0/1/0/all/0/1\">Niko Partanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>",
          "description": "Texts written in Old Literary Finnish represent the first literary work ever\nwritten in Finnish starting from the 16th century. There have been several\nprojects in Finland that have digitized old publications and made them\navailable for research use. However, using modern NLP methods in such data\nposes great challenges. In this paper we propose an approach for simultaneously\nnormalizing and lemmatizing Old Literary Finnish into modern spelling. Our best\nmodel reaches to 96.3\\% accuracy in texts written by Agricola and 87.7\\%\naccuracy in other contemporary out-of-domain text. Our method has been made\nfreely available on Zenodo and Github.",
          "link": "http://arxiv.org/abs/2107.03266",
          "publishedOn": "2021-07-08T01:57:56.456Z",
          "wordCount": 545,
          "title": "Lemmatization of Historical Old Literary Finnish Texts in Modern Orthography. (arXiv:2107.03266v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fangyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhigang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hongchen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>",
          "description": "Most of the recent state-of-the-art results for speaker verification are\nachieved by X-vector and its subsequent variants. In this paper, we propose a\nnew network architecture which aggregates the channel and context\ninterdependence features from multi aspect based on Time Delay Neural Network\n(TDNN). Firstly, we use the SE-Res2Blocks as in ECAPA-TDNN to explicitly model\nthe channel interdependence to realize adaptive calibration of channel\nfeatures, and process local context features in a multi-scale way at a more\ngranular level compared with conventional TDNN-based methods. Secondly, we\nexplore to use the encoder structure of Transformer to model the global context\ninterdependence features at an utterance level which can capture better long\nterm temporal characteristics. Before the pooling layer, we aggregate the\noutputs of SE-Res2Blocks and Transformer encoder to leverage the complementary\nchannel and context interdependence features learned by themself respectively.\nFinally, instead of performing a single attentive statistics pooling, we also\nfind it beneficial to extend the pooling method in a multi-head way which can\ndiscriminate features from multiple aspect. The proposed MACCIF-TDNN\narchitecture can outperform most of the state-of-the-art TDNN-based systems on\nVoxCeleb1 test sets.",
          "link": "http://arxiv.org/abs/2107.03104",
          "publishedOn": "2021-07-08T01:57:56.177Z",
          "wordCount": 650,
          "title": "MACCIF-TDNN: Multi aspect aggregation of channel and context interdependence features in TDNN-based speaker verification. (arXiv:2107.03104v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Alvin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1\">Ali Madani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_B/0/1/0/all/0/1\">Ben Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1\">Nikhil Naik</a>",
          "description": "Attribute extrapolation in sample generation is challenging for deep neural\nnetworks operating beyond the training distribution. We formulate a new task\nfor extrapolation in sequence generation, focusing on natural language and\nproteins, and propose GENhance, a generative framework that enhances attributes\nthrough a learned latent space. Trained on movie reviews and a computed protein\nstability dataset, GENhance can generate strongly-positive text reviews and\nhighly stable protein sequences without being exposed to similar data during\ntraining. We release our benchmark tasks and models to contribute to the study\nof generative modeling extrapolation and data-driven design in biology and\nchemistry.",
          "link": "http://arxiv.org/abs/2107.02968",
          "publishedOn": "2021-07-08T01:57:56.168Z",
          "wordCount": 532,
          "title": "Deep Extrapolation for Attribute-Enhanced Generation. (arXiv:2107.02968v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Javed_T/0/1/0/all/0/1\">Taimoor Ahmed Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_W/0/1/0/all/0/1\">Waseem Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arshad_U/0/1/0/all/0/1\">Umair Arshad</a>",
          "description": "Digital text is increasing day by day on the internet. It is very challenging\nto classify a large and heterogeneous collection of data, which require\nimproved information processing methods to organize text. To classify large\nsize of corpus, one common approach is to use hierarchical text classification,\nwhich aims to classify textual data in a hierarchical structure. Several\napproaches have been proposed to tackle classification of text but most of the\nresearch has been done on English language. This paper proposes a deep learning\nmodel for hierarchical text classification of news in Urdu language -\nconsisting of 51,325 sentences from 8 online news websites belonging to the\nfollowing genres: Sports; Technology; and Entertainment. The objectives of this\npaper are twofold: (1) to develop a large human-annotated dataset of news in\nUrdu language for hierarchical text classification; and (2) to classify Urdu\nnews hierarchically using our proposed model based on LSTM mechanism named as\nHierarchical Multi-layer LSTMs (HMLSTM). Our model consists of two modules:\nText Representing Layer, for obtaining text representation in which we use\nWord2vec embedding to transform the words to vector and Urdu Hierarchical LSTM\nLayer (UHLSTML) an end-to-end fully connected deep LSTMs network to perform\nautomatic feature learning, we train one LSTM layer for each level of the class\nhierarchy. We have performed extensive experiments on our self created dataset\nnamed as Urdu News Dataset for Hierarchical Text Classification (UNDHTC). The\nresult shows that our proposed method is very effective for hierarchical text\nclassification and it outperforms baseline methods significantly and also\nachieved good results as compare to deep neural model.",
          "link": "http://arxiv.org/abs/2107.03141",
          "publishedOn": "2021-07-08T01:57:56.156Z",
          "wordCount": 704,
          "title": "Hierarchical Text Classification of Urdu News using Deep Neural Network. (arXiv:2107.03141v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1\">Markus Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufhold_M/0/1/0/all/0/1\">Marc-Andr&#xe9; Kaufhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1\">Christian Reuter</a>",
          "description": "Data augmentation, the artificial creation of training data for machine\nlearning by transformations, is a widely studied research field across machine\nlearning disciplines. While it is useful for increasing the generalization\ncapabilities of a model, it can also address many other challenges and\nproblems, from overcoming a limited amount of training data over regularizing\nthe objective to limiting the amount data used to protect privacy. Based on a\nprecise description of the goals and applications of data augmentation (C1) and\na taxonomy for existing works (C2), this survey is concerned with data\naugmentation methods for textual classification and aims to achieve a concise\nand comprehensive overview for researchers and practitioners (C3). Derived from\nthe taxonomy, we divided more than 100 methods into 12 different groupings and\nprovide state-of-the-art references expounding which methods are highly\npromising (C4). Finally, research perspectives that may constitute a building\nblock for future work are given (C5).",
          "link": "http://arxiv.org/abs/2107.03158",
          "publishedOn": "2021-07-08T01:57:56.147Z",
          "wordCount": 591,
          "title": "A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liyanapathirana_U/0/1/0/all/0/1\">Upuli Liyanapathirana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasinghe_K/0/1/0/all/0/1\">Kaumini Gunasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dias_G/0/1/0/all/0/1\">Gihan Dias</a>",
          "description": "We have built SinSpell, a comprehensive spelling checker for the Sinhala\nlanguage which is spoken by over 16 million people, mainly in Sri Lanka.\nHowever, until recently, Sinhala had no spelling checker with acceptable\ncoverage. Sinspell is still the only open source Sinhala spelling checker.\nSinSpell identifies possible spelling errors and suggests corrections. It also\ncontains a module which auto-corrects evident errors. To maintain accuracy,\nSinSpell was designed as a rule-based system based on Hunspell. A set of words\nwas compiled from several sources and verified. These were divided into\nmorphological classes, and the valid roots, suffixes and prefixes for each\nclass were identified, together with lists of irregular words and exceptions.\nThe errors in a corpus of Sinhala documents were analysed and commonly\nmisspelled words and types of common errors were identified. We found that the\nmost common errors were in vowel length and similar sounding letters. Errors\ndue to incorrect typing and encoding were also found. This analysis was used to\ndevelop the suggestion generator and auto-corrector.",
          "link": "http://arxiv.org/abs/2107.02983",
          "publishedOn": "2021-07-08T01:57:56.138Z",
          "wordCount": 595,
          "title": "SinSpell: A Comprehensive Spelling Checker for Sinhala. (arXiv:2107.02983v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03007",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Huahuan Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1\">Wenjie Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jinsong Zhang</a>",
          "description": "Automatic speech recognition systems have been largely improved in the past\nfew decades and current systems are mainly hybrid-based and end-to-end-based.\nThe recently proposed CTC-CRF framework inherits the data-efficiency of the\nhybrid approach and the simplicity of the end-to-end approach. In this paper,\nwe further advance CTC-CRF based ASR technique with explorations on modeling\nunits and neural architectures. Specifically, we investigate techniques to\nenable the recently developed wordpiece modeling units and Conformer neural\nnetworks to be succesfully applied in CTC-CRFs. Experiments are conducted on\ntwo English datasets (Switchboard, Librispeech) and a German dataset from\nCommonVoice. Experimental results suggest that (i) Conformer can improve the\nrecognition performance significantly; (ii) Wordpiece-based systems perform\nslightly worse compared with phone-based systems for the target language with a\nlow degree of grapheme-phoneme correspondence (e.g. English), while the two\nsystems can perform equally strong when such degree of correspondence is high\nfor the target language (e.g. German).",
          "link": "http://arxiv.org/abs/2107.03007",
          "publishedOn": "2021-07-08T01:57:56.113Z",
          "wordCount": 606,
          "title": "Advancing CTC-CRF Based End-to-End Speech Recognition with Wordpieces and Conformers. (arXiv:2107.03007v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Daniel Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chao-Chun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1\">Keh-Yih Su</a>",
          "description": "We present a novel approach to answer the Chinese elementary school Social\nStudy Multiple Choice questions. Although BERT has demonstrated excellent\nperformance on Reading Comprehension tasks, it is found not good at handling\nsome specific types of questions, such as Negation, All-of-the-above, and\nNone-of-the-above. We thus propose a novel framework to cascade BERT with a\nPre-Processor and an Answer-Selector modules to tackle the above challenges.\nExperimental results show the proposed approach effectively improves the\nperformance of BERT, and thus demonstrate the feasibility of supplementing BERT\nwith additional modules.",
          "link": "http://arxiv.org/abs/2107.02893",
          "publishedOn": "2021-07-08T01:57:56.100Z",
          "wordCount": 520,
          "title": "Answering Chinese Elementary School Social Study Multiple Choice Questions. (arXiv:2107.02893v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alastruey_B/0/1/0/all/0/1\">Belen Alastruey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>",
          "description": "The advent of Transformer-based models has surpassed the barriers of text.\nWhen working with speech, we must face a problem: the sequence length of an\naudio input is not suitable for the Transformer. To bypass this problem, a\nusual approach is adding strided convolutional layers, to reduce the sequence\nlength before using the Transformer. In this paper, we propose a new approach\nfor direct Speech Translation, where thanks to an efficient Transformer we can\nwork with a spectrogram without having to use convolutional layers before the\nTransformer. This allows the encoder to learn directly from the spectrogram and\nno information is lost. We have created an encoder-decoder model, where the\nencoder is an efficient Transformer -- the Longformer -- and the decoder is a\ntraditional Transformer decoder. Our results, which are close to the ones\nobtained with the standard approach, show that this is a promising research\ndirection.",
          "link": "http://arxiv.org/abs/2107.03069",
          "publishedOn": "2021-07-08T01:57:56.089Z",
          "wordCount": 647,
          "title": "Efficient Transformer for Direct Speech Translation. (arXiv:2107.03069v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kraljevic_Z/0/1/0/all/0/1\">Zeljko Kraljevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shek_A/0/1/0/all/0/1\">Anthony Shek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bean_D/0/1/0/all/0/1\">Daniel Bean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendayan_R/0/1/0/all/0/1\">Rebecca Bendayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1\">James Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1\">Richard Dobson</a>",
          "description": "The data available in Electronic Health Records (EHRs) provides the\nopportunity to transform care, and the best way to provide better care for one\npatient is through learning from the data available on all other patients.\nTemporal modelling of a patient's medical history, which takes into account the\nsequence of past events, can be used to predict future events such as a\ndiagnosis of a new disorder or complication of a previous or existing disorder.\nWhile most prediction approaches use mostly the structured data in EHRs or a\nsubset of single-domain predictions and outcomes, we present MedGPT a novel\ntransformer-based pipeline that uses Named Entity Recognition and Linking tools\n(i.e. MedCAT) to structure and organize the free text portion of EHRs and\nanticipate a range of future medical events (initially disorders). Since a\nlarge portion of EHR data is in text form, such an approach benefits from a\ngranular and detailed view of a patient while introducing modest additional\nnoise. MedGPT effectively deals with the noise and the added granularity, and\nachieves a precision of 0.344, 0.552 and 0.640 (vs LSTM 0.329, 0.538 and 0.633)\nwhen predicting the top 1, 3 and 5 candidate future disorders on real world\nhospital data from King's College Hospital, London, UK (\\textasciitilde600k\npatients). We also show that our model captures medical knowledge by testing it\non an experimental medical multiple choice question answering task, and by\nexamining the attentional focus of the model using gradient-based saliency\nmethods.",
          "link": "http://arxiv.org/abs/2107.03134",
          "publishedOn": "2021-07-08T01:57:56.080Z",
          "wordCount": 683,
          "title": "MedGPT: Medical Concept Prediction from Clinical Narratives. (arXiv:2107.03134v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jessica Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_J/0/1/0/all/0/1\">Jeremy Goldwasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1\">Neha Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Wai Pan Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuzumlali_M/0/1/0/all/0/1\">Muhammed Yavuz Nuzumlal&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosand_B/0/1/0/all/0/1\">Benjamin Rosand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Matthew Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">David Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">R. Andrew Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krumholz_H/0/1/0/all/0/1\">Harlan M. Krumholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>",
          "description": "Electronic health records (EHRs), digital collections of patient healthcare\nevents and observations, are ubiquitous in medicine and critical to healthcare\ndelivery, operations, and research. Despite this central role, EHRs are\nnotoriously difficult to process automatically. Well over half of the\ninformation stored within EHRs is in the form of unstructured text (e.g.\nprovider notes, operation reports) and remains largely untapped for secondary\nuse. Recently, however, newer neural network and deep learning approaches to\nNatural Language Processing (NLP) have made considerable advances,\noutperforming traditional statistical and rule-based systems on a variety of\ntasks. In this survey paper, we summarize current neural NLP methods for EHR\napplications. We focus on a broad scope of tasks, namely, classification and\nprediction, word embeddings, extraction, generation, and other topics such as\nquestion answering, phenotyping, knowledge graphs, medical dialogue,\nmultilinguality, interpretability, etc.",
          "link": "http://arxiv.org/abs/2107.02975",
          "publishedOn": "2021-07-08T01:57:56.071Z",
          "wordCount": 615,
          "title": "Neural Natural Language Processing for Unstructured Data in Electronic Health Records: a Review. (arXiv:2107.02975v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1\">Daniel Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarlow_D/0/1/0/all/0/1\">Danny Tarlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1\">Rianne van den Berg</a>",
          "description": "Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown\nimpressive results on image and waveform generation in continuous state spaces.\nHere, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs),\ndiffusion-like generative models for discrete data that generalize the\nmultinomial diffusion model of Hoogeboom et al. 2021, by going beyond\ncorruption processes with uniform transition probabilities. This includes\ncorruption with transition matrices that mimic Gaussian kernels in continuous\nspace, matrices based on nearest neighbors in embedding space, and matrices\nthat introduce absorbing states. The third allows us to draw a connection\nbetween diffusion models and autoregressive and mask-based generative models.\nWe show that the choice of transition matrix is an important design decision\nthat leads to improved results in image and text domains. We also introduce a\nnew loss function that combines the variational lower bound with an auxiliary\ncross entropy loss. For text, this model class achieves strong results on\ncharacter-level text generation while scaling to large vocabularies on LM1B. On\nthe image dataset CIFAR-10, our models approach the sample quality and exceed\nthe log-likelihood of the continuous-space DDPM model.",
          "link": "http://arxiv.org/abs/2107.03006",
          "publishedOn": "2021-07-08T01:57:56.044Z",
          "wordCount": 641,
          "title": "Structured Denoising Diffusion Models in Discrete State-Spaces. (arXiv:2107.03006v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ernie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiue_Y/0/1/0/all/0/1\">Yow-Ting Shiue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_H/0/1/0/all/0/1\">Hui-Syuan Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>",
          "description": "In this paper, we aim to address the challenges surrounding the translation\nof ancient Chinese text: (1) The linguistic gap due to the difference in eras\nresults in translations that are poor in quality, and (2) most translations are\nmissing the contextual information that is often very crucial to understanding\nthe text. To this end, we improve upon past translation techniques by proposing\nthe following: We reframe the task as a multi-label prediction task where the\nmodel predicts both the translation and its particular era. We observe that\nthis helps to bridge the linguistic gap as chronological context is also used\nas auxiliary information. % As a natural step of generalization, we pivot on\nthe modern Chinese translations to generate multilingual outputs. %We show\nexperimentally the efficacy of our framework in producing quality translation\noutputs and also validate our framework on a collected task-specific parallel\ncorpus. We validate our framework on a parallel corpus annotated with\nchronology information and show experimentally its efficacy in producing\nquality translation outputs. We release both the code and the data\nhttps://github.com/orina1123/time-aware-ancient-text-translation for future\nresearch.",
          "link": "http://arxiv.org/abs/2107.03179",
          "publishedOn": "2021-07-08T01:57:56.035Z",
          "wordCount": 618,
          "title": "Time-Aware Ancient Chinese Text Translation and Inference. (arXiv:2107.03179v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiachong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>",
          "description": "With the development of dialogue systems and natural language generation\ntechniques, the resurgence of dialogue summarization has attracted significant\nresearch attentions, which aims to condense the original dialogue into a\nshorter version covering salient information. However, there remains a lack of\ncomprehensive survey for this task. To this end, we take the first step and\npresent a thorough review of this research field. In detail, we provide an\noverview of publicly available research datasets, summarize existing works\naccording to the domain of input dialogue as well as organize leaderboards\nunder unified metrics. Furthermore, we discuss some future directions and give\nour thoughts. We hope that this first survey of dialogue summarization can\nprovide the community with a quick access and a general picture to this task\nand motivate future researches.",
          "link": "http://arxiv.org/abs/2107.03175",
          "publishedOn": "2021-07-08T01:57:56.021Z",
          "wordCount": 564,
          "title": "A Survey on Dialogue Summarization: Recent Advances and New Frontiers. (arXiv:2107.03175v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1\">Sevil Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Can_B/0/1/0/all/0/1\">Burcu Can</a>",
          "description": "Android is among the most targeted platform by attackers. While attackers are\nimproving their techniques, traditional solutions based on static and dynamic\nanalysis have been also evolving. In addition to the application code, Android\napplications have some metadata that could be useful for security analysis of\napplications. Unlike traditional application distribution mechanisms, Android\napplications are distributed centrally in mobile markets. Therefore, beside\napplication packages, such markets contain app information provided by app\ndevelopers and app users. The availability of such useful textual data together\nwith the advancement in Natural Language Processing (NLP) that is used to\nprocess and understand textual data has encouraged researchers to investigate\nthe use of NLP techniques in Android security. Especially, security solutions\nbased on NLP have accelerated in the last 5 years and proven to be useful. This\nstudy reviews these proposals and aim to explore possible research directions\nfor future studies by presenting state-of-the-art in this domain. We mainly\nfocus on NLP-based solutions under four categories: description-to-behaviour\nfidelity, description generation, privacy and malware detection.",
          "link": "http://arxiv.org/abs/2107.03072",
          "publishedOn": "2021-07-08T01:57:56.012Z",
          "wordCount": 605,
          "title": "Android Security using NLP Techniques: A Review. (arXiv:2107.03072v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xueyuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E_H/0/1/0/all/0/1\">Haihong E</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haoran Luo</a>",
          "description": "Entity alignment (EA) is to discover entities referring to the same object in\nthe real world from different knowledge graphs (KGs). It plays an important\nrole in automatically integrating KGs from multiple sources.\n\nExisting knowledge graph embedding (KGE) methods based on Graph Neural\nNetworks (GNNs) have achieved promising results, which enhance entity\nrepresentation with relation information unidirectionally. Besides, more and\nmore methods introduce semi-supervision to ask for more labeled training data.\n\nHowever, two challenges still exist in these methods: (1) Insufficient\ninteraction: The interaction between entities and relations is insufficiently\nutilized. (2) Low-quality bootstrapping: The generated semi-supervised data is\nof low quality.\n\nIn this paper, we propose a novel framework, Echo Entity Alignment (EchoEA),\nwhich leverages self-attention mechanism to spread entity information to\nrelations and echo back to entities. The relation representation is dynamically\ncomputed from entity representation. Symmetrically, the next entity\nrepresentation is dynamically calculated from relation representation, which\nshows sufficient interaction.\n\nFurthermore, we propose attribute-combined bi-directional global-filtered\nstrategy (ABGS) to improve bootstrapping, reduce false samples and generate\nhigh-quality training data.\n\nThe experimental results on three real-world cross-lingual datasets are\nstable at around 96\\% at hits@1 on average, showing that our approach not only\nsignificantly outperforms the state-of-the-art methods, but also is universal\nand transferable for existing KGE methods.",
          "link": "http://arxiv.org/abs/2107.03054",
          "publishedOn": "2021-07-08T01:57:56.004Z",
          "wordCount": 650,
          "title": "EchoEA: Echo Information between Entities and Relations for Entity Alignment. (arXiv:2107.03054v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diomedi_D/0/1/0/all/0/1\">Daniel Diomedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogan_A/0/1/0/all/0/1\">Aidan Hogan</a>",
          "description": "The goal of Question Answering over Knowledge Graphs (KGQA) is to find\nanswers for natural language questions over a knowledge graph. Recent KGQA\napproaches adopt a neural machine translation (NMT) approach, where the natural\nlanguage question is translated into a structured query language. However, NMT\nsuffers from the out-of-vocabulary problem, where terms in a question may not\nhave been seen during training, impeding their translation. This issue is\nparticularly problematic for the millions of entities that large knowledge\ngraphs describe. We rather propose a KGQA approach that delegates the\nprocessing of entities to entity linking (EL) systems. NMT is then used to\ncreate a query template with placeholders that are filled by entities\nidentified in an EL phase. Slot filling is used to decide which entity fills\nwhich placeholder. Experiments for QA over Wikidata show that our approach\noutperforms pure NMT: while there remains a strong dependence on having seen\nsimilar query templates during training, errors relating to entities are\ngreatly reduced.",
          "link": "http://arxiv.org/abs/2107.02865",
          "publishedOn": "2021-07-08T01:57:55.977Z",
          "wordCount": 601,
          "title": "Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking. (arXiv:2107.02865v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seok Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunchang Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nam Soo Kim</a>",
          "description": "Most speech-to-text (S2T) translation studies use English speech as a source,\nwhich makes it difficult for non-English speakers to take advantage of the S2T\ntechnologies. For some languages, this problem was tackled through corpus\nconstruction, but the farther linguistically from English or the more\nunder-resourced, this deficiency and underrepresentedness becomes more\nsignificant. In this paper, we introduce kosp2e (read as `kospi'), a corpus\nthat allows Korean speech to be translated into English text in an end-to-end\nmanner. We adopt open license speech recognition corpus, translation corpus,\nand spoken language corpora to make our dataset freely available to the public,\nand check the performance through the pipeline and training-based approaches.\nUsing pipeline and various end-to-end schemes, we obtain the highest BLEU of\n21.3 and 18.0 for each based on the English hypothesis, validating the\nfeasibility of our data. We plan to supplement annotations for other target\nlanguages through community contributions in the future.",
          "link": "http://arxiv.org/abs/2107.02875",
          "publishedOn": "2021-07-08T01:57:55.956Z",
          "wordCount": 589,
          "title": "Kosp2e: Korean Speech to English Translation Corpus. (arXiv:2107.02875v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sterneck_R/0/1/0/all/0/1\">Rachel Sterneck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polish_A/0/1/0/all/0/1\">Annie Polish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowern_C/0/1/0/all/0/1\">Claire Bowern</a>",
          "description": "This article presents the results of investigations using topic modeling of\nthe Voynich Manuscript (Beinecke MS408). Topic modeling is a set of\ncomputational methods which are used to identify clusters of subjects within\ntext. We use latent dirichlet allocation, latent semantic analysis, and\nnonnegative matrix factorization to cluster Voynich pages into `topics'. We\nthen compare the topics derived from the computational models to clusters\nderived from the Voynich illustrations and from paleographic analysis. We find\nthat computationally derived clusters match closely to a conjunction of scribe\nand subject matter (as per the illustrations), providing further evidence that\nthe Voynich Manuscript contains meaningful text.",
          "link": "http://arxiv.org/abs/2107.02858",
          "publishedOn": "2021-07-08T01:57:55.946Z",
          "wordCount": 552,
          "title": "Topic Modeling in the Voynich Manuscript. (arXiv:2107.02858v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02852",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>",
          "description": "Speaker-attributed automatic speech recognition (SA-ASR) is a task to\nrecognize \"who spoke what\" from multi-talker recordings. An SA-ASR system\nusually consists of multiple modules such as speech separation, speaker\ndiarization and ASR. On the other hand, considering the joint optimization, an\nend-to-end (E2E) SA-ASR model has recently been proposed with promising results\non simulation data. In this paper, we present our recent study on the\ncomparison of such modular and joint approaches towards SA-ASR on real monaural\nrecordings. We develop state-of-the-art SA-ASR systems for both modular and\njoint approaches by leveraging large-scale training data, including 75 thousand\nhours of ASR training data and the VoxCeleb corpus for speaker representation\nlearning. We also propose a new pipeline that performs the E2E SA-ASR model\nafter speaker clustering. Our evaluation on the AMI meeting corpus reveals that\nafter fine-tuning with a small real data, the joint system performs 9.2--29.4%\nbetter in accuracy compared to the best modular system while the modular system\nperforms better before such fine-tuning. We also conduct various error analyses\nto show the remaining issues for the monaural SA-ASR.",
          "link": "http://arxiv.org/abs/2107.02852",
          "publishedOn": "2021-07-08T01:57:55.850Z",
          "wordCount": 654,
          "title": "A Comparative Study of Modular and Joint Approaches for Speaker-Attributed ASR on Monaural Long-Form Audio. (arXiv:2107.02852v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Escobar_Grisales_D/0/1/0/all/0/1\">Daniel Escobar-Grisales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasquez_Correa_J/0/1/0/all/0/1\">Juan Camilo Vasquez-Correa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orozco_Arroyave_J/0/1/0/all/0/1\">Juan Rafael Orozco-Arroyave</a>",
          "description": "The interest in demographic information retrieval based on text data has\nincreased in the research community because applications have shown success in\ndifferent sectors such as security, marketing, heath-care, and others.\nRecognition and identification of demographic traits such as gender, age,\nlocation, or personality based on text data can help to improve different\nmarketing strategies. For instance it makes it possible to segment and to\npersonalize offers, thus products and services are exposed to the group of\ngreatest interest. This type of technology has been discussed widely in\ndocuments from social media. However, the methods have been poorly studied in\ndata with a more formal structure, where there is no access to emoticons,\nmentions, and other linguistic phenomena that are only present in social media.\nThis paper proposes the use of recurrent and convolutional neural networks, and\na transfer learning strategy for gender recognition in documents that are\nwritten in informal and formal languages. Models are tested in two different\ndatabases consisting of Tweets and call-center conversations. Accuracies of up\nto 75\\% are achieved for both databases. The results also indicate that it is\npossible to transfer the knowledge from a system trained on a specific type of\nexpressions or idioms such as those typically used in social media into a more\nformal type of text data, where the amount of data is more scarce and its\nstructure is completely different.",
          "link": "http://arxiv.org/abs/2107.02759",
          "publishedOn": "2021-07-07T01:57:11.722Z",
          "wordCount": 673,
          "title": "Gender Recognition in Informal and Formal Language Scenarios via Transfer Learning. (arXiv:2107.02759v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Listenmaa_I/0/1/0/all/0/1\">Inari Listenmaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_J/0/1/0/all/0/1\">Jason Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_A/0/1/0/all/0/1\">Alfred Ang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanafiah_M/0/1/0/all/0/1\">Maryam Hanafiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheong_R/0/1/0/all/0/1\">Regina Cheong</a>",
          "description": "We present the NLG component for L4, a prototype domain-specific language\n(DSL) for drafting laws and contracts. As a concrete use case, we describe a\npipeline for a legal expert system created from L4 code. The NLG component is\nused in two steps. The first step is to create an interview, whose answers are\nprocessed into a query for an automated reasoner. The second step is to render\nthe answers of the reasoner in natural language.",
          "link": "http://arxiv.org/abs/2107.02421",
          "publishedOn": "2021-07-07T01:57:11.501Z",
          "wordCount": 518,
          "title": "An NLG pipeline for a legal expert system: a work in progress. (arXiv:2107.02421v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02418",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chun Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "In this paper, we investigate the problem of reasoning over natural language\nstatements. Prior neural based approaches do not explicitly consider the\ninter-dependency among answers and their proofs. In this paper, we propose\nPRobr, a novel approach for joint answer prediction and proof generation. PRobr\ndefines a joint probabilistic distribution over all possible proof graphs and\nanswers via an induced graphical model. We then optimize the model using\nvariational approximation on top of neural textual representation. Experiments\non multiple datasets under diverse settings (fully supervised, few-shot and\nzero-shot evaluation) verify the effectiveness of PRobr, e.g., achieving\n10%-30% improvement on QA accuracy in few/zero-shot evaluation. Our codes and\nmodels can be found at https://github.com/changzhisun/PRobr/.",
          "link": "http://arxiv.org/abs/2107.02418",
          "publishedOn": "2021-07-07T01:57:11.313Z",
          "wordCount": 559,
          "title": "Probabilistic Graph Reasoning for Natural Proof Generation. (arXiv:2107.02418v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Haibo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhe Feng</a>",
          "description": "We study the problem of building entity tagging systems by using a few rules\nas weak supervision. Previous methods mostly focus on disambiguation entity\ntypes based on contexts and expert-provided rules, while assuming entity spans\nare given. In this work, we propose a novel method TALLOR that bootstraps\nhigh-quality logical rules to train a neural tagger in a fully automated\nmanner. Specifically, we introduce compound rules that are composed from simple\nrules to increase the precision of boundary detection and generate more diverse\npseudo labels. We further design a dynamic label selection strategy to ensure\npseudo label quality and therefore avoid overfitting the neural tagger.\nExperiments on three datasets demonstrate that our method outperforms other\nweakly supervised methods and even rivals a state-of-the-art distantly\nsupervised tagger with a lexicon of over 2,000 terms when starting from only 20\nsimple rules. Our method can serve as a tool for rapidly building taggers in\nemerging domains and tasks. Case studies show that learned rules can\npotentially explain the predicted entities.",
          "link": "http://arxiv.org/abs/2107.02282",
          "publishedOn": "2021-07-07T01:57:10.660Z",
          "wordCount": 608,
          "title": "Weakly Supervised Named Entity Tagging with Learnable Logical Rules. (arXiv:2107.02282v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lucia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_N/0/1/0/all/0/1\">Neel Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Brandon R. Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1\">Peter Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>",
          "description": "While self-supervised learning has made rapid advances in natural language\nprocessing, it remains unclear when researchers should engage in\nresource-intensive domain-specific pretraining (domain pretraining). The law,\npuzzlingly, has yielded few documented instances of substantial gains to domain\npretraining in spite of the fact that legal language is widely seen to be\nunique. We hypothesize that these existing results stem from the fact that\nexisting legal NLP tasks are too easy and fail to meet conditions for when\ndomain pretraining can help. To address this, we first present CaseHOLD (Case\nHoldings On Legal Decisions), a new dataset comprised of over 53,000+ multiple\nchoice questions to identify the relevant holding of a cited case. This dataset\npresents a fundamental task to lawyers and is both legally meaningful and\ndifficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second,\nwe assess performance gains on CaseHOLD and existing legal NLP datasets. While\na Transformer architecture (BERT) pretrained on a general corpus (Google Books\nand Wikipedia) improves performance, domain pretraining (using corpus of\napproximately 3.5M decisions across all courts in the U.S. that is larger than\nBERT's) with a custom legal vocabulary exhibits the most substantial\nperformance gains with CaseHOLD (gain of 7.2% on F1, representing a 12%\nimprovement on BERT) and consistent performance gains across two other legal\ntasks. Third, we show that domain pretraining may be warranted when the task\nexhibits sufficient similarity to the pretraining corpus: the level of\nperformance increase in three legal tasks was directly tied to the domain\nspecificity of the task. Our findings inform when researchers should engage\nresource-intensive pretraining and show that Transformer-based architectures,\ntoo, learn embeddings suggestive of distinct legal language.",
          "link": "http://arxiv.org/abs/2104.08671",
          "publishedOn": "2021-07-07T01:57:10.597Z",
          "wordCount": 770,
          "title": "When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset. (arXiv:2104.08671v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hatonen_V/0/1/0/all/0/1\">Vili H&#xe4;t&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melzer_F/0/1/0/all/0/1\">Fiona Melzer</a>",
          "description": "Decades of research on climate have provided a consensus that human activity\nhas changed the climate and we are currently heading into a climate crisis.\nWhile public discussion and research efforts on climate change mitigation have\nincreased, potential solutions need to not only be discussed but also\neffectively deployed. For preventing mismanagement and holding policy makers\naccountable, transparency and degree of information about government processes\nhave been shown to be crucial. However, currently the quantity of information\nabout climate change discussions and the range of sources make it increasingly\ndifficult for the public and civil society to maintain an overview to hold\npoliticians accountable.\n\nIn response, we propose a multi-source topic aggregation system (MuSTAS)\nwhich processes policy makers speech and rhetoric from several publicly\navailable sources into an easily digestible topic summary. MuSTAS uses novel\nmulti-source hybrid latent Dirichlet allocation to model topics from a variety\nof documents. This topic digest will serve the general public and civil society\nin assessing where, how, and when politicians talk about climate and climate\npolicies, enabling them to hold politicians accountable for their actions to\nmitigate climate change and lack thereof.",
          "link": "http://arxiv.org/abs/2010.08346",
          "publishedOn": "2021-07-07T01:57:10.563Z",
          "wordCount": 684,
          "title": "From Talk to Action with Accountability: Monitoring the Public Discussion of Policy Makers with Deep Neural Networks and Topic Modelling. (arXiv:2010.08346v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_M/0/1/0/all/0/1\">Malcolm Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matt Botvinick</a>",
          "description": "Neural networks have achieved success in a wide array of perceptual tasks but\noften fail at tasks involving both perception and higher-level reasoning. On\nthese more challenging tasks, bespoke approaches (such as modular symbolic\ncomponents, independent dynamics models or semantic parsers) targeted towards\nthat specific type of task have typically performed better. The downside to\nthese targeted approaches, however, is that they can be more brittle than\ngeneral-purpose neural networks, requiring significant modification or even\nredesign according to the particular task at hand. Here, we propose a more\ngeneral neural-network-based approach to dynamic visual reasoning problems that\nobtains state-of-the-art performance on three different domains, in each case\noutperforming bespoke modular approaches tailored specifically to the task. Our\nmethod relies on learned object-centric representations, self-attention and\nself-supervised dynamics learning, and all three elements together are required\nfor strong performance to emerge. The success of this combination suggests that\nthere may be no need to trade off flexibility for performance on problems\ninvolving spatio-temporal or causal-style reasoning. With the right soft biases\nand learning objectives in a neural network we may be able to attain the best\nof both worlds.",
          "link": "http://arxiv.org/abs/2012.08508",
          "publishedOn": "2021-07-07T01:57:10.541Z",
          "wordCount": 676,
          "title": "Attention over learned object embeddings enables complex visual reasoning. (arXiv:2012.08508v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hei_Y/0/1/0/all/0/1\">Yiming Hei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Rui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "Schema-based event extraction is a critical technique to apprehend the\nessential content of events promptly. With the rapid development of deep\nlearning technology, event extraction technology based on deep learning has\nbecome a research hotspot. Numerous methods, datasets, and evaluation metrics\nhave been proposed in the literature, raising the need for a comprehensive and\nupdated survey. This paper fills the gap by reviewing the state-of-the-art\napproaches, focusing on deep learning-based models. We summarize the task\ndefinition, paradigm, and models of schema-based event extraction and then\ndiscuss each of these in detail. We introduce benchmark datasets that support\ntests of predictions and evaluation metrics. A comprehensive comparison between\ndifferent techniques is also provided in this survey. Finally, we conclude by\nsummarizing future research directions facing the research area.",
          "link": "http://arxiv.org/abs/2107.02126",
          "publishedOn": "2021-07-07T01:57:10.534Z",
          "wordCount": 591,
          "title": "Deep Learning Schema-based Event Extraction: Literature Review and Current Trends. (arXiv:2107.02126v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mollas_I/0/1/0/all/0/1\">Ioannis Mollas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysopoulou_Z/0/1/0/all/0/1\">Zoe Chrysopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlos_S/0/1/0/all/0/1\">Stamatis Karlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>",
          "description": "Online hate speech is a recent problem in our society that is rising at a\nsteady pace by leveraging the vulnerabilities of the corresponding regimes that\ncharacterise most social media platforms. This phenomenon is primarily fostered\nby offensive comments, either during user interaction or in the form of a\nposted multimedia context. Nowadays, giant corporations own platforms where\nmillions of users log in every day, and protection from exposure to similar\nphenomena appears to be necessary in order to comply with the corresponding\nlegislation and maintain a high level of service quality. A robust and reliable\nsystem for detecting and preventing the uploading of relevant content will have\na significant impact on our digitally interconnected society. Several aspects\nof our daily lives are undeniably linked to our social profiles, making us\nvulnerable to abusive behaviours. As a result, the lack of accurate hate speech\ndetection mechanisms would severely degrade the overall user experience,\nalthough its erroneous operation would pose many ethical concerns. In this\npaper, we present 'ETHOS', a textual dataset with two variants: binary and\nmulti-label, based on YouTube and Reddit comments validated using the\nFigure-Eight crowdsourcing platform. Furthermore, we present the annotation\nprotocol used to create this dataset: an active sampling procedure for\nbalancing our data in relation to the various aspects defined. Our key\nassumption is that, even gaining a small amount of labelled data from such a\ntime-consuming process, we can guarantee hate speech occurrences in the\nexamined material.",
          "link": "http://arxiv.org/abs/2006.08328",
          "publishedOn": "2021-07-07T01:57:10.528Z",
          "wordCount": 739,
          "title": "ETHOS: an Online Hate Speech Detection Dataset. (arXiv:2006.08328v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">S&#xf6;ren Auer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersen_T/0/1/0/all/0/1\">Ted Pedersen</a>",
          "description": "There is currently a gap between the natural language expression of scholarly\npublications and their structured semantic content modeling to enable\nintelligent content search. With the volume of research growing exponentially\nevery year, a search feature operating over semantically structured content is\ncompelling. The SemEval-2021 Shared Task NLPContributionGraph (a.k.a. 'the NCG\ntask') tasks participants to develop automated systems that structure\ncontributions from NLP scholarly articles in the English language. Being the\nfirst-of-its-kind in the SemEval series, the task released structured data from\nNLP scholarly articles at three levels of information granularity, i.e. at\nsentence-level, phrase-level, and phrases organized as triples toward Knowledge\nGraph (KG) building. The sentence-level annotations comprised the few sentences\nabout the article's contribution. The phrase-level annotations were scientific\nterm and predicate phrases from the contribution sentences. Finally, the\ntriples constituted the research overview KG. For the Shared Task,\nparticipating systems were then expected to automatically classify contribution\nsentences, extract scientific terms and relations from the sentences, and\norganize them as KG triples.\n\nOverall, the task drew a strong participation demographic of seven teams and\n27 participants. The best end-to-end task system classified contribution\nsentences at 57.27% F1, phrases at 46.41% F1, and triples at 22.28% F1. While\nthe absolute performance to generate triples remains low, in the conclusion of\nthis article, the difficulty of producing such data and as a consequence of\nmodeling it is highlighted.",
          "link": "http://arxiv.org/abs/2106.07385",
          "publishedOn": "2021-07-07T01:57:10.509Z",
          "wordCount": 732,
          "title": "SemEval-2021 Task 11: NLPContributionGraph -- Structuring Scholarly NLP Contributions for a Research Knowledge Graph. (arXiv:2106.07385v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neely_M/0/1/0/all/0/1\">Michael Neely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schouten_S/0/1/0/all/0/1\">Stefan F. Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleeker_M/0/1/0/all/0/1\">Maurits J. R. Bleeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1\">Ana Lucic</a>",
          "description": "By computing the rank correlation between attention weights and\nfeature-additive explanation methods, previous analyses either invalidate or\nsupport the role of attention-based explanations as a faithful and plausible\nmeasure of salience. To investigate whether this approach is appropriate, we\ncompare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and\nattention-based explanations, applied to two neural architectures trained on\nsingle- and pair-sequence language tasks. In most cases, we find that none of\nour chosen methods agree. Based on our empirical observations and theoretical\nobjections, we conclude that rank correlation does not measure the quality of\nfeature-additive methods. Practitioners should instead use the numerous and\nrigorous diagnostic methods proposed by the community.",
          "link": "http://arxiv.org/abs/2105.03287",
          "publishedOn": "2021-07-07T01:57:10.502Z",
          "wordCount": 604,
          "title": "Order in the Court: Explainable AI Methods Prone to Disagreement. (arXiv:2105.03287v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zineng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Since visual perception can give rich information beyond text descriptions\nfor world understanding, there has been increasing interest in leveraging\nvisual grounding for language learning. Recently, vokenization has attracted\nattention by using the predictions of a text-to-image retrieval model as labels\nfor language model supervision. Despite its success, the method suffers from\napproximation error of using finite image labels and the lack of vocabulary\ndiversity of a small image-text dataset. To overcome these limitations, we\npresent VidLanKD, a video-language knowledge distillation method for improving\nlanguage understanding. We train a multi-modal teacher model on a video-text\ndataset, and then transfer its knowledge to a student language model with a\ntext dataset. To avoid approximation error, we propose to use different\nknowledge distillation objectives. In addition, the use of a large-scale\nvideo-text dataset helps learn diverse and richer vocabularies. In our\nexperiments, VidLanKD achieves consistent improvements over text-only language\nmodels and vokenization models, on several downstream language understanding\ntasks including GLUE, SQuAD, and SWAG. We also demonstrate the improved world\nknowledge, physical reasoning, and temporal reasoning capabilities of our model\nby evaluating on the GLUE-diagnostics, PIQA, and TRACIE datasets. Lastly, we\npresent comprehensive ablation studies as well as visualizations of the learned\ntext-to-video grounding results of our teacher and student language models. Our\ncode and models are available at: https://github.com/zinengtang/VidLanKD",
          "link": "http://arxiv.org/abs/2107.02681",
          "publishedOn": "2021-07-07T01:57:10.493Z",
          "wordCount": 674,
          "title": "VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer. (arXiv:2107.02681v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuzi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "While recent text to speech (TTS) models perform very well in synthesizing\nreading-style (e.g., audiobook) speech, it is still challenging to synthesize\nspontaneous-style speech (e.g., podcast or conversation), mainly because of two\nreasons: 1) the lack of training data for spontaneous speech; 2) the difficulty\nin modeling the filled pauses (um and uh) and diverse rhythms in spontaneous\nspeech. In this paper, we develop AdaSpeech 3, an adaptive TTS system that\nfine-tunes a well-trained reading-style TTS model for spontaneous-style speech.\nSpecifically, 1) to insert filled pauses (FP) in the text sequence\nappropriately, we introduce an FP predictor to the TTS model; 2) to model the\nvarying rhythms, we introduce a duration predictor based on mixture of experts\n(MoE), which contains three experts responsible for the generation of fast,\nmedium and slow speech respectively, and fine-tune it as well as the pitch\npredictor for rhythm adaptation; 3) to adapt to other speaker timbre, we\nfine-tune some parameters in the decoder with few speech data. To address the\nchallenge of lack of training data, we mine a spontaneous speech dataset to\nsupport our research this work and facilitate future research on spontaneous\nTTS. Experiments show that AdaSpeech 3 synthesizes speech with natural FP and\nrhythms in spontaneous styles, and achieves much better MOS and SMOS scores\nthan previous adaptive TTS systems.",
          "link": "http://arxiv.org/abs/2107.02530",
          "publishedOn": "2021-07-07T01:57:10.486Z",
          "wordCount": 679,
          "title": "AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style. (arXiv:2107.02530v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02527",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gutierrez_E/0/1/0/all/0/1\">Elijah Gutierrez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oplustil_Gallegos_P/0/1/0/all/0/1\">Pilar Oplustil-Gallegos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>",
          "description": "Text-to-Speech synthesis systems are generally evaluated using Mean Opinion\nScore (MOS) tests, where listeners score samples of synthetic speech on a\nLikert scale. A major drawback of MOS tests is that they only offer a general\nmeasure of overall quality-i.e., the naturalness of an utterance-and so cannot\ntell us where exactly synthesis errors occur. This can make evaluation of the\nappropriateness of prosodic variation within utterances inconclusive. To\naddress this, we propose a novel evaluation method based on the Rapid Prosody\nTranscription paradigm. This allows listeners to mark the locations of errors\nin an utterance in real-time, providing a probabilistic representation of the\nperceptual errors that occur in the synthetic signal. We conduct experiments\nthat confirm that the fine-grained evaluation can be mapped to system rankings\nof standard MOS tests, but the error marking gives a much more comprehensive\nassessment of synthesized prosody. In particular, for standard audiobook test\nset samples, we see that error marks consistently cluster around words at major\nprosodic boundaries indicated by punctuation. However, for question-answer\nbased stimuli, where we control information structure, we see differences\nemerge in the ability of neural TTS systems to generate context-appropriate\nprosodic prominence.",
          "link": "http://arxiv.org/abs/2107.02527",
          "publishedOn": "2021-07-07T01:57:10.475Z",
          "wordCount": 658,
          "title": "Location, Location: Enhancing the Evaluation of Text-to-Speech Synthesis Using the Rapid Prosody Transcription Paradigm. (arXiv:2107.02527v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Golubev_A/0/1/0/all/0/1\">Anton Golubev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loukachevitch_N/0/1/0/all/0/1\">Natalia Loukachevitch</a>",
          "description": "In this study, we test transfer learning approach on Russian sentiment\nbenchmark datasets using additional train sample created with distant\nsupervision technique. We compare several variants of combining additional data\nwith benchmark train samples. The best results were achieved using three-step\napproach of sequential training on general, thematic and original train\nsamples. For most datasets, the results were improved by more than 3% to the\ncurrent state-of-the-art methods. The BERT-NLI model treating sentiment\nclassification problem as a natural language inference task reached the human\nlevel of sentiment analysis on one of the datasets.",
          "link": "http://arxiv.org/abs/2107.02499",
          "publishedOn": "2021-07-07T01:57:10.453Z",
          "wordCount": 526,
          "title": "Transfer Learning for Improving Results on Russian Sentiment Datasets. (arXiv:2107.02499v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02720",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Benedetto_M/0/1/0/all/0/1\">Maria-Gabriella Di Benedetto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shattuck_Hufnagel_S/0/1/0/all/0/1\">Stefanie Shattuck-Hufnagel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1\">Jeung-Yoon Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nardis_L/0/1/0/all/0/1\">Luca De Nardis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arango_J/0/1/0/all/0/1\">Javier Arango</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_I/0/1/0/all/0/1\">Ian Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+DeCaprio_A/0/1/0/all/0/1\">Alec DeCaprio</a>",
          "description": "Modelling the process that a listener actuates in deriving the words intended\nby a speaker requires setting a hypothesis on how lexical items are stored in\nmemory. This work aims at developing a system that imitates humans when\nidentifying words in running speech and, in this way, provide a framework to\nbetter understand human speech processing. We build a speech recognizer for\nItalian based on the principles of Stevens' model of Lexical Access in which\nwords are stored as hierarchical arrangements of distinctive features (Stevens,\nK. N. (2002). \"Toward a model for lexical access based on acoustic landmarks\nand distinctive features,\" J. Acoust. Soc. Am., 111(4):1872-1891). Over the\npast few decades, the Speech Communication Group at the Massachusetts Institute\nof Technology (MIT) developed a speech recognition system for English based on\nthis approach. Italian will be the first language beyond English to be\nexplored; the extension to another language provides the opportunity to test\nthe hypothesis that words are represented in memory as a set of\nhierarchically-arranged distinctive features, and reveal which of the\nunderlying mechanisms may have a language-independent nature. This paper also\nintroduces a new Lexical Access corpus, the LaMIT database, created and labeled\nspecifically for this work, that will be provided freely to the speech research\ncommunity. Future developments will test the hypothesis that specific acoustic\ndiscontinuities - called landmarks - that serve as cues to features, are\nlanguage independent, while other cues may be language-dependent, with powerful\nimplications for understanding how the human brain recognizes speech.",
          "link": "http://arxiv.org/abs/2107.02720",
          "publishedOn": "2021-07-07T01:57:10.446Z",
          "wordCount": 756,
          "title": "Lexical Access Model for Italian -- Modeling human speech processing: identification of words in running speech toward lexical access based on the detection of landmarks and other acoustic cues to features. (arXiv:2107.02720v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yi-Ling Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekiroglu_S/0/1/0/all/0/1\">Serra Sinem Tekiroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonelli_S/0/1/0/all/0/1\">Sara Tonelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1\">Marco Guerini</a>",
          "description": "Studies on online hate speech have mostly focused on the automated detection\nof harmful messages. Little attention has been devoted so far to the\ndevelopment of effective strategies to fight hate speech, in particular through\nthe creation of counter-messages. While existing manual scrutiny and\nintervention strategies are time-consuming and not scalable, advances in\nnatural language processing have the potential to provide a systematic approach\nto hatred management. In this paper, we introduce a novel ICT platform that NGO\noperators can use to monitor and analyze social media data, along with a\ncounter-narrative suggestion tool. Our platform aims at increasing the\nefficiency and effectiveness of operators' activities against islamophobia. We\ntest the platform with more than one hundred NGO operators in three countries\nthrough qualitative and quantitative evaluation. Results show that NGOs favor\nthe platform solution with the suggestion tool, and that the time required to\nproduce counter-narratives significantly decreases.",
          "link": "http://arxiv.org/abs/2107.02472",
          "publishedOn": "2021-07-07T01:57:10.439Z",
          "wordCount": 609,
          "title": "Empowering NGOs in Countering Online Hate Messages. (arXiv:2107.02472v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhibin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenchao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "Hierarchical topic models such as the gamma belief network (GBN) have\ndelivered promising results in mining multi-layer document representations and\ndiscovering interpretable topic taxonomies. However, they often assume in the\nprior that the topics at each layer are independently drawn from the Dirichlet\ndistribution, ignoring the dependencies between the topics both at the same\nlayer and across different layers. To relax this assumption, we propose\nsawtooth factorial topic embedding guided GBN, a deep generative model of\ndocuments that captures the dependencies and semantic similarities between the\ntopics in the embedding space. Specifically, both the words and topics are\nrepresented as embedding vectors of the same dimension. The topic matrix at a\nlayer is factorized into the product of a factor loading matrix and a topic\nembedding matrix, the transpose of which is set as the factor loading matrix of\nthe layer above. Repeating this particular type of factorization, which shares\ncomponents between adjacent layers, leads to a structure referred to as\nsawtooth factorization. An auto-encoding variational inference network is\nconstructed to optimize the model parameter via stochastic gradient descent.\nExperiments on big corpora show that our models outperform other neural topic\nmodels on extracting deeper interpretable topics and deriving better document\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.02757",
          "publishedOn": "2021-07-07T01:57:10.432Z",
          "wordCount": 649,
          "title": "Sawtooth Factorial Topic Embeddings Guided Gamma Belief Network. (arXiv:2107.02757v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nye_M/0/1/0/all/0/1\">Maxwell Nye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1\">Brenden M. Lake</a>",
          "description": "Human reasoning can often be understood as an interplay between two systems:\nthe intuitive and associative (\"System 1\") and the deliberative and logical\n(\"System 2\"). Neural sequence models -- which have been increasingly successful\nat performing complex, structured tasks -- exhibit the advantages and failure\nmodes of System 1: they are fast and learn patterns from data, but are often\ninconsistent and incoherent. In this work, we seek a lightweight, training-free\nmeans of improving existing System 1-like sequence models by adding System\n2-inspired logical reasoning. We explore several variations on this theme in\nwhich candidate generations from a neural sequence model are examined for\nlogical consistency by a symbolic reasoning module, which can either accept or\nreject the generations. Our approach uses neural inference to mediate between\nthe neural System 1 and the logical System 2. Results in robust story\ngeneration and grounded instruction-following show that this approach can\nincrease the coherence and accuracy of neurally-based generations.",
          "link": "http://arxiv.org/abs/2107.02794",
          "publishedOn": "2021-07-07T01:57:10.425Z",
          "wordCount": 607,
          "title": "Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning. (arXiv:2107.02794v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoqian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaowen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Laohu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Canan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>",
          "description": "This paper describes the submission of the NiuTrans end-to-end speech\ntranslation system for the IWSLT 2021 offline task, which translates from the\nEnglish audio to German text directly without intermediate transcription. We\nuse the Transformer-based model architecture and enhance it by Conformer,\nrelative position encoding, and stacked acoustic and textual encoding. To\naugment the training data, the English transcriptions are translated to German\ntranslations. Finally, we employ ensemble decoding to integrate the predictions\nfrom several models trained with the different datasets. Combining these\ntechniques, we achieve 33.84 BLEU points on the MuST-C En-De test set, which\nshows the enormous potential of the end-to-end model.",
          "link": "http://arxiv.org/abs/2107.02444",
          "publishedOn": "2021-07-07T01:57:10.403Z",
          "wordCount": 551,
          "title": "The NiuTrans End-to-End Speech Translation System \\\\for IWSLT 2021 Offline Task. (arXiv:2107.02444v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zixia Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>",
          "description": "This paper describes the system used in submission from SHANGHAITECH team to\nthe IWPT 2021 Shared Task. Our system is a graph-based parser with the\ntechnique of Automated Concatenation of Embeddings (ACE). Because recent work\nfound that better word representations can be obtained by concatenating\ndifferent types of embeddings, we use ACE to automatically find the better\nconcatenation of embeddings for the task of enhanced universal dependencies.\nAccording to official results averaged on 17 languages, our system ranks 2nd\nover 9 teams.",
          "link": "http://arxiv.org/abs/2107.02416",
          "publishedOn": "2021-07-07T01:57:10.393Z",
          "wordCount": 529,
          "title": "Enhanced Universal Dependency Parsing with Automated Concatenation of Embeddings. (arXiv:2107.02416v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zelasko_P/0/1/0/all/0/1\">Piotr &#x17b;elasko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappagari_R/0/1/0/all/0/1\">Raghavendra Pappagari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehak_N/0/1/0/all/0/1\">Najim Dehak</a>",
          "description": "Dialog acts can be interpreted as the atomic units of a conversation, more\nfine-grained than utterances, characterized by a specific communicative\nfunction. The ability to structure a conversational transcript as a sequence of\ndialog acts -- dialog act recognition, including the segmentation -- is\ncritical for understanding dialog. We apply two pre-trained transformer models,\nXLNet and Longformer, to this task in English and achieve strong results on\nSwitchboard Dialog Act and Meeting Recorder Dialog Act corpora with dialog act\nsegmentation error rates (DSER) of 8.4% and 14.2%. To understand the key\nfactors affecting dialog act recognition, we perform a comparative analysis of\nmodels trained under different conditions. We find that the inclusion of a\nbroader conversational context helps disambiguate many dialog act classes,\nespecially those infrequent in the training data. The presence of punctuation\nin the transcripts has a massive effect on the models' performance, and a\ndetailed analysis reveals specific segmentation patterns observed in its\nabsence. Finally, we find that the label set specificity does not affect dialog\nact segmentation performance. These findings have significant practical\nimplications for spoken language understanding applications that depend heavily\non a good-quality segmentation being available.",
          "link": "http://arxiv.org/abs/2107.02294",
          "publishedOn": "2021-07-07T01:57:10.352Z",
          "wordCount": 663,
          "title": "What Helps Transformers Recognize Conversational Structure? Importance of Context, Punctuation, and Labels in Dialog Act Recognition. (arXiv:2107.02294v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3$\\times$ as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results~(e.g., Top-1 accuracy 84.1% trained on 224$\\times$224\nImageNet-1K only), while being more scalable on high-resolution images. The\nmodels and source code will be released soon.",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-07T01:57:10.237Z",
          "wordCount": 671,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1\">Siddharth Karamcheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>",
          "description": "Active learning promises to alleviate the massive data needs of supervised\nmachine learning: it has successfully improved sample efficiency by an order of\nmagnitude on traditional tasks like topic classification and object\nrecognition. However, we uncover a striking contrast to this promise: across 5\nmodels and 4 datasets on the task of visual question answering, a wide variety\nof active learning approaches fail to outperform random selection. To\nunderstand this discrepancy, we profile 8 active learning methods on a\nper-example basis, and identify the problem as collective outliers -- groups of\nexamples that active learning methods prefer to acquire but models fail to\nlearn (e.g., questions that ask about text in images or require external\nknowledge). Through systematic ablation experiments and qualitative\nvisualizations, we verify that collective outliers are a general phenomenon\nresponsible for degrading pool-based active learning. Notably, we show that\nactive learning sample efficiency increases significantly as the number of\ncollective outliers in the active learning pool decreases. We conclude with a\ndiscussion and prescriptive recommendations for mitigating the effects of these\noutliers in future work.",
          "link": "http://arxiv.org/abs/2107.02331",
          "publishedOn": "2021-07-07T01:57:10.225Z",
          "wordCount": 650,
          "title": "Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering. (arXiv:2107.02331v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobian_H/0/1/0/all/0/1\">Hamed Yaghoobian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabnia_H/0/1/0/all/0/1\">Hamid R. Arabnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_K/0/1/0/all/0/1\">Khaled Rasheed</a>",
          "description": "Sarcasm detection is the task of identifying irony containing utterances in\nsentiment-bearing text. However, the figurative and creative nature of sarcasm\nposes a great challenge for affective computing systems performing sentiment\nanalysis. This article compiles and reviews the salient work in the literature\nof automatic sarcasm detection. Thus far, three main paradigm shifts have\noccurred in the way researchers have approached this task: 1) semi-supervised\npattern extraction to identify implicit sentiment, 2) use of hashtag-based\nsupervision, and 3) incorporation of context beyond target text. In this\narticle, we provide a comprehensive review of the datasets, approaches, trends,\nand issues in sarcasm and irony detection.",
          "link": "http://arxiv.org/abs/2107.02276",
          "publishedOn": "2021-07-07T01:57:10.201Z",
          "wordCount": 533,
          "title": "Sarcasm Detection: A Comparative Study. (arXiv:2107.02276v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huber_C/0/1/0/all/0/1\">Christian Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_J/0/1/0/all/0/1\">Juan Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuker_S/0/1/0/all/0/1\">Sebastian St&#xfc;ker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>",
          "description": "Neural sequence-to-sequence systems deliver state-of-the-art performance for\nautomatic speech recognition (ASR). When using appropriate modeling units,\ne.g., byte-pair encoded characters, these systems are in principal open\nvocabulary systems. In practice, however, they often fail to recognize words\nnot seen during training, e.g., named entities, numbers or technical terms. To\nalleviate this problem we supplement an end-to-end ASR system with a\nword/phrase memory and a mechanism to access this memory to recognize the words\nand phrases correctly. After the training of the ASR system, and when it has\nalready been deployed, a relevant word can be added or subtracted instantly\nwithout the need for further training. In this paper we demonstrate that\nthrough this mechanism our system is able to recognize more than 85% of newly\nadded words that it previously failed to recognize compared to a strong\nbaseline.",
          "link": "http://arxiv.org/abs/2107.02268",
          "publishedOn": "2021-07-07T01:57:10.170Z",
          "wordCount": 582,
          "title": "Instant One-Shot Word-Learning for Context-Specific Neural Sequence-to-Sequence Speech Recognition. (arXiv:2107.02268v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verlinden_S/0/1/0/all/0/1\">Severine Verlinden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaporojets_K/0/1/0/all/0/1\">Klim Zaporojets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1\">Johannes Deleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>",
          "description": "We consider a joint information extraction (IE) model, solving named entity\nrecognition, coreference resolution and relation extraction jointly over the\nwhole document. In particular, we study how to inject information from a\nknowledge base (KB) in such IE model, based on unsupervised entity linking. The\nused KB entity representations are learned from either (i) hyperlinked text\ndocuments (Wikipedia), or (ii) a knowledge graph (Wikidata), and appear\ncomplementary in raising IE performance. Representations of corresponding\nentity linking (EL) candidates are added to text span representations of the\ninput document, and we experiment with (i) taking a weighted average of the EL\ncandidate representations based on their prior (in Wikipedia), and (ii) using\nan attention scheme over the EL candidate list. Results demonstrate an increase\nof up to 5% F1-score for the evaluated IE tasks on two datasets. Despite a\nstrong performance of the prior-based model, our quantitative and qualitative\nanalysis reveals the advantage of using the attention-based approach.",
          "link": "http://arxiv.org/abs/2107.02286",
          "publishedOn": "2021-07-07T01:57:10.144Z",
          "wordCount": 603,
          "title": "Injecting Knowledge Base Information into End-to-End Joint Entity and Relation Extraction and Coreference Resolution. (arXiv:2107.02286v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lepekhin_M/0/1/0/all/0/1\">Mikhail Lepekhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharoff_S/0/1/0/all/0/1\">Serge Sharoff</a>",
          "description": "Neural models based on pre-trained transformers, such as BERT or XLM-RoBERTa,\ndemonstrate SOTA results in many NLP tasks, including non-topical\nclassification, such as genre identification. However, often these approaches\nexhibit low reliability to minor alterations of the test texts. A related\nprobelm concerns topical biases in the training corpus, for example, the\nprevalence of words on a specific topic in a specific genre can trick the genre\nclassifier to recognise any text on this topic in this genre. In order to\nmitigate the reliability problem, this paper investigates techniques for\nattacking genre classifiers to understand the limitations of the transformer\nmodels and to improve their performance. While simple text attacks, such as\nthose based on word replacement using keywords extracted by tf-idf, are not\ncapable of deceiving powerful models like XLM-RoBERTa, we show that\nembedding-based algorithms which can replace some of the most ``significant''\nwords with words similar to them, for example, TextFooler, have the ability to\ninfluence model predictions in a significant proportion of cases.",
          "link": "http://arxiv.org/abs/2107.02246",
          "publishedOn": "2021-07-07T01:57:10.133Z",
          "wordCount": 592,
          "title": "Experiments with adversarial attacks on text genres. (arXiv:2107.02246v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aimal_M/0/1/0/all/0/1\">Mohammad Aimal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakhtyar_M/0/1/0/all/0/1\">Maheen Bakhtyar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baber_J/0/1/0/all/0/1\">Junaid Baber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakho_S/0/1/0/all/0/1\">Sadia Lakho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_U/0/1/0/all/0/1\">Umar Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1\">Warda Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_J/0/1/0/all/0/1\">Jahanvash Karim</a>",
          "description": "Automatic sentiment analysis play vital role in decision making. Many\norganizations spend a lot of budget to understand their customer satisfaction\nby manually going over their feedback/comments or tweets. Automatic sentiment\nanalysis can give overall picture of the comments received against any event,\nproduct, or activity. Usually, the comments/tweets are classified into two main\nclasses that are negative or positive. However, the negative comments are too\nabstract to understand the basic reason or the context. organizations are\ninterested to identify the exact reason for the negativity. In this research\nstudy, we hierarchically goes down into negative comments, and link them with\nmore classes. Tweets are extracted from social media sites such as Twitter and\nFacebook. If the sentiment analysis classifies any tweet into negative class,\nthen we further try to associates that negative comments with more possible\nnegative classes. Based on expert opinions, the negative comments/tweets are\nfurther classified into 8 classes. Different machine learning algorithms are\nevaluated and their accuracy are reported.",
          "link": "http://arxiv.org/abs/2107.02175",
          "publishedOn": "2021-07-07T01:57:10.063Z",
          "wordCount": 625,
          "title": "Identifying negativity factors from social media text corpus using sentiment analysis method. (arXiv:2107.02175v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Congyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>",
          "description": "The majority of Chinese characters are monophonic, while a special group of\ncharacters, called polyphonic characters, have multiple pronunciations. As a\nprerequisite of performing speech-related generative tasks, the correct\npronunciation must be identified among several candidates. This process is\ncalled Polyphone Disambiguation. Although the problem has been well explored\nwith both knowledge-based and learning-based approaches, it remains challenging\ndue to the lack of publicly available labeled datasets and the irregular nature\nof polyphone in Mandarin Chinese. In this paper, we propose a novel\nsemi-supervised learning (SSL) framework for Mandarin Chinese polyphone\ndisambiguation that can potentially leverage unlimited unlabeled text data. We\nexplore the effect of various proxy labeling strategies including\nentropy-thresholding and lexicon-based labeling. Qualitative and quantitative\nexperiments demonstrate that our method achieves state-of-the-art performance.\nIn addition, we publish a novel dataset specifically for the polyphone\ndisambiguation task to promote further researches.",
          "link": "http://arxiv.org/abs/2102.00621",
          "publishedOn": "2021-07-06T01:58:07.256Z",
          "wordCount": 606,
          "title": "Polyphone Disambiguition in Mandarin Chinese with Semi-Supervised Learning. (arXiv:2102.00621v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S. M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-07-06T01:58:07.208Z",
          "wordCount": 641,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.08081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>",
          "description": "In sequence-to-sequence learning, the decoder relies on the attention\nmechanism to efficiently extract information from the encoder. While it is\ncommon practice to draw information from only the last encoder layer, recent\nwork has proposed to use representations from different encoder layers for\ndiversified levels of information. Nonetheless, the decoder still obtains only\na single view of the source sequences, which might lead to insufficient\ntraining of the encoder layer stack due to the hierarchy bypassing problem. In\nthis work, we propose layer-wise cross-view decoding, where for each decoder\nlayer, together with the representations from the last encoder layer, which\nserve as a global view, those from other encoder layers are supplemented for a\nstereoscopic view of the source sequences. Systematic experiments show that we\nsuccessfully address the hierarchy bypassing problem and substantially improve\nthe performance of sequence-to-sequence learning with deep representations on\ndiverse tasks.",
          "link": "http://arxiv.org/abs/2005.08081",
          "publishedOn": "2021-07-06T01:58:07.152Z",
          "wordCount": 637,
          "title": "Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning. (arXiv:2005.08081v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huiyuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>",
          "description": "Scarcity of parallel data causes formality style transfer models to have\nscarce success in preserving content. We show that fine-tuning pre-trained\nlanguage (GPT-2) and sequence-to-sequence (BART) models boosts content\npreservation, and that this is possible even with limited amounts of parallel\ndata. Augmenting these models with rewards that target style and content -- the\ntwo core aspects of the task -- we achieve a new state-of-the-art.",
          "link": "http://arxiv.org/abs/2105.06947",
          "publishedOn": "2021-07-06T01:58:07.137Z",
          "wordCount": 527,
          "title": "Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer. (arXiv:2105.06947v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lasota_S/0/1/0/all/0/1\">S&#x142;awomir Lasota</a>",
          "description": "Petri nets, equivalently presentable as vector addition systems with states,\nare an established model of concurrency with widespread applications. The\nreachability problem, where we ask whether from a given initial configuration\nthere exists a sequence of valid execution steps reaching a given final\nconfiguration, is the central algorithmic problem for this model. The\ncomplexity of the problem has remained, until recently, one of the hardest open\nquestions in verification of concurrent systems. A first upper bound has been\nprovided only in 2015 by Leroux and Schmitz, then refined by the same authors\nto non-primitive recursive Ackermannian upper bound in 2019. The exponential\nspace lower bound, shown by Lipton already in 1976, remained the only known for\nover 40 years until a breakthrough non-elementary lower bound by\nCzerwi{\\'n}ski, Lasota, Lazic, Leroux and Mazowiecki in 2019. Finally, a\nmatching Ackermannian lower bound announced this year by Czerwi{\\'n}ski and\nOrlikowski, and independently by Leroux, established the complexity of the\nproblem.\n\nOur contribution is an improvement of the former construction, making it\nconceptually simpler and more direct. On the way we improve the lower bound for\nvector addition systems with states in fixed dimension (or, equivalently, Petri\nnets with fixed number of places): while Czerwi{\\'n}ski and Orlikowski prove\n$F_k$-hardness (hardness for $k$th level in Grzegorczyk Hierarchy) in dimension\n$6k$, and Leroux in dimension $4k+5$, our simplified construction yields\n$F_k$-hardness already in dimension $3k+2$.",
          "link": "http://arxiv.org/abs/2105.08551",
          "publishedOn": "2021-07-06T01:58:07.119Z",
          "wordCount": 694,
          "title": "Improved Ackermannian lower bound for the Petri nets reachability problem. (arXiv:2105.08551v2 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_Z/0/1/0/all/0/1\">Zeinab Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ShamsFard_M/0/1/0/all/0/1\">Mehrnoush ShamsFard</a>",
          "description": "Detection of semantic contradictory sentences is one of the most challenging\nand fundamental issues for NLP applications such as recognition of textual\nentailments. Contradiction in this study includes different types of semantic\nconfrontation, such as conflict and antonymy. Due to lack of sufficient data to\napply precise machine learning and specifically deep learning methods to\nPersian and other low resource languages, rule-based approaches that can\nfunction similarly to these systems will be of a great interest. Also recently,\nemergence of new methods such as transfer learning, has opened up the\npossibility of deep learning for low-resource languages. Considering two above\npoints, in this study, along with a simple rule-base baseline, a novel\nrule-base system for identifying semantic contradiction along with a Bert base\ndeep contradiction detection system for Persian texts have been introduced. The\nrule base system has used frequent rule mining method to extract appropriate\ncontradiction rules using a development set. Extracted rules are tested for\ndifferent categories of contradictory sentences. In this system the maximum\nf-measure among contradiction categories is obtained for negation about 90% and\nthe average F-measure of system for all classes is about 76% which outperforms\nother algorithms on Persian texts. On the other hand, because of medium\nperformance of rule base system for some categories of contradiction, we use a\nBert base deep learning system using our translated dataset; with average\nF-measure of 73. Our hybrid system has f-measure of about 80.",
          "link": "http://arxiv.org/abs/2107.01987",
          "publishedOn": "2021-07-06T01:58:07.064Z",
          "wordCount": 667,
          "title": "Contradiction Detection in Persian Text. (arXiv:2107.01987v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbott_J/0/1/0/all/0/1\">Jade Abbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1\">Daniel D&#x27;souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1\">Chester Palen-Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzaaba_H/0/1/0/all/0/1\">Happy Buzaaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1\">Shruti Rijhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayhew_S/0/1/0/all/0/1\">Stephen Mayhew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azime_I/0/1/0/all/0/1\">Israel Abebe Azime</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakatumba_Nabende_J/0/1/0/all/0/1\">Joyce Nakatumba-Nabende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogayo_P/0/1/0/all/0/1\">Perez Ogayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aremu_A/0/1/0/all/0/1\">Anuoluwapo Aremu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gitau_C/0/1/0/all/0/1\">Catherine Gitau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mbaye_D/0/1/0/all/0/1\">Derguene Mbaye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1\">Seid Muhie Yimam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajuddeen Gwadabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezeani_I/0/1/0/all/0/1\">Ignatius Ezeani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niyongabo_R/0/1/0/all/0/1\">Rubungo Andre Niyongabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukiibi_J/0/1/0/all/0/1\">Jonathan Mukiibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otiende_V/0/1/0/all/0/1\">Verrah Otiende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orife_I/0/1/0/all/0/1\">Iroro Orife</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_D/0/1/0/all/0/1\">Davis David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngom_S/0/1/0/all/0/1\">Samba Ngom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayson_P/0/1/0/all/0/1\">Paul Rayson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofetoluwa Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muriuki_G/0/1/0/all/0/1\">Gerald Muriuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anebi_E/0/1/0/all/0/1\">Emmanuel Anebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chukwuneke_C/0/1/0/all/0/1\">Chiamaka Chukwuneke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odu_N/0/1/0/all/0/1\">Nkiruka Odu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wairagala_E/0/1/0/all/0/1\">Eric Peter Wairagala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyerinde_S/0/1/0/all/0/1\">Samuel Oyerinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siro_C/0/1/0/all/0/1\">Clemencia Siro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bateesa_T/0/1/0/all/0/1\">Tobius Saul Bateesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oloyede_T/0/1/0/all/0/1\">Temilola Oloyede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wambui_Y/0/1/0/all/0/1\">Yvonne Wambui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akinode_V/0/1/0/all/0/1\">Victor Akinode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabagereka_D/0/1/0/all/0/1\">Deborah Nabagereka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katusiime_M/0/1/0/all/0/1\">Maurice Katusiime</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awokoya_A/0/1/0/all/0/1\">Ayodele Awokoya</a>, et al. (15 additional authors not shown)",
          "description": "We take a step towards addressing the under-representation of the African\ncontinent in NLP research by creating the first large publicly available\nhigh-quality dataset for named entity recognition (NER) in ten African\nlanguages, bringing together a variety of stakeholders. We detail\ncharacteristics of the languages to help researchers understand the challenges\nthat these languages pose for NER. We analyze our datasets and conduct an\nextensive empirical evaluation of state-of-the-art methods across both\nsupervised and transfer learning settings. We release the data, code, and\nmodels in order to inspire future research on African NLP.",
          "link": "http://arxiv.org/abs/2103.11811",
          "publishedOn": "2021-07-06T01:58:07.037Z",
          "wordCount": 691,
          "title": "MasakhaNER: Named Entity Recognition for African Languages. (arXiv:2103.11811v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1\">Xin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kangping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_H/0/1/0/all/0/1\">Hui Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_H/0/1/0/all/0/1\">Hongying Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>",
          "description": "Artificial Intelligence (AI), along with the recent progress in biomedical\nlanguage understanding, is gradually changing medical practice. With the\ndevelopment of biomedical language understanding benchmarks, AI applications\nare widely used in the medical field. However, most benchmarks are limited to\nEnglish, which makes it challenging to replicate many of the successes in\nEnglish for other languages. To facilitate research in this direction, we\ncollect real-world biomedical data and present the first Chinese Biomedical\nLanguage Understanding Evaluation (CBLUE) benchmark: a collection of natural\nlanguage understanding tasks including named entity recognition, information\nextraction, clinical diagnosis normalization, single-sentence/sentence-pair\nclassification, and an associated online platform for model evaluation,\ncomparison, and analysis. To establish evaluation on these tasks, we report\nempirical results with the current 11 pre-trained Chinese models, and\nexperimental results show that state-of-the-art neural models perform by far\nworse than the human ceiling. Our benchmark is released at\n\\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&lang=en-us}.",
          "link": "http://arxiv.org/abs/2106.08087",
          "publishedOn": "2021-07-06T01:58:07.010Z",
          "wordCount": 653,
          "title": "CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. (arXiv:2106.08087v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Dongwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Miao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wei Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangang Li</a>",
          "description": "Self-supervised visual pretraining has shown significant progress recently.\nAmong those methods, SimCLR greatly advanced the state of the art in\nself-supervised and semi-supervised learning on ImageNet. The input feature\nrepresentations for speech and visual tasks are both continuous, so it is\nnatural to consider applying similar objective on speech representation\nlearning. In this paper, we propose Speech SimCLR, a new self-supervised\nobjective for speech representation learning. During training, Speech SimCLR\napplies augmentation on raw speech and its spectrogram. Its objective is the\ncombination of contrastive loss that maximizes agreement between differently\naugmented samples in the latent space and reconstruction loss of input\nrepresentation. The proposed method achieved competitive results on speech\nemotion recognition and speech recognition.",
          "link": "http://arxiv.org/abs/2010.13991",
          "publishedOn": "2021-07-06T01:58:06.990Z",
          "wordCount": 588,
          "title": "Speech SIMCLR: Combining Contrastive and Reconstruction Objective for Self-supervised Speech Representation Learning. (arXiv:2010.13991v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sedova_A/0/1/0/all/0/1\">Anastasiia Sedova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stephan_A/0/1/0/all/0/1\">Andreas Stephan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speranskaya_M/0/1/0/all/0/1\">Marina Speranskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>",
          "description": "Strategies for improving the training and prediction quality of weakly\nsupervised machine learning models vary in how much they are tailored to a\nspecific task or integrated with a specific model architecture. In this work,\nwe introduce Knodle, a software framework that treats weak data annotations,\ndeep learning models, and methods for improving weakly supervised training as\nseparate, modular components. This modularization gives the training process\naccess to fine-grained information such as data set characteristics, matches of\nheuristic rules, or elements of the deep learning model ultimately used for\nprediction. Hence, our framework can encompass a wide range of training methods\nfor improving weak supervision, ranging from methods that only look at\ncorrelations of rules and output classes (independently of the machine learning\nmodel trained with the resulting labels), to those that harness the interplay\nof neural networks and weakly labeled data. We illustrate the benchmarking\npotential of the framework with a performance comparison of several reference\nimplementations on a selection of datasets that are already available in\nKnodle.\n\nThe framework is published as an open-source Python package knodle and\navailable at https://github.com/knodle/knodle.",
          "link": "http://arxiv.org/abs/2104.11557",
          "publishedOn": "2021-07-06T01:58:06.975Z",
          "wordCount": 652,
          "title": "Knodle: Modular Weakly Supervised Learning with PyTorch. (arXiv:2104.11557v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jerry Zikun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoran Wang</a>",
          "description": "Query reformulation aims to alter noisy or ambiguous text sequences into\ncoherent ones closer to natural language questions. This is to prevent errors\nfrom propagating in a client-facing pipeline and promote better communication\nwith users. Besides, it is crucial to maintain performance in downstream\nenvironments like question answering when rephrased queries are given as input.\nWe show that under the previous framework (AQA), attempts to alter RL\nalgorithms do not bring significant benefits to either reward acquisition or\nsequence fluency. Instead, we leverage a query-reformulating text-to-text\ntransformer (QRT5) and apply policy-based RL algorithms to further nudge this\nreformulator and obtain better answers downstream by generating\nreward-acquiring query trajectories. QRT5 shows better sample efficiency in RL\nto achieve the same level of QA performance as the previous approach. It can\ngenerate reformulations with more readability based on query well-formedness\nevaluations and can generalize to out-of-sample data. Our framework is\ndemonstrated to be flexible, allowing reward signals to be sourced from\ndifferent downstream environments such as intent classification.",
          "link": "http://arxiv.org/abs/2012.10033",
          "publishedOn": "2021-07-06T01:58:06.967Z",
          "wordCount": 649,
          "title": "Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning. (arXiv:2012.10033v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a low-latency real-time (LLRT) non-parallel voice\nconversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\nand multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\nrobust non-parallel multispeaker spectral model, which utilizes a\nspeaker-independent latent space and a speaker-dependent code to generate\nreconstructed/converted spectral features given the spectral features of an\ninput speaker. On the other hand, MWDLP is an efficient and a high-quality\nneural vocoder that can handle multispeaker data and generate speech waveform\nfor LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\npropose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\nfeatures and is built with a sparse network architecture. Further, to improve\nthe modeling performance, we also propose a novel fine-tuning procedure that\nrefines the frame-rate CycleVAE network by utilizing the waveform loss from the\nMWDLP network. The experimental results demonstrate that the proposed framework\nachieves high-performance VC, while allowing for LLRT usage with a single-core\nof $2.1$--$2.7$ GHz CPU on a real-time factor of $0.87$--$0.95$, including\ninput/output, feature extraction, on a frame shift of $10$ ms, a window length\nof $27.5$ ms, and $2$ lookup frames.",
          "link": "http://arxiv.org/abs/2105.09858",
          "publishedOn": "2021-07-06T01:58:06.924Z",
          "wordCount": 681,
          "title": "Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hua Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Feng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Junyi An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Weikang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Furao Shen</a>",
          "description": "Subtext is a kind of deep semantics which can be acquired after one or more\nrounds of expression transformation. As a popular way of expressing one's\nintentions, it is well worth studying. In this paper, we try to make computers\nunderstand whether there is a subtext by means of machine learning. We build a\nChinese dataset whose source data comes from the popular social media (e.g.\nWeibo, Netease Music, Zhihu, and Bilibili). In addition, we also build a\nbaseline model called SASICM to deal with subtext recognition. The F1 score of\nSASICMg, whose pretrained model is GloVe, is as high as 64.37%, which is 3.97%\nhigher than that of BERT based model, 12.7% higher than that of traditional\nmethods on average, including support vector machine, logistic regression\nclassifier, maximum entropy classifier, naive bayes classifier and decision\ntree and 2.39% higher than that of the state-of-the-art, including MARIN and\nBTM. The F1 score of SASICMBERT, whose pretrained model is BERT, is 65.12%,\nwhich is 0.75% higher than that of SASICMg. The accuracy rates of SASICMg and\nSASICMBERT are 71.16% and 70.76%, respectively, which can compete with those of\nother methods which are mentioned before.",
          "link": "http://arxiv.org/abs/2106.06944",
          "publishedOn": "2021-07-06T01:58:06.912Z",
          "wordCount": 683,
          "title": "SASICM A Multi-Task Benchmark For Subtext Recognition. (arXiv:2106.06944v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1\">M. A&#xdf;enmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corvonato_A/0/1/0/all/0/1\">A. Corvonato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heumann_C/0/1/0/all/0/1\">C. Heumann</a>",
          "description": "The lack of a commonly used benchmark data set (collection) such as\n(Super-)GLUE (Wang et al., 2018, 2019) for the evaluation of non-English\npre-trained language models is a severe shortcoming of current English-centric\nNLP-research. It concentrates a large part of the research on English,\nneglecting the uncertainty when transferring conclusions found for the English\nlanguage to other languages. We evaluate the performance of the German and\nmultilingual BERT-based models currently available via the huggingface\ntransformers library on the four tasks of the GermEval17 workshop. We compare\nthem to pre-BERT architectures (Wojatzki et al., 2017; Schmitt et al., 2018;\nAttia et al., 2018) as well as to an ELMo-based architecture (Biesialska et\nal., 2020) and a BERT-based approach (Guhr et al., 2020). The observed\nimprovements are put in relation to those for similar tasks and similar models\n(pre-BERT vs. BERT-based) for the English language in order to draw tentative\nconclusions about whether the observed improvements are transferable to German\nor potentially other related languages.",
          "link": "http://arxiv.org/abs/2102.12330",
          "publishedOn": "2021-07-06T01:58:06.886Z",
          "wordCount": 642,
          "title": "Re-Evaluating GermEval17 Using German Pre-Trained Language Models. (arXiv:2102.12330v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a novel high-fidelity and low-latency universal neural\nvocoder framework based on multiband WaveRNN with data-driven linear prediction\nfor discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN\narchitecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit\nwith a relatively large size of hidden units is utilized, while the multiband\nmodeling is deployed to achieve real-time low-latency usage. A novel technique\nfor data-driven linear prediction (LP) with discrete waveform modeling is\nproposed, where the LP coefficients are estimated in a data-driven manner.\nMoreover, a novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed. The\nexperimental results demonstrate that the proposed MWDLP framework generates\nhigh-fidelity synthetic speech for seen and unseen speakers and/or language on\n300 speakers training data including clean and noisy/reverberant conditions,\nwhere the number of training utterances is limited to 60 per speaker, while\nallowing for real-time low-latency processing using a single core of $\\sim\\!$\n2.1--2.7 GHz CPU with $\\sim\\!$ 0.57--0.64 real-time factor including\ninput/output and feature extraction.",
          "link": "http://arxiv.org/abs/2105.09856",
          "publishedOn": "2021-07-06T01:58:06.869Z",
          "wordCount": 670,
          "title": "High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barry_J/0/1/0/all/0/1\">James Barry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Joachim Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>",
          "description": "We describe the DCU-EPFL submission to the IWPT 2021 Shared Task on Parsing\ninto Enhanced Universal Dependencies. The task involves parsing Enhanced UD\ngraphs, which are an extension of the basic dependency trees designed to be\nmore facilitative towards representing semantic structure. Evaluation is\ncarried out on 29 treebanks in 17 languages and participants are required to\nparse the data from each language starting from raw strings. Our approach uses\nthe Stanza pipeline to preprocess the text files, XLMRoBERTa to obtain\ncontextualized token representations, and an edge-scoring and labeling model to\npredict the enhanced graph. Finally, we run a post-processing script to ensure\nall of our outputs are valid Enhanced UD graphs. Our system places 6th out of 9\nparticipants with a coarse Enhanced Labeled Attachment Score (ELAS) of 83.57.\nWe carry out additional post-deadline experiments which include using Trankit\nfor pre-processing, XLM-RoBERTa-LARGE, treebank concatenation, and multitask\nlearning between a basic and an enhanced dependency parser. All of these\nmodifications improve our initial score and our final system has a coarse ELAS\nof 88.04.",
          "link": "http://arxiv.org/abs/2107.01982",
          "publishedOn": "2021-07-06T01:58:06.856Z",
          "wordCount": 635,
          "title": "The DCU-EPFL Enhanced Dependency Parser at the IWPT 2021 Shared Task. (arXiv:2107.01982v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yichi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Question: I have five fingers but I am not alive. What am I? Answer: a glove.\nAnswering such a riddle-style question is a challenging cognitive process, in\nthat it requires complex commonsense reasoning abilities, an understanding of\nfigurative language, and counterfactual reasoning skills, which are all\nimportant abilities for advanced natural language understanding (NLU). However,\nthere are currently no dedicated datasets aiming to test these abilities.\nHerein, we present RiddleSense, a new multiple-choice question answering task,\nwhich comes with the first large dataset (5.7k examples) for answering\nriddle-style commonsense questions. We systematically evaluate a wide range of\nmodels over the challenge, and point out that there is a large gap between the\nbest-supervised model and human performance -- suggesting intriguing future\nresearch in the direction of higher-order commonsense reasoning and linguistic\ncreativity towards building advanced NLU systems.",
          "link": "http://arxiv.org/abs/2101.00376",
          "publishedOn": "2021-07-06T01:58:06.653Z",
          "wordCount": 623,
          "title": "RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge. (arXiv:2101.00376v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nhung Thi-Hong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_P/0/1/0/all/0/1\">Phuong Phan-Dieu Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Luan Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "Customer product reviews play a role in improving the quality of products and\nservices for business organizations or their brands. Complaining is an attitude\nthat expresses dissatisfaction with an event or a product not meeting customer\nexpectations. In this paper, we build a Open-domain Complaint Detection dataset\n(UIT-ViOCD), including 5,485 human-annotated reviews on four categories about\nproduct reviews on e-commerce sites. After the data collection phase, we\nproceed to the annotation task and achieve the inter-annotator agreement Am of\n87%. Then, we present an extensive methodology for the research purposes and\nachieve 92.16% by F1-score for identifying complaints. With the results, in the\nfuture, we aim to build a system for open-domain complaint detection in\nE-commerce websites.",
          "link": "http://arxiv.org/abs/2104.11969",
          "publishedOn": "2021-07-06T01:58:06.456Z",
          "wordCount": 587,
          "title": "Vietnamese Complaint Detection on E-Commerce Websites. (arXiv:2104.11969v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Lanqing Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Duocai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Rap generation, which aims to produce lyrics and corresponding singing beats,\nneeds to model both rhymes and rhythms. Previous works for rap generation\nfocused on rhyming lyrics but ignored rhythmic beats, which are important for\nrap performance. In this paper, we develop DeepRapper, a Transformer-based rap\ngeneration system that can model both rhymes and rhythms. Since there is no\navailable rap dataset with rhythmic beats, we develop a data mining pipeline to\ncollect a large-scale rap dataset, which includes a large number of rap songs\nwith aligned lyrics and rhythmic beats. Second, we design a Transformer-based\nautoregressive language model which carefully models rhymes and rhythms.\nSpecifically, we generate lyrics in the reverse order with rhyme representation\nand constraint for rhyme enhancement and insert a beat symbol into lyrics for\nrhythm/beat modeling. To our knowledge, DeepRapper is the first system to\ngenerate rap with both rhymes and rhythms. Both objective and subjective\nevaluations demonstrate that DeepRapper generates creative and high-quality\nraps with rhymes and rhythms. Code will be released on GitHub.",
          "link": "http://arxiv.org/abs/2107.01875",
          "publishedOn": "2021-07-06T01:58:06.448Z",
          "wordCount": 636,
          "title": "DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling. (arXiv:2107.01875v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>",
          "description": "Sentiment analysis is a costly yet necessary task for enterprises to study\nthe opinions of their customers to improve their products and to determine\noptimal marketing strategies. Due to the existence of a wide range of domains\nacross different products and services, cross-domain sentiment analysis methods\nhave received significant attention. These methods mitigate the domain gap\nbetween different applications by training cross-domain generalizable\nclassifiers which help to relax the need for data annotation for each domain.\nMost existing methods focus on learning domain-agnostic representations that\nare invariant with respect to both the source and the target domains. As a\nresult, a classifier that is trained using the source domain annotated data\nwould generalize well in a related target domain. We introduce a new domain\nadaptation method which induces large margins between different classes in an\nembedding space. This embedding space is trained to be domain-agnostic by\nmatching the data distributions across the domains. Large intraclass margins in\nthe source domain help to reduce the effect of \"domain shift\" on the classifier\nperformance in the target domain. Theoretical and empirical analysis are\nprovided to demonstrate that the proposed method is effective.",
          "link": "http://arxiv.org/abs/2107.01598",
          "publishedOn": "2021-07-06T01:58:06.385Z",
          "wordCount": 624,
          "title": "Domain Adaptation for Sentiment Analysis Using Increased Intraclass Separation. (arXiv:2107.01598v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.00492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>",
          "description": "Sentiment analysis in conversations has gained increasing attention in recent\nyears for the growing amount of applications it can serve, e.g., sentiment\nanalysis, recommender systems, and human-robot interaction. The main difference\nbetween conversational sentiment analysis and single sentence sentiment\nanalysis is the existence of context information which may influence the\nsentiment of an utterance in a dialogue. How to effectively encode contextual\ninformation in dialogues, however, remains a challenge. Existing approaches\nemploy complicated deep learning structures to distinguish different parties in\na conversation and then model the context information. In this paper, we\npropose a fast, compact and parameter-efficient party-ignorant framework named\nbidirectional emotional recurrent unit for conversational sentiment analysis.\nIn our system, a generalized neural tensor block followed by a two-channel\nclassifier is designed to perform context compositionality and sentiment\nclassification, respectively. Extensive experiments on three standard datasets\ndemonstrate that our model outperforms the state of the art in most cases.",
          "link": "http://arxiv.org/abs/2006.00492",
          "publishedOn": "2021-07-06T01:58:06.375Z",
          "wordCount": 633,
          "title": "BiERU: Bidirectional Emotional Recurrent Unit for Conversational Sentiment Analysis. (arXiv:2006.00492v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>",
          "description": "Data augmentation has recently seen increased interest in NLP due to more\nwork in low-resource domains, new tasks, and the popularity of large-scale\nneural networks that require large amounts of training data. Despite this\nrecent upsurge, this area is still relatively underexplored, perhaps due to the\nchallenges posed by the discrete nature of language data. In this paper, we\npresent a comprehensive and unifying survey of data augmentation for NLP by\nsummarizing the literature in a structured manner. We first introduce and\nmotivate data augmentation for NLP, and then discuss major methodologically\nrepresentative approaches. Next, we highlight techniques that are used for\npopular NLP applications and tasks. We conclude by outlining current challenges\nand directions for future research. Overall, our paper aims to clarify the\nlandscape of existing literature in data augmentation for NLP and motivate\nadditional work in this area. We also present a GitHub repository with a paper\nlist that will be continuously updated at\nhttps://github.com/styfeng/DataAug4NLP",
          "link": "http://arxiv.org/abs/2105.03075",
          "publishedOn": "2021-07-06T01:58:06.367Z",
          "wordCount": 672,
          "title": "A Survey of Data Augmentation Approaches for NLP. (arXiv:2105.03075v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mingyue Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yinglin Wang</a>",
          "description": "Pretrained language models (PLM) achieve surprising performance on the Choice\nof Plausible Alternatives (COPA) task. However, whether PLMs have truly\nacquired the ability of causal reasoning remains a question. In this paper, we\ninvestigate the problem of semantic similarity bias and reveal the\nvulnerability of current COPA models by certain attacks. Previous solutions\nthat tackle the superficial cues of unbalanced token distribution still\nencounter the same problem of semantic bias, even more seriously due to the\nutilization of more training data. We mitigate this problem by simply adding a\nregularization loss and experimental results show that this solution not only\nimproves the model's generalization ability, but also assists the models to\nperform more robustly on a challenging dataset, BCOPA-CE, which has unbiased\ntoken distribution and is more difficult for models to distinguish cause and\neffect.",
          "link": "http://arxiv.org/abs/2107.01791",
          "publishedOn": "2021-07-06T01:58:06.331Z",
          "wordCount": 578,
          "title": "Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models. (arXiv:2107.01791v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.13662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>",
          "description": "In this paper, we propose FFCI, a framework for fine-grained summarization\nevaluation that comprises four elements: faithfulness (degree of factual\nconsistency with the source), focus (precision of summary content relative to\nthe reference), coverage (recall of summary content relative to the reference),\nand inter-sentential coherence (document fluency between adjacent sentences).\nWe construct a novel dataset for focus, coverage, and inter-sentential\ncoherence, and develop automatic methods for evaluating each of the four\ndimensions of FFCI based on cross-comparison of evaluation metrics and\nmodel-based evaluation methods, including question answering (QA) approaches,\nSTS, next-sentence prediction (NSP), and scores derived from 19 pre-trained\nlanguage models. We then apply the developed metrics in evaluating a broad\nrange of summarization models across two datasets, with some surprising\nfindings.",
          "link": "http://arxiv.org/abs/2011.13662",
          "publishedOn": "2021-07-06T01:58:06.323Z",
          "wordCount": 594,
          "title": "FFCI: A Framework for Interpretable Automatic Evaluation of Summarization. (arXiv:2011.13662v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>",
          "description": "With the advent of globalization, there is an increasing demand for\nmultilingual automatic speech recognition (ASR), handling language and\ndialectal variation of spoken content. Recent studies show its efficacy over\nmonolingual systems. In this study, we design a large multilingual end-to-end\nASR using self-attention based conformer architecture. We trained the system\nusing Arabic (Ar), English (En) and French (Fr) languages. We evaluate the\nsystem performance handling: (i) monolingual (Ar, En and Fr); (ii)\nmulti-dialectal (Modern Standard Arabic, along with dialectal variation such as\nEgyptian and Moroccan); (iii) code-switching -- cross-lingual (Ar-En/Fr) and\ndialectal (MSA-Egyptian dialect) test cases, and compare with current\nstate-of-the-art systems. Furthermore, we investigate the influence of\ndifferent embedding/character representations including character vs\nword-piece; shared vs distinct input symbol per language. Our findings\ndemonstrate the strength of such a model by outperforming state-of-the-art\nmonolingual dialectal Arabic and code-switching Arabic ASR.",
          "link": "http://arxiv.org/abs/2105.14779",
          "publishedOn": "2021-07-06T01:58:06.313Z",
          "wordCount": 651,
          "title": "Towards One Model to Rule All: Multilingual Strategy for Dialectal Code-Switching Arabic ASR. (arXiv:2105.14779v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1\">Hidetaka Kamigaito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Katsuhiko Hayashi</a>",
          "description": "In knowledge graph embedding, the theoretical relationship between the\nsoftmax cross-entropy and negative sampling loss functions has not been\ninvestigated. This makes it difficult to fairly compare the results of the two\ndifferent loss functions. We attempted to solve this problem by using the\nBregman divergence to provide a unified interpretation of the softmax\ncross-entropy and negative sampling loss functions. Under this interpretation,\nwe can derive theoretical findings for fair comparison. Experimental results on\nthe FB15k-237 and WN18RR datasets show that the theoretical findings are valid\nin practical settings.",
          "link": "http://arxiv.org/abs/2106.07250",
          "publishedOn": "2021-07-06T01:58:06.289Z",
          "wordCount": 570,
          "title": "Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding. (arXiv:2106.07250v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Manh Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doo Soon Kim</a>",
          "description": "Since the first end-to-end neural coreference resolution model was\nintroduced, many extensions to the model have been proposed, ranging from using\nhigher-order inference to directly optimizing evaluation metrics using\nreinforcement learning. Despite improving the coreference resolution\nperformance by a large margin, these extensions add a lot of extra complexity\nto the original model. Motivated by this observation and the recent advances in\npre-trained Transformer language models, we propose a simple yet effective\nbaseline for coreference resolution. Our model is a simplified version of the\noriginal neural coreference resolution model, however, it achieves impressive\nperformance, outperforming all recent extended works on the public English\nOntoNotes benchmark. Our work provides evidence for the necessity of carefully\njustifying the complexity of existing or newly proposed models, as introducing\na conceptual or practical simplification to an existing model can still yield\ncompetitive results.",
          "link": "http://arxiv.org/abs/2107.01700",
          "publishedOn": "2021-07-06T01:58:06.276Z",
          "wordCount": 576,
          "title": "End-to-end Neural Coreference Resolution Revisited: A Simple yet Effective Baseline. (arXiv:2107.01700v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1\">Mana Ihori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriya_T/0/1/0/all/0/1\">Takafumi Moriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashihara_T/0/1/0/all/0/1\">Takanori Ashihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1\">Shota Orihashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>",
          "description": "We propose a cross-modal transformer-based neural correction models that\nrefines the output of an automatic speech recognition (ASR) system so as to\nexclude ASR errors. Generally, neural correction models are composed of\nencoder-decoder networks, which can directly model sequence-to-sequence mapping\nproblems. The most successful method is to use both input speech and its ASR\noutput text as the input contexts for the encoder-decoder networks. However,\nthe conventional method cannot take into account the relationships between\nthese two different modal inputs because the input contexts are separately\nencoded for each modal. To effectively leverage the correlated information\nbetween the two different modal inputs, our proposed models encode two\ndifferent contexts jointly on the basis of cross-modal self-attention using a\ntransformer. We expect that cross-modal self-attention can effectively capture\nthe relationships between two different modals for refining ASR hypotheses. We\nalso introduce a shallow fusion technique to efficiently integrate the\nfirst-pass ASR model and our proposed neural correction model. Experiments on\nJapanese natural language ASR tasks demonstrated that our proposed models\nachieve better ASR performance than conventional neural correction models.",
          "link": "http://arxiv.org/abs/2107.01569",
          "publishedOn": "2021-07-06T01:58:06.201Z",
          "wordCount": 629,
          "title": "Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition. (arXiv:2107.01569v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01545",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Horiguchi_S/0/1/0/all/0/1\">Shota Horiguchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_P/0/1/0/all/0/1\">Paola Garcia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_Y/0/1/0/all/0/1\">Yawen Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takashima_Y/0/1/0/all/0/1\">Yuki Takashima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawaguchi_Y/0/1/0/all/0/1\">Yohei Kawaguchi</a>",
          "description": "Attractor-based end-to-end diarization is achieving comparable accuracy to\nthe carefully tuned conventional clustering-based methods on challenging\ndatasets. However, the main drawback is that it cannot deal with the case where\nthe number of speakers is larger than the one observed during training. This is\nbecause its speaker counting relies on supervised learning. In this work, we\nintroduce an unsupervised clustering process embedded in the attractor-based\nend-to-end diarization. We first split a sequence of frame-wise embeddings into\nshort subsequences and then perform attractor-based diarization for each\nsubsequence. Given subsequence-wise diarization results, inter-subsequence\nspeaker correspondence is obtained by unsupervised clustering of the vectors\ncomputed from the attractors from all the subsequences. This makes it possible\nto produce diarization results of a large number of speakers for the whole\nrecording even if the number of output speakers for each subsequence is\nlimited. Experimental results showed that our method could produce accurate\ndiarization results of an unseen number of speakers. Our method achieved 11.84\n%, 28.33 %, and 19.49 % on the CALLHOME, DIHARD II, and DIHARD III datasets,\nrespectively, each of which is better than the conventional end-to-end\ndiarization methods.",
          "link": "http://arxiv.org/abs/2107.01545",
          "publishedOn": "2021-07-06T01:58:06.189Z",
          "wordCount": 647,
          "title": "Towards Neural Diarization for Unlimited Numbers of Speakers Using Global and Local Attractors. (arXiv:2107.01545v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaitang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>",
          "description": "The goal of text generation is to make machines express in human language. It\nis one of the most important yet challenging tasks in natural language\nprocessing (NLP). Since 2014, various neural encoder-decoder models pioneered\nby Seq2Seq have been proposed to achieve the goal by learning to map input text\nto output text. However, the input text alone often provides limited knowledge\nto generate the desired output, so the performance of text generation is still\nfar from satisfaction in many real-world scenarios. To address this issue,\nresearchers have considered incorporating various forms of knowledge beyond the\ninput text into the generation models. This research direction is known as\nknowledge-enhanced text generation. In this survey, we present a comprehensive\nreview of the research on knowledge enhanced text generation over the past five\nyears. The main content includes two parts: (i) general methods and\narchitectures for integrating knowledge into text generation; (ii) specific\ntechniques and applications according to different forms of knowledge data.\nThis survey can have broad audiences, researchers and practitioners, in\nacademia and industry.",
          "link": "http://arxiv.org/abs/2010.04389",
          "publishedOn": "2021-07-06T01:58:06.176Z",
          "wordCount": 660,
          "title": "A Survey of Knowledge-Enhanced Text Generation. (arXiv:2010.04389v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01583",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hei_Y/0/1/0/all/0/1\">Yiming Hei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbo Xu</a>",
          "description": "Event extraction (EE) is a crucial information extraction task that aims to\nextract event information in texts. Most existing methods assume that events\nappear in sentences without overlaps, which are not applicable to the\ncomplicated overlapping event extraction. This work systematically studies the\nrealistic event overlapping problem, where a word may serve as triggers with\nseveral types or arguments with different roles. To tackle the above problem,\nwe propose a novel joint learning framework with cascade decoding for\noverlapping event extraction, termed as CasEE. Particularly, CasEE sequentially\nperforms type detection, trigger extraction and argument extraction, where the\noverlapped targets are extracted separately conditioned on the specific former\nprediction. All the subtasks are jointly learned in a framework to capture\ndependencies among the subtasks. The evaluation on a public event extraction\nbenchmark FewFC demonstrates that CasEE achieves significant improvements on\noverlapping event extraction over previous competitive methods.",
          "link": "http://arxiv.org/abs/2107.01583",
          "publishedOn": "2021-07-06T01:58:06.158Z",
          "wordCount": 599,
          "title": "CasEE: A Joint Learning Framework with Cascade Decoding for Overlapping Event Extraction. (arXiv:2107.01583v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "Neural Machine Translation (NMT) is a predominant machine translation\ntechnology nowadays because of its end-to-end trainable flexibility. However,\nNMT still struggles to translate properly in low-resource settings specifically\non distant language pairs. One way to overcome this is to use the information\nfrom other modalities if available. The idea is that despite differences in\nlanguages, both the source and target language speakers see the same thing and\nthe visual representation of both the source and target is the same, which can\npositively assist the system. Multimodal information can help the NMT system to\nimprove the translation by removing ambiguity on some phrases or words. We\nparticipate in the 8th Workshop on Asian Translation (WAT - 2021) for\nEnglish-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU\npoints for Evaluation and Challenge subset, respectively.",
          "link": "http://arxiv.org/abs/2107.01656",
          "publishedOn": "2021-07-06T01:58:06.118Z",
          "wordCount": 573,
          "title": "IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task. (arXiv:2107.01656v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>",
          "description": "It is prevalent to utilize external knowledge to help machine answer\nquestions that need background commonsense, which faces a problem that\nunlimited knowledge will transmit noisy and misleading information. Towards the\nissue of introducing related knowledge, we propose a semantic-driven\nknowledge-aware QA framework, which controls the knowledge injection in a\ncoarse-to-careful fashion. We devise a tailoring strategy to filter extracted\nknowledge under monitoring of the coarse semantic of question on the knowledge\nextraction stage. And we develop a semantic-aware knowledge fetching module\nthat engages structural knowledge information and fuses proper knowledge\naccording to the careful semantic of questions in a hierarchical way.\nExperiments demonstrate that the proposed approach promotes the performance on\nthe CommonsenseQA dataset comparing with strong baselines.",
          "link": "http://arxiv.org/abs/2107.01592",
          "publishedOn": "2021-07-06T01:58:06.105Z",
          "wordCount": 557,
          "title": "Coarse-to-Careful: Seeking Semantic-related Knowledge for Open-domain Commonsense Question Answering. (arXiv:2107.01592v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okamura_D/0/1/0/all/0/1\">Daiki Okamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1\">Mana Ihori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1\">Shota Orihashi</a>",
          "description": "In this paper, we present a novel modeling method for single-channel\nmulti-talker overlapped automatic speech recognition (ASR) systems. Fully\nneural network based end-to-end models have dramatically improved the\nperformance of multi-taker overlapped ASR tasks. One promising approach for\nend-to-end modeling is autoregressive modeling with serialized output training\nin which transcriptions of multiple speakers are recursively generated one\nafter another. This enables us to naturally capture relationships between\nspeakers. However, the conventional modeling method cannot explicitly take into\naccount the speaker attributes of individual utterances such as gender and age\ninformation. In fact, the performance deteriorates when each speaker is the\nsame gender or is close in age. To address this problem, we propose unified\nautoregressive modeling for joint end-to-end multi-talker overlapped ASR and\nspeaker attribute estimation. Our key idea is to handle gender and age\nestimation tasks within the unified autoregressive modeling. In the proposed\nmethod, transformer-based autoregressive model recursively generates not only\ntextual tokens but also attribute tokens of each speaker. This enables us to\neffectively utilize speaker attributes for improving multi-talker overlapped\nASR. Experiments on Japanese multi-talker overlapped ASR tasks demonstrate the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2107.01549",
          "publishedOn": "2021-07-06T01:58:06.097Z",
          "wordCount": 655,
          "title": "Unified Autoregressive Modeling for Joint End-to-End Multi-Talker Overlapped Speech Recognition and Speaker Attribute Estimation. (arXiv:2107.01549v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Helin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "While Machine Comprehension (MC) has attracted extensive research interests\nin recent years, existing approaches mainly belong to the category of Machine\nReading Comprehension task which mines textual inputs (paragraphs and\nquestions) to predict the answers (choices or text spans). However, there are a\nlot of MC tasks that accept audio input in addition to the textual input, e.g.\nEnglish listening comprehension test. In this paper, we target the problem of\nAudio-Oriented Multimodal Machine Comprehension, and its goal is to answer\nquestions based on the given audio and textual information. To solve this\nproblem, we propose a Dynamic Inter- and Intra-modality Attention (DIIA) model\nto effectively fuse the two modalities (audio and textual). DIIA can work as an\nindependent component and thus be easily integrated into existing MC models.\nMoreover, we further develop a Multimodal Knowledge Distillation (MKD) module\nto enable our multimodal MC model to accurately predict the answers based only\non either the text or the audio. As a result, the proposed approach can handle\nvarious tasks including: Audio-Oriented Multimodal Machine Comprehension,\nMachine Reading Comprehension and Machine Listening Comprehension, in a single\nmodel, making fair comparisons possible between our model and the existing\nunimodal MC models. Experimental results and analysis prove the effectiveness\nof the proposed approaches. First, the proposed DIIA boosts the baseline models\nby up to 21.08% in terms of accuracy; Second, under the unimodal scenarios, the\nMKD module allows our multimodal MC model to significantly outperform the\nunimodal models by up to 18.87%, which are trained and tested with only audio\nor textual data.",
          "link": "http://arxiv.org/abs/2107.01571",
          "publishedOn": "2021-07-06T01:58:06.088Z",
          "wordCount": 698,
          "title": "Audio-Oriented Multimodal Machine Comprehension: Task, Dataset and Model. (arXiv:2107.01571v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01275",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lohrenz_T/0/1/0/all/0/1\">Timo Lohrenz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwarz_P/0/1/0/all/0/1\">Patrick Schwarz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhengyang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>",
          "description": "Recently, attention-based encoder-decoder (AED) models have shown high\nperformance for end-to-end automatic speech recognition (ASR) across several\ntasks. Addressing overconfidence in such models, in this paper we introduce the\nconcept of relaxed attention, which is a simple gradual injection of a uniform\ndistribution to the encoder-decoder attention weights during training that is\neasily implemented with two lines of code. We investigate the effect of relaxed\nattention across different AED model architectures and two prominent ASR tasks,\nWall Street Journal (WSJ) and Librispeech. We found that transformers trained\nwith relaxed attention outperform the standard baseline models consistently\nduring decoding with external language models. On WSJ, we set a new benchmark\nfor transformer-based end-to-end speech recognition with a word error rate of\n3.65%, outperforming state of the art (4.20%) by 13.1% relative, while\nintroducing only a single hyperparameter. Upon acceptance, models will be\npublished on github.",
          "link": "http://arxiv.org/abs/2107.01275",
          "publishedOn": "2021-07-06T01:58:06.079Z",
          "wordCount": 610,
          "title": "Relaxed Attention: A Simple Method to Boost Performance of End-to-End Automatic Speech Recognition. (arXiv:2107.01275v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A.Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>",
          "description": "Modern neural text generation systems can produce remarkably fluent and\ngrammatical texts. While earlier language models suffered from repetition and\nsyntactic errors, the errors made by contemporary models are often semantic,\nnarrative, or discourse failures.\n\nTo facilitate research of these complex error types, we introduce a new\nstructured, crowdsourced error annotation schema called Scarecrow. The error\ncategories used in Scarecrow -- such as redundancy, commonsense errors, and\nincoherence -- were identified by combining expert analysis with several pilot\nrounds of ontology-free crowd annotation to arrive at a schema which covers the\nerror phenomena found in real machine generated text.\n\nWe use Scarecrow to collect 13k annotations of 1.3k human and machine\ngenerate paragraphs of English language news text, amounting to over 41k spans\neach labeled with its error category, severity, a natural language explanation,\nand antecedent span (where relevant). We collect annotations for text generated\nby state-of-the-art systems with varying known performance levels, from GPT-2\nSmall through the largest GPT-3. We isolate several factors for detailed\nanalysis, including parameter count, training data, and decoding technique. Our\nresults show both expected and surprising differences across these settings.\nThese findings demonstrate the value of Scarecrow annotations in the assessment\nof current and future text generation systems. We release our complete\nannotation toolkit and dataset at https://yao-dou.github.io/scarecrow/.",
          "link": "http://arxiv.org/abs/2107.01294",
          "publishedOn": "2021-07-06T01:58:06.051Z",
          "wordCount": 656,
          "title": "Scarecrow: A Framework for Scrutinizing Machine Text. (arXiv:2107.01294v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hifny_Y/0/1/0/all/0/1\">Yasser Hifny</a>",
          "description": "Code-switching in automatic speech recognition (ASR) is an important\nchallenge due to globalization. Recent research in multilingual ASR shows\npotential improvement over monolingual systems. We study key issues related to\nmultilingual modeling for ASR through a series of large-scale ASR experiments.\nOur innovative framework deploys a multi-graph approach in the weighted finite\nstate transducers (WFST) framework. We compare our WFST decoding strategies\nwith a transformer sequence to sequence system trained on the same data. Given\na code-switching scenario between Arabic and English languages, our results\nshow that the WFST decoding approaches were more suitable for the\nintersentential code-switching datasets. In addition, the transformer system\nperformed better for intrasentential code-switching task. With this study, we\nrelease an artificially generated development and test sets, along with\necological code-switching test set, to benchmark the ASR performance.",
          "link": "http://arxiv.org/abs/2107.01573",
          "publishedOn": "2021-07-06T01:58:06.029Z",
          "wordCount": 586,
          "title": "Arabic Code-Switching Speech Recognition using Monolingual Data. (arXiv:2107.01573v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaabouni_R/0/1/0/all/0/1\">Rahma Chaabouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>",
          "description": "Despite their practical success, modern seq2seq architectures are unable to\ngeneralize systematically on several SCAN tasks. Hence, it is not clear if\nSCAN-style compositional generalization is useful in realistic NLP tasks. In\nthis work, we study the benefit that such compositionality brings about to\nseveral machine translation tasks. We present several focused modifications of\nTransformer that greatly improve generalization capabilities on SCAN and select\none that remains on par with a vanilla Transformer on a standard machine\ntranslation (MT) task. Next, we study its performance in low-resource settings\nand on a newly introduced distribution-shifted English-French translation task.\nOverall, we find that improvements of a SCAN-capable model do not directly\ntransfer to the resource-rich MT setup. In contrast, in the low-resource setup,\ngeneral modifications lead to an improvement of up to 13.1% BLEU score w.r.t. a\nvanilla Transformer. Similarly, an improvement of 14% in an accuracy-based\nmetric is achieved in the introduced compositional English-French translation\ntask. This provides experimental evidence that the compositional generalization\nassessed in SCAN is particularly useful in resource-starved and domain-shifted\nscenarios.",
          "link": "http://arxiv.org/abs/2107.01366",
          "publishedOn": "2021-07-06T01:58:06.005Z",
          "wordCount": 621,
          "title": "Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN. (arXiv:2107.01366v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jinghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yining Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jianheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Previous math word problem solvers following the encoder-decoder paradigm\nfail to explicitly incorporate essential math symbolic constraints, leading to\nunexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic\nSolver (NS-Solver) to explicitly and seamlessly incorporate different levels of\nsymbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem\nreader to encode problems, a programmer to generate symbolic equations, and a\nsymbolic executor to obtain answers. Along with target expression supervision,\nour solver is also optimized via 4 new auxiliary objectives to enforce\ndifferent symbolic reasoning: a) self-supervised number prediction task\npredicting both number quantity and number locations; b) commonsense constant\nprediction task predicting what prior knowledge (e.g. how many legs a chicken\nhas) is required; c) program consistency checker computing the semantic loss\nbetween predicted equation and target equation to ensure reasonable equation\nmapping; d) duality exploiting task exploiting the quasi duality between\nsymbolic equation generation and problem's part-of-speech generation to enhance\nthe understanding ability of a solver. Besides, to provide a more realistic and\nchallenging benchmark for developing a universal and scalable solver, we also\nconstruct a new large-scale MWP benchmark CM17K consisting of 4 kinds of MWPs\n(arithmetic, one-unknown linear, one-unknown non-linear, equation set) with\nmore than 17K samples. Extensive experiments on Math23K and our CM17k\ndemonstrate the superiority of our NS-Solver compared to state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2107.01431",
          "publishedOn": "2021-07-06T01:58:05.922Z",
          "wordCount": 656,
          "title": "Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks. (arXiv:2107.01431v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouhizadeh_H/0/1/0/all/0/1\">Hossein Rouhizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajalli_V/0/1/0/all/0/1\">Vahideh Tajalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhziadeh_M/0/1/0/all/0/1\">Masoud Rouhziadeh</a>",
          "description": "Word Sense Disambiguation (WSD) is a long-standing task in Natural Language\nProcessing(NLP) that aims to automatically identify the most relevant meaning\nof the words in a given context. Developing standard WSD test collections can\nbe mentioned as an important prerequisite for developing and evaluating\ndifferent WSD systems in the language of interest. Although many WSD test\ncollections have been developed for a variety of languages, no standard\nAll-words WSD benchmark is available for Persian. In this paper, we address\nthis shortage for the Persian language by introducing SBU-WSD-Corpus, as the\nfirst standard test set for the Persian All-words WSD task. SBU-WSD-Corpus is\nmanually annotated with senses from the Persian WordNet (FarsNet) sense\ninventory. To this end, three annotators used SAMP (a tool for sense annotation\nbased on FarsNet lexical graph) to perform the annotation task. SBU-WSD-Corpus\nconsists of 19 Persian documents in different domains such as Sports, Science,\nArts, etc. It includes 5892 content words of Persian running text and 3371\nmanually sense annotated words (2073 nouns, 566 verbs, 610 adjectives, and 122\nadverbs). Providing baselines for future studies on the Persian All-words WSD\ntask, we evaluate several WSD models on SBU-WSD-Corpus. The corpus is publicly\navailable at https://github.com/hrouhizadeh/SBU-WSD-Corpus.",
          "link": "http://arxiv.org/abs/2107.01540",
          "publishedOn": "2021-07-06T01:58:05.906Z",
          "wordCount": 639,
          "title": "Persian-WSD-Corpus: A Sense Annotated Corpus for Persian All-words Word Sense Disambiguation. (arXiv:2107.01540v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.01542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1\">Mao Nguyen Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Loi Duc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khiem Vinh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "Machine reading comprehension (MRC) is a sub-field in natural language\nprocessing that aims to assist computers understand unstructured texts and then\nanswer questions related to them. In practice, the conversation is an essential\nway to communicate and transfer information. To help machines understand\nconversation texts, we present UIT-ViCoQA, a new corpus for conversational\nmachine reading comprehension in the Vietnamese language. This corpus consists\nof 10,000 questions with answers over 2,000 conversations about health news\narticles. Then, we evaluate several baseline approaches for conversational\nmachine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1\nscore of 45.27%, which is 30.91 points behind human performance (76.18%),\nindicating that there is ample room for improvement. Our dataset is available\nat our website: this http URL for research purposes.",
          "link": "http://arxiv.org/abs/2105.01542",
          "publishedOn": "2021-07-05T01:54:57.338Z",
          "wordCount": 637,
          "title": "Conversational Machine Reading Comprehension for Vietnamese Healthcare Texts. (arXiv:2105.01542v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lin-shan Lee</a>",
          "description": "Automatic speech recognition (ASR) technologies today are primarily optimized\nfor given datasets; thus, any changes in the application environment (e.g.,\nacoustic conditions or topic domains) may inevitably degrade the performance.\nWe can collect new data describing the new environment and fine-tune the\nsystem, but this naturally leads to higher error rates for the earlier\ndatasets, referred to as catastrophic forgetting. The concept of lifelong\nlearning (LLL) aiming to enable a machine to sequentially learn new tasks from\nnew datasets describing the changing real world without forgetting the\npreviously learned knowledge is thus brought to attention. This paper reports,\nto our knowledge, the first effort to extensively consider and analyze the use\nof various approaches of LLL in end-to-end (E2E) ASR, including proposing novel\nmethods in saving data for past domains to mitigate the catastrophic forgetting\nproblem. An overall relative reduction of 28.7% in WER was achieved compared to\nthe fine-tuning baseline when sequentially learning on three very different\nbenchmark corpora. This can be the first step toward the highly desired ASR\ntechnologies capable of synchronizing with the continuously changing real\nworld.",
          "link": "http://arxiv.org/abs/2104.01616",
          "publishedOn": "2021-07-05T01:54:57.284Z",
          "wordCount": 663,
          "title": "Towards Lifelong Learning of End-to-end ASR. (arXiv:2104.01616v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pritzen_J/0/1/0/all/0/1\">Julia Pritzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gref_M/0/1/0/all/0/1\">Michael Gref</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuhlke_D/0/1/0/all/0/1\">Dietlind Z&#xfc;hlke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_C/0/1/0/all/0/1\">Christoph Schmidt</a>",
          "description": "Loanwords, such as Anglicisms, are a challenge in German speech recognition.\nDue to their irregular pronunciation compared to native German words,\nautomatically generated pronunciation dictionaries often include faulty phoneme\nsequences for Anglicisms. In this work, we propose a multitask\nsequence-to-sequence approach for grapheme-to-phoneme conversion to improve the\nphonetization of Anglicisms. We extended a grapheme-to-phoneme model with a\nclassifier to distinguish Anglicisms from native German words. With this\napproach, the model learns to generate pronunciations differently depending on\nthe classification result. We used our model to create supplementary Anglicism\npronunciation dictionaries that are added to an existing German speech\nrecognition model. Tested on a dedicated Anglicism evaluation set, we improved\nthe recognition of Anglicisms compared to a baseline model, reducing the word\nerror rate by 1 % and the Anglicism error rate by 3 %. We show that multitask\nlearning can help solving the challenge of loanwords in German speech\nrecognition.",
          "link": "http://arxiv.org/abs/2105.12708",
          "publishedOn": "2021-07-05T01:54:57.265Z",
          "wordCount": 629,
          "title": "Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in German Speech Recognition. (arXiv:2105.12708v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1\">Simon Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>",
          "description": "State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.",
          "link": "http://arxiv.org/abs/2106.12672",
          "publishedOn": "2021-07-05T01:54:57.257Z",
          "wordCount": 665,
          "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1\">Th&#xe9;o Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vermeiren_W/0/1/0/all/0/1\">Walter Vermeiren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranwez_S/0/1/0/all/0/1\">Sylvie Ranwez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Binbin Xu</a>",
          "description": "Patent analysis and mining are time-consuming and costly processes for\ncompanies, but nevertheless essential if they are willing to remain\ncompetitive. To face the overload induced by numerous patents, the idea is to\nautomatically filter them, bringing only few to read to experts. This paper\nreports a successful application of fine-tuning and retraining on pre-trained\ndeep Natural Language Processing models on patent classification. The solution\nthat we propose combines several state-of-the-art treatments to achieve our\ngoal - decrease the workload while preserving recall and precision metrics.",
          "link": "http://arxiv.org/abs/2105.03979",
          "publishedOn": "2021-07-05T01:54:57.250Z",
          "wordCount": 557,
          "title": "Improving Patent Mining and Relevance Classification using Transformers. (arXiv:2105.03979v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basta_C/0/1/0/all/0/1\">Christine Basta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>",
          "description": "The scientific community is increasingly aware of the necessity to embrace\npluralism and consistently represent major and minor social groups. Currently,\nthere are no standard evaluation techniques for different types of biases.\nAccordingly, there is an urgent need to provide evaluation sets and protocols\nto measure existing biases in our automatic systems. Evaluating the biases\nshould be an essential step towards mitigating them in the systems.\n\nThis paper introduces WinoST, a new freely available challenge set for\nevaluating gender bias in speech translation. WinoST is the speech version of\nWinoMT which is a MT challenge set and both follow an evaluation protocol to\nmeasure gender accuracy. Using a state-of-the-art end-to-end speech translation\nsystem, we report the gender bias evaluation on four language pairs and we show\nthat gender accuracy in speech translation is more than 23% lower than in MT.",
          "link": "http://arxiv.org/abs/2010.14465",
          "publishedOn": "2021-07-05T01:54:57.243Z",
          "wordCount": 611,
          "title": "Evaluating Gender Bias in Speech Translation. (arXiv:2010.14465v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Toan Q. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>",
          "description": "In this paper, we investigate the driving factors behind concatenation, a\nsimple but effective data augmentation method for low-resource neural machine\ntranslation. Our experiments suggest that discourse context is unlikely the\ncause for the improvement of about +1 BLEU across four language pairs. Instead,\nwe demonstrate that the improvement comes from three other factors unrelated to\ndiscourse: context diversity, length diversity, and (to a lesser extent)\nposition shifting.",
          "link": "http://arxiv.org/abs/2105.01691",
          "publishedOn": "2021-07-05T01:54:57.227Z",
          "wordCount": 542,
          "title": "Data Augmentation by Concatenation for Low-Resource Translation: A Mystery and a Solution. (arXiv:2105.01691v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>",
          "description": "The end-to-end architecture has made promising progress in speech translation\n(ST). However, the ST task is still challenging under low-resource conditions.\nMost ST models have shown unsatisfactory results, especially in the absence of\nword information from the source speech utterance. In this study, we survey\nmethods to improve ST performance without using source transcription, and\npropose a learning framework that utilizes a language-independent universal\nphone recognizer. The framework is based on an attention-based\nsequence-to-sequence model, where the encoder generates the phonetic embeddings\nand phone-aware acoustic representations, and the decoder controls the fusion\nof the two embedding streams to produce the target token sequence. In addition\nto investigating different fusion strategies, we explore the specific usage of\nbyte pair encoding (BPE), which compresses a phone sequence into a\nsyllable-like segmented sequence. Due to the conversion of symbols, a segmented\nsequence represents not only pronunciation but also language-dependent\ninformation lacking in phones. Experiments conducted on the Fisher\nSpanish-English and Taigi-Mandarin drama corpora show that our method\noutperforms the conformer-based baseline, and the performance is close to that\nof the existing best method using source transcription.",
          "link": "http://arxiv.org/abs/2105.00171",
          "publishedOn": "2021-07-05T01:54:57.220Z",
          "wordCount": 649,
          "title": "AlloST: Low-resource Speech Translation without Source Transcription. (arXiv:2105.00171v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaicheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zihuiwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "With the rapid development of NLP research, leaderboards have emerged as one\ntool to track the performance of various systems on various NLP tasks. They are\neffective in this goal to some extent, but generally present a rather\nsimplistic one-dimensional view of the submitted systems, communicated only\nthrough holistic accuracy numbers. In this paper, we present a new\nconceptualization and implementation of NLP evaluation: the ExplainaBoard,\nwhich in addition to inheriting the functionality of the standard leaderboard,\nalso allows researchers to (i) diagnose strengths and weaknesses of a single\nsystem (e.g.~what is the best-performing system bad at?) (ii) interpret\nrelationships between multiple systems. (e.g.~where does system A outperform\nsystem B? What if we combine systems A, B, and C?) and (iii) examine prediction\nresults closely (e.g.~what are common errors made by multiple systems, or in\nwhat contexts do particular errors occur?). So far, ExplainaBoard covers more\nthan 400 systems, 50 datasets, 40 languages, and 12 tasks. ExplainaBoard keeps\nupdated and is recently upgraded by supporting (1) multilingual multi-task\nbenchmark, (2) meta-evaluation, and (3) more complicated task: machine\ntranslation, which reviewers also suggested.} We not only released an online\nplatform on the website \\url{this http URL} but also make\nour evaluation tool an API with MIT Licence at Github\n\\url{https://github.com/neulab/explainaBoard} and PyPi\n\\url{https://pypi.org/project/interpret-eval/} that allows users to\nconveniently assess their models offline. We additionally release all output\nfiles from systems that we have run or collected to motivate \"output-driven\"\nresearch in the future.",
          "link": "http://arxiv.org/abs/2104.06387",
          "publishedOn": "2021-07-05T01:54:57.212Z",
          "wordCount": 724,
          "title": "ExplainaBoard: An Explainable Leaderboard for NLP. (arXiv:2104.06387v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>",
          "description": "Recent years have witnessed the prosperity of legal artificial intelligence\nwith the development of technologies. In this paper, we propose a novel legal\napplication of legal provision prediction (LPP), which aims to predict the\nrelated legal provisions of affairs. We formulate this task as a challenging\nknowledge graph completion problem, which requires not only text understanding\nbut also graph reasoning. To this end, we propose a novel text-guided graph\nreasoning approach. We collect amounts of real-world legal provision data from\nthe Guangdong government service website and construct a legal dataset called\nLegalLPP. Extensive experimental results on the dataset show that our approach\nachieves better performance compared with baselines. The code and dataset are\navailable in \\url{https://github.com/zjunlp/LegalPP} for reproducibility.",
          "link": "http://arxiv.org/abs/2104.02284",
          "publishedOn": "2021-07-05T01:54:57.199Z",
          "wordCount": 582,
          "title": "Text-guided Legal Knowledge Graph Reasoning. (arXiv:2104.02284v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_T/0/1/0/all/0/1\">Thamme Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattmann_C/0/1/0/all/0/1\">Chris A Mattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>",
          "description": "While there are more than 7000 languages in the world, most translation\nresearch efforts have targeted a few high-resource languages. Commercial\ntranslation systems support only one hundred languages or fewer, and do not\nmake these models available for transfer to low resource languages. In this\nwork, we present useful tools for machine translation research: MTData,\nNLCodec, and RTG. We demonstrate their usefulness by creating a multilingual\nneural machine translation model capable of translating from 500 source\nlanguages to English. We make this multilingual model readily downloadable and\nusable as a service, or as a parent model for transfer-learning to even\nlower-resource languages.",
          "link": "http://arxiv.org/abs/2104.00290",
          "publishedOn": "2021-07-05T01:54:57.191Z",
          "wordCount": 578,
          "title": "Many-to-English Machine Translation Tools, Data, and Pretrained Models. (arXiv:2104.00290v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>",
          "description": "Several high-profile events, such as the use of biased recidivism systems and\nmass testing of emotion recognition systems on vulnerable sub-populations, have\nhighlighted how technology will often lead to more adverse outcomes for those\nthat are already marginalized. In this paper, I will make a case for thinking\nabout ethical considerations not just at the level of individual models and\ndatasets, but also at the level of AI tasks. I will present a new form of such\nan effort, Ethics Sheets for AI Tasks, dedicated to fleshing out the\nassumptions and ethical considerations hidden in how a task is commonly framed\nand in the choices we make regarding the data, method, and evaluation. Finally,\nI will provide an example ethics sheet for automatic emotion recognition.\nTogether with Data Sheets for datasets and Model Cards for AI systems, Ethics\nSheets aid in the development and deployment of responsible AI systems.",
          "link": "http://arxiv.org/abs/2107.01183",
          "publishedOn": "2021-07-05T01:54:57.155Z",
          "wordCount": 574,
          "title": "Ethics Sheets for AI Tasks. (arXiv:2107.01183v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2007.05290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xueqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lewen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shufang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Sequence learning has attracted much research attention from the machine\nlearning community in recent years. In many applications, a sequence learning\ntask is usually associated with multiple temporally correlated auxiliary tasks,\nwhich are different in terms of how much input information to use or which\nfuture step to predict. For example, (i) in simultaneous machine translation,\none can conduct translation under different latency (i.e., how many input words\nto read/wait before translation); (ii) in stock trend forecasting, one can\npredict the price of a stock in different future days (e.g., tomorrow, the day\nafter tomorrow). While it is clear that those temporally correlated tasks can\nhelp each other, there is a very limited exploration on how to better leverage\nmultiple auxiliary tasks to boost the performance of the main task. In this\nwork, we introduce a learnable scheduler to sequence learning, which can\nadaptively select auxiliary tasks for training depending on the model status\nand the current training data. The scheduler and the model for the main task\nare jointly trained through bi-level optimization. Experiments show that our\nmethod significantly improves the performance of simultaneous machine\ntranslation and stock trend forecasting.",
          "link": "http://arxiv.org/abs/2007.05290",
          "publishedOn": "2021-07-05T01:54:57.097Z",
          "wordCount": 665,
          "title": "Temporally Correlated Task Scheduling for Sequence Learning. (arXiv:2007.05290v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This\ntask aims to build a model for identifying toxic words in whole posts. We use\nthe BiLSTM-CRF model combining with ToxicBERT Classification to train the\ndetection model for identifying toxic words in posts. Our model achieves 62.23%\nby F1-score on the Toxic Spans Detection task.",
          "link": "http://arxiv.org/abs/2104.10100",
          "publishedOn": "2021-07-05T01:54:57.086Z",
          "wordCount": 552,
          "title": "UIT-ISE-NLP at SemEval-2021 Task 5: Toxic Spans Detection with BiLSTM-CRF and ToxicBERT Comment Classification. (arXiv:2104.10100v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Apel_R/0/1/0/all/0/1\">Reut Apel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erev_I/0/1/0/all/0/1\">Ido Erev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennenholtz_M/0/1/0/all/0/1\">Moshe Tennenholtz</a>",
          "description": "Sender-receiver interactions, and specifically persuasion games, are widely\nresearched in economic modeling and artificial intelligence, and serve as a\nsolid foundation for powerful applications. However, in the classic persuasion\ngames setting, the messages sent from the expert to the decision-maker are\nabstract or well-structured application-specific signals rather than natural\n(human) language messages, although natural language is a very common\ncommunication signal in real-world persuasion setups. This paper addresses the\nuse of natural language in persuasion games, exploring its impact on the\ndecisions made by the players and aiming to construct effective models for the\nprediction of these decisions. For this purpose, we conduct an online repeated\ninteraction experiment. At each trial of the interaction, an informed expert\naims to sell an uninformed decision-maker a vacation in a hotel, by sending her\na review that describes the hotel. While the expert is exposed to several\nscored reviews, the decision-maker observes only the single review sent by the\nexpert, and her payoff in case she chooses to take the hotel is a random draw\nfrom the review score distribution available to the expert only. The expert's\npayoff, in turn, depends on the number of times the decision-maker chooses the\nhotel. We consider a number of modeling approaches for this setup, differing\nfrom each other in the model type (deep neural network (DNN) vs. linear\nclassifier), the type of features used by the model (textual, behavioral or\nboth) and the source of the textual features (DNN-based vs. hand-crafted). Our\nresults demonstrate that given a prefix of the interaction sequence, our models\ncan predict the future decisions of the decision-maker, particularly when a\nsequential modeling approach and hand-crafted textual features are applied.",
          "link": "http://arxiv.org/abs/2012.09966",
          "publishedOn": "2021-07-05T01:54:57.073Z",
          "wordCount": 767,
          "title": "Predicting Decisions in Language Based Persuasion Games. (arXiv:2012.09966v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhukova_A/0/1/0/all/0/1\">Anastasia Zhukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1\">Karsten Donnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>",
          "description": "Unsupervised concept identification through clustering, i.e., identification\nof semantically related words and phrases, is a common approach to identify\ncontextual primitives employed in various use cases, e.g., text dimension\nreduction, i.e., replace words with the concepts to reduce the vocabulary size,\nsummarization, and named entity resolution. We demonstrate the first results of\nan unsupervised approach for the identification of groups of persons as actors\nextracted from a set of related articles. Specifically, the approach clusters\nmentions of groups of persons that act as non-named entity actors in the texts,\ne.g., \"migrant families\" = \"asylum-seekers.\" Compared to our baseline, the\napproach keeps the mentions of the geopolitical entities separated, e.g., \"Iran\nleaders\" != \"European leaders,\" and clusters (in)directly related mentions with\ndiverse wording, e.g., \"American officials\" = \"Trump Administration.\"",
          "link": "http://arxiv.org/abs/2107.00955",
          "publishedOn": "2021-07-05T01:54:57.046Z",
          "wordCount": 578,
          "title": "Concept Identification of Directly and Indirectly Related Mentions Referring to Groups of Persons. (arXiv:2107.00955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>",
          "description": "In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.",
          "link": "http://arxiv.org/abs/2107.01198",
          "publishedOn": "2021-07-05T01:54:57.037Z",
          "wordCount": 585,
          "title": "DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_P/0/1/0/all/0/1\">Pierpaolo Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazzi_M/0/1/0/all/0/1\">Marya Bazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1\">Mihai Cucuringu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGillivray_B/0/1/0/all/0/1\">Barbara McGillivray</a>",
          "description": "Lexical semantic change (detecting shifts in the meaning and usage of words)\nis an important task for social and cultural studies as well as for Natural\nLanguage Processing applications. Diachronic word embeddings (time-sensitive\nvector representations of words that preserve their meaning) have become the\nstandard resource for this task. However, given the significant computational\nresources needed for their generation, very few resources exist that make\ndiachronic word embeddings available to the scientific community.\n\nIn this paper we present DUKweb, a set of large-scale resources designed for\nthe diachronic analysis of contemporary English. DUKweb was created from the\nJISC UK Web Domain Dataset (1996-2013), a very large archive which collects\nresources from the Internet Archive that were hosted on domains ending in\n`.uk'. DUKweb consists of a series word co-occurrence matrices and two types of\nword embeddings for each year in the JISC UK Web Domain dataset. We show the\nreuse potential of DUKweb and its quality standards via a case study on word\nmeaning change detection.",
          "link": "http://arxiv.org/abs/2107.01076",
          "publishedOn": "2021-07-05T01:54:57.024Z",
          "wordCount": 617,
          "title": "DUKweb: Diachronic word representations from the UK Web Archive corpus. (arXiv:2107.01076v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Mohd Zeeshan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1\">M M Sufyan Beg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Tanvir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohd Jazib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasim_G/0/1/0/all/0/1\">Ghazali Wasim</a>",
          "description": "Language identification of social media text has been an interesting problem\nof study in recent years. Social media messages are predominantly in code mixed\nin non-English speaking states. Prior knowledge by pre-training contextual\nembeddings have shown state of the art results for a range of downstream tasks.\nRecently, models such as BERT have shown that using a large amount of unlabeled\ndata, the pretrained language models are even more beneficial for learning\ncommon language representations. Extensive experiments exploiting transfer\nlearning and fine-tuning BERT models to identify language on Twitter are\npresented in this paper. The work utilizes a data collection of\nHindi-English-Urdu codemixed text for language pre-training and Hindi-English\ncodemixed for subsequent word-level language classification. The results show\nthat the representations pre-trained over codemixed data produce better results\nby their monolingual counterpart.",
          "link": "http://arxiv.org/abs/2107.01202",
          "publishedOn": "2021-07-05T01:54:57.016Z",
          "wordCount": 573,
          "title": "Language Identification of Hindi-English tweets using code-mixed BERT. (arXiv:2107.01202v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yafang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>",
          "description": "Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2107.00967",
          "publishedOn": "2021-07-05T01:54:57.006Z",
          "wordCount": 582,
          "title": "R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling. (arXiv:2107.00967v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalalvand_S/0/1/0/all/0/1\">Shahab Jalalvand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bangalore_S/0/1/0/all/0/1\">Srinivas Bangalore</a>",
          "description": "An intelligent virtual assistant (IVA) enables effortless conversations in\ncall routing through spoken utterance classification (SUC) which is a special\nform of spoken language understanding (SLU). Building a SUC system requires a\nlarge amount of supervised in-domain data that is not always available. In this\npaper, we introduce an unsupervised spoken utterance classification approach\n(USUC) that does not require any in-domain data except for the intent labels\nand a few para-phrases per intent. USUC is consisting of a KNN classifier (K=1)\nand a complex embedding model trained on a large amount of unsupervised\ncustomer service corpus. Among all embedding models, we demonstrate that Elmo\nworks best for USUC. However, an Elmo model is too slow to be used at run-time\nfor call routing. To resolve this issue, first, we compute the uni- and bi-gram\nembedding vectors offline and we build a lookup table of n-grams and their\ncorresponding embedding vector. Then we use this table to compute sentence\nembedding vectors at run-time, along with back-off techniques for unseen\nn-grams. Experiments show that USUC outperforms the traditional utterance\nclassification methods by reducing the classification error rate from 32.9% to\n27.0% without requiring supervised data. Moreover, our lookup and back-off\ntechnique increases the processing speed from 16 utterances per second to 118\nutterances per second.",
          "link": "http://arxiv.org/abs/2107.01068",
          "publishedOn": "2021-07-05T01:54:56.986Z",
          "wordCount": 639,
          "title": "Unsupervised Spoken Utterance Classification. (arXiv:2107.01068v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1\">Grgur Kova&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1\">R&#xe9;my Portelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Building embodied autonomous agents capable of participating in social\ninteractions with humans is one of the main challenges in AI. Within the Deep\nReinforcement Learning (DRL) field, this objective motivated multiple works on\nembodied language use. However, current approaches focus on language as a\ncommunication tool in very simplified and non-diverse social situations: the\n\"naturalness\" of language is reduced to the concept of high vocabulary size and\nvariability. In this paper, we argue that aiming towards human-level AI\nrequires a broader set of key social skills: 1) language use in complex and\nvariable social contexts; 2) beyond language, complex embodied communication in\nmultimodal settings within constantly evolving social worlds. We explain how\nconcepts from cognitive sciences could help AI to draw a roadmap towards\nhuman-like intelligence, with a focus on its social dimensions. As a first\nstep, we propose to expand current research to a broader set of core social\nskills. To do this, we present SocialAI, a benchmark to assess the acquisition\nof social skills of DRL agents using multiple grid-world environments featuring\nother (scripted) social agents. We then study the limits of a recent SOTA DRL\napproach when tested on SocialAI and discuss important next steps towards\nproficient social agents. Videos and code are available at\nhttps://sites.google.com/view/socialai.",
          "link": "http://arxiv.org/abs/2107.00956",
          "publishedOn": "2021-07-05T01:54:56.956Z",
          "wordCount": 663,
          "title": "SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement Learning Agents. (arXiv:2107.00956v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nitish Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>",
          "description": "While pretrained language models achieve excellent performance on natural\nlanguage understanding benchmarks, they tend to rely on spurious correlations\nand generalize poorly to out-of-distribution (OOD) data. Recent work has\nexplored using counterfactually-augmented data (CAD) -- data generated by\nminimally perturbing examples to flip the ground-truth label -- to identify\nrobust features that are invariant under distribution shift. However, empirical\nresults using CAD for OOD generalization have been mixed. To explain this\ndiscrepancy, we draw insights from a linear Gaussian model and demonstrate the\npitfalls of CAD. Specifically, we show that (a) while CAD is effective at\nidentifying robust features, it may prevent the model from learning unperturbed\nrobust features, and (b) CAD may exacerbate existing spurious correlations in\nthe data. Our results show that the lack of perturbation diversity in current\nCAD datasets limits its effectiveness on OOD generalization, calling for\ninnovative crowdsourcing procedures to elicit diverse perturbation of examples.",
          "link": "http://arxiv.org/abs/2107.00753",
          "publishedOn": "2021-07-05T01:54:56.933Z",
          "wordCount": 584,
          "title": "An Investigation of the (In)effectiveness of Counterfactually Augmented Data. (arXiv:2107.00753v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marz_L/0/1/0/all/0/1\">Luisa M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweter_S/0/1/0/all/0/1\">Stefan Schweter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poerner_N/0/1/0/all/0/1\">Nina Poerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "We propose new methods for in-domain and cross-domain Named Entity\nRecognition (NER) on historical data for Dutch and French. For the cross-domain\ncase, we address domain shift by integrating unsupervised in-domain data via\ncontextualized string embeddings; and OCR errors by injecting synthetic OCR\nerrors into the source domain and address data centric domain adaptation. We\npropose a general approach to imitate OCR errors in arbitrary input data. Our\ncross-domain as well as our in-domain results outperform several strong\nbaselines and establish state-of-the-art results. We publish preprocessed\nversions of the French and Dutch Europeana NER corpora.",
          "link": "http://arxiv.org/abs/2107.00927",
          "publishedOn": "2021-07-05T01:54:56.896Z",
          "wordCount": 543,
          "title": "Data Centric Domain Adaptation for Historical Text with OCR Errors. (arXiv:2107.00927v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorsley_D/0/1/0/all/0/1\">David Thorsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassoun_J/0/1/0/all/0/1\">Joseph Hassoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "A major challenge in deploying transformer models is their prohibitive\ninference cost, which quadratically scales with the input sequence length. This\nmakes it especially difficult to use transformers for processing long\nsequences. To address this, we present a novel Learned Token Pruning (LTP)\nmethod that reduces redundant tokens as the data passes through the different\nlayers of the transformer. In particular, LTP prunes tokens with an attention\nscore below a threshold value, which is learned during training. Importantly,\nour threshold based method avoids algorithmically expensive operations such as\ntop-k token selection which are used in prior token pruning methods, and also\nleads to structured pruning. We extensively test the performance of our\napproach on multiple GLUE tasks and show that our learned threshold based\nmethod consistently outperforms the prior state-of-the-art top-k token based\nmethod by up to ~2% higher accuracy with the same amount of FLOPs. Furthermore,\nour preliminary results show up to 1.4x and 1.9x throughput improvement on\nTesla T4 GPU and Intel Haswell CPU, respectively, with less than 1% of accuracy\ndrop (and up to 2.1x FLOPs reduction). Our code has been developed in PyTorch\nand has been open-sourced.",
          "link": "http://arxiv.org/abs/2107.00910",
          "publishedOn": "2021-07-05T01:54:56.867Z",
          "wordCount": 621,
          "title": "Learned Token Pruning for Transformers. (arXiv:2107.00910v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kambara_M/0/1/0/all/0/1\">Motonari Kambara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>",
          "description": "There have been many studies in robotics to improve the communication skills\nof domestic service robots. Most studies, however, have not fully benefited\nfrom recent advances in deep neural networks because the training datasets are\nnot large enough. In this paper, our aim is to augment the datasets based on a\ncrossmodal language generation model. We propose the Case Relation Transformer\n(CRT), which generates a fetching instruction sentence from an image, such as\n\"Move the blue flip-flop to the lower left box.\" Unlike existing methods, the\nCRT uses the Transformer to integrate the visual features and geometry features\nof objects in the image. The CRT can handle the objects because of the Case\nRelation Block. We conducted comparison experiments and a human evaluation. The\nexperimental results show the CRT outperforms baseline methods.",
          "link": "http://arxiv.org/abs/2107.00789",
          "publishedOn": "2021-07-05T01:54:56.804Z",
          "wordCount": 580,
          "title": "Case Relation Transformer: A Crossmodal Language Generation Model for Fetching Instructions. (arXiv:2107.00789v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_S/0/1/0/all/0/1\">Shintaro Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>",
          "description": "Currently, domestic service robots have an insufficient ability to interact\nnaturally through language. This is because understanding human instructions is\ncomplicated by various ambiguities and missing information. In existing\nmethods, the referring expressions that specify the relationships between\nobjects are insufficiently modeled. In this paper, we propose Target-dependent\nUNITER, which learns the relationship between the target object and other\nobjects directly by focusing on the relevant regions within an image, rather\nthan the whole image. Our method is an extension of the UNITER-based\nTransformer that can be pretrained on general-purpose datasets. We extend the\nUNITER approach by introducing a new architecture for handling the target\ncandidates. Our model is validated on two standard datasets, and the results\nshow that Target-dependent UNITER outperforms the baseline method in terms of\nclassification accuracy.",
          "link": "http://arxiv.org/abs/2107.00811",
          "publishedOn": "2021-07-05T01:54:56.794Z",
          "wordCount": 580,
          "title": "Target-dependent UNITER: A Transformer-Based Multimodal Language Comprehension Model for Domestic Service Robots. (arXiv:2107.00811v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jian-Cheng Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zi-Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan-Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_H/0/1/0/all/0/1\">Hamido Fujita</a>",
          "description": "Multi-hop machine reading comprehension is a challenging task in natural\nlanguage processing, which requires more reasoning ability and explainability.\nSpectral models based on graph convolutional networks grant the inferring\nabilities and lead to competitive results, however, part of them still face the\nchallenge of analyzing the reasoning in a human-understandable way. Inspired by\nthe concept of the Grandmother Cells in cognitive neuroscience, a spatial graph\nattention framework named crname, imitating the procedure was proposed. This\nmodel is designed to assemble the semantic features in multi-angle\nrepresentations and automatically concentrate or alleviate the information for\nreasoning. The name \"crname\" is a metaphor for the pattern of the model: regard\nthe subjects of queries as the start points of clues, take the reasoning\nentities as bridge points, and consider the latent candidate entities as the\ngrandmother cells, and the clues end up in candidate entities. The proposed\nmodel allows us to visualize the reasoning graph and analyze the importance of\nedges connecting two entities and the selectivity in the mention and candidate\nnodes, which can be easier to be comprehended empirically. The official\nevaluations in open-domain multi-hop reading dataset WikiHop and Drug-drug\nInteractions dataset MedHop prove the validity of our approach and show the\nprobability of the application of the model in the molecular biology domain.",
          "link": "http://arxiv.org/abs/2107.00841",
          "publishedOn": "2021-07-05T01:54:56.783Z",
          "wordCount": 651,
          "title": "Heterogeneous Graph Attention Network for Multi-hop Machine Reading Comprehension. (arXiv:2107.00841v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagtap_R/0/1/0/all/0/1\">Raj Jagtap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shakshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_C/0/1/0/all/0/1\">Clint P. George</a>",
          "description": "Millions of people use platforms such as YouTube, Facebook, Twitter, and\nother mass media. Due to the accessibility of these platforms, they are often\nused to establish a narrative, conduct propaganda, and disseminate\nmisinformation. This work proposes an approach that uses state-of-the-art NLP\ntechniques to extract features from video captions (subtitles). To evaluate our\napproach, we utilize a publicly accessible and labeled dataset for classifying\nvideos as misinformation or not. The motivation behind exploring video captions\nstems from our analysis of videos metadata. Attributes such as the number of\nviews, likes, dislikes, and comments are ineffective as videos are hard to\ndifferentiate using this information. Using caption dataset, the proposed\nmodels can classify videos among three classes (Misinformation, Debunking\nMisinformation, and Neutral) with 0.85 to 0.90 F1-score. To emphasize the\nrelevance of the misinformation class, we re-formulate our classification\nproblem as a two-class classification - Misinformation vs. others (Debunking\nMisinformation and Neutral). In our experiments, the proposed models can\nclassify videos with 0.92 to 0.95 F1-score and 0.78 to 0.90 AUC ROC.",
          "link": "http://arxiv.org/abs/2107.00941",
          "publishedOn": "2021-07-05T01:54:56.773Z",
          "wordCount": 610,
          "title": "Misinformation Detection on YouTube Using Video Captions. (arXiv:2107.00941v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nanjiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marneffe_M/0/1/0/all/0/1\">Marie-Catherine de Marneffe</a>",
          "description": "We investigate how well BERT performs on predicting factuality in several\nexisting English datasets, encompassing various linguistic constructions.\nAlthough BERT obtains a strong performance on most datasets, it does so by\nexploiting common surface patterns that correlate with certain factuality\nlabels, and it fails on instances where pragmatic reasoning is necessary.\nContrary to what the high performance suggests, we are still far from having a\nrobust system for factuality prediction.",
          "link": "http://arxiv.org/abs/2107.00807",
          "publishedOn": "2021-07-05T01:54:56.744Z",
          "wordCount": 522,
          "title": "He Thinks He Knows Better than the Doctors: BERT for Event Factuality Fails on Pragmatics. (arXiv:2107.00807v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1\">Gowtham Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>",
          "description": "Multilingual Language Models (MLLMs) such as mBERT, XLM, XLM-R, \\textit{etc.}\nhave emerged as a viable option for bringing the power of pretraining to a\nlarge number of languages. Given their success in zero shot transfer learning,\nthere has emerged a large body of work in (i) building bigger MLLMs covering a\nlarge number of languages (ii) creating exhaustive benchmarks covering a wider\nvariety of tasks and languages for evaluating MLLMs (iii) analysing the\nperformance of MLLMs on monolingual, zero shot crosslingual and bilingual tasks\n(iv) understanding the universal language patterns (if any) learnt by MLLMs and\n(v) augmenting the (often) limited capacity of MLLMs to improve their\nperformance on seen or even unseen languages. In this survey, we review the\nexisting literature covering the above broad areas of research pertaining to\nMLLMs. Based on our survey, we recommend some promising directions of future\nresearch.",
          "link": "http://arxiv.org/abs/2107.00676",
          "publishedOn": "2021-07-05T01:54:56.663Z",
          "wordCount": 578,
          "title": "A Primer on Pretrained Multilingual Language Models. (arXiv:2107.00676v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shillingford_B/0/1/0/all/0/1\">Brendan Shillingford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assael_Y/0/1/0/all/0/1\">Yannis Assael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denil_M/0/1/0/all/0/1\">Misha Denil</a>",
          "description": "This work describes an interactive decoding method to improve the performance\nof visual speech recognition systems using user input to compensate for the\ninherent ambiguity of the task. Unlike most phoneme-to-word decoding pipelines,\nwhich produce phonemes and feed these through a finite state transducer, our\nmethod instead expands words in lockstep, facilitating the insertion of\ninteraction points at each word position. Interaction points enable us to\nsolicit input during decoding, allowing users to interactively direct the\ndecoding process. We simulate the behavior of user input using an oracle to\ngive an automated evaluation, and show promise for the use of this method for\ntext input.",
          "link": "http://arxiv.org/abs/2107.00692",
          "publishedOn": "2021-07-05T01:54:56.570Z",
          "wordCount": 540,
          "title": "Interactive decoding of words from visual speech recognition models. (arXiv:2107.00692v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>",
          "description": "The Transformer model is widely used in natural language processing for\nsentence representation. However, the previous Transformer-based models focus\non function words that have limited meaning in most cases and could merely\nextract high-level semantic abstraction features. In this paper, two approaches\nare introduced to improve the performance of Transformers. We calculated the\nattention score by multiplying the part-of-speech weight vector with the\ncorrelation coefficient, which helps extract the words with more practical\nmeaning. The weight vector is obtained by the input text sequence based on the\nimportance of the part-of-speech. Furthermore, we fuse the features of each\nlayer to make the sentence representation results more comprehensive and\naccurate. In experiments, we demonstrate the effectiveness of our model\nTransformer-F on three standard text classification datasets. Experimental\nresults show that our proposed model significantly boosts the performance of\ntext classification as compared to the baseline model. Specifically, we obtain\na 5.28% relative improvement over the vanilla Transformer on the simple tasks.",
          "link": "http://arxiv.org/abs/2107.00653",
          "publishedOn": "2021-07-05T01:54:56.469Z",
          "wordCount": 597,
          "title": "Transformer-F: A Transformer network with effective methods for learning universal sentence representation. (arXiv:2107.00653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Anubhab Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honore_A/0/1/0/all/0/1\">Antoine Honor&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Saikat Chatterjee</a>",
          "description": "In pursuit of explainability, we develop generative models for sequential\ndata. The proposed models provide state-of-the-art classification results and\nrobust performance for speech phone classification. We combine modern neural\nnetworks (normalizing flows) and traditional generative models (hidden Markov\nmodels - HMMs). Normalizing flow-based mixture models (NMMs) are used to model\nthe conditional probability distribution given the hidden state in the HMMs.\nModel parameters are learned through judicious combinations of time-tested\nBayesian learning methods and contemporary neural network learning methods. We\nmainly combine expectation-maximization (EM) and mini-batch gradient descent.\nThe proposed generative models can compute likelihood of a data and hence\ndirectly suitable for maximum-likelihood (ML) classification approach. Due to\nstructural flexibility of HMMs, we can use different normalizing flow models.\nThis leads to different types of HMMs providing diversity in data modeling\ncapacity. The diversity provides an opportunity for easy decision fusion from\ndifferent models. For a standard speech phone classification setup involving 39\nphones (classes) and the TIMIT dataset, we show that the use of standard\nfeatures called mel-frequency-cepstral-coeffcients (MFCCs), the proposed\ngenerative models, and the decision fusion together can achieve $86.6\\%$\naccuracy by generative training only. This result is close to state-of-the-art\nresults, for examples, $86.2\\%$ accuracy of PyTorch-Kaldi toolkit [1], and\n$85.1\\%$ accuracy using light gated recurrent units [2]. We do not use any\ndiscriminative learning approach and related sophisticated features in this\narticle.",
          "link": "http://arxiv.org/abs/2107.00730",
          "publishedOn": "2021-07-05T01:54:56.430Z",
          "wordCount": 690,
          "title": "Normalizing Flow based Hidden Markov Models for Classification of Speech Phones with Explainability. (arXiv:2107.00730v1 [cs.LG])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2105.05710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takiguchi_K/0/1/0/all/0/1\">Kentaro Takiguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fain_M/0/1/0/all/0/1\">Mikhail Fain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twomey_N/0/1/0/all/0/1\">Niall Twomey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaquero_L/0/1/0/all/0/1\">Luis M Vaquero</a>",
          "description": "Explicitly modelling field interactions and correlations in complex document\nstructures has recently gained popularity in neural document embedding and\nretrieval tasks. Although this requires the specification of bespoke\ntask-dependent models, encouraging empirical results are beginning to emerge.\nWe present the first in-depth analyses of non-linear multi-field interaction\n(NL-MFI) ranking in the cooking domain in this work. Our results show that\nfield-weighted factorisation machines models provide a statistically\nsignificant improvement over baselines in recipe retrieval tasks. Additionally,\nwe show that sparsely capturing subsets of field interactions based on domain\nknowledge and feature selection heuristics offers significant advantages over\nbaselines and exhaustive alternatives. Although field-interaction aware models\nare more elaborate from an architectural basis, they are often more\ndata-efficient in optimisation and are better suited for explainability due to\nmirrored document and model factorisation.",
          "link": "http://arxiv.org/abs/2105.05710",
          "publishedOn": "2021-07-09T01:58:24.869Z",
          "wordCount": 592,
          "title": "Evaluation of Field-Aware Neural Ranking Models for Recipe Search. (arXiv:2105.05710v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>",
          "description": "Recent studies have shown that providing personalized explanations alongside\nrecommendations increases trust and perceived quality. Furthermore, it gives\nusers an opportunity to refine the recommendations by critiquing parts of the\nexplanations. On one hand, current recommender systems model the\nrecommendation, explanation, and critiquing objectives jointly, but this\ncreates an inherent trade-off between their respective performance. On the\nother hand, although recent latent linear critiquing approaches are built upon\nan existing recommender system, they suffer from computational inefficiency at\ninference due to the objective optimized at each conversation's turn. We\naddress these deficiencies with M&Ms-VAE, a novel variational autoencoder for\nrecommendation and explanation that is based on multimodal modeling\nassumptions. We train the model under a weak supervision scheme to simulate\nboth fully and partially observed variables. Then, we leverage the\ngeneralization ability of a trained M&Ms-VAE model to embed the user preference\nand the critique separately. Our work's most important innovation is our\ncritiquing module, which is built upon and trained in a self-supervised manner\nwith a simple ranking objective. Experiments on four real-world datasets\ndemonstrate that among state-of-the-art models, our system is the first to\ndominate or match the performance in terms of recommendation, explanation, and\nmulti-step critiquing. Moreover, M&Ms-VAE processes the critiques up to 25.6x\nfaster than the best baselines. Finally, we show that our model infers coherent\njoint and cross generation, even under weak supervision, thanks to our\nmultimodal-based modeling and training scheme.",
          "link": "http://arxiv.org/abs/2105.00774",
          "publishedOn": "2021-07-09T01:58:24.689Z",
          "wordCount": 706,
          "title": "Fast Multi-Step Critiquing for VAE-based Recommender Systems. (arXiv:2105.00774v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuqi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Manli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangzhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haoyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yizhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guoxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jingyuan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Heng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baogui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weihao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zongzheng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yueqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yida Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuqing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wanqing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1\">Danyang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chuhao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuchong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Multi-modal pre-training models have been intensively explored to bridge\nvision and language in recent years. However, most of them explicitly model the\ncross-modal interaction between image-text pairs, by assuming that there exists\nstrong semantic correlation between the text and image modalities. Since this\nstrong assumption is often invalid in real-world scenarios, we choose to\nimplicitly model the cross-modal correlation for large-scale multi-modal\npre-training, which is the focus of the Chinese project `WenLan' led by our\nteam. Specifically, with the weak correlation assumption over image-text pairs,\nwe propose a two-tower pre-training model called BriVL within the cross-modal\ncontrastive learning framework. Unlike OpenAI CLIP that adopts a simple\ncontrastive learning method, we devise a more advanced algorithm by adapting\nthe latest method MoCo into the cross-modal scenario. By building a large\nqueue-based dictionary, our BriVL can incorporate more negative samples in\nlimited GPU resources. We further construct a large Chinese multi-source\nimage-text dataset called RUC-CAS-WenLan for pre-training our BriVL model.\nExtensive experiments demonstrate that the pre-trained BriVL model outperforms\nboth UNITER and OpenAI CLIP on various downstream tasks.",
          "link": "http://arxiv.org/abs/2103.06561",
          "publishedOn": "2021-07-09T01:58:24.680Z",
          "wordCount": 761,
          "title": "WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training. (arXiv:2103.06561v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "Personalized news recommendation is an important technique to help users find\ntheir interested news information and alleviate their information overload. It\nhas been extensively studied over decades and has achieved notable success in\nimproving users' news reading experience. However, there are still many\nunsolved problems and challenges that need to be further studied. To help\nresearchers master the advances in personalized news recommendation over the\npast years, in this paper we present a comprehensive overview of personalized\nnews recommendation. Instead of following the conventional taxonomy of news\nrecommendation methods, in this paper we propose a novel perspective to\nunderstand personalized news recommendation based on its core problems and the\nassociated techniques and challenges. We first review the techniques for\ntackling each core problem in a personalized news recommender system and the\nchallenges they face. Next, we introduce the public datasets and evaluation\nmethods for personalized news recommendation. We then discuss the key points on\nimproving the responsibility of personalized news recommender systems. Finally,\nwe raise several research directions that are worth investigating in the\nfuture. This paper can provide up-to-date and comprehensive views to help\nreaders understand the personalized news recommendation field. We hope this\npaper can facilitate research on personalized news recommendation and as well\nas related fields in natural language processing and data mining.",
          "link": "http://arxiv.org/abs/2106.08934",
          "publishedOn": "2021-07-09T01:58:24.665Z",
          "wordCount": 667,
          "title": "Personalized News Recommendation: A Survey. (arXiv:2106.08934v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansoury_M/0/1/0/all/0/1\">Masoud Mansoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdollahpouri_H/0/1/0/all/0/1\">Himan Abdollahpouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobasher_B/0/1/0/all/0/1\">Bamshad Mobasher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_R/0/1/0/all/0/1\">Robin Burke</a>",
          "description": "Fairness is a critical system-level objective in recommender systems that has\nbeen the subject of extensive recent research. A specific form of fairness is\nsupplier exposure fairness where the objective is to ensure equitable coverage\nof items across all suppliers in recommendations provided to users. This is\nespecially important in multistakeholder recommendation scenarios where it may\nbe important to optimize utilities not just for the end-user, but also for\nother stakeholders such as item sellers or producers who desire a fair\nrepresentation of their items. This type of supplier fairness is sometimes\naccomplished by attempting to increasing aggregate diversity in order to\nmitigate popularity bias and to improve the coverage of long-tail items in\nrecommendations. In this paper, we introduce FairMatch, a general graph-based\nalgorithm that works as a post processing approach after recommendation\ngeneration to improve exposure fairness for items and suppliers. The algorithm\niteratively adds high quality items that have low visibility or items from\nsuppliers with low exposure to the users' final recommendation lists. A\ncomprehensive set of experiments on two datasets and comparison with\nstate-of-the-art baselines show that FairMatch, while significantly improves\nexposure fairness and aggregate diversity, maintains an acceptable level of\nrelevance of the recommendations.",
          "link": "http://arxiv.org/abs/2107.03415",
          "publishedOn": "2021-07-09T01:58:24.650Z",
          "wordCount": 653,
          "title": "A Graph-based Approach for Mitigating Multi-sided Exposure Bias in Recommender Systems. (arXiv:2107.03415v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cantador_I/0/1/0/all/0/1\">Iv&#xe1;n Cantador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1\">Andr&#xe9;s Carvallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diez_F/0/1/0/all/0/1\">Fernando Diez</a>",
          "description": "The success of neural network embeddings has entailed a renewed interest in\nusing knowledge graphs for a wide variety of machine learning and information\nretrieval tasks. In particular, recent recommendation methods based on graph\nembeddings have shown state-of-the-art performance. In general, these methods\nencode latent rating patterns and content features. Differently from previous\nwork, in this paper, we propose to exploit embeddings extracted from graphs\nthat combine information from ratings and aspect-based opinions expressed in\ntextual reviews. We then adapt and evaluate state-of-the-art graph embedding\ntechniques over graphs generated from Amazon and Yelp reviews on six domains,\noutperforming baseline recommenders. Additionally, our method has the advantage\nof providing explanations that involve the coverage of aspect-based opinions\ngiven by users about recommended items.",
          "link": "http://arxiv.org/abs/2107.03385",
          "publishedOn": "2021-07-09T01:58:24.618Z",
          "wordCount": 568,
          "title": "Rating and aspect-based opinion graph embeddings for explainable recommendations. (arXiv:2107.03385v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1\">Craig Macdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>",
          "description": "Leveraging the side information associated with entities (i.e.\\ users and\nitems) to enhance the performance of recommendation systems has been widely\nrecognized as an important modelling dimension. While many existing approaches\nfocus on the \\emph{integration scheme} to incorporate entity side information\n-- by combining the recommendation loss function with an extra side\ninformation-aware loss -- in this paper, we propose instead a novel\n\\emph{pre-training scheme} for leveraging the side information. In particular,\nwe first pre-train a representation model using the side information of the\nentities, and then fine-tune it using an existing general representation-based\nrecommendation model. Specifically, we propose two pre-training models, named\n\\gcn{} and \\com{}, by considering the entities and their relations constructed\nfrom side information as two different types of graphs respectively, to\npre-train entity embeddings. For the \\gcn{} model, two single-relational graphs\nare constructed from all the users' and items' side information respectively,\nto pre-train entity representations by using the Graph Convolutional Networks.\nFor the \\com{} model, two multi-relational graphs are constructed to pre-train\nthe entity representations by using the Composition-based Graph Convolutional\nNetworks. An extensive evaluation of our pre-training models fine-tuned under\nfour general representation-based recommender models, i.e.\\ MF, NCF, NGCF and\nLightGCN, shows that effectively pre-training embeddings with both the user's\nand item's side information can significantly improve these original models in\nterms of both effectiveness and stability.",
          "link": "http://arxiv.org/abs/2107.03936",
          "publishedOn": "2021-07-09T01:58:24.600Z",
          "wordCount": 655,
          "title": "Graph Neural Pre-training for Enhancing Recommendations using Side Information. (arXiv:2107.03936v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">SeongKu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyun_D/0/1/0/all/0/1\">Dongmin Hyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hwanjo Yu</a>",
          "description": "Session-based Recommender Systems (SRSs) have been actively developed to\nrecommend the next item of an anonymous short item sequence (i.e., session).\nUnlike sequence-aware recommender systems where the whole interaction sequence\nof each user can be used to model both the short-term interest and the general\ninterest of the user, the absence of user-dependent information in SRSs makes\nit difficult to directly derive the user's general interest from data.\nTherefore, existing SRSs have focused on how to effectively model the\ninformation about short-term interest within the sessions, but they are\ninsufficient to capture the general interest of users. To this end, we propose\na novel framework to overcome the limitation of SRSs, named ProxySR, which\nimitates the missing information in SRSs (i.e., general interest of users) by\nmodeling proxies of sessions. ProxySR selects a proxy for the input session in\nan unsupervised manner, and combines it with the encoded short-term interest of\nthe session. As a proxy is jointly learned with the short-term interest and\nselected by multiple sessions, a proxy learns to play the role of the general\ninterest of a user and ProxySR learns how to select a suitable proxy for an\ninput session. Moreover, we propose another real-world situation of SRSs where\na few users are logged-in and leave their identifiers in sessions, and a\nrevision of ProxySR for the situation. Our experiments on real-world datasets\nshow that ProxySR considerably outperforms the state-of-the-art competitors,\nand the proxies successfully imitate the general interest of the users without\nany user-dependent information.",
          "link": "http://arxiv.org/abs/2107.03564",
          "publishedOn": "2021-07-09T01:58:24.586Z",
          "wordCount": 690,
          "title": "Unsupervised Proxy Selection for Session-based Recommender Systems. (arXiv:2107.03564v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvir Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1\">Janntatul Tajrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naira Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>",
          "description": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.",
          "link": "http://arxiv.org/abs/2107.03844",
          "publishedOn": "2021-07-09T01:58:24.571Z",
          "wordCount": 691,
          "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yitong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1\">Qi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ethan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>",
          "description": "Predicting the next interaction of a short-term interaction session is a\nchallenging task in session-based recommendation. Almost all existing works\nrely on item transition patterns, and neglect the impact of user historical\nsessions while modeling user preference, which often leads to non-personalized\nrecommendation. Additionally, existing personalized session-based recommenders\ncapture user preference only based on the sessions of the current user, but\nignore the useful item-transition patterns from other user's historical\nsessions. To address these issues, we propose a novel Heterogeneous Global\nGraph Neural Networks (HG-GNN) to exploit the item transitions over all\nsessions in a subtle manner for better inferring user preference from the\ncurrent and historical sessions. To effectively exploit the item transitions\nover all sessions from users, we propose a novel heterogeneous global graph\nthat contains item transitions of sessions, user-item interactions and global\nco-occurrence items. Moreover, to capture user preference from sessions\ncomprehensively, we propose to learn two levels of user representations from\nthe global graph via two graph augmented preference encoders. Specifically, we\ndesign a novel heterogeneous graph neural network (HGNN) on the heterogeneous\nglobal graph to learn the long-term user preference and item representations\nwith rich semantics. Based on the HGNN, we propose the Current Preference\nEncoder and the Historical Preference Encoder to capture the different levels\nof user preference from the current and historical sessions, respectively. To\nachieve personalized recommendation, we integrate the representations of the\nuser current preference and historical interests to generate the final user\npreference representation. Extensive experimental results on three real-world\ndatasets show that our model outperforms other state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.03813",
          "publishedOn": "2021-07-09T01:58:24.546Z",
          "wordCount": 709,
          "title": "Heterogeneous Global Graph Neural Networks for Personalized Session-based Recommendation. (arXiv:2107.03813v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "Collaborative filtering (CF) is widely used to learn an informative latent\nrepresentation of a user or item from observed interactions. Existing CF-based\nmethods commonly adopt negative sampling to discriminate different items. That\nis, observed user-item pairs are treated as positive instances; unobserved\npairs are considered as negative instances and are sampled under a defined\ndistribution for training. Training with negative sampling on large datasets is\ncomputationally expensive. Further, negative items should be carefully sampled\nunder the defined distribution, in order to avoid selecting an observed\npositive item in the training dataset. Unavoidably, some negative items sampled\nfrom the training dataset could be positive in the test set. Recently,\nself-supervised learning (SSL) has emerged as a powerful tool to learn a model\nwithout negative samples. In this paper, we propose a self-supervised\ncollaborative filtering framework (SelfCF), that is specially designed for\nrecommender scenario with implicit feedback. The main idea of SelfCF is to\naugment the output embeddings generated by backbone networks, because it is\ninfeasible to augment raw input of user/item ids. We propose and study three\noutput perturbation techniques that can be applied to different types of\nbackbone networks including both traditional CF models and graph-based models.\nBy encapsulating two popular recommendation models into the framework, our\nexperiments on three datasets show that the best performance of our framework\nis comparable or better than the supervised counterpart. We also show that\nSelfCF can boost up the performance by up to 8.93\\% on average, compared with\nanother self-supervised framework as the baseline. Source codes are available\nat: https://github.com/enoche/SelfCF.",
          "link": "http://arxiv.org/abs/2107.03019",
          "publishedOn": "2021-07-08T01:57:55.907Z",
          "wordCount": 698,
          "title": "SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. (arXiv:2107.03019v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chia_P/0/1/0/all/0/1\">Patrick John Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bingqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>",
          "description": "Large eCommerce players introduced comparison tables as a new type of\nrecommendations. However, building comparisons at scale without pre-existing\ntraining/taxonomy data remains an open challenge, especially within the\noperational constraints of shops in the long tail. We present preliminary\nresults from building a comparison pipeline designed to scale in a multi-shop\nscenario: we describe our design choices and run extensive benchmarks on\nmultiple shops to stress-test it. Finally, we run a small user study on\nproperty selection and conclude by discussing potential improvements and\nhighlighting the questions that remain to be addressed.",
          "link": "http://arxiv.org/abs/2107.03256",
          "publishedOn": "2021-07-08T01:57:55.877Z",
          "wordCount": 534,
          "title": "\"Are you sure?\": Preliminary Insights from Scaling Product Comparisons to Multiple Shops. (arXiv:2107.03256v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cantador_I/0/1/0/all/0/1\">Iv&#xe1;n Cantador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1\">Andr&#xe9;s Carvallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diez_F/0/1/0/all/0/1\">Fernando Diez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1\">Denis Parra</a>",
          "description": "The success of neural network embeddings has entailed a renewed interest in\nusing knowledge graphs for a wide variety of machine learning and information\nretrieval tasks. In particular, current recommendation methods based on graph\nembeddings have shown state-of-the-art performance. These methods commonly\nencode latent rating patterns and content features. Different from previous\nwork, in this paper, we propose to exploit embeddings extracted from graphs\nthat combine information from ratings and aspect-based opinions expressed in\ntextual reviews. We then adapt and evaluate state-of-the-art graph embedding\ntechniques over graphs generated from Amazon and Yelp reviews on six domains,\noutperforming baseline recommenders. Our approach has the advantage of\nproviding explanations which leverage aspect-based opinions given by users\nabout recommended items. Furthermore, we also provide examples of the\napplicability of recommendations utilizing aspect opinions as explanations in a\nvisualization dashboard, which allows obtaining information about the most and\nleast liked aspects of similar users obtained from the embeddings of an input\ngraph.",
          "link": "http://arxiv.org/abs/2107.03226",
          "publishedOn": "2021-07-08T01:57:55.820Z",
          "wordCount": 609,
          "title": "Graphing else matters: exploiting aspect opinions and ratings in explainable graph-based recommendations. (arXiv:2107.03226v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hande Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Keping Yang</a>",
          "description": "Recommender systems rely on user behavior data like ratings and clicks to\nbuild personalization model. However, the collected data is observational\nrather than experimental, causing various biases in the data which\nsignificantly affect the learned model. Most existing work for recommendation\ndebiasing, such as the inverse propensity scoring and imputation approaches,\nfocuses on one or two specific biases, lacking the universal capacity that can\naccount for mixed or even unknown biases in the data.\n\nTowards this research gap, we first analyze the origin of biases from the\nperspective of \\textit{risk discrepancy} that represents the difference between\nthe expectation empirical risk and the true risk. Remarkably, we derive a\ngeneral learning framework that well summarizes most existing debiasing\nstrategies by specifying some parameters of the general framework. This\nprovides a valuable opportunity to develop a universal solution for debiasing,\ne.g., by learning the debiasing parameters from data. However, the training\ndata lacks important signal of how the data is biased and what the unbiased\ndata looks like. To move this idea forward, we propose \\textit{AotoDebias} that\nleverages another (small) set of uniform data to optimize the debiasing\nparameters by solving the bi-level optimization problem with meta-learning.\nThrough theoretical analyses, we derive the generalization bound for AutoDebias\nand prove its ability to acquire the appropriate debiasing strategy. Extensive\nexperiments on two real datasets and a simulated dataset demonstrated\neffectiveness of AutoDebias. The code is available at\n\\url{https://github.com/DongHande/AutoDebias}.",
          "link": "http://arxiv.org/abs/2105.04170",
          "publishedOn": "2021-07-08T01:57:55.795Z",
          "wordCount": 713,
          "title": "AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oosterhuis_H/0/1/0/all/0/1\">Harrie Oosterhuis</a>",
          "description": "Recent work has proposed stochastic Plackett-Luce (PL) ranking models as a\nrobust choice for optimizing relevance and fairness metrics. Unlike their\ndeterministic counterparts that require heuristic optimization algorithms, PL\nmodels are fully differentiable. Theoretically, they can be used to optimize\nranking metrics via stochastic gradient descent. However, in practice, the\ncomputation of the gradient is infeasible because it requires one to iterate\nover all possible permutations of items. Consequently, actual applications rely\non approximating the gradient via sampling techniques. In this paper, we\nintroduce a novel algorithm: PL-Rank, that estimates the gradient of a PL\nranking model w.r.t. both relevance and fairness metrics. Unlike existing\napproaches that are based on policy gradients, PL-Rank makes use of the\nspecific structure of PL models and ranking metrics. Our experimental analysis\nshows that PL-Rank has a greater sample-efficiency and is computationally less\ncostly than existing policy gradients, resulting in faster convergence at\nhigher performance. PL-Rank further enables the industry to apply PL models for\nmore relevant and fairer real-world ranking systems.",
          "link": "http://arxiv.org/abs/2105.00855",
          "publishedOn": "2021-07-08T01:57:55.749Z",
          "wordCount": 632,
          "title": "Computationally Efficient Optimization of Plackett-Luce Ranking Models for Relevance and Fairness. (arXiv:2105.00855v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dinneen_J/0/1/0/all/0/1\">Jesse David Dinneen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Ba Xuan Nguyen</a>",
          "description": "Improving file management interfaces and optimising system performance\nrequires current data about users' digital collections and particularly about\nthe file size distributions of such collections. However, prior works have\nexamined only the sizes of system files and users' work files in varied\ncontexts, and there has been no such study since 2013; it therefore remains\nunclear how today's file sizes are distributed, particularly personal files,\nand further if distributions differ among the major operating systems or common\noccupations. Here we examine such differences among 49 million files in 348\nuser collections. We find that the average file size has grown more than\nten-fold since the mid-2000s, though most files are still under 8 MB, and that\nthere are demographic and technological influences in the size distributions.\nWe discuss the implications for user interfaces, system optimisation, and PIM\nresearch.",
          "link": "http://arxiv.org/abs/2107.03272",
          "publishedOn": "2021-07-08T01:57:55.715Z",
          "wordCount": 600,
          "title": "How Big Are Peoples' Computer Files? File Size Distributions Among User-managed Collections. (arXiv:2107.03272v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2007.02445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shevkunov_K/0/1/0/all/0/1\">Kirill Shevkunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1\">Liudmila Prokhorenkova</a>",
          "description": "Various non-trivial spaces are becoming popular for embedding structured data\nsuch as graphs, texts, or images. Following spherical and hyperbolic spaces,\nmore general product spaces have been proposed. However, searching for the best\nconfiguration of product space is a resource-intensive procedure, which reduces\nthe practical applicability of the idea. We generalize the concept of product\nspace and introduce an overlapping space that does not have the configuration\nsearch problem. The main idea is to allow subsets of coordinates to be shared\nbetween spaces of different types (Euclidean, hyperbolic, spherical). As a\nresult, parameter optimization automatically learns the optimal configuration.\nAdditionally, overlapping spaces allow for more compact representations since\ntheir geometry is more complex. Our experiments confirm that overlapping spaces\noutperform the competitors in graph embedding tasks. Here, we consider both\ndistortion setup, where the aim is to preserve distances, and ranking setup,\nwhere the relative order should be preserved. The proposed method effectively\nsolves the problem and outperforms the competitors in both settings. We also\nperform an empirical analysis in a realistic information retrieval task, where\nwe compare all spaces by incorporating them into DSSM. In this case, the\nproposed overlapping space consistently achieves nearly optimal results without\nany configuration tuning. This allows for reducing training time, which can be\nsignificant in large-scale applications.",
          "link": "http://arxiv.org/abs/2007.02445",
          "publishedOn": "2021-07-07T01:57:10.089Z",
          "wordCount": 674,
          "title": "Overlapping Spaces for Compact Graph Representations. (arXiv:2007.02445v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amjadi_M/0/1/0/all/0/1\">Mehrnaz Amjadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taheri_S/0/1/0/all/0/1\">Seyed Danial Mohseni Taheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulabandhula_T/0/1/0/all/0/1\">Theja Tulabandhula</a>",
          "description": "Sequential recommendation systems model dynamic preferences of users based on\ntheir historical interactions with platforms. Despite recent progress, modeling\nshort-term and long-term behavior of users in such systems is nontrivial and\nchallenging. To address this, we present a solution enhanced by a knowledge\ngraph called KATRec (Knowledge Aware aTtentive sequential Recommendations).\nKATRec learns the short and long-term interests of users by modeling their\nsequence of interacted items and leveraging pre-existing side information\nthrough a knowledge graph attention network. Our novel knowledge graph-enhanced\nsequential recommender contains item multi-relations at the entity-level and\nusers' dynamic sequences at the item-level. KATRec improves item representation\nlearning by considering higher-order connections and incorporating them in user\npreference representation while recommending the next item. Experiments on\nthree public datasets show that KATRec outperforms state-of-the-art\nrecommendation models and demonstrates the importance of modeling both temporal\nand side information to achieve high-quality recommendations.",
          "link": "http://arxiv.org/abs/2012.03323",
          "publishedOn": "2021-07-07T01:57:10.074Z",
          "wordCount": 609,
          "title": "KATRec: Knowledge Aware aTtentive Sequential Recommendations. (arXiv:2012.03323v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">S&#xf6;ren Auer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersen_T/0/1/0/all/0/1\">Ted Pedersen</a>",
          "description": "There is currently a gap between the natural language expression of scholarly\npublications and their structured semantic content modeling to enable\nintelligent content search. With the volume of research growing exponentially\nevery year, a search feature operating over semantically structured content is\ncompelling. The SemEval-2021 Shared Task NLPContributionGraph (a.k.a. 'the NCG\ntask') tasks participants to develop automated systems that structure\ncontributions from NLP scholarly articles in the English language. Being the\nfirst-of-its-kind in the SemEval series, the task released structured data from\nNLP scholarly articles at three levels of information granularity, i.e. at\nsentence-level, phrase-level, and phrases organized as triples toward Knowledge\nGraph (KG) building. The sentence-level annotations comprised the few sentences\nabout the article's contribution. The phrase-level annotations were scientific\nterm and predicate phrases from the contribution sentences. Finally, the\ntriples constituted the research overview KG. For the Shared Task,\nparticipating systems were then expected to automatically classify contribution\nsentences, extract scientific terms and relations from the sentences, and\norganize them as KG triples.\n\nOverall, the task drew a strong participation demographic of seven teams and\n27 participants. The best end-to-end task system classified contribution\nsentences at 57.27% F1, phrases at 46.41% F1, and triples at 22.28% F1. While\nthe absolute performance to generate triples remains low, in the conclusion of\nthis article, the difficulty of producing such data and as a consequence of\nmodeling it is highlighted.",
          "link": "http://arxiv.org/abs/2106.07385",
          "publishedOn": "2021-07-07T01:57:10.053Z",
          "wordCount": 732,
          "title": "SemEval-2021 Task 11: NLPContributionGraph -- Structuring Scholarly NLP Contributions for a Research Knowledge Graph. (arXiv:2106.07385v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruihong_Q/0/1/0/all/0/1\">Qiu Ruihong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_W/0/1/0/all/0/1\">Wang Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_C/0/1/0/all/0/1\">Chen Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongzhi_Y/0/1/0/all/0/1\">Yin Hongzhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_H/0/1/0/all/0/1\">Huang Zi</a>",
          "description": "Visually-aware recommendation on E-commerce platforms aims to leverage visual\ninformation of items to predict a user's preference. It is commonly observed\nthat user's attention to visual features does not always reflect the real\npreference. Although a user may click and view an item in light of a visual\nsatisfaction of their expectations, a real purchase does not always occur due\nto the unsatisfaction of other essential features (e.g., brand, material,\nprice). We refer to the reason for such a visually related interaction\ndeviating from the real preference as a visual bias. Existing visually-aware\nmodels make use of the visual features as a separate collaborative signal\nsimilarly to other features to directly predict the user's preference without\nconsidering a potential bias, which gives rise to a visually biased\nrecommendation. In this paper, we derive a causal graph to identify and analyze\nthe visual bias of these existing methods. In this causal graph, the visual\nfeature of an item acts as a mediator, which could introduce a spurious\nrelationship between the user and the item. To eliminate this spurious\nrelationship that misleads the prediction of the user's real preference, an\nintervention and a counterfactual inference are developed over the mediator.\nParticularly, the Total Indirect Effect is applied for a debiased prediction\nduring the testing phase of the model. This causal inference framework is model\nagnostic such that it can be integrated into the existing methods. Furthermore,\nwe propose a debiased visually-aware recommender system, denoted as CausalRec\nto effectively retain the supportive significance of the visual information and\nremove the visual bias. Extensive experiments are conducted on eight benchmark\ndatasets, which shows the state-of-the-art performance of CausalRec and the\nefficacy of debiasing.",
          "link": "http://arxiv.org/abs/2107.02390",
          "publishedOn": "2021-07-07T01:57:10.038Z",
          "wordCount": 713,
          "title": "CausalRec: Causal Inference for Visual Debiasing in Visually-Aware Recommendation. (arXiv:2107.02390v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhibin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenchao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "Hierarchical topic models such as the gamma belief network (GBN) have\ndelivered promising results in mining multi-layer document representations and\ndiscovering interpretable topic taxonomies. However, they often assume in the\nprior that the topics at each layer are independently drawn from the Dirichlet\ndistribution, ignoring the dependencies between the topics both at the same\nlayer and across different layers. To relax this assumption, we propose\nsawtooth factorial topic embedding guided GBN, a deep generative model of\ndocuments that captures the dependencies and semantic similarities between the\ntopics in the embedding space. Specifically, both the words and topics are\nrepresented as embedding vectors of the same dimension. The topic matrix at a\nlayer is factorized into the product of a factor loading matrix and a topic\nembedding matrix, the transpose of which is set as the factor loading matrix of\nthe layer above. Repeating this particular type of factorization, which shares\ncomponents between adjacent layers, leads to a structure referred to as\nsawtooth factorization. An auto-encoding variational inference network is\nconstructed to optimize the model parameter via stochastic gradient descent.\nExperiments on big corpora show that our models outperform other neural topic\nmodels on extracting deeper interpretable topics and deriving better document\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.02757",
          "publishedOn": "2021-07-07T01:57:09.852Z",
          "wordCount": 649,
          "title": "Sawtooth Factorial Topic Embeddings Guided Gamma Belief Network. (arXiv:2107.02757v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mehak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saifi_S/0/1/0/all/0/1\">Shayan Saifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_K/0/1/0/all/0/1\">Konark Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekha_K/0/1/0/all/0/1\">Kumari Rekha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Aaditeshwar Seth</a>",
          "description": "Understanding what factors bring about socio-economic development may often\nsuffer from the streetlight effect, of analyzing the effect of only those\nvariables that have been measured and are therefore available for analysis. How\ndo we check whether all worthwhile variables have been instrumented and\nconsidered when building an econometric development model? We attempt to\naddress this question by building unsupervised learning methods to identify and\nrank news articles about diverse events occurring in different districts of\nIndia, that can provide insights about what may have transpired in the\ndistricts. This can help determine whether variables related to these events\nare indeed available or not to model the development of these districts. We\nalso describe several other applications that emerge from this approach, such\nas to use news articles to understand why pairs of districts that may have had\nsimilar socio-economic indicators approximately ten years back ended up at\ndifferent levels of development currently, and another application that\ngenerates a newsfeed of unusual news articles that do not conform to news\narticles about typical districts with a similar socio-economic profile. These\napplications outline the need for qualitative data to augment models based on\nquantitative data, and are meant to open up research on new ways to mine\ninformation from unstructured qualitative data to understand development.",
          "link": "http://arxiv.org/abs/2107.02765",
          "publishedOn": "2021-07-07T01:57:09.832Z",
          "wordCount": 688,
          "title": "Exploring the Scope of Using News Articles to Understand Development Patterns of Districts in India. (arXiv:2107.02765v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Escobar_Grisales_D/0/1/0/all/0/1\">Daniel Escobar-Grisales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasquez_Correa_J/0/1/0/all/0/1\">Juan Camilo Vasquez-Correa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orozco_Arroyave_J/0/1/0/all/0/1\">Juan Rafael Orozco-Arroyave</a>",
          "description": "The interest in demographic information retrieval based on text data has\nincreased in the research community because applications have shown success in\ndifferent sectors such as security, marketing, heath-care, and others.\nRecognition and identification of demographic traits such as gender, age,\nlocation, or personality based on text data can help to improve different\nmarketing strategies. For instance it makes it possible to segment and to\npersonalize offers, thus products and services are exposed to the group of\ngreatest interest. This type of technology has been discussed widely in\ndocuments from social media. However, the methods have been poorly studied in\ndata with a more formal structure, where there is no access to emoticons,\nmentions, and other linguistic phenomena that are only present in social media.\nThis paper proposes the use of recurrent and convolutional neural networks, and\na transfer learning strategy for gender recognition in documents that are\nwritten in informal and formal languages. Models are tested in two different\ndatabases consisting of Tweets and call-center conversations. Accuracies of up\nto 75\\% are achieved for both databases. The results also indicate that it is\npossible to transfer the knowledge from a system trained on a specific type of\nexpressions or idioms such as those typically used in social media into a more\nformal type of text data, where the amount of data is more scarce and its\nstructure is completely different.",
          "link": "http://arxiv.org/abs/2107.02759",
          "publishedOn": "2021-07-07T01:57:09.760Z",
          "wordCount": 673,
          "title": "Gender Recognition in Informal and Formal Language Scenarios via Transfer Learning. (arXiv:2107.02759v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhishan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dawei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>",
          "description": "As a critical component for online advertising and marking, click-through\nrate (CTR) prediction has draw lots of attentions from both industry and\nacademia field. Recently, the deep learning has become the mainstream\nmethodological choice for CTR. Despite of sustainable efforts have been made,\nexisting approaches still pose several challenges. On the one hand, high-order\ninteraction between the features is under-explored. On the other hand,\nhigh-order interactions may neglect the semantic information from the low-order\nfields. In this paper, we proposed a novel prediction method, named FINT, that\nemploys the Field-aware INTeraction layer which captures high-order feature\ninteractions while retaining the low-order field information. To empirically\ninvestigate the effectiveness and robustness of the FINT, we perform extensive\nexperiments on the three realistic databases: KDD2012, Criteo and Avazu. The\nobtained results demonstrate that the FINT can significantly improve the\nperformance compared to the existing methods, without increasing the amount of\ncomputation required. Moreover, the proposed method brought about 2.72\\%\nincrease to the advertising revenue of a big online video app through A/B\ntesting. To better promote the research in CTR field, we will release our code\nas well as reference implementation of those baseline models in the final\nversion.",
          "link": "http://arxiv.org/abs/2107.01999",
          "publishedOn": "2021-07-06T01:58:06.138Z",
          "wordCount": 642,
          "title": "FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weiyue Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zeyang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Hui Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Siming Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yunsheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shikun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>",
          "description": "WikiKG90M in KDD Cup 2021 is a large encyclopedic knowledge graph, which\ncould benefit various downstream applications such as question answering and\nrecommender systems. Participants are invited to complete the knowledge graph\nby predicting missing triplets. Recent representation learning methods have\nachieved great success on standard datasets like FB15k-237. Thus, we train the\nadvanced algorithms in different domains to learn the triplets, including OTE,\nQuatE, RotatE and TransE. Significantly, we modified OTE into NOTE (short for\nNorm-OTE) for better performance. Besides, we use both the DeepWalk and the\npost-smoothing technique to capture the graph structure for supplementation. In\naddition to the representations, we also use various statistical probabilities\namong the head entities, the relations and the tail entities for the final\nprediction. Experimental results show that the ensemble of state-of-the-art\nrepresentation learning methods could draw on each others strengths. And we\ndevelop feature engineering from validation candidates for further\nimprovements. Please note that we apply the same strategy on the test set for\nfinal inference. And these features may not be practical in the real world when\nconsidering ranking against all the entities.",
          "link": "http://arxiv.org/abs/2107.01892",
          "publishedOn": "2021-07-06T01:58:06.039Z",
          "wordCount": 637,
          "title": "NOTE: Solution for KDD-CUP 2021 WikiKG90M-LSC. (arXiv:2107.01892v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.14531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Draws_T/0/1/0/all/0/1\">Tim Draws</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tintarev_N/0/1/0/all/0/1\">Nava Tintarev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadiraju_U/0/1/0/all/0/1\">Ujwal Gadiraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozzon_A/0/1/0/all/0/1\">Alessandro Bozzon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timmermans_B/0/1/0/all/0/1\">Benjamin Timmermans</a>",
          "description": "The way pages are ranked in search results influences whether the users of\nsearch engines are exposed to more homogeneous, or rather to more diverse\nviewpoints. However, this viewpoint diversity is not trivial to assess. In this\npaper we use existing and novel ranking fairness metrics to evaluate viewpoint\ndiversity in search result rankings. We conduct a controlled simulation study\nthat shows how ranking fairness metrics can be used for viewpoint diversity,\nhow their outcome should be interpreted, and which metric is most suitable\ndepending on the situation. This paper lays out important ground work for\nfuture research to measure and assess viewpoint diversity in real search result\nrankings.",
          "link": "http://arxiv.org/abs/2010.14531",
          "publishedOn": "2021-07-06T01:58:05.948Z",
          "wordCount": 589,
          "title": "Assessing Viewpoint Diversity in Search Results Using Ranking Fairness Metrics. (arXiv:2010.14531v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>",
          "description": "Modelling mix-and-match relationships among fashion items has become\nincreasingly demanding yet challenging for modern E-commerce recommender\nsystems. When performing clothes matching, most existing approaches leverage\nthe latent visual features extracted from fashion item images for compatibility\nmodelling, which lacks explainability of generated matching results and can\nhardly convince users of the recommendations. Though recent methods start to\nincorporate pre-defined attribute information (e.g., colour, style, length,\netc.) for learning item representations and improving the model\ninterpretability, their utilisation of attribute information is still mainly\nreserved for enhancing the learned item representations and generating\nexplanations via post-processing. As a result, this creates a severe bottleneck\nwhen we are trying to advance the recommendation accuracy and generating\nfine-grained explanations since the explicit attributes have only loose\nconnections to the actual recommendation process. This work aims to tackle the\nexplainability challenge in fashion recommendation tasks by proposing a novel\nAttribute-aware Fashion Recommender (AFRec). Specifically, AFRec recommender\nassesses the outfit compatibility by explicitly leveraging the extracted\nattribute-level representations from each item's visual feature. The attributes\nserve as the bridge between two fashion items, where we quantify the affinity\nof a pair of items through the learned compatibility between their attributes.\nExtensive experiments have demonstrated that, by making full use of the\nexplicit attributes in the recommendation process, AFRec is able to achieve\nstate-of-the-art recommendation accuracy and generate intuitive explanations at\nthe same time.",
          "link": "http://arxiv.org/abs/2107.01655",
          "publishedOn": "2021-07-06T01:58:05.936Z",
          "wordCount": 654,
          "title": "Attribute-aware Explainable Complementary Clothing Recommendation. (arXiv:2107.01655v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yakhchi_S/0/1/0/all/0/1\">Shahpar Yakhchi</a>",
          "description": "Recommender systems (RSs) have emerged as very useful tools to help customers\nwith their decision-making process, find items of their interest, and alleviate\nthe information overload problem. There are two different lines of approaches\nin RSs: (1) general recommenders with the main goal of discovering long-term\nusers' preferences, and (2) sequential recommenders with the main focus of\ncapturing short-term users' preferences in a session of user-item interaction\n(here, a session refers to a record of purchasing multiple items in one\nshopping event). While considering short-term users' preferences may satisfy\ntheir current needs and interests, long-term users' preferences provide users\nwith the items that they may interact with, eventually. In this thesis, we\nfirst focus on improving the performance of general RSs. Most of the existing\ngeneral RSs tend to exploit the users' rating patterns on common items to\ndetect similar users. The data sparsity problem (i.e. the lack of available\ninformation) is one of the major challenges for the current general RSs, and\nthey may fail to have any recommendations when there are no common items of\ninterest among users. We call this problem data sparsity with no feedback on\ncommon items (DSW-n-FCI). To overcome this problem, we propose a\npersonality-based RS in which similar users are identified based on the\nsimilarity of their personality traits.",
          "link": "http://arxiv.org/abs/2107.01529",
          "publishedOn": "2021-07-06T01:58:05.483Z",
          "wordCount": 655,
          "title": "Learning Complex Users' Preferences for Recommender Systems. (arXiv:2107.01529v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Arshad Shaikh</a>",
          "description": "Session-based recommendation systems suggest relevant items to users by\nmodeling user behavior and preferences using short-term anonymous sessions.\nExisting methods leverage Graph Neural Networks (GNNs) that propagate and\naggregate information from neighboring nodes i.e., local message passing. Such\ngraph-based architectures have representational limits, as a single sub-graph\nis susceptible to overfit the sequential dependencies instead of accounting for\ncomplex transitions between items in different sessions. We propose using a\nTransformer in combination with a target attentive GNN, which allows richer\nRepresentation Learning. Our experimental results and ablation show that our\nproposed method outperforms the existing methods on real-world benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2107.01516",
          "publishedOn": "2021-07-06T01:58:05.449Z",
          "wordCount": 540,
          "title": "Improved Representation Learning for Session-based Recommendation. (arXiv:2107.01516v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruihong_Q/0/1/0/all/0/1\">Qiu Ruihong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_H/0/1/0/all/0/1\">Huang Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jingjing_L/0/1/0/all/0/1\">Li Jingjing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongzhi_Y/0/1/0/all/0/1\">Yin Hongzhi</a>",
          "description": "Different from the traditional recommender system, the session-based\nrecommender system introduces the concept of the session, i.e., a sequence of\ninteractions between a user and multiple items within a period, to preserve the\nuser's recent interest. The existing work on the session-based recommender\nsystem mainly relies on mining sequential patterns within individual sessions,\nwhich are not expressive enough to capture more complicated dependency\nrelationships among items. In addition, it does not consider the cross-session\ninformation due to the anonymity of the session data, where the linkage between\ndifferent sessions is prevented. In this paper, we solve these problems with\nthe graph neural networks technique. First, each session is represented as a\ngraph rather than a linear sequence structure, based on which a novel Full\nGraph Neural Network (FGNN) is proposed to learn complicated item dependency.\nTo exploit and incorporate cross-session information in the individual\nsession's representation learning, we further construct a Broadly Connected\nSession (BCS) graph to link different sessions and a novel Mask-Readout\nfunction to improve session embedding based on the BCS graph. Extensive\nexperiments have been conducted on two e-commerce benchmark datasets, i.e.,\nYoochoose and Diginetica, and the experimental results demonstrate the\nsuperiority of our proposal through comparisons with state-of-the-art\nsession-based recommender models.",
          "link": "http://arxiv.org/abs/2107.00852",
          "publishedOn": "2021-07-05T01:54:56.559Z",
          "wordCount": 651,
          "title": "Exploiting Cross-Session Information for Session-based Recommendation with Graph Neural Networks. (arXiv:2107.00852v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brockmeier_M/0/1/0/all/0/1\">Malte Brockmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yawen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pateer_S/0/1/0/all/0/1\">Sunita Pateer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertling_S/0/1/0/all/0/1\">Sven Hertling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>",
          "description": "Modern large-scale knowledge graphs, such as DBpedia, are datasets which\nrequire large computational resources to serve and process. Moreover, they\noften have longer release cycles, which leads to outdated information in those\ngraphs. In this paper, we present DBpedia on Demand -- a system which serves\nDBpedia resources on demand without the need to materialize and store the\nentire graph, and which even provides limited querying functionality.",
          "link": "http://arxiv.org/abs/2107.00873",
          "publishedOn": "2021-07-05T01:54:56.528Z",
          "wordCount": 516,
          "title": "On-Demand and Lightweight Knowledge Graph Generation -- a Demonstration with DBpedia. (arXiv:2107.00873v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruihong_Q/0/1/0/all/0/1\">Qiu Ruihong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_H/0/1/0/all/0/1\">Huang Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_C/0/1/0/all/0/1\">Chen Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongzhi_Y/0/1/0/all/0/1\">Yin Hongzhi</a>",
          "description": "For present e-commerce platforms, session-based recommender systems are\ndeveloped to predict users' preference for next-item recommendation. Although a\nsession can usually reflect a user's current preference, a local shift of the\nuser's intention within the session may still exist. Specifically, the\ninteractions that take place in the early positions within a session generally\nindicate the user's initial intention, while later interactions are more likely\nto represent the latest intention. Such positional information has been rarely\nconsidered in existing methods, which restricts their ability to capture the\nsignificance of interactions at different positions. To thoroughly exploit the\npositional information within a session, a theoretical framework is developed\nin this paper to provide an in-depth analysis of the positional information. We\nformally define the properties of forward-awareness and backward-awareness to\nevaluate the ability of positional encoding schemes in capturing the initial\nand the latest intention. According to our analysis, existing positional\nencoding schemes are generally forward-aware only, which can hardly represent\nthe dynamics of the intention in a session. To enhance the positional encoding\nscheme for the session-based recommendation, a dual positional encoding (DPE)\nis proposed to account for both forward-awareness and backward-awareness. Based\non DPE, we propose a novel Positional Recommender (PosRec) model with a\nwell-designed Position-aware Gated Graph Neural Network module to fully exploit\nthe positional information for session-based recommendation tasks. Extensive\nexperiments are conducted on two e-commerce benchmark datasets, Yoochoose and\nDiginetica and the experimental results show the superiority of the PosRec by\ncomparing it with the state-of-the-art session-based recommender models.",
          "link": "http://arxiv.org/abs/2107.00846",
          "publishedOn": "2021-07-05T01:54:56.486Z",
          "wordCount": 680,
          "title": "Exploiting Positional Information for Session-based Recommendation. (arXiv:2107.00846v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curmei_M/0/1/0/all/0/1\">Mihaela Curmei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_S/0/1/0/all/0/1\">Sarah Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1\">Benjamin Recht</a>",
          "description": "In this work, we consider how preference models in interactive recommendation\nsystems determine the availability of content and users' opportunities for\ndiscovery. We propose an evaluation procedure based on stochastic reachability\nto quantify the maximum probability of recommending a target piece of content\nto an user for a set of allowable strategic modifications. This framework\nallows us to compute an upper bound on the likelihood of recommendation with\nminimal assumptions about user behavior. Stochastic reachability can be used to\ndetect biases in the availability of content and diagnose limitations in the\nopportunities for discovery granted to users. We show that this metric can be\ncomputed efficiently as a convex program for a variety of practical settings,\nand further argue that reachability is not inherently at odds with accuracy. We\ndemonstrate evaluations of recommendation algorithms trained on large datasets\nof explicit and implicit ratings. Our results illustrate how preference models,\nselection rules, and user interventions impact reachability and how these\neffects can be distributed unevenly.",
          "link": "http://arxiv.org/abs/2107.00833",
          "publishedOn": "2021-07-05T01:54:56.398Z",
          "wordCount": 610,
          "title": "Quantifying Availability and Discovery in Recommender Systems via Stochastic Reachability. (arXiv:2107.00833v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.16036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1\">Chris Chafe</a>",
          "description": "This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet. This is fully probabilistic,\nauto-regressive, and causal, i.e. each sample generated depends only on the\npreviously observed samples. Our approach outperforms a widely used wavenet\narchitecture by up to 9% on a similar dataset for predicting the next step.\nUsing the attention mechanism, we enable the architecture to learn which audio\nsamples are important for the prediction of the future sample. We show how\ncausal transformer generative models can be used for raw waveform synthesis. We\nalso show that this performance can be improved by another 2% by conditioning\nsamples over a wider context. The flexibility of the current model to\nsynthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.",
          "link": "http://arxiv.org/abs/2106.16036",
          "publishedOn": "2021-07-09T01:58:24.045Z",
          "wordCount": 646,
          "title": "A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Mingde Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zichao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yifeng Shi</a>",
          "description": "Long-range and short-range temporal modeling are two complementary and\ncrucial aspects of video recognition. Most of the state-of-the-arts focus on\nshort-range spatio-temporal modeling and then average multiple snippet-level\npredictions to yield the final video-level prediction. Thus, their video-level\nprediction does not consider spatio-temporal features of how video evolves\nalong the temporal dimension. In this paper, we introduce a novel Dynamic\nSegment Aggregation (DSA) module to capture relationship among snippets. To be\nmore specific, we attempt to generate a dynamic kernel for a convolutional\noperation to aggregate long-range temporal information among adjacent snippets\nadaptively. The DSA module is an efficient plug-and-play module and can be\ncombined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform\npowerful long-range modeling with minimal overhead. The final video\narchitecture, coined as DSANet. We conduct extensive experiments on several\nvideo recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,\nSomething-Something V1 and ActivityNet) to show its superiority. Our proposed\nDSA module is shown to benefit various video recognition models significantly.\nFor example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is\nimproved from 74.9% to 78.2% on Kinetics-400. Codes are available at\nhttps://github.com/whwu95/DSANet.",
          "link": "http://arxiv.org/abs/2105.12085",
          "publishedOn": "2021-07-08T01:57:55.966Z",
          "wordCount": 675,
          "title": "DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Shiyin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>",
          "description": "This paper describes a variational auto-encoder based non-autoregressive\ntext-to-speech (VAENAR-TTS) model. The autoregressive TTS (AR-TTS) models based\non the sequence-to-sequence architecture can generate high-quality speech, but\ntheir sequential decoding process can be time-consuming. Recently,\nnon-autoregressive TTS (NAR-TTS) models have been shown to be more efficient\nwith the parallel decoding process. However, these NAR-TTS models rely on\nphoneme-level durations to generate a hard alignment between the text and the\nspectrogram. Obtaining duration labels, either through forced alignment or\nknowledge distillation, is cumbersome. Furthermore, hard alignment based on\nphoneme expansion can degrade the naturalness of the synthesized speech. In\ncontrast, the proposed model of VAENAR-TTS is an end-to-end approach that does\nnot require phoneme-level durations. The VAENAR-TTS model does not contain\nrecurrent structures and is completely non-autoregressive in both the training\nand inference phases. Based on the VAE architecture, the alignment information\nis encoded in the latent variable, and attention-based soft alignment between\nthe text and the latent variable is used in the decoder to reconstruct the\nspectrogram. Experiments show that VAENAR-TTS achieves state-of-the-art\nsynthesis quality, while the synthesis speed is comparable with other NAR-TTS\nmodels.",
          "link": "http://arxiv.org/abs/2107.03298",
          "publishedOn": "2021-07-08T01:57:55.934Z",
          "wordCount": 626,
          "title": "VAENAR-TTS: Variational Auto-Encoder based Non-AutoRegressive Text-to-Speech Synthesis. (arXiv:2107.03298v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gaowen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1\">Hugo Latapie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1\">Jason Corso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>",
          "description": "Cross-view video synthesis task seeks to generate video sequences of one view\nfrom another dramatically different view. In this paper, we investigate the\nexocentric (third-person) view to egocentric (first-person) view video\ngeneration task. This is challenging because egocentric view sometimes is\nremarkably different from the exocentric view. Thus, transforming the\nappearances across the two different views is a non-trivial task. Particularly,\nwe propose a novel Bi-directional Spatial Temporal Attention Fusion Generative\nAdversarial Network (STA-GAN) to learn both spatial and temporal information to\ngenerate egocentric video sequences from the exocentric view. The proposed\nSTA-GAN consists of three parts: temporal branch, spatial branch, and attention\nfusion. First, the temporal and spatial branches generate a sequence of fake\nframes and their corresponding features. The fake frames are generated in both\ndownstream and upstream directions for both temporal and spatial branches.\nNext, the generated four different fake frames and their corresponding features\n(spatial and temporal branches in two directions) are fed into a novel\nmulti-generation attention fusion module to produce the final video sequence.\nMeanwhile, we also propose a novel temporal and spatial dual-discriminator for\nmore robust network optimization. Extensive experiments on the Side2Ego and\nTop2Ego datasets show that the proposed STA-GAN significantly outperforms the\nexisting methods.",
          "link": "http://arxiv.org/abs/2107.03120",
          "publishedOn": "2021-07-08T01:57:55.834Z",
          "wordCount": 644,
          "title": "Cross-View Exocentric to Egocentric Video Synthesis. (arXiv:2107.03120v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zibin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiyu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shutao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Maowei Hu</a>",
          "description": "Compared with tedious per-pixel mask annotating, it is much easier to\nannotate data by clicks, which costs only several seconds for an image.\nHowever, applying clicks to learn video semantic segmentation model has not\nbeen explored before. In this work, we propose an effective weakly-supervised\nvideo semantic segmentation pipeline with click annotations, called WeClick,\nfor saving laborious annotating effort by segmenting an instance of the\nsemantic class with only a single click. Since detailed semantic information is\nnot captured by clicks, directly training with click labels leads to poor\nsegmentation predictions. To mitigate this problem, we design a novel memory\nflow knowledge distillation strategy to exploit temporal information (named\nmemory flow) in abundant unlabeled video frames, by distilling the neighboring\npredictions to the target frame via estimated motion. Moreover, we adopt\nvanilla knowledge distillation for model compression. In this case, WeClick\nlearns compact video semantic segmentation models with the low-cost click\nannotations during the training phase yet achieves real-time and accurate\nmodels during the inference period. Experimental results on Cityscapes and\nCamvid show that WeClick outperforms the state-of-the-art methods, increases\nperformance by 10.24% mIoU than baseline, and achieves real-time execution.",
          "link": "http://arxiv.org/abs/2107.03088",
          "publishedOn": "2021-07-08T01:57:55.772Z",
          "wordCount": 644,
          "title": "WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations. (arXiv:2107.03088v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>",
          "description": "Over the last few decades, artificial intelligence research has made\ntremendous strides, but it still heavily relies on fixed datasets in stationary\nenvironments. Continual learning is a growing field of research that examines\nhow AI systems can learn sequentially from a continuous stream of linked data\nin the same way that biological systems do. Simultaneously, fake media such as\ndeepfakes and synthetic face images have emerged as significant to current\nmultimedia technologies. Recently, numerous method has been proposed which can\ndetect deepfakes with high accuracy. However, they suffer significantly due to\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\nthis work, we apply continuous learning to neural networks' learning dynamics,\nemphasizing its potential to increase data efficiency significantly. We propose\nContinual Representation using Distillation (CoReD) method that employs the\nconcept of Continual Learning (CoL), Representation Learning (ReL), and\nKnowledge Distillation (KD). We design CoReD to perform sequential domain\nadaptation tasks on new deepfake and GAN-generated synthetic face datasets,\nwhile effectively minimizing the catastrophic forgetting in a teacher-student\nmodel setting. Our extensive experimental results demonstrate that our method\nis efficient at domain adaptation to detect low-quality deepfakes videos and\nGAN-generated images from several datasets, outperforming the-state-of-art\nbaseline methods.",
          "link": "http://arxiv.org/abs/2107.02408",
          "publishedOn": "2021-07-07T01:57:11.618Z",
          "wordCount": 675,
          "title": "CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3$\\times$ as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results~(e.g., Top-1 accuracy 84.1% trained on 224$\\times$224\nImageNet-1K only), while being more scalable on high-resolution images. The\nmodels and source code will be released soon.",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-07T01:57:11.519Z",
          "wordCount": 671,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data will be made available.",
          "link": "http://arxiv.org/abs/2106.14118",
          "publishedOn": "2021-07-07T01:57:10.603Z",
          "wordCount": 620,
          "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_L/0/1/0/all/0/1\">Long Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>",
          "description": "Image editing techniques enable people to modify the content of an image\nwithout leaving visual traces and thus may cause serious security risks. Hence\nthe detection and localization of these forgeries become quite necessary and\nchallenging. Furthermore, unlike other tasks with extensive data, there is\nusually a lack of annotated forged images for training due to annotation\ndifficulties. In this paper, we propose a self-adversarial training strategy\nand a reliable coarse-to-fine network that utilizes a self-attention mechanism\nto localize forged regions in forgery images. The self-attention module is\nbased on a Channel-Wise High Pass Filter block (CW-HPF). CW-HPF leverages\ninter-channel relationships of features and extracts noise features by high\npass filters. Based on the CW-HPF, a self-attention mechanism, called forgery\nattention, is proposed to capture rich contextual dependencies of intrinsic\ninconsistency extracted from tampered regions. Specifically, we append two\ntypes of attention modules on top of CW-HPF respectively to model internal\ninterdependencies in spatial dimension and external dependencies among\nchannels. We exploit a coarse-to-fine network to enhance the noise\ninconsistency between original and tampered regions. More importantly, to\naddress the issue of insufficient training data, we design a self-adversarial\ntraining strategy that expands training data dynamically to achieve more robust\nperformance. Specifically, in each training iteration, we perform adversarial\nattacks against our network to generate adversarial examples and train our\nmodel on them. Extensive experimental results demonstrate that our proposed\nalgorithm steadily outperforms state-of-the-art methods by a clear margin in\ndifferent benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.02434",
          "publishedOn": "2021-07-07T01:57:10.571Z",
          "wordCount": 688,
          "title": "Self-Adversarial Training incorporating Forgery Attention for Image Forgery Localization. (arXiv:2107.02434v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiri Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>",
          "description": "Nowadays, most existing blind image quality assessment (BIQA) models 1) are\ndeveloped for synthetically-distorted images and often generalize poorly to\nauthentic ones; 2) heavily rely on human ratings, which are prohibitively\nlabor-expensive to collect. Here, we propose an $opinion$-$free$ BIQA method\nthat learns from synthetically-distorted images and multiple agents to assess\nthe perceptual quality of authentically-distorted ones captured in the wild\nwithout relying on human labels. Specifically, we first assemble a large number\nof image pairs from synthetically-distorted images and use a set of\nfull-reference image quality assessment (FR-IQA) models to assign pseudo-binary\nlabels of each pair indicating which image has higher quality as the\nsupervisory signal. We then train a convolutional neural network (CNN)-based\nBIQA model to rank the perceptual quality, optimized for consistency with the\nbinary labels. Since there exists domain shift between the synthetically- and\nauthentically-distorted images, an unsupervised domain adaptation (UDA) module\nis introduced to alleviate this issue. Extensive experiments demonstrate the\neffectiveness of our proposed $opinion$-$free$ BIQA model, yielding\nstate-of-the-art performance in terms of correlation with human opinion scores,\nas well as gMAD competition. Codes will be made publicly available upon\nacceptance.",
          "link": "http://arxiv.org/abs/2106.14076",
          "publishedOn": "2021-07-06T01:58:06.226Z",
          "wordCount": 659,
          "title": "Learning from Synthetic Data for Opinion-free Blind Image Quality Assessment in the Wild. (arXiv:2106.14076v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1\">Sabato Marco Siniscalchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xianjun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuanjun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuzhong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>",
          "description": "We propose a novel neural model compression strategy combining data\naugmentation, knowledge transfer, pruning, and quantization for device-robust\nacoustic scene classification (ASC). Specifically, we tackle the ASC task in a\nlow-resource environment leveraging a recently proposed advanced neural network\npruning mechanism, namely Lottery Ticket Hypothesis (LTH), to find a\nsub-network neural model associated with a small amount non-zero model\nparameters. The effectiveness of LTH for low-complexity acoustic modeling is\nassessed by investigating various data augmentation and compression schemes,\nand we report an efficient joint framework for low-complexity multi-device ASC,\ncalled Acoustic Lottery. Acoustic Lottery could compress an ASC model over\n$1/10^{4}$ and attain a superior performance (validation accuracy of 74.01% and\nLog loss of 0.76) compared to its not compressed seed model. All results\nreported in this work are based on a joint effort of four groups, namely\nGT-USTC-UKE-Tencent, aiming to address the \"Low-Complexity Acoustic Scene\nClassification (ASC) with Multiple Devices\" in the DCASE 2021 Challenge Task\n1a.",
          "link": "http://arxiv.org/abs/2107.01461",
          "publishedOn": "2021-07-06T01:58:06.018Z",
          "wordCount": 647,
          "title": "A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification. (arXiv:2107.01461v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/1711.04916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Y/0/1/0/all/0/1\">Yan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_T/0/1/0/all/0/1\">Tingting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyuan Yang</a>",
          "description": "The distortion in steganography that usually comes from the modification or\nrecoding on the cover image during the embedding process leaves the\nsteganalyzer with possibility of discriminating. Faced with such a risk, we\npropose generative steganography with Kerckhoffs' principle (GSK) in this\nletter. In GSK, the secret messages are generated by a cover image using a\ngenerator rather than embedded into the cover, thus resulting in no\nmodifications in the cover. To ensure the security, the generators are trained\nto meet Kerckhoffs' principle based on generative adversarial networks (GAN).\nEverything about the GSK system, except the extraction key, is public knowledge\nfor the receivers. The secret messages can be outputted by the generator if and\nonly if the extraction key and the cover image are both inputted. In the\ngenerator training procedures, there are two GANs, Message- GAN and Cover-GAN,\ndesigned to work jointly making the generated results under the control of the\nextraction key and the cover image. We provide experimental results on the\ntraining process and give an example of the working process by adopting a\ngenerator trained on MNIST, which demonstrate that GSK can use a cover image\nwithout any modification to generate messages, and without the extraction key\nor the cover image, only meaningless results would be obtained.",
          "link": "http://arxiv.org/abs/1711.04916",
          "publishedOn": "2021-07-06T01:58:05.972Z",
          "wordCount": 673,
          "title": "Generative Steganography with Kerckhoffs' Principle. (arXiv:1711.04916v3 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.04463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kangle Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Aayush Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>",
          "description": "We present an unsupervised approach that converts the input speech of any\nindividual into audiovisual streams of potentially-infinitely many output\nspeakers. Our approach builds on simple autoencoders that project out-of-sample\ndata onto the distribution of the training set. We use Exemplar Autoencoders to\nlearn the voice, stylistic prosody, and visual appearance of a specific target\nexemplar speech. In contrast to existing methods, the proposed approach can be\neasily extended to an arbitrarily large number of speakers and styles using\nonly 3 minutes of target audio-video data, without requiring {\\em any} training\ndata for the input speaker. To do so, we learn audiovisual bottleneck\nrepresentations that capture the structured linguistic content of speech. We\noutperform prior approaches on both audio and video synthesis, and provide\nextensive qualitative analysis on our project page --\nhttps://www.cs.cmu.edu/~exemplar-ae/.",
          "link": "http://arxiv.org/abs/2001.04463",
          "publishedOn": "2021-07-06T01:58:05.872Z",
          "wordCount": 626,
          "title": "Unsupervised Audiovisual Synthesis via Exemplar Autoencoders. (arXiv:2001.04463v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the development set,\nthe achieved CCC is 0.410 for valence and 0.661 for arousal, which\nsignificantly outperforms the baseline method with the corresponding CCC of\n0.210 and 0.230 for valence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.",
          "link": "http://arxiv.org/abs/2107.01175",
          "publishedOn": "2021-07-05T01:54:56.329Z",
          "wordCount": 611,
          "title": "Audio-visual Attentive Fusion for Continuous Emotion Recognition. (arXiv:2107.01175v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+de_Lima_Santos_M/0/1/0/all/0/1\">Mathias-Felipe de-Lima-Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooli_A/0/1/0/all/0/1\">Arwa Kooli</a>",
          "description": "News outlets are developing formats dedicated to social platforms that\ncapture audience attention, such as Instagram stories, Facebook Instant\narticles, and YouTube videos. In some cases, these formats are created in\ncollaboration with the tech companies themselves. At the same time, the use of\ndata-driven storytelling is becoming increasingly integrated into the\never-complex business models of news outlets, generating more impact and\nvisibility. Previous studies have focused on studying these two effects\nseparately. To address this gap in the literature, this paper identifies and\nanalyzes the use of data journalism on the Instagram content of AJ Labs, the\nteam dedicated to producing data-driven and interactive stories for the Al\nJazeera news network. Drawing upon a mixed-method approach, this study examines\nthe use and characteristics of data stories on social media platforms. Results\nsuggest that there is reliance on producing visual content that covers topics\nsuch as politics and violence. In general, AJ Labs relies on the use of\ninfographics and produces its own unique data. To conclude, this paper suggests\npotential ways to improve the use of Instagram to tell data stories.",
          "link": "http://arxiv.org/abs/2107.00938",
          "publishedOn": "2021-07-05T01:54:56.294Z",
          "wordCount": 635,
          "title": "Instagrammable Data: Using Visuals to Showcase More Than Numbers on AJ Labs Instagram Page. (arXiv:2107.00938v1 [cs.CY])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.05956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1\">Ekdeep Singh Lubana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1\">Hidenori Tanaka</a>",
          "description": "Inspired by BatchNorm, there has been an explosion of normalization layers\nfor deep neural networks (DNNs). However, these alternative normalization\nlayers have seen minimal use, partially due to a lack of guiding principles\nthat can help identify when these layers can serve as a replacement for\nBatchNorm. To address this problem, we take a theoretical approach,\ngeneralizing the known beneficial mechanisms of BatchNorm to several recently\nproposed normalization techniques. Our generalized theory leads to the\nfollowing set of principles: (i) similar to BatchNorm, activations-based\nnormalization layers can prevent exponential growth of activations in ResNets,\nbut parametric layers require explicit remedies; (ii) use of GroupNorm can\nensure informative forward propagation, with different samples being assigned\ndissimilar activations, but increasing group size results in increasingly\nindistinguishable activations for different samples, explaining slow\nconvergence speed in models with LayerNorm; (iii) small group sizes result in\nlarge gradient norm in earlier layers, hence explaining training instability\nissues in Instance Normalization and illustrating a speed-stability tradeoff in\nGroupNorm. Overall, our analysis reveals a unified set of mechanisms that\nunderpin the success of normalization methods in deep learning, providing us\nwith a compass to systematically explore the vast design space of DNN\nnormalization layers.",
          "link": "http://arxiv.org/abs/2106.05956",
          "publishedOn": "2021-07-09T01:58:27.809Z",
          "wordCount": 669,
          "title": "Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning. (arXiv:2106.05956v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1\">Vincent Sitzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Pulkit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>",
          "description": "Humans have a strong intuitive understanding of the 3D environment around us.\nThe mental model of the physics in our brain applies to objects of different\nmaterials and enables us to perform a wide range of manipulation tasks that are\nfar beyond the reach of current robots. In this work, we desire to learn models\nfor dynamic 3D scenes purely from 2D visual observations. Our model combines\nNeural Radiance Fields (NeRF) and time contrastive learning with an\nautoencoding framework, which learns viewpoint-invariant 3D-aware scene\nrepresentations. We show that a dynamics model, constructed over the learned\nrepresentation space, enables visuomotor control for challenging manipulation\ntasks involving both rigid bodies and fluids, where the target is specified in\na viewpoint different from what the robot operates on. When coupled with an\nauto-decoding framework, it can even support goal specification from camera\nviewpoints that are outside the training distribution. We further demonstrate\nthe richness of the learned 3D dynamics model by performing future prediction\nand novel view synthesis. Finally, we provide detailed ablation studies\nregarding different system designs and qualitative analysis of the learned\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.04004",
          "publishedOn": "2021-07-09T01:58:27.789Z",
          "wordCount": 632,
          "title": "3D Neural Scene Representations for Visuomotor Control. (arXiv:2107.04004v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lynch_C/0/1/0/all/0/1\">Corey Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1\">Pierre Sermanet</a>",
          "description": "Natural language is perhaps the most flexible and intuitive way for humans to\ncommunicate tasks to a robot. Prior work in imitation learning typically\nrequires each task be specified with a task id or goal image -- something that\nis often impractical in open-world environments. On the other hand, previous\napproaches in instruction following allow agent behavior to be guided by\nlanguage, but typically assume structure in the observations, actuators, or\nlanguage that limit their applicability to complex settings like robotics. In\nthis work, we present a method for incorporating free-form natural language\nconditioning into imitation learning. Our approach learns perception from\npixels, natural language understanding, and multitask continuous control\nend-to-end as a single neural network. Unlike prior work in imitation learning,\nour method is able to incorporate unlabeled and unstructured demonstration data\n(i.e. no task or language labels). We show this dramatically improves language\nconditioned performance, while reducing the cost of language annotation to less\nthan 1% of total data. At test time, a single language conditioned visuomotor\npolicy trained with our method can perform a wide variety of robotic\nmanipulation skills in a 3D environment, specified only with natural language\ndescriptions of each task (e.g. \"open the drawer...now pick up the block...now\npress the green button...\"). To scale up the number of instructions an agent\ncan follow, we propose combining text conditioned policies with large\npretrained neural language models. We find this allows a policy to be robust to\nmany out-of-distribution synonym instructions, without requiring new\ndemonstrations. See videos of a human typing live text commands to our agent at\nlanguage-play.github.io",
          "link": "http://arxiv.org/abs/2005.07648",
          "publishedOn": "2021-07-09T01:58:27.761Z",
          "wordCount": 735,
          "title": "Language Conditioned Imitation Learning over Unstructured Data. (arXiv:2005.07648v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+sun_W/0/1/0/all/0/1\">Wei sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>",
          "description": "To improve the viewer's quality of experience and optimize processing systems\nin computer graphics applications, the 3D quality assessment (3D-QA) has become\nan important task in the multimedia area. Point cloud and mesh are the two most\nwidely used electronic representation formats of 3D models, the quality of\nwhich is quite sensitive to operations like simplification and compression.\nTherefore, many studies concerning point cloud quality assessment (PCQA) and\nmesh quality assessment (MQA) have been carried out to measure the visual\nquality degradations caused by lossy operations. However, a large part of\nprevious studies utilizes full-reference (FR) metrics, which means they may\nfail to predict the accurate quality level of 3D models when the reference 3D\nmodel is not available. Furthermore, limited numbers of 3D-QA metrics are\ncarried out to take color features into consideration, which significantly\nrestricts the effectiveness and scope of application. In many quality\nassessment studies, natural scene statistics (NSS) have shown a good ability to\nquantify the distortion of natural scenes to statistical parameters. Therefore,\nwe propose an NSS-based no-reference quality assessment metric for colored 3D\nmodels. In this paper, quality-aware features are extracted from the aspects of\ncolor and geometry directly from the 3D models. Then the statistic parameters\nare estimated using different distribution models to describe the\ncharacteristic of the 3D models. Our method is mainly validated on the colored\npoint cloud quality assessment database (SJTU-PCQA) and the colored mesh\nquality assessment database (CMDM). The experimental results show that the\nproposed method outperforms all the state-of-art NR 3D-QA metrics and obtains\nan acceptable gap with the state-of-art FR 3D-QA metrics.",
          "link": "http://arxiv.org/abs/2107.02041",
          "publishedOn": "2021-07-09T01:58:27.712Z",
          "wordCount": 746,
          "title": "No-Reference Quality Assessment for Colored Point Cloud and Mesh Based on Natural Scene Statistics. (arXiv:2107.02041v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.05556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1\">Pedro Henrique Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1\">Vlad Niculae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinho_Z/0/1/0/all/0/1\">Zita Marinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; Martins</a>",
          "description": "Visual attention mechanisms are widely used in multimodal tasks, as visual\nquestion answering (VQA). One drawback of softmax-based attention mechanisms is\nthat they assign some probability mass to all image regions, regardless of\ntheir adjacency structure and of their relevance to the text. In this paper, to\nbetter link the image structure with the text, we replace the traditional\nsoftmax attention mechanism with two alternative sparsity-promoting\ntransformations: sparsemax, which is able to select only the relevant regions\n(assigning zero weight to the rest), and a newly proposed Total-Variation\nSparse Attention (TVmax), which further encourages the joint selection of\nadjacent spatial locations. Experiments in VQA show gains in accuracy as well\nas higher similarity to human attention, which suggests better\ninterpretability.",
          "link": "http://arxiv.org/abs/2002.05556",
          "publishedOn": "2021-07-09T01:58:27.706Z",
          "wordCount": 587,
          "title": "Sparse and Structured Visual Attention. (arXiv:2002.05556v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Luyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanning Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huangjing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pheng_P/0/1/0/all/0/1\">Pheng-Ann Pheng</a>",
          "description": "Chest X-ray (CXR) is the most typical diagnostic X-ray examination for\nscreening various thoracic diseases. Automatically localizing lesions from CXR\nis promising for alleviating radiologists' reading burden. However, CXR\ndatasets are often with massive image-level annotations and scarce lesion-level\nannotations, and more often, without annotations. Thus far, unifying different\nsupervision granularities to develop thoracic disease detection algorithms has\nnot been comprehensively addressed. In this paper, we present OXnet, the first\ndeep omni-supervised thoracic disease detection network to our best knowledge\nthat uses as much available supervision as possible for CXR diagnosis. We first\nintroduce supervised learning via a one-stage detection model. Then, we inject\na global classification head to the detection model and propose dual attention\nalignment to guide the global gradient to the local detection branch, which\nenables learning lesion detection from image-level annotations. We also impose\nintra-class compactness and inter-class separability with global prototype\nalignment to further enhance the global information learning. Moreover, we\nleverage a soft focal loss to distill the soft pseudo-labels of unlabeled data\ngenerated by a teacher model. Extensive experiments on a large-scale chest\nX-ray dataset show the proposed OXnet outperforms competitive methods with\nsignificant margins. Further, we investigate omni-supervision under various\nannotation granularities and corroborate OXnet is a promising choice to\nmitigate the plight of annotation shortage for medical image diagnosis.",
          "link": "http://arxiv.org/abs/2104.03218",
          "publishedOn": "2021-07-09T01:58:27.692Z",
          "wordCount": 696,
          "title": "OXnet: Omni-supervised Thoracic Disease Detection from Chest X-rays. (arXiv:2104.03218v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuqi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Manli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangzhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haoyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yizhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guoxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jingyuan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Heng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baogui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weihao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zongzheng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yueqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yida Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuqing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wanqing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1\">Danyang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chuhao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuchong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Multi-modal pre-training models have been intensively explored to bridge\nvision and language in recent years. However, most of them explicitly model the\ncross-modal interaction between image-text pairs, by assuming that there exists\nstrong semantic correlation between the text and image modalities. Since this\nstrong assumption is often invalid in real-world scenarios, we choose to\nimplicitly model the cross-modal correlation for large-scale multi-modal\npre-training, which is the focus of the Chinese project `WenLan' led by our\nteam. Specifically, with the weak correlation assumption over image-text pairs,\nwe propose a two-tower pre-training model called BriVL within the cross-modal\ncontrastive learning framework. Unlike OpenAI CLIP that adopts a simple\ncontrastive learning method, we devise a more advanced algorithm by adapting\nthe latest method MoCo into the cross-modal scenario. By building a large\nqueue-based dictionary, our BriVL can incorporate more negative samples in\nlimited GPU resources. We further construct a large Chinese multi-source\nimage-text dataset called RUC-CAS-WenLan for pre-training our BriVL model.\nExtensive experiments demonstrate that the pre-trained BriVL model outperforms\nboth UNITER and OpenAI CLIP on various downstream tasks.",
          "link": "http://arxiv.org/abs/2103.06561",
          "publishedOn": "2021-07-09T01:58:27.613Z",
          "wordCount": 761,
          "title": "WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training. (arXiv:2103.06561v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1\">Patrick Godau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>",
          "description": "Shortage of annotated data is one of the greatest bottlenecks in biomedical\nimage analysis. Meta learning studies how learning systems can increase in\nefficiency through experience and could thus evolve as an important concept to\novercome data sparsity. However, the core capability of meta learning-based\napproaches is the identification of similar previous tasks given a new task - a\nchallenge largely unexplored in the biomedical imaging domain. In this paper,\nwe address the problem of quantifying task similarity with a concept that we\nrefer to as task fingerprinting. The concept involves converting a given task,\nrepresented by imaging data and corresponding labels, to a fixed-length vector\nrepresentation. In fingerprint space, different tasks can be directly compared\nirrespective of their data set sizes, types of labels or specific resolutions.\nAn initial feasibility study in the field of surgical data science (SDS) with\n26 classification tasks from various medical and non-medical domains suggests\nthat task fingerprinting could be leveraged for both (1) selecting appropriate\ndata sets for pretraining and (2) selecting appropriate architectures for a new\ntask. Task fingerprinting could thus become an important tool for meta learning\nin SDS and other fields of biomedical image analysis.",
          "link": "http://arxiv.org/abs/2107.03949",
          "publishedOn": "2021-07-09T01:58:27.588Z",
          "wordCount": 639,
          "title": "Task Fingerprinting for Meta Learning in Biomedical Image Analysis. (arXiv:2107.03949v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xuejing Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Ganning Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaitai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>",
          "description": "An explainable, efficient and lightweight method for texture generation,\ncalled TGHop (an acronym of Texture Generation PixelHop), is proposed in this\nwork. Although synthesis of visually pleasant texture can be achieved by deep\nneural networks, the associated models are large in size, difficult to explain\nin theory, and computationally expensive in training. In contrast, TGHop is\nsmall in its model size, mathematically transparent, efficient in training and\ninference, and able to generate high quality texture. Given an exemplary\ntexture, TGHop first crops many sample patches out of it to form a collection\nof sample patches called the source. Then, it analyzes pixel statistics of\nsamples from the source and obtains a sequence of fine-to-coarse subspaces for\nthese patches by using the PixelHop++ framework. To generate texture patches\nwith TGHop, we begin with the coarsest subspace, which is called the core, and\nattempt to generate samples in each subspace by following the distribution of\nreal samples. Finally, texture patches are stitched to form texture images of a\nlarge size. It is demonstrated by experimental results that TGHop can generate\ntexture images of superior quality with a small model size and at a fast speed.",
          "link": "http://arxiv.org/abs/2107.04020",
          "publishedOn": "2021-07-09T01:58:27.515Z",
          "wordCount": 644,
          "title": "TGHop: An Explainable, Efficient and Lightweight Method for Texture Generation. (arXiv:2107.04020v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>",
          "description": "We show that cascaded diffusion models are capable of generating high\nfidelity images on the class-conditional ImageNet generation challenge, without\nany assistance from auxiliary image classifiers to boost sample quality. A\ncascaded diffusion model comprises a pipeline of multiple diffusion models that\ngenerate images of increasing resolution, beginning with a standard diffusion\nmodel at the lowest resolution, followed by one or more super-resolution\ndiffusion models that successively upsample the image and add higher resolution\ndetails. We find that the sample quality of a cascading pipeline relies\ncrucially on conditioning augmentation, our proposed method of data\naugmentation of the lower resolution conditioning inputs to the\nsuper-resolution models. Our experiments show that conditioning augmentation\nprevents compounding error during sampling in a cascaded model, helping us to\ntrain cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at\n128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and\nclassification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256,\noutperforming VQ-VAE-2.",
          "link": "http://arxiv.org/abs/2106.15282",
          "publishedOn": "2021-07-09T01:58:27.118Z",
          "wordCount": 624,
          "title": "Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsai%2A_F/0/1/0/all/0/1\">Fu-Jen Tsai*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng%2A_Y/0/1/0/all/0/1\">Yan-Tsung Peng*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1\">Chung-Chi Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>",
          "description": "Image motion blur usually results from moving objects or camera shakes. Such\nblur is generally directional and non-uniform. Previous research efforts\nattempt to solve non-uniform blur by using self-recurrent multi-scale or\nmulti-patch architectures accompanying with self-attention. However, using\nself-recurrent frameworks typically leads to a longer inference time, while\ninter-pixel or inter-channel self-attention may cause excessive memory usage.\nThis paper proposes blur-aware attention networks (BANet) that accomplish\naccurate and efficient deblurring via a single forward pass. Our BANet utilizes\nregion-based self-attention with multi-kernel strip pooling to disentangle blur\npatterns of different degrees and with cascaded parallel dilated convolution to\naggregate multi-scale content features. Extensive experimental results on the\nGoPro and HIDE benchmarks demonstrate that the proposed BANet performs\nfavorably against the state-of-the-art in blurred image restoration and can\nprovide deblurred results in real-time.",
          "link": "http://arxiv.org/abs/2101.07518",
          "publishedOn": "2021-07-09T01:58:27.061Z",
          "wordCount": 597,
          "title": "BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring. (arXiv:2101.07518v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vacher_J/0/1/0/all/0/1\">Jonathan Vacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_C/0/1/0/all/0/1\">Claire Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coen_Cagli_R/0/1/0/all/0/1\">Ruben Coen-Cagli</a>",
          "description": "Probabilistic finite mixture models are widely used for unsupervised\nclustering. These models can often be improved by adapting them to the topology\nof the data. For instance, in order to classify spatially adjacent data points\nsimilarly, it is common to introduce a Laplacian constraint on the posterior\nprobability that each data point belongs to a class. Alternatively, the mixing\nprobabilities can be treated as free parameters, while assuming Gauss-Markov or\nmore complex priors to regularize those mixing probabilities. However, these\napproaches are constrained by the shape of the prior and often lead to\ncomplicated or intractable inference. Here, we propose a new parametrization of\nthe Dirichlet distribution to flexibly regularize the mixing probabilities of\nover-parametrized mixture distributions. Using the Expectation-Maximization\nalgorithm, we show that our approach allows us to define any linear update rule\nfor the mixing probabilities, including spatial smoothing regularization as a\nspecial case. We then show that this flexible design can be extended to share\nclass information between multiple mixture models. We apply our algorithm to\nartificial and natural image segmentation tasks, and we provide quantitative\nand qualitative comparison of the performance of Gaussian and Student-t\nmixtures on the Berkeley Segmentation Dataset. We also demonstrate how to\npropagate class information across the layers of deep convolutional neural\nnetworks in a probabilistically optimal way, suggesting a new interpretation\nfor feedback signals in biological visual systems. Our flexible approach can be\neasily generalized to adapt probabilistic mixture models to arbitrary data\ntopologies.",
          "link": "http://arxiv.org/abs/1905.10629",
          "publishedOn": "2021-07-09T01:58:26.998Z",
          "wordCount": 731,
          "title": "Flexibly Regularized Mixture Models and Application to Image Segmentation. (arXiv:1905.10629v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03442",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hamghalam_M/0/1/0/all/0/1\">Mohammad Hamghalam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F. Frangi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simpson_A/0/1/0/all/0/1\">Amber L. Simpson</a>",
          "description": "In large studies involving multi protocol Magnetic Resonance Imaging (MRI),\nit can occur to miss one or more sub-modalities for a given patient owing to\npoor quality (e.g. imaging artifacts), failed acquisitions, or hallway\ninterrupted imaging examinations. In some cases, certain protocols are\nunavailable due to limited scan time or to retrospectively harmonise the\nimaging protocols of two independent studies. Missing image modalities pose a\nchallenge to segmentation frameworks as complementary information contributed\nby the missing scans is then lost. In this paper, we propose a novel model,\nMulti-modal Gaussian Process Prior Variational Autoencoder (MGP-VAE), to impute\none or more missing sub-modalities for a patient scan. MGP-VAE can leverage the\nGaussian Process (GP) prior on the Variational Autoencoder (VAE) to utilize the\nsubjects/patients and sub-modalities correlations. Instead of designing one\nnetwork for each possible subset of present sub-modalities or using frameworks\nto mix feature maps, missing data can be generated from a single model based on\nall the available samples. We show the applicability of MGP-VAE on brain tumor\nsegmentation where either, two, or three of four sub-modalities may be missing.\nOur experiments against competitive segmentation baselines with missing\nsub-modality on BraTS'19 dataset indicate the effectiveness of the MGP-VAE\nmodel for segmentation tasks.",
          "link": "http://arxiv.org/abs/2107.03442",
          "publishedOn": "2021-07-09T01:58:26.878Z",
          "wordCount": 666,
          "title": "Modality Completion via Gaussian Process Prior Variational Autoencoders for Multi-Modal Glioma Segmentation. (arXiv:2107.03442v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.02243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haixu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhiyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>",
          "description": "This paper tackles video prediction from a new dimension of predicting\nspacetime-varying motions that are incessantly changing across both space and\ntime. Prior methods mainly capture the temporal state transitions but overlook\nthe complex spatiotemporal variations of the motion itself, making them\ndifficult to adapt to ever-changing motions. We observe that physical world\nmotions can be decomposed into transient variation and motion trend, while the\nlatter can be regarded as the accumulation of previous motions. Thus,\nsimultaneously capturing the transient variation and the motion trend is the\nkey to make spacetime-varying motions more predictable. Based on these\nobservations, we propose the MotionRNN framework, which can capture the complex\nvariations within motions and adapt to spacetime-varying scenarios. MotionRNN\nhas two main contributions. The first is that we design the MotionGRU unit,\nwhich can model the transient variation and motion trend in a unified way. The\nsecond is that we apply the MotionGRU to RNN-based predictive models and\nindicate a new flexible video prediction architecture with a Motion Highway\nthat can significantly improve the ability to predict changeable motions and\navoid motion vanishing for stacked multiple-layer predictive models. With high\nflexibility, this framework can adapt to a series of models for deterministic\nspatiotemporal prediction. Our MotionRNN can yield significant improvements on\nthree challenging benchmarks for video prediction with spacetime-varying\nmotions.",
          "link": "http://arxiv.org/abs/2103.02243",
          "publishedOn": "2021-07-09T01:58:26.854Z",
          "wordCount": 694,
          "title": "MotionRNN: A Flexible Model for Video Prediction with Spacetime-Varying Motions. (arXiv:2103.02243v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keipour_A/0/1/0/all/0/1\">Azarakhsh Keipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1\">Guilherme A. S. Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>",
          "description": "We propose a new algorithm for real-time detection and tracking of elliptic\npatterns suitable for real-world robotics applications. The method fits\nellipses to each contour in the image frame and rejects ellipses that do not\nyield a good fit. The resulting detection and tracking method is lightweight\nenough to be used on robots' resource-limited onboard computers, can deal with\nlighting variations and detect the pattern even when the view is partial. The\nmethod is tested on an example application of an autonomous UAV landing on a\nfast-moving vehicle to show its performance indoors, outdoors, and in\nsimulation on a real-world robotics task. The comparison with other well-known\nellipse detection methods shows that our proposed algorithm outperforms other\nmethods with the F1 score of 0.981 on a dataset with over 1500 frames. The\nvideos of experiments, the source codes, and the collected dataset are provided\nwith the paper at https://theairlab.org/landing-on-vehicle .",
          "link": "http://arxiv.org/abs/2102.12670",
          "publishedOn": "2021-07-09T01:58:26.846Z",
          "wordCount": 623,
          "title": "Real-Time Ellipse Detection for Robotics Applications. (arXiv:2102.12670v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10553",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Luo_L/0/1/0/all/0/1\">Luyang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Y/0/1/0/all/0/1\">Yongjie Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanning Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vardhanabhuti_V/0/1/0/all/0/1\">Varut Vardhanabhuti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Mingxiang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Deep learning has demonstrated radiograph screening performances that are\ncomparable or superior to radiologists. However, recent studies show that deep\nmodels for thoracic disease classification usually show degraded performance\nwhen applied to external data. Such phenomena can be categorized into shortcut\nlearning, where the deep models learn unintended decision rules that can fit\nthe identically distributed training and test set but fail to generalize to\nother distributions. A natural way to alleviate this defect is explicitly\nindicating the lesions and focusing the model on learning the intended\nfeatures. In this paper, we conduct extensive retrospective experiments to\ncompare a popular thoracic disease classification model, CheXNet, and a\nthoracic lesion detection model, CheXDet. We first showed that the two models\nachieved similar image-level classification performance on the internal test\nset with no significant differences under many scenarios. Meanwhile, we found\nincorporating external training data even led to performance degradation for\nCheXNet. Then, we compared the models' internal performance on the lesion\nlocalization task and showed that CheXDet achieved significantly better\nperformance than CheXNet even when given 80% less training data. By further\nvisualizing the models' decision-making regions, we revealed that CheXNet\nlearned patterns other than the target lesions, demonstrating its shortcut\nlearning defect. Moreover, CheXDet achieved significantly better external\nperformance than CheXNet on both the image-level classification task and the\nlesion localization task. Our findings suggest improving annotation granularity\nfor training deep learning systems as a promising way to elevate future deep\nlearning-based diagnosis systems for clinical usage.",
          "link": "http://arxiv.org/abs/2104.10553",
          "publishedOn": "2021-07-09T01:58:26.823Z",
          "wordCount": 738,
          "title": "Rethinking annotation granularity for overcoming deep shortcut learning: A retrospective study on chest radiographs. (arXiv:2104.10553v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17171",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pohjonen_J/0/1/0/all/0/1\">Joona Pohjonen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sturenberg_C/0/1/0/all/0/1\">Carolin St&#xfc;renberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rannikko_A/0/1/0/all/0/1\">Antti Rannikko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mirtti_T/0/1/0/all/0/1\">Tuomas Mirtti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pitkanen_E/0/1/0/all/0/1\">Esa Pitk&#xe4;nen</a>",
          "description": "Many current neural networks for medical imaging generalise poorly to data\nunseen during training. Such behaviour can be caused by networks overfitting\neasy-to-learn, or statistically dominant, features while disregarding other\npotentially informative features. For example, indistinguishable differences in\nthe sharpness of the images from two different scanners can degrade the\nperformance of the network significantly. All neural networks intended for\nclinical practice need to be robust to variation in data caused by differences\nin imaging equipment, sample preparation and patient populations.\n\nTo address these challenges, we evaluate the utility of spectral decoupling\nas an implicit bias mitigation method. Spectral decoupling encourages the\nneural network to learn more features by simply regularising the networks'\nunnormalised prediction scores with an L2 penalty, thus having no added\ncomputational costs.\n\nWe show that spectral decoupling allows training neural networks on datasets\nwith strong spurious correlations. Networks trained without spectral decoupling\ndo not learn the original task and appear to make false predictions based on\nthe spurious correlations. Spectral decoupling also increases networks'\nrobustness for data distribution shifts. To validate our findings, we train\nnetworks with and without spectral decoupling to detect prostate cancer tissue\nslides and COVID-19 in chest radiographs. Networks trained with spectral\ndecoupling achieve substantially higher performance on all evaluation datasets.\n\nOur results show that spectral decoupling helps with generalisation issues\nassociated with neural networks. We recommend using spectral decoupling as an\nimplicit bias mitigation method in any neural network intended for clinical\nuse.",
          "link": "http://arxiv.org/abs/2103.17171",
          "publishedOn": "2021-07-09T01:58:26.798Z",
          "wordCount": 784,
          "title": "Spectral decoupling allows training transferable neural networks in medical imaging. (arXiv:2103.17171v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yu-Wei Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhen-Duo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin-Shun Xu</a>",
          "description": "With the rapid development of social websites, recent years have witnessed an\nexplosive growth of social images with user-provided tags which continuously\narrive in a streaming fashion. Due to the fast query speed and low storage\ncost, hashing-based methods for image search have attracted increasing\nattention. However, existing hashing methods for social image retrieval are\nbased on batch mode which violates the nature of social images, i.e., social\nimages are usually generated periodically or collected in a stream fashion.\nAlthough there exist many online image hashing methods, they either adopt\nunsupervised learning which ignore the relevant tags, or are designed in the\nsupervised manner which needs high-quality labels. In this paper, to overcome\nthe above limitations, we propose a new method named Weakly-supervised Online\nHashing (WOH). In order to learn high-quality hash codes, WOH exploits the weak\nsupervision by considering the semantics of tags and removing the noise.\nBesides, We develop a discrete online optimization algorithm for WOH, which is\nefficient and scalable. Extensive experiments conducted on two real-world\ndatasets demonstrate the superiority of WOH compared with several\nstate-of-the-art hashing baselines.",
          "link": "http://arxiv.org/abs/2009.07436",
          "publishedOn": "2021-07-09T01:58:26.790Z",
          "wordCount": 646,
          "title": "Weakly-Supervised Online Hashing. (arXiv:2009.07436v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fursa_I/0/1/0/all/0/1\">Ivan Fursa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fandi_E/0/1/0/all/0/1\">Elias Fandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_V/0/1/0/all/0/1\">Valentina Musat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culley_J/0/1/0/all/0/1\">Jacob Culley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gil_E/0/1/0/all/0/1\">Enric Gil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teeti_I/0/1/0/all/0/1\">Izzeddin Teeti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilous_L/0/1/0/all/0/1\">Louise Bilous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sluis_I/0/1/0/all/0/1\">Isaac Vander Sluis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rast_A/0/1/0/all/0/1\">Alexander Rast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1\">Andrew Bradley</a>",
          "description": "Autonomous vehicles rely heavily upon their perception subsystems to see the\nenvironment in which they operate. Unfortunately, the effect of variable\nweather conditions presents a significant challenge to object detection\nalgorithms, and thus it is imperative to test the vehicle extensively in all\nconditions which it may experience. However, development of robust autonomous\nvehicle subsystems requires repeatable, controlled testing - while real weather\nis unpredictable and cannot be scheduled. Real-world testing in adverse\nconditions is an expensive and time-consuming task, often requiring access to\nspecialist facilities. Simulation is commonly relied upon as a substitute, with\nincreasingly visually realistic representations of the real-world being\ndeveloped. In the context of the complete autonomous vehicle control pipeline,\nsubsystems downstream of perception need to be tested with accurate recreations\nof the perception system output, rather than focusing on subjective visual\nrealism of the input - whether in simulation or the real world. This study\ndevelops the untapped potential of a lightweight weather augmentation method in\nan autonomous racing vehicle - focusing not on visual accuracy, but rather the\neffect upon perception subsystem performance in real time. With minimal\nadjustment, the prototype developed in this study can replicate the effects of\nwater droplets on the camera lens, and fading light conditions. This approach\nintroduces a latency of less than 8 ms using compute hardware well suited to\nbeing carried in the vehicle - rendering it ideal for real-time implementation\nthat can be run during experiments in simulation, and augmented reality testing\nin the real world.",
          "link": "http://arxiv.org/abs/2103.02760",
          "publishedOn": "2021-07-09T01:58:26.770Z",
          "wordCount": 752,
          "title": "Worsening Perception: Real-time Degradation of Autonomous Vehicle Perception Performance for Simulation of Adverse Weather Conditions. (arXiv:2103.02760v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ancha_S/0/1/0/all/0/1\">Siddharth Ancha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_G/0/1/0/all/0/1\">Gaurav Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_S/0/1/0/all/0/1\">Srinivasa G. Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>",
          "description": "To safely navigate unknown environments, robots must accurately perceive\ndynamic obstacles. Instead of directly measuring the scene depth with a LiDAR\nsensor, we explore the use of a much cheaper and higher resolution sensor:\nprogrammable light curtains. Light curtains are controllable depth sensors that\nsense only along a surface that a user selects. We use light curtains to\nestimate the safety envelope of a scene: a hypothetical surface that separates\nthe robot from all obstacles. We show that generating light curtains that sense\nrandom locations (from a particular distribution) can quickly discover the\nsafety envelope for scenes with unknown objects. Importantly, we produce\ntheoretical safety guarantees on the probability of detecting an obstacle using\nrandom curtains. We combine random curtains with a machine learning based model\nthat forecasts and tracks the motion of the safety envelope efficiently. Our\nmethod accurately estimates safety envelopes while providing probabilistic\nsafety guarantees that can be used to certify the efficacy of a robot\nperception system to detect and avoid dynamic obstacles. We evaluate our\napproach in a simulated urban driving environment and a real-world environment\nwith moving pedestrians using a light curtain device and show that we can\nestimate safety envelopes efficiently and effectively. Project website:\nhttps://siddancha.github.io/projects/active-safety-envelopes-with-guarantees",
          "link": "http://arxiv.org/abs/2107.04000",
          "publishedOn": "2021-07-09T01:58:26.762Z",
          "wordCount": 665,
          "title": "Active Safety Envelopes using Light Curtains with Probabilistic Guarantees. (arXiv:2107.04000v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingyun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yuanfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>",
          "description": "Precise localization of polyp is crucial for early cancer screening in\ngastrointestinal endoscopy. Videos given by endoscopy bring both richer\ncontextual information as well as more challenges than still images. The\ncamera-moving situation, instead of the common camera-fixed-object-moving one,\nleads to significant background variation between frames. Severe internal\nartifacts (e.g. water flow in the human body, specular reflection by tissues)\ncan make the quality of adjacent frames vary considerately. These factors\nhinder a video-based model to effectively aggregate features from neighborhood\nframes and give better predictions. In this paper, we present Spatial-Temporal\nFeature Transformation (STFT), a multi-frame collaborative framework to address\nthese issues. Spatially, STFT mitigates inter-frame variations in the\ncamera-moving situation with feature alignment by proposal-guided deformable\nconvolutions. Temporally, STFT proposes a channel-aware attention module to\nsimultaneously estimate the quality and correlation of adjacent frames for\nadaptive feature aggregation. Empirical studies and superior results\ndemonstrate the effectiveness and stability of our method. For example, STFT\nimproves the still image baseline FCOS by 10.6% and 20.6% on the comprehensive\nF1-score of the polyp localization task in CVC-Clinic and ASUMayo datasets,\nrespectively, and outperforms the state-of-the-art video-based method by 3.6%\nand 8.0%, respectively. Code is available at\n\\url{https://github.com/lingyunwu14/STFT}.",
          "link": "http://arxiv.org/abs/2107.03609",
          "publishedOn": "2021-07-09T01:58:26.755Z",
          "wordCount": 648,
          "title": "Multi-frame Collaboration for Effective Endoscopic Video Polyp Detection via Spatial-Temporal Feature Transformation. (arXiv:2107.03609v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03648",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Temburwar_S/0/1/0/all/0/1\">Shrikant Temburwar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajesh_B/0/1/0/all/0/1\">Bulla Rajesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Javed_M/0/1/0/all/0/1\">Mohammed Javed</a>",
          "description": "Content-based image retrieval (CBIR) systems on pixel domain use low-level\nfeatures, such as colour, texture and shape, to retrieve images. In this\ncontext, two types of image representations i.e. local and global image\nfeatures have been studied in the literature. Extracting these features from\npixel images and comparing them with images from the database is very\ntime-consuming. Therefore, in recent years, there has been some effort to\naccomplish image analysis directly in the compressed domain with lesser\ncomputations. Furthermore, most of the images in our daily transactions are\nstored in the JPEG compressed format. Therefore, it would be ideal if we could\nretrieve features directly from the partially decoded or compressed data and\nuse them for retrieval. Here, we propose a unified model for image retrieval\nwhich takes DCT coefficients as input and efficiently extracts global and local\nfeatures directly in the JPEG compressed domain for accurate image retrieval.\nThe experimental findings indicate that our proposed model performed similarly\nto the current DELG model which takes RGB features as an input with reference\nto mean average precision while having a faster training and retrieval speed.",
          "link": "http://arxiv.org/abs/2107.03648",
          "publishedOn": "2021-07-09T01:58:26.748Z",
          "wordCount": 636,
          "title": "Deep Learning Based Image Retrieval in the JPEG Compressed Domain. (arXiv:2107.03648v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huazhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "We propose to address quadrupedal locomotion tasks using Reinforcement\nLearning (RL) with a Transformer-based model that learns to combine\nproprioceptive information and high-dimensional depth sensor inputs. While\nlearning-based locomotion has made great advances using RL, most methods still\nrely on domain randomization for training blind agents that generalize to\nchallenging terrains. Our key insight is that proprioceptive states only offer\ncontact measurements for immediate reaction, whereas an agent equipped with\nvisual sensory observations can learn to proactively maneuver environments with\nobstacles and uneven terrain by anticipating changes in the environment many\nsteps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL\nmethod for quadrupedal locomotion that leverages a Transformer-based model for\nfusing proprioceptive states and visual observations. We evaluate our method in\nchallenging simulated environments with different obstacles and uneven terrain.\nWe show that our method obtains significant improvements over policies with\nonly proprioceptive state inputs, and that Transformer-based models further\nimprove generalization across environments. Our project page with videos is at\nhttps://RchalYang.github.io/LocoTransformer .",
          "link": "http://arxiv.org/abs/2107.03996",
          "publishedOn": "2021-07-09T01:58:26.708Z",
          "wordCount": 619,
          "title": "Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers. (arXiv:2107.03996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Regatti_J/0/1/0/all/0/1\">Jayanth Reddy Regatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1\">Aniket Anand Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manavoglu_E/0/1/0/all/0/1\">Eren Manavoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogan_U/0/1/0/all/0/1\">Urun Dogan</a>",
          "description": "Recent advances in deep clustering and unsupervised representation learning\nare based on the idea that different views of an input image (generated through\ndata augmentation techniques) must either be closer in the representation\nspace, or have a similar cluster assignment. Bootstrap Your Own Latent (BYOL)\nis one such representation learning algorithm that has achieved\nstate-of-the-art results in self-supervised image classification on ImageNet\nunder the linear evaluation protocol. However, the utility of the learnt\nfeatures of BYOL to perform clustering is not explored. In this work, we study\nthe clustering ability of BYOL and observe that features learnt using BYOL may\nnot be optimal for clustering. We propose a novel consensus clustering based\nloss function, and train BYOL with the proposed loss in an end-to-end way that\nimproves the clustering ability and outperforms similar clustering based\nmethods on some popular computer vision datasets.",
          "link": "http://arxiv.org/abs/2010.01245",
          "publishedOn": "2021-07-09T01:58:26.540Z",
          "wordCount": 623,
          "title": "Consensus Clustering With Unsupervised Representation Learning. (arXiv:2010.01245v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dezhao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Bo Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>",
          "description": "Most of the existing video self-supervised methods mainly leverage temporal\nsignals of videos, ignoring that the semantics of moving objects and\nenvironmental information are all critical for video-related tasks. In this\npaper, we propose a novel self-supervised method for video representation\nlearning, referred to as Video 3D Sampling (V3S). In order to sufficiently\nutilize the information (spatial and temporal) provided in videos, we\npre-process a video from three dimensions (width, height, time). As a result,\nwe can leverage the spatial information (the size of objects), temporal\ninformation (the direction and magnitude of motions) as our learning target. In\nour implementation, we combine the sampling of the three dimensions and propose\nthe scale and projection transformations in space and time respectively. The\nexperimental results show that, when applied to action recognition, video\nretrieval and action similarity labeling, our approach improves the\nstate-of-the-arts with significant margins.",
          "link": "http://arxiv.org/abs/2107.03578",
          "publishedOn": "2021-07-09T01:58:26.456Z",
          "wordCount": 587,
          "title": "Video 3D Sampling for Self-supervised Representation Learning. (arXiv:2107.03578v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lucas_L/0/1/0/all/0/1\">Luis Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomas_D/0/1/0/all/0/1\">David Tomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Rodriguez_J/0/1/0/all/0/1\">Jose Garcia-Rodriguez</a>",
          "description": "One of the main issues related to unsupervised machine learning is the cost\nof processing and extracting useful information from large datasets. In this\nwork, we propose a classifier ensemble based on the transferable learning\ncapabilities of the CLIP neural network architecture in multimodal environments\n(image and text) from social media. For this purpose, we used the InstaNY100K\ndataset and proposed a validation approach based on sampling techniques. Our\nexperiments, based on image classification tasks according to the labels of the\nPlaces dataset, are performed by first considering only the visual part, and\nthen adding the associated texts as support. The results obtained demonstrated\nthat trained neural networks such as CLIP can be successfully applied to image\nclassification with little fine-tuning, and considering the associated texts to\nthe images can help to improve the accuracy depending on the goal. The results\ndemonstrated what seems to be a promising research direction.",
          "link": "http://arxiv.org/abs/2107.03751",
          "publishedOn": "2021-07-09T01:58:26.406Z",
          "wordCount": 608,
          "title": "Exploiting the relationship between visual and textual features in social networks for image classification with zero-shot deep learning. (arXiv:2107.03751v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hong-Xia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">I-Hsuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_L/0/1/0/all/0/1\">Ling Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1\">Hong-Han Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wen-Huang Cheng</a>",
          "description": "In this work, we describe our method for tackling the valence-arousal\nestimation challenge from ABAW2 ICCV-2021 Competition. The competition\norganizers provide an in-the-wild Aff-Wild2 dataset for participants to analyze\naffective behavior in real-life settings. We use a two stream model to learn\nemotion features from appearance and action respectively. To solve data\nimbalanced problem, we apply label distribution smoothing (LDS) to re-weight\nlabels. Our proposed method achieves Concordance Correlation Coefficient (CCC)\nof 0.591 and 0.617 for valence and arousal on the validation set of Aff-wild2\ndataset.",
          "link": "http://arxiv.org/abs/2107.03891",
          "publishedOn": "2021-07-09T01:58:26.398Z",
          "wordCount": 533,
          "title": "Technical Report for Valence-Arousal Estimation in ABAW2 Challenge. (arXiv:2107.03891v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_N/0/1/0/all/0/1\">Noriaki Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takagi_Y/0/1/0/all/0/1\">Yusuke Takagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masuda_H/0/1/0/all/0/1\">Hiroki Masuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyoshi_H/0/1/0/all/0/1\">Hiroaki Miyoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohno_K/0/1/0/all/0/1\">Kei Kohno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagaishi_M/0/1/0/all/0/1\">Miharu Nagaishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1\">Kensaku Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_M/0/1/0/all/0/1\">Mai Takeuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_T/0/1/0/all/0/1\">Takuya Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawamoto_K/0/1/0/all/0/1\">Keisuke Kawamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_K/0/1/0/all/0/1\">Kyohei Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moritsubo_M/0/1/0/all/0/1\">Mayuko Moritsubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1\">Kanako Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimasaki_Y/0/1/0/all/0/1\">Yasumasa Shimasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogura_Y/0/1/0/all/0/1\">Yusuke Ogura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imamoto_T/0/1/0/all/0/1\">Teppei Imamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishina_T/0/1/0/all/0/1\">Tatsuzo Mishina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohshima_K/0/1/0/all/0/1\">Koichi Ohshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hontani_H/0/1/0/all/0/1\">Hidekata Hontani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_I/0/1/0/all/0/1\">Ichiro Takeuchi</a>",
          "description": "In the present study, we propose a novel case-based similar image retrieval\n(SIR) method for hematoxylin and eosin (H&E)-stained histopathological images\nof malignant lymphoma. When a whole slide image (WSI) is used as an input\nquery, it is desirable to be able to retrieve similar cases by focusing on\nimage patches in pathologically important regions such as tumor cells. To\naddress this problem, we employ attention-based multiple instance learning,\nwhich enables us to focus on tumor-specific regions when the similarity between\ncases is computed. Moreover, we employ contrastive distance metric learning to\nincorporate immunohistochemical (IHC) staining patterns as useful supervised\ninformation for defining appropriate similarity between heterogeneous malignant\nlymphoma cases. In the experiment with 249 malignant lymphoma patients, we\nconfirmed that the proposed method exhibited higher evaluation measures than\nthe baseline case-based SIR methods. Furthermore, the subjective evaluation by\npathologists revealed that our similarity measure using IHC staining patterns\nis appropriate for representing the similarity of H&E-stained tissue images for\nmalignant lymphoma.",
          "link": "http://arxiv.org/abs/2107.03602",
          "publishedOn": "2021-07-09T01:58:26.367Z",
          "wordCount": 652,
          "title": "Case-based similar image retrieval for weakly annotated large histopathological images of malignant lymphoma using deep metric learning. (arXiv:2107.03602v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sibendu Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Kunal Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coviello_G/0/1/0/all/0/1\">Giuseppe Coviello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaradas_M/0/1/0/all/0/1\">Murugan Sankaradas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_O/0/1/0/all/0/1\">Oliver Po</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Y. Charlie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1\">Srimat T. Chakradhar</a>",
          "description": "Complex sensors like video cameras include tens of configurable parameters,\nwhich can be set by end-users to customize the sensors to specific application\nscenarios. Although parameter settings significantly affect the quality of the\nsensor output and the accuracy of insights derived from sensor data, most\nend-users use a fixed parameter setting because they lack the skill or\nunderstanding to appropriately configure these parameters. We propose CamTuner,\nwhich is a system to automatically, and dynamically adapt the complex sensor to\nchanging environments. CamTuner includes two key components. First, a bespoke\nanalytics quality estimator, which is a deep-learning model to automatically\nand continuously estimate the quality of insights from an analytics unit as the\nenvironment around a sensor change. Second, a reinforcement learning (RL)\nmodule, which reacts to the changes in quality, and automatically adjusts the\ncamera parameters to enhance the accuracy of insights. We improve the training\ntime of the RL module by an order of magnitude by designing virtual models to\nmimic essential behavior of the camera: we design virtual knobs that can be set\nto different values to mimic the effects of assigning different values to the\ncamera's configurable parameters, and we design a virtual camera model that\nmimics the output from a video camera at different times of the day. These\nvirtual models significantly accelerate training because (a) frame rates from a\nreal camera are limited to 25-30 fps while the virtual models enable processing\nat 300 fps, (b) we do not have to wait until the real camera sees different\nenvironments, which could take weeks or months, and (c) virtual knobs can be\nupdated instantly, while it can take 200-500 ms to change the camera parameter\nsettings. Our dynamic tuning approach results in up to 12% improvement in the\naccuracy of insights from several video analytics tasks.",
          "link": "http://arxiv.org/abs/2107.03964",
          "publishedOn": "2021-07-09T01:58:26.350Z",
          "wordCount": 751,
          "title": "CamTuner: Reinforcement-Learning based System for Camera Parameter Tuning to enhance Analytics. (arXiv:2107.03964v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1906.09744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_K/0/1/0/all/0/1\">Kai Ming Ting</a>",
          "description": "This paper presents a new insight into improving the performance of\nStochastic Neighbour Embedding (t-SNE) by using Isolation kernel instead of\nGaussian kernel. Isolation kernel outperforms Gaussian kernel in two aspects.\nFirst, the use of Isolation kernel in t-SNE overcomes the drawback of\nmisrepresenting some structures in the data, which often occurs when Gaussian\nkernel is applied in t-SNE. This is because Gaussian kernel determines each\nlocal bandwidth based on one local point only, while Isolation kernel is\nderived directly from the data based on space partitioning. Second, the use of\nIsolation kernel yields a more efficient similarity computation because\ndata-dependent Isolation kernel has only one parameter that needs to be tuned.\nIn contrast, the use of data-independent Gaussian kernel increases the\ncomputational cost by determining n bandwidths for a dataset of n points. As\nthe root cause of these deficiencies in t-SNE is Gaussian kernel, we show that\nsimply replacing Gaussian kernel with Isolation kernel in t-SNE significantly\nimproves the quality of the final visualisation output (without creating\nmisrepresented structures) and removes one key obstacle that prevents t-SNE\nfrom processing large datasets. Moreover, Isolation kernel enables t-SNE to\ndeal with large-scale datasets in less runtime without trading off accuracy,\nunlike existing methods in speeding up t-SNE.",
          "link": "http://arxiv.org/abs/1906.09744",
          "publishedOn": "2021-07-09T01:58:26.343Z",
          "wordCount": 696,
          "title": "Improving the Effectiveness and Efficiency of Stochastic Neighbour Embedding with Isolation Kernel. (arXiv:1906.09744v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.06519",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Platonova_G/0/1/0/all/0/1\">Ganna Platonova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stys_D/0/1/0/all/0/1\">Dalibor Stys</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soucek_P/0/1/0/all/0/1\">Pavel Soucek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lonhus_K/0/1/0/all/0/1\">Kirill Lonhus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valenta_J/0/1/0/all/0/1\">Jan Valenta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rychtarikova_R/0/1/0/all/0/1\">Renata Rychtarikova</a>",
          "description": "The most realistic information about the transparent sample such as a live\ncell can be obtained only using bright-field light microscopy. At\nhigh-intensity pulsing LED illumination, we captured a primary\n12-bit-per-channel (bpc) response from an observed sample using a bright-field\nmicroscope equipped with a high-resolution (4872x3248) image sensor. In order\nto suppress data distortions originating from the light interactions with\nelements in the optical path, poor sensor reproduction (geometrical defects of\nthe camera sensor and some peculiarities of sensor sensitivity), we propose a\nspectroscopic approach for the correction of this uncompressed 12-bpc data by\nsimultaneous calibration of all parts of the experimental arrangement.\nMoreover, the final intensities of the corrected images are proportional to the\nphoton fluxes detected by a camera sensor. It can be visualized in 8-bpc\nintensity depth after the Least Information Loss compression [Lect. Notes\nBioinform. 9656, 527 (2016)].",
          "link": "http://arxiv.org/abs/1903.06519",
          "publishedOn": "2021-07-09T01:58:26.336Z",
          "wordCount": 632,
          "title": "Spectroscopic Approach to Correction and Visualisation of Bright-Field Light Transmission Microscopy Biological Data. (arXiv:1903.06519v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03642",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_N/0/1/0/all/0/1\">Ningyuan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_J/0/1/0/all/0/1\">Jiayan Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jiangjian Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1\">Chengbin Peng</a>",
          "description": "PSNR and SSIM are the most widely used metrics in super-resolution problems,\nbecause they are easy to use and can evaluate the similarities between\ngenerated images and reference images. However, single image super-resolution\nis an ill-posed problem, there are multiple corresponding high-resolution\nimages for the same low-resolution image. The similarities can't totally\nreflect the restoration effect. The perceptual quality of generated images is\nalso important, but PSNR and SSIM do not reflect perceptual quality well. To\nsolve the problem, we proposed a method called regional differential\ninformation entropy to measure both of the similarities and perceptual quality.\nTo overcome the problem that traditional image information entropy can't\nreflect the structure information, we proposed to measure every region's\ninformation entropy with sliding window. Considering that the human visual\nsystem is more sensitive to the brightness difference at low brightness, we\ntake $\\gamma$ quantization rather than linear quantization. To accelerate the\nmethod, we reorganized the calculation procedure of information entropy with a\nneural network. Through experiments on our IQA dataset and PIPAL, this paper\nproves that RDIE can better quantify perceptual quality of images especially\nGAN-based images.",
          "link": "http://arxiv.org/abs/2107.03642",
          "publishedOn": "2021-07-09T01:58:26.316Z",
          "wordCount": 643,
          "title": "Regional Differential Information Entropy for Super-Resolution Image Quality Assessment. (arXiv:2107.03642v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Longyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sham_C/0/1/0/all/0/1\">Chiu-Wing Sham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1\">Chun Yan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xinchao Zhong</a>",
          "description": "Extracting and analyzing iris textures for biometric recognition has been\nextensively studied. As the transition of iris recognition from lab technology\nto nation-scale applications, most systems are facing high complexity in either\ntime or space, leading to unfitness for embedded devices. In this paper, the\nproposed design includes a minimal set of computer vision modules and\nmulti-mode QC-LDPC decoder which can alleviate variability and noise caused by\niris acquisition and follow-up process. Several classes of QC-LDPC code from\nIEEE 802.16 are tested for the validity of accuracy improvement. Some of the\ncodes mentioned above are used for further QC-LDPC decoder quantization,\nvalidation and comparison to each other. We show that we can apply Dynamic\nPartial Reconfiguration technology to implement the multi-mode QC-LDPC decoder\nfor the iris recognition system. The results show that the implementation is\npower-efficient and good for edge applications.",
          "link": "http://arxiv.org/abs/2107.03688",
          "publishedOn": "2021-07-09T01:58:26.305Z",
          "wordCount": 590,
          "title": "An Embedded Iris Recognition System Optimization using Dynamically ReconfigurableDecoder with LDPC Codes. (arXiv:2107.03688v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vakhitov_A/0/1/0/all/0/1\">Alexander Vakhitov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colomina_L/0/1/0/all/0/1\">Luis Ferraz Colomina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1\">Antonio Agudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>",
          "description": "Perspective-n-Point-and-Line (P$n$PL) algorithms aim at fast, accurate, and\nrobust camera localization with respect to a 3D model from 2D-3D feature\ncorrespondences, being a major part of modern robotic and AR/VR systems.\nCurrent point-based pose estimation methods use only 2D feature detection\nuncertainties, and the line-based methods do not take uncertainties into\naccount. In our setup, both 3D coordinates and 2D projections of the features\nare considered uncertain. We propose PnP(L) solvers based on EPnP and DLS for\nthe uncertainty-aware pose estimation. We also modify motion-only bundle\nadjustment to take 3D uncertainties into account. We perform exhaustive\nsynthetic and real experiments on two different visual odometry datasets. The\nnew PnP(L) methods outperform the state-of-the-art on real data in isolation,\nshowing an increase in mean translation accuracy by 18% on a representative\nsubset of KITTI, while the new uncertain refinement improves pose accuracy for\nmost of the solvers, e.g. decreasing mean translation error for the EPnP by 16%\ncompared to the standard refinement on the same dataset. The code is available\nat https://alexandervakhitov.github.io/uncertain-pnp/.",
          "link": "http://arxiv.org/abs/2107.03890",
          "publishedOn": "2021-07-09T01:58:26.298Z",
          "wordCount": 613,
          "title": "Uncertainty-Aware Camera Pose Estimation from Points and Lines. (arXiv:2107.03890v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jetchev_N/0/1/0/all/0/1\">Nikolay Jetchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_G/0/1/0/all/0/1\">G&#xf6;khan Yildirim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bracher_C/0/1/0/all/0/1\">Christian Bracher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollgraf_R/0/1/0/all/0/1\">Roland Vollgraf</a>",
          "description": "Attention is a general reasoning mechanism than can flexibly deal with image\ninformation, but its memory requirements had made it so far impractical for\nhigh resolution image generation. We present Grid Partitioned Attention (GPA),\na new approximate attention algorithm that leverages a sparse inductive bias\nfor higher computational and memory efficiency in image domains: queries attend\nonly to few keys, spatially close queries attend to close keys due to\ncorrelations. Our paper introduces the new attention layer, analyzes its\ncomplexity and how the trade-off between memory usage and model power can be\ntuned by the hyper-parameters.We will show how such attention enables novel\ndeep learning architectures with copying modules that are especially useful for\nconditional image generation tasks like pose morphing. Our contributions are\n(i) algorithm and code1of the novel GPA layer, (ii) a novel deep\nattention-copying architecture, and (iii) new state-of-the art experimental\nresults in human pose morphing generation benchmarks.",
          "link": "http://arxiv.org/abs/2107.03742",
          "publishedOn": "2021-07-09T01:58:26.290Z",
          "wordCount": 607,
          "title": "Grid Partitioned Attention: Efficient TransformerApproximation with Inductive Bias for High Resolution Detail Generation. (arXiv:2107.03742v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benkner_M/0/1/0/all/0/1\">Marcel Seelbach Benkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>",
          "description": "Matching problems on 3D shapes and images are challenging as they are\nfrequently formulated as combinatorial quadratic assignment problems (QAPs)\nwith permutation matrix constraints, which are NP-hard. In this work, we\naddress such problems with emerging quantum computing technology and propose\nseveral reformulations of QAPs as unconstrained problems suitable for efficient\nexecution on quantum hardware. We investigate several ways to inject\npermutation matrix constraints in a quadratic unconstrained binary optimization\nproblem which can be mapped to quantum hardware. We focus on obtaining a\nsufficient spectral gap, which further increases the probability to measure\noptimal solutions and valid permutation matrices in a single run. We perform\nour experiments on the quantum computer D-Wave 2000Q (2^11 qubits, adiabatic).\nDespite the observed discrepancy between simulated adiabatic quantum computing\nand execution on real quantum hardware, our reformulation of permutation matrix\nconstraints increases the robustness of the numerical computations over other\npenalty approaches in our experiments. The proposed algorithm has the potential\nto scale to higher dimensions on future quantum computing architectures, which\nopens up multiple new directions for solving matching problems in 3D computer\nvision and graphics.",
          "link": "http://arxiv.org/abs/2107.04032",
          "publishedOn": "2021-07-09T01:58:26.284Z",
          "wordCount": 638,
          "title": "Adiabatic Quantum Graph Matching with Permutation Matrix Constraints. (arXiv:2107.04032v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oishi_S/0/1/0/all/0/1\">Shuji Oishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koide_K/0/1/0/all/0/1\">Kenji Koide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokozuka_M/0/1/0/all/0/1\">Masashi Yokozuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banno_A/0/1/0/all/0/1\">Atsuhiko Banno</a>",
          "description": "This study presents a framework for capturing human attention in the\nspatio-temporal domain using eye-tracking glasses. Attention mapping is a key\ntechnology for human perceptual activity analysis or Human-Robot Interaction\n(HRI) to support human visual cognition; however, measuring human attention in\ndynamic environments is challenging owing to the difficulty in localizing the\nsubject and dealing with moving objects. To address this, we present a\ncomprehensive framework, 4D Attention, for unified gaze mapping onto static and\ndynamic objects. Specifically, we estimate the glasses pose by leveraging a\nloose coupling of direct visual localization and Inertial Measurement Unit\n(IMU) values. Further, by installing reconstruction components into our\nframework, dynamic objects not captured in the 3D environment map are\ninstantiated based on the input images. Finally, a scene rendering component\nsynthesizes a first-person view with identification (ID) textures and performs\ndirect 2D-3D gaze association. Quantitative evaluations showed the\neffectiveness of our framework. Additionally, we demonstrated the applications\nof 4D Attention through experiments in real situations.",
          "link": "http://arxiv.org/abs/2107.03606",
          "publishedOn": "2021-07-09T01:58:26.261Z",
          "wordCount": 597,
          "title": "4D Attention: Comprehensive Framework for Spatio-Temporal Gaze Mapping. (arXiv:2107.03606v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2005.08307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertugli_A/0/1/0/all/0/1\">Alessia Bertugli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1\">Simone Calderara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coscia_P/0/1/0/all/0/1\">Pasquale Coscia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Anticipating human motion in crowded scenarios is essential for developing\nintelligent transportation systems, social-aware robots and advanced video\nsurveillance applications. A key component of this task is represented by the\ninherently multi-modal nature of human paths which makes socially acceptable\nmultiple futures when human interactions are involved. To this end, we propose\na generative architecture for multi-future trajectory predictions based on\nConditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning\nmainly relies on prior belief maps, representing most likely moving directions\nand forcing the model to consider past observed dynamics in generating future\npositions. Human interactions are modeled with a graph-based attention\nmechanism enabling an online attentive hidden state refinement of the recurrent\nestimation. To corroborate our model, we perform extensive experiments on\npublicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS\nSportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its\neffectiveness in crowded scenes compared to several state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2005.08307",
          "publishedOn": "2021-07-09T01:58:26.255Z",
          "wordCount": 628,
          "title": "AC-VRNN: Attentive Conditional-VRNN for Multi-Future Trajectory Prediction. (arXiv:2005.08307v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1\">Martin Knoche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1\">Stefan H&#xf6;rmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>",
          "description": "Face recognition approaches often rely on equal image resolution for\nverification faces on two images. However, in practical applications, those\nimage resolutions are usually not in the same range due to different image\ncapture mechanisms or sources. In this work, we first analyze the impact of\nimage resolutions on the face verification performance with a state-of-the-art\nface recognition model. For images, synthetically reduced to $5\\, \\times 5\\,\n\\mathrm{px}$ resolution, the verification performance drops from $99.23\\%$\nincreasingly down to almost $55\\%$. Especially, for cross-resolution image\npairs (one high- and one low-resolution image), the verification accuracy\ndecreases even further. We investigate this behavior more in-depth by looking\nat the feature distances for every 2-image test pair. To tackle this problem,\nwe propose the following two methods: 1) Train a state-of-the-art\nface-recognition model straightforward with $50\\%$ low-resolution images\ndirectly within each batch. \\\\ 2) Train a siamese-network structure and adding\na cosine distance feature loss between high- and low-resolution features. Both\nmethods show an improvement for cross-resolution scenarios and can increase the\naccuracy at very low resolution to approximately $70\\%$. However, a\ndisadvantage is that a specific model needs to be trained for every\nresolution-pair ...",
          "link": "http://arxiv.org/abs/2107.03769",
          "publishedOn": "2021-07-09T01:58:26.248Z",
          "wordCount": 636,
          "title": "Image Resolution Susceptibility of Face Recognition Models. (arXiv:2107.03769v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03987",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1\">Dingjie Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yubo Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakravorti_S/0/1/0/all/0/1\">Srijata Chakravorti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noble_J/0/1/0/all/0/1\">Jack H. Noble</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dawant_B/0/1/0/all/0/1\">Be-noit M. Dawant</a>",
          "description": "We propose an atlas-based method to segment the intracochlear anatomy (ICA)\nin the post-implantation CT (Post-CT) images of cochlear implant (CI)\nrecipients that preserves the point-to-point correspondence between the meshes\nin the atlas and the segmented volumes. To solve this problem, which is\nchallenging because of the strong artifacts produced by the implant, we use a\npair of co-trained deep networks that generate dense deformation fields (DDFs)\nin opposite directions. One network is tasked with registering an atlas image\nto the Post-CT images and the other network is tasked with registering the\nPost-CT images to the atlas image. The networks are trained using loss\nfunctions based on voxel-wise labels, image content, fiducial registration\nerror, and cycle-consistency constraint. The segmentation of the ICA in the\nPost-CT images is subsequently obtained by transferring the predefined\nsegmentation meshes of the ICA in the atlas image to the Post-CT images using\nthe corresponding DDFs generated by the trained registration networks. Our\nmodel can learn the underlying geometric features of the ICA even though they\nare obscured by the metal artifacts. We show that our end-to-end network\nproduces results that are comparable to the current state of the art (SOTA)\nthat relies on a two-steps approach that first uses conditional generative\nadversarial networks to synthesize artifact-free images from the Post-CT images\nand then uses an active shape model-based method to segment the ICA in the\nsynthetic images. Our method requires a fraction of the time needed by the\nSOTA, which is important for end-user acceptance.",
          "link": "http://arxiv.org/abs/2107.03987",
          "publishedOn": "2021-07-09T01:58:26.241Z",
          "wordCount": 727,
          "title": "Atlas-Based Segmentation of Intracochlear Anatomy in Metal Artifact Affected CT Images of the Ear with Co-trained Deep Neural Networks. (arXiv:2107.03987v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03846",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aertsen_M/0/1/0/all/0/1\">Michael Aertsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Emam_D/0/1/0/all/0/1\">Doaa Emam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mufti_N/0/1/0/all/0/1\">Nada Mufti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guffens_F/0/1/0/all/0/1\">Frederic Guffens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_T/0/1/0/all/0/1\">Thomas Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demaerel_P/0/1/0/all/0/1\">Philippe Demaerel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Melbourne_A/0/1/0/all/0/1\">Andrew Melbourne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1\">Jam Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>",
          "description": "Deep neural networks have increased the accuracy of automatic segmentation,\nhowever, their accuracy depends on the availability of a large number of fully\nsegmented images. Methods to train deep neural networks using images for which\nsome, but not all, regions of interest are segmented are necessary to make\nbetter use of partially annotated datasets. In this paper, we propose the first\naxiomatic definition of label-set loss functions that are the loss functions\nthat can handle partially segmented images. We prove that there is one and only\none method to convert a classical loss function for fully segmented images into\na proper label-set loss function. Our theory also allows us to define the\nleaf-Dice loss, a label-set generalization of the Dice loss particularly suited\nfor partial supervision with only missing labels. Using the leaf-Dice loss, we\nset a new state of the art in partially supervised learning for fetal brain 3D\nMRI segmentation. We achieve a deep neural network able to segment white\nmatter, ventricles, cerebellum, extra-ventricular CSF, cortical gray matter,\ndeep gray matter, brainstem, and corpus callosum based on fetal brain 3D MRI of\nanatomically normal fetuses or with open spina bifida. Our implementation of\nthe proposed label-set loss functions is available at\nhttps://github.com/LucasFidon/label-set-loss-functions",
          "link": "http://arxiv.org/abs/2107.03846",
          "publishedOn": "2021-07-09T01:58:26.233Z",
          "wordCount": 687,
          "title": "Label-set Loss Functions for Partial Supervision: Application to Fetal Brain 3D MRI Parcellation. (arXiv:2107.03846v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>",
          "description": "Online continual learning aims to learn from a non-IID stream of data from a\nnumber of different tasks, where the learner is only allowed to consider data\nonce. Methods are typically allowed to use a limited buffer to store some of\nthe images in the stream. Recently, it was found that feature replay, where an\nintermediate layer representation of the image is stored (or generated) leads\nto superior results than image replay, while requiring less memory. Quantized\nexemplars can further reduce the memory usage. However, a drawback of these\nmethods is that they use a fixed (or very intransigent) backbone network. This\nsignificantly limits the learning of representations that can discriminate\nbetween all tasks. To address this problem, we propose an auxiliary classifier\nauto-encoder (ACAE) module for feature replay at intermediate layers with high\ncompression rates. The reduced memory footprint per image allows us to save\nmore exemplars for replay. In our experiments, we conduct task-agnostic\nevaluation under online continual learning setting and get state-of-the-art\nperformance on ImageNet-Subset, CIFAR100 and CIFAR10 dataset.",
          "link": "http://arxiv.org/abs/2105.08595",
          "publishedOn": "2021-07-09T01:58:26.213Z",
          "wordCount": 644,
          "title": "ACAE-REMIND for Online Continual Learning with Compressed Feature Replay. (arXiv:2105.08595v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1\">Yonghao Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>",
          "description": "Video-based human pose estimation (HPE) is a vital yet challenging task.\nWhile deep learning methods have made significant progress for the HPE, most\napproaches to this task detect each joint independently, damaging the pose\nstructural information. In this paper, unlike the prior methods, we propose a\nRelation-based Pose Semantics Transfer Network (RPSTN) to locate joints\nassociatively. Specifically, we design a lightweight joint relation extractor\n(JRE) to model the pose structural features and associatively generate heatmaps\nfor joints by modeling the relation between any two joints heuristically\ninstead of building each joint heatmap independently. Actually, the proposed\nJRE module models the spatial configuration of human poses through the\nrelationship between any two joints. Moreover, considering the temporal\nsemantic continuity of videos, the pose semantic information in the current\nframe is beneficial for guiding the location of joints in the next frame.\nTherefore, we use the idea of knowledge reuse to propagate the pose semantic\ninformation between consecutive frames. In this way, the proposed RPSTN\ncaptures temporal dynamics of poses. On the one hand, the JRE module can infer\ninvisible joints according to the relationship between the invisible joints and\nother visible joints in space. On the other hand, in the time, the propose\nmodel can transfer the pose semantic features from the non-occluded frame to\nthe occluded frame to locate occluded joints. Therefore, our method is robust\nto the occlusion and achieves state-of-the-art results on the two challenging\ndatasets, which demonstrates its effectiveness for video-based human pose\nestimation. We will release the code and models publicly.",
          "link": "http://arxiv.org/abs/2107.03591",
          "publishedOn": "2021-07-09T01:58:26.206Z",
          "wordCount": 691,
          "title": "Relation-Based Associative Joint Location for Human Pose Estimation in Videos. (arXiv:2107.03591v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1\">Yannis Panagakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1\">Jean Kossaifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1\">Grigorios G. Chrysos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oldfield_J/0/1/0/all/0/1\">James Oldfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolaou_M/0/1/0/all/0/1\">Mihalis A. Nicolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>",
          "description": "Tensors, or multidimensional arrays, are data structures that can naturally\nrepresent visual data of multiple dimensions. Inherently able to efficiently\ncapture structured, latent semantic spaces and high-order interactions, tensors\nhave a long history of applications in a wide span of computer vision problems.\nWith the advent of the deep learning paradigm shift in computer vision, tensors\nhave become even more fundamental. Indeed, essential ingredients in modern deep\nlearning architectures, such as convolutions and attention mechanisms, can\nreadily be considered as tensor mappings. In effect, tensor methods are\nincreasingly finding significant applications in deep learning, including the\ndesign of memory and compute efficient network architectures, improving\nrobustness to random noise and adversarial attacks, and aiding the theoretical\nunderstanding of deep networks.\n\nThis article provides an in-depth and practical review of tensors and tensor\nmethods in the context of representation learning and deep learning, with a\nparticular focus on visual data analysis and computer vision applications.\nConcretely, besides fundamental work in tensor-based visual data analysis\nmethods, we focus on recent developments that have brought on a gradual\nincrease of tensor methods, especially in deep learning architectures, and\ntheir implications in computer vision applications. To further enable the\nnewcomer to grasp such concepts quickly, we provide companion Python notebooks,\ncovering key aspects of the paper and implementing them, step-by-step with\nTensorLy.",
          "link": "http://arxiv.org/abs/2107.03436",
          "publishedOn": "2021-07-09T01:58:26.199Z",
          "wordCount": 670,
          "title": "Tensor Methods in Computer Vision and Deep Learning. (arXiv:2107.03436v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gangal_A/0/1/0/all/0/1\">Ayushe Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Peeyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumari_S/0/1/0/all/0/1\">Sunita Kumari</a>",
          "description": "In the following paper, we have combined the various basic functionalities\nprovided by the NumPy library and OpenCv library, which is an open source for\nComputer Vision applications, like conversion of colored images to grayscale,\ncalculating threshold, finding contours and using those contour points to take\nperspective transform of the image inputted by the user, using Python version\n3.7. Additional features include cropping, rotating and saving as well. All\nthese functions and features, when implemented step by step, results in a\ncomplete scanning application. The applied procedure involves the following\nsteps: Finding contours, applying Perspective transform and brightening the\nimage, Adaptive Thresholding and applying filters for noise cancellation, and\nRotation features and perspective transform for a special cropping algorithm.\nThe described technique is implemented on various samples.",
          "link": "http://arxiv.org/abs/2107.03700",
          "publishedOn": "2021-07-09T01:58:26.192Z",
          "wordCount": 562,
          "title": "Complete Scanning Application Using OpenCv. (arXiv:2107.03700v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>",
          "description": "Affective Analysis is not a single task, and the valence-arousal value,\nexpression class and action unit can be predicted at the same time. Previous\nresearches failed to take them as a whole task or ignore the entanglement and\nhierarchical relation of this three facial attributes. We propose a novel model\nnamed feature pyramid networks for multi-task affect analysis. The hierarchical\nfeatures are extracted to predict three labels and we apply teacher-student\ntraining strategy to learn from pretrained single-task models. Extensive\nexperiment results demonstrate the proposed model outperform other models. The\ncode and model are available for research purposes at\n$\\href{https://github.com/ryanhe312/ABAW2-FPNMAA}{\\text{this link}}$.",
          "link": "http://arxiv.org/abs/2107.03670",
          "publishedOn": "2021-07-09T01:58:26.184Z",
          "wordCount": 540,
          "title": "Feature Pyramid Network for Multi-task Affective Analysis. (arXiv:2107.03670v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Subhranil Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bathula_D/0/1/0/all/0/1\">Deepti R. Bathula</a>",
          "description": "Different categories of visual stimuli activate different responses in the\nhuman brain. These signals can be captured with EEG for utilization in\napplications such as Brain-Computer Interface (BCI). However, accurate\nclassification of single-trial data is challenging due to low signal-to-noise\nratio of EEG. This work introduces an EEG-ConvTranformer network that is based\non multi-headed self-attention. Unlike other transformers, the model\nincorporates self-attention to capture inter-region interactions. It further\nextends to adjunct convolutional filters with multi-head attention as a single\nmodule to learn temporal patterns. Experimental results demonstrate that\nEEG-ConvTransformer achieves improved classification accuracy over the\nstate-of-the-art techniques across five different visual stimuli classification\ntasks. Finally, quantitative analysis of inter-head diversity also shows low\nsimilarity in representational subspaces, emphasizing the implicit diversity of\nmulti-head attention.",
          "link": "http://arxiv.org/abs/2107.03983",
          "publishedOn": "2021-07-09T01:58:26.147Z",
          "wordCount": 569,
          "title": "EEG-ConvTransformer for Single-Trial EEG based Visual Stimuli Classification. (arXiv:2107.03983v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaoliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wulong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>",
          "description": "Shift neural networks reduce computation complexity by removing expensive\nmultiplication operations and quantizing continuous weights into low-bit\ndiscrete values, which are fast and energy efficient compared to conventional\nneural networks. However, existing shift networks are sensitive to the weight\ninitialization, and also yield a degraded performance caused by vanishing\ngradient and weight sign freezing problem. To address these issues, we propose\nS low-bit re-parameterization, a novel technique for training low-bit shift\nnetworks. Our method decomposes a discrete parameter in a sign-sparse-shift\n3-fold manner. In this way, it efficiently learns a low-bit network with a\nweight dynamics similar to full-precision networks and insensitive to weight\ninitialization. Our proposed training method pushes the boundaries of shift\nneural networks and shows 3-bit shift networks out-performs their\nfull-precision counterparts in terms of top-1 accuracy on ImageNet.",
          "link": "http://arxiv.org/abs/2107.03453",
          "publishedOn": "2021-07-09T01:58:26.141Z",
          "wordCount": 581,
          "title": "$S^3$: Sign-Sparse-Shift Reparametrization for Effective Training of Low-bit Shift Networks. (arXiv:2107.03453v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>",
          "description": "Many existing deep neural networks (DNNs) for 3D point cloud semantic\nsegmentation require a large amount of fully labeled training data. However,\nmanually assigning point-level labels on the complex scenes is time-consuming.\nWhile unlabeled point clouds can be easily obtained from sensors or\nreconstruction, we propose a superpoint constrained semi-supervised\nsegmentation network for 3D point clouds, named as SCSS-Net. Specifically, we\nuse the pseudo labels predicted from unlabeled point clouds for self-training,\nand the superpoints produced by geometry-based and color-based Region Growing\nalgorithms are combined to modify and delete pseudo labels with low confidence.\nAdditionally, we propose an edge prediction module to constrain the features\nfrom edge points of geometry and color. A superpoint feature aggregation module\nand superpoint feature consistency loss functions are introduced to smooth the\npoint features in each superpoint. Extensive experimental results on two 3D\npublic indoor datasets demonstrate that our method can achieve better\nperformance than some state-of-the-art point cloud segmentation networks and\nsome popular semi-supervised segmentation methods with few labeled scenes.",
          "link": "http://arxiv.org/abs/2107.03601",
          "publishedOn": "2021-07-09T01:58:26.133Z",
          "wordCount": 604,
          "title": "SCSS-Net: Superpoint Constrained Semi-supervised Segmentation Network for 3D Indoor Scenes. (arXiv:2107.03601v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antoniadis_P/0/1/0/all/0/1\">Panagiotis Antoniadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pikoulis_I/0/1/0/all/0/1\">Ioannis Pikoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filntisis_P/0/1/0/all/0/1\">Panagiotis P. Filntisis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1\">Petros Maragos</a>",
          "description": "In this work we tackle the task of video-based audio-visual emotion\nrecognition, within the premises of the 2nd Workshop and Competition on\nAffective Behavior Analysis in-the-wild (ABAW). Standard methodologies that\nrely solely on the extraction of facial features often fall short of accurate\nemotion prediction in cases where the aforementioned source of affective\ninformation is inaccessible due to head/body orientation, low resolution and\npoor illumination. We aspire to alleviate this problem by leveraging bodily as\nwell as contextual features, as part of a broader emotion recognition\nframework. A standard CNN-RNN cascade constitutes the backbone of our proposed\nmodel for sequence-to-sequence (seq2seq) learning. Apart from learning through\nthe \\textit{RGB} input modality, we construct an aural stream which operates on\nsequences of extracted mel-spectrograms. Our extensive experiments on the\nchallenging and newly assembled Affect-in-the-wild-2 (Aff-Wild2) dataset verify\nthe superiority of our methods over existing approaches, while by properly\nincorporating all of the aforementioned modules in a network ensemble, we\nmanage to surpass the previous best published recognition scores, in the\nofficial validation set. All the code was implemented using\nPyTorch\\footnote{\\url{https://pytorch.org/}} and is publicly\navailable\\footnote{\\url{https://github.com/PanosAntoniadis/NTUA-ABAW2021}}.",
          "link": "http://arxiv.org/abs/2107.03465",
          "publishedOn": "2021-07-09T01:58:26.124Z",
          "wordCount": 654,
          "title": "An audiovisual and contextual approach for categorical and continuous emotion recognition in-the-wild. (arXiv:2107.03465v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jian Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Houjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiqi Huang</a>",
          "description": "Pedestrian attribute recognition aims to assign multiple attributes to one\npedestrian image captured by a video surveillance camera. Although numerous\nmethods are proposed and make tremendous progress, we argue that it is time to\nstep back and analyze the status quo of the area. We review and rethink the\nrecent progress from three perspectives. First, given that there is no explicit\nand complete definition of pedestrian attribute recognition, we formally define\nand distinguish pedestrian attribute recognition from other similar tasks.\nSecond, based on the proposed definition, we expose the limitations of the\nexisting datasets, which violate the academic norm and are inconsistent with\nthe essential requirement of practical industry application. Thus, we propose\ntwo datasets, PETA\\textsubscript{$ZS$} and RAP\\textsubscript{$ZS$}, constructed\nfollowing the zero-shot settings on pedestrian identity. In addition, we also\nintroduce several realistic criteria for future pedestrian attribute dataset\nconstruction. Finally, we reimplement existing state-of-the-art methods and\nintroduce a strong baseline method to give reliable evaluations and fair\ncomparisons. Experiments are conducted on four existing datasets and two\nproposed datasets to measure progress on pedestrian attribute recognition.",
          "link": "http://arxiv.org/abs/2107.03576",
          "publishedOn": "2021-07-09T01:58:26.056Z",
          "wordCount": 635,
          "title": "Rethinking of Pedestrian Attribute Recognition: A Reliable Evaluation under Zero-Shot Pedestrian Identity Setting. (arXiv:2107.03576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03904",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liang_S/0/1/0/all/0/1\">Shuang Liang</a>",
          "description": "In this paper, we present a hybrid deep learning framework named CTNet which\ncombines convolutional neural network and transformer together for the\ndetection of COVID-19 via 3D chest CT images. It consists of a CNN feature\nextractor module with SE attention to extract sufficient features from CT\nscans, together with a transformer model to model the discriminative features\nof the 3D CT scans. Compared to previous works, CTNet provides an effective and\nefficient method to perform COVID-19 diagnosis via 3D CT scans with data\nresampling strategy. Advanced results on a large and public benchmarks,\nCOV19-CT-DB database was achieved by the proposed CTNet, over the\nstate-of-the-art baseline approachproposed together with the dataset.",
          "link": "http://arxiv.org/abs/2107.03904",
          "publishedOn": "2021-07-09T01:58:26.034Z",
          "wordCount": 611,
          "title": "A hybrid deep learning framework for Covid-19 detection via 3D Chest CT Images. (arXiv:2107.03904v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oh_G/0/1/0/all/0/1\">Geesung Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_E/0/1/0/all/0/1\">Euiseok Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sejoon Lim</a>",
          "description": "Among human affective behavior research, facial expression recognition\nresearch is improving in performance along with the development of deep\nlearning. However, for improved performance, not only past images but also\nfuture images should be used along with corresponding facial images, but there\nare obstacles to the application of this technique to real-time environments.\nIn this paper, we propose the causal affect prediction network (CAPNet), which\nuses only past facial images to predict corresponding affective valence and\narousal. We train CAPNet to learn causal inference between past images and\ncorresponding affective valence and arousal through supervised learning by\npairing the sequence of past images with the current label using the Aff-Wild2\ndataset. We show through experiments that the well-trained CAPNet outperforms\nthe baseline of the second challenge of the Affective Behavior Analysis\nin-the-wild (ABAW2) Competition by predicting affective valence and arousal\nonly with past facial images one-third of a second earlier. Therefore, in\nreal-time application, CAPNet can reliably predict affective valence and\narousal only with past data.",
          "link": "http://arxiv.org/abs/2107.03886",
          "publishedOn": "2021-07-09T01:58:26.022Z",
          "wordCount": 606,
          "title": "Causal affect prediction model using a facial image sequence. (arXiv:2107.03886v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ai_D/0/1/0/all/0/1\">Dige Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>",
          "description": "In practice, the problems encountered in Neural Architecture Search (NAS)\ntraining are not simple problems, but often a series of difficult combinations\n(wrong compensation estimation, curse of dimension, overfitting, high\ncomplexity, etc.). In this paper, we propose a framework to decouple network\nstructure from operator search space, and use two BOHBs to search\nalternatively. Considering that activation function and initialization are also\nimportant parts of neural network, the generalization ability of the model will\nbe affected. We introduce an activation function and an initialization method\ndomain, and add them into the operator search space to form a generalized\nsearch space, so as to improve the generalization ability of the child model.\nWe then trained a GCN-based predictor using feedback from the child model. This\ncan not only improve the search efficiency, but also solve the problem of\ndimension curse. Next, unlike other NAS studies, we used predictors to analyze\nthe stability of different network structures. Finally, we applied our\nframework to neural structure search and achieved significant improvements on\nmultiple datasets.",
          "link": "http://arxiv.org/abs/2103.11820",
          "publishedOn": "2021-07-09T01:58:25.953Z",
          "wordCount": 676,
          "title": "GPNAS: A Neural Network Architecture Search Framework Based on Graphical Predictor. (arXiv:2103.11820v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinhyung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1\">Xinshuo Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Man_Y/0/1/0/all/0/1\">Yunze Man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>",
          "description": "Point clouds and RGB images are naturally complementary modalities for 3D\nvisual understanding - the former provides sparse but accurate locations of\npoints on objects, while the latter contains dense color and texture\ninformation. Despite this potential for close sensor fusion, many methods train\ntwo models in isolation and use simple feature concatenation to represent 3D\nsensor data. This separated training scheme results in potentially sub-optimal\nperformance and prevents 3D tasks from being used to benefit 2D tasks that are\noften useful on their own. To provide a more integrated approach, we propose a\nnovel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box\nproposals to improve 2D segmentation predictions, which are then used to\nfurther refine the 3D boxes. We show that including a 2D network between two\nstages of 3D modules significantly improves both 2D and 3D task performance.\nMoreover, to prevent the 3D module from over-relying on the overfitted 2D\npredictions, we propose a dual-head 2D segmentation training and inference\nscheme, allowing the 2nd 3D module to learn to interpret imperfect 2D\nsegmentation predictions. Evaluating our model on the challenging SUN RGB-D\ndataset, we improve upon state-of-the-art results of both single modality and\nfusion networks by a large margin ($\\textbf{+3.8}$ mAP@0.5). Code will be\nreleased $\\href{https://github.com/Divadi/MTC_RCNN}{\\text{here.}}$",
          "link": "http://arxiv.org/abs/2107.04013",
          "publishedOn": "2021-07-09T01:58:25.882Z",
          "wordCount": 656,
          "title": "Multi-Modality Task Cascade for 3D Object Detection. (arXiv:2107.04013v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjio_G/0/1/0/all/0/1\">Gabriel Tjio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>",
          "description": "Convolutional neural networks may perform poorly when the test and train data\nare from different domains. While this problem can be mitigated by using the\ntarget domain data to align the source and target domain feature\nrepresentations, the target domain data may be unavailable due to privacy\nconcerns. Consequently, there is a need for methods that generalize well\nwithout access to target domain data during training. In this work, we propose\nan adversarial hallucination approach, which combines a class-wise\nhallucination module and a semantic segmentation module. Since the segmentation\nperformance varies across different classes, we design a semantic-conditioned\nstyle hallucination layer to adaptively stylize each class. The classwise\nstylization parameters are generated from the semantic knowledge in the\nsegmentation probability maps of the source domain image. Both modules compete\nadversarially, with the hallucination module generating increasingly\n'difficult' style images to challenge the segmentation module. In response, the\nsegmentation module improves its performance as it is trained with generated\nsamples at an appropriate class-wise difficulty level. Experiments on state of\nthe art domain adaptation work demonstrate the efficacy of our proposed method\nwhen no target domain data are available for training.",
          "link": "http://arxiv.org/abs/2106.04144",
          "publishedOn": "2021-07-09T01:58:25.857Z",
          "wordCount": 666,
          "title": "Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation. (arXiv:2106.04144v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1\">Seyed Mojtaba Marvasti-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaghani_J/0/1/0/all/0/1\">Javad Khaghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanei_Yakhdan_H/0/1/0/all/0/1\">Hossein Ghanei-Yakhdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1\">Shohreh Kasaei</a>",
          "description": "A strong visual object tracker nowadays relies on its well-crafted modules,\nwhich typically consist of manually-designed network architectures to deliver\nhigh-quality tracking results. Not surprisingly, the manual design process\nbecomes a particularly challenging barrier, as it demands sufficient prior\nexperience, enormous effort, intuition and perhaps some good luck. Meanwhile,\nneural architecture search has gaining grounds in practical applications such\nas image segmentation, as a promising method in tackling the issue of automated\nsearch of feasible network structures. In this work, we propose a novel\ncell-level differentiable architecture search mechanism to automate the network\ndesign of the tracking module, aiming to adapt backbone features to the\nobjective of a tracking network during offline training. The proposed approach\nis simple, efficient, and with no need to stack a series of modules to\nconstruct a network. Our approach is easy to be incorporated into existing\ntrackers, which is empirically validated using different differentiable\narchitecture search-based methods and tracking objectives. Extensive\nexperimental evaluations demonstrate the superior performance of our approach\nover five commonly-used benchmarks. Meanwhile, our automated searching process\ntakes 41 (18) hours for the second (first) order DARTS method on the\nTrackingNet dataset.",
          "link": "http://arxiv.org/abs/2107.03463",
          "publishedOn": "2021-07-09T01:58:25.837Z",
          "wordCount": 652,
          "title": "CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search. (arXiv:2107.03463v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03887",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Savioli_N/0/1/0/all/0/1\">Nicolo Savioli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ORegan_D/0/1/0/all/0/1\">Declan O&#x27;Regan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cook_S/0/1/0/all/0/1\">Stuart Cook</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>",
          "description": "In cardiac magnetic resonance (CMR) imaging, a 3D high-resolution\nsegmentation of the heart is essential for detailed description of its\nanatomical structures. However, due to the limit of acquisition duration and\nrespiratory/cardiac motion, stacks of multi-slice 2D images are acquired in\nclinical routine. The segmentation of these images provides a low-resolution\nrepresentation of cardiac anatomy, which may contain artefacts caused by\nmotion. Here we propose a novel latent optimisation framework that jointly\nperforms motion correction and super resolution for cardiac image\nsegmentations. Given a low-resolution segmentation as input, the framework\naccounts for inter-slice motion in cardiac MR imaging and super-resolves the\ninput into a high-resolution segmentation consistent with input. A multi-view\nloss is incorporated to leverage information from both short-axis view and\nlong-axis view of cardiac imaging. To solve the inverse problem, iterative\noptimisation is performed in a latent space, which ensures the anatomical\nplausibility. This alleviates the need of paired low-resolution and\nhigh-resolution images for supervised learning. Experiments on two cardiac MR\ndatasets show that the proposed framework achieves high performance, comparable\nto state-of-the-art super-resolution approaches and with better cross-domain\ngeneralisability and anatomical plausibility.",
          "link": "http://arxiv.org/abs/2107.03887",
          "publishedOn": "2021-07-09T01:58:25.761Z",
          "wordCount": 665,
          "title": "Joint Motion Correction and Super Resolution for Cardiac Segmentation via Latent Optimisation. (arXiv:2107.03887v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Du Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1\">Lorenzo Torresani</a>",
          "description": "Video transformers have recently emerged as a competitive alternative to 3D\nCNNs for video understanding. However, due to their large number of parameters\nand reduced inductive biases, these models require supervised pretraining on\nlarge-scale image datasets to achieve top performance. In this paper, we\nempirically demonstrate that self-supervised pretraining of video transformers\non video-only datasets can lead to action recognition results that are on par\nor better than those obtained with supervised pretraining on large-scale image\ndatasets, even massive ones such as ImageNet-21K. Since transformer-based\nmodels are effective at capturing dependencies over extended temporal spans, we\npropose a simple learning procedure that forces the model to match a long-term\nview to a short-term view of the same video. Our approach, named Long-Short\nTemporal Contrastive Learning (LSTCL), enables video transformers to learn an\neffective clip-level representation by predicting temporal context captured\nfrom a longer temporal extent. To demonstrate the generality of our findings,\nwe implement and validate our approach under three different self-supervised\ncontrastive learning frameworks (MoCo v3, BYOL, SimSiam) using two distinct\nvideo-transformer architectures, including an improved variant of the Swin\nTransformer augmented with space-time attention. We conduct a thorough ablation\nstudy and show that LSTCL achieves competitive performance on multiple video\nbenchmarks and represents a convincing alternative to supervised image-based\npretraining.",
          "link": "http://arxiv.org/abs/2106.09212",
          "publishedOn": "2021-07-09T01:58:25.751Z",
          "wordCount": 681,
          "title": "Long-Short Temporal Contrastive Learning of Video Transformers. (arXiv:2106.09212v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yikang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhao Zhong</a>",
          "description": "In this paper, we propose a Collaboration of Experts (CoE) framework to pool\ntogether the expertise of multiple networks towards a common aim. Each expert\nis an individual network with expertise on a unique portion of the dataset,\nwhich enhances the collective capacity. Given a sample, an expert is selected\nby the delegator, which simultaneously outputs a rough prediction to support\nearly termination. To fulfill this framework, we propose three modules to impel\neach model to play its role, namely weight generation module (WGM), label\ngeneration module (LGM) and variance calculation module (VCM). Our method\nachieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy\nwith 194M FLOPs. Combined with PWLU activation function and CondConv, CoE\nfurther achieves the accuracy of 80.0% with only 100M FLOPs for the first time.\nMore importantly, our method is hardware friendly and achieves a 3-6x speedup\ncompared with some existing conditional computation approaches.",
          "link": "http://arxiv.org/abs/2107.03815",
          "publishedOn": "2021-07-09T01:58:25.744Z",
          "wordCount": 594,
          "title": "Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs. (arXiv:2107.03815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zunhu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhimeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yu Ding</a>",
          "description": "Automatic affective recognition has been an important research topic in human\ncomputer interaction (HCI) area. With recent development of deep learning\ntechniques and large scale in-the-wild annotated datasets, the facial emotion\nanalysis is now aimed at challenges in the real world settings. In this paper,\nwe introduce our submission to the 2nd Affective Behavior Analysis in-the-wild\n(ABAW2) Competition. In dealing with different emotion representations,\nincluding Categorical Emotions (CE), Action Units (AU), and Valence Arousal\n(VA), we propose a multi-task streaming network by a heuristic that the three\nrepresentations are intrinsically associated with each other. Besides, we\nleverage an advanced facial expression embedding as prior knowledge, which is\ncapable of capturing identity-invariant expression features while preserving\nthe expression similarities, to aid the down-streaming recognition tasks. The\nextensive quantitative evaluations as well as ablation studies on the Aff-Wild2\ndataset prove the effectiveness of our proposed prior aided streaming network\napproach.",
          "link": "http://arxiv.org/abs/2107.03708",
          "publishedOn": "2021-07-09T01:58:25.717Z",
          "wordCount": 596,
          "title": "Prior Aided Streaming Network for Multi-task Affective Recognitionat the 2nd ABAW2 Competition. (arXiv:2107.03708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noh_B/0/1/0/all/0/1\">Byeongjoon Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_W/0/1/0/all/0/1\">Wonjun Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">David Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>",
          "description": "Pedestrians are exposed to risk of death or serious injuries on roads,\nespecially unsignalized crosswalks, for a variety of reasons. To date, an\nextensive variety of studies have reported on vision based traffic safety\nsystem. However, many studies required manual inspection of the volumes of\ntraffic video to reliably obtain traffic related objects behavioral factors. In\nthis paper, we propose an automated and simpler system for effectively\nextracting object behavioral features from video sensors deployed on the road.\nWe conduct basic statistical analysis on these features, and show how they can\nbe useful for monitoring the traffic behavior on the road. We confirm the\nfeasibility of the proposed system by applying our prototype to two\nunsignalized crosswalks in Osan city, South Korea. To conclude, we compare\nbehaviors of vehicles and pedestrians in those two areas by simple statistical\nanalysis. This study demonstrates the potential for a network of connected\nvideo sensors to provide actionable data for smart cities to improve pedestrian\nsafety in dangerous road environments.",
          "link": "http://arxiv.org/abs/2107.03554",
          "publishedOn": "2021-07-09T01:58:25.710Z",
          "wordCount": 620,
          "title": "Automated Object Behavioral Feature Extraction for Potential Risk Analysis based on Video Sensor. (arXiv:2107.03554v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asam_M/0/1/0/all/0/1\">Muhammad Asam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Saddam Hussain Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamal_T/0/1/0/all/0/1\">Tauseef Jamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahoora_U/0/1/0/all/0/1\">Umme Zahoora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asifullah Khan</a>",
          "description": "Malicious activities in cyberspace have gone further than simply hacking\nmachines and spreading viruses. It has become a challenge for a nations\nsurvival and hence has evolved to cyber warfare. Malware is a key component of\ncyber-crime, and its analysis is the first line of defence against attack. This\nwork proposes a novel deep boosted hybrid learning-based malware classification\nframework and named as Deep boosted Feature Space-based Malware classification\n(DFS-MC). In the proposed framework, the discrimination power is enhanced by\nfusing the feature spaces of the best performing customized CNN architectures\nmodels and its discrimination by an SVM for classification. The discrimination\ncapacity of the proposed classification framework is assessed by comparing it\nagainst the standard customized CNNs. The customized CNN models are implemented\nin two ways: softmax classifier and deep hybrid learning-based malware\nclassification. In the hybrid learning, Deep features are extracted from\ncustomized CNN architectures and fed into the conventional machine learning\nclassifier to improve the classification performance. We also introduced the\nconcept of transfer learning in a customized CNN architecture based malware\nclassification framework through fine-tuning. The performance of the proposed\nmalware classification approaches are validated on the MalImg malware dataset\nusing the hold-out cross-validation technique. Experimental comparisons were\nconducted by employing innovative, customized CNN, trained from scratch and\nfine-tuning the customized CNN using transfer learning. The proposed\nclassification framework DFS-MC showed improved results, Accuracy: 98.61%,\nF-score: 0.96, Precision: 0.96, and Recall: 0.96.",
          "link": "http://arxiv.org/abs/2107.04008",
          "publishedOn": "2021-07-09T01:58:25.703Z",
          "wordCount": 677,
          "title": "Malware Classification Using Deep Boosted Learning. (arXiv:2107.04008v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1\">Pengxiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>",
          "description": "Human motion prediction is essential for tasks such as human motion analysis\nand human-robot interactions. Most existing approaches have been proposed to\nrealize motion prediction. However, they ignore an important task, the\nevaluation of the quality of the predicted result. It is far more enough for\ncurrent approaches in actual scenarios because people can't know how to\ninteract with the machine without the evaluation of prediction, and unreliable\npredictions may mislead the machine to harm the human. Hence, we propose an\nuncertainty-aware framework for human motion prediction (UA-HMP). Concretely,\nwe first design an uncertainty-aware predictor through Gaussian modeling to\nachieve the value and the uncertainty of predicted motion. Then, an\nuncertainty-guided learning scheme is proposed to quantitate the uncertainty\nand reduce the negative effect of the noisy samples during optimization for\nbetter performance. Our proposed framework is easily combined with current SOTA\nbaselines to overcome their weakness in uncertainty modeling with slight\nparameters increment. Extensive experiments also show that they can achieve\nbetter performance in both short and long-term predictions in H3.6M, CMU-Mocap.",
          "link": "http://arxiv.org/abs/2107.03575",
          "publishedOn": "2021-07-09T01:58:25.692Z",
          "wordCount": 597,
          "title": "Uncertainty-aware Human Motion Prediction. (arXiv:2107.03575v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhaoyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingfu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>",
          "description": "Crowd counting is critical for numerous video surveillance scenarios. One of\nthe main issues in this task is how to handle the dramatic scale variations of\npedestrians caused by the perspective effect. To address this issue, this paper\nproposes a novel convolution neural network-based crowd counting method, termed\nPerspective-guided Fractional-Dilation Network (PFDNet). By modeling the\ncontinuous scale variations, the proposed PFDNet is able to select the proper\nfractional dilation kernels for adapting to different spatial locations. It\nsignificantly improves the flexibility of the state-of-the-arts that only\nconsider the discrete representative scales. In addition, by avoiding the\nmulti-scale or multi-column architecture that used in other methods, it is\ncomputationally more efficient. In practice, the proposed PFDNet is constructed\nby stacking multiple Perspective-guided Fractional-Dilation Convolutions (PFC)\non a VGG16-BN backbone. By introducing a novel generalized dilation convolution\noperation, the PFC can handle fractional dilation ratios in the spatial domain\nunder the guidance of perspective annotations, achieving continuous scales\nmodeling of pedestrians. To deal with the problem of unavailable perspective\ninformation in some cases, we further introduce an effective perspective\nestimation branch to the proposed PFDNet, which can be trained in either\nsupervised or weakly-supervised setting once the branch has been pre-trained.\nExtensive experiments show that the proposed PFDNet outperforms\nstate-of-the-art methods on ShanghaiTech A, ShanghaiTech B, WorldExpo'10,\nUCF-QNRF, UCF_CC_50 and TRANCOS dataset, achieving MAE 53.8, 6.5, 6.8, 84.3,\n205.8, and 3.06 respectively.",
          "link": "http://arxiv.org/abs/2107.03665",
          "publishedOn": "2021-07-09T01:58:25.685Z",
          "wordCount": 674,
          "title": "Crowd Counting via Perspective-Guided Fractional-Dilation Convolution. (arXiv:2107.03665v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03651",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bar_David_D/0/1/0/all/0/1\">Daniel Bar-David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bar_David_L/0/1/0/all/0/1\">Laura Bar-David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shapira_Y/0/1/0/all/0/1\">Yinon Shapira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leibu_R/0/1/0/all/0/1\">Rina Leibu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dori_D/0/1/0/all/0/1\">Dalia Dori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schneor_R/0/1/0/all/0/1\">Ronit Schneor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischer_A/0/1/0/all/0/1\">Anath Fischer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soudry_S/0/1/0/all/0/1\">Shiri Soudry</a>",
          "description": "To explore the clinical validity of elastic deformation of optical coherence\ntomography (OCT) images for data augmentation in the development of\ndeep-learning model for detection of diabetic macular edema (DME).",
          "link": "http://arxiv.org/abs/2107.03651",
          "publishedOn": "2021-07-09T01:58:25.650Z",
          "wordCount": 508,
          "title": "Elastic deformation of optical coherence tomography images of diabetic macular edema for deep-learning models training: how far to go?. (arXiv:2107.03651v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kopru_B/0/1/0/all/0/1\">Berkay K&#xf6;pr&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erzin_E/0/1/0/all/0/1\">Engin Erzin</a>",
          "description": "Increasing volume of user-generated human-centric video content and their\napplications, such as video retrieval and browsing, require compact\nrepresentations that are addressed by the video summarization literature.\nCurrent supervised studies formulate video summarization as a\nsequence-to-sequence learning problem and the existing solutions often neglect\nthe surge of human-centric view, which inherently contains affective content.\nIn this study, we investigate the affective-information enriched supervised\nvideo summarization task for human-centric videos. First, we train a visual\ninput-driven state-of-the-art continuous emotion recognition model (CER-NET) on\nthe RECOLA dataset to estimate emotional attributes. Then, we integrate the\nestimated emotional attributes and the high-level representations from the\nCER-NET with the visual information to define the proposed affective video\nsummarization architectures (AVSUM). In addition, we investigate the use of\nattention to improve the AVSUM architectures and propose two new architectures\nbased on temporal attention (TA-AVSUM) and spatial attention (SA-AVSUM). We\nconduct video summarization experiments on the TvSum database. The proposed\nAVSUM-GRU architecture with an early fusion of high level GRU embeddings and\nthe temporal attention based TA-AVSUM architecture attain competitive video\nsummarization performances by bringing strong performance improvements for the\nhuman-centric videos compared to the state-of-the-art in terms of F-score and\nself-defined face recall metrics.",
          "link": "http://arxiv.org/abs/2107.03783",
          "publishedOn": "2021-07-09T01:58:25.642Z",
          "wordCount": 642,
          "title": "Use of Affective Visual Information for Summarization of Human-Centric Videos. (arXiv:2107.03783v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.05244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Adversarial robustness of deep neural networks has been actively\ninvestigated. However, most existing defense approaches are limited to a\nspecific type of adversarial perturbations. Specifically, they often fail to\noffer resistance to multiple attack types simultaneously, i.e., they lack\nmulti-perturbation robustness. Furthermore, compared to image recognition\nproblems, the adversarial robustness of video recognition models is relatively\nunexplored. While several studies have proposed how to generate adversarial\nvideos, only a handful of approaches about the defense strategies have been\npublished in the literature. In this paper, we propose one of the first defense\nstrategies against multiple types of adversarial videos for video recognition.\nThe proposed method, referred to as MultiBN, performs adversarial training on\nmultiple adversarial video types using multiple independent batch normalization\n(BN) layers with a learning-based BN selection module. With a multiple BN\nstructure, each BN brach is responsible for learning the distribution of a\nsingle perturbation type and thus provides more precise distribution\nestimations. This mechanism benefits dealing with multiple perturbation types.\nThe BN selection module detects the attack type of an input video and sends it\nto the corresponding BN branch, making MultiBN fully automatic and allow\nend-to-end training. Compared to present adversarial training approaches, the\nproposed MultiBN exhibits stronger multi-perturbation robustness against\ndifferent and even unforeseen adversarial video types, ranging from Lp-bounded\nattacks and physically realizable attacks. This holds true on different\ndatasets and target models. Moreover, we conduct an extensive analysis to study\nthe properties of the multiple BN structure.",
          "link": "http://arxiv.org/abs/2009.05244",
          "publishedOn": "2021-07-09T01:58:25.621Z",
          "wordCount": 710,
          "title": "Defending Against Multiple and Unforeseen Adversarial Videos. (arXiv:2009.05244v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Long Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wangbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Conventional salient object detection models cannot differentiate the\nimportance of different salient objects. Recently, two works have been proposed\nto detect saliency ranking by assigning different degrees of saliency to\ndifferent objects. However, one of these models cannot differentiate object\ninstances and the other focuses more on sequential attention shift order\ninference. In this paper, we investigate a practical problem setting that\nrequires simultaneously segment salient instances and infer their relative\nsaliency rank order. We present a novel unified model as the first end-to-end\nsolution, where an improved Mask R-CNN is first used to segment salient\ninstances and a saliency ranking branch is then added to infer the relative\nsaliency. For relative saliency ranking, we build a new graph reasoning module\nby combining four graphs to incorporate the instance interaction relation,\nlocal contrast, global contrast, and a high-level semantic prior, respectively.\nA novel loss function is also proposed to effectively train the saliency\nranking branch. Besides, a new dataset and an evaluation metric are proposed\nfor this task, aiming at pushing forward this field of research. Finally,\nexperimental results demonstrate that our proposed model is more effective than\nprevious methods. We also show an example of its practical usage on adaptive\nimage retargeting.",
          "link": "http://arxiv.org/abs/2107.03824",
          "publishedOn": "2021-07-09T01:58:25.607Z",
          "wordCount": 643,
          "title": "Instance-Level Relative Saliency Ranking with Graph Reasoning. (arXiv:2107.03824v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jeffrey Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Serena Yeung</a>",
          "description": "Creating representations of shapes that are invari-ant to isometric or\nalmost-isometric transforma-tions has long been an area of interest in shape\nanal-ysis, since enforcing invariance allows the learningof more effective and\nrobust shape representations.Most existing invariant shape representations\narehandcrafted, and previous work on learning shaperepresentations do not focus\non producing invariantrepresentations. To solve the problem of\nlearningunsupervised invariant shape representations, weuse contrastive\nlearning, which produces discrimi-native representations through learning\ninvarianceto user-specified data augmentations. To producerepresentations that\nare specifically isometry andalmost-isometry invariant, we propose new\ndataaugmentations that randomly sample these transfor-mations. We show\nexperimentally that our methodoutperforms previous unsupervised learning\nap-proaches in both effectiveness and robustness.",
          "link": "http://arxiv.org/abs/2107.03552",
          "publishedOn": "2021-07-09T01:58:25.592Z",
          "wordCount": 541,
          "title": "Staying in Shape: Learning Invariant Shape Representations using Contrastive Learning. (arXiv:2107.03552v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wangyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "There is a prevailing trend to study urban morphology quantitatively thanks\nto the growing accessibility to various forms of spatial big data, increasing\ncomputing power, and use cases benefiting from such information. The methods\ndeveloped up to now measure urban morphology with numerical indices describing\ndensity, proportion, and mixture, but they do not directly represent\nmorphological features from the human's visual and intuitive perspective. We\ntake the first step to bridge the gap by proposing a deep learning-based\ntechnique to automatically classify road networks into four classes on a visual\nbasis. The method is implemented by generating an image of the street network\n(Colored Road Hierarchy Diagram), which we introduce in this paper, and\nclassifying it using a deep convolutional neural network (ResNet-34). The model\nachieves an overall classification accuracy of 0.875. Nine cities around the\nworld are selected as the study areas with their road networks acquired from\nOpenStreetMap. Latent subgroups among the cities are uncovered through\nclustering on the percentage of each road network category. In the subsequent\npart of the paper, we focus on the usability of such classification: we apply\nour method in a case study of urban vitality prediction. An advanced tree-based\nregression model (LightGBM) is for the first time designated to establish the\nrelationship between morphological indices and vitality indicators. The effect\nof road network classification is found to be small but positively associated\nwith urban vitality. This work expands the toolkit of quantitative urban\nmorphology study with new techniques, supporting further studies in the future.",
          "link": "http://arxiv.org/abs/2105.09908",
          "publishedOn": "2021-07-09T01:58:25.584Z",
          "wordCount": 726,
          "title": "Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zipeng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>",
          "description": "Successful real-world deployment of legged robots would require them to adapt\nin real-time to unseen scenarios like changing terrains, changing payloads,\nwear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to\nsolve this problem of real-time online adaptation in quadruped robots. RMA\nconsists of two components: a base policy and an adaptation module. The\ncombination of these components enables the robot to adapt to novel situations\nin fractions of a second. RMA is trained completely in simulation without using\nany domain knowledge like reference trajectories or predefined foot trajectory\ngenerators and is deployed on the A1 robot without any fine-tuning. We train\nRMA on a varied terrain generator using bioenergetics-inspired rewards and\ndeploy it on a variety of difficult terrains including rocky, slippery,\ndeformable surfaces in environments with grass, long vegetation, concrete,\npebbles, stairs, sand, etc. RMA shows state-of-the-art performance across\ndiverse real-world as well as simulation experiments. Video results at\nhttps://ashish-kmr.github.io/rma-legged-robots/",
          "link": "http://arxiv.org/abs/2107.04034",
          "publishedOn": "2021-07-09T01:58:25.577Z",
          "wordCount": 606,
          "title": "RMA: Rapid Motor Adaptation for Legged Robots. (arXiv:2107.04034v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dupont_R/0/1/0/all/0/1\">Robin Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_G/0/1/0/all/0/1\">Guillaume Michel</a>",
          "description": "Pruning seeks to design lightweight architectures by removing redundant\nweights in overparameterized networks. Most of the existing techniques first\nremove structured sub-networks (filters, channels,...) and then fine-tune the\nresulting networks to maintain a high accuracy. However, removing a whole\nstructure is a strong topological prior and recovering the accuracy, with\nfine-tuning, is highly cumbersome. In this paper, we introduce an \"end-to-end\"\nlightweight network design that achieves training and pruning simultaneously\nwithout fine-tuning. The design principle of our method relies on\nreparametrization that learns not only the weights but also the topological\nstructure of the lightweight sub-network. This reparametrization acts as a\nprior (or regularizer) that defines pruning masks implicitly from the weights\nof the underlying network, without increasing the number of training\nparameters. Sparsity is induced with a budget loss that provides an accurate\npruning. Extensive experiments conducted on the CIFAR10 and the TinyImageNet\ndatasets, using standard architectures (namely Conv4, VGG19 and ResNet18), show\ncompelling results without fine-tuning.",
          "link": "http://arxiv.org/abs/2107.03909",
          "publishedOn": "2021-07-09T01:58:25.561Z",
          "wordCount": 598,
          "title": "Weight Reparametrization for Budget-Aware Network Pruning. (arXiv:2107.03909v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lofqvist_M/0/1/0/all/0/1\">Martina Lofqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1\">Jos&#xe9; Cano</a>",
          "description": "There is a proliferation in the number of satellites launched each year,\nresulting in downlinking of terabytes of data each day. The data received by\nground stations is often unprocessed, making this an expensive process\nconsidering the large data sizes and that not all of the data is useful. This,\ncoupled with the increasing demand for real-time data processing, has led to a\ngrowing need for on-orbit processing solutions. In this work, we investigate\nthe performance of CNN-based object detectors on constrained devices by\napplying different image compression techniques to satellite data. We examine\nthe capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier;\nlow-power, high-performance computers, with integrated GPUs, small enough to\nfit on-board a nanosatellite. We take a closer look at object detection\nnetworks, including the Single Shot MultiBox Detector (SSD) and Region-based\nFully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a\nLarge Scale Dataset for Object Detection in Aerial Images. The performance is\nmeasured in terms of execution time, memory consumption, and accuracy, and are\ncompared against a baseline containing a server with two powerful GPUs. The\nresults show that by applying image compression techniques, we are able to\nimprove the execution time and memory consumption, achieving a fully runnable\ndataset. A lossless compression technique achieves roughly a 10% reduction in\nexecution time and about a 3% reduction in memory consumption, with no impact\non the accuracy. While a lossy compression technique improves the execution\ntime by up to 144% and the memory consumption is reduced by as much as 97%.\nHowever, it has a significant impact on accuracy, varying depending on the\ncompression ratio. Thus the application and ratio of these compression\ntechniques may differ depending on the required level of accuracy for a\nparticular task.",
          "link": "http://arxiv.org/abs/2107.03774",
          "publishedOn": "2021-07-09T01:58:25.507Z",
          "wordCount": 777,
          "title": "Optimizing Data Processing in Space for Object Detection in Satellite Imagery. (arXiv:2107.03774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Guerrero_C/0/1/0/all/0/1\">Carmina P&#xe9;rez-Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palacios_A/0/1/0/all/0/1\">Adriana Palacios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_C/0/1/0/all/0/1\">Christian Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Mendoza_M/0/1/0/all/0/1\">Miguel Gonzalez-Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_Morales_L/0/1/0/all/0/1\">Luis Eduardo Falc&#xf3;n-Morales</a>",
          "description": "Risk assessment is relevant in any workplace, however there is a degree of\nunpredictability when dealing with flammable or hazardous materials so that\ndetection of fire accidents by itself may not be enough. An example of this is\nthe impingement of jet fires, where the heat fluxes of the flame could reach\nnearby equipment and dramatically increase the probability of a domino effect\nwith catastrophic results. Because of this, the characterization of such fire\naccidents is important from a risk management point of view. One such\ncharacterization would be the segmentation of different radiation zones within\nthe flame, so this paper presents an exploratory research regarding several\ntraditional computer vision and Deep Learning segmentation approaches to solve\nthis specific problem. A data set of propane jet fires is used to train and\nevaluate the different approaches and given the difference in the distribution\nof the zones and background of the images, different loss functions, that seek\nto alleviate data imbalance, are also explored. Additionally, different metrics\nare correlated to a manual ranking performed by experts to make an evaluation\nthat closely resembles the expert's criteria. The Hausdorff Distance and\nAdjsted Random Index were the metrics with the highest correlation and the best\nresults were obtained from the UNet architecture with a Weighted Cross-Entropy\nLoss. These results can be used in future research to extract more geometric\ninformation from the segmentation masks or could even be implemented on other\ntypes of fire accidents.",
          "link": "http://arxiv.org/abs/2107.03461",
          "publishedOn": "2021-07-09T01:58:25.403Z",
          "wordCount": 701,
          "title": "Comparing ML based Segmentation Models on Jet Fire Radiation Zone. (arXiv:2107.03461v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>",
          "description": "As the data scale grows, deep recognition models often suffer from\nlong-tailed data distributions due to the heavy imbalanced sample number across\ncategories. Indeed, real-world data usually exhibit some similarity relation\namong different categories (e.g., pigeons and sparrows), called category\nsimilarity in this work. It is doubly difficult when the imbalance occurs\nbetween such categories with similar appearances. However, existing solutions\nmainly focus on the sample number to re-balance data distribution. In this\nwork, we systematically investigate the essence of the long-tailed problem from\na unified perspective. Specifically, we demonstrate that long-tailed\nrecognition suffers from both sample number and category similarity.\nIntuitively, using a toy example, we first show that sample number is not the\nunique influence factor for performance dropping of long-tailed recognition.\nTheoretically, we demonstrate that (1) category similarity, as an inevitable\nfactor, would also influence the model learning under long-tailed distribution\nvia similar samples, (2) using more discriminative representation methods\n(e.g., self-supervised learning) for similarity reduction, the classifier bias\ncan be further alleviated with greatly improved performance. Extensive\nexperiments on several long-tailed datasets verify the rationality of our\ntheoretical analysis, and show that based on existing state-of-the-arts\n(SOTAs), the performance could be further improved by similarity reduction. Our\ninvestigations highlight the essence behind the long-tailed problem, and claim\nseveral feasible directions for future work.",
          "link": "http://arxiv.org/abs/2107.03758",
          "publishedOn": "2021-07-09T01:58:25.295Z",
          "wordCount": 653,
          "title": "Investigate the Essence of Long-Tailed Recognition from a Unified Perspective. (arXiv:2107.03758v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuaiqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>",
          "description": "Optical flow estimation is a fundamental problem of computer vision and has\nmany applications in the fields of robot learning and autonomous driving. This\npaper reveals novel geometric laws of optical flow based on the insight and\ndetailed definition of non-occlusion. Then, two novel loss functions are\nproposed for the unsupervised learning of optical flow based on the geometric\nlaws of non-occlusion. Specifically, after the occlusion part of the images are\nmasked, the flowing process of pixels is carefully considered and geometric\nconstraints are conducted based on the geometric laws of optical flow. First,\nneighboring pixels in the first frame will not intersect during the pixel\ndisplacement to the second frame. Secondly, when the cluster containing\nadjacent four pixels in the first frame moves to the second frame, no other\npixels will flow into the quadrilateral formed by them. According to the two\ngeometrical constraints, the optical flow non-intersection loss and the optical\nflow non-blocking loss in the non-occlusion regions are proposed. Two loss\nfunctions punish the irregular and inexact optical flows in the non-occlusion\nregions. The experiments on datasets demonstrated that the proposed\nunsupervised losses of optical flow based on the geometric laws in\nnon-occlusion regions make the estimated optical flow more refined in detail,\nand improve the performance of unsupervised learning of optical flow. In\naddition, the experiments training on synthetic data and evaluating on real\ndata show that the generalization ability of optical flow network is improved\nby our proposed unsupervised approach.",
          "link": "http://arxiv.org/abs/2107.03610",
          "publishedOn": "2021-07-09T01:58:25.283Z",
          "wordCount": 690,
          "title": "NccFlow: Unsupervised Learning of Optical Flow With Non-occlusion from Geometry. (arXiv:2107.03610v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1\">Ricard Durall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1\">Stanislav Frolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>",
          "description": "Transformer models have recently attracted much interest from computer vision\nresearchers and have since been successfully employed for several problems\ntraditionally addressed with convolutional neural networks. At the same time,\nimage synthesis using generative adversarial networks (GANs) has drastically\nimproved over the last few years. The recently proposed TransGAN is the first\nGAN using only transformer-based architectures and achieves competitive results\nwhen compared to convolutional GANs. However, since transformers are\ndata-hungry architectures, TransGAN requires data augmentation, an auxiliary\nsuper-resolution task during training, and a masking prior to guide the\nself-attention mechanism. In this paper, we study the combination of a\ntransformer-based generator and convolutional discriminator and successfully\nremove the need of the aforementioned required design choices. We evaluate our\napproach by conducting a benchmark of well-known CNN discriminators, ablate the\nsize of the transformer-based generator, and show that combining both\narchitectural elements into a hybrid model leads to better results.\nFurthermore, we investigate the frequency spectrum properties of generated\nimages and observe that our model retains the benefits of an attention based\ngenerator.",
          "link": "http://arxiv.org/abs/2105.10189",
          "publishedOn": "2021-07-09T01:58:25.268Z",
          "wordCount": 633,
          "title": "Combining Transformer Generators with Convolutional Discriminators. (arXiv:2105.10189v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ningyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jiayan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yaojun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiangjian Xiao</a>",
          "description": "Angular measurements is essential to make a resonable treatment for Hallux\nvalgus (HV), a common forefoot deformity. However, it still depends on manual\nlabeling and measurement, which is time-consuming and sometimes unreliable.\nAutomating this process is a thing of concern. However, it lack of dataset and\nthe keypoints based method which made a great success in pose estimation is not\nsuitable for this field.To solve the problems, we made a dataset and developed\nan algorithm based on deep learning and linear regression. It shows great\nfitting ability to the ground truth.",
          "link": "http://arxiv.org/abs/2107.03640",
          "publishedOn": "2021-07-09T01:58:25.202Z",
          "wordCount": 552,
          "title": "A Dataset and Method for Hallux Valgus Angle Estimation Based on Deep Learing. (arXiv:2107.03640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03225",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yanwen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_L/0/1/0/all/0/1\">Luyang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Huangjing Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "The novel coronavirus disease 2019 (COVID-19) characterized by atypical\npneumonia has caused millions of deaths worldwide. Automatically segmenting\nlesions from chest Computed Tomography (CT) is a promising way to assist\ndoctors in COVID-19 screening, treatment planning, and follow-up monitoring.\nHowever, voxel-wise annotations are extremely expert-demanding and scarce,\nespecially when it comes to novel diseases, while an abundance of unlabeled\ndata could be available. To tackle the challenge of limited annotations, in\nthis paper, we propose an uncertainty-guided dual-consistency learning network\n(UDC-Net) for semi-supervised COVID-19 lesion segmentation from CT images.\nSpecifically, we present a dual-consistency learning scheme that simultaneously\nimposes image transformation equivalence and feature perturbation invariance to\neffectively harness the knowledge from unlabeled data. We then quantify the\nsegmentation uncertainty in two forms and employ them together to guide the\nconsistency regularization for more reliable unsupervised learning. Extensive\nexperiments showed that our proposed UDC-Net improves the fully supervised\nmethod by 6.3% in Dice and outperforms other competitive semi-supervised\napproaches by significant margins, demonstrating high potential in real-world\nclinical practice.",
          "link": "http://arxiv.org/abs/2104.03225",
          "publishedOn": "2021-07-09T01:58:25.194Z",
          "wordCount": 704,
          "title": "Dual-Consistency Semi-Supervised Learning with Uncertainty Quantification for COVID-19 Lesion Segmentation from CT Images. (arXiv:2104.03225v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1\">Junha Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1\">Karthik Desingh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "To realize robots that can understand human instructions and perform\nmeaningful tasks in the near future, it is important to develop learned models\nthat can understand referential language to identify common objects in\nreal-world 3D scenes. In this paper, we develop a spatial-language model for a\n3D visual grounding problem. Specifically, given a reconstructed 3D scene in\nthe form of a point cloud with 3D bounding boxes of potential object\ncandidates, and a language utterance referring to a target object in the scene,\nour model identifies the target object from a set of potential candidates. Our\nspatial-language model uses a transformer-based architecture that combines\nspatial embedding from bounding-box with a finetuned language embedding from\nDistilBert and reasons among the objects in the 3D scene to find the target\nobject. We show that our model performs competitively on visio-linguistic\ndatasets proposed by ReferIt3D. We provide additional analysis of performance\nin spatial reasoning tasks decoupled from perception noise, the effect of\nview-dependent utterances in terms of accuracy, and view-point annotations for\npotential robotics applications.",
          "link": "http://arxiv.org/abs/2107.03438",
          "publishedOn": "2021-07-09T01:58:25.164Z",
          "wordCount": 614,
          "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding. (arXiv:2107.03438v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1\">Lie Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wanji He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yelin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiufen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>",
          "description": "In medical image segmentation, it is difficult to mark ambiguous areas\naccurately with binary masks, especially when dealing with small lesions.\nTherefore, it is a challenge for radiologists to reach a consensus by using\nbinary masks under the condition of multiple annotations. However, these areas\nmay contain anatomical structures that are conducive to diagnosis. Uncertainty\nis introduced to study these situations. Nevertheless, the uncertainty is\nusually measured by the variances between predictions in a multiple trial way.\nIt is not intuitive, and there is no exact correspondence in the image.\nInspired by image matting, we introduce matting as a soft segmentation method\nand a new perspective to deal with and represent uncertain regions into medical\nscenes, namely medical matting. More specifically, because there is no\navailable medical matting dataset, we first labeled two medical datasets with\nalpha matte. Secondly, the matting method applied to the natural image is not\nsuitable for the medical scene, so we propose a new architecture to generate\nbinary masks and alpha matte in a row. Thirdly, the uncertainty map is\nintroduced to highlight the ambiguous regions from the binary results and\nimprove the matting performance. Evaluated on these datasets, the proposed\nmodel outperformed state-of-the-art matting algorithms by a large margin, and\nalpha matte is proved to be a more efficient labeling form than a binary mask.",
          "link": "http://arxiv.org/abs/2106.09887",
          "publishedOn": "2021-07-09T01:58:25.128Z",
          "wordCount": 710,
          "title": "Medical Matting: A New Perspective on Medical Segmentation with Uncertainty. (arXiv:2106.09887v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eun_K/0/1/0/all/0/1\">Kim Ji Eun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>",
          "description": "Deep Generative Models (DGMs) are known for their superior capability in\ngenerating realistic data. Extending purely data-driven approaches, recent\nspecialized DGMs may satisfy additional controllable requirements such as\nembedding a traffic sign in a driving scene, by manipulating patterns\n\\textit{implicitly} in the neuron or feature level. In this paper, we introduce\na novel method to incorporate domain knowledge \\textit{explicitly} in the\ngeneration process to achieve semantically controllable scene generation. We\ncategorize our knowledge into two types to be consistent with the composition\nof natural scenes, where the first type represents the property of objects and\nthe second type represents the relationship among objects. We then propose a\ntree-structured generative model to learn complex scene representation, whose\nnodes and edges are naturally corresponding to the two types of knowledge\nrespectively. Knowledge can be explicitly integrated to enable semantically\ncontrollable scene generation by imposing semantic rules on properties of nodes\nand edges in the tree structure. We construct a synthetic example to illustrate\nthe controllability and explainability of our method in a clean setting. We\nfurther extend the synthetic example to realistic autonomous vehicle driving\nenvironments and conduct extensive experiments to show that our method\nefficiently identifies adversarial traffic scenes against different\nstate-of-the-art 3D point cloud segmentation models satisfying the traffic\nrules specified as the explicit knowledge.",
          "link": "http://arxiv.org/abs/2106.04066",
          "publishedOn": "2021-07-08T01:57:58.977Z",
          "wordCount": 694,
          "title": "Semantically Controllable Scene Generation with Guidance of Explicit Knowledge. (arXiv:2106.04066v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Junior_C/0/1/0/all/0/1\">Celso A. M. Lopes Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_R/0/1/0/all/0/1\">Ricardo B. das Neves Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bezerra_B/0/1/0/all/0/1\">Byron L. D. Bezerra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toselli_A/0/1/0/all/0/1\">Alejandro H. Toselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Impedovo_D/0/1/0/all/0/1\">Donato Impedovo</a>",
          "description": "This paper describes the short-term competition on Components Segmentation\nTask of Document Photos that was prepared in the context of the 16th\nInternational Conference on Document Analysis and Recognition (ICDAR 2021).\nThis competition aims to bring together researchers working on the filed of\nidentification document image processing and provides them a suitable benchmark\nto compare their techniques on the component segmentation task of document\nimages. Three challenge tasks were proposed entailing different segmentation\nassignments to be performed on a provided dataset. The collected data are from\nseveral types of Brazilian ID documents, whose personal information was\nconveniently replaced. There were 16 participants whose results obtained for\nsome or all the three tasks show different rates for the adopted metrics, like\nDice Similarity Coefficient ranging from 0.06 to 0.99. Different Deep Learning\nmodels were applied by the entrants with diverse strategies to achieve the best\nresults in each of the tasks. Obtained results show that the current applied\nmethods for solving one of the proposed tasks (document boundary detection) are\nalready well stablished. However, for the other two challenge tasks (text zone\nand handwritten sign detection) research and development of more robust\napproaches are still required to achieve acceptable results.",
          "link": "http://arxiv.org/abs/2106.08499",
          "publishedOn": "2021-07-08T01:57:58.513Z",
          "wordCount": 676,
          "title": "ICDAR 2021 Competition on Components Segmentation Task of Document Photos. (arXiv:2106.08499v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1\">Himanshu Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Saurabh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shichong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>",
          "description": "Shape completion is the problem of completing partial input shapes such as\npartial scans. This problem finds important applications in computer vision and\nrobotics due to issues such as occlusion or sparsity in real-world data.\nHowever, most of the existing research related to shape completion has been\nfocused on completing shapes by learning a one-to-one mapping which limits the\ndiversity and creativity of the produced results. We propose a novel multimodal\nshape completion technique that is effectively able to learn a one-to-many\nmapping and generates diverse complete shapes. Our approach is based on the\nconditional Implicit MaximumLikelihood Estimation (IMLE) technique wherein we\ncondition our inputs on partial 3D point clouds. We extensively evaluate our\napproach by comparing it to various baselines both quantitatively and\nqualitatively. We show that our method is superior to alternatives in terms of\ncompleteness and diversity of shapes.",
          "link": "http://arxiv.org/abs/2106.16237",
          "publishedOn": "2021-07-08T01:57:58.477Z",
          "wordCount": 596,
          "title": "Multimodal Shape Completion via IMLE. (arXiv:2106.16237v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhicheng Cai</a>",
          "description": "Traditionally, CNN models possess hierarchical structures and utilize the\nfeature mapping of the last layer to obtain the prediction output. However, it\ncan be difficulty to settle the optimal network depth and make the middle\nlayers learn distinguished features. This paper proposes the Interflow\nalgorithm specially for traditional CNN models. Interflow divides CNNs into\nseveral stages according to the depth and makes predictions by the feature\nmappings in each stage. Subsequently, we input these prediction branches into a\nwell-designed attention module, which learns the weights of these prediction\nbranches, aggregates them and obtains the final output. Interflow weights and\nfuses the features learned in both shallower and deeper layers, making the\nfeature information at each stage processed reasonably and effectively,\nenabling the middle layers to learn more distinguished features, and enhancing\nthe model representation ability. In addition, Interflow can alleviate gradient\nvanishing problem, lower the difficulty of network depth selection, and lighten\npossible over-fitting problem by introducing attention mechanism. Besides, it\ncan avoid network degradation as a byproduct. Compared with the original model,\nthe CNN model with Interflow achieves higher test accuracy on multiple\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2106.14073",
          "publishedOn": "2021-07-08T01:57:58.385Z",
          "wordCount": 645,
          "title": "Interflow: Aggregating Multi-layer Feature Mappings with Attention Mechanism. (arXiv:2106.14073v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanduo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>",
          "description": "Recently, convolutional neural networks (CNNs) have been widely employed to\npromote the face hallucination due to the ability to predict high-frequency\ndetails from a large number of samples. However, most of them fail to take into\naccount the overall facial profile and fine texture details simultaneously,\nresulting in reduced naturalness and fidelity of the reconstructed face, and\nfurther impairing the performance of downstream tasks (e.g., face detection,\nfacial recognition). To tackle this issue, we propose a novel external-internal\nsplit attention group (ESAG), which encompasses two paths responsible for\nfacial structure information and facial texture details, respectively. By\nfusing the features from these two paths, the consistency of facial structure\nand the fidelity of facial details are strengthened at the same time. Then, we\npropose a split-attention in split-attention network (SISN) to reconstruct\nphotorealistic high-resolution facial images by cascading several ESAGs.\nExperimental results on face hallucination and face recognition unveil that the\nproposed method not only significantly improves the clarity of hallucinated\nfaces, but also encourages the subsequent face recognition performance\nsubstantially. Codes have been released at\nhttps://github.com/mdswyz/SISN-Face-Hallucination.",
          "link": "http://arxiv.org/abs/2010.11575",
          "publishedOn": "2021-07-08T01:57:58.332Z",
          "wordCount": 660,
          "title": "Face Hallucination via Split-Attention in Split-Attention Network. (arXiv:2010.11575v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Mingde Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zichao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yifeng Shi</a>",
          "description": "Long-range and short-range temporal modeling are two complementary and\ncrucial aspects of video recognition. Most of the state-of-the-arts focus on\nshort-range spatio-temporal modeling and then average multiple snippet-level\npredictions to yield the final video-level prediction. Thus, their video-level\nprediction does not consider spatio-temporal features of how video evolves\nalong the temporal dimension. In this paper, we introduce a novel Dynamic\nSegment Aggregation (DSA) module to capture relationship among snippets. To be\nmore specific, we attempt to generate a dynamic kernel for a convolutional\noperation to aggregate long-range temporal information among adjacent snippets\nadaptively. The DSA module is an efficient plug-and-play module and can be\ncombined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform\npowerful long-range modeling with minimal overhead. The final video\narchitecture, coined as DSANet. We conduct extensive experiments on several\nvideo recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,\nSomething-Something V1 and ActivityNet) to show its superiority. Our proposed\nDSA module is shown to benefit various video recognition models significantly.\nFor example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is\nimproved from 74.9% to 78.2% on Kinetics-400. Codes are available at\nhttps://github.com/whwu95/DSANet.",
          "link": "http://arxiv.org/abs/2105.12085",
          "publishedOn": "2021-07-08T01:57:58.318Z",
          "wordCount": 675,
          "title": "DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davtyan_A/0/1/0/all/0/1\">Aram Davtyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sameni_S/0/1/0/all/0/1\">Sepehr Sameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerkezi_L/0/1/0/all/0/1\">Llukman Cerkezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meishvilli_G/0/1/0/all/0/1\">Givi Meishvilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielski_A/0/1/0/all/0/1\">Adam Bielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>",
          "description": "Optimization is often cast as a deterministic problem, where the solution is\nfound through some iterative procedure such as gradient descent. However, when\ntraining neural networks the loss function changes over (iteration) time due to\nthe randomized selection of a subset of the samples. This randomization turns\nthe optimization problem into a stochastic one. We propose to consider the loss\nas a noisy observation with respect to some reference optimum. This\ninterpretation of the loss allows us to adopt Kalman filtering as an optimizer,\nas its recursive formulation is designed to estimate unknown parameters from\nnoisy measurements. Moreover, we show that the Kalman Filter dynamical model\nfor the evolution of the unknown parameters can be used to capture the gradient\ndynamics of advanced methods such as Momentum and Adam. We call this stochastic\noptimization method KaFiStO. KaFiStO is an easy to implement, scalable, and\nefficient method to train neural networks. We show that it also yields\nparameter estimates that are on par with or better than existing optimization\nalgorithms across several neural network architectures and machine learning\ntasks, such as computer vision and language modeling.",
          "link": "http://arxiv.org/abs/2107.03331",
          "publishedOn": "2021-07-08T01:57:58.244Z",
          "wordCount": 637,
          "title": "KaFiStO: A Kalman Filtering Framework for Stochastic Optimization. (arXiv:2107.03331v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.02096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shiqing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liying_L/0/1/0/all/0/1\">Liu Liying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Ye Luo</a>",
          "description": "Convolutional neural networks (CNNs) have been used in many machine learning\nfields. In practical applications, the computational cost of convolutional\nneural networks is often high with the deepening of the network and the growth\nof data volume, mostly due to a large amount of multiplication operations of\nfloating-point numbers in convolution operations. To reduce the amount of\nmultiplications, we propose a new type of CNNs called Tropical Convolutional\nNeural Networks (TCNNs) which are built on tropical convolutions in which the\nmultiplications and additions in conventional convolutional layers are replaced\nby additions and min/max operations respectively. In addition, since tropical\nconvolution operators are essentially nonlinear operators, we expect TCNNs to\nhave higher nonlinear fitting ability than conventional CNNs. In the\nexperiments, we test and analyze several different architectures of TCNNs for\nimage classification tasks in comparison with similar-sized conventional CNNs.\nThe results show that TCNN can achieve higher expressive power than ordinary\nconvolutional layers on the MNIST and CIFAR10 image data set. In different\nnoise environments, there are wins and losses in the robustness of TCNN and\nordinary CNNs.",
          "link": "http://arxiv.org/abs/2103.02096",
          "publishedOn": "2021-07-08T01:57:58.230Z",
          "wordCount": 667,
          "title": "An Alternative Practice of Tropical Convolution to Traditional Convolutional Neural Networks. (arXiv:2103.02096v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1\">Guoshun Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1\">Rui Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_S/0/1/0/all/0/1\">Sicong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>",
          "description": "Video grounding aims to localize a moment from an untrimmed video for a given\ntextual query. Existing approaches focus more on the alignment of visual and\nlanguage stimuli with various likelihood-based matching or regression\nstrategies, i.e., P(Y|X). Consequently, these models may suffer from spurious\ncorrelations between the language and video features due to the selection bias\nof the dataset. 1) To uncover the causality behind the model and data, we first\npropose a novel paradigm from the perspective of the causal inference, i.e.,\ninterventional video grounding (IVG) that leverages backdoor adjustment to\ndeconfound the selection bias based on structured causal model (SCM) and\ndo-calculus P(Y|do(X)). Then, we present a simple yet effective method to\napproximate the unobserved confounder as it cannot be directly sampled from the\ndataset. 2) Meanwhile, we introduce a dual contrastive learning approach (DCL)\nto better align the text and video by maximizing the mutual information (MI)\nbetween query and video clips, and the MI between start/end frames of a target\nmoment and the others within a video to learn more informative visual\nrepresentations. Experiments on three standard benchmarks show the\neffectiveness of our approaches. Our code is available on GitHub:\nhttps://github.com/nanguoshun/IVG.",
          "link": "http://arxiv.org/abs/2106.11013",
          "publishedOn": "2021-07-08T01:57:58.222Z",
          "wordCount": 677,
          "title": "Interventional Video Grounding with Dual Contrastive Learning. (arXiv:2106.11013v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_N/0/1/0/all/0/1\">Nicolo Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>",
          "description": "We propose a new gradient-based approach for extracting sub-architectures\nfrom a given large model. Contrarily to existing pruning methods, which are\nunable to disentangle the network architecture and the corresponding weights,\nour architecture-pruning scheme produces transferable new structures that can\nbe successfully retrained to solve different tasks. We focus on a\ntransfer-learning setup where architectures can be trained on a large data set\nbut very few data points are available for fine-tuning them on new tasks. We\ndefine a new gradient-based algorithm that trains architectures of arbitrarily\nlow complexity independently from the attached weights. Given a search space\ndefined by an existing large neural model, we reformulate the architecture\nsearch task as a complexity-penalized subset-selection problem and solve it\nthrough a two-temperature relaxation scheme. We provide theoretical convergence\nguarantees and validate the proposed transfer-learning strategy on real data.",
          "link": "http://arxiv.org/abs/2107.03375",
          "publishedOn": "2021-07-08T01:57:58.189Z",
          "wordCount": 590,
          "title": "Differentiable Architecture Pruning for Transfer Learning. (arXiv:2107.03375v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.15526",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Payette_K/0/1/0/all/0/1\">Kelly Payette</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dumast_P/0/1/0/all/0/1\">Priscille de Dumast</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kebiri_H/0/1/0/all/0/1\">Hamza Kebiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ezhov_I/0/1/0/all/0/1\">Ivan Ezhov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paetzold_J/0/1/0/all/0/1\">Johannes C. Paetzold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shit_S/0/1/0/all/0/1\">Suprosanna Shit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iqbal_A/0/1/0/all/0/1\">Asim Iqbal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_R/0/1/0/all/0/1\">Romesa Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kottke_R/0/1/0/all/0/1\">Raimund Kottke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grehten_P/0/1/0/all/0/1\">Patrice Grehten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_H/0/1/0/all/0/1\">Hui Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lanczi_L/0/1/0/all/0/1\">Levente Lanczi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nagy_M/0/1/0/all/0/1\">Marianna Nagy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beresova_M/0/1/0/all/0/1\">Monika Beresova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1\">Thi Dao Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Natalucci_G/0/1/0/all/0/1\">Giancarlo Natalucci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karayannis_T/0/1/0/all/0/1\">Theofanis Karayannis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cuadra_M/0/1/0/all/0/1\">Meritxell Bach Cuadra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jakab_A/0/1/0/all/0/1\">Andras Jakab</a>",
          "description": "It is critical to quantitatively analyse the developing human fetal brain in\norder to fully understand neurodevelopment in both normal fetuses and those\nwith congenital disorders. To facilitate this analysis, automatic multi-tissue\nfetal brain segmentation algorithms are needed, which in turn requires open\ndatabases of segmented fetal brains. Here we introduce a publicly available\ndatabase of 50 manually segmented pathological and non-pathological fetal\nmagnetic resonance brain volume reconstructions across a range of gestational\nages (20 to 33 weeks) into 7 different tissue categories (external\ncerebrospinal fluid, grey matter, white matter, ventricles, cerebellum, deep\ngrey matter, brainstem/spinal cord). In addition, we quantitatively evaluate\nthe accuracy of several automatic multi-tissue segmentation algorithms of the\ndeveloping human fetal brain. Four research groups participated, submitting a\ntotal of 10 algorithms, demonstrating the benefits the database for the\ndevelopment of automatic algorithms.",
          "link": "http://arxiv.org/abs/2010.15526",
          "publishedOn": "2021-07-08T01:57:58.053Z",
          "wordCount": 697,
          "title": "An automatic multi-tissue human fetal brain segmentation benchmark using the Fetal Tissue Annotation Dataset. (arXiv:2010.15526v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guile Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>",
          "description": "Deep learning has been successful for many computer vision tasks due to the\navailability of shared and centralised large-scale training data. However,\nincreasing awareness of privacy concerns poses new challenges to deep learning,\nespecially for human subject related recognition such as person\nre-identification (Re-ID). In this work, we solve the Re-ID problem by\ndecentralised learning from non-shared private training data distributed at\nmultiple user sites of independent multi-domain label spaces. We propose a\nnovel paradigm called Federated Person Re-Identification (FedReID) to construct\na generalisable global model (a central server) by simultaneously learning with\nmultiple privacy-preserved local models (local clients). Specifically, each\nlocal client receives global model updates from the server and trains a local\nmodel using its local data independent from all the other clients. Then, the\ncentral server aggregates transferrable local model updates to construct a\ngeneralisable global feature embedding model without accessing local data so to\npreserve local privacy. This client-server collaborative learning process is\niteratively performed under privacy control, enabling FedReID to realise\ndecentralised learning without sharing distributed data nor collecting any\ncentralised data. Extensive experiments on ten Re-ID benchmarks show that\nFedReID achieves compelling generalisation performance beyond any locally\ntrained models without using shared training data, whilst inherently protects\nthe privacy of each local client. This is uniquely advantageous over\ncontemporary Re-ID methods.",
          "link": "http://arxiv.org/abs/2006.04150",
          "publishedOn": "2021-07-08T01:57:58.031Z",
          "wordCount": 704,
          "title": "Decentralised Learning from Independent Multi-Domain Labels for Person Re-Identification. (arXiv:2006.04150v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blokland_B/0/1/0/all/0/1\">Bart Iver van Blokland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theoharis_T/0/1/0/all/0/1\">Theoharis Theoharis</a>",
          "description": "A complete pipeline is presented for accurate and efficient partial 3D object\nretrieval based on Quick Intersection Count Change Image (QUICCI) binary local\ndescriptors and a novel indexing tree. It is shown how a modification to the\nQUICCI query descriptor makes it ideal for partial retrieval. An indexing\nstructure called Dissimilarity Tree is proposed which can significantly\naccelerate searching the large space of local descriptors; this is applicable\nto QUICCI and other binary descriptors. The index exploits the distribution of\nbits within descriptors for efficient retrieval. The retrieval pipeline is\ntested on the artificial part of SHREC'16 dataset with near-ideal retrieval\nresults.",
          "link": "http://arxiv.org/abs/2107.03368",
          "publishedOn": "2021-07-08T01:57:58.023Z",
          "wordCount": 559,
          "title": "Partial 3D Object Retrieval using Local Binary QUICCI Descriptors and Dissimilarity Tree Indexing. (arXiv:2107.03368v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Huayi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1\">Xiaorong Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaofeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Ming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>",
          "description": "Multi-view clustering, a long-standing and important research problem,\nfocuses on mining complementary information from diverse views. However,\nexisting works often fuse multiple views' representations or handle clustering\nin a common feature space, which may result in their entanglement especially\nfor visual representations. To address this issue, we present a novel VAE-based\nmulti-view clustering framework (Multi-VAE) by learning disentangled visual\nrepresentations. Concretely, we define a view-common variable and multiple\nview-peculiar variables in the generative model. The prior of view-common\nvariable obeys approximately discrete Gumbel Softmax distribution, which is\nintroduced to extract the common cluster factor of multiple views. Meanwhile,\nthe prior of view-peculiar variable follows continuous Gaussian distribution,\nwhich is used to represent each view's peculiar visual factors. By controlling\nthe mutual information capacity to disentangle the view-common and\nview-peculiar representations, continuous visual information of multiple views\ncan be separated so that their common discrete cluster information can be\neffectively mined. Experimental results demonstrate that Multi-VAE enjoys the\ndisentangled and explainable visual representations, while obtaining superior\nclustering performance compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.11232",
          "publishedOn": "2021-07-08T01:57:58.008Z",
          "wordCount": 693,
          "title": "Multi-VAE: Learning Disentangled View-common and View-peculiar Visual Representations for Multi-view Clustering. (arXiv:2106.11232v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Szandala_T/0/1/0/all/0/1\">Tomasz Szandala</a>",
          "description": "In this paper, an enhancement technique for the class activation mapping\nmethods such as gradient-weighted class activation maps or excitation\nbackpropagation is proposed to present the visual explanations of decisions\nfrom convolutional neural network-based models. The proposed idea, called\nGradual Extrapolation, can supplement any method that generates a heatmap\npicture by sharpening the output. Instead of producing a coarse localization\nmap that highlights the important predictive regions in the image, the proposed\nmethod outputs the specific shape that most contributes to the model output.\nThus, the proposed method improves the accuracy of saliency maps. The effect\nhas been achieved by the gradual propagation of the crude map obtained in the\ndeep layer through all preceding layers with respect to their activations. In\nvalidation tests conducted on a selected set of images, the faithfulness,\ninterpretability, and applicability of the method are evaluated. The proposed\ntechnique significantly improves the localization detection of the neural\nnetworks attention at low additional computational costs. Furthermore, the\nproposed method is applicable to a variety deep neural network models. The code\nfor the method can be found at\nhttps://github.com/szandala/gradual-extrapolation",
          "link": "http://arxiv.org/abs/2104.04945",
          "publishedOn": "2021-07-08T01:57:57.848Z",
          "wordCount": 672,
          "title": "Enhancing Deep Neural Network Saliency Visualizations with Gradual Extrapolation. (arXiv:2104.04945v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lienen_J/0/1/0/all/0/1\">Julian Lienen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nommensen_N/0/1/0/all/0/1\">Nils Nommensen</a>",
          "description": "In many real-world applications, the relative depth of objects in an image is\ncrucial for scene understanding. Recent approaches mainly tackle the problem of\ndepth prediction in monocular images by treating the problem as a regression\ntask. Yet, being interested in an order relation in the first place, ranking\nmethods suggest themselves as a natural alternative to regression, and indeed,\nranking approaches leveraging pairwise comparisons as training information\n(\"object A is closer to the camera than B\") have shown promising performance on\nthis problem. In this paper, we elaborate on the use of so-called listwise\nranking as a generalization of the pairwise approach. Our method is based on\nthe Plackett-Luce (PL) model, a probability distribution on rankings, which we\ncombine with a state-of-the-art neural network architecture and a simple\nsampling strategy to reduce training complexity. Moreover, taking advantage of\nthe representation of PL as a random utility model, the proposed predictor\noffers a natural way to recover (shift-invariant) metric depth information from\nranking-only data provided at training time. An empirical evaluation on several\nbenchmark datasets in a \"zero-shot\" setting demonstrates the effectiveness of\nour approach compared to existing ranking and regression methods.",
          "link": "http://arxiv.org/abs/2010.13118",
          "publishedOn": "2021-07-08T01:57:57.840Z",
          "wordCount": 713,
          "title": "Monocular Depth Estimation via Listwise Ranking using the Plackett-Luce Model. (arXiv:2010.13118v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhicheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chenglei Peng</a>",
          "description": "This paper proposes a novel nonlinear activation mechanism typically for\nconvolutional neural network (CNN), named as reborn mechanism. In sharp\ncontrast to ReLU which cuts off the negative phase value, the reborn mechanism\nenjoys the capacity to reborn and reconstruct dead neurons. Compared to other\nimproved ReLU functions, reborn mechanism introduces a more proper way to\nutilize the negative phase information. Extensive experiments validate that\nthis activation mechanism is able to enhance the model representation ability\nmore significantly and make the better use of the input data information while\nmaintaining the advantages of the original ReLU function. Moreover, reborn\nmechanism enables a non-symmetry that is hardly achieved by traditional CNNs\nand can act as a channel compensation method, offering competitive or even\nbetter performance but with fewer learned parameters than traditional methods.\nReborn mechanism was tested on various benchmark datasets, all obtaining better\nperformance than previous nonlinear activation functions.",
          "link": "http://arxiv.org/abs/2106.07026",
          "publishedOn": "2021-07-08T01:57:57.830Z",
          "wordCount": 617,
          "title": "Reborn Mechanism: Rethinking the Negative Phase Information Flow in Convolutional Neural Network. (arXiv:2106.07026v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.12780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paterson_C/0/1/0/all/0/1\">Colin Paterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calinescu_R/0/1/0/all/0/1\">Radu Calinescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picardi_C/0/1/0/all/0/1\">Chiara Picardi</a>",
          "description": "Regions of high-dimensional input spaces that are underrepresented in\ntraining datasets reduce machine-learnt classifier performance, and may lead to\ncorner cases and unwanted bias for classifiers used in decision making systems.\nWhen these regions belong to otherwise well-represented classes, their presence\nand negative impact are very hard to identify. We propose an approach for the\ndetection and mitigation of such rare subclasses in deep neural network\nclassifiers. The new approach is underpinned by an easy-to-compute commonality\nmetric that supports the detection of rare subclasses, and comprises methods\nfor reducing the impact of these subclasses during both model training and\nmodel exploitation. We demonstrate our approach using two well-known datasets,\nMNIST's handwritten digits and Kaggle's cats/dogs, identifying rare subclasses\nand producing models which compensate for subclass rarity. In addition we\ndemonstrate how our run-time approach increases the ability of users to\nidentify samples likely to be misclassified at run-time.",
          "link": "http://arxiv.org/abs/1911.12780",
          "publishedOn": "2021-07-08T01:57:57.822Z",
          "wordCount": 628,
          "title": "Detection and Mitigation of Rare Subclasses in Deep Neural Network Classifiers. (arXiv:1911.12780v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02927",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mishra_S/0/1/0/all/0/1\">Suraj Mishra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1\">Danny Z. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">X. Sharon Hu</a>",
          "description": "Compression is a standard procedure for making convolutional neural networks\n(CNNs) adhere to some specific computing resource constraints. However,\nsearching for a compressed architecture typically involves a series of\ntime-consuming training/validation experiments to determine a good compromise\nbetween network size and performance accuracy. To address this, we propose an\nimage complexity-guided network compression technique for biomedical image\nsegmentation. Given any resource constraints, our framework utilizes data\ncomplexity and network architecture to quickly estimate a compressed model\nwhich does not require network training. Specifically, we map the dataset\ncomplexity to the target network accuracy degradation caused by compression.\nSuch mapping enables us to predict the final accuracy for different network\nsizes, based on the computed dataset complexity. Thus, one may choose a\nsolution that meets both the network size and segmentation accuracy\nrequirements. Finally, the mapping is used to determine the convolutional\nlayer-wise multiplicative factor for generating a compressed network. We\nconduct experiments using 5 datasets, employing 3 commonly-used CNN\narchitectures for biomedical image segmentation as representative networks. Our\nproposed framework is shown to be effective for generating compressed\nsegmentation networks, retaining up to $\\approx 95\\%$ of the full-sized network\nsegmentation accuracy, and at the same time, utilizing $\\approx 32x$ fewer\nnetwork trainable weights (average reduction) of the full-sized networks.",
          "link": "http://arxiv.org/abs/2107.02927",
          "publishedOn": "2021-07-08T01:57:57.751Z",
          "wordCount": 658,
          "title": "Image Complexity Guided Network Compression for Biomedical Image Segmentation. (arXiv:2107.02927v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aafaq_N/0/1/0/all/0/1\">Nayyer Aafaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>",
          "description": "Deep learning is found to be vulnerable to adversarial examples. However, its\nadversarial susceptibility in image caption generation is under-explored. We\nstudy adversarial examples for vision and language models, which typically\nadopt an encoder-decoder framework consisting of two major components: a\nConvolutional Neural Network (i.e., CNN) for image feature extraction and a\nRecurrent Neural Network (RNN) for caption generation. In particular, we\ninvestigate attacks on the visual encoder's hidden layer that is fed to the\nsubsequent recurrent network. The existing methods either attack the\nclassification layer of the visual encoder or they back-propagate the gradients\nfrom the language model. In contrast, we propose a GAN-based algorithm for\ncrafting adversarial examples for neural image captioning that mimics the\ninternal representation of the CNN such that the resulting deep features of the\ninput image enable a controlled incorrect caption generation through the\nrecurrent network. Our contribution provides new insights for understanding\nadversarial attacks on vision systems with language component. The proposed\nmethod employs two strategies for a comprehensive evaluation. The first\nexamines if a neural image captioning system can be misled to output targeted\nimage captions. The second analyzes the possibility of keywords into the\npredicted captions. Experiments show that our algorithm can craft effective\nadversarial images based on the CNN hidden layers to fool captioning framework.\nMoreover, we discover the proposed attack to be highly transferable. Our work\nleads to new robustness implications for neural image captioning.",
          "link": "http://arxiv.org/abs/2107.03050",
          "publishedOn": "2021-07-08T01:57:57.744Z",
          "wordCount": 678,
          "title": "Controlled Caption Generation for Images Through Adversarial Attacks. (arXiv:2107.03050v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaoyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>",
          "description": "Deepfakes pose growing challenges to the trust of information on the\nInternet. Thus, detecting deepfakes has attracted increasing attentions from\nboth academia and industry. State-of-the-art deepfake detection methods consist\nof two key components, i.e., face extractor and face classifier, which extract\nthe face region in an image and classify it to be real/fake, respectively.\nExisting studies mainly focused on improving the detection performance in\nnon-adversarial settings, leaving security of deepfake detection in adversarial\nsettings largely unexplored. In this work, we aim to bridge the gap. In\nparticular, we perform a systematic measurement study to understand the\nsecurity of the state-of-the-art deepfake detection methods in adversarial\nsettings. We use two large-scale public deepfakes data sources including\nFaceForensics++ and Facebook Deepfake Detection Challenge, where the deepfakes\nare fake face images; and we train state-of-the-art deepfake detection methods.\nThese detection methods can achieve 0.94--0.99 accuracies in non-adversarial\nsettings on these datasets. However, our measurement results uncover multiple\nsecurity limitations of the deepfake detection methods in adversarial settings.\nFirst, we find that an attacker can evade a face extractor, i.e., the face\nextractor fails to extract the correct face regions, via adding small Gaussian\nnoise to its deepfake images. Second, we find that a face classifier trained\nusing deepfakes generated by one method cannot detect deepfakes generated by\nanother method, i.e., an attacker can evade detection via generating deepfakes\nusing a new method. Third, we find that an attacker can leverage backdoor\nattacks developed by the adversarial machine learning community to evade a face\nclassifier. Our results highlight that deepfake detection should consider the\nadversarial nature of the problem.",
          "link": "http://arxiv.org/abs/2107.02045",
          "publishedOn": "2021-07-08T01:57:57.728Z",
          "wordCount": 724,
          "title": "Understanding the Security of Deepfake Detection. (arXiv:2107.02045v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balzategui_J/0/1/0/all/0/1\">Julen Balzategui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eciolaza_L/0/1/0/all/0/1\">Luka Eciolaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maestro_Watson_D/0/1/0/all/0/1\">Daniel Maestro-Watson</a>",
          "description": "Quality inspection applications in industry are required to move towards a\nzero-defect manufacturing scenario, withnon-destructive inspection and\ntraceability of 100 % of produced parts. Developing robust fault detection and\nclassification modelsfrom the start-up of the lines is challenging due to the\ndifficulty in getting enough representative samples of the faulty patternsand\nthe need to manually label them. This work presents a methodology to develop a\nrobust inspection system, targeting thesepeculiarities, in the context of solar\ncell manufacturing. The methodology is divided into two phases: In the first\nphase, an anomalydetection model based on a Generative Adversarial Network\n(GAN) is employed. This model enables the detection and localizationof\nanomalous patterns within the solar cells from the beginning, using only\nnon-defective samples for training and without anymanual labeling involved. In\na second stage, as defective samples arise, the detected anomalies will be used\nas automaticallygenerated annotations for the supervised training of a Fully\nConvolutional Network that is capable of detecting multiple types offaults. The\nexperimental results using 1873 EL images of monocrystalline cells show that\n(a) the anomaly detection scheme can beused to start detecting features with\nvery little available data, (b) the anomaly detection may serve as automatic\nlabeling in order totrain a supervised model, and (c) segmentation and\nclassification results of supervised models trained with automatic labels\narecomparable to the ones obtained from the models trained with manual labels.",
          "link": "http://arxiv.org/abs/2103.03518",
          "publishedOn": "2021-07-08T01:57:57.708Z",
          "wordCount": 762,
          "title": "Anomaly detection and automatic labeling for solar cell quality inspection based on Generative Adversarial Network. (arXiv:2103.03518v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yicheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "Semi-supervised learning has attracted great attention in the field of\nmachine learning, especially for medical image segmentation tasks, since it\nalleviates the heavy burden of collecting abundant densely annotated data for\ntraining. However, most of existing methods underestimate the importance of\nchallenging regions (e.g. small branches or blurred edges) during training. We\nbelieve that these unlabeled regions may contain more crucial information to\nminimize the uncertainty prediction for the model and should be emphasized in\nthe training process. Therefore, in this paper, we propose a novel Mutual\nConsistency Network (MC-Net) for semi-supervised left atrium segmentation from\n3D MR images. Particularly, our MC-Net consists of one encoder and two slightly\ndifferent decoders, and the prediction discrepancies of two decoders are\ntransformed as an unsupervised loss by our designed cycled pseudo label scheme\nto encourage mutual consistency. Such mutual consistency encourages the two\ndecoders to have consistent and low-entropy predictions and enables the model\nto gradually capture generalized features from these unlabeled challenging\nregions. We evaluate our MC-Net on the public Left Atrium (LA) database and it\nobtains impressive performance gains by exploiting the unlabeled data\neffectively. Our MC-Net outperforms six recent semi-supervised methods for left\natrium segmentation, and sets the new state-of-the-art performance on the LA\ndatabase.",
          "link": "http://arxiv.org/abs/2103.02911",
          "publishedOn": "2021-07-08T01:57:57.694Z",
          "wordCount": 677,
          "title": "Semi-supervised Left Atrium Segmentation with Mutual Consistency Training. (arXiv:2103.02911v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03145",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Umer_R/0/1/0/all/0/1\">Rao Muhammad Umer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munir_A/0/1/0/all/0/1\">Asad Munir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Micheloni_C/0/1/0/all/0/1\">Christian Micheloni</a>",
          "description": "Recently, most of state-of-the-art single image super-resolution (SISR)\nmethods have attained impressive performance by using deep convolutional neural\nnetworks (DCNNs). The existing SR methods have limited performance due to a\nfixed degradation settings, i.e. usually a bicubic downscaling of\nlow-resolution (LR) image. However, in real-world settings, the LR degradation\nprocess is unknown which can be bicubic LR, bilinear LR, nearest-neighbor LR,\nor real LR. Therefore, most SR methods are ineffective and inefficient in\nhandling more than one degradation settings within a single network. To handle\nthe multiple degradation, i.e. refers to multi-domain image super-resolution,\nwe propose a deep Super-Resolution Residual StarGAN (SR2*GAN), a novel and\nscalable approach that super-resolves the LR images for the multiple LR domains\nusing only a single model. The proposed scheme is trained in a StarGAN like\nnetwork topology with a single generator and discriminator networks. We\ndemonstrate the effectiveness of our proposed approach in quantitative and\nqualitative experiments compared to other state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.03145",
          "publishedOn": "2021-07-08T01:57:57.686Z",
          "wordCount": 632,
          "title": "A Deep Residual Star Generative Adversarial Network for multi-domain Image Super-Resolution. (arXiv:2107.03145v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1\">Xiaohan Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yuenan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Max Q.-H. Meng</a>",
          "description": "The amount of medical images for training deep classification models is\ntypically very scarce, making these deep models prone to overfit the training\ndata. Studies showed that knowledge distillation (KD), especially the\nmean-teacher framework which is more robust to perturbations, can help mitigate\nthe over-fitting effect. However, directly transferring KD from computer vision\nto medical image classification yields inferior performance as medical images\nsuffer from higher intra-class variance and class imbalance. To address these\nissues, we propose a novel Categorical Relation-preserving Contrastive\nKnowledge Distillation (CRCKD) algorithm, which takes the commonly used\nmean-teacher model as the supervisor. Specifically, we propose a novel\nClass-guided Contrastive Distillation (CCD) module to pull closer positive\nimage pairs from the same class in the teacher and student models, while\npushing apart negative image pairs from different classes. With this\nregularization, the feature distribution of the student model shows higher\nintra-class similarity and inter-class variance. Besides, we propose a\nCategorical Relation Preserving (CRP) loss to distill the teacher's relational\nknowledge in a robust and class-balanced manner. With the contribution of the\nCCD and CRP, our CRCKD algorithm can distill the relational knowledge more\ncomprehensively. Extensive experiments on the HAM10000 and APTOS datasets\ndemonstrate the superiority of the proposed CRCKD method.",
          "link": "http://arxiv.org/abs/2107.03225",
          "publishedOn": "2021-07-08T01:57:57.679Z",
          "wordCount": 646,
          "title": "Categorical Relation-Preserving Contrastive Knowledge Distillation for Medical Image Classification. (arXiv:2107.03225v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gaowen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1\">Hugo Latapie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1\">Jason Corso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>",
          "description": "Cross-view video synthesis task seeks to generate video sequences of one view\nfrom another dramatically different view. In this paper, we investigate the\nexocentric (third-person) view to egocentric (first-person) view video\ngeneration task. This is challenging because egocentric view sometimes is\nremarkably different from the exocentric view. Thus, transforming the\nappearances across the two different views is a non-trivial task. Particularly,\nwe propose a novel Bi-directional Spatial Temporal Attention Fusion Generative\nAdversarial Network (STA-GAN) to learn both spatial and temporal information to\ngenerate egocentric video sequences from the exocentric view. The proposed\nSTA-GAN consists of three parts: temporal branch, spatial branch, and attention\nfusion. First, the temporal and spatial branches generate a sequence of fake\nframes and their corresponding features. The fake frames are generated in both\ndownstream and upstream directions for both temporal and spatial branches.\nNext, the generated four different fake frames and their corresponding features\n(spatial and temporal branches in two directions) are fed into a novel\nmulti-generation attention fusion module to produce the final video sequence.\nMeanwhile, we also propose a novel temporal and spatial dual-discriminator for\nmore robust network optimization. Extensive experiments on the Side2Ego and\nTop2Ego datasets show that the proposed STA-GAN significantly outperforms the\nexisting methods.",
          "link": "http://arxiv.org/abs/2107.03120",
          "publishedOn": "2021-07-08T01:57:57.660Z",
          "wordCount": 644,
          "title": "Cross-View Exocentric to Egocentric Video Synthesis. (arXiv:2107.03120v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shoukui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wankou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1\">Erjin Zhou</a>",
          "description": "The 2D heatmap representation has dominated human pose estimation for years\ndue to its high performance. However, heatmap-based approaches have some\ndrawbacks: 1) The performance drops dramatically in the low-resolution images,\nwhich are frequently encountered in real-world scenarios. 2) To improve the\nlocalization precision, multiple upsample layers may be needed to recover the\nfeature map resolution from low to high, which are computationally expensive.\n3) Extra coordinate refinement is usually necessary to reduce the quantization\nerror of downscaled heatmaps. To address these issues, we propose a\n\\textbf{Sim}ple yet promising \\textbf{D}isentangled \\textbf{R}epresentation for\nkeypoint coordinate (\\emph{SimDR}), reformulating human keypoint localization\nas a task of classification. In detail, we propose to disentangle the\nrepresentation of horizontal and vertical coordinates for keypoint location,\nleading to a more efficient scheme without extra upsampling and refinement.\nComprehensive experiments conducted over COCO dataset show that the proposed\n\\emph{heatmap-free} methods outperform \\emph{heatmap-based} counterparts in all\ntested input resolutions, especially in lower resolutions by a large margin.\nCode will be made publicly available at \\url{https://github.com/leeyegy/SimDR}.",
          "link": "http://arxiv.org/abs/2107.03332",
          "publishedOn": "2021-07-08T01:57:57.645Z",
          "wordCount": 626,
          "title": "Is 2D Heatmap Representation Even Necessary for Human Pose Estimation?. (arXiv:2107.03332v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "In this paper, we present Long Short-term TRansformer (LSTR), a new temporal\nmodeling algorithm for online action detection, by employing a long- and\nshort-term memories mechanism that is able to model prolonged sequence data. It\nconsists of an LSTR encoder that is capable of dynamically exploiting\ncoarse-scale historical information from an extensively long time window (e.g.,\n2048 long-range frames of up to 8 minutes), together with an LSTR decoder that\nfocuses on a short time window (e.g., 32 short-range frames of 8 seconds) to\nmodel the fine-scale characterization of the ongoing event. Compared to prior\nwork, LSTR provides an effective and efficient method to model long videos with\nless heuristic algorithm design. LSTR achieves significantly improved results\non standard online action detection benchmarks, THUMOS'14, TVSeries, and HACS\nSegment, over the existing state-of-the-art approaches. Extensive empirical\nanalysis validates the setup of the long- and short-term memories and the\ndesign choices of LSTR.",
          "link": "http://arxiv.org/abs/2107.03377",
          "publishedOn": "2021-07-08T01:57:57.638Z",
          "wordCount": 593,
          "title": "Long Short-Term Transformer for Online Action Detection. (arXiv:2107.03377v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1\">Francesco Barbato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1\">Marco Toldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1\">Umberto Michieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>",
          "description": "Deep convolutional neural networks for semantic segmentation achieve\noutstanding accuracy, however they also have a couple of major drawbacks:\nfirst, they do not generalize well to distributions slightly different from the\none of the training data; second, they require a huge amount of labeled data\nfor their optimization. In this paper, we introduce feature-level space-shaping\nregularization strategies to reduce the domain discrepancy in semantic\nsegmentation. In particular, for this purpose we jointly enforce a clustering\nobjective, a perpendicularity constraint and a norm alignment goal on the\nfeature vectors corresponding to source and target samples. Additionally, we\npropose a novel measure able to capture the relative efficacy of an adaptation\nstrategy compared to supervised training. We verify the effectiveness of such\nmethods in the autonomous driving setting achieving state-of-the-art results in\nmultiple synthetic-to-real road scenes benchmarks.",
          "link": "http://arxiv.org/abs/2104.02633",
          "publishedOn": "2021-07-08T01:57:57.631Z",
          "wordCount": 621,
          "title": "Latent Space Regularization for Unsupervised Domain Adaptation in Semantic Segmentation. (arXiv:2104.02633v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03337",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Harbrecht_H/0/1/0/all/0/1\">Helmut Harbrecht</a>, <a href=\"http://arxiv.org/find/math/1/au:+Multerer_M/0/1/0/all/0/1\">Michael Multerer</a>",
          "description": "In this article, we introduce the novel concept of samplets by transferring\nthe construction of Tausch-White wavelets to the realm of data. This way we\nobtain a multilevel representation of discrete data which directly enables data\ncompression, detection of singularities and adaptivity. Applying samplets to\nrepresent kernel matrices, as they arise in kernel based learning or Gaussian\nprocess regression, we end up with quasi-sparse matrices. By thresholding small\nentries, these matrices are compressible to O(N log N) relevant entries, where\nN is the number of data points. This feature allows for the use of fill-in\nreducing reorderings to obtain a sparse factorization of the compressed\nmatrices. Besides the comprehensive introduction to samplets and their\nproperties, we present extensive numerical studies to benchmark the approach.\nOur results demonstrate that samplets mark a considerable step in the direction\nof making large data sets accessible for analysis.",
          "link": "http://arxiv.org/abs/2107.03337",
          "publishedOn": "2021-07-08T01:57:57.624Z",
          "wordCount": 581,
          "title": "Samplets: A new paradigm for data compression. (arXiv:2107.03337v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1\">Poojan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hacihaliloglu_I/0/1/0/all/0/1\">Ilker Hacihaliloglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Over the past decade, Deep Convolutional Neural Networks have been widely\nadopted for medical image segmentation and shown to achieve adequate\nperformance. However, due to the inherent inductive biases present in the\nconvolutional architectures, they lack understanding of long-range dependencies\nin the image. Recently proposed Transformer-based architectures that leverage\nself-attention mechanism encode long-range dependencies and learn\nrepresentations that are highly expressive. This motivates us to explore\nTransformer-based solutions and study the feasibility of using\nTransformer-based network architectures for medical image segmentation tasks.\nMajority of existing Transformer-based network architectures proposed for\nvision applications require large-scale datasets to train properly. However,\ncompared to the datasets for vision applications, for medical imaging the\nnumber of data samples is relatively low, making it difficult to efficiently\ntrain transformers for medical applications. To this end, we propose a Gated\nAxial-Attention model which extends the existing architectures by introducing\nan additional control mechanism in the self-attention module. Furthermore, to\ntrain the model effectively on medical images, we propose a Local-Global\ntraining strategy (LoGo) which further improves the performance. Specifically,\nwe operate on the whole image and patches to learn global and local features,\nrespectively. The proposed Medical Transformer (MedT) is evaluated on three\ndifferent medical image segmentation datasets and it is shown that it achieves\nbetter performance than the convolutional and other related transformer-based\narchitectures. Code: https://github.com/jeya-maria-jose/Medical-Transformer",
          "link": "http://arxiv.org/abs/2102.10662",
          "publishedOn": "2021-07-08T01:57:57.604Z",
          "wordCount": 698,
          "title": "Medical Transformer: Gated Axial-Attention for Medical Image Segmentation. (arXiv:2102.10662v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1\">Mohamed Elgharib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendiratta_M/0/1/0/all/0/1\">Mohit Mendiratta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1\">Hans-Peter Seidel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ayush Tewari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>",
          "description": "We introduce a method for egocentric videoconferencing that enables\nhands-free video calls, for instance by people wearing smart glasses or other\nmixed-reality devices. Videoconferencing portrays valuable non-verbal\ncommunication and face expression cues, but usually requires a front-facing\ncamera. Using a frontal camera in a hands-free setting when a person is on the\nmove is impractical. Even holding a mobile phone camera in the front of the\nface while sitting for a long duration is not convenient. To overcome these\nissues, we propose a low-cost wearable egocentric camera setup that can be\nintegrated into smart glasses. Our goal is to mimic a classical video call, and\ntherefore, we transform the egocentric perspective of this camera into a front\nfacing video. To this end, we employ a conditional generative adversarial\nneural network that learns a transition from the highly distorted egocentric\nviews to frontal views common in videoconferencing. Our approach learns to\ntransfer expression details directly from the egocentric view without using a\ncomplex intermediate parametric expressions model, as it is used by related\nface reenactment methods. We successfully handle subtle expressions, not easily\ncaptured by parametric blendshape-based solutions, e.g., tongue movement, eye\nmovements, eye blinking, strong expressions and depth varying movements. To get\ncontrol over the rigid head movements in the target view, we condition the\ngenerator on synthetic renderings of a moving neutral face. This allows us to\nsynthesis results at different head poses. Our technique produces temporally\nsmooth video-realistic renderings in real-time using a video-to-video\ntranslation network in conjunction with a temporal discriminator. We\ndemonstrate the improved capabilities of our technique by comparing against\nrelated state-of-the art approaches.",
          "link": "http://arxiv.org/abs/2107.03109",
          "publishedOn": "2021-07-08T01:57:57.596Z",
          "wordCount": 732,
          "title": "Egocentric Videoconferencing. (arXiv:2107.03109v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>",
          "description": "Independently exploring unknown spaces or finding objects in an indoor\nenvironment is a daily but challenging task for visually impaired people.\nHowever, common 2D assistive systems lack depth relationships between various\nobjects, resulting in difficulty to obtain accurate spatial layout and relative\npositions of objects. To tackle these issues, we propose HIDA, a lightweight\nassistive system based on 3D point cloud instance segmentation with a\nsolid-state LiDAR sensor, for holistic indoor detection and avoidance. Our\nentire system consists of three hardware components, two interactive\nfunctions~(obstacle avoidance and object finding) and a voice user interface.\nBased on voice guidance, the point cloud from the most recent state of the\nchanging indoor environment is captured through an on-site scanning performed\nby the user. In addition, we design a point cloud segmentation model with dual\nlightweight decoders for semantic and offset predictions, which satisfies the\nefficiency of the whole system. After the 3D instance segmentation, we\npost-process the segmented point cloud by removing outliers and projecting all\npoints onto a top-view 2D map representation. The system integrates the\ninformation above and interacts with users intuitively by acoustic feedback.\nThe proposed 3D instance segmentation model has achieved state-of-the-art\nperformance on ScanNet v2 dataset. Comprehensive field tests with various tasks\nin a user study verify the usability and effectiveness of our system for\nassisting visually impaired people in holistic indoor understanding, obstacle\navoidance and object search.",
          "link": "http://arxiv.org/abs/2107.03180",
          "publishedOn": "2021-07-08T01:57:57.587Z",
          "wordCount": 705,
          "title": "HIDA: Towards Holistic Indoor Understanding for the Visually Impaired via Semantic Instance Segmentation with a Wearable Solid-State LiDAR Sensor. (arXiv:2107.03180v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>",
          "description": "Assigning meaning to parts of image data is the goal of semantic image\nsegmentation. Machine learning methods, specifically supervised learning is\ncommonly used in a variety of tasks formulated as semantic segmentation. One of\nthe major challenges in the supervised learning approaches is expressing and\ncollecting the rich knowledge that experts have with respect to the meaning\npresent in the image data. Towards this, typically a fixed set of labels is\nspecified and experts are tasked with annotating the pixels, patches or\nsegments in the images with the given labels. In general, however, the set of\nclasses does not fully capture the rich semantic information present in the\nimages. For example, in medical imaging such as histology images, the different\nparts of cells could be grouped and sub-grouped based on the expertise of the\npathologist.\n\nTo achieve such a precise semantic representation of the concepts in the\nimage, we need access to the full depth of knowledge of the annotator. In this\nwork, we develop a novel approach to collect segmentation annotations from\nexperts based on psychometric testing. Our method consists of the psychometric\ntesting procedure, active query selection, query enhancement, and a deep metric\nlearning model to achieve a patch-level image embedding that allows for\nsemantic segmentation of images. We show the merits of our method with\nevaluation on the synthetically generated image, aerial image and histology\nimage.",
          "link": "http://arxiv.org/abs/2107.03212",
          "publishedOn": "2021-07-08T01:57:57.579Z",
          "wordCount": 670,
          "title": "Hierarchical Semantic Segmentation using Psychometric Learning. (arXiv:2107.03212v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>",
          "description": "Generalized Zero-Shot Learning (GZSL) is the task of leveraging semantic\ninformation (e.g., attributes) to recognize the seen and unseen samples, where\nunseen classes are not observable during training. It is natural to derive\ngenerative models and hallucinate training samples for unseen classes based on\nthe knowledge learned from the seen samples. However, most of these models\nsuffer from the `generation shifts', where the synthesized samples may drift\nfrom the real distribution of unseen data. In this paper, we conduct an\nin-depth analysis on this issue and propose a novel Generation Shifts\nMitigating Flow (GSMFlow) framework, which is comprised of multiple conditional\naffine coupling layers for learning unseen data synthesis efficiently and\neffectively. In particular, we identify three potential problems that trigger\nthe generation shifts, i.e., semantic inconsistency, variance decay, and\nstructural permutation and address them respectively. First, to reinforce the\ncorrelations between the generated samples and the respective attributes, we\nexplicitly embed the semantic information into the transformations in each of\nthe coupling layers. Second, to recover the intrinsic variance of the\nsynthesized unseen features, we introduce a visual perturbation strategy to\ndiversify the intra-class variance of generated data and hereby help adjust the\ndecision boundary of the classifier. Third, to avoid structural permutation in\nthe semantic space, we propose a relative positioning strategy to manipulate\nthe attribute embeddings, guiding which to fully preserve the inter-class\ngeometric structure. Experimental results demonstrate that GSMFlow achieves\nstate-of-the-art recognition performance in both conventional and generalized\nzero-shot settings. Our code is available at:\nhttps://github.com/uqzhichen/GSMFlow",
          "link": "http://arxiv.org/abs/2107.03163",
          "publishedOn": "2021-07-08T01:57:57.571Z",
          "wordCount": 694,
          "title": "Mitigating Generation Shifts for Generalized Zero-Shot Learning. (arXiv:2107.03163v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1\">Junya Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_X/0/1/0/all/0/1\">Xiaoyu Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_A/0/1/0/all/0/1\">Akiyoshi Uchida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youoku_S/0/1/0/all/0/1\">Sachihiro Youoku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_T/0/1/0/all/0/1\">Takahisa Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murase_K/0/1/0/all/0/1\">Kentaro Murase</a>",
          "description": "Facial Action Units (AUs) represent a set of facial muscular activities and\nvarious combinations of AUs can represent a wide range of emotions. AU\nrecognition is often used in many applications, including marketing,\nhealthcare, education, and so forth. Although a lot of studies have developed\nvarious methods to improve recognition accuracy, it still remains a major\nchallenge for AU recognition. In the Affective Behavior Analysis in-the-wild\n(ABAW) 2020 competition, we proposed a new automatic Action Units (AUs)\nrecognition method using a pairwise deep architecture to derive the\nPseudo-Intensities of each AU and then convert them into predicted intensities.\nThis year, we introduced a new technique to last year's framework to further\nreduce AU recognition errors due to temporary face occlusion such as temporary\nface occlusion such as face hiding or large face orientation. We obtained a\nscore of 0.65 in the validation data set for this year's competition.",
          "link": "http://arxiv.org/abs/2107.03143",
          "publishedOn": "2021-07-08T01:57:57.551Z",
          "wordCount": 589,
          "title": "Action Units Recognition Using Improved Pairwise Deep Architecture. (arXiv:2107.03143v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03172",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantinescu_A/0/1/0/all/0/1\">Angela Constantinescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Karin M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>",
          "description": "Common fully glazed facades and transparent objects present architectural\nbarriers and impede the mobility of people with low vision or blindness, for\ninstance, a path detected behind a glass door is inaccessible unless it is\ncorrectly perceived and reacted. However, segmenting these safety-critical\nobjects is rarely covered by conventional assistive technologies. To tackle\nthis issue, we construct a wearable system with a novel dual-head Transformer\nfor Transparency (Trans4Trans) model, which is capable of segmenting general\nand transparent objects and performing real-time wayfinding to assist people\nwalking alone more safely. Especially, both decoders created by our proposed\nTransformer Parsing Module (TPM) enable effective joint learning from different\ndatasets. Besides, the efficient Trans4Trans model composed of symmetric\ntransformer-based encoder and decoder, requires little computational expenses\nand is readily deployed on portable GPUs. Our Trans4Trans model outperforms\nstate-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2\ndatasets and obtains mIoU of 45.13% and 75.14%, respectively. Through various\npre-tests and a user study conducted in indoor and outdoor scenarios, the\nusability and reliability of our assistive system have been extensively\nverified.",
          "link": "http://arxiv.org/abs/2107.03172",
          "publishedOn": "2021-07-08T01:57:57.544Z",
          "wordCount": 648,
          "title": "Trans4Trans: Efficient Transformer for Transparent Object Segmentation to Help Visually Impaired People Navigate in the Real World. (arXiv:2107.03172v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>",
          "description": "In this paper, we tackle the problem of novel visual category discovery,\ni.e., grouping unlabelled images from new classes into different semantic\npartitions by leveraging a labelled dataset that contains images from other\ndifferent but relevant categories. This is a more realistic and challenging\nsetting than conventional semi-supervised learning. We propose a two-branch\nlearning framework for this problem, with one branch focusing on local\npart-level information and the other branch focusing on overall\ncharacteristics. To transfer knowledge from the labelled data to the\nunlabelled, we propose using dual ranking statistics on both branches to\ngenerate pseudo labels for training on the unlabelled data. We further\nintroduce a mutual knowledge distillation method to allow information exchange\nand encourage agreement between the two branches for discovering new\ncategories, allowing our model to enjoy the benefits of global and local\nfeatures. We comprehensively evaluate our method on public benchmarks for\ngeneric object classification, as well as the more challenging datasets for\nfine-grained visual recognition, achieving state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2107.03358",
          "publishedOn": "2021-07-08T01:57:57.530Z",
          "wordCount": 604,
          "title": "Novel Visual Category Discovery with Dual Ranking Statistics and Mutual Knowledge Distillation. (arXiv:2107.03358v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02571",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Stafylakis_T/0/1/0/all/0/1\">Themos Stafylakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rohdin_J/0/1/0/all/0/1\">Johan Rohdin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burget_L/0/1/0/all/0/1\">Lukas Burget</a>",
          "description": "Speaker embeddings extracted with deep 2D convolutional neural networks are\ntypically modeled as projections of first and second order statistics of\nchannel-frequency pairs onto a linear layer, using either average or attentive\npooling along the time axis. In this paper we examine an alternative pooling\nmethod, where pairwise correlations between channels for given frequencies are\nused as statistics. The method is inspired by style-transfer methods in\ncomputer vision, where the style of an image, modeled by the matrix of\nchannel-wise correlations, is transferred to another image, in order to produce\na new image having the style of the first and the content of the second. By\ndrawing analogies between image style and speaker characteristics, and between\nimage content and phonetic sequence, we explore the use of such channel-wise\ncorrelations features to train a ResNet architecture in an end-to-end fashion.\nOur experiments on VoxCeleb demonstrate the effectiveness of the proposed\npooling method in speaker recognition.",
          "link": "http://arxiv.org/abs/2104.02571",
          "publishedOn": "2021-07-08T01:57:57.522Z",
          "wordCount": 615,
          "title": "Speaker embeddings by modeling channel-wise correlations. (arXiv:2104.02571v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kaiwen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gongjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>",
          "description": "Generative Adversarial Networks (GANs) have become the de-facto standard in\nimage synthesis. However, without considering the foreground-background\ndecomposition, existing GANs tend to capture excessive content correlation\nbetween foreground and background, thus constraining the diversity in image\ngeneration. This paper presents a novel Foreground-Background Composition GAN\n(FBC-GAN) that performs image generation by generating foreground objects and\nbackground scenes concurrently and independently, followed by composing them\nwith style and geometrical consistency. With this explicit design, FBC-GAN can\ngenerate images with foregrounds and backgrounds that are mutually independent\nin contents, thus lifting the undesirably learned content correlation\nconstraint and achieving superior diversity. It also provides excellent\nflexibility by allowing the same foreground object with different background\nscenes, the same background scene with varying foreground objects, or the same\nforeground object and background scene with different object positions, sizes\nand poses. It can compose foreground objects and background scenes sampled from\ndifferent datasets as well. Extensive experiments over multiple datasets show\nthat FBC-GAN achieves competitive visual realism and superior diversity as\ncompared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.03166",
          "publishedOn": "2021-07-08T01:57:57.502Z",
          "wordCount": 610,
          "title": "FBC-GAN: Diverse and Flexible Image Synthesis via Foreground-Background Composition. (arXiv:2107.03166v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guillory_D/0/1/0/all/0/1\">Devin Guillory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1\">Vaishaal Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>",
          "description": "Recent work has shown that the performance of machine learning models can\nvary substantially when models are evaluated on data drawn from a distribution\nthat is close to but different from the training distribution. As a result,\npredicting model performance on unseen distributions is an important challenge.\nOur work connects techniques from domain adaptation and predictive uncertainty\nliterature, and allows us to predict model accuracy on challenging unseen\ndistributions without access to labeled data. In the context of distribution\nshift, distributional distances are often used to adapt models and improve\ntheir performance on new domains, however accuracy estimation, or other forms\nof predictive uncertainty, are often neglected in these investigations. Through\ninvestigating a wide range of established distributional distances, such as\nFrechet distance or Maximum Mean Discrepancy, we determine that they fail to\ninduce reliable estimates of performance under distribution shift. On the other\nhand, we find that the difference of confidences (DoC) of a classifier's\npredictions successfully estimates the classifier's performance change over a\nvariety of shifts. We specifically investigate the distinction between\nsynthetic and natural distribution shifts and observe that despite its\nsimplicity DoC consistently outperforms other quantifications of distributional\ndifference. $DoC$ reduces predictive error by almost half ($46\\%$) on several\nrealistic and challenging distribution shifts, e.g., on the ImageNet-Vid-Robust\nand ImageNet-Rendition datasets.",
          "link": "http://arxiv.org/abs/2107.03315",
          "publishedOn": "2021-07-08T01:57:57.493Z",
          "wordCount": 657,
          "title": "Predicting with Confidence on Unseen Distributions. (arXiv:2107.03315v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yante Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jinsheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadifoumani_S/0/1/0/all/0/1\">Seyednavid Mohammadifoumani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>",
          "description": "Micro-expressions (MEs) are involuntary facial movements revealing people's\nhidden feelings in high-stake situations and have practical importance in\nmedical treatment, national security, interrogations and many human-computer\ninteraction systems. Early methods for MER mainly based on traditional\nappearance and geometry features. Recently, with the success of deep learning\n(DL) in various fields, neural networks have received increasing interests in\nMER. Different from macro-expressions, MEs are spontaneous, subtle, and rapid\nfacial movements, leading to difficult data collection, thus have small-scale\ndatasets. DL based MER becomes challenging due to above ME characters. To data,\nvarious DL approaches have been proposed to solve the ME issues and improve MER\nperformance. In this survey, we provide a comprehensive review of deep\nmicro-expression recognition (MER), including datasets, deep MER pipeline, and\nthe bench-marking of most influential methods. This survey defines a new\ntaxonomy for the field, encompassing all aspects of MER based on DL. For each\naspect, the basic approaches and advanced developments are summarized and\ndiscussed. In addition, we conclude the remaining challenges and and potential\ndirections for the design of robust deep MER systems. To the best of our\nknowledge, this is the first survey of deep MER methods, and this survey can\nserve as a reference point for future MER research.",
          "link": "http://arxiv.org/abs/2107.02823",
          "publishedOn": "2021-07-08T01:57:57.486Z",
          "wordCount": 652,
          "title": "Deep Learning based Micro-expression Recognition: A Survey. (arXiv:2107.02823v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanyi Hu</a>",
          "description": "Many recent works show that a spatial manipulation module could boost the\nperformances of deep neural networks (DNNs) for 3D point cloud analysis. In\nthis paper, we aim to provide an insight into spatial manipulation modules.\nFirstly, we find that the smaller the rotational degree of freedom (RDF) of\nobjects is, the more easily these objects are handled by these DNNs. Then, we\ninvestigate the effect of the popular T-Net module and find that it could not\nreduce the RDF of objects. Motivated by the above two issues, we propose a\nrotation transformation network for point cloud analysis, called RTN, which\ncould reduce the RDF of input 3D objects to 0. The RTN could be seamlessly\ninserted into many existing DNNs for point cloud analysis. Extensive\nexperimental results on 3D point cloud classification and segmentation tasks\ndemonstrate that the proposed RTN could improve the performances of several\nstate-of-the-art methods significantly.",
          "link": "http://arxiv.org/abs/2107.03105",
          "publishedOn": "2021-07-08T01:57:57.474Z",
          "wordCount": 604,
          "title": "Rotation Transformation Network: Learning View-Invariant Point Cloud for Classification and Segmentation. (arXiv:2107.03105v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pino_O/0/1/0/all/0/1\">Omar Vidal Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1\">Erickson Rangel Nascimento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_M/0/1/0/all/0/1\">Mario Fernando Montenegro Campos</a>",
          "description": "In this paper, we hypothesize that the effects of the degree of typicality in\nnatural semantic categories can be generated based on the structure of\nartificial categories learned with deep learning models. Motivated by the human\napproach to representing natural semantic categories and based on the Prototype\nTheory foundations, we propose a novel Computational Prototype Model (CPM) to\nrepresent the internal structure of semantic categories. Unlike other prototype\nlearning approaches, our mathematical framework proposes a first approach to\nprovide deep neural networks with the ability to model abstract semantic\nconcepts such as category central semantic meaning, typicality degree of an\nobject's image, and family resemblance relationship. We proposed several\nmethodologies based on the typicality's concept to evaluate our CPM-model in\nimage semantic processing tasks such as image classification, a global semantic\ndescription, and transfer learning. Our experiments on different image\ndatasets, such as ImageNet and Coco, showed that our approach might be an\nadmissible proposition in the effort to endow machines with greater power of\nabstraction for the semantic representation of objects' categories.",
          "link": "http://arxiv.org/abs/2107.03279",
          "publishedOn": "2021-07-08T01:57:57.465Z",
          "wordCount": 660,
          "title": "Introducing the structural bases of typicality effects in deep learning. (arXiv:2107.03279v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1\">Erin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>",
          "description": "Data imbalance is a ubiquitous problem in machine learning. In large scale\ncollected and annotated datasets, data imbalance is either mitigated manually\nby undersampling frequent classes and oversampling rare classes, or planned for\nwith imputation and augmentation techniques. In both cases balancing data\nrequires labels. In other words, only annotated data can be balanced.\nCollecting fully annotated datasets is challenging, especially for large scale\nsatellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.\nAlthough the NASA Earth Imagery dataset is unlabeled, there are implicit\nproperties of the data source that we can rely on to hypothesize about its\nimbalance, such as distribution of land and water in the case of the Earth's\nimagery. We present a new iterative method to balance unlabeled data. Our\nmethod utilizes image embeddings as a proxy for image labels that can be used\nto balance data, and ultimately when trained increases overall accuracy.",
          "link": "http://arxiv.org/abs/2107.03227",
          "publishedOn": "2021-07-08T01:57:57.441Z",
          "wordCount": 614,
          "title": "Scalable Data Balancing for Unlabeled Satellite Imagery. (arXiv:2107.03227v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zibin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiyu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shutao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Maowei Hu</a>",
          "description": "Compared with tedious per-pixel mask annotating, it is much easier to\nannotate data by clicks, which costs only several seconds for an image.\nHowever, applying clicks to learn video semantic segmentation model has not\nbeen explored before. In this work, we propose an effective weakly-supervised\nvideo semantic segmentation pipeline with click annotations, called WeClick,\nfor saving laborious annotating effort by segmenting an instance of the\nsemantic class with only a single click. Since detailed semantic information is\nnot captured by clicks, directly training with click labels leads to poor\nsegmentation predictions. To mitigate this problem, we design a novel memory\nflow knowledge distillation strategy to exploit temporal information (named\nmemory flow) in abundant unlabeled video frames, by distilling the neighboring\npredictions to the target frame via estimated motion. Moreover, we adopt\nvanilla knowledge distillation for model compression. In this case, WeClick\nlearns compact video semantic segmentation models with the low-cost click\nannotations during the training phase yet achieves real-time and accurate\nmodels during the inference period. Experimental results on Cityscapes and\nCamvid show that WeClick outperforms the state-of-the-art methods, increases\nperformance by 10.24% mIoU than baseline, and achieves real-time execution.",
          "link": "http://arxiv.org/abs/2107.03088",
          "publishedOn": "2021-07-08T01:57:57.432Z",
          "wordCount": 644,
          "title": "WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations. (arXiv:2107.03088v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Ling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaben Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>",
          "description": "We present a new solution to tracking and mapping with an event camera. The\nmotion of the camera contains both rotation and translation, and the\ndisplacements happen in an arbitrarily structured environment. As a result, the\nimage matching may no longer be represented by a low-dimensional homographic\nwarping, thus complicating an application of the commonly used Image of Warped\nEvents (IWE). We introduce a new solution to this problem by performing\ncontrast maximization in 3D. The 3D location of the rays cast for each event is\nsmoothly varied as a function of a continuous-time motion parametrization, and\nthe optimal parameters are found by maximizing the contrast in a volumetric ray\ndensity field. Our method thus performs joint optimization over motion and\nstructure. The practical validity of our approach is supported by an\napplication to AGV motion estimation and 3D reconstruction with a single\nvehicle-mounted event camera. The method approaches the performance obtained\nwith regular cameras, and eventually outperforms in challenging visual\nconditions.",
          "link": "http://arxiv.org/abs/2107.03011",
          "publishedOn": "2021-07-08T01:57:57.425Z",
          "wordCount": 618,
          "title": "Visual Odometry with an Event Camera Using Continuous Ray Warping and Volumetric Contrast Maximization. (arXiv:2107.03011v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.07290",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ramzi_Z/0/1/0/all/0/1\">Zaccharie Ramzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ciuciu_P/0/1/0/all/0/1\">Philippe Ciuciu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Starck_J/0/1/0/all/0/1\">Jean-Luc Starck</a>",
          "description": "We present a new neural network, the XPDNet, for MRI reconstruction from\nperiodically under-sampled multi-coil data. We inform the design of this\nnetwork by taking best practices from MRI reconstruction and computer vision.\nWe show that this network can achieve state-of-the-art reconstruction results,\nas shown by its ranking of second in the fastMRI 2020 challenge.",
          "link": "http://arxiv.org/abs/2010.07290",
          "publishedOn": "2021-07-08T01:57:57.418Z",
          "wordCount": 549,
          "title": "XPDNet for MRI Reconstruction: an application to the 2020 fastMRI challenge. (arXiv:2010.07290v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaodong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Junbao Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>",
          "description": "Semi-supervised domain adaptation (SSDA) aims to solve tasks in target domain\nby utilizing transferable information learned from the available source domain\nand a few labeled target data. However, source data is not always accessible in\npractical scenarios, which restricts the application of SSDA in real world\ncircumstances. In this paper, we propose a novel task named Semi-supervised\nSource Hypothesis Transfer (SSHT), which performs domain adaptation based on\nsource trained model, to generalize well in target domain with a few\nsupervisions. In SSHT, we are facing two challenges: (1) The insufficient\nlabeled target data may result in target features near the decision boundary,\nwith the increased risk of mis-classification; (2) The data are usually\nimbalanced in source domain, so the model trained with these data is biased.\nThe biased model is prone to categorize samples of minority categories into\nmajority ones, resulting in low prediction diversity. To tackle the above\nissues, we propose Consistency and Diversity Learning (CDL), a simple but\neffective framework for SSHT by facilitating prediction consistency between two\nrandomly augmented unlabeled data and maintaining the prediction diversity when\nadapting model to target domain. Encouraging consistency regularization brings\ndifficulty to memorize the few labeled target data and thus enhances the\ngeneralization ability of the learned model. We further integrate Batch\nNuclear-norm Maximization into our method to enhance the discriminability and\ndiversity. Experimental results show that our method outperforms existing SSDA\nmethods and unsupervised model adaptation methods on DomainNet, Office-Home and\nOffice-31 datasets. The code is available at\nhttps://github.com/Wang-xd1899/SSHT.",
          "link": "http://arxiv.org/abs/2107.03008",
          "publishedOn": "2021-07-08T01:57:57.410Z",
          "wordCount": 700,
          "title": "Learning Invariant Representation with Consistency and Diversity for Semi-supervised Source Hypothesis Transfer. (arXiv:2107.03008v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.05348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>",
          "description": "Human beings can recognize new objects with only a few labeled examples,\nhowever, few-shot learning remains a challenging problem for machine learning\nsystems. Most previous algorithms in few-shot learning only utilize spatial\ninformation of the images. In this paper, we propose to integrate the frequency\ninformation into the learning model to boost the discrimination ability of the\nsystem. We employ Discrete Cosine Transformation (DCT) to generate the\nfrequency representation, then, integrate the features from both the spatial\ndomain and frequency domain for classification. The proposed strategy and its\neffectiveness are validated with different backbones, datasets, and algorithms.\nExtensive experiments demonstrate that the frequency information is\ncomplementary to the spatial representations in few-shot classification. The\nclassification accuracy is boosted significantly by integrating features from\nboth the spatial and frequency domains in different few-shot learning tasks.",
          "link": "http://arxiv.org/abs/2105.05348",
          "publishedOn": "2021-07-08T01:57:57.389Z",
          "wordCount": 598,
          "title": "Few-Shot Learning by Integrating Spatial and Frequency Representation. (arXiv:2105.05348v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanbin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hailin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linfang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinglu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "The performance of human pose estimation depends on the spatial accuracy of\nkeypoint localization. Most existing methods pursue the spatial accuracy\nthrough learning the high-resolution (HR) representation from input images. By\nthe experimental analysis, we find that the HR representation leads to a sharp\nincrease of computational cost, while the accuracy improvement remains marginal\ncompared with the low-resolution (LR) representation. In this paper, we propose\na design paradigm for cost-effective network with LR representation for\nefficient pose estimation, named FasterPose. Whereas the LR design largely\nshrinks the model complexity, yet how to effectively train the network with\nrespect to the spatial accuracy is a concomitant challenge. We study the\ntraining behavior of FasterPose, and formulate a novel regressive cross-entropy\n(RCE) loss function for accelerating the convergence and promoting the\naccuracy. The RCE loss generalizes the ordinary cross-entropy loss from the\nbinary supervision to a continuous range, thus the training of pose estimation\nnetwork is able to benefit from the sigmoid function. By doing so, the output\nheatmap can be inferred from the LR features without loss of spatial accuracy,\nwhile the computational cost and model size has been significantly reduced.\nCompared with the previously dominant network of pose estimation, our method\nreduces 58% of the FLOPs and simultaneously gains 1.3% improvement of accuracy.\nExtensive experiments show that FasterPose yields promising results on the\ncommon benchmarks, i.e., COCO and MPII, consistently validating the\neffectiveness and efficiency for practical utilization, especially the\nlow-latency and low-energy-budget applications in the non-GPU scenarios.",
          "link": "http://arxiv.org/abs/2107.03215",
          "publishedOn": "2021-07-08T01:57:57.382Z",
          "wordCount": 696,
          "title": "FasterPose: A Faster Simple Baseline for Human Pose Estimation. (arXiv:2107.03215v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Haiwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuning He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kejia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_B/0/1/0/all/0/1\">Bo Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chunling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kun Shi</a>",
          "description": "Medical Visual Question Answering (VQA) is a multi-modal challenging task\nwidely considered by research communities of the computer vision and natural\nlanguage processing. Since most current medical VQA models focus on visual\ncontent, ignoring the importance of text, this paper proposes a multi-view\nattention-based model(MuVAM) for medical visual question answering which\nintegrates the high-level semantics of medical images on the basis of text\ndescription. Firstly, different methods are utilized to extract the features of\nthe image and the question for the two modalities of vision and text. Secondly,\nthis paper proposes a multi-view attention mechanism that include\nImage-to-Question (I2Q) attention and Word-to-Text (W2T) attention. Multi-view\nattention can correlate the question with image and word in order to better\nanalyze the question and get an accurate answer. Thirdly, a composite loss is\npresented to predict the answer accurately after multi-modal feature fusion and\nimprove the similarity between visual and textual cross-modal features. It\nconsists of classification loss and image-question complementary (IQC) loss.\nFinally, for data errors and missing labels in the VQA-RAD dataset, we\ncollaborate with medical experts to correct and complete this dataset and then\nconstruct an enhanced dataset, VQA-RADPh. The experiments on these two datasets\nshow that the effectiveness of MuVAM surpasses the state-of-the-art method.",
          "link": "http://arxiv.org/abs/2107.03216",
          "publishedOn": "2021-07-08T01:57:57.363Z",
          "wordCount": 650,
          "title": "MuVAM: A Multi-View Attention-based Model for Medical Visual Question Answering. (arXiv:2107.03216v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chengzhi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yanzhou Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haiwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haijun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>",
          "description": "Existing classification-based face recognition methods have achieved\nremarkable progress, introducing large margin into hypersphere manifold to\nlearn discriminative facial representations. However, the feature distribution\nis ignored. Poor feature distribution will wipe out the performance improvement\nbrought about by margin scheme. Recent studies focus on the unbalanced\ninter-class distribution and form a equidistributed feature representations by\npenalizing the angle between identity and its nearest neighbor. But the problem\nis more than that, we also found the anisotropy of intra-class distribution. In\nthis paper, we propose the `gradient-enhancing term' that concentrates on the\ndistribution characteristics within the class. This method, named IntraLoss,\nexplicitly performs gradient enhancement in the anisotropic region so that the\nintra-class distribution continues to shrink, resulting in isotropic and more\ncompact intra-class distribution and further margin between identities. The\nexperimental results on LFW, YTF and CFP-FP show that our outperforms\nstate-of-the-art methods by gradient enhancement, demonstrating the superiority\nof our method. In addition, our method has intuitive geometric interpretation\nand can be easily combined with existing methods to solve the previously\nignored problems.",
          "link": "http://arxiv.org/abs/2107.03352",
          "publishedOn": "2021-07-08T01:57:57.355Z",
          "wordCount": 618,
          "title": "IntraLoss: Further Margin via Gradient-Enhancing Term for Deep Face Recognition. (arXiv:2107.03352v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenhongyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiaofei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zhengjun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>",
          "description": "Deep learning-based dense object detectors have achieved great success in the\npast few years and have been applied to numerous multimedia applications such\nas video understanding. However, the current training pipeline for dense\ndetectors is compromised to lots of conjunctions that may not hold. In this\npaper, we investigate three such important conjunctions: 1) only samples\nassigned as positive in classification head are used to train the regression\nhead; 2) classification and regression share the same input feature and\ncomputational fields defined by the parallel head architecture; and 3) samples\ndistributed in different feature pyramid layers are treated equally when\ncomputing the loss. We first carry out a series of pilot experiments to show\ndisentangling such conjunctions can lead to persistent performance improvement.\nThen, based on these findings, we propose Disentangled Dense Object Detector\n(DDOD), in which simple and effective disentanglement mechanisms are designed\nand integrated into the current state-of-the-art dense object detectors.\nExtensive experiments on MS COCO benchmark show that our approach can lead to\n2.0 mAP, 2.4 mAP and 2.2 mAP absolute improvements on RetinaNet, FCOS, and ATSS\nbaselines with negligible extra overhead. Notably, our best model reaches 55.0\nmAP on the COCO test-dev set and 93.5 AP on the hard subset of WIDER FACE,\nachieving new state-of-the-art performance on these two competitive benchmarks.\nCode is available at https://github.com/zehuichen123/DDOD.",
          "link": "http://arxiv.org/abs/2107.02963",
          "publishedOn": "2021-07-08T01:57:57.332Z",
          "wordCount": 656,
          "title": "Disentangle Your Dense Object Detector. (arXiv:2107.02963v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03323",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cvetko_T/0/1/0/all/0/1\">Tim Cvetko</a>",
          "description": "Brain tumor segmentation is a challenging problem in medical image analysis.\nThe endpoint is to generate the salient masks that accurately identify brain\ntumor regions in an fMRI screening. In this paper, we propose a novel attention\ngate (AG model) for brain tumor segmentation that utilizes both the edge\ndetecting unit and the attention gated network to highlight and segment the\nsalient regions from fMRI images. This feature enables us to eliminate the\nnecessity of having to explicitly point towards the damaged area(external\ntissue localization) and classify(classification) as per classical computer\nvision techniques. AGs can easily be integrated within the deep convolutional\nneural networks(CNNs). Minimal computional overhead is required while the AGs\nincrease the sensitivity scores significantly. We show that the edge detector\nalong with an attention gated mechanism provide a sufficient enough method for\nbrain segmentation reaching an IOU of 0.78",
          "link": "http://arxiv.org/abs/2107.03323",
          "publishedOn": "2021-07-08T01:57:57.304Z",
          "wordCount": 594,
          "title": "AGD-Autoencoder: Attention Gated Deep Convolutional Autoencoder for Brain Tumor Segmentation. (arXiv:2107.03323v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>",
          "description": "Blind image super-resolution (SR), aiming to super-resolve low-resolution\nimages with unknown degradation, has attracted increasing attention due to its\nsignificance in promoting real-world applications. Many novel and effective\nsolutions have been proposed recently, especially with the powerful deep\nlearning techniques. Despite years of efforts, it still remains as a\nchallenging research problem. This paper serves as a systematic review on\nrecent progress in blind image SR, and proposes a taxonomy to categorize\nexisting methods into three different classes according to their ways of\ndegradation modelling and the data used for solving the SR model. This taxonomy\nhelps summarize and distinguish among existing methods. We hope to provide\ninsights into current research states, as well as to reveal novel research\ndirections worth exploring. In addition, we make a summary on commonly used\ndatasets and previous competitions related to blind image SR. Last but not\nleast, a comparison among different methods is provided with detailed analysis\non their merits and demerits using both synthetic and real testing images.",
          "link": "http://arxiv.org/abs/2107.03055",
          "publishedOn": "2021-07-08T01:57:57.286Z",
          "wordCount": 602,
          "title": "Blind Image Super-Resolution: A Survey and Beyond. (arXiv:2107.03055v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Waters_E/0/1/0/all/0/1\">Emily Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oghaz_M/0/1/0/all/0/1\">Mahdi Maktabdar Oghaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saheer_L/0/1/0/all/0/1\">Lakshmi Babu Saheer</a>",
          "description": "Urban trees help regulate temperature, reduce energy consumption, improve\nurban air quality, reduce wind speeds, and mitigating the urban heat island\neffect. Urban trees also play a key role in climate change mitigation and\nglobal warming by capturing and storing atmospheric carbon-dioxide which is the\nlargest contributor to greenhouse gases. Automated tree detection and species\nclassification using aerial imagery can be a powerful tool for sustainable\nforest and urban tree management. Hence, This study first offers a pipeline for\ngenerating labelled dataset of urban trees using Google Map's aerial images and\nthen investigates how state of the art deep Convolutional Neural Network models\nsuch as VGG and ResNet handle the classification problem of urban tree aerial\nimages under different parameters. Experimental results show our best model\nachieves an average accuracy of 60% over 6 tree species.",
          "link": "http://arxiv.org/abs/2107.03182",
          "publishedOn": "2021-07-08T01:57:57.258Z",
          "wordCount": 594,
          "title": "Urban Tree Species Classification Using Aerial Imagery. (arXiv:2107.03182v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kaiwen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Generative adversarial networks (GANs) have achieved great success in image\ntranslation and manipulation. However, high-fidelity image generation with\nfaithful style control remains a grand challenge in computer vision. This paper\npresents a versatile image translation and manipulation framework that achieves\naccurate semantic and style guidance in image generation by explicitly building\na correspondence. To handle the quadratic complexity incurred by building the\ndense correspondences, we introduce a bi-level feature alignment strategy that\nadopts a top-$k$ operation to rank block-wise features followed by dense\nattention between block features which reduces memory cost substantially. As\nthe top-$k$ operation involves index swapping which precludes the gradient\npropagation, we propose to approximate the non-differentiable top-$k$ operation\nwith a regularized earth mover's problem so that its gradient can be\neffectively back-propagated. In addition, we design a novel semantic position\nencoding mechanism that builds up coordinate for each individual semantic\nregion to preserve texture structures while building correspondences. Further,\nwe design a novel confidence feature injection module which mitigates mismatch\nproblem by fusing features adaptively according to the reliability of built\ncorrespondences. Extensive experiments show that our method achieves superior\nperformance qualitatively and quantitatively as compared with the\nstate-of-the-art. The code is available at\n\\href{https://github.com/fnzhan/RABIT}{https://github.com/fnzhan/RABIT}.",
          "link": "http://arxiv.org/abs/2107.03021",
          "publishedOn": "2021-07-08T01:57:57.233Z",
          "wordCount": 650,
          "title": "Bi-level Feature Alignment for Versatile Image Translation and Manipulation. (arXiv:2107.03021v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03292",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Asvadi_A/0/1/0/all/0/1\">Alireza Asvadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dardenne_G/0/1/0/all/0/1\">Guillaume Dardenne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Troccaz_J/0/1/0/all/0/1\">Jocelyne Troccaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burdin_V/0/1/0/all/0/1\">Valerie Burdin</a>",
          "description": "In this study, we investigated a method allowing the determination of the\nfemur bone surface as well as its mechanical axis from some easy-to-identify\nbony landmarks. The reconstruction of the whole femur is therefore performed\nfrom these landmarks using a Statistical Shape Model (SSM). The aim of this\nresearch is therefore to assess the impact of the number, the position, and the\naccuracy of the landmarks for the reconstruction of the femur and the\ndetermination of its related mechanical axis, an important clinical parameter\nto consider for the lower limb analysis. Two statistical femur models were\ncreated from our in-house dataset and a publicly available dataset. Both were\nevaluated in terms of average point-to-point surface distance error and through\nthe mechanical axis of the femur. Furthermore, the clinical impact of using\nlandmarks on the skin in replacement of bony landmarks is investigated. The\npredicted proximal femurs from bony landmarks were more accurate compared to\non-skin landmarks while both had less than 3.5 degrees mechanical axis angle\ndeviation error. The results regarding the non-invasive determination of the\nmechanical axis are very encouraging and could open very interesting clinical\nperspectives for the analysis of the lower limb either for orthopedics or\nfunctional rehabilitation.",
          "link": "http://arxiv.org/abs/2107.03292",
          "publishedOn": "2021-07-08T01:57:57.226Z",
          "wordCount": 674,
          "title": "Bone Surface Reconstruction and Clinical Features Estimation from Sparse Landmarks and Statistical Shape Models: A feasibility study on the femur. (arXiv:2107.03292v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hattori_S/0/1/0/all/0/1\">Shota Hattori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yatagawa_T/0/1/0/all/0/1\">Tatsuya Yatagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohtake_Y/0/1/0/all/0/1\">Yutaka Ohtake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hiromasa Suzuki</a>",
          "description": "This paper addresses mesh restoration problems, i.e., denoising and\ncompletion, by learning self-similarity in an unsupervised manner. For this\npurpose, the proposed method, which we refer to as Deep Mesh Prior, uses a\ngraph convolutional network on meshes to learn the self-similarity. The network\ntakes a single incomplete mesh as input data and directly outputs the\nreconstructed mesh without being trained using large-scale datasets. Our method\ndoes not use any intermediate representations such as an implicit field because\nthe whole process works on a mesh. We demonstrate that our unsupervised method\nperforms equally well or even better than the state-of-the-art methods using\nlarge-scale datasets.",
          "link": "http://arxiv.org/abs/2107.02909",
          "publishedOn": "2021-07-08T01:57:57.205Z",
          "wordCount": 555,
          "title": "Deep Mesh Prior: Unsupervised Mesh Restoration using Graph Convolutional Networks. (arXiv:2107.02909v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cleveston_I/0/1/0/all/0/1\">Iury Cleveston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombini_E/0/1/0/all/0/1\">Esther L. Colombini</a>",
          "description": "Building vehicles capable of operating without human supervision requires the\ndetermination of the agent's pose. Visual Odometry (VO) algorithms estimate the\negomotion using only visual changes from the input images. The most recent VO\nmethods implement deep-learning techniques using convolutional neural networks\n(CNN) extensively, which add a substantial cost when dealing with\nhigh-resolution images. Furthermore, in VO tasks, more input data does not mean\na better prediction; on the contrary, the architecture may filter out useless\ninformation. Therefore, the implementation of computationally efficient and\nlightweight architectures is essential. In this work, we propose the RAM-VO, an\nextension of the Recurrent Attention Model (RAM) for visual odometry tasks.\nRAM-VO improves the visual and temporal representation of information and\nimplements the Proximal Policy Optimization (PPO) algorithm to learn robust\npolicies. The results indicate that RAM-VO can perform regressions with six\ndegrees of freedom from monocular input images using approximately 3 million\nparameters. In addition, experiments on the KITTI dataset demonstrate that\nRAM-VO achieves competitive results using only 5.7% of the available visual\ninformation.",
          "link": "http://arxiv.org/abs/2107.02974",
          "publishedOn": "2021-07-08T01:57:57.166Z",
          "wordCount": 608,
          "title": "RAM-VO: Less is more in Visual Odometry. (arXiv:2107.02974v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1\">Monty Santarossa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_L/0/1/0/all/0/1\">Lukas Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelenka_C/0/1/0/all/0/1\">Claudius Zelenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_U/0/1/0/all/0/1\">Uwe Franke</a>",
          "description": "Stixels have been successfully applied to a wide range of vision tasks in\nautonomous driving, recently including instance segmentation. However, due to\ntheir sparse occurrence in the image, until now Stixels seldomly served as\ninput for Deep Learning algorithms, restricting their utility for such\napproaches. In this work we present StixelPointNet, a novel method to perform\nfast instance segmentation directly on Stixels. By regarding the Stixel\nrepresentation as unstructured data similar to point clouds, architectures like\nPointNet are able to learn features from Stixels. We use a bounding box\ndetector to propose candidate instances, for which the relevant Stixels are\nextracted from the input image. On these Stixels, a PointNet models learns\nbinary segmentations, which we then unify throughout the whole image in a final\nselection step. StixelPointNet achieves state-of-the-art performance on\nStixel-level, is considerably faster than pixel-based segmentation methods, and\nshows that with our approach the Stixel domain can be introduced to many new 3D\nDeep Learning tasks.",
          "link": "http://arxiv.org/abs/2107.03070",
          "publishedOn": "2021-07-08T01:57:56.949Z",
          "wordCount": 600,
          "title": "Learning Stixel-based Instance Segmentation. (arXiv:2107.03070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03035",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1\">Xinghua Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1\">Gongning Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kuanquan Wang</a>",
          "description": "Coronary artery disease (CAD) has posed a leading threat to the lives of\ncardiovascular disease patients worldwide for a long time. Therefore, automated\ndiagnosis of CAD has indispensable significance in clinical medicine. However,\nthe complexity of coronary artery plaques that cause CAD makes the automatic\ndetection of coronary artery stenosis in Coronary CT angiography (CCTA) a\ndifficult task. In this paper, we propose a Transformer network (TR-Net) for\nthe automatic detection of significant stenosis (i.e. luminal narrowing > 50%)\nwhile practically completing the computer-assisted diagnosis of CAD. The\nproposed TR-Net introduces a novel Transformer, and tightly combines\nconvolutional layers and Transformer encoders, allowing their advantages to be\ndemonstrated in the task. By analyzing semantic information sequences, TR-Net\ncan fully understand the relationship between image information in each\nposition of a multiplanar reformatted (MPR) image, and accurately detect\nsignificant stenosis based on both local and global information. We evaluate\nour TR-Net on a dataset of 76 patients from different patients annotated by\nexperienced radiologists. Experimental results illustrate that our TR-Net has\nachieved better results in ACC (0.92), Spec (0.96), PPV (0.84), F1 (0.79) and\nMCC (0.74) indicators compared with the state-of-the-art methods. The source\ncode is publicly available from the link (https://github.com/XinghuaMa/TR-Net).",
          "link": "http://arxiv.org/abs/2107.03035",
          "publishedOn": "2021-07-08T01:57:56.931Z",
          "wordCount": 654,
          "title": "Transformer Network for Significant Stenosis Detection in CCTA of Coronary Arteries. (arXiv:2107.03035v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02958",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nashed_Y/0/1/0/all/0/1\">Youssef S. G. Nashed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poitevin_F/0/1/0/all/0/1\">Frederic Poitevin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_H/0/1/0/all/0/1\">Harshit Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woollard_G/0/1/0/all/0/1\">Geoffrey Woollard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kagan_M/0/1/0/all/0/1\">Michael Kagan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoon_C/0/1/0/all/0/1\">Chuck Yoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ratner_D/0/1/0/all/0/1\">Daniel Ratner</a>",
          "description": "Cryogenic electron microscopy (cryo-EM) provides images from different copies\nof the same biomolecule in arbitrary orientations. Here, we present an\nend-to-end unsupervised approach that learns individual particle orientations\nfrom cryo-EM data while reconstructing the average 3D map of the biomolecule,\nstarting from a random initialization. The approach relies on an auto-encoder\narchitecture where the latent space is explicitly interpreted as orientations\nused by the decoder to form an image according to the linear projection model.\nWe evaluate our method on simulated data and show that it is able to\nreconstruct 3D particle maps from noisy- and CTF-corrupted 2D projection images\nof unknown particle orientations.",
          "link": "http://arxiv.org/abs/2107.02958",
          "publishedOn": "2021-07-08T01:57:56.911Z",
          "wordCount": 578,
          "title": "End-to-End Simultaneous Learning of Single-particle Orientation and 3D Map Reconstruction from Cryo-electron Microscopy Data. (arXiv:2107.02958v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weixin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenker_E/0/1/0/all/0/1\">Eric Schwenker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spreadbury_T/0/1/0/all/0/1\">Trevor Spreadbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_M/0/1/0/all/0/1\">Maria K.Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cossairt_O/0/1/0/all/0/1\">Oliver Cossairt</a>",
          "description": "Different types of spectroscopies, such as X-ray absorption near edge\nstructure (XANES) and Raman spectroscopy, play a very important role in\nanalyzing the characteristics of different materials. In scientific literature,\nXANES/Raman data are usually plotted in line graphs which is a visually\nappropriate way to represent the information when the end-user is a human\nreader. However, such graphs are not conducive to direct programmatic analysis\ndue to the lack of automatic tools. In this paper, we develop a plot digitizer,\nnamed Plot2Spectra, to extract data points from spectroscopy graph images in an\nautomatic fashion, which makes it possible for large scale data acquisition and\nanalysis. Specifically, the plot digitizer is a two-stage framework. In the\nfirst axis alignment stage, we adopt an anchor-free detector to detect the plot\nregion and then refine the detected bounding boxes with an edge-based\nconstraint to locate the position of two axes. We also apply scene text\ndetector to extract and interpret all tick information below the x-axis. In the\nsecond plot data extraction stage, we first employ semantic segmentation to\nseparate pixels belonging to plot lines from the background, and from there,\nincorporate optical flow constraints to the plot line pixels to assign them to\nthe appropriate line (data instance) they encode. Extensive experiments are\nconducted to validate the effectiveness of the proposed plot digitizer, which\nshows that such a tool could help accelerate the discovery and machine learning\nof materials properties.",
          "link": "http://arxiv.org/abs/2107.02827",
          "publishedOn": "2021-07-08T01:57:56.904Z",
          "wordCount": 675,
          "title": "Plot2Spectra: an Automatic Spectra Extraction Tool. (arXiv:2107.02827v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bifu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kenan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuchun Sun</a>",
          "description": "We proposed a convolutional neural network for vertex classification on\n3-dimensional dental meshes, and used it to detect teeth margins. An expanding\nlayer was constructed to collect statistic values of neighbor vertex features\nand compute new features for each vertex with convolutional neural networks. An\nend-to-end neural network was proposed to take vertex features, including\ncoordinates, curvatures and distance, as input and output each vertex\nclassification label. Several network structures with different parameters of\nexpanding layers and a base line network without expanding layers were designed\nand trained by 1156 dental meshes. The accuracy, recall and precision were\nvalidated on 145 dental meshes to rate the best network structures, which were\nfinally tested on another 144 dental meshes. All networks with our expanding\nlayers performed better than baseline, and the best one achieved an accuracy of\n0.877 both on validation dataset and test dataset.",
          "link": "http://arxiv.org/abs/2107.03030",
          "publishedOn": "2021-07-08T01:57:56.895Z",
          "wordCount": 603,
          "title": "A convolutional neural network for teeth margin detection on 3-dimensional dental meshes. (arXiv:2107.03030v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Ye Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1\">Abhimitra Meka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1\">Mohamed Elgharib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1\">Hans-Peter Seidel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1\">William A. P. Smith</a>",
          "description": "Outdoor scene relighting is a challenging problem that requires good\nunderstanding of the scene geometry, illumination and albedo. Current\ntechniques are completely supervised, requiring high quality synthetic\nrenderings to train a solution. Such renderings are synthesized using priors\nlearned from limited data. In contrast, we propose a self-supervised approach\nfor relighting. Our approach is trained only on corpora of images collected\nfrom the internet without any user-supervision. This virtually endless source\nof training data allows training a general relighting solution. Our approach\nfirst decomposes an image into its albedo, geometry and illumination. A novel\nrelighting is then produced by modifying the illumination parameters. Our\nsolution capture shadow using a dedicated shadow prediction map, and does not\nrely on accurate geometry estimation. We evaluate our technique subjectively\nand objectively using a new dataset with ground-truth relighting. Results show\nthe ability of our technique to produce photo-realistic and physically\nplausible results, that generalizes to unseen scenes.",
          "link": "http://arxiv.org/abs/2107.03106",
          "publishedOn": "2021-07-08T01:57:56.888Z",
          "wordCount": 599,
          "title": "Self-supervised Outdoor Scene Relighting. (arXiv:2107.03106v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mozhdehi_R/0/1/0/all/0/1\">Reza Jalil Mozhdehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medeiros_H/0/1/0/all/0/1\">Henry Medeiros</a>",
          "description": "This work proposes a novel framework for visual tracking based on the\nintegration of an iterative particle filter, a deep convolutional neural\nnetwork, and a correlation filter. The iterative particle filter enables the\nparticles to correct themselves and converge to the correct target position. We\nemploy a novel strategy to assess the likelihood of the particles after the\niterations by applying K-means clustering. Our approach ensures a consistent\nsupport for the posterior distribution. Thus, we do not need to perform\nresampling at every video frame, improving the utilization of prior\ndistribution information. Experimental results on two different benchmark\ndatasets show that our tracker performs favorably against state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2107.02984",
          "publishedOn": "2021-07-08T01:57:56.877Z",
          "wordCount": 551,
          "title": "Deep Convolutional Correlation Iterative Particle Filter for Visual Tracking. (arXiv:2107.02984v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Babiloni_F/0/1/0/all/0/1\">Francesca Babiloni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marras_I/0/1/0/all/0/1\">Ioannis Marras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokkinos_F/0/1/0/all/0/1\">Filippos Kokkinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiankang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1\">Grigorios Chrysos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>",
          "description": "Spatial self-attention layers, in the form of Non-Local blocks, introduce\nlong-range dependencies in Convolutional Neural Networks by computing pairwise\nsimilarities among all possible positions. Such pairwise functions underpin the\neffectiveness of non-local layers, but also determine a complexity that scales\nquadratically with respect to the input size both in space and time. This is a\nseverely limiting factor that practically hinders the applicability of\nnon-local blocks to even moderately sized inputs. Previous works focused on\nreducing the complexity by modifying the underlying matrix operations, however\nin this work we aim to retain full expressiveness of non-local layers while\nkeeping complexity linear. We overcome the efficiency limitation of non-local\nblocks by framing them as special cases of 3rd order polynomial functions. This\nfact enables us to formulate novel fast Non-Local blocks, capable of reducing\nthe complexity from quadratic to linear with no loss in performance, by\nreplacing any direct computation of pairwise similarities with element-wise\nmultiplications. The proposed method, which we dub as \"Poly-NL\", is competitive\nwith state-of-the-art performance across image recognition, instance\nsegmentation, and face detection tasks, while having considerably less\ncomputational overhead.",
          "link": "http://arxiv.org/abs/2107.02859",
          "publishedOn": "2021-07-08T01:57:56.849Z",
          "wordCount": 626,
          "title": "Poly-NL: Linear Complexity Non-local Layers with Polynomials. (arXiv:2107.02859v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Linhua Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zengfu Wang</a>",
          "description": "We propose a simple yet reliable bottom-up approach with a good trade-off\nbetween accuracy and efficiency for the problem of multi-person pose\nestimation. Given an image, we employ an Hourglass Network to infer all the\nkeypoints from different persons indiscriminately as well as the guiding\noffsets connecting the adjacent keypoints belonging to the same persons. Then,\nwe greedily group the candidate keypoints into multiple human poses (if any),\nutilizing the predicted guiding offsets. And we refer to this process as greedy\noffset-guided keypoint grouping (GOG). Moreover, we revisit the\nencoding-decoding method for the multi-person keypoint coordinates and reveal\nsome important facts affecting accuracy. Experiments have demonstrated the\nobvious performance improvements brought by the introduced components. Our\napproach is comparable to the state of the art on the challenging COCO dataset\nunder fair conditions. The source code and our pre-trained model are publicly\navailable online.",
          "link": "http://arxiv.org/abs/2107.03098",
          "publishedOn": "2021-07-08T01:57:56.842Z",
          "wordCount": 590,
          "title": "Greedy Offset-Guided Keypoint Grouping for Human Pose Estimation. (arXiv:2107.03098v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>",
          "description": "How to learn long-range dependencies from 3D point clouds is a challenging\nproblem in 3D point cloud analysis. Addressing this problem, we propose a\nglobal attention network for point cloud semantic segmentation, named as\nGA-Net, consisting of a point-independent global attention module and a\npoint-dependent global attention module for obtaining contextual information of\n3D point clouds in this paper. The point-independent global attention module\nsimply shares a global attention map for all 3D points. In the point-dependent\nglobal attention module, for each point, a novel random cross attention block\nusing only two randomly sampled subsets is exploited to learn the contextual\ninformation of all the points. Additionally, we design a novel point-adaptive\naggregation block to replace linear skip connection for aggregating more\ndiscriminate features. Extensive experimental results on three 3D public\ndatasets demonstrate that our method outperforms state-of-the-art methods in\nmost cases.",
          "link": "http://arxiv.org/abs/2107.03101",
          "publishedOn": "2021-07-08T01:57:56.815Z",
          "wordCount": 588,
          "title": "GA-NET: Global Attention Network for Point Cloud Semantic Segmentation. (arXiv:2107.03101v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Youoku_S/0/1/0/all/0/1\">Sachihiro Youoku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_T/0/1/0/all/0/1\">Takahisa Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1\">Junya Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_A/0/1/0/all/0/1\">Akiyoshi Uchida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_X/0/1/0/all/0/1\">Xiaoyu Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Ziqiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongling Liu</a>",
          "description": "Human affective recognition is an important factor in human-computer\ninteraction. However, the method development with in-the-wild data is not yet\naccurate enough for practical usage. In this paper, we introduce the affective\nrecognition method focusing on facial expression (EXP) and valence-arousal\ncalculation that was submitted to the Affective Behavior Analysis in-the-wild\n(ABAW) 2021 Contest.\n\nWhen annotating facial expressions from a video, we thought that it would be\njudged not only from the features common to all people, but also from the\nrelative changes in the time series of individuals. Therefore, after learning\nthe common features for each frame, we constructed a facial expression\nestimation model and valence-arousal model using time-series data after\ncombining the common features and the standardized features for each video.\nFurthermore, the above features were learned using multi-modal data such as\nimage features, AU, Head pose, and Gaze. In the validation set, our model\nachieved a facial expression score of 0.546. These verification results reveal\nthat our proposed framework can improve estimation accuracy and robustness\neffectively.",
          "link": "http://arxiv.org/abs/2107.03009",
          "publishedOn": "2021-07-08T01:57:56.808Z",
          "wordCount": 624,
          "title": "Multi-modal Affect Analysis using standardized data within subjects in the Wild. (arXiv:2107.03009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1\">Daniel Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarlow_D/0/1/0/all/0/1\">Danny Tarlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1\">Rianne van den Berg</a>",
          "description": "Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown\nimpressive results on image and waveform generation in continuous state spaces.\nHere, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs),\ndiffusion-like generative models for discrete data that generalize the\nmultinomial diffusion model of Hoogeboom et al. 2021, by going beyond\ncorruption processes with uniform transition probabilities. This includes\ncorruption with transition matrices that mimic Gaussian kernels in continuous\nspace, matrices based on nearest neighbors in embedding space, and matrices\nthat introduce absorbing states. The third allows us to draw a connection\nbetween diffusion models and autoregressive and mask-based generative models.\nWe show that the choice of transition matrix is an important design decision\nthat leads to improved results in image and text domains. We also introduce a\nnew loss function that combines the variational lower bound with an auxiliary\ncross entropy loss. For text, this model class achieves strong results on\ncharacter-level text generation while scaling to large vocabularies on LM1B. On\nthe image dataset CIFAR-10, our models approach the sample quality and exceed\nthe log-likelihood of the continuous-space DDPM model.",
          "link": "http://arxiv.org/abs/2107.03006",
          "publishedOn": "2021-07-08T01:57:56.791Z",
          "wordCount": 641,
          "title": "Structured Denoising Diffusion Models in Discrete State-Spaces. (arXiv:2107.03006v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Ying Siu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dongkyu Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwok_K/0/1/0/all/0/1\">Kenneth Kwok</a>",
          "description": "Reliable perception is essential for robots that interact with the world. But\nsensors alone are often insufficient to provide this capability, and they are\nprone to errors due to various conditions in the environment. Furthermore,\nthere is a need for robots to maintain a model of its surroundings even when\nobjects go out of view and are no longer visible. This requires anchoring\nperceptual information onto symbols that represent the objects in the\nenvironment. In this paper, we present a model for action-aware perceptual\nanchoring that enables robots to track objects in a persistent manner. Our\nrule-based approach considers inductive biases to perform high-level reasoning\nover the results from low-level object detection, and it improves the robot's\nperceptual capability for complex tasks. We evaluate our model against existing\nbaseline models for object permanence and show that it outperforms these on a\nsnitch localisation task using a dataset of 1,371 videos. We also integrate our\naction-aware perceptual anchoring in the context of a cognitive architecture\nand demonstrate its benefits in a realistic gearbox assembly task on a\nUniversal Robot.",
          "link": "http://arxiv.org/abs/2107.03038",
          "publishedOn": "2021-07-08T01:57:56.784Z",
          "wordCount": 631,
          "title": "Maintaining a Reliable World Model using Action-aware Perceptual Anchoring. (arXiv:2107.03038v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taira_H/0/1/0/all/0/1\">Hajime Taira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onbe_K/0/1/0/all/0/1\">Koki Onbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyashita_N/0/1/0/all/0/1\">Naoyuki Miyashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1\">Masatoshi Okutomi</a>",
          "description": "In this paper we introduce a new camera localization strategy designed for\nimage sequences captured in challenging industrial situations such as\nindustrial parts inspection. To deal with peculiar appearances that hurt\nstandard 3D reconstruction pipeline, we exploit pre-knowledge of the scene by\nselecting key frames in the sequence (called as anchors) which are roughly\nconnected to a certain location. Our method then seek the location of each\nframe in time-order, while recursively updating an augmented 3D model which can\nprovide current camera location and surrounding 3D structure. In an experiment\non a practical industrial situation, our method can localize over 99% frames in\nthe input sequence, whereas standard localization methods fail to reconstruct a\ncomplete camera trajectory.",
          "link": "http://arxiv.org/abs/2107.03068",
          "publishedOn": "2021-07-08T01:57:56.777Z",
          "wordCount": 581,
          "title": "Video-Based Camera Localization Using Anchor View Detection and Recursive 3D Reconstruction. (arXiv:2107.03068v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aouayeb_M/0/1/0/all/0/1\">Mouath Aouayeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soladie_C/0/1/0/all/0/1\">Catherine Soladie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kpalma_K/0/1/0/all/0/1\">Kidiyo Kpalma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seguier_R/0/1/0/all/0/1\">Renaud Seguier</a>",
          "description": "As various databases of facial expressions have been made accessible over the\nlast few decades, the Facial Expression Recognition (FER) task has gotten a lot\nof interest. The multiple sources of the available databases raised several\nchallenges for facial recognition task. These challenges are usually addressed\nby Convolution Neural Network (CNN) architectures. Different from CNN models, a\nTransformer model based on attention mechanism has been presented recently to\naddress vision tasks. One of the major issue with Transformers is the need of a\nlarge data for training, while most FER databases are limited compared to other\nvision applications. Therefore, we propose in this paper to learn a vision\nTransformer jointly with a Squeeze and Excitation (SE) block for FER task. The\nproposed method is evaluated on different publicly available FER databases\nincluding CK+, JAFFE,RAF-DB and SFEW. Experiments demonstrate that our model\noutperforms state-of-the-art methods on CK+ and SFEW and achieves competitive\nresults on JAFFE and RAF-DB.",
          "link": "http://arxiv.org/abs/2107.03107",
          "publishedOn": "2021-07-08T01:57:56.770Z",
          "wordCount": 599,
          "title": "Learning Vision Transformer with Squeeze and Excitation for Facial Expression Recognition. (arXiv:2107.03107v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yuanxin Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minghan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Huei Peng</a>",
          "description": "A unified neural network structure is presented for joint 3D object detection\nand point cloud segmentation in this paper. We leverage rich supervision from\nboth detection and segmentation labels rather than using just one of them. In\naddition, an extension based on single-stage object detectors is proposed based\non the implicit function widely used in 3D scene and object understanding. The\nextension branch takes the final feature map from the object detection module\nas input, and produces an implicit function that generates semantic\ndistribution for each point for its corresponding voxel center. We demonstrated\nthe performance of our structure on nuScenes-lidarseg, a large-scale outdoor\ndataset. Our solution achieves competitive results against state-of-the-art\nmethods in both 3D object detection and point cloud segmentation with little\nadditional computation load compared with object detection solutions. The\ncapability of efficient weakly supervision semantic segmentation of the\nproposed method is also validated by experiments.",
          "link": "http://arxiv.org/abs/2107.02980",
          "publishedOn": "2021-07-08T01:57:56.760Z",
          "wordCount": 593,
          "title": "VIN: Voxel-based Implicit Network for Joint 3D Object Detection and Segmentation for Lidars. (arXiv:2107.02980v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Danfeng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianru Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaza_A/0/1/0/all/0/1\">Antonio Plaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>",
          "description": "Hyperspectral (HS) images are characterized by approximately contiguous\nspectral information, enabling the fine identification of materials by\ncapturing subtle spectral discrepancies. Owing to their excellent locally\ncontextual modeling ability, convolutional neural networks (CNNs) have been\nproven to be a powerful feature extractor in HS image classification. However,\nCNNs fail to mine and represent the sequence attributes of spectral signatures\nwell due to the limitations of their inherent network backbone. To solve this\nissue, we rethink HS image classification from a sequential perspective with\ntransformers, and propose a novel backbone network called \\ul{SpectralFormer}.\nBeyond band-wise representations in classic transformers, SpectralFormer is\ncapable of learning spectrally local sequence information from neighboring\nbands of HS images, yielding group-wise spectral embeddings. More\nsignificantly, to reduce the possibility of losing valuable information in the\nlayer-wise propagation process, we devise a cross-layer skip connection to\nconvey memory-like components from shallow to deep layers by adaptively\nlearning to fuse \"soft\" residuals across layers. It is worth noting that the\nproposed SpectralFormer is a highly flexible backbone network, which can be\napplicable to both pixel- and patch-wise inputs. We evaluate the classification\nperformance of the proposed SpectralFormer on three HS datasets by conducting\nextensive experiments, showing the superiority over classic transformers and\nachieving a significant improvement in comparison with state-of-the-art\nbackbone networks. The codes of this work will be available at\n\\url{https://sites.google.com/view/danfeng-hong} for the sake of\nreproducibility.",
          "link": "http://arxiv.org/abs/2107.02988",
          "publishedOn": "2021-07-08T01:57:56.687Z",
          "wordCount": 674,
          "title": "SpectralFormer: Rethinking Hyperspectral Image Classification with Transformers. (arXiv:2107.02988v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingjing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>",
          "description": "Recent research has seen numerous supervised learning-based methods for 3D\nshape segmentation and remarkable performance has been achieved on various\nbenchmark datasets. These supervised methods require a large amount of\nannotated data to train deep neural networks to ensure the generalization\nability on the unseen test set. In this paper, we introduce a\nmeta-learning-based method for few-shot 3D shape segmentation where only a few\nlabeled samples are provided for the unseen classes. To achieve this, we treat\nthe shape segmentation as a point labeling problem in the metric space.\nSpecifically, we first design a meta-metric learner to transform input shapes\ninto embedding space and our model learns to learn a proper metric space for\neach object class based on point embeddings. Then, for each class, we design a\nmetric learner to extract part-specific prototype representations from a few\nsupport shapes and our model performs per-point segmentation over the query\nshapes by matching each point to its nearest prototype in the learned metric\nspace. A metric-based loss function is used to dynamically modify distances\nbetween point embeddings thus maximizes in-part similarity while minimizing\ninter-part similarity. A dual segmentation branch is adopted to make full use\nof the support information and implicitly encourages consistency between the\nsupport and query prototypes. We demonstrate the superior performance of our\nproposed on the ShapeNet part dataset under the few-shot scenario, compared\nwith well-established baseline and state-of-the-art semi-supervised methods.",
          "link": "http://arxiv.org/abs/2107.02972",
          "publishedOn": "2021-07-08T01:57:56.661Z",
          "wordCount": 675,
          "title": "Learn to Learn Metric Space for Few-Shot Segmentation of 3D Shapes. (arXiv:2107.02972v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Numair Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Min H. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1\">James Tompkin</a>",
          "description": "We present an algorithm to estimate fast and accurate depth maps from light\nfields via a sparse set of depth edges and gradients. Our proposed approach is\nbased around the idea that true depth edges are more sensitive than texture\nedges to local constraints, and so they can be reliably disambiguated through a\nbidirectional diffusion process. First, we use epipolar-plane images to\nestimate sub-pixel disparity at a sparse set of pixels. To find sparse points\nefficiently, we propose an entropy-based refinement approach to a line estimate\nfrom a limited set of oriented filter banks. Next, to estimate the diffusion\ndirection away from sparse points, we optimize constraints at these points via\nour bidirectional diffusion method. This resolves the ambiguity of which\nsurface the edge belongs to and reliably separates depth from texture edges,\nallowing us to diffuse the sparse set in a depth-edge and occlusion-aware\nmanner to obtain accurate dense depth maps.",
          "link": "http://arxiv.org/abs/2107.02967",
          "publishedOn": "2021-07-08T01:57:56.646Z",
          "wordCount": 597,
          "title": "Edge-aware Bidirectional Diffusion for Dense Depth Estimation from Light Fields. (arXiv:2107.02967v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02970",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sundaram_S/0/1/0/all/0/1\">Shobhita Sundaram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hulkund_N/0/1/0/all/0/1\">Neha Hulkund</a>",
          "description": "A common problem in computer vision -- particularly in medical applications\n-- is a lack of sufficiently diverse, large sets of training data. These\ndatasets often suffer from severe class imbalance. As a result, networks often\noverfit and are unable to generalize to novel examples. Generative Adversarial\nNetworks (GANs) offer a novel method of synthetic data augmentation. In this\nwork, we evaluate the use of GAN- based data augmentation to artificially\nexpand the CheXpert dataset of chest radiographs. We compare performance to\ntraditional augmentation and find that GAN-based augmentation leads to higher\ndownstream performance for underrepresented classes. Furthermore, we see that\nthis result is pronounced in low data regimens. This suggests that GAN-based\naugmentation a promising area of research to improve network performance when\ndata collection is prohibitively expensive.",
          "link": "http://arxiv.org/abs/2107.02970",
          "publishedOn": "2021-07-08T01:57:56.619Z",
          "wordCount": 588,
          "title": "GAN-based Data Augmentation for Chest X-ray Classification. (arXiv:2107.02970v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sayo_A/0/1/0/all/0/1\">Akihiko Sayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_D/0/1/0/all/0/1\">Diego Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawasaki_H/0/1/0/all/0/1\">Hiroshi Kawasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeuchi_K/0/1/0/all/0/1\">Katsushi Ikeuchi</a>",
          "description": "We propose a new 2D pose refinement network that learns to predict the human\nbias in the estimated 2D pose. There are biases in 2D pose estimations that are\ndue to differences between annotations of 2D joint locations based on\nannotators' perception and those defined by motion capture (MoCap) systems.\nThese biases are crafted into publicly available 2D pose datasets and cannot be\nremoved with existing error reduction approaches. Our proposed pose refinement\nnetwork allows us to efficiently remove the human bias in the estimated 2D\nposes and achieve highly accurate multi-view 3D human pose estimation.",
          "link": "http://arxiv.org/abs/2107.03000",
          "publishedOn": "2021-07-08T01:57:56.611Z",
          "wordCount": 544,
          "title": "PoseRN: A 2D pose refinement network for bias-free multi-view 3D human pose estimation. (arXiv:2107.03000v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+yan_J/0/1/0/all/0/1\">Junjie yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "We introduce the first Neural Architecture Search (NAS) method to find a\nbetter transformer architecture for image recognition. Recently, transformers\nwithout CNN-based backbones are found to achieve impressive performance for\nimage recognition. However, the transformer is designed for NLP tasks and thus\ncould be sub-optimal when directly used for image recognition. In order to\nimprove the visual representation ability for transformers, we propose a new\nsearch space and searching algorithm. Specifically, we introduce a locality\nmodule that models the local correlations in images explicitly with fewer\ncomputational cost. With the locality module, our search space is defined to\nlet the search algorithm freely trade off between global and local information\nas well as optimizing the low-level design choice in each module. To tackle the\nproblem caused by huge search space, a hierarchical neural architecture search\nmethod is proposed to search the optimal vision transformer from two levels\nseparately with the evolutionary algorithm. Extensive experiments on the\nImageNet dataset demonstrate that our method can find more discriminative and\nefficient transformer variants than the ResNet family (e.g., ResNet101) and the\nbaseline ViT for image classification.",
          "link": "http://arxiv.org/abs/2107.02960",
          "publishedOn": "2021-07-08T01:57:56.603Z",
          "wordCount": 632,
          "title": "GLiT: Neural Architecture Search for Global and Local Image Transformer. (arXiv:2107.02960v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xumeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuehui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guorong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhenjun Han</a>",
          "description": "Unsupervised person re-identification (re-ID) remains a challenging task,\nwhere the classifier and feature representation could be easily misled by the\nnoisy pseudo labels towards deteriorated over-fitting. In this paper, we\npropose a simple yet effective approach, termed Group Sampling, to alleviate\nthe negative impact of noisy pseudo labels within unsupervised person re-ID\nmodels. The idea behind Group Sampling is that it can gather a group of samples\nfrom the same class in the same mini-batch, such that the model is trained upon\ngroup normalized samples while alleviating the effect of a single sample. Group\nsampling updates the pipeline of pseudo label generation by guaranteeing the\nsamples to be better divided into the correct classes. Group Sampling\nregularizes classifier training and representation learning, leading to the\nstatistical stability of feature representation in a progressive fashion.\nQualitative and quantitative experiments on Market-1501, DukeMTMC-reID, and\nMSMT17 show that Grouping Sampling improves the state-of-the-arts by up to\n2.2%~6.1%. Code is available at https://github.com/wavinflaghxm/GroupSampling.",
          "link": "http://arxiv.org/abs/2107.03024",
          "publishedOn": "2021-07-08T01:57:56.595Z",
          "wordCount": 599,
          "title": "Group Sampling for Unsupervised Person Re-identification. (arXiv:2107.03024v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yijing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magoulianitis_V/0/1/0/all/0/1\">Vasileios Magoulianitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>",
          "description": "Based on PixelHop and PixelHop++, which are recently developed using the\nsuccessive subspace learning (SSL) framework, we propose an enhanced solution\nfor object classification, called E-PixelHop, in this work. E-PixelHop consists\nof the following steps. First, to decouple the color channels for a color\nimage, we apply principle component analysis and project RGB three color\nchannels onto two principle subspaces which are processed separately for\nclassification. Second, to address the importance of multi-scale features, we\nconduct pixel-level classification at each hop with various receptive fields.\nThird, to further improve pixel-level classification accuracy, we develop a\nsupervised label smoothing (SLS) scheme to ensure prediction consistency.\nForth, pixel-level decisions from each hop and from each color subspace are\nfused together for image-level decision. Fifth, to resolve confusing classes\nfor further performance boosting, we formulate E-PixelHop as a two-stage\npipeline. In the first stage, multi-class classification is performed to get a\nsoft decision for each class, where the top 2 classes with the highest\nprobabilities are called confusing classes. Then,we conduct a binary\nclassification in the second stage. The main contributions lie in Steps 1, 3\nand 5.We use the classification of the CIFAR-10 dataset as an example to\ndemonstrate the effectiveness of the above-mentioned key components of\nE-PixelHop.",
          "link": "http://arxiv.org/abs/2107.02966",
          "publishedOn": "2021-07-08T01:57:56.586Z",
          "wordCount": 646,
          "title": "E-PixelHop: An Enhanced PixelHop Method for Object Classification. (arXiv:2107.02966v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiatong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenglu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Honglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shichuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>",
          "description": "Ki67 is a significant biomarker in the diagnosis and prognosis of cancer,\nwhose index can be evaluated by quantifying its expression in Ki67\nimmunohistochemistry (IHC) stained images. However, quantitative analysis on\nmulti-source Ki67 images is yet a challenging task in practice due to\ncross-domain distribution differences, which result from imaging variation,\nstaining styles, and lesion types. Many recent studies have made some efforts\non domain generalization (DG), whereas there are still some noteworthy\nlimitations. Specifically in the case of Ki67 images, learning invariant\nrepresentation is at the mercy of the insufficient number of domains and the\ncell categories mismatching in different domains. In this paper, we propose a\nnovel method to improve DG by searching the domain-agnostic subnetwork in a\ndomain merging scenario. Partial model parameters are iteratively pruned\naccording to the domain gap, which is caused by the data converting from a\nsingle domain into merged domains during training. In addition, the model is\noptimized by fine-tuning on merged domains to eliminate the interference of\nclass mismatching among various domains. Furthermore, an appropriate\nimplementation is attained by applying the pruning method to different parts of\nthe framework. Compared with known DG methods, our method yields excellent\nperformance in multiclass nucleus recognition of Ki67 IHC images, especially in\nthe lost category cases. Moreover, our competitive results are also evaluated\non the public dataset over the state-of-the-art DG methods.",
          "link": "http://arxiv.org/abs/2107.02500",
          "publishedOn": "2021-07-07T01:57:14.025Z",
          "wordCount": 677,
          "title": "Generalizing Nucleus Recognition Model in Multi-source Images via Pruning. (arXiv:2107.02500v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengxiong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuwu Zhang</a>",
          "description": "Most deep learning-based super-resolution (SR) methods are not\nimage-specific: 1) They are exhaustively trained on datasets synthesized by\npredefined blur kernels (\\eg bicubic), regardless of the domain gap with test\nimages. 2) Their model weights are fixed during testing, which means that test\nimages with various degradations are super-resolved by the same set of weights.\nHowever, degradations of real images are various and unknown (\\ie blind SR). It\nis hard for a single model to perform well in all cases. To address these\nissues, we propose an online super-resolution (ONSR) method. It does not rely\non predefined blur kernels and allows the model weights to be updated according\nto the degradation of the test image. Specifically, ONSR consists of two\nbranches, namely internal branch (IB) and external branch (EB). IB could learn\nthe specific degradation of the given test LR image, and EB could learn to\nsuper resolve images degraded by the learned degradation. In this way, ONSR\ncould customize a specific model for each test image, and thus could be more\ntolerant with various degradations in real applications. Extensive experiments\non both synthesized and real-world images show that ONSR can generate more\nvisually favorable SR results and achieve state-of-the-art performance in blind\nSR.",
          "link": "http://arxiv.org/abs/2107.02398",
          "publishedOn": "2021-07-07T01:57:14.013Z",
          "wordCount": 650,
          "title": "From General to Specific: Online Updating for Blind Super-Resolution. (arXiv:2107.02398v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rogoz_A/0/1/0/all/0/1\">Ana-Cristina Rogoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muntean_R/0/1/0/all/0/1\">Radu Muntean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobeli_S/0/1/0/all/0/1\">Stefan Cobeli</a>",
          "description": "Detecting objects of interest in images was always a compelling task to\nautomate. In recent years this task was more and more explored using deep\nlearning techniques, mostly using region-based convolutional networks. In this\nproject we propose an alternative semantic segmentation technique making use of\nGenerative Adversarial Networks. We consider semantic segmentation to be a\ndomain transfer problem. Thus, we train a feed forward network (FFNN) to\nreceive as input a seed real image and generate as output its segmentation\nmask.",
          "link": "http://arxiv.org/abs/2107.02525",
          "publishedOn": "2021-07-07T01:57:13.169Z",
          "wordCount": 531,
          "title": "Semantic Segmentation Alternative Technique: Segmentation Domain Generation. (arXiv:2107.02525v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yixiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopcroft_J/0/1/0/all/0/1\">John E. Hopcroft</a>",
          "description": "The square kernel is a standard unit for contemporary Convolutional Neural\nNetworks (CNNs), as it fits well on the tensor computation for the convolution\noperation. However, the receptive field in the human visual system is actually\nisotropic like a circle. Motivated by this observation, we propose using circle\nkernels with isotropic receptive fields for the convolution, and our training\ntakes approximately equivalent amount of calculation when compared with the\ncorresponding CNN with square kernels. Our preliminary experiments demonstrate\nthe rationality of circle kernels. We then propose a kernel boosting strategy\nthat integrates the circle kernels with square kernels for the training and\ninference, and we further let the kernel size/radius be learnable during the\ntraining. Note that we reparameterize the circle kernels or integrated kernels\nbefore the inference, thus taking no extra computation as well as the number of\nparameter overhead for the testing. Extensive experiments on several standard\ndatasets, ImageNet, CIFAR-10 and CIFAR-100, using the circle kernels or\nintegrated kernels on typical existing CNNs, show that our approach exhibits\nhighly competitive performance. Specifically, on ImageNet with standard data\naugmentation, our approach dramatically boosts the performance of\nMobileNetV3-Small by 5.20% top-1 accuracy and 3.39% top-5 accuracy, and boosts\nthe performance of MobileNetV3-Large by 2.16% top-1 accuracy and 1.18% top-5\naccuracy.",
          "link": "http://arxiv.org/abs/2107.02451",
          "publishedOn": "2021-07-07T01:57:13.030Z",
          "wordCount": 657,
          "title": "Integrating Circle Kernels into Convolutional Neural Networks. (arXiv:2107.02451v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data will be made available.",
          "link": "http://arxiv.org/abs/2106.14118",
          "publishedOn": "2021-07-07T01:57:12.349Z",
          "wordCount": 620,
          "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00809",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tao_M/0/1/0/all/0/1\">Min Tao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chuah_C/0/1/0/all/0/1\">Chen-Nee Chuah</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nagy_J/0/1/0/all/0/1\">James Nagy</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lou_Y/0/1/0/all/0/1\">Yifei Lou</a>",
          "description": "In this paper, we study the L1/L2 minimization on the gradient for imaging\napplications. Several recent works have demonstrated that L1/L2 is better than\nthe L1 norm when approximating the L0 norm to promote sparsity. Consequently,\nwe postulate that applying L1/L2 on the gradient is better than the classic\ntotal variation (the L1 norm on the gradient) to enforce the sparsity of the\nimage gradient. To verify our hypothesis, we consider a constrained formulation\nto reveal empirical evidence on the superiority of L1/L2 over L1 when\nrecovering piecewise constant signals from low-frequency measurements.\nNumerically, we design a specific splitting scheme, under which we can prove\nsubsequential and global convergence for the alternating direction method of\nmultipliers (ADMM) under certain conditions. Experimentally, we demonstrate\nvisible improvements of L1/L2 over L1 and other nonconvex regularizations for\nimage recovery from low-frequency measurements and two medical applications of\nMRI and CT reconstruction. All the numerical results show the efficiency of our\nproposed approach.",
          "link": "http://arxiv.org/abs/2101.00809",
          "publishedOn": "2021-07-07T01:57:12.283Z",
          "wordCount": 618,
          "title": "Minimizing L1 over L2 norms on the gradient. (arXiv:2101.00809v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02643",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Budd_S/0/1/0/all/0/1\">Samuel Budd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinclair_M/0/1/0/all/0/1\">Matthew Sinclair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Day_T/0/1/0/all/0/1\">Thomas Day</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vlontzos_A/0/1/0/all/0/1\">Athanasios Vlontzos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_J/0/1/0/all/0/1\">Jeremy Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tianrui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matthew_J/0/1/0/all/0/1\">Jaqueline Matthew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Skelton_E/0/1/0/all/0/1\">Emily Skelton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simpson_J/0/1/0/all/0/1\">John Simpson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_E/0/1/0/all/0/1\">Emma C. Robinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>",
          "description": "Fetal ultrasound screening during pregnancy plays a vital role in the early\ndetection of fetal malformations which have potential long-term health impacts.\nThe level of skill required to diagnose such malformations from live ultrasound\nduring examination is high and resources for screening are often limited. We\npresent an interpretable, atlas-learning segmentation method for automatic\ndiagnosis of Hypo-plastic Left Heart Syndrome (HLHS) from a single `4 Chamber\nHeart' view image. We propose to extend the recently introduced\nImage-and-Spatial Transformer Networks (Atlas-ISTN) into a framework that\nenables sensitising atlas generation to disease. In this framework we can\njointly learn image segmentation, registration, atlas construction and disease\nprediction while providing a maximum level of clinical interpretability\ncompared to direct image classification methods. As a result our segmentation\nallows diagnoses competitive with expert-derived manual diagnosis and yields an\nAUC-ROC of 0.978 (1043 cases for training, 260 for validation and 325 for\ntesting).",
          "link": "http://arxiv.org/abs/2107.02643",
          "publishedOn": "2021-07-07T01:57:12.269Z",
          "wordCount": 629,
          "title": "Detecting Hypo-plastic Left Heart Syndrome in Fetal Ultrasound via Disease-specific Atlas Maps. (arXiv:2107.02643v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08949",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1\">Chun-Mei Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_S/0/1/0/all/0/1\">Shuhao Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>",
          "description": "Super-resolution (SR) plays a crucial role in improving the image quality of\nmagnetic resonance imaging (MRI). MRI produces multi-contrast images and can\nprovide a clear display of soft tissues. However, current super-resolution\nmethods only employ a single contrast, or use a simple multi-contrast fusion\nmechanism, ignoring the rich relations among different contrasts, which are\nvaluable for improving SR. In this work, we propose a multi-stage integration\nnetwork (i.e., MINet) for multi-contrast MRI SR, which explicitly models the\ndependencies between multi-contrast images at different stages to guide image\nSR. In particular, our MINet first learns a hierarchical feature representation\nfrom multiple convolutional stages for each of different-contrast image.\nSubsequently, we introduce a multi-stage integration module to mine the\ncomprehensive relations between the representations of the multi-contrast\nimages. Specifically, the module matches each representation with all other\nfeatures, which are integrated in terms of their similarities to obtain an\nenriched representation. Extensive experiments on fastMRI and real-world\nclinical datasets demonstrate that 1) our MINet outperforms state-of-the-art\nmulti-contrast SR methods in terms of various metrics and 2) our multi-stage\nintegration module is able to excavate complex interactions among\nmulti-contrast features at different stages, leading to improved target-image\nquality.",
          "link": "http://arxiv.org/abs/2105.08949",
          "publishedOn": "2021-07-07T01:57:12.255Z",
          "wordCount": 683,
          "title": "Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network. (arXiv:2105.08949v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1\">Christoph Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paschali_M/0/1/0/all/0/1\">Magdalini Paschali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamnitsas_K/0/1/0/all/0/1\">Konstantinos Kamnitsas</a>",
          "description": "Image classification models deployed in the real world may receive inputs\noutside the intended data distribution. For critical applications such as\nclinical decision making, it is important that a model can detect such\nout-of-distribution (OOD) inputs and express its uncertainty. In this work, we\nassess the capability of various state-of-the-art approaches for\nconfidence-based OOD detection through a comparative study and in-depth\nanalysis. First, we leverage a computer vision benchmark to reproduce and\ncompare multiple OOD detection methods. We then evaluate their capabilities on\nthe challenging task of disease classification using chest X-rays. Our study\nshows that high performance in a computer vision task does not directly\ntranslate to accuracy in a medical imaging task. We analyse factors that affect\nperformance of the methods between the two tasks. Our results provide useful\ninsights for developing the next generation of OOD detection methods.",
          "link": "http://arxiv.org/abs/2107.02568",
          "publishedOn": "2021-07-07T01:57:12.238Z",
          "wordCount": 582,
          "title": "Confidence-based Out-of-Distribution Detection: A Comparative Study and Analysis. (arXiv:2107.02568v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_T/0/1/0/all/0/1\">Tzu-Ming Harry Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yin-Chih Chelsea Wang</a>",
          "description": "Clinical finding summaries from an orthopantomogram, or a dental panoramic\nradiograph, have significant potential to improve patient communication and\nspeed up clinical judgments. While orthopantomogram is a first-line tool for\ndental examinations, no existing work has explored the summarization of\nfindings from it. A finding summary has to find teeth in the imaging study and\nlabel the teeth with several types of past treatments. To tackle the problem,\nwe developDeepOPG that breaks the summarization process into functional\nsegmentation and tooth localization, the latter of which is further refined by\na novel dental coherence module. We also leverage weak supervision labels to\nimprove detection results in a reinforcement learning scenario. Experiments\nshow high efficacy of DeepOPG on finding summarization, achieving an overall\nAUC of 88.2% in detecting six types of findings. The proposed dental coherence\nand weak supervision both are shown to improve DeepOPG by adding 5.9% and 0.4%\nto AP@IoU=0.5.",
          "link": "http://arxiv.org/abs/2103.08290",
          "publishedOn": "2021-07-07T01:57:12.222Z",
          "wordCount": 612,
          "title": "DeepOPG: Improving Orthopantomogram Finding Summarization with Weak Supervision. (arXiv:2103.08290v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_A/0/1/0/all/0/1\">Arun Narenthiran Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_S/0/1/0/all/0/1\">Sahil Modi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasparino_M/0/1/0/all/0/1\">Mateus Valverde Gasparino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_C/0/1/0/all/0/1\">Che Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1\">Andres Eduardo Baquero Velasquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1\">Girish Chowdhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Saurabh Gupta</a>",
          "description": "We describe a system for visually guided autonomous navigation of\nunder-canopy farm robots. Low-cost under-canopy robots can drive between crop\nrows under the plant canopy and accomplish tasks that are infeasible for\nover-the-canopy drones or larger agricultural equipment. However, autonomously\nnavigating them under the canopy presents a number of challenges: unreliable\nGPS and LiDAR, high cost of sensing, challenging farm terrain, clutter due to\nleaves and weeds, and large variability in appearance over the season and\nacross crop types. We address these challenges by building a modular system\nthat leverages machine learning for robust and generalizable perception from\nmonocular RGB images from low-cost cameras, and model predictive control for\naccurate control in challenging terrain. Our system, CropFollow, is able to\nautonomously drive 485 meters per intervention on average, outperforming a\nstate-of-the-art LiDAR based system (286 meters per intervention) in extensive\nfield testing spanning over 25 km.",
          "link": "http://arxiv.org/abs/2107.02792",
          "publishedOn": "2021-07-07T01:57:12.174Z",
          "wordCount": 609,
          "title": "Learned Visual Navigation for Under-Canopy Agricultural Robots. (arXiv:2107.02792v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1\">Leitian Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_L/0/1/0/all/0/1\">Li Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nannan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xianhang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yaosi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "For a typical Scene Graph Generation (SGG) method, there is often a large gap\nin the performance of the predicates' head classes and tail classes. This\nphenomenon is mainly caused by the semantic overlap between different\npredicates as well as the long-tailed data distribution. In this paper, a\nPredicate Correlation Learning (PCL) method for SGG is proposed to address the\nabove two problems by taking the correlation between predicates into\nconsideration. To describe the semantic overlap between strong-correlated\npredicate classes, a Predicate Correlation Matrix (PCM) is defined to quantify\nthe relationship between predicate pairs, which is dynamically updated to\nremove the matrix's long-tailed bias. In addition, PCM is integrated into a\nPredicate Correlation Loss function ($L_{PC}$) to reduce discouraging gradients\nof unannotated classes. The proposed method is evaluated on Visual Genome\nbenchmark, where the performance of the tail classes is significantly improved\nwhen built on the existing methods.",
          "link": "http://arxiv.org/abs/2107.02713",
          "publishedOn": "2021-07-07T01:57:12.167Z",
          "wordCount": 586,
          "title": "Predicate correlation learning for scene graph generation. (arXiv:2107.02713v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strauss_K/0/1/0/all/0/1\">Kevin Strauss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savkin_A/0/1/0/all/0/1\">Artem Savkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Synthetic data became already an essential component of machine\nlearning-based perception in the field of autonomous driving. Yet it still\ncannot replace real data completely due to the sim2real domain shift. In this\nwork, we propose a method that leverages the advantages of the augmentation\nprocess and adversarial training to synthesize realistic data for the\npedestrian recognition task. Our approach utilizes an attention mechanism\ndriven by an adversarial loss to learn domain discrepancies and improve\nsim2real adaptation. Our experiments confirm that the proposed adaptation\nmethod is robust to such discrepancies and reveals both visual realism and\nsemantic consistency. Furthermore, we evaluate our data generation pipeline on\nthe task of pedestrian recognition and demonstrate that generated data resemble\nproperties of the real domain.",
          "link": "http://arxiv.org/abs/2107.02673",
          "publishedOn": "2021-07-07T01:57:12.133Z",
          "wordCount": 554,
          "title": "Attention-based Adversarial Appearance Learning of Augmented Pedestrians. (arXiv:2107.02673v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Mengxi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xinhua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>",
          "description": "Person re-identification (re-ID) under various occlusions has been a\nlong-standing challenge as person images with different types of occlusions\noften suffer from misalignment in image matching and ranking. Most existing\nmethods tackle this challenge by aligning spatial features of body parts\naccording to external semantic cues or feature similarities but this alignment\napproach is complicated and sensitive to noises. We design DRL-Net, a\ndisentangled representation learning network that handles occluded re-ID\nwithout requiring strict person image alignment or any additional supervision.\nLeveraging transformer architectures, DRL-Net achieves alignment-free re-ID via\nglobal reasoning of local features of occluded person images. It measures image\nsimilarity by automatically disentangling the representation of undefined\nsemantic components, e.g., human body parts or obstacles, under the guidance of\nsemantic preference object queries in the transformer. In addition, we design a\ndecorrelation constraint in the transformer decoder and impose it over object\nqueries for better focus on different semantic components. To better eliminate\ninterference from occlusions, we design a contrast feature learning technique\n(CFL) for better separation of occlusion features and discriminative ID\nfeatures. Extensive experiments over occluded and holistic re-ID benchmarks\n(Occluded-DukeMTMC, Market1501 and DukeMTMC) show that the DRL-Net achieves\nsuperior re-ID performance consistently and outperforms the state-of-the-art by\nlarge margins for Occluded-DukeMTMC.",
          "link": "http://arxiv.org/abs/2107.02380",
          "publishedOn": "2021-07-07T01:57:12.021Z",
          "wordCount": 645,
          "title": "Learning Disentangled Representation Implicitly via Transformer for Occluded Person Re-Identification. (arXiv:2107.02380v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coccomini_D/0/1/0/all/0/1\">Davide Coccomini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messina_N/0/1/0/all/0/1\">Nicola Messina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1\">Claudio Gennaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1\">Fabrizio Falchi</a>",
          "description": "Deepfakes are the result of digital manipulation to obtain credible videos in\norder to deceive the viewer. This is done through deep learning techniques\nbased on autoencoders or GANs that become more accessible and accurate year\nafter year, resulting in fake videos that are very difficult to distinguish\nfrom real ones. Traditionally, CNN networks have been used to perform deepfake\ndetection, with the best results obtained using methods based on EfficientNet\nB7. In this study, we combine various types of Vision Transformers with a\nconvolutional EfficientNet B0 used as a feature extractor, obtaining comparable\nresults with some very recent methods that use Vision Transformers. Differently\nfrom the state-of-the-art approaches, we use neither distillation nor ensemble\nmethods. The best model achieved an AUC of 0.951 and an F1 score of 88.0%, very\nclose to the state-of-the-art on the DeepFake Detection Challenge (DFDC).",
          "link": "http://arxiv.org/abs/2107.02612",
          "publishedOn": "2021-07-07T01:57:11.849Z",
          "wordCount": 579,
          "title": "Combining EfficientNet and Vision Transformers for Video Deepfake Detection. (arXiv:2107.02612v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sanket Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1\">Pau Riba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1\">Josep Llad&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>",
          "description": "Despite significant progress on current state-of-the-art image generation\nmodels, synthesis of document images containing multiple and complex object\nlayouts is a challenging task. This paper presents a novel approach, called\nDocSynth, to automatically synthesize document images based on a given layout.\nIn this work, given a spatial layout (bounding boxes with object categories) as\na reference by the user, our proposed DocSynth model learns to generate a set\nof realistic document images consistent with the defined layout. Also, this\nframework has been adapted to this work as a superior baseline model for\ncreating synthetic document image datasets for augmenting real data during\ntraining for document layout analysis tasks. Different sets of learning\nobjectives have been also used to improve the model performance.\nQuantitatively, we also compare the generated results of our model with real\ndata using standard evaluation metrics. The results highlight that our model\ncan successfully generate realistic and diverse document images with multiple\nobjects. We also present a comprehensive qualitative analysis summary of the\ndifferent scopes of synthetic image generation tasks. Lastly, to our knowledge\nthis is the first work of its kind.",
          "link": "http://arxiv.org/abs/2107.02638",
          "publishedOn": "2021-07-07T01:57:11.781Z",
          "wordCount": 629,
          "title": "DocSynth: A Layout Guided Approach for Controllable Document Image Synthesis. (arXiv:2107.02638v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.09809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+yang_Y/0/1/0/all/0/1\">Yi yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>",
          "description": "This paper considers deep visual recognition on long-tailed data. To be\ngeneral, we consider two applied scenarios, \\ie, deep classification and deep\nmetric learning. Under the long-tailed data distribution, the majority classes\n(\\ie, tail classes) only occupy relatively few samples and are prone to lack of\nwithin-class diversity. A radical solution is to augment the tail classes with\nhigher diversity. To this end, we introduce a simple and reliable method named\nMemory-based Jitter (MBJ). We observe that during training, the deep model\nconstantly changes its parameters after every iteration, yielding the\nphenomenon of \\emph{weight jitters}. Consequentially, given a same image as the\ninput, two historical editions of the model generate two different features in\nthe deeply-embedded space, resulting in \\emph{feature jitters}. Using a memory\nbank, we collect these (model or feature) jitters across multiple training\niterations and get the so-called Memory-based Jitter. The accumulated jitters\nenhance the within-class diversity for the tail classes and consequentially\nimproves long-tailed visual recognition. With slight modifications, MBJ is\napplicable for two fundamental visual recognition tasks, \\emph{i.e.}, deep\nimage classification and deep metric learning (on long-tailed data). Extensive\nexperiments on five long-tailed classification benchmarks and two deep metric\nlearning benchmarks demonstrate significant improvement. Moreover, the achieved\nperformance are on par with the state of the art on both tasks.",
          "link": "http://arxiv.org/abs/2008.09809",
          "publishedOn": "2021-07-07T01:57:11.765Z",
          "wordCount": 753,
          "title": "Memory-based Jitter: Improving Visual Recognition on Long-tailed Data with Diversity In Memory. (arXiv:2008.09809v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1\">Juncong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidenreich_P/0/1/0/all/0/1\">Philipp Heidenreich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Frank Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>",
          "description": "Semantic understanding of the surrounding environment is essential for\nautomated vehicles. The recent publication of the SemanticKITTI dataset\nstimulates the research on semantic segmentation of LiDAR point clouds in urban\nscenarios. While most existing approaches predict sparse pointwise semantic\nclasses for the sparse input LiDAR scan, we propose PillarSegNet to be able to\noutput a dense semantic grid map. In contrast to a previously proposed grid map\nmethod, PillarSegNet uses PointNet to learn features directly from the 3D point\ncloud and then conducts 2D semantic segmentation in the top view. To train and\nevaluate our approach, we use both sparse and dense ground truth, where the\ndense ground truth is obtained from multiple superimposed scans. Experimental\nresults on the SemanticKITTI dataset show that PillarSegNet achieves a\nperformance gain of about 10% mIoU over the state-of-the-art grid map method.",
          "link": "http://arxiv.org/abs/2105.04169",
          "publishedOn": "2021-07-07T01:57:11.758Z",
          "wordCount": 620,
          "title": "PillarSegNet: Pillar-based Semantic Grid Map Estimation using Sparse LiDAR Data. (arXiv:2105.04169v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>",
          "description": "This paper proposes a method for representation learning of multimodal data\nusing contrastive losses. A traditional approach is to contrast different\nmodalities to learn the information shared between them. However, that approach\ncould fail to learn the complementary synergies between modalities that might\nbe useful for downstream tasks. Another approach is to concatenate all the\nmodalities into a tuple and then contrast positive and negative tuple\ncorrespondences. However, that approach could consider only the stronger\nmodalities while ignoring the weaker ones. To address these issues, we propose\na novel contrastive learning objective, TupleInfoNCE. It contrasts tuples based\nnot only on positive and negative correspondences but also by composing new\nnegative tuples using modalities describing different scenes. Training with\nthese additional negatives encourages the learning model to examine the\ncorrespondences among modalities in the same tuple, ensuring that weak\nmodalities are not ignored. We provide a theoretical justification based on\nmutual information for why this approach works, and we propose a sample\noptimization algorithm to generate positive and negative samples to maximize\ntraining efficacy. We find that TupleInfoNCE significantly outperforms the\nprevious state of the arts on three different downstream tasks.",
          "link": "http://arxiv.org/abs/2107.02575",
          "publishedOn": "2021-07-07T01:57:11.730Z",
          "wordCount": 624,
          "title": "Contrastive Multimodal Fusion with TupleInfoNCE. (arXiv:2107.02575v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1\">Pranav Jeevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a> (Indian Institute of Technology Bombay)",
          "description": "Linear attention mechanisms provide hope for overcoming the bottleneck of\nquadratic complexity which restricts application of transformer models in\nvision tasks. We modify the ViT architecture to work on longer sequence data by\nreplacing the quadratic attention with efficient transformers like Performer,\nLinformer and Nystr\\\"omformer of linear complexity creating Vision X-formers\n(ViX). We show that ViX performs better than ViT in image classification\nconsuming lesser computing resources. We further show that replacing the\nembedding linear layer by convolutional layers in ViX further increases their\nperformance. Our test on recent visions transformer models like LeViT and\nCompact Convolutional Transformer (CCT) show that replacing the attention with\nNystr\\\"omformer or Performer saves GPU usage and memory without deteriorating\nperformance. Incorporating these changes can democratize transformers by making\nthem accessible to those with limited data and computing resources.",
          "link": "http://arxiv.org/abs/2107.02239",
          "publishedOn": "2021-07-07T01:57:11.688Z",
          "wordCount": 598,
          "title": "Vision Xformers: Efficient Attention for Image Classification. (arXiv:2107.02239v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jing_J/0/1/0/all/0/1\">Junfeng Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weichuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changming Sun</a>",
          "description": "Interest point detection is one of the most fundamental and critical problems\nin computer vision and image processing. In this paper, we carry out a\ncomprehensive review on image feature information (IFI) extraction techniques\nfor interest point detection. To systematically introduce how the existing\ninterest point detection methods extract IFI from an input image, we propose a\ntaxonomy of the IFI extraction techniques for interest point detection.\nAccording to this taxonomy, we discuss different types of IFI extraction\ntechniques for interest point detection. Furthermore, we identify the main\nunresolved issues related to the existing IFI extraction techniques for\ninterest point detection and any interest point detection methods that have not\nbeen discussed before. The existing popular datasets and evaluation standards\nare provided and the performances for eighteen state-of-the-art approaches are\nevaluated and discussed. Moreover, future research directions on IFI extraction\ntechniques for interest point detection are elaborated.",
          "link": "http://arxiv.org/abs/2106.07929",
          "publishedOn": "2021-07-07T01:57:11.682Z",
          "wordCount": 637,
          "title": "Image Feature Information Extraction for Interest Point Detection: A Comprehensive Review. (arXiv:2106.07929v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tissera_D/0/1/0/all/0/1\">Dumindu Tissera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vithanage_K/0/1/0/all/0/1\">Kasun Vithanage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijessinghe_R/0/1/0/all/0/1\">Rukshan Wijessinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_S/0/1/0/all/0/1\">Subha Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>",
          "description": "Neural networks are known to give better performance with increased depth due\nto their ability to learn more abstract features. Although the deepening of\nnetworks has been well established, there is still room for efficient feature\nextraction within a layer which would reduce the need for mere parameter\nincrement. The conventional widening of networks by having more filters in each\nlayer introduces a quadratic increment of parameters. Having multiple parallel\nconvolutional/dense operations in each layer solves this problem, but without\nany context-dependent allocation of resources among these operations: the\nparallel computations tend to learn similar features making the widening\nprocess less effective. Therefore, we propose the use of multi-path neural\nnetworks with data-dependent resource allocation among parallel computations\nwithin layers, which also lets an input to be routed end-to-end through these\nparallel paths. To do this, we first introduce a cross-prediction based\nalgorithm between parallel tensors of subsequent layers. Second, we further\nreduce the routing overhead by introducing feature-dependent cross-connections\nbetween parallel tensors of successive layers. Our multi-path networks show\nsuperior performance to existing widening and adaptive feature extraction, and\neven ensembles, and deeper networks at similar complexity in the image\nrecognition task.",
          "link": "http://arxiv.org/abs/2107.02450",
          "publishedOn": "2021-07-07T01:57:11.675Z",
          "wordCount": 637,
          "title": "End-To-End Data-Dependent Routing in Multi-Path Neural Networks. (arXiv:2107.02450v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vorontsov_E/0/1/0/all/0/1\">Eugene Vorontsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadoury_S/0/1/0/all/0/1\">Samuel Kadoury</a>",
          "description": "Imperfect labels limit the quality of predictions learned by deep neural\nnetworks. This is particularly relevant in medical image segmentation, where\nreference annotations are difficult to collect and vary significantly even\nacross expert annotators. Prior work on mitigating label noise focused on\nsimple models of mostly uniform noise. In this work, we explore biased and\nunbiased errors artificially introduced to brain tumour annotations on MRI\ndata. We found that supervised and semi-supervised segmentation methods are\nrobust or fairly robust to unbiased errors but sensitive to biased errors. It\nis therefore important to identify the sorts of errors expected in medical\nimage labels and especially mitigate the biased errors.",
          "link": "http://arxiv.org/abs/2107.02189",
          "publishedOn": "2021-07-07T01:57:11.668Z",
          "wordCount": 550,
          "title": "Label noise in segmentation networks : mitigation must deal with bias. (arXiv:2107.02189v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shenghua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shaoqun Zeng</a>",
          "description": "Digital gigapixel whole slide image (WSI) is widely used in clinical\ndiagnosis, and automated WSI analysis is key for computer-aided diagnosis.\nCurrently, analyzing the integrated descriptor of probabilities or feature maps\nfrom massive local patches encoded by ResNet classifier is the main manner for\nWSI-level prediction. Feature representations of the sparse and tiny lesion\ncells in cervical slides, however, are still challengeable for the\nunder-promoted upstream encoders, while the unused spatial representations of\ncervical cells are the available features to supply the semantics analysis. As\nwell as patches sampling with overlap and repetitive processing incur the\ninefficiency and the unpredictable side effect. This study designs a novel\ninline connection network (InCNet) by enriching the multi-scale connectivity to\nbuild the lightweight model named You Only Look Cytopathology Once (YOLCO) with\nthe additional supervision of spatial information. The proposed model allows\nthe input size enlarged to megapixel that can stitch the WSI without any\noverlap by the average repeats decreased from $10^3\\sim10^4$ to $10^1\\sim10^2$\nfor collecting features and predictions at two scales. Based on Transformer for\nclassifying the integrated multi-scale multi-task features, the experimental\nresults appear $0.872$ AUC score better and $2.51\\times$ faster than the best\nconventional method in WSI classification on multicohort datasets of 2,019\nslides from four scanning devices.",
          "link": "http://arxiv.org/abs/2106.15113",
          "publishedOn": "2021-07-07T01:57:11.663Z",
          "wordCount": 693,
          "title": "An Efficient Cervical Whole Slide Image Analysis Framework Based on Multi-scale Semantic and Spatial Deep Features. (arXiv:2106.15113v2 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keetha_N/0/1/0/all/0/1\">Nikhil Varma Keetha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>",
          "description": "Visual Place Recognition (VPR) approaches have typically attempted to match\nplaces by identifying visual cues, image regions or landmarks that have high\n``utility'' in identifying a specific place. But this concept of utility is not\nsingular - rather it can take a range of forms. In this paper, we present a\nnovel approach to deduce two key types of utility for VPR: the utility of\nvisual cues `specific' to an environment, and to a particular place. We employ\ncontrastive learning principles to estimate both the environment- and\nplace-specific utility of Vector of Locally Aggregated Descriptors (VLAD)\nclusters in an unsupervised manner, which is then used to guide local feature\nmatching through keypoint selection. By combining these two utility measures,\nour approach achieves state-of-the-art performance on three challenging\nbenchmark datasets, while simultaneously reducing the required storage and\ncompute time. We provide further analysis demonstrating that unsupervised\ncluster selection results in semantically meaningful results, that finer\ngrained categorization often has higher utility for VPR than high level\nsemantic categorization (e.g. building, road), and characterise how these two\nutility measures vary across different places and environments. Source code is\nmade publicly available at https://github.com/Nik-V9/HEAPUtil.",
          "link": "http://arxiv.org/abs/2107.02440",
          "publishedOn": "2021-07-07T01:57:11.645Z",
          "wordCount": 650,
          "title": "A Hierarchical Dual Model of Environment- and Place-Specific Utility for Visual Place Recognition. (arXiv:2107.02440v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albanwan_H/0/1/0/all/0/1\">Hessah Albanwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>",
          "description": "Remote sensing images and techniques are powerful tools to investigate earth\nsurface. Data quality is the key to enhance remote sensing applications and\nobtaining a clear and noise-free set of data is very difficult in most\nsituations due to the varying acquisition (e.g., atmosphere and season),\nsensor, and platform (e.g., satellite angles and sensor characteristics)\nconditions. With the increasing development of satellites, nowadays Terabytes\nof remote sensing images can be acquired every day. Therefore, information and\ndata fusion can be particularly important in the remote sensing community. The\nfusion integrates data from various sources acquired asynchronously for\ninformation extraction, analysis, and quality improvement. In this chapter, we\naim to discuss the theory of spatiotemporal fusion by investigating previous\nworks, in addition to describing the basic concepts and some of its\napplications by summarizing our prior and ongoing works.",
          "link": "http://arxiv.org/abs/2107.02701",
          "publishedOn": "2021-07-07T01:57:11.637Z",
          "wordCount": 563,
          "title": "Spatiotemporal Fusion in Remote Sensing. (arXiv:2107.02701v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02572",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Barbano_R/0/1/0/all/0/1\">Riccardo Barbano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kereta_Z/0/1/0/all/0/1\">Zeljko Kereta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hauptmann_A/0/1/0/all/0/1\">Andreas Hauptmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arridge_S/0/1/0/all/0/1\">Simon R. Arridge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_B/0/1/0/all/0/1\">Bangti Jin</a>",
          "description": "Deep learning-based image reconstruction approaches have demonstrated\nimpressive empirical performance in many imaging modalities. These approaches\ngenerally require a large amount of high-quality training data, which is often\nnot available. To circumvent this issue, we develop a novel unsupervised\nknowledge-transfer paradigm for learned iterative reconstruction within a\nBayesian framework. The proposed approach learns an iterative reconstruction\nnetwork in two phases. The first phase trains a reconstruction network with a\nset of ordered pairs comprising of ground truth images and measurement data.\nThe second phase fine-tunes the pretrained network to the measurement data\nwithout supervision. Furthermore, the framework delivers uncertainty\ninformation over the reconstructed image. We present extensive experimental\nresults on low-dose and sparse-view computed tomography, showing that the\nproposed framework significantly improves reconstruction quality not only\nvisually, but also quantitatively in terms of PSNR and SSIM, and is competitive\nwith several state-of-the-art supervised and unsupervised reconstruction\ntechniques.",
          "link": "http://arxiv.org/abs/2107.02572",
          "publishedOn": "2021-07-07T01:57:11.612Z",
          "wordCount": 591,
          "title": "Unsupervised Knowledge-Transfer for Learned Image Reconstruction. (arXiv:2107.02572v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1904.08084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nanni_L/0/1/0/all/0/1\">L. Nanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahnam_S/0/1/0/all/0/1\">S. Brahnam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidoni_S/0/1/0/all/0/1\">S. Ghidoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguolo_G/0/1/0/all/0/1\">G. Maguolo</a>",
          "description": "Bioimage classification plays a crucial role in many biological problems. In\nthis work, we present a new General Purpose (GenP) ensemble that boosts\nperformance by combining local features, dense sampling features, and deep\nlearning approaches. First, we introduce three new methods for data\naugmentation based on PCA/DCT; second, we show that different data augmentation\napproaches can boost the performance of an ensemble of CNNs; and, finally, we\npropose a set of handcrafted/learned descriptors that are highly generalizable.\nEach handcrafted descriptor is used to train a different Support Vector Machine\n(SVM), and the different SVMs are combined with the ensemble of CNNs. Our\nmethod is evaluated on a diverse set of bioimage classification problems.\nResults demonstrate that the proposed GenP bioimage ensemble obtains\nstate-of-the-art performance without any ad-hoc dataset tuning of parameters\n(thus avoiding the risk of overfitting/overtraining).",
          "link": "http://arxiv.org/abs/1904.08084",
          "publishedOn": "2021-07-07T01:57:11.596Z",
          "wordCount": 642,
          "title": "General Purpose (GenP) Bioimage Ensemble of Handcrafted and Learned Features with Data Augmentation. (arXiv:1904.08084v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02476",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zaridis_D/0/1/0/all/0/1\">Dimitrios G. Zaridis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mylona_E/0/1/0/all/0/1\">Eugenia Mylona</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marias_K/0/1/0/all/0/1\">Kostas Marias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papanikolaou_N/0/1/0/all/0/1\">Nikolaos Papanikolaou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tachos_N/0/1/0/all/0/1\">Nikolaos S. Tachos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fotiadis_D/0/1/0/all/0/1\">Dimitrios I. Fotiadis</a>",
          "description": "Prostate segmentation from magnetic resonance imaging (MRI) is a challenging\ntask. In recent years, several network architectures have been proposed to\nautomate this process and alleviate the burden of manual annotation. Although\nthe performance of these models has achieved promising results, there is still\nroom for improvement before these models can be used safely and effectively in\nclinical practice. One of the major challenges in prostate MR image\nsegmentation is the presence of class imbalance in the image labels where the\nbackground pixels dominate over the prostate. In the present work we propose a\nDL-based pipeline for cropping the region around the prostate from MRI images\nto produce a more balanced distribution of the foreground pixels (prostate) and\nthe background pixels and improve segmentation accuracy. The effect of\nDL-cropping for improving the segmentation performance compared to standard\ncenter-cropping is assessed using five popular DL networks for prostate\nsegmentation, namely U-net, U-net+, Res Unet++, Bridge U-net and Dense U-net.\nThe proposed smart-cropping outperformed the standard center cropping in terms\nof segmentation accuracy for all the evaluated prostate segmentation networks.\nIn terms of Dice score, the highest improvement was achieved for the U-net+ and\nResU-net++ architectures corresponding to 8.9% and 8%, respectively.",
          "link": "http://arxiv.org/abs/2107.02476",
          "publishedOn": "2021-07-07T01:57:11.508Z",
          "wordCount": 668,
          "title": "A new smart-cropping pipeline for prostate segmentation using deep learning networks. (arXiv:2107.02476v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qingyong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Linhai Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_S/0/1/0/all/0/1\">Stefano Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>",
          "description": "We study the problem of efficient semantic segmentation of large-scale 3D\npoint clouds. By relying on expensive sampling techniques or computationally\nheavy pre/post-processing steps, most existing approaches are only able to be\ntrained and operate over small-scale point clouds. In this paper, we introduce\nRandLA-Net, an efficient and lightweight neural architecture to directly infer\nper-point semantics for large-scale point clouds. The key to our approach is to\nuse random point sampling instead of more complex point selection approaches.\nAlthough remarkably computation and memory efficient, random sampling can\ndiscard key features by chance. To overcome this, we introduce a novel local\nfeature aggregation module to progressively increase the receptive field for\neach 3D point, thereby effectively preserving geometric details. Comparative\nexperiments show that our RandLA-Net can process 1 million points in a single\npass up to 200x faster than existing approaches. Moreover, extensive\nexperiments on five large-scale point cloud datasets, including Semantic3D,\nSemanticKITTI, Toronto3D, NPM3D and S3DIS, demonstrate the state-of-the-art\nsemantic segmentation performance of our RandLA-Net.",
          "link": "http://arxiv.org/abs/2107.02389",
          "publishedOn": "2021-07-07T01:57:11.490Z",
          "wordCount": 638,
          "title": "Learning Semantic Segmentation of Large-Scale Point Clouds with Random Sampling. (arXiv:2107.02389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02319",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johansen_D/0/1/0/all/0/1\">Dag Johansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johansen_H/0/1/0/all/0/1\">H&#xe5;vard D. Johansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>",
          "description": "Minimally invasive surgery is a surgical intervention used to examine the\norgans inside the abdomen and has been widely used due to its effectiveness\nover open surgery. Due to the hardware improvements such as high definition\ncameras, this procedure has significantly improved and new software methods\nhave demonstrated potential for computer-assisted procedures. However, there\nexists challenges and requirements to improve detection and tracking of the\nposition of the instruments during these surgical procedures. To this end, we\nevaluate and compare some popular deep learning methods that can be explored\nfor the automated segmentation of surgical instruments in laparoscopy, an\nimportant step towards tool tracking. Our experimental results exhibit that the\nDual decoder attention network (DDANet) produces a superior result compared to\nother recent deep learning methods. DDANet yields a Dice coefficient of 0.8739\nand mean intersection-over-union of 0.8183 for the Robust Medical Instrument\nSegmentation (ROBUST-MIS) Challenge 2019 dataset, at a real-time speed of\n101.36 frames-per-second that is critical for such procedures.",
          "link": "http://arxiv.org/abs/2107.02319",
          "publishedOn": "2021-07-07T01:57:11.469Z",
          "wordCount": 621,
          "title": "Exploring Deep Learning Methods for Real-Time Surgical Instrument Segmentation in Laparoscopy. (arXiv:2107.02319v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.14785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md Zakir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1\">Shafin Rahman</a>",
          "description": "Interactive facial image manipulation attempts to edit single and multiple\nface attributes using a photo-realistic face and/or semantic mask as input. In\nthe absence of the photo-realistic image (only sketch/mask available), previous\nmethods only retrieve the original face but ignore the potential of aiding\nmodel controllability and diversity in the translation process. This paper\nproposes a sketch-to-image generation framework called S2FGAN, aiming to\nimprove users' ability to interpret and flexibility of face attribute editing\nfrom a simple sketch. The proposed framework modifies the constrained latent\nspace semantics trained on Generative Adversarial Networks (GANs). We employ\ntwo latent spaces to control the face appearance and adjust the desired\nattributes of the generated face. Instead of constraining the translation\nprocess by using a reference image, the users can command the model to retouch\nthe generated images by involving the semantic information in the generation\nprocess. In this way, our method can manipulate single or multiple face\nattributes by only specifying attributes to be changed. Extensive experimental\nresults on CelebAMask-HQ dataset empirically shows our superior performance and\neffectiveness on this task. Our method successfully outperforms\nstate-of-the-art methods on attribute manipulation by exploiting greater\ncontrol of attribute intensity.",
          "link": "http://arxiv.org/abs/2011.14785",
          "publishedOn": "2021-07-07T01:57:11.463Z",
          "wordCount": 657,
          "title": "S2FGAN: Semantically Aware Interactive Sketch-to-Face Translation. (arXiv:2011.14785v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lianmin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>",
          "description": "The increasing size of neural network models has been critical for\nimprovements in their accuracy, but device memory is not growing at the same\nrate. This creates fundamental challenges for training neural networks within\nlimited memory environments. In this work, we propose ActNN, a memory-efficient\ntraining framework that stores randomly quantized activations for back\npropagation. We prove the convergence of ActNN for general network\narchitectures, and we characterize the impact of quantization on the\nconvergence via an exact expression for the gradient variance. Using our\ntheory, we propose novel mixed-precision quantization strategies that exploit\nthe activation's heterogeneity across feature dimensions, samples, and layers.\nThese techniques can be readily applied to existing dynamic graph frameworks,\nsuch as PyTorch, simply by substituting the layers. We evaluate ActNN on\nmainstream computer vision models for classification, detection, and\nsegmentation tasks. On all these tasks, ActNN compresses the activation to 2\nbits on average, with negligible accuracy loss. ActNN reduces the memory\nfootprint of the activation by 12x, and it enables training with a 6.6x to 14x\nlarger batch size.",
          "link": "http://arxiv.org/abs/2104.14129",
          "publishedOn": "2021-07-07T01:57:11.457Z",
          "wordCount": 667,
          "title": "ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training. (arXiv:2104.14129v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06757",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kamran_S/0/1/0/all/0/1\">Sharif Amit Kamran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hossain_K/0/1/0/all/0/1\">Khondker Fariha Hossain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tavakkoli_A/0/1/0/all/0/1\">Alireza Tavakkoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuckerbrod_S/0/1/0/all/0/1\">Stewart Lee Zuckerbrod</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baker_S/0/1/0/all/0/1\">Salah A. Baker</a>",
          "description": "In Fluorescein Angiography (FA), an exogenous dye is injected in the\nbloodstream to image the vascular structure of the retina. The injected dye can\ncause adverse reactions such as nausea, vomiting, anaphylactic shock, and even\ndeath. In contrast, color fundus imaging is a non-invasive technique used for\nphotographing the retina but does not have sufficient fidelity for capturing\nits vascular structure. The only non-invasive method for capturing retinal\nvasculature is optical coherence tomography-angiography (OCTA). However, OCTA\nequipment is quite expensive, and stable imaging is limited to small areas on\nthe retina. In this paper, we propose a novel conditional generative\nadversarial network (GAN) capable of simultaneously synthesizing FA images from\nfundus photographs while predicting retinal degeneration. The proposed system\nhas the benefit of addressing the problem of imaging retinal vasculature in a\nnon-invasive manner as well as predicting the existence of retinal\nabnormalities. We use a semi-supervised approach to train our GAN using\nmultiple weighted losses on different modalities of data. Our experiments\nvalidate that the proposed architecture exceeds recent state-of-the-art\ngenerative networks for fundus-to-angiography synthesis. Moreover, our vision\ntransformer-based discriminators generalize quite well on out-of-distribution\ndata sets for retinal disease prediction.",
          "link": "http://arxiv.org/abs/2104.06757",
          "publishedOn": "2021-07-07T01:57:11.451Z",
          "wordCount": 673,
          "title": "VTGAN: Semi-supervised Retinal Image Synthesis and Disease Prediction using Vision Transformers. (arXiv:2104.06757v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Scene text detection and recognition have been well explored in the past few\nyears. Despite the progress, efficient and accurate end-to-end spotting of\narbitrarily-shaped text remains challenging. In this work, we propose an\nend-to-end text spotting framework, termed PAN++, which can efficiently detect\nand recognize text of arbitrary shapes in natural scenes. PAN++ is based on the\nkernel representation that reformulates a text line as a text kernel (central\nregion) surrounded by peripheral pixels. By systematically comparing with\nexisting scene text representations, we show that our kernel representation can\nnot only describe arbitrarily-shaped text but also well distinguish adjacent\ntext. Moreover, as a pixel-based representation, the kernel representation can\nbe predicted by a single fully convolutional network, which is very friendly to\nreal-time applications. Taking the advantages of the kernel representation, we\ndesign a series of components as follows: 1) a computationally efficient\nfeature enhancement network composed of stacked Feature Pyramid Enhancement\nModules (FPEMs); 2) a lightweight detection head cooperating with Pixel\nAggregation (PA); and 3) an efficient attention-based recognition head with\nMasked RoI. Benefiting from the kernel representation and the tailored\ncomponents, our method achieves high inference speed while maintaining\ncompetitive accuracy. Extensive experiments show the superiority of our method.\nFor example, the proposed PAN++ achieves an end-to-end text spotting F-measure\nof 64.9 at 29.2 FPS on the Total-Text dataset, which significantly outperforms\nthe previous best method. Code will be available at: https://git.io/PAN.",
          "link": "http://arxiv.org/abs/2105.00405",
          "publishedOn": "2021-07-07T01:57:11.434Z",
          "wordCount": 728,
          "title": "PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text. (arXiv:2105.00405v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06742",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1\">Chun-Mei Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yunlu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>",
          "description": "The core problem of Magnetic Resonance Imaging (MRI) is the trade off between\nacceleration and image quality. Image reconstruction and super-resolution are\ntwo crucial techniques in Magnetic Resonance Imaging (MRI). Current methods are\ndesigned to perform these tasks separately, ignoring the correlations between\nthem. In this work, we propose an end-to-end task transformer network\n(T$^2$Net) for joint MRI reconstruction and super-resolution, which allows\nrepresentations and feature transmission to be shared between multiple task to\nachieve higher-quality, super-resolved and motion-artifacts-free images from\nhighly undersampled and degenerated MRI data. Our framework combines both\nreconstruction and super-resolution, divided into two sub-branches, whose\nfeatures are expressed as queries and keys. Specifically, we encourage joint\nfeature learning between the two tasks, thereby transferring accurate task\ninformation. We first use two separate CNN branches to extract task-specific\nfeatures. Then, a task transformer module is designed to embed and synthesize\nthe relevance between the two tasks. Experimental results show that our\nmulti-task model significantly outperforms advanced sequential methods, both\nquantitatively and qualitatively.",
          "link": "http://arxiv.org/abs/2106.06742",
          "publishedOn": "2021-07-07T01:57:11.428Z",
          "wordCount": 643,
          "title": "Task Transformer Network for Joint MRI Reconstruction and Super-Resolution. (arXiv:2106.06742v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Li Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">He Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>",
          "description": "Traffic event cognition and reasoning in videos is an important task that has\na wide range of applications in intelligent transportation, assisted driving,\nand autonomous vehicles. In this paper, we create a novel dataset,\nSUTD-TrafficQA (Traffic Question Answering), which takes the form of video QA\nbased on the collected 10,080 in-the-wild videos and annotated 62,535 QA pairs,\nfor benchmarking the cognitive capability of causal inference and event\nunderstanding models in complex traffic scenarios. Specifically, we propose 6\nchallenging reasoning tasks corresponding to various traffic scenarios, so as\nto evaluate the reasoning capability over different kinds of complex yet\npractical traffic events. Moreover, we propose Eclipse, a novel Efficient\nglimpse network via dynamic inference, in order to achieve\ncomputation-efficient and reliable video reasoning. The experiments show that\nour method achieves superior performance while reducing the computation cost\nsignificantly. The project page: https://github.com/SUTDCV/SUTD-TrafficQA.",
          "link": "http://arxiv.org/abs/2103.15538",
          "publishedOn": "2021-07-07T01:57:11.421Z",
          "wordCount": 633,
          "title": "SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events. (arXiv:2103.15538v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01321",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Naruenatthanaset_K/0/1/0/all/0/1\">Korranat Naruenatthanaset</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chalidabhongse_T/0/1/0/all/0/1\">Thanarat H. Chalidabhongse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palasuwan_D/0/1/0/all/0/1\">Duangdao Palasuwan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palasuwan_A/0/1/0/all/0/1\">Attakorn Palasuwan</a>",
          "description": "Automated red blood cell (RBC) classification on blood smear images helps\nhematologists to analyze RBC lab results in a reduced time and cost. However,\noverlapping cells can cause incorrect predicted results, and so they have to be\nseparated into multiple single RBCs before classifying. To classify multiple\nclasses with deep learning, imbalance problems are common in medical imaging\nbecause normal samples are always higher than rare disease samples. This paper\npresents a new method to segment and classify RBCs from blood smear images,\nspecifically to tackle cell overlapping and data imbalance problems. Focusing\non overlapping cell separation, our segmentation process first estimates\nellipses to represent RBCs. The method detects the concave points and then\nfinds the ellipses using directed ellipse fitting. The accuracy from 20 blood\nsmear images was 0.889. Classification requires balanced training datasets.\nHowever, some RBC types are rare. The imbalance ratio of this dataset was\n34.538 for 12 RBC classes from 20,875 individual RBC samples. The use of\nmachine learning for RBC classification with an imbalanced dataset is hence\nmore challenging than many other applications. We analyzed techniques to deal\nwith this problem. The best accuracy and F1-score were 0.921 and 0.8679,\nrespectively, using EfficientNet-B1 with augmentation. Experimental results\nshowed that the weight balancing technique with augmentation had the potential\nto deal with imbalance problems by improving the F1-score on minority classes,\nwhile data augmentation significantly improved the overall classification\nperformance.",
          "link": "http://arxiv.org/abs/2012.01321",
          "publishedOn": "2021-07-07T01:57:11.415Z",
          "wordCount": 744,
          "title": "Red Blood Cell Segmentation with Overlapping Cell Separation and Classification on Imbalanced Dataset. (arXiv:2012.01321v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.10274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Ying Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhenzhou Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hairong Qi</a>",
          "description": "Photorealistic stylization aims to transfer the style of a reference photo\nonto a content photo in a natural fashion, such that the stylized image looks\nlike a real photo taken by a camera. State-of-the-art methods stylize the image\nlocally within each matched semantic region and are prone to global color\ninconsistency across semantic objects/parts, making the stylized image less\nphotorealistic. To tackle the challenging issues, we propose a non-local\nrepresentation scheme, constrained with a mutual affine-transfer network\n(NL-MAT). Through a dictionary-based decomposition, NL-MAT is able to\nsuccessfully decouple matched non-local representations and color information\nof the image pair, such that the context correspondence between the image pair\nis incorporated naturally, which largely facilitates local style transfer in a\nglobal-consistent fashion. To the best of our knowledge, this is the first\nattempt to address the photorealistic stylization problem with a non-local\nrepresentation scheme, such that no additional models or steps for semantic\nmatching are required during stylization. Experimental results demonstrate that\nthe proposed method is able to generate photorealistic results with local style\ntransfer while preserving both the spatial structure and global color\nconsistency of the content image.",
          "link": "http://arxiv.org/abs/1907.10274",
          "publishedOn": "2021-07-07T01:57:11.408Z",
          "wordCount": 662,
          "title": "Non-Local Representation based Mutual Affine-Transfer Network for Photorealistic Stylization. (arXiv:1907.10274v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teh_E/0/1/0/all/0/1\">Eu Wern Teh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeVries_T/0/1/0/all/0/1\">Terrance DeVries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duke_B/0/1/0/all/0/1\">Brendan Duke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Ruowei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aarabi_P/0/1/0/all/0/1\">Parham Aarabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1\">Graham W. Taylor</a>",
          "description": "We consider the task of semi-supervised semantic segmentation, where we aim\nto produce pixel-wise semantic object masks given only a small number of\nhuman-labeled training examples. We focus on iterative self-training methods in\nwhich we explore the behavior of self-training over multiple refinement stages.\nWe show that iterative self-training leads to performance degradation if done\nna\\\"ively with a fixed ratio of human-labeled to pseudo-labeled training\nexamples. We propose Greedy Iterative Self-Training (GIST) and Random Iterative\nSelf-Training (RIST) strategies that alternate between training on either\nhuman-labeled data or pseudo-labeled data at each refinement stage, resulting\nin a performance boost rather than degradation. We further show that GIST and\nRIST can be combined with existing semi-supervised learning methods to boost\nperformance.",
          "link": "http://arxiv.org/abs/2103.17105",
          "publishedOn": "2021-07-07T01:57:11.390Z",
          "wordCount": 596,
          "title": "The GIST and RIST of Iterative Self-Training for Semi-Supervised Segmentation. (arXiv:2103.17105v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yezhen Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1\">Or Litany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>",
          "description": "3D object detection is an important yet demanding task that heavily relies on\ndifficult to obtain 3D annotations. To reduce the required amount of\nsupervision, we propose 3DIoUMatch, a novel semi-supervised method for 3D\nobject detection applicable to both indoor and outdoor scenes. We leverage a\nteacher-student mutual learning framework to propagate information from the\nlabeled to the unlabeled train set in the form of pseudo-labels. However, due\nto the high task complexity, we observe that the pseudo-labels suffer from\nsignificant noise and are thus not directly usable. To that end, we introduce a\nconfidence-based filtering mechanism, inspired by FixMatch. We set confidence\nthresholds based upon the predicted objectness and class probability to filter\nlow-quality pseudo-labels. While effective, we observe that these two measures\ndo not sufficiently capture localization quality. We therefore propose to use\nthe estimated 3D IoU as a localization metric and set category-aware\nself-adjusted thresholds to filter poorly localized proposals. We adopt VoteNet\nas our backbone detector on indoor datasets while we use PV-RCNN on the\nautonomous driving dataset, KITTI. Our method consistently improves\nstate-of-the-art methods on both ScanNet and SUN-RGBD benchmarks by significant\nmargins under all label ratios (including fully labeled setting). For example,\nwhen training using only 10\\% labeled data on ScanNet, 3DIoUMatch achieves 7.7%\nabsolute improvement on mAP@0.25 and 8.5% absolute improvement on mAP@0.5 upon\nthe prior art. On KITTI, we are the first to demonstrate semi-supervised 3D\nobject detection and our method surpasses a fully supervised baseline from 1.8%\nto 7.6% under different label ratios and categories.",
          "link": "http://arxiv.org/abs/2012.04355",
          "publishedOn": "2021-07-07T01:57:11.384Z",
          "wordCount": 736,
          "title": "3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection. (arXiv:2012.04355v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiaozhong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "Scene text spotting aims to detect and recognize the entire word or sentence\nwith multiple characters in natural images. It is still challenging because\nambiguity often occurs when the spacing between characters is large or the\ncharacters are evenly spread in multiple rows and columns, making many visually\nplausible groupings of the characters (e.g. \"BERLIN\" is incorrectly detected as\n\"BERL\" and \"IN\" in Fig. 1(c)). Unlike previous works that merely employed\nvisual features for text detection, this work proposes a novel text spotter,\nnamed Ambiguity Eliminating Text Spotter (AE TextSpotter), which learns both\nvisual and linguistic features to significantly reduce ambiguity in text\ndetection. The proposed AE TextSpotter has three important benefits. 1) The\nlinguistic representation is learned together with the visual representation in\na framework. To our knowledge, it is the first time to improve text detection\nby using a language model. 2) A carefully designed language module is utilized\nto reduce the detection confidence of incorrect text lines, making them easily\npruned in the detection stage. 3) Extensive experiments show that AE\nTextSpotter outperforms other state-of-the-art methods by a large margin. For\nexample, we carefully select a validation set of extremely ambiguous samples\nfrom the IC19-ReCTS dataset, where our approach surpasses other methods by more\nthan 4%. The code has been released at\nhttps://github.com/whai362/AE_TextSpotter. The image list and evaluation\nscripts of the validation set have been released at\nhttps://github.com/whai362/TDA-ReCTS.",
          "link": "http://arxiv.org/abs/2008.00714",
          "publishedOn": "2021-07-07T01:57:11.376Z",
          "wordCount": 749,
          "title": "AE TextSpotter: Learning Visual and Linguistic Representation for Ambiguous Text Spotting. (arXiv:2008.00714v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1\">Vittorio Mazzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvetti_F/0/1/0/all/0/1\">Francesco Salvetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>",
          "description": "Deep convolutional neural networks, assisted by architectural design\nstrategies, make extensive use of data augmentation techniques and layers with\na high number of feature maps to embed object transformations. That is highly\ninefficient and for large datasets implies a massive redundancy of features\ndetectors. Even though capsules networks are still in their infancy, they\nconstitute a promising solution to extend current convolutional networks and\nendow artificial visual perception with a process to encode more efficiently\nall feature affine transformations. Indeed, a properly working capsule network\nshould theoretically achieve higher results with a considerably lower number of\nparameters count due to intrinsic capability to generalize to novel viewpoints.\nNevertheless, little attention has been given to this relevant aspect. In this\npaper, we investigate the efficiency of capsule networks and, pushing their\ncapacity to the limits with an extreme architecture with barely 160K\nparameters, we prove that the proposed architecture is still able to achieve\nstate-of-the-art results on three different datasets with only 2% of the\noriginal CapsNet parameters. Moreover, we replace dynamic routing with a novel\nnon-iterative, highly parallelizable routing algorithm that can easily cope\nwith a reduced number of capsules. Extensive experimentation with other capsule\nimplementations has proved the effectiveness of our methodology and the\ncapability of capsule networks to efficiently embed visual representations more\nprone to generalization.",
          "link": "http://arxiv.org/abs/2101.12491",
          "publishedOn": "2021-07-07T01:57:11.369Z",
          "wordCount": 680,
          "title": "Efficient-CapsNet: Capsule Network with Self-Attention Routing. (arXiv:2101.12491v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.04026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chunbo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Peng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_G/0/1/0/all/0/1\">Geyong Min</a>",
          "description": "This paper proposes fractional order graph neural networks (FGNNs), optimized\nby the approximation strategy to address the challenges of local optimum of\nclassic and fractional graph neural networks which are specialised at\naggregating information from the feature and adjacent matrices of connected\nnodes and their neighbours to solve learning tasks on non-Euclidean data such\nas graphs. Meanwhile the approximate calculation of fractional order gradients\nalso overcomes the high computational complexity of fractional order\nderivations. We further prove that such an approximation is feasible and the\nFGNN is unbiased towards global optimization solution. Extensive experiments on\ncitation networks show that FGNN achieves great advantage over baseline models\nwhen selected appropriate fractional order.",
          "link": "http://arxiv.org/abs/2001.04026",
          "publishedOn": "2021-07-07T01:57:11.363Z",
          "wordCount": 604,
          "title": "Fractional order graph neural network. (arXiv:2001.04026v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02704",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Varadarajan_D/0/1/0/all/0/1\">Divya Varadarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bouman_K/0/1/0/all/0/1\">Katherine L. Bouman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kouwe_A/0/1/0/all/0/1\">Andre van der Kouwe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1\">Bruce Fischl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>",
          "description": "In neuroimaging, MRI tissue properties characterize underlying neurobiology,\nprovide quantitative biomarkers for neurological disease detection and\nanalysis, and can be used to synthesize arbitrary MRI contrasts. Estimating\ntissue properties from a single scan session using a protocol available on all\nclinical scanners promises to reduce scan time and cost, enable quantitative\nanalysis in routine clinical scans and provide scan-independent biomarkers of\ndisease. However, existing tissue properties estimation methods - most often\n$\\mathbf{T_1}$ relaxation, $\\mathbf{T_2^*}$ relaxation, and proton density\n($\\mathbf{PD}$) - require data from multiple scan sessions and cannot estimate\nall properties from a single clinically available MRI protocol such as the\nmultiecho MRI scan. In addition, the widespread use of non-standard acquisition\nparameters across clinical imaging sites require estimation methods that can\ngeneralize across varying scanner parameters. However, existing learning\nmethods are acquisition protocol specific and cannot estimate from heterogenous\nclinical data from different imaging sites. In this work we propose an\nunsupervised deep-learning strategy that employs MRI physics to estimate all\nthree tissue properties from a single multiecho MRI scan session, and\ngeneralizes across varying acquisition parameters. The proposed strategy\noptimizes accurate synthesis of new MRI contrasts from estimated latent tissue\nproperties, enabling unsupervised training, we also employ random acquisition\nparameters during training to achieve acquisition generalization. We provide\nthe first demonstration of estimating all tissue properties from a single\nmultiecho scan session. We demonstrate improved accuracy and generalizability\nfor tissue property estimation and MRI synthesis.",
          "link": "http://arxiv.org/abs/2107.02704",
          "publishedOn": "2021-07-07T01:57:11.346Z",
          "wordCount": 706,
          "title": "Unsupervised learning of MRI tissue properties using MRI physics models. (arXiv:2107.02704v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_M/0/1/0/all/0/1\">Malcolm Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matt Botvinick</a>",
          "description": "Neural networks have achieved success in a wide array of perceptual tasks but\noften fail at tasks involving both perception and higher-level reasoning. On\nthese more challenging tasks, bespoke approaches (such as modular symbolic\ncomponents, independent dynamics models or semantic parsers) targeted towards\nthat specific type of task have typically performed better. The downside to\nthese targeted approaches, however, is that they can be more brittle than\ngeneral-purpose neural networks, requiring significant modification or even\nredesign according to the particular task at hand. Here, we propose a more\ngeneral neural-network-based approach to dynamic visual reasoning problems that\nobtains state-of-the-art performance on three different domains, in each case\noutperforming bespoke modular approaches tailored specifically to the task. Our\nmethod relies on learned object-centric representations, self-attention and\nself-supervised dynamics learning, and all three elements together are required\nfor strong performance to emerge. The success of this combination suggests that\nthere may be no need to trade off flexibility for performance on problems\ninvolving spatio-temporal or causal-style reasoning. With the right soft biases\nand learning objectives in a neural network we may be able to attain the best\nof both worlds.",
          "link": "http://arxiv.org/abs/2012.08508",
          "publishedOn": "2021-07-07T01:57:11.339Z",
          "wordCount": 676,
          "title": "Attention over learned object embeddings enables complex visual reasoning. (arXiv:2012.08508v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiacheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>",
          "description": "Visual Semantic Embedding (VSE) is a dominant approach for vision-language\nretrieval, which aims at learning a deep embedding space such that visual data\nare embedded close to their semantic text labels or descriptions. Recent VSE\nmodels use complex methods to better contextualize and aggregate multi-modal\nfeatures into holistic embeddings. However, we discover that surprisingly\nsimple (but carefully selected) global pooling functions (e.g., max pooling)\noutperform those complex models, across different feature extractors. Despite\nits simplicity and effectiveness, seeking the best pooling function for\ndifferent data modality and feature extractor is costly and tedious, especially\nwhen the size of features varies (e.g., text, video). Therefore, we propose a\nGeneralized Pooling Operator (GPO), which learns to automatically adapt itself\nto the best pooling strategy for different features, requiring no manual tuning\nwhile staying effective and efficient. We extend the VSE model using this\nproposed GPO and denote it as VSE$\\infty$.\n\nWithout bells and whistles, VSE$\\infty$ outperforms previous VSE methods\nsignificantly on image-text retrieval benchmarks across popular feature\nextractors. With a simple adaptation, variants of VSE$\\infty$ further\ndemonstrate its strength by achieving the new state of the art on two\nvideo-text retrieval datasets. Comprehensive experiments and visualizations\nconfirm that GPO always discovers the best pooling strategy and can be a\nplug-and-play feature aggregation module for standard VSE models. Code and\npre-trained models are available at https://vse-infty.github.io.",
          "link": "http://arxiv.org/abs/2011.04305",
          "publishedOn": "2021-07-07T01:57:11.306Z",
          "wordCount": 737,
          "title": "Learning the Best Pooling Strategy for Visual Semantic Embedding. (arXiv:2011.04305v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.04947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1\">Marc Habermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyeongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>",
          "description": "Synthesizing realistic videos of humans using neural networks has been a\npopular alternative to the conventional graphics-based rendering pipeline due\nto its high efficiency. Existing works typically formulate this as an\nimage-to-image translation problem in 2D screen space, which leads to artifacts\nsuch as over-smoothing, missing body parts, and temporal instability of\nfine-scale detail, such as pose-dependent wrinkles in the clothing. In this\npaper, we propose a novel human video synthesis method that approaches these\nlimiting factors by explicitly disentangling the learning of time-coherent\nfine-scale details from the embedding of the human in 2D screen space. More\nspecifically, our method relies on the combination of two convolutional neural\nnetworks (CNNs). Given the pose information, the first CNN predicts a dynamic\ntexture map that contains time-coherent high-frequency details, and the second\nCNN conditions the generation of the final video on the temporally coherent\noutput of the first CNN. We demonstrate several applications of our approach,\nsuch as human reenactment and novel view synthesis from monocular video, where\nwe show significant improvement over the state of the art both qualitatively\nand quantitatively.",
          "link": "http://arxiv.org/abs/2001.04947",
          "publishedOn": "2021-07-07T01:57:11.287Z",
          "wordCount": 669,
          "title": "Neural Human Video Rendering by Learning Dynamic Textures and Rendering-to-Video Translation. (arXiv:2001.04947v3 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Arash Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farazi_H/0/1/0/all/0/1\">Hafez Farazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "Pose estimation commonly refers to computer vision methods that recognize\npeople's body postures in images or videos. With recent advancements in deep\nlearning, we now have compelling models to tackle the problem in real-time.\nSince these models are usually designed for human images, one needs to adapt\nexisting models to work on other creatures, including robots. This paper\nexamines different state-of-the-art pose estimation models and proposes a\nlightweight model that can work in real-time on humanoid robots in the RoboCup\nHumanoid League environment. Additionally, we present a novel dataset called\nthe HumanoidRobotPose dataset. The results of this work have the potential to\nenable many advanced behaviors for soccer-playing robots.",
          "link": "http://arxiv.org/abs/2107.02675",
          "publishedOn": "2021-07-07T01:57:11.279Z",
          "wordCount": 544,
          "title": "Real-time Pose Estimation from Images for Multiple Humanoid Robots. (arXiv:2107.02675v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/1906.01082",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yifeng Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_T/0/1/0/all/0/1\">Tingran Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhizhen Zhao</a>",
          "description": "We develop in this paper a novel intrinsic classification algorithm --\nmulti-frequency class averaging (MFCA) -- for classifying noisy projection\nimages obtained from three-dimensional cryo-electron microscopy (cryo-EM) by\nthe similarity among their viewing directions. This new algorithm leverages\nmultiple irreducible representations of the unitary group to introduce\nadditional redundancy into the representation of the optimal in-plane\nrotational alignment, extending and outperforming the existing class averaging\nalgorithm that uses only a single representation. The formal algebraic model\nand representation theoretic patterns of the proposed MFCA algorithm extend the\nframework of Hadani and Singer to arbitrary irreducible representations of the\nunitary group. We conceptually establish the consistency and stability of MFCA\nby inspecting the spectral properties of a generalized local parallel transport\noperator through the lens of Wigner $D$-matrices. We demonstrate the efficacy\nof the proposed algorithm with numerical experiments.",
          "link": "http://arxiv.org/abs/1906.01082",
          "publishedOn": "2021-07-07T01:57:11.273Z",
          "wordCount": 642,
          "title": "Representation Theoretic Patterns in Multi-Frequency Class Averaging for Three-Dimensional Cryo-Electron Microscopy. (arXiv:1906.01082v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02672",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1\">Nam Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_J/0/1/0/all/0/1\">J. Morris Chang</a>",
          "description": "This study proposed a novel framework for COVID-19 severity prediction, which\nis a combination of data-centric and model-centric approaches. First, we\npropose a data-centric pre-training for extremely scare data scenarios of the\ninvestigating dataset. Second, we propose two hybrid convolution-attention\nneural architectures that leverage the self-attention from Transformer and\nHopfield networks. Our proposed approach achieves significant improvement from\nthe conventional baseline approach. The best model from our proposed approach\nachieves $R^2 = 0.85 \\pm 0.05$ and Pearson correlation coefficient $\\rho = 0.92\n\\pm 0.02$ in geographic extend and $R^2 = 0.72 \\pm 0.09, \\rho = 0.85\\pm 0.06$\nin opacity prediction.",
          "link": "http://arxiv.org/abs/2107.02672",
          "publishedOn": "2021-07-07T01:57:11.266Z",
          "wordCount": 588,
          "title": "COVID-19 Pneumonia Severity Prediction using Hybrid Convolution-Attention Neural Architectures. (arXiv:2107.02672v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zineng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Since visual perception can give rich information beyond text descriptions\nfor world understanding, there has been increasing interest in leveraging\nvisual grounding for language learning. Recently, vokenization has attracted\nattention by using the predictions of a text-to-image retrieval model as labels\nfor language model supervision. Despite its success, the method suffers from\napproximation error of using finite image labels and the lack of vocabulary\ndiversity of a small image-text dataset. To overcome these limitations, we\npresent VidLanKD, a video-language knowledge distillation method for improving\nlanguage understanding. We train a multi-modal teacher model on a video-text\ndataset, and then transfer its knowledge to a student language model with a\ntext dataset. To avoid approximation error, we propose to use different\nknowledge distillation objectives. In addition, the use of a large-scale\nvideo-text dataset helps learn diverse and richer vocabularies. In our\nexperiments, VidLanKD achieves consistent improvements over text-only language\nmodels and vokenization models, on several downstream language understanding\ntasks including GLUE, SQuAD, and SWAG. We also demonstrate the improved world\nknowledge, physical reasoning, and temporal reasoning capabilities of our model\nby evaluating on the GLUE-diagnostics, PIQA, and TRACIE datasets. Lastly, we\npresent comprehensive ablation studies as well as visualizations of the learned\ntext-to-video grounding results of our teacher and student language models. Our\ncode and models are available at: https://github.com/zinengtang/VidLanKD",
          "link": "http://arxiv.org/abs/2107.02681",
          "publishedOn": "2021-07-07T01:57:11.257Z",
          "wordCount": 674,
          "title": "VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer. (arXiv:2107.02681v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02586",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Remerscheid_N/0/1/0/all/0/1\">Nicolas Remerscheid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus Makowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1\">Rickmer Braren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "Collaborative machine learning techniques such as federated learning (FL)\nenable the training of models on effectively larger datasets without data\ntransfer. Recent initiatives have demonstrated that segmentation models trained\nwith FL can achieve performance similar to locally trained models. However, FL\nis not a fully privacy-preserving technique and privacy-centred attacks can\ndisclose confidential patient data. Thus, supplementing FL with\nprivacy-enhancing technologies (PTs) such as differential privacy (DP) is a\nrequirement for clinical applications in a multi-institutional setting. The\napplication of PTs to FL in medical imaging and the trade-offs between privacy\nguarantees and model utility, the ramifications on training performance and the\nsusceptibility of the final models to attacks have not yet been conclusively\ninvestigated. Here we demonstrate the first application of differentially\nprivate gradient descent-based FL on the task of semantic segmentation in\ncomputed tomography. We find that high segmentation performance is possible\nunder strong privacy guarantees with an acceptable training time penalty. We\nfurthermore demonstrate the first successful gradient-based model inversion\nattack on a semantic segmentation model and show that the application of DP\nprevents it from divulging sensitive image features.",
          "link": "http://arxiv.org/abs/2107.02586",
          "publishedOn": "2021-07-07T01:57:11.240Z",
          "wordCount": 655,
          "title": "Differentially private federated deep learning for multi-site medical image segmentation. (arXiv:2107.02586v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jeremy Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Benjamin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Day_T/0/1/0/all/0/1\">Thomas Day</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1\">John Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>",
          "description": "Supervised learning of every possible pathology is unrealistic for many\nprimary care applications like health screening. Image anomaly detection\nmethods that learn normal appearance from only healthy data have shown\npromising results recently. We propose an alternative to image\nreconstruction-based and image embedding-based methods and propose a new\nself-supervised method to tackle pathological anomaly detection. Our approach\noriginates in the foreign patch interpolation (FPI) strategy that has shown\nsuperior performance on brain MRI and abdominal CT data. We propose to use a\nbetter patch interpolation strategy, Poisson image interpolation (PII), which\nmakes our method suitable for applications in challenging data regimes. PII\noutperforms state-of-the-art methods by a good margin when tested on surrogate\ntasks like identifying common lung anomalies in chest X-rays or hypo-plastic\nleft heart syndrome in prenatal, fetal cardiac ultrasound images. Code\navailable at https://github.com/jemtan/PII.",
          "link": "http://arxiv.org/abs/2107.02622",
          "publishedOn": "2021-07-07T01:57:11.233Z",
          "wordCount": 578,
          "title": "Detecting Outliers with Poisson Image Interpolation. (arXiv:2107.02622v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patrikar_D/0/1/0/all/0/1\">Devashree R. Patrikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parate_M/0/1/0/all/0/1\">Mayur Rajram Parate</a>",
          "description": "The current concept of Smart Cities influences urban planners and researchers\nto provide modern, secured and sustainable infrastructure and give a decent\nquality of life to its residents. To fulfill this need video surveillance\ncameras have been deployed to enhance the safety and well-being of the\ncitizens. Despite technical developments in modern science, abnormal event\ndetection in surveillance video systems is challenging and requires exhaustive\nhuman efforts. In this paper, we surveyed various methodologies developed to\ndetect anomalies in intelligent video surveillance. Firstly, we revisit the\nsurveys on anomaly detection in the last decade. We then present a systematic\ncategorization of methodologies developed for ease of understanding.\nConsidering the notion of anomaly depends on context, we identify different\nobjects-of-interest and publicly available datasets in anomaly detection. Since\nanomaly detection is considered a time-critical application of computer vision,\nour emphasis is on anomaly detection using edge devices and approaches\nexplicitly designed for them. Further, we discuss the challenges and\nopportunities involved in anomaly detection at the edge.",
          "link": "http://arxiv.org/abs/2107.02778",
          "publishedOn": "2021-07-07T01:57:11.226Z",
          "wordCount": 610,
          "title": "Anomaly Detection using Edge Computing in Video Surveillance System: Review. (arXiv:2107.02778v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blattmann_A/0/1/0/all/0/1\">Andreas Blattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1\">Timo Milbich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorkenwald_M/0/1/0/all/0/1\">Michael Dorkenwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "How would a static scene react to a local poke? What are the effects on other\nparts of an object if you could locally push it? There will be distinctive\nmovement, despite evident variations caused by the stochastic nature of our\nworld. These outcomes are governed by the characteristic kinematics of objects\nthat dictate their overall motion caused by a local interaction. Conversely,\nthe movement of an object provides crucial information about its underlying\ndistinctive kinematics and the interdependencies between its parts. This\ntwo-way relation motivates learning a bijective mapping between object\nkinematics and plausible future image sequences. Therefore, we propose iPOKE -\ninvertible Prediction of Object Kinematics - that, conditioned on an initial\nframe and a local poke, allows to sample object kinematics and establishes a\none-to-one correspondence to the corresponding plausible videos, thereby\nproviding a controlled stochastic video synthesis. In contrast to previous\nworks, we do not generate arbitrary realistic videos, but provide efficient\ncontrol of movements, while still capturing the stochastic nature of our\nenvironment and the diversity of plausible outcomes it entails. Moreover, our\napproach can transfer kinematics onto novel object instances and is not\nconfined to particular object classes. Project page is available at\nhttps://bit.ly/3dJN4Lf",
          "link": "http://arxiv.org/abs/2107.02790",
          "publishedOn": "2021-07-07T01:57:11.221Z",
          "wordCount": 651,
          "title": "iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis. (arXiv:2107.02790v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1\">Takehiko Ohkawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yagi_T/0/1/0/all/0/1\">Takuma Yagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1\">Atsushi Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1\">Yoshitaka Ushiku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>",
          "description": "Hand segmentation is a crucial task in first-person vision. Since\nfirst-person images exhibit strong bias in appearance among different\nenvironments, adapting a pre-trained segmentation model to a new domain is\nrequired in hand segmentation. Here, we focus on appearance gaps for hand\nregions and backgrounds separately. We propose (i) foreground-aware image\nstylization and (ii) consensus pseudo-labeling for domain adaptation of hand\nsegmentation. We stylize source images independently for the foreground and\nbackground using target images as style. To resolve the domain shift that the\nstylization has not addressed, we apply careful pseudo-labeling by taking a\nconsensus between the models trained on the source and stylized source images.\nWe validated our method on domain adaptation of hand segmentation from real and\nsimulation images. Our method achieved state-of-the-art performance in both\nsettings. We also demonstrated promising results in challenging multi-target\ndomain adaptation and domain generalization settings. Code is available at\nhttps://github.com/ut-vision/FgSty-CPL.",
          "link": "http://arxiv.org/abs/2107.02718",
          "publishedOn": "2021-07-07T01:57:11.212Z",
          "wordCount": 604,
          "title": "Foreground-Aware Stylization and Consensus Pseudo-Labeling for Domain Adaptation of First-Person Hand Segmentation. (arXiv:2107.02718v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_H/0/1/0/all/0/1\">Hasan Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morshed_M/0/1/0/all/0/1\">Mashrur Mahmud Morshed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Kamrul Hasan</a>",
          "description": "Any spatio-temporal movement or reorientation of the hand, done with the\nintention of conveying a specific meaning, can be considered as a hand gesture.\nInputs to hand gesture recognition systems can be in several forms, such as\ndepth images, monocular RGB, or skeleton joint points. We observe that raw\ndepth images possess low contrasts in the hand regions of interest (ROI). They\ndo not highlight important details to learn, such as finger bending information\n(whether a finger is overlapping the palm, or another finger). Recently, in\ndeep-learning--based dynamic hand gesture recognition, researchers are tying to\nfuse different input modalities (e.g. RGB or depth images and hand skeleton\njoint points) to improve the recognition accuracy. In this paper, we focus on\ndynamic hand gesture (DHG) recognition using depth quantized image features and\nhand skeleton joint points. In particular, we explore the effect of using\ndepth-quantized features in Convolutional Neural Network (CNN) and Recurrent\nNeural Network (RNN) based multi-modal fusion networks. We find that our method\nimproves existing results on the SHREC-DHG-14 dataset. Furthermore, using our\nmethod, we show that it is possible to reduce the resolution of the input\nimages by more than four times and still obtain comparable or better accuracy\nto that of the resolutions used in previous methods.",
          "link": "http://arxiv.org/abs/2107.02543",
          "publishedOn": "2021-07-07T01:57:11.205Z",
          "wordCount": 655,
          "title": "A deep-learning--based multimodal depth-aware dynamic hand gesture recognition system. (arXiv:2107.02543v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kangle Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andrew Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>",
          "description": "One common failure mode of Neural Radiance Field (NeRF) models is fitting\nincorrect geometries when given an insufficient number of input views. We\npropose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning\nneural radiance fields that takes advantage of readily-available depth\nsupervision. Our key insight is that sparse depth supervision can be used to\nregularize the learned geometry, a crucial component for effectively rendering\nnovel views using NeRF. We exploit the fact that current NeRF pipelines require\nimages with known camera poses that are typically estimated by running\nstructure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that\ncan be used as ``free\" depth supervision during training: we simply add a loss\nto ensure that depth rendered along rays that intersect these 3D points is\nclose to the observed depth. We find that DS-NeRF can render more accurate\nimages given fewer training views while training 2-6x faster. With only two\ntraining views on real-world images, DS-NeRF significantly outperforms NeRF as\nwell as other sparse-view variants. We show that our loss is compatible with\nthese NeRF models, demonstrating that depth is a cheap and easily digestible\nsupervisory signal. Finally, we show that DS-NeRF supports other types of depth\nsupervision such as scanned depth sensors and RGBD reconstruction outputs.",
          "link": "http://arxiv.org/abs/2107.02791",
          "publishedOn": "2021-07-07T01:57:11.188Z",
          "wordCount": 662,
          "title": "Depth-supervised NeRF: Fewer Views and Faster Training for Free. (arXiv:2107.02791v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Minjie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Heyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Pengpeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_E/0/1/0/all/0/1\">Erkang Cheng</a>",
          "description": "Robust and accurate localization is an essential component for robotic\nnavigation and autonomous driving. The use of cameras for localization with\nhigh definition map (HD Map) provides an affordable localization sensor set.\nExisting methods suffer from pose estimation failure due to error prone data\nassociation or initialization with accurate initial pose requirement. In this\npaper, we propose a cost-effective vehicle localization system with HD map for\nautonomous driving that uses cameras as primary sensors. To this end, we\nformulate vision-based localization as a data association problem that maps\nvisual semantics to landmarks in HD map. Specifically, system initialization is\nfinished in a coarse to fine manner by combining coarse GPS (Global Positioning\nSystem) measurement and fine pose searching. In tracking stage, vehicle pose is\nrefined by implicitly aligning the semantic segmentation result between image\nand landmarks in HD maps with photometric consistency. Finally, vehicle pose is\ncomputed by pose graph optimization in a sliding window fashion. We evaluate\nour method on two datasets and demonstrate that the proposed approach yields\npromising localization results in different driving scenarios. Additionally,\nour approach is suitable for both monocular camera and multi-cameras that\nprovides flexibility and improves robustness for the localization system.",
          "link": "http://arxiv.org/abs/2107.02557",
          "publishedOn": "2021-07-07T01:57:11.181Z",
          "wordCount": 655,
          "title": "Coarse-to-fine Semantic Localization with HD Map for Autonomous Driving in Structural Scenes. (arXiv:2107.02557v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02660",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuaizheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengxing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yue Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Min Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Junzhi Yu</a>",
          "description": "Robust vision restoration for an underwater image remains a challenging\nproblem. For the lack of aligned underwater-terrestrial image pairs, the\nunsupervised method is more suited to this task. However, the pure data-driven\nunsupervised method usually has difficulty in achieving realistic color\ncorrection for lack of optical constraint. In this paper, we propose a data-\nand physics-driven unsupervised architecture that learns underwater vision\nrestoration from unpaired underwater-terrestrial images. For sufficient domain\ntransformation and detail preservation, the underwater degeneration needs to be\nexplicitly constructed based on the optically unambiguous physics law. Thus, we\nemploy the Jaffe-McGlamery degradation theory to design the generation models,\nand use neural networks to describe the process of underwater degradation.\nFurthermore, to overcome the problem of invalid gradient when optimizing the\nhybrid physical-neural model, we fully investigate the intrinsic correlation\nbetween the scene depth and the degradation factors for the backscattering\nestimation, to improve the restoration performance through physical\nconstraints. Our experimental results show that the proposed method is able to\nperform high-quality restoration for unconstrained underwater images without\nany supervision. On multiple benchmarks, we outperform several state-of-the-art\nsupervised and unsupervised approaches. We also demonstrate that our methods\nyield encouraging results on real-world applications.",
          "link": "http://arxiv.org/abs/2107.02660",
          "publishedOn": "2021-07-07T01:57:11.175Z",
          "wordCount": 649,
          "title": "HybrUR: A Hybrid Physical-Neural Solution for Unsupervised Underwater Image Restoration. (arXiv:2107.02660v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbera_G/0/1/0/all/0/1\">Giammarco La Barbera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussaid_H/0/1/0/all/0/1\">Haithem Boussaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belucci_B/0/1/0/all/0/1\">Bruno Belucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delmonte_A/0/1/0/all/0/1\">Alessandro Delmonte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulin_J/0/1/0/all/0/1\">Jeanne Goulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarnacki_S/0/1/0/all/0/1\">Sabine Sarnacki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouet_L/0/1/0/all/0/1\">Laurence Rouet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloch_I/0/1/0/all/0/1\">Isabelle Bloch</a>",
          "description": "Due to a high heterogeneity in pose and size and to a limited number of\navailable data, segmentation of pediatric images is challenging for deep\nlearning methods. In this work, we propose a new CNN architecture that is pose\nand scale invariant thanks to the use of Spatial Transformer Network (STN). Our\narchitecture is composed of three sequential modules that are estimated\ntogether during training: (i) a regression module to estimate a similarity\nmatrix to normalize the input image to a reference one; (ii) a differentiable\nmodule to find the region of interest to segment; (iii) a segmentation module,\nbased on the popular UNet architecture, to delineate the object. Unlike the\noriginal UNet, which strives to learn a complex mapping, including pose and\nscale variations, from a finite training dataset, our segmentation module\nlearns a simpler mapping focusing on images with normalized pose and size.\nFurthermore, the use of an automatic bounding box detection through STN allows\nsaving time and especially memory, while keeping similar performance. We test\nthe proposed method in kidney and renal tumor segmentation on abdominal\npediatric CT scanners. Results indicate that the estimated STN homogenization\nof size and pose accelerates the segmentation (25h), compared to standard\ndata-augmentation (33h), while obtaining a similar quality for the kidney\n(88.01\\% of Dice score) and improving the renal tumor delineation (from 85.52\\%\nto 87.12\\%).",
          "link": "http://arxiv.org/abs/2107.02655",
          "publishedOn": "2021-07-07T01:57:11.168Z",
          "wordCount": 712,
          "title": "Automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric segmentation. (arXiv:2107.02655v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Hyperspectral pansharpening aims to synthesize a low-resolution hyperspectral\nimage (LR-HSI) with a registered panchromatic image (PAN) to generate an\nenhanced HSI with high spectral and spatial resolution. Recently proposed HS\npansharpening methods have obtained remarkable results using deep convolutional\nnetworks (ConvNets), which typically consist of three steps: (1) up-sampling\nthe LR-HSI, (2) predicting the residual image via a ConvNet, and (3) obtaining\nthe final fused HSI by adding the outputs from first and second steps. Recent\nmethods have leveraged Deep Image Prior (DIP) to up-sample the LR-HSI due to\nits excellent ability to preserve both spatial and spectral information,\nwithout learning from large data sets. However, we observed that the quality of\nup-sampled HSIs can be further improved by introducing an additional\nspatial-domain constraint to the conventional spectral-domain energy function.\nWe define our spatial-domain constraint as the $L_1$ distance between the\npredicted PAN image and the actual PAN image. To estimate the PAN image of the\nup-sampled HSI, we also propose a learnable spectral response function (SRF).\nMoreover, we noticed that the residual image between the up-sampled HSI and the\nreference HSI mainly consists of edge information and very fine structures. In\norder to accurately estimate fine information, we propose a novel over-complete\nnetwork, called HyperKite, which focuses on learning high-level features by\nconstraining the receptive from increasing in the deep layers. We perform\nexperiments on three HSI datasets to demonstrate the superiority of our\nDIP-HyperKite over the state-of-the-art pansharpening methods. The deployment\ncodes, pre-trained models, and final fusion outputs of our DIP-HyperKite and\nthe methods used for the comparisons will be publicly made available at\nhttps://github.com/wgcban/DIP-HyperKite.git.",
          "link": "http://arxiv.org/abs/2107.02630",
          "publishedOn": "2021-07-07T01:57:11.151Z",
          "wordCount": 726,
          "title": "Hyperspectral Pansharpening Based on Improved Deep Image Prior and Residual Reconstruction. (arXiv:2107.02630v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>",
          "description": "Homography estimation is an important task in computer vision, such as image\nstitching, video stabilization, and camera calibration. Traditional homography\nestimation methods heavily depend on the quantity and distribution of feature\npoints, leading to poor robustness in textureless scenes. The learning\nsolutions, on the contrary, try to learn robust deep features but demonstrate\nunsatisfying performance in the scenes of low overlap rates. In this paper, we\naddress the two problems simultaneously, by designing a contextual correlation\nlayer, which can capture the long-range correlation on feature maps and\nflexibly be bridged in a learning framework. In addition, considering that a\nsingle homography can not represent the complex spatial transformation in\ndepth-varying images with parallax, we propose to predict multi-grid homography\nfrom global to local. Moreover, we equip our network with depth perception\ncapability, by introducing a novel depth-aware shape-preserved loss. Extensive\nexperiments demonstrate the superiority of our method over other\nstate-of-the-art solutions in the synthetic benchmark dataset and real-world\ndataset. The codes and models will be available at\nhttps://github.com/nie-lang/Multi-Grid-Deep-Homogarphy.",
          "link": "http://arxiv.org/abs/2107.02524",
          "publishedOn": "2021-07-07T01:57:11.145Z",
          "wordCount": 612,
          "title": "Depth-Aware Multi-Grid Deep Homography Estimation with Contextual Correlation. (arXiv:2107.02524v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_Sanchez_A/0/1/0/all/0/1\">Amelia Jim&#xe9;nez-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tardy_M/0/1/0/all/0/1\">Mickael Tardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballester_M/0/1/0/all/0/1\">Miguel A. Gonz&#xe1;lez Ballester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mateus_D/0/1/0/all/0/1\">Diana Mateus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piella_G/0/1/0/all/0/1\">Gemma Piella</a>",
          "description": "For early breast cancer detection, regular screening with mammography imaging\nis recommended. Routinary examinations result in datasets with a predominant\namount of negative samples. A potential solution to such class-imbalance is\njoining forces across multiple institutions. Developing a collaborative\ncomputer-aided diagnosis system is challenging in different ways. Patient\nprivacy and regulations need to be carefully respected. Data across\ninstitutions may be acquired from different devices or imaging protocols,\nleading to heterogeneous non-IID data. Also, for learning-based methods, new\noptimization strategies working on distributed data are required. Recently,\nfederated learning has emerged as an effective tool for collaborative learning.\nIn this setting, local models perform computation on their private data to\nupdate the global model. The order and the frequency of local updates influence\nthe final global model. Hence, the order in which samples are locally presented\nto the optimizers plays an important role. In this work, we define a\nmemory-aware curriculum learning method for the federated setting. Our\ncurriculum controls the order of the training samples paying special attention\nto those that are forgotten after the deployment of the global model. Our\napproach is combined with unsupervised domain adaptation to deal with domain\nshift while preserving data privacy. We evaluate our method with three clinical\ndatasets from different vendors. Our results verify the effectiveness of\nfederated adversarial learning for the multi-site breast cancer classification.\nMoreover, we show that our proposed memory-aware curriculum method is\nbeneficial to further improve classification performance. Our code is publicly\navailable at: https://github.com/ameliajimenez/curriculum-federated-learning.",
          "link": "http://arxiv.org/abs/2107.02504",
          "publishedOn": "2021-07-07T01:57:11.132Z",
          "wordCount": 692,
          "title": "Memory-aware curriculum federated learning for breast cancer classification. (arXiv:2107.02504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hilt_P/0/1/0/all/0/1\">Paul Hilt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaziakhmedov_E/0/1/0/all/0/1\">Edgar Kaziakhmedov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhide_S/0/1/0/all/0/1\">Sourabh Bhide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leptin_M/0/1/0/all/0/1\">Maria Leptin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pape_C/0/1/0/all/0/1\">Constantin Pape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreshuk_A/0/1/0/all/0/1\">Anna Kreshuk</a>",
          "description": "Instance segmentation is an important computer vision problem which remains\nchallenging despite impressive recent advances due to deep learning-based\nmethods. Given sufficient training data, fully supervised methods can yield\nexcellent performance, but annotation of ground-truth data remains a major\nbottleneck, especially for biomedical applications where it has to be performed\nby domain experts. The amount of labels required can be drastically reduced by\nusing rules derived from prior knowledge to guide the segmentation. However,\nthese rules are in general not differentiable and thus cannot be used with\nexisting methods. Here, we relax this requirement by using stateless actor\ncritic reinforcement learning, which enables non-differentiable rewards. We\nformulate the instance segmentation problem as graph partitioning and the actor\ncritic predicts the edge weights driven by the rewards, which are based on the\nconformity of segmented instances to high-level priors on object shape,\nposition or size. The experiments on toy and real datasets demonstrate that we\ncan achieve excellent performance without any direct supervision based only on\na rich set of priors.",
          "link": "http://arxiv.org/abs/2107.02600",
          "publishedOn": "2021-07-07T01:57:11.123Z",
          "wordCount": 609,
          "title": "Stateless actor-critic for instance segmentation with high-level priors. (arXiv:2107.02600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianqiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1\">Sameera Ramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>",
          "description": "It is well noted that coordinate based MLPs benefit greatly -- in terms of\npreserving high-frequency information -- through the encoding of coordinate\npositions as an array of Fourier features. Hitherto, the rationale for the\neffectiveness of these positional encodings has been solely studied through a\nFourier lens. In this paper, we strive to broaden this understanding by showing\nthat alternative non-Fourier embedding functions can indeed be used for\npositional encoding. Moreover, we show that their performance is entirely\ndetermined by a trade-off between the stable rank of the embedded matrix and\nthe distance preservation between embedded coordinates. We further establish\nthat the now ubiquitous Fourier feature mapping of position is a special case\nthat fulfills these conditions. Consequently, we present a more general theory\nto analyze positional encoding in terms of shifted basis functions. To this\nend, we develop the necessary theoretical formulae and empirically verify that\nour theoretical claims hold in practice. Codes available at\nhttps://github.com/osiriszjq/Rethinking-positional-encoding.",
          "link": "http://arxiv.org/abs/2107.02561",
          "publishedOn": "2021-07-07T01:57:11.116Z",
          "wordCount": 586,
          "title": "Rethinking Positional Encoding. (arXiv:2107.02561v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02555",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Freirich_D/0/1/0/all/0/1\">Dror Freirich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Michaeli_T/0/1/0/all/0/1\">Tomer Michaeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meir_R/0/1/0/all/0/1\">Ron Meir</a>",
          "description": "The lower the distortion of an estimator, the more the distribution of its\noutputs generally deviates from the distribution of the signals it attempts to\nestimate. This phenomenon, known as the perception-distortion tradeoff, has\ncaptured significant attention in image restoration, where it implies that\nfidelity to ground truth images comes at the expense of perceptual quality\n(deviation from statistics of natural images). However, despite the increasing\npopularity of performing comparisons on the perception-distortion plane, there\nremains an important open question: what is the minimal distortion that can be\nachieved under a given perception constraint? In this paper, we derive a closed\nform expression for this distortion-perception (DP) function for the mean\nsquared-error (MSE) distortion and the Wasserstein-2 perception index. We prove\nthat the DP function is always quadratic, regardless of the underlying\ndistribution. This stems from the fact that estimators on the DP curve form a\ngeodesic in Wasserstein space. In the Gaussian setting, we further provide a\nclosed form expression for such estimators. For general distributions, we show\nhow these estimators can be constructed from the estimators at the two extremes\nof the tradeoff: The global MSE minimizer, and a minimizer of the MSE under a\nperfect perceptual quality constraint. The latter can be obtained as a\nstochastic transformation of the former.",
          "link": "http://arxiv.org/abs/2107.02555",
          "publishedOn": "2021-07-07T01:57:11.084Z",
          "wordCount": 658,
          "title": "A Theory of the Distortion-Perception Tradeoff in Wasserstein Space. (arXiv:2107.02555v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02583",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lifa Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongrui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Changwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Fernandez_F/0/1/0/all/0/1\">Francisco G&#xf3;mez-Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Ninghua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Ziyong Feng</a>",
          "description": "3D point cloud registration is a fundamental task in robotics and computer\nvision. Recently, many learning-based point cloud registration methods based on\ncorrespondences have emerged. However, these methods heavily rely on such\ncorrespondences and meet great challenges with partial overlap. In this paper,\nwe propose ROPNet, a new deep learning model using Representative Overlapping\nPoints with discriminative features for registration that transforms\npartial-to-partial registration into partial-to-complete registration.\nSpecifically, we propose a context-guided module which uses an encoder to\nextract global features for predicting point overlap score. To better find\nrepresentative overlapping points, we use the extracted global features for\ncoarse alignment. Then, we introduce a Transformer to enrich point features and\nremove non-representative points based on point overlap score and feature\nmatching. A similarity matrix is built in a partial-to-complete mode, and\nfinally, weighted SVD is adopted to estimate a transformation matrix. Extensive\nexperiments over ModelNet40 using noisy and partially overlapping point clouds\nshow that the proposed method outperforms traditional and learning-based\nmethods, achieving state-of-the-art performance. The code is available at\nhttps://github.com/zhulf0804/ROPNet.",
          "link": "http://arxiv.org/abs/2107.02583",
          "publishedOn": "2021-07-07T01:57:11.068Z",
          "wordCount": 613,
          "title": "Point Cloud Registration using Representative Overlapping Points. (arXiv:2107.02583v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaomeng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhenxun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianmin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>",
          "description": "As cameras are increasingly deployed in new application domains such as\nautonomous driving, performing 3D object detection on monocular images becomes\nan important task for visual scene understanding. Recent advances on monocular\n3D object detection mainly rely on the ``pseudo-LiDAR'' generation, which\nperforms monocular depth estimation and lifts the 2D pixels to pseudo 3D\npoints. However, depth estimation from monocular images, due to its poor\naccuracy, leads to inevitable position shift of pseudo-LiDAR points within the\nobject. Therefore, the predicted bounding boxes may suffer from inaccurate\nlocation and deformed shape. In this paper, we present a novel neighbor-voting\nmethod that incorporates neighbor predictions to ameliorate object detection\nfrom severely deformed pseudo-LiDAR point clouds. Specifically, each feature\npoint around the object forms their own predictions, and then the ``consensus''\nis achieved through voting. In this way, we can effectively combine the\nneighbors' predictions with local prediction and achieve more accurate 3D\ndetection. To further enlarge the difference between the foreground region of\ninterest (ROI) pseudo-LiDAR points and the background points, we also encode\nthe ROI prediction scores of 2D foreground pixels into the corresponding\npseudo-LiDAR points. We conduct extensive experiments on the KITTI benchmark to\nvalidate the merits of our proposed method. Our results on the bird's eye view\ndetection outperform the state-of-the-art performance by a large margin,\nespecially for the ``hard'' level detection.",
          "link": "http://arxiv.org/abs/2107.02493",
          "publishedOn": "2021-07-07T01:57:11.060Z",
          "wordCount": 675,
          "title": "Neighbor-Vote: Improving Monocular 3D Object Detection through Neighbor Distance Voting. (arXiv:2107.02493v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>",
          "description": "The ability to recognize the position and order of the floor-level lines that\ndivide adjacent building floors can benefit many applications, for example,\nurban augmented reality (AR). This work tackles the problem of locating\nfloor-level lines in street-view images, using a supervised deep learning\napproach. Unfortunately, very little data is available for training such a\nnetwork $-$ current street-view datasets contain either semantic annotations\nthat lack geometric attributes, or rectified facades without perspective\npriors. To address this issue, we first compile a new dataset and develop a new\ndata augmentation scheme to synthesize training samples by harassing (i) the\nrich semantics of existing rectified facades and (ii) perspective priors of\nbuildings in diverse street views. Next, we design FloorLevel-Net, a multi-task\nlearning network that associates explicit features of building facades and\nimplicit floor-level lines, along with a height-attention mechanism to help\nenforce a vertical ordering of floor-level lines. The generated segmentations\nare then passed to a second-stage geometry post-processing to exploit\nself-constrained geometric priors for plausible and consistent reconstruction\nof floor-level lines. Quantitative and qualitative evaluations conducted on\nassorted facades in existing datasets and street views from Google demonstrate\nthe effectiveness of our approach. Also, we present context-aware image overlay\nresults and show the potentials of our approach in enriching AR-related\napplications.",
          "link": "http://arxiv.org/abs/2107.02462",
          "publishedOn": "2021-07-07T01:57:11.019Z",
          "wordCount": 645,
          "title": "FloorLevel-Net: Recognizing Floor-Level Lines with Height-Attention-Guided Multi-task Learning. (arXiv:2107.02462v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1\">Takami Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Alfred Chen</a>",
          "description": "After the 2017 TuSimple Lane Detection Challenge, its evaluation based on\naccuracy and F1 score has become the de facto standard to measure the\nperformance of lane detection methods. In this work, we conduct the first\nlarge-scale empirical study to evaluate the robustness of state-of-the-art lane\ndetection methods under physical-world adversarial attacks in autonomous\ndriving. We evaluate 4 major types of lane detection approaches with the\nconventional evaluation and end-to-end evaluation in autonomous driving\nscenarios and then discuss the security proprieties of each lane detection\nmodel. We demonstrate that the conventional evaluation fails to reflect the\nrobustness in end-to-end autonomous driving scenarios. Our results show that\nthe most robust model on the conventional metrics is the least robust in the\nend-to-end evaluation. Although the competition dataset and its metrics have\nplayed a substantial role in developing performant lane detection methods along\nwith the rapid development of deep neural networks, the conventional evaluation\nis becoming obsolete and the gap between the metrics and practicality is\ncritical. We hope that our study will help the community make further progress\nin building a more comprehensive framework to evaluate lane detection models.",
          "link": "http://arxiv.org/abs/2107.02488",
          "publishedOn": "2021-07-07T01:57:11.005Z",
          "wordCount": 636,
          "title": "On Robustness of Lane Detection Models to Physical-World Adversarial Attacks in Autonomous Driving. (arXiv:2107.02488v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1\">Lap-pui Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex C. Kot</a>",
          "description": "Though convolutional neural networks are widely used in different tasks, lack\nof generalization capability in the absence of sufficient and representative\ndata is one of the challenges that hinder their practical application. In this\npaper, we propose a simple, effective, and plug-and-play training strategy\nnamed Knowledge Distillation for Domain Generalization (KDDG) which is built\nupon a knowledge distillation framework with the gradient filter as a novel\nregularization term. We find that both the ``richer dark knowledge\" from the\nteacher network, as well as the gradient filter we proposed, can reduce the\ndifficulty of learning the mapping which further improves the generalization\nability of the model. We also conduct experiments extensively to show that our\nframework can significantly improve the generalization capability of deep\nneural networks in different tasks including image classification,\nsegmentation, reinforcement learning by comparing our method with existing\nstate-of-the-art domain generalization techniques. Last but not the least, we\npropose to adopt two metrics to analyze our proposed method in order to better\nunderstand how our proposed method benefits the generalization capability of\ndeep neural networks.",
          "link": "http://arxiv.org/abs/2107.02629",
          "publishedOn": "2021-07-07T01:57:10.999Z",
          "wordCount": 628,
          "title": "Embracing the Dark Knowledge: Domain Generalization Using Regularized Knowledge Distillation. (arXiv:2107.02629v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huafeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hei_G/0/1/0/all/0/1\">Guangyue Hei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Rong Du</a>",
          "description": "In recent years, benefiting from the expressivepower of Graph Convolutional\nNetworks (GCNs),significant breakthroughs have been made in faceclustering.\nHowever, rare attention has been paidto GCN-based clustering on imbalanced\ndata. Al-though imbalance problem has been extensivelystudied, the impact of\nimbalanced data on GCN-based linkage prediction task is quite different,which\nwould cause problems in two aspects: im-balanced linkage labels and biased\ngraph represen-tations. The problem of imbalanced linkage labelsis similar to\nthat in image classification task, but thelatter is a particular problem in\nGCN-based clus-tering via linkage prediction. Significantly biasedgraph\nrepresentations in training can cause catas-trophic overfitting of a GCN model.\nTo tacklethese problems, we evaluate the feasibility of thoseexisting methods\nfor imbalanced image classifica-tion problem on graphs with extensive\nexperiments,and present a new method to alleviate the imbal-anced labels and\nalso augment graph representa-tions using a Reverse-Imbalance Weighted\nSam-pling (RIWS) strategy, followed with insightfulanalyses and discussions. A\nseries of imbalancedbenchmark datasets synthesized from MS-Celeb-1M and\nDeepFashion will be openly available.",
          "link": "http://arxiv.org/abs/2107.02477",
          "publishedOn": "2021-07-07T01:57:10.993Z",
          "wordCount": 607,
          "title": "GCN-Based Linkage Prediction for Face Clusteringon Imbalanced Datasets: An Empirical Study. (arXiv:2107.02477v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Kai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yinru Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Minqiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bin Hu</a>",
          "description": "The main challenges of image-to-image (I2I) translation are to make the\ntranslated image realistic and retain as much information from the source\ndomain as possible. To address this issue, we propose a novel architecture,\ntermed as IEGAN, which removes the encoder of each network and introduces an\nencoder that is independent of other networks. Compared with previous models,\nit embodies three advantages of our model: Firstly, it is more directly and\ncomprehensively to grasp image information since the encoder no longer receives\nloss from generator and discriminator. Secondly, the independent encoder allows\neach network to focus more on its own goal which makes the translated image\nmore realistic. Thirdly, the reduction in the number of encoders performs more\nunified image representation. However, when the independent encoder applies two\ndown-sampling blocks, it's hard to extract semantic information. To tackle this\nproblem, we propose deep and shallow information space containing\ncharacteristic and semantic information, which can guide the model to translate\nhigh-quality images under the task with significant shape or texture change. We\ncompare IEGAN with other previous models, and conduct researches on semantic\ninformation consistency and component ablation at the same time. These\nexperiments show the superiority and effectiveness of our architecture. Our\ncode is published on: https://github.com/Elvinky/IEGAN.",
          "link": "http://arxiv.org/abs/2107.02494",
          "publishedOn": "2021-07-07T01:57:10.986Z",
          "wordCount": 651,
          "title": "Independent Encoder for Deep Hierarchical Unsupervised Image-to-Image Translation. (arXiv:2107.02494v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leinen_F/0/1/0/all/0/1\">Fabian Leinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cozzolino_V/0/1/0/all/0/1\">Vittorio Cozzolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1\">Torsten Sch&#xf6;n</a>",
          "description": "Human body volume estimation from a single RGB image is a challenging problem\ndespite minimal attention from the research community. However VolNet, an\narchitecture leveraging 2D and 3D pose estimation, body part segmentation and\nvolume regression extracted from a single 2D RGB image combined with the\nsubject's body height can be used to estimate the total body volume. VolNet is\ndesigned to predict the 2D and 3D pose as well as the body part segmentation in\nintermediate tasks. We generated a synthetic, large-scale dataset of\nphoto-realistic images of human bodies with a wide range of body shapes and\nrealistic poses called SURREALvols. By using Volnet and combining multiple\nstacked hourglass networks together with ResNeXt, our model correctly predicted\nthe volume in ~82% of cases with a 10% tolerance threshold. This is a\nconsiderable improvement compared to state-of-the-art solutions such as BodyNet\nwith only a ~38% success rate.",
          "link": "http://arxiv.org/abs/2107.02259",
          "publishedOn": "2021-07-07T01:57:10.980Z",
          "wordCount": 596,
          "title": "VolNet: Estimating Human Body Part Volumes from a Single RGB Image. (arXiv:2107.02259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vysogorets_A/0/1/0/all/0/1\">Artem Vysogorets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kempe_J/0/1/0/all/0/1\">Julia Kempe</a>",
          "description": "Neural network pruning is a fruitful area of research with surging interest\nin high sparsity regimes. Benchmarking in this domain heavily relies on\nfaithful representation of the sparsity of subnetworks, which has been\ntraditionally computed as the fraction of removed connections (direct\nsparsity). This definition, however, fails to recognize unpruned parameters\nthat detached from input or output layers of underlying subnetworks,\npotentially underestimating actual effective sparsity: the fraction of\ninactivated connections. While this effect might be negligible for moderately\npruned networks (up to 10-100 compression rates), we find that it plays an\nincreasing role for thinner subnetworks, greatly distorting comparison between\ndifferent pruning algorithms. For example, we show that effective compression\nof a randomly pruned LeNet-300-100 can be orders of magnitude larger than its\ndirect counterpart, while no discrepancy is ever observed when using SynFlow\nfor pruning [Tanaka et al., 2020]. In this work, we adopt the lens of effective\nsparsity to reevaluate several recent pruning algorithms on common benchmark\narchitectures (e.g., LeNet-300-100, VGG-19, ResNet-18) and discover that their\nabsolute and relative performance changes dramatically in this new and more\nappropriate framework. To aim for effective, rather than direct, sparsity, we\ndevelop a low-cost extension to most pruning algorithms. Further, equipped with\neffective sparsity as a reference frame, we partially reconfirm that random\npruning with appropriate sparsity allocation across layers performs as well or\nbetter than more sophisticated algorithms for pruning at initialization [Su et\nal., 2020]. In response to this observation, using a simple analogy of pressure\ndistribution in coupled cylinders from physics, we design novel layerwise\nsparsity quotas that outperform all existing baselines in the context of random\npruning.",
          "link": "http://arxiv.org/abs/2107.02306",
          "publishedOn": "2021-07-07T01:57:10.968Z",
          "wordCount": 710,
          "title": "Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity. (arXiv:2107.02306v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1\">Joseph Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_T/0/1/0/all/0/1\">Talfan Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>",
          "description": "In this article, we present a visual introduction to Gaussian Belief\nPropagation (GBP), an approximate probabilistic inference algorithm that\noperates by passing messages between the nodes of arbitrarily structured factor\ngraphs. A special case of loopy belief propagation, GBP updates rely only on\nlocal information and will converge independently of the message schedule. Our\nkey argument is that, given recent trends in computing hardware, GBP has the\nright computational properties to act as a scalable distributed probabilistic\ninference framework for future machine learning systems.",
          "link": "http://arxiv.org/abs/2107.02308",
          "publishedOn": "2021-07-07T01:57:10.961Z",
          "wordCount": 534,
          "title": "A visual introduction to Gaussian Belief Propagation. (arXiv:2107.02308v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>",
          "description": "Over the last few decades, artificial intelligence research has made\ntremendous strides, but it still heavily relies on fixed datasets in stationary\nenvironments. Continual learning is a growing field of research that examines\nhow AI systems can learn sequentially from a continuous stream of linked data\nin the same way that biological systems do. Simultaneously, fake media such as\ndeepfakes and synthetic face images have emerged as significant to current\nmultimedia technologies. Recently, numerous method has been proposed which can\ndetect deepfakes with high accuracy. However, they suffer significantly due to\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\nthis work, we apply continuous learning to neural networks' learning dynamics,\nemphasizing its potential to increase data efficiency significantly. We propose\nContinual Representation using Distillation (CoReD) method that employs the\nconcept of Continual Learning (CoL), Representation Learning (ReL), and\nKnowledge Distillation (KD). We design CoReD to perform sequential domain\nadaptation tasks on new deepfake and GAN-generated synthetic face datasets,\nwhile effectively minimizing the catastrophic forgetting in a teacher-student\nmodel setting. Our extensive experimental results demonstrate that our method\nis efficient at domain adaptation to detect low-quality deepfakes videos and\nGAN-generated images from several datasets, outperforming the-state-of-art\nbaseline methods.",
          "link": "http://arxiv.org/abs/2107.02408",
          "publishedOn": "2021-07-07T01:57:10.955Z",
          "wordCount": 675,
          "title": "CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_L/0/1/0/all/0/1\">Long Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>",
          "description": "Image editing techniques enable people to modify the content of an image\nwithout leaving visual traces and thus may cause serious security risks. Hence\nthe detection and localization of these forgeries become quite necessary and\nchallenging. Furthermore, unlike other tasks with extensive data, there is\nusually a lack of annotated forged images for training due to annotation\ndifficulties. In this paper, we propose a self-adversarial training strategy\nand a reliable coarse-to-fine network that utilizes a self-attention mechanism\nto localize forged regions in forgery images. The self-attention module is\nbased on a Channel-Wise High Pass Filter block (CW-HPF). CW-HPF leverages\ninter-channel relationships of features and extracts noise features by high\npass filters. Based on the CW-HPF, a self-attention mechanism, called forgery\nattention, is proposed to capture rich contextual dependencies of intrinsic\ninconsistency extracted from tampered regions. Specifically, we append two\ntypes of attention modules on top of CW-HPF respectively to model internal\ninterdependencies in spatial dimension and external dependencies among\nchannels. We exploit a coarse-to-fine network to enhance the noise\ninconsistency between original and tampered regions. More importantly, to\naddress the issue of insufficient training data, we design a self-adversarial\ntraining strategy that expands training data dynamically to achieve more robust\nperformance. Specifically, in each training iteration, we perform adversarial\nattacks against our network to generate adversarial examples and train our\nmodel on them. Extensive experimental results demonstrate that our proposed\nalgorithm steadily outperforms state-of-the-art methods by a clear margin in\ndifferent benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.02434",
          "publishedOn": "2021-07-07T01:57:10.931Z",
          "wordCount": 688,
          "title": "Self-Adversarial Training incorporating Forgery Attention for Image Forgery Localization. (arXiv:2107.02434v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyemin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daijin Kim</a>",
          "description": "We propose Uncertainty Augmented Context Attention network (UACANet) for\npolyp segmentation which consider a uncertain area of the saliency map. We\nconstruct a modified version of U-Net shape network with additional encoder and\ndecoder and compute a saliency map in each bottom-up stream prediction module\nand propagate to the next prediction module. In each prediction module,\npreviously predicted saliency map is utilized to compute foreground, background\nand uncertain area map and we aggregate the feature map with three area maps\nfor each representation. Then we compute the relation between each\nrepresentation and each pixel in the feature map. We conduct experiments on\nfive popular polyp segmentation benchmarks, Kvasir, CVC-ClinicDB, ETIS,\nCVC-ColonDB and CVC-300, and achieve state-of-the-art performance. Especially,\nwe achieve 76.6% mean Dice on ETIS dataset which is 13.8% improvement compared\nto the previous state-of-the-art method.",
          "link": "http://arxiv.org/abs/2107.02368",
          "publishedOn": "2021-07-07T01:57:10.922Z",
          "wordCount": 603,
          "title": "UACANet: Uncertainty Augmented Context Attention for Polyp Semgnetaion. (arXiv:2107.02368v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02338",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1\">Varun A. Kelkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Granstedt_J/0/1/0/all/0/1\">Jason Granstedt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>",
          "description": "Deep learning-based image super-resolution (DL-SR) has shown great promise in\nmedical imaging applications. To date, most of the proposed methods for DL-SR\nhave only been assessed by use of traditional measures of image quality (IQ)\nthat are commonly employed in the field of computer vision. However, the impact\nof these methods on objective measures of image quality that are relevant to\nmedical imaging tasks remains largely unexplored. In this study, we investigate\nthe impact of DL-SR methods on binary signal detection performance. Two popular\nDL-SR methods, the super-resolution convolutional neural network (SRCNN) and\nthe super-resolution generative adversarial network (SRGAN), were trained by\nuse of simulated medical image data. Binary signal-known-exactly with\nbackground-known-statistically (SKE/BKS) and signal-known-statistically with\nbackground-known-statistically (SKS/BKS) detection tasks were formulated.\nNumerical observers, which included a neural network-approximated ideal\nobserver and common linear numerical observers, were employed to assess the\nimpact of DL-SR on task performance. The impact of the complexity of the DL-SR\nnetwork architectures on task-performance was quantified. In addition, the\nutility of DL-SR for improving the task-performance of sub-optimal observers\nwas investigated. Our numerical experiments confirmed that, as expected, DL-SR\ncould improve traditional measures of IQ. However, for many of the study\ndesigns considered, the DL-SR methods provided little or no improvement in task\nperformance and could even degrade it. It was observed that DL-SR could improve\nthe task-performance of sub-optimal observers under certain conditions. The\npresented study highlights the urgent need for the objective assessment of\nDL-SR methods and suggests avenues for improving their efficacy in medical\nimaging applications.",
          "link": "http://arxiv.org/abs/2107.02338",
          "publishedOn": "2021-07-07T01:57:10.896Z",
          "wordCount": 712,
          "title": "Impact of deep learning-based image super-resolution on binary signal detection. (arXiv:2107.02338v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tissera_D/0/1/0/all/0/1\">Dumindu Tissera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vithanage_K/0/1/0/all/0/1\">Kasun Vithanage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijesinghe_R/0/1/0/all/0/1\">Rukshan Wijesinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xavier_A/0/1/0/all/0/1\">Alex Xavier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasena_S/0/1/0/all/0/1\">Sanath Jayasena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_S/0/1/0/all/0/1\">Subha Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>",
          "description": "Any clustering algorithm must synchronously learn to model the clusters and\nallocate data to those clusters in the absence of labels. Mixture model-based\nmethods model clusters with pre-defined statistical distributions and allocate\ndata to those clusters based on the cluster likelihoods. They iteratively\nrefine those distribution parameters and member assignments following the\nExpectation-Maximization (EM) algorithm. However, the cluster representability\nof such hand-designed distributions that employ a limited amount of parameters\nis not adequate for most real-world clustering tasks. In this paper, we realize\nmixture model-based clustering with a neural network where the final layer\nneurons, with the aid of an additional transformation, approximate cluster\ndistribution outputs. The network parameters pose as the parameters of those\ndistributions. The result is an elegant, much-generalized representation of\nclusters than a restricted mixture of hand-designed distributions. We train the\nnetwork end-to-end via batch-wise EM iterations where the forward pass acts as\nthe E-step and the backward pass acts as the M-step. In image clustering, the\nmixture-based EM objective can be used as the clustering objective along with\nexisting representation learning methods. In particular, we show that when\nmixture-EM optimization is fused with consistency optimization, it improves the\nsole consistency optimization performance in clustering. Our trained networks\noutperform single-stage deep clustering methods that still depend on k-means,\nwith unsupervised classification accuracy of 63.8% in STL10, 58% in CIFAR10,\n25.9% in CIFAR100, and 98.9% in MNIST.",
          "link": "http://arxiv.org/abs/2107.02453",
          "publishedOn": "2021-07-07T01:57:10.883Z",
          "wordCount": 688,
          "title": "Neural Mixture Models with Expectation-Maximization for End-to-end Deep Clustering. (arXiv:2107.02453v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Donghuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadeesan_J/0/1/0/all/0/1\">Jayender Jagadeesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William Wells III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frisken_S/0/1/0/all/0/1\">Sarah Frisken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1\">Raymond Kai-yu Tong</a>",
          "description": "In order to tackle the difficulty associated with the ill-posed nature of the\nimage registration problem, researchers use regularization to constrain the\nsolution space. For most learning-based registration approaches, the\nregularization usually has a fixed weight and only constrains the spatial\ntransformation. Such convention has two limitations: (1) The regularization\nstrength of a specific image pair should be associated with the content of the\nimages, thus the ``one value fits all'' scheme is not ideal; (2) Only spatially\nregularizing the transformation (but overlooking the temporal consistency of\ndifferent estimations) may not be the best strategy to cope with the\nill-posedness. In this study, we propose a mean-teacher based registration\nframework. This framework incorporates an additional \\textit{temporal\nregularization} term by encouraging the teacher model's temporal ensemble\nprediction to be consistent with that of the student model. At each training\nstep, it also automatically adjusts the weights of the \\textit{spatial\nregularization} and the \\textit{temporal regularization} by taking account of\nthe transformation uncertainty and appearance uncertainty derived from the\nperturbed teacher model. We perform experiments on multi- and uni-modal\nregistration tasks, and the results show that our strategy outperforms the\ntraditional and learning-based benchmark methods.",
          "link": "http://arxiv.org/abs/2107.02433",
          "publishedOn": "2021-07-07T01:57:10.863Z",
          "wordCount": 658,
          "title": "Double-Uncertainty Assisted Spatial and Temporal Regularization Weighting for Learning-based Registration. (arXiv:2107.02433v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02407",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1\">Marc Habermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>",
          "description": "We propose an efficient method for non-rigid surface tracking from monocular\nRGB videos. Given a video and a template mesh, our algorithm sequentially\nregisters the template non-rigidly to each frame. We formulate the per-frame\nregistration as an optimization problem that includes a novel texture term\nspecifically tailored towards tracking objects with uniform texture but\nfine-scale structure, such as the regular micro-structural patterns of fabric.\nOur texture term exploits the orientation information in the micro-structures\nof the objects, e.g., the yarn patterns of fabrics. This enables us to\naccurately track uniformly colored materials that have these high frequency\nmicro-structures, for which traditional photometric terms are usually less\neffective. The results demonstrate the effectiveness of our method on both\ngeneral textured non-rigid objects and monochromatic fabrics.",
          "link": "http://arxiv.org/abs/2107.02407",
          "publishedOn": "2021-07-07T01:57:10.845Z",
          "wordCount": 561,
          "title": "NRST: Non-rigid Surface Tracking from Monocular Video. (arXiv:2107.02407v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koga_Y/0/1/0/all/0/1\">Yohei Koga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyazaki_H/0/1/0/all/0/1\">Hiroyuki Miyazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibasaki_R/0/1/0/all/0/1\">Ryosuke Shibasaki</a>",
          "description": "While recent advancement of domain adaptation techniques is significant, most\nof methods only align a feature extractor and do not adapt a classifier to\ntarget domain, which would be a cause of performance degradation. We propose\nnovel domain adaptation technique for object detection that aligns prediction\noutput space. In addition to feature alignment, we aligned predictions of\nlocations and class confidences of our vehicle detector for satellite images by\nadversarial training. The proposed method significantly improved AP score by\nover 5%, which shows effectivity of our method for object detection tasks in\nsatellite images.",
          "link": "http://arxiv.org/abs/2107.02411",
          "publishedOn": "2021-07-07T01:57:10.837Z",
          "wordCount": 556,
          "title": "Adapting Vehicle Detector to Target Domain by Adversarial Prediction Alignment. (arXiv:2107.02411v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02299",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadid_Pecht_O/0/1/0/all/0/1\">Orly Yadid-Pecht</a>",
          "description": "Deep convolutional neural networks (DCNN) aided high dynamic range (HDR)\nimaging recently received a lot of attention. The quality of DCNN generated HDR\nimages have overperformed the traditional counterparts. However, DCNN is prone\nto be computationally intensive and power-hungry. To address the challenge, we\npropose LightFuse, a light-weight CNN-based algorithm for extreme dual-exposure\nimage fusion, which can be implemented on various embedded computing platforms\nwith limited power and hardware resources. Two sub-networks are utilized: a\nGlobalNet (G) and a DetailNet (D). The goal of G is to learn the global\nillumination information on the spatial dimension, whereas D aims to enhance\nlocal details on the channel dimension. Both G and D are based solely on\ndepthwise convolution (D Conv) and pointwise convolution (P Conv) to reduce\nrequired parameters and computations. Experimental results display that the\nproposed technique could generate HDR images with plausible details in\nextremely exposed regions. Our PSNR score exceeds the other state-of-the-art\napproaches by 1.2 to 1.6 times and achieves 1.4 to 20 times FLOP and parameter\nreduction compared with others.",
          "link": "http://arxiv.org/abs/2107.02299",
          "publishedOn": "2021-07-07T01:57:10.821Z",
          "wordCount": 611,
          "title": "LightFuse: Lightweight CNN based Dual-exposure Fusion. (arXiv:2107.02299v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalander_M/0/1/0/all/0/1\">Marcus Kalander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chanfei Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lujia Pan</a>",
          "description": "We consider the problem of training robust and accurate deep neural networks\n(DNNs) when subject to various proportions of noisy labels. Large-scale\ndatasets tend to contain mislabeled samples that can be memorized by DNNs,\nimpeding the performance. With appropriate handling, this degradation can be\nalleviated. There are two problems to consider: how to distinguish clean\nsamples and how to deal with noisy samples. In this paper, we present Ensemble\nNoise-robust K-fold Cross-Validation Selection (E-NKCVS) to effectively select\nclean samples from noisy data, solving the first problem. For the second\nproblem, we create a new pseudo label for any sample determined to have an\nuncertain or likely corrupt label. E-NKCVS obtains multiple predicted labels\nfor each sample and the entropy of these labels is used to tune the weight\ngiven to the pseudo label and the given label. Theoretical analysis and\nextensive verification of the algorithms in the noisy label setting are\nprovided. We evaluate our approach on various image and text classification\ntasks where the labels have been manually corrupted with different noise\nratios. Additionally, two large real-world noisy datasets are also used,\nClothing-1M and WebVision. E-NKCVS is empirically shown to be highly tolerant\nto considerable proportions of label noise and has a consistent improvement\nover state-of-the-art methods. Especially on more difficult datasets with\nhigher noise ratios, we can achieve a significant improvement over the\nsecond-best model. Moreover, our proposed approach can easily be integrated\ninto existing DNN methods to improve their robustness against label noise.",
          "link": "http://arxiv.org/abs/2107.02347",
          "publishedOn": "2021-07-07T01:57:10.815Z",
          "wordCount": 698,
          "title": "An Ensemble Noise-Robust K-fold Cross-Validation Selection Method for Noisy Labels. (arXiv:2107.02347v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3$\\times$ as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results~(e.g., Top-1 accuracy 84.1% trained on 224$\\times$224\nImageNet-1K only), while being more scalable on high-resolution images. The\nmodels and source code will be released soon.",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-07T01:57:10.809Z",
          "wordCount": 671,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huaju Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Hongyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xinbo Lv</a>",
          "description": "This paper proposes an artificial neural network to determine orientation\nusing polarized skylight. This neural network has specific dilated convolution,\nwhich can extract light intensity information of different polarization\ndirections. Then, the degree of polarization (DOP) and angle of polarization\n(AOP) are directly extracted in the network. In addition, the exponential\nfunction encoding of orientation is designed as the network output, which can\nbetter reflect the insect's encoding of polarization information, and improve\nthe accuracy of orientation determination. Finally, training and testing were\nconducted on a public polarized skylight navigation dataset, and the\nexperimental results proved the stability and effectiveness of the network.",
          "link": "http://arxiv.org/abs/2107.02328",
          "publishedOn": "2021-07-07T01:57:10.802Z",
          "wordCount": 540,
          "title": "Polarized skylight orientation determination artificial neural network. (arXiv:2107.02328v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kato_S/0/1/0/all/0/1\">Sota Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotta_K/0/1/0/all/0/1\">Kazuhiro Hotta</a>",
          "description": "In this paper, we propose mean squared error (MSE) loss with outlying label\nfor class imbalanced classification. Cross entropy (CE) loss, which is widely\nused for image recognition, is learned so that the probability value of true\nclass is closer to one by back propagation. However, for imbalanced datasets,\nthe learning is insufficient for the classes with a small number of samples.\nTherefore, we propose a novel classification method using the MSE loss that can\nbe learned the relationships of all classes no matter which image is input.\nUnlike CE loss, MSE loss is possible to equalize the number of back propagation\nfor all classes and to learn the feature space considering the relationships\nbetween classes as metric learning. Furthermore, instead of the usual one-hot\nteacher label, we use a novel teacher label that takes the number of class\nsamples into account. This induces the outlying label which depends on the\nnumber of samples in each class, and the class with a small number of samples\nhas outlying margin in a feature space. It is possible to create the feature\nspace for separating high-difficulty classes and low-difficulty classes. By the\nexperiments on imbalanced classification and semantic segmentation, we\nconfirmed that the proposed method was much improved in comparison with\nstandard CE loss and conventional methods, even though only the loss and\nteacher labels were changed.",
          "link": "http://arxiv.org/abs/2107.02393",
          "publishedOn": "2021-07-07T01:57:10.796Z",
          "wordCount": 657,
          "title": "MSE Loss with Outlying Label for Imbalanced Classification. (arXiv:2107.02393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1\">Siddharth Karamcheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>",
          "description": "Active learning promises to alleviate the massive data needs of supervised\nmachine learning: it has successfully improved sample efficiency by an order of\nmagnitude on traditional tasks like topic classification and object\nrecognition. However, we uncover a striking contrast to this promise: across 5\nmodels and 4 datasets on the task of visual question answering, a wide variety\nof active learning approaches fail to outperform random selection. To\nunderstand this discrepancy, we profile 8 active learning methods on a\nper-example basis, and identify the problem as collective outliers -- groups of\nexamples that active learning methods prefer to acquire but models fail to\nlearn (e.g., questions that ask about text in images or require external\nknowledge). Through systematic ablation experiments and qualitative\nvisualizations, we verify that collective outliers are a general phenomenon\nresponsible for degrading pool-based active learning. Notably, we show that\nactive learning sample efficiency increases significantly as the number of\ncollective outliers in the active learning pool decreases. We conclude with a\ndiscussion and prescriptive recommendations for mitigating the effects of these\noutliers in future work.",
          "link": "http://arxiv.org/abs/2107.02331",
          "publishedOn": "2021-07-07T01:57:10.780Z",
          "wordCount": 650,
          "title": "Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering. (arXiv:2107.02331v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02287",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Cardoso_N/0/1/0/all/0/1\">N. M. Cardoso</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Schwarz_G/0/1/0/all/0/1\">G. B. O. Schwarz</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dias_L/0/1/0/all/0/1\">L. O. Dias</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bom_C/0/1/0/all/0/1\">C. R. Bom</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sodre_L/0/1/0/all/0/1\">L. Sodr&#xe9; Jr.</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Oliveira_C/0/1/0/all/0/1\">C. Mendes de Oliveira</a>",
          "description": "The universe is composed of galaxies that have diverse shapes. Once the\nstructure of a galaxy is determined, it is possible to obtain important\ninformation about its formation and evolution. Morphologically classifying\ngalaxies means cataloging them according to their visual appearance and the\nclassification is linked to the physical properties of the galaxy. A\nmorphological classification made through visual inspection is subject to\nbiases introduced by subjective observations made by human volunteers. For this\nreason, systematic, objective and easily reproducible classification of\ngalaxies has been gaining importance since the astronomer Edwin Hubble created\nhis famous classification method. In this work, we combine accurate visual\nclassifications of the Galaxy Zoo project with \\emph {Deep Learning} methods.\nThe goal is to find an efficient technique at human performance level\nclassification, but in a systematic and automatic way, for classification of\nelliptical and spiral galaxies. For this, a neural network model was created\nthrough an Ensemble of four other convolutional models, allowing a greater\naccuracy in the classification than what would be obtained with any one\nindividual. Details of the individual models and improvements made are also\ndescribed. The present work is entirely based on the analysis of images (not\nparameter tables) from DR1 (www.datalab.noao.edu) of the Southern Photometric\nLocal Universe Survey (S-PLUS). In terms of classification, we achieved, with\nthe Ensemble, an accuracy of $\\approx 99 \\%$ in the test sample (using\npre-trained networks).",
          "link": "http://arxiv.org/abs/2107.02287",
          "publishedOn": "2021-07-07T01:57:10.756Z",
          "wordCount": 713,
          "title": "Morphological Classification of Galaxies in S-PLUS using an Ensemble of Convolutional Networks. (arXiv:2107.02287v1 [astro-ph.GA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>",
          "description": "Online tracking of multiple objects in videos requires strong capacity of\nmodeling and matching object appearances. Previous methods for learning\nappearance embedding mostly rely on instance-level matching without considering\nthe temporal continuity provided by videos. We design a new instance-to-track\nmatching objective to learn appearance embedding that compares a candidate\ndetection to the embedding of the tracks persisted in the tracker. It enables\nus to learn not only from videos labeled with complete tracks, but also\nunlabeled or partially labeled videos. We implement this learning objective in\na unified form following the spirit of constrastive loss. Experiments on\nmultiple object tracking datasets demonstrate that our method can effectively\nlearning discriminative appearance embeddings in a semi-supervised fashion and\noutperform state of the art methods on representative benchmarks.",
          "link": "http://arxiv.org/abs/2107.02396",
          "publishedOn": "2021-07-07T01:57:10.749Z",
          "wordCount": 561,
          "title": "Semi-TCL: Semi-Supervised Track Contrastive Representation Learning. (arXiv:2107.02396v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02345",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1\">Ricky Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_T/0/1/0/all/0/1\">Timothy T. Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_G/0/1/0/all/0/1\">Gavin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_D/0/1/0/all/0/1\">Da Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarunic_M/0/1/0/all/0/1\">Marinko V. Sarunic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beg_M/0/1/0/all/0/1\">Mirza Faisal Beg</a>",
          "description": "With the FDA approval of Artificial Intelligence (AI) for point-of-care\nclinical diagnoses, model generalizability is of the utmost importance as\nclinical decision-making must be domain-agnostic. A method of tackling the\nproblem is to increase the dataset to include images from a multitude of\ndomains; while this technique is ideal, the security requirements of medical\ndata is a major limitation. Additionally, researchers with developed tools\nbenefit from the addition of open-sourced data, but are limited by the\ndifference in domains. Herewith, we investigated the implementation of a\nCycle-Consistent Generative Adversarial Networks (CycleGAN) for the domain\nadaptation of Optical Coherence Tomography (OCT) volumes. This study was done\nin collaboration with the Biomedical Optics Research Group and Functional &\nAnatomical Imaging & Shape Analysis Lab at Simon Fraser University. In this\nstudy, we investigated a learning-based approach of adapting the domain of a\npublicly available dataset, UK Biobank dataset (UKB). To evaluate the\nperformance of domain adaptation, we utilized pre-existing retinal layer\nsegmentation tools developed on a different set of RETOUCH OCT data. This study\nprovides insight on state-of-the-art tools for domain adaptation compared to\ntraditional processing techniques as well as a pipeline for adapting publicly\navailable retinal data to the domains previously used by our collaborators.",
          "link": "http://arxiv.org/abs/2107.02345",
          "publishedOn": "2021-07-07T01:57:10.735Z",
          "wordCount": 675,
          "title": "Domain Adaptation via CycleGAN for Retina Segmentation in Optical Coherence Tomography. (arXiv:2107.02345v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02211",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Peciulis_R/0/1/0/all/0/1\">Rokas Pe&#x10d;iulis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lukosevicius_M/0/1/0/all/0/1\">Mantas Luko&#x161;evi&#x10d;ius</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krisciukaitis_A/0/1/0/all/0/1\">Algimantas Kri&#x161;&#x10d;iukaitis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petrolis_R/0/1/0/all/0/1\">Robertas Petrolis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buteikiene_D/0/1/0/all/0/1\">Dovil&#x117; Buteikien&#x117;</a>",
          "description": "This work aims to research an automatic method for detecting Age-related\nMacular Degeneration (AMD) lesions in RGB eye fundus images. For this, we align\ninvasively obtained eye fundus contrast images (the \"golden standard\"\ndiagnostic) to the RGB ones and use them to hand-annotate the lesions. This is\ndone using our custom-made tool. Using the data, we train and test five\ndifferent convolutional neural networks: a custom one to classify healthy and\nAMD-affected eye fundi, and four well-known networks: ResNet50, ResNet101,\nMobileNetV3, and UNet to segment (localize) the AMD lesions in the affected eye\nfundus images. We achieve 93.55% accuracy or 69.71% Dice index as the\npreliminary best results in segmentation with MobileNetV3.",
          "link": "http://arxiv.org/abs/2107.02211",
          "publishedOn": "2021-07-07T01:57:10.717Z",
          "wordCount": 575,
          "title": "Automated age-related macular degeneration area estimation -- first results. (arXiv:2107.02211v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>",
          "description": "Nowadays, deep learning is widely applied to extract features for similarity\ncomputation in person re-identification (re-ID) and have achieved great\nsuccess. However, due to the non-overlapping between training and testing IDs,\nthe difference between the data used for model training and the testing data\nmakes the performance of learned feature degraded during testing. Hence,\nre-ranking is proposed to mitigate this issue and various algorithms have been\ndeveloped. However, most of existing re-ranking methods focus on replacing the\nEuclidean distance with sophisticated distance metrics, which are not friendly\nto downstream tasks and hard to be used for fast retrieval of massive data in\nreal applications. In this work, we propose a graph-based re-ranking method to\nimprove learned features while still keeping Euclidean distance as the\nsimilarity metric. Inspired by graph convolution networks, we develop an\noperator to propagate features over an appropriate graph. Since graph is the\nessential key for the propagation, two important criteria are considered for\ndesigning the graph, and three different graphs are explored accordingly.\nFurthermore, a simple yet effective method is proposed to generate a profile\nvector for each tracklet in videos, which helps extend our method to video\nre-ID. Extensive experiments on three benchmark data sets, e.g., Market-1501,\nDuke, and MARS, demonstrate the effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2107.02220",
          "publishedOn": "2021-07-07T01:57:10.711Z",
          "wordCount": 652,
          "title": "Graph Convolution for Re-ranking in Person Re-identification. (arXiv:2107.02220v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02293",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tayebi_R/0/1/0/all/0/1\">Rohollah Moosavi Tayebi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mu_Y/0/1/0/all/0/1\">Youqing Mu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dehkharghanian_T/0/1/0/all/0/1\">Taher Dehkharghanian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ross_C/0/1/0/all/0/1\">Catherine Ross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sur_M/0/1/0/all/0/1\">Monalisa Sur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Foley_R/0/1/0/all/0/1\">Ronan Foley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1\">Hamid R. Tizhoosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campbell_C/0/1/0/all/0/1\">Clinton JV Campbell</a>",
          "description": "Bone marrow cytology is required to make a hematological diagnosis,\ninfluencing critical clinical decision points in hematology. However, bone\nmarrow cytology is tedious, limited to experienced reference centers and\nassociated with high inter-observer variability. This may lead to a delayed or\nincorrect diagnosis, leaving an unmet need for innovative supporting\ntechnologies. We have developed the first ever end-to-end deep learning-based\ntechnology for automated bone marrow cytology. Starting with a bone marrow\naspirate digital whole slide image, our technology rapidly and automatically\ndetects suitable regions for cytology, and subsequently identifies and\nclassifies all bone marrow cells in each region. This collective\ncytomorphological information is captured in a novel representation called\nHistogram of Cell Types (HCT) quantifying bone marrow cell class probability\ndistribution and acting as a cytological \"patient fingerprint\". The approach\nachieves high accuracy in region detection (0.97 accuracy and 0.99 ROC AUC),\nand cell detection and cell classification (0.75 mAP, 0.78 F1-score,\nLog-average miss rate of 0.31). HCT has potential to revolutionize\nhematopathology diagnostic workflows, leading to more cost-effective, accurate\ndiagnosis and opening the door to precision medicine.",
          "link": "http://arxiv.org/abs/2107.02293",
          "publishedOn": "2021-07-07T01:57:10.698Z",
          "wordCount": 644,
          "title": "Histogram of Cell Types: Deep Learning for Automated Bone Marrow Cytology. (arXiv:2107.02293v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bozic_A/0/1/0/all/0/1\">Alja&#x17e; Bo&#x17e;i&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palafox_P/0/1/0/all/0/1\">Pablo Palafox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>",
          "description": "We introduce TransformerFusion, a transformer-based 3D scene reconstruction\napproach. From an input monocular RGB video, the video frames are processed by\na transformer network that fuses the observations into a volumetric feature\ngrid representing the scene; this feature grid is then decoded into an implicit\n3D scene representation. Key to our approach is the transformer architecture\nthat enables the network to learn to attend to the most relevant image frames\nfor each 3D location in the scene, supervised only by the scene reconstruction\ntask. Features are fused in a coarse-to-fine fashion, storing fine-level\nfeatures only where needed, requiring lower memory storage and enabling fusion\nat interactive rates. The feature grid is then decoded to a higher-resolution\nscene reconstruction, using an MLP-based surface occupancy prediction from\ninterpolated coarse-to-fine 3D features. Our approach results in an accurate\nsurface reconstruction, outperforming state-of-the-art multi-view stereo depth\nestimation methods, fully-convolutional 3D reconstruction approaches, and\napproaches using LSTM- or GRU-based recurrent networks for video sequence\nfusion.",
          "link": "http://arxiv.org/abs/2107.02191",
          "publishedOn": "2021-07-07T01:57:10.691Z",
          "wordCount": 608,
          "title": "TransformerFusion: Monocular RGB Scene Reconstruction using Transformers. (arXiv:2107.02191v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>",
          "description": "The core for tackling the fine-grained visual categorization (FGVC) is to\nlearn subtleyet discriminative features. Most previous works achieve this by\nexplicitly selecting thediscriminative parts or integrating the attention\nmechanism via CNN-based approaches.However, these methods enhance the\ncomputational complexity and make the modeldominated by the regions containing\nthe most of the objects. Recently, vision trans-former (ViT) has achieved SOTA\nperformance on general image recognition tasks. Theself-attention mechanism\naggregates and weights the information from all patches to theclassification\ntoken, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation\ntoken in the deep layer pays more attention to the global information,\nlackingthe local and low-level features that are essential for FGVC. In this\nwork, we proposea novel pure transformer-based framework Feature Fusion Vision\nTransformer (FFVT)where we aggregate the important tokens from each transformer\nlayer to compensate thelocal, low-level and middle-level information. We design\na novel token selection mod-ule called mutual attention weight selection (MAWS)\nto guide the network effectivelyand efficiently towards selecting\ndiscriminative tokens without introducing extra param-eters. We verify the\neffectiveness of FFVT on three benchmarks where FFVT achievesthe\nstate-of-the-art performance.",
          "link": "http://arxiv.org/abs/2107.02341",
          "publishedOn": "2021-07-07T01:57:10.685Z",
          "wordCount": 619,
          "title": "Feature Fusion Vision Transformer Fine-Grained Visual Categorization. (arXiv:2107.02341v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodasara_S/0/1/0/all/0/1\">Satyam Ghodasara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilello_M/0/1/0/all/0/1\">Michel Bilello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Suyash Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calabrese_E/0/1/0/all/0/1\">Evan Calabrese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colak_E/0/1/0/all/0/1\">Errol Colak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahani_K/0/1/0/all/0/1\">Keyvan Farahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_F/0/1/0/all/0/1\">Felipe C. Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pati_S/0/1/0/all/0/1\">Sarthak Pati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevedello_L/0/1/0/all/0/1\">Luciano M. Prevedello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudie_J/0/1/0/all/0/1\">Jeffrey D. Rudie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sako_C/0/1/0/all/0/1\">Chiharu Sako</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinohara_R/0/1/0/all/0/1\">Russell T. Shinohara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergquist_T/0/1/0/all/0/1\">Timothy Bergquist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_R/0/1/0/all/0/1\">Rong Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eddy_J/0/1/0/all/0/1\">James Eddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_J/0/1/0/all/0/1\">Julia Elliott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reade_W/0/1/0/all/0/1\">Walter Reade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaffter_T/0/1/0/all/0/1\">Thomas Schaffter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Thomas Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiaxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annotators_B/0/1/0/all/0/1\">BraTS Annotators</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1\">Christos Davatzikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mongan_J/0/1/0/all/0/1\">John Mongan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hess_C/0/1/0/all/0/1\">Christopher Hess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Soonmee Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villanueva_Meyer_J/0/1/0/all/0/1\">Javier Villanueva-Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freymann_J/0/1/0/all/0/1\">John B. Freymann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirby_J/0/1/0/all/0/1\">Justin S. Kirby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1\">Benedikt Wiestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crivellaro_P/0/1/0/all/0/1\">Priscila Crivellaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colen_R/0/1/0/all/0/1\">Rivka R.Colen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotrotsou_A/0/1/0/all/0/1\">Aikaterini Kotrotsou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_D/0/1/0/all/0/1\">Daniel Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milchenko_M/0/1/0/all/0/1\">Mikhail Milchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazeri_A/0/1/0/all/0/1\">Arash Nazeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathallah_Shaykh_H/0/1/0/all/0/1\">Hassan Fathallah-Shaykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiest_R/0/1/0/all/0/1\">Roland Wiest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakab_A/0/1/0/all/0/1\">Andras Jakab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1\">Marc-Andre Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1\">Abhishek Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanders_A/0/1/0/all/0/1\">Adam E. Flanders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>",
          "description": "The BraTS 2021 challenge celebrates its 10th anniversary and is jointly\norganized by the Radiological Society of North America (RSNA), the American\nSociety of Neuroradiology (ASNR), and the Medical Image Computing and Computer\nAssisted Interventions (MICCAI) society. Since its inception, BraTS has been\nfocusing on being a common benchmarking venue for brain glioma segmentation\nalgorithms, with well-curated multi-institutional multi-parametric magnetic\nresonance imaging (mpMRI) data. Gliomas are the most common primary\nmalignancies of the central nervous system, with varying degrees of\naggressiveness and prognosis. The RSNA-ASNR-MICCAI BraTS 2021 challenge targets\nthe evaluation of computational algorithms assessing the same tumor\ncompartmentalization, as well as the underlying tumor's molecular\ncharacterization, in pre-operative baseline mpMRI data from 2,000 patients.\nSpecifically, the two tasks that BraTS 2021 focuses on are: a) the segmentation\nof the histologically distinct brain tumor sub-regions, and b) the\nclassification of the tumor's O[6]-methylguanine-DNA methyltransferase (MGMT)\npromoter methylation status. The performance evaluation of all participating\nalgorithms in BraTS 2021 will be conducted through the Sage Bionetworks Synapse\nplatform (Task 1) and Kaggle (Task 2), concluding in distributing to the top\nranked participants monetary awards of $60,000 collectively.",
          "link": "http://arxiv.org/abs/2107.02314",
          "publishedOn": "2021-07-07T01:57:10.668Z",
          "wordCount": 725,
          "title": "The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification. (arXiv:2107.02314v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S. M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-07-06T01:58:09.964Z",
          "wordCount": null,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajora_H/0/1/0/all/0/1\">Harish Rajora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "Worldwide, several cases go undiagnosed due to poor healthcare support in\nremote areas. In this context, a centralized system is needed for effective\nmonitoring and analysis of the medical records. A web-based patient diagnostic\nsystem is a central platform to store the medical history and predict the\npossible disease based on the current symptoms experienced by a patient to\nensure faster and accurate diagnosis. Early disease prediction can help the\nusers determine the severity of the disease and take quick action. The proposed\nweb-based disease prediction system utilizes machine learning based\nclassification techniques on a data set acquired from the National Centre of\nDisease Control (NCDC). $K$-nearest neighbor (K-NN), random forest and naive\nbayes classification approaches are utilized and an ensemble voting algorithm\nis also proposed where each classifier is assigned weights dynamically based on\nthe prediction confidence. The proposed system is also equipped with a\nrecommendation scheme to recommend the type of tests based on the existing\nsymptoms of the patient, so that necessary precautions can be taken. A\ncentralized database ensures that the medical data is preserved and there is\ntransparency in the system. The tampering into the system is prevented by\ngiving the no \"updation\" rights once the diagnosis is created.",
          "link": "http://arxiv.org/abs/2106.02813",
          "publishedOn": "2021-07-06T01:58:09.963Z",
          "wordCount": null,
          "title": "Machine learning equipped web based disease prediction and recommender system. (arXiv:2106.02813v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morrison_K/0/1/0/all/0/1\">Katelyn Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilby_B/0/1/0/all/0/1\">Benjamin Gilby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipchak_C/0/1/0/all/0/1\">Colton Lipchak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattioli_A/0/1/0/all/0/1\">Adam Mattioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>",
          "description": "Recently, vision transformers and MLP-based models have been developed in\norder to address some of the prevalent weaknesses in convolutional neural\nnetworks. Due to the novelty of transformers being used in this domain along\nwith the self-attention mechanism, it remains unclear to what degree these\narchitectures are robust to corruptions. Despite some works proposing that data\naugmentation remains essential for a model to be robust against corruptions, we\npropose to explore the impact that the architecture has on corruption\nrobustness. We find that vision transformer architectures are inherently more\nrobust to corruptions than the ResNet-50 and MLP-Mixers. We also find that\nvision transformers with 5 times fewer parameters than a ResNet-50 have more\nshape bias. Our code is available to reproduce.",
          "link": "http://arxiv.org/abs/2106.13122",
          "publishedOn": "2021-07-06T01:58:09.961Z",
          "wordCount": null,
          "title": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers. (arXiv:2106.13122v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiangshi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tengfei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sham_C/0/1/0/all/0/1\">Chiu-Wing Sham</a>",
          "description": "Modeling implicit feature interaction patterns is of significant importance\nto object detection tasks. However, in the two-stage detectors, due to the\nexcessive use of hand-crafted components, it is very difficult to reason about\nthe implicit relationship of the instance features. To tackle this problem, we\nanalyze three different levels of feature interaction relationships, namely,\nthe dependency relationship between the cropped local features and global\nfeatures, the feature autocorrelation within the instance, and the\ncross-correlation relationship between the instances. To this end, we propose a\nmore compact object detector head network (CODH), which can not only preserve\nglobal context information and condense the information density, but also\nallows instance-wise feature enhancement and relational reasoning in a larger\nmatrix space. Without bells and whistles, our method can effectively improve\nthe detection performance while significantly reducing the parameters of the\nmodel, e.g., with our method, the parameters of the head network is 0.6 times\nsmaller than the state-of-the-art Cascade R-CNN, yet the performance boost is\n1.3% on COCO test-dev. Without losing generality, we can also build a more\nlighter head network for other multi-stage detectors by assembling our method.",
          "link": "http://arxiv.org/abs/2106.14475",
          "publishedOn": "2021-07-06T01:58:09.944Z",
          "wordCount": null,
          "title": "A More Compact Object Detector Head Network with Feature Enhancement and Relational Reasoning. (arXiv:2106.14475v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaohui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Renjie Liao</a>",
          "description": "In this paper, we present a non-parametric structured latent variable model\nfor image generation, called NP-DRAW, which sequentially draws on a latent\ncanvas in a part-by-part fashion and then decodes the image from the canvas.\nOur key contributions are as follows. 1) We propose a non-parametric prior\ndistribution over the appearance of image parts so that the latent variable\n``what-to-draw'' per step becomes a categorical random variable. This improves\nthe expressiveness and greatly eases the learning compared to Gaussians used in\nthe literature. 2) We model the sequential dependency structure of parts via a\nTransformer, which is more powerful and easier to train compared to RNNs used\nin the literature. 3) We propose an effective heuristic parsing algorithm to\npre-train the prior. Experiments on MNIST, Omniglot, CIFAR-10, and CelebA show\nthat our method significantly outperforms previous structured image models like\nDRAW and AIR and is competitive to other generic generative models. Moreover,\nwe show that our model's inherent compositionality and interpretability bring\nsignificant benefits in the low-data learning regime and latent space editing.\nCode is available at https://github.com/ZENGXH/NPDRAW.",
          "link": "http://arxiv.org/abs/2106.13435",
          "publishedOn": "2021-07-06T01:58:09.943Z",
          "wordCount": null,
          "title": "NP-DRAW: A Non-Parametric Structured Latent Variable Model for Image Generation. (arXiv:2106.13435v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>",
          "description": "Convolutional Neural Networks (CNNs) have achieved remarkable success in\nvarious computer vision tasks but rely on tremendous computational cost. To\nsolve this problem, existing approaches either compress well-trained\nlarge-scale models or learn lightweight models with carefully designed network\nstructures. In this work, we make a close study of the convolution operator,\nwhich is the basic unit used in CNNs, to reduce its computing load. In\nparticular, we propose a compact convolution module, called CompConv, to\nfacilitate efficient feature learning. With the divide-and-conquer strategy,\nCompConv is able to save a great many computations as well as parameters to\nproduce a certain dimensional feature map. Furthermore, CompConv discreetly\nintegrates the input features into the outputs to efficiently inherit the input\ninformation. More importantly, the novel CompConv is a plug-and-play module\nthat can be directly applied to modern CNN structures to replace the vanilla\nconvolution layers without further effort. Extensive experimental results\nsuggest that CompConv can adequately compress the benchmark CNN structures yet\nbarely sacrifice the performance, surpassing other competitors.",
          "link": "http://arxiv.org/abs/2106.10486",
          "publishedOn": "2021-07-06T01:58:09.936Z",
          "wordCount": null,
          "title": "CompConv: A Compact Convolution Module for Efficient Feature Learning. (arXiv:2106.10486v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicek_S/0/1/0/all/0/1\">Safa Cicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We present a method for inferring dense depth maps from images and sparse\ndepth measurements by leveraging synthetic data to learn the association of\nsparse point clouds with dense natural shapes, and using the image as evidence\nto validate the predicted depth map. Our learned prior for natural shapes uses\nonly sparse depth as input, not images, so the method is not affected by the\ncovariate shift when attempting to transfer learned models from synthetic data\nto real ones. This allows us to use abundant synthetic data with ground truth\nto learn the most difficult component of the reconstruction process, which is\ntopology estimation, and use the image to refine the prediction based on\nphotometric evidence. Our approach uses fewer parameters than previous methods,\nyet, achieves the state of the art on both indoor and outdoor benchmark\ndatasets. Code available at:\nhttps://github.com/alexklwong/learning-topology-synthetic-data.",
          "link": "http://arxiv.org/abs/2106.02994",
          "publishedOn": "2021-07-06T01:58:09.935Z",
          "wordCount": null,
          "title": "Learning Topology from Synthetic Data for Unsupervised Depth Completion. (arXiv:2106.02994v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingjian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changbin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Feature pyramid network (FPN) is a critical component in modern object\ndetection frameworks. The performance gain in most of the existing FPN variants\nis mainly attributed to the increase of computational burden. An attempt to\nenhance the FPN is enriching the spatial information by expanding the receptive\nfields, which is promising to largely improve the detection accuracy. In this\npaper, we first investigate how expanding the receptive fields affect the\naccuracy and computational costs of FPN. We explore a baseline model called\ninception FPN in which each lateral connection contains convolution filters\nwith different kernel sizes. Moreover, we point out that not all objects need\nsuch a complicated calculation and propose a new dynamic FPN (DyFPN). The\noutput features of DyFPN will be calculated by using the adaptively selected\nbranch according to a dynamic gating operation. Therefore, the proposed method\ncan provide a more efficient dynamic inference for achieving a better trade-off\nbetween accuracy and computational cost. Extensive experiments conducted on\nMS-COCO benchmark demonstrate that the proposed DyFPN significantly improves\nperformance with the optimal allocation of computation resources. For instance,\nreplacing inception FPN with DyFPN reduces about 40% of its FLOPs while\nmaintaining similar high performance.",
          "link": "http://arxiv.org/abs/2012.00779",
          "publishedOn": "2021-07-06T01:58:09.933Z",
          "wordCount": null,
          "title": "Dynamic Feature Pyramid Networks for Object Detection. (arXiv:2012.00779v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.07238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Recently, many deep neural networks were designed to process 3D point clouds,\nbut a common drawback is that rotation invariance is not ensured, leading to\npoor generalization to arbitrary orientations. In this paper, we introduce a\nnew low-level purely rotation-invariant representation to replace common 3D\nCartesian coordinates as the network inputs. Also, we present a network\narchitecture to embed these representations into features, encoding local\nrelations between points and their neighbors, and the global shape structure.\nTo alleviate inevitable global information loss caused by the\nrotation-invariant representations, we further introduce a region relation\nconvolution to encode local and non-local information. We evaluate our method\non multiple point cloud analysis tasks, including shape classification, part\nsegmentation, and shape retrieval. Experimental results show that our method\nachieves consistent, and also the best performance, on inputs at arbitrary\norientations, compared with the state-of-the-arts.",
          "link": "http://arxiv.org/abs/2003.07238",
          "publishedOn": "2021-07-06T01:58:09.932Z",
          "wordCount": null,
          "title": "A Rotation-Invariant Framework for Deep Point Cloud Analysis. (arXiv:2003.07238v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>",
          "description": "Crowd counting has drawn much attention due to its importance in\nsafety-critical surveillance systems. Especially, deep neural network (DNN)\nmethods have significantly reduced estimation errors for crowd counting\nmissions. Recent studies have demonstrated that DNNs are vulnerable to\nadversarial attacks, i.e., normal images with human-imperceptible perturbations\ncould mislead DNNs to make false predictions. In this work, we propose a robust\nattack strategy called Adversarial Patch Attack with Momentum (APAM) to\nsystematically evaluate the robustness of crowd counting models, where the\nattacker's goal is to create an adversarial perturbation that severely degrades\ntheir performances, thus leading to public safety accidents (e.g., stampede\naccidents). Especially, the proposed attack leverages the extreme-density\nbackground information of input images to generate robust adversarial patches\nvia a series of transformations (e.g., interpolation, rotation, etc.). We\nobserve that by perturbing less than 6\\% of image pixels, our attacks severely\ndegrade the performance of crowd counting systems, both digitally and\nphysically. To better enhance the adversarial robustness of crowd counting\nmodels, we propose the first regression model-based Randomized Ablation (RA),\nwhich is more sufficient than Adversarial Training (ADT) (Mean Absolute Error\nof RA is 5 lower than ADT on clean samples and 30 lower than ADT on adversarial\nexamples). Extensive experiments on five crowd counting models demonstrate the\neffectiveness and generality of the proposed method. Code is available at\n\\url{https://github.com/harrywuhust2022/Adv-Crowd-analysis}.",
          "link": "http://arxiv.org/abs/2104.10868",
          "publishedOn": "2021-07-06T01:58:09.925Z",
          "wordCount": null,
          "title": "Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting. (arXiv:2104.10868v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Couturier_R/0/1/0/all/0/1\">Rapha&#xeb;l Couturier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noura_H/0/1/0/all/0/1\">Hassan N. Noura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salman_O/0/1/0/all/0/1\">Ola Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sider_A/0/1/0/all/0/1\">Abderrahmane Sider</a>",
          "description": "Clustering is an unsupervised machine learning method grouping data samples\ninto clusters of similar objects. In practice, clustering has been used in\nnumerous applications such as banking customers profiling, document retrieval,\nimage segmentation, and e-commerce recommendation engines. However, the\nexisting clustering techniques present significant limitations, from which is\nthe dependability of their stability on the initialization parameters (e.g.\nnumber of clusters, centroids). Different solutions were presented in the\nliterature to overcome this limitation (i.e. internal and external validation\nmetrics). However, these solutions require high computational complexity and\nmemory consumption, especially when dealing with big data. In this paper, we\napply the recent object detection Deep Learning (DL) model, named YOLO-v5, to\ndetect the initial clustering parameters such as the number of clusters with\ntheir sizes and centroids. Mainly, the proposed solution consists of adding a\nDL-based initialization phase making the clustering algorithms free of\ninitialization. Two model solutions are provided in this work, one for isolated\nclusters and the other one for overlapping clusters. The features of the\nincoming dataset determine which model to use. Moreover, The results show that\nthe proposed solution can provide near-optimal clusters initialization\nparameters with low computational and resources overhead compared to existing\nsolutions.",
          "link": "http://arxiv.org/abs/2104.13634",
          "publishedOn": "2021-07-06T01:58:09.922Z",
          "wordCount": null,
          "title": "A Deep Learning Object Detection Method for an Efficient Clusters Initialization. (arXiv:2104.13634v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mok_T/0/1/0/all/0/1\">Tony C. W. Mok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_A/0/1/0/all/0/1\">Albert C. S. Chung</a>",
          "description": "Recent deep learning-based methods have shown promising results and runtime\nadvantages in deformable image registration. However, analyzing the effects of\nhyperparameters and searching for optimal regularization parameters prove to be\ntoo prohibitive in deep learning-based methods. This is because it involves\ntraining a substantial number of separate models with distinct hyperparameter\nvalues. In this paper, we propose a conditional image registration method and a\nnew self-supervised learning paradigm for deep deformable image registration.\nBy learning the conditional features that are correlated with the\nregularization hyperparameter, we demonstrate that optimal solutions with\narbitrary hyperparameters can be captured by a single deep convolutional neural\nnetwork. In addition, the smoothness of the resulting deformation field can be\nmanipulated with arbitrary strength of smoothness regularization during\ninference. Extensive experiments on a large-scale brain MRI dataset show that\nour proposed method enables the precise control of the smoothness of the\ndeformation field without sacrificing the runtime advantage or registration\naccuracy.",
          "link": "http://arxiv.org/abs/2106.12673",
          "publishedOn": "2021-07-06T01:58:08.145Z",
          "wordCount": 636,
          "title": "Conditional Deformable Image Registration with Convolutional Neural Network. (arXiv:2106.12673v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuo_H/0/1/0/all/0/1\">Hongya Tuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Z/0/1/0/all/0/1\">Zhongliang Jing</a>",
          "description": "Domain shift is a major challenge for object detectors to generalize well to\nreal world applications. Emerging techniques of domain adaptation for two-stage\ndetectors help to tackle this problem. However, two-stage detectors are not the\nfirst choice for industrial applications due to its long time consumption. In\nthis paper, a novel Domain Adaptive YOLO (DA-YOLO) is proposed to improve\ncross-domain performance for one-stage detectors. Image level features\nalignment is used to strictly match for local features like texture, and\nloosely match for global features like illumination. Multi-scale instance level\nfeatures alignment is presented to reduce instance domain shift effectively ,\nsuch as variations in object appearance and viewpoint. A consensus\nregularization to these domain classifiers is employed to help the network\ngenerate domain-invariant detections. We evaluate our proposed method on\npopular datasets like Cityscapes, KITTI, SIM10K and etc.. The results\ndemonstrate significant improvement when tested under different cross-domain\nscenarios.",
          "link": "http://arxiv.org/abs/2106.13939",
          "publishedOn": "2021-07-06T01:58:08.117Z",
          "wordCount": 609,
          "title": "Domain Adaptive YOLO for One-Stage Cross-Domain Detection. (arXiv:2106.13939v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1\">Poojan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindagi_V/0/1/0/all/0/1\">Vishwanath A. Sindagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1\">Vibashan VS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Recent advances in deep learning have led to the development of accurate and\nefficient models for various computer vision applications such as\nclassification, segmentation, and detection. However, learning highly accurate\nmodels relies on the availability of large-scale annotated datasets. Due to\nthis, model performance drops drastically when evaluated on label-scarce\ndatasets having visually distinct images, termed as domain adaptation problem.\nThere is a plethora of works to adapt classification and segmentation models to\nlabel-scarce target datasets through unsupervised domain adaptation.\nConsidering that detection is a fundamental task in computer vision, many\nrecent works have focused on developing novel domain adaptive detection\ntechniques. Here, we describe in detail the domain adaptation problem for\ndetection and present an extensive survey of the various methods. Furthermore,\nwe highlight strategies proposed and the associated shortcomings. Subsequently,\nwe identify multiple aspects of the problem that are most promising for future\nresearch. We believe that this survey shall be valuable to the pattern\nrecognition experts working in the fields of computer vision, biometrics,\nmedical imaging, and autonomous navigation by introducing them to the problem,\nand familiarizing them with the current status of the progress while providing\npromising directions for future research.",
          "link": "http://arxiv.org/abs/2105.13502",
          "publishedOn": "2021-07-06T01:58:08.110Z",
          "wordCount": 667,
          "title": "Unsupervised Domain Adaptation of Object Detectors: A Survey. (arXiv:2105.13502v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiulou Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yuxia Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shouju Wang</a>",
          "description": "Intratumoral nanoparticles (NPs) distribution is critical for the success of\nnanomedicine in imaging and treatment, but computational models to describe the\nNPs distribution remain unavailable due to the complex tumor-nano interactions.\nHere, we develop a Generative Adversarial Network for Distribution Analysis\n(GANDA) to describe and conditionally generates the intratumoral quantum dots\n(QDs) distribution after i.v. injection. This deep generative model is trained\nautomatically by 27 775 patches of tumor vessels and cell nuclei decomposed\nfrom whole-slide images of 4T1 breast cancer sections. The GANDA model can\nconditionally generate images of intratumoral QDs distribution under the\nconstraint of given tumor vessels and cell nuclei channels with the same\nspatial resolution (pixels-to-pixels), minimal loss (mean squared error, MSE =\n1.871) and excellent reliability (intraclass correlation, ICC = 0.94).\nQuantitative analysis of QDs extravasation distance (ICC = 0.95) and subarea\ndistribution (ICC = 0.99) is allowed on the generated images without knowing\nthe real QDs distribution. We believe this deep generative model may provide\nopportunities to investigate how influencing factors affect NPs distribution in\nindividual tumors and guide nanomedicine optimization for molecular imaging and\npersonalized treatment.",
          "link": "http://arxiv.org/abs/2012.12561",
          "publishedOn": "2021-07-06T01:58:08.104Z",
          "wordCount": 668,
          "title": "GANDA: A deep generative adversarial network predicts the spatial distribution of nanoparticles in tumor pixelly. (arXiv:2012.12561v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laidlaw_C/0/1/0/all/0/1\">Cassidy Laidlaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sahil Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>",
          "description": "A key challenge in adversarial robustness is the lack of a precise\nmathematical characterization of human perception, used in the very definition\nof adversarial attacks that are imperceptible to human eyes. Most current\nattacks and defenses try to avoid this issue by considering restrictive\nadversarial threat models such as those bounded by $L_2$ or $L_\\infty$\ndistance, spatial perturbations, etc. However, models that are robust against\nany of these restrictive threat models are still fragile against other threat\nmodels. To resolve this issue, we propose adversarial training against the set\nof all imperceptible adversarial examples, approximated using deep neural\nnetworks. We call this threat model the neural perceptual threat model (NPTM);\nit includes adversarial examples with a bounded neural perceptual distance (a\nneural network-based approximation of the true perceptual distance) to natural\nimages. Through an extensive perceptual study, we show that the neural\nperceptual distance correlates well with human judgements of perceptibility of\nadversarial examples, validating our threat model.\n\nUnder the NPTM, we develop novel perceptual adversarial attacks and defenses.\nBecause the NPTM is very broad, we find that Perceptual Adversarial Training\n(PAT) against a perceptual attack gives robustness against many other types of\nadversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five\ndiverse adversarial attacks. We find that PAT achieves state-of-the-art\nrobustness against the union of these five attacks, more than doubling the\naccuracy over the next best model, without training against any of them. That\nis, PAT generalizes well to unforeseen perturbation types. This is vital in\nsensitive applications where a particular threat model cannot be assumed, and\nto the best of our knowledge, PAT is the first adversarial training defense\nwith this property.",
          "link": "http://arxiv.org/abs/2006.12655",
          "publishedOn": "2021-07-06T01:58:08.097Z",
          "wordCount": 777,
          "title": "Perceptual Adversarial Robustness: Defense Against Unseen Threat Models. (arXiv:2006.12655v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Transformer recently has shown encouraging progresses in computer vision. In\nthis work, we present new baselines by improving the original Pyramid Vision\nTransformer (abbreviated as PVTv1) by adding three designs, including (1)\noverlapping patch embedding, (2) convolutional feed-forward networks, and (3)\nlinear complexity attention layers.\n\nWith these modifications, our PVTv2 significantly improves PVTv1 on three\ntasks e.g., classification, detection, and segmentation. Moreover, PVTv2\nachieves comparable or better performances than recent works such as Swin\nTransformer. We hope this work will facilitate state-of-the-art Transformer\nresearches in computer vision. Code is available at\nhttps://github.com/whai362/PVT .",
          "link": "http://arxiv.org/abs/2106.13797",
          "publishedOn": "2021-07-06T01:58:08.074Z",
          "wordCount": 581,
          "title": "PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Hoang Phan</a>",
          "description": "This paper proposed a new methodology for machine learning in 2-dimensional\nspace (2-D ML) in inline coordinates. It is a full machine learning approach\nthat does not require to deal with n-dimensional data in n-dimensional space.\nIt allows discovering n-D patterns in 2-D space without loss of n-D information\nusing graph representations of n-D data in 2-D. Specifically, it can be done\nwith the inline based coordinates in different modifications, including static\nand dynamic ones. The classification and regression algorithms based on these\ninline coordinates were introduced. A successful case study based on a\nbenchmark data demonstrated the feasibility of the approach. This approach\nhelps to consolidate further a whole new area of full 2-D machine learning as a\npromising ML methodology. It has advantages of abilities to involve actively\nthe end-users into the discovering of models and their justification. Another\nadvantage is providing interpretable ML models.",
          "link": "http://arxiv.org/abs/2106.07568",
          "publishedOn": "2021-07-06T01:58:08.064Z",
          "wordCount": 612,
          "title": "Full interpretable machine learning in 2D with inline coordinates. (arXiv:2106.07568v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.01489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_C/0/1/0/all/0/1\">Ciaran Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horgan_J/0/1/0/all/0/1\">Jonathan Horgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varley_P/0/1/0/all/0/1\">Padraig Varley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODea_D/0/1/0/all/0/1\">Derek O&#x27;Dea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uricar_M/0/1/0/all/0/1\">Michal Uricar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milz_S/0/1/0/all/0/1\">Stefan Milz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_M/0/1/0/all/0/1\">Martin Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amende_K/0/1/0/all/0/1\">Karl Amende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1\">Christian Witt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chennupati_S/0/1/0/all/0/1\">Sumanth Chennupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Sanjaya Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_S/0/1/0/all/0/1\">Saquib Mansoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perroton_X/0/1/0/all/0/1\">Xavier Perroton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick Perez</a>",
          "description": "Fisheye cameras are commonly employed for obtaining a large field of view in\nsurveillance, augmented reality and in particular automotive applications. In\nspite of their prevalence, there are few public datasets for detailed\nevaluation of computer vision algorithms on fisheye images. We release the\nfirst extensive fisheye automotive dataset, WoodScape, named after Robert Wood\nwho invented the fisheye camera in 1906. WoodScape comprises of four surround\nview cameras and nine tasks including segmentation, depth estimation, 3D\nbounding box detection and soiling detection. Semantic annotation of 40 classes\nat the instance level is provided for over 10,000 images and annotation for\nother tasks are provided for over 100,000 images. With WoodScape, we would like\nto encourage the community to adapt computer vision models for fisheye camera\ninstead of using naive rectification.",
          "link": "http://arxiv.org/abs/1905.01489",
          "publishedOn": "2021-07-06T01:58:08.054Z",
          "wordCount": 681,
          "title": "WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving. (arXiv:1905.01489v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deziel_J/0/1/0/all/0/1\">Jean-Luc D&#xe9;ziel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merriaux_P/0/1/0/all/0/1\">Pierre Merriaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tremblay_F/0/1/0/all/0/1\">Francis Tremblay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lessard_D/0/1/0/all/0/1\">Dave Lessard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plourde_D/0/1/0/all/0/1\">Dominique Plourde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanguennec_J/0/1/0/all/0/1\">Julien Stanguennec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulet_P/0/1/0/all/0/1\">Pierre Goulet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olivier_P/0/1/0/all/0/1\">Pierre Olivier</a>",
          "description": "Leddar PixSet is a new publicly available dataset (dataset.leddartech.com)\nfor autonomous driving research and development. One key novelty of this\ndataset is the presence of full-waveform data from the Leddar Pixell sensor, a\nsolid-state flash LiDAR. Full-waveform data has been shown to improve the\nperformance of perception algorithms in airborne applications but is yet to be\ndemonstrated for terrestrial applications such as autonomous driving. The\nPixSet dataset contains approximately 29k frames from 97 sequences recorded in\nhigh-density urban areas, using a set of various sensors (cameras, LiDARs,\nradar, IMU, etc.) Each frame has been manually annotated with 3D bounding\nboxes.",
          "link": "http://arxiv.org/abs/2102.12010",
          "publishedOn": "2021-07-06T01:58:08.047Z",
          "wordCount": 600,
          "title": "PixSet : An Opportunity for 3D Computer Vision to Go Beyond Point Clouds With a Full-Waveform LiDAR Dataset. (arXiv:2102.12010v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1\">Jun Hao Liew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xuecheng Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "We consider the challenging multi-person 3D body mesh estimation task in this\nwork. Existing methods are mostly two-stage based--one stage for person\nlocalization and the other stage for individual body mesh estimation, leading\nto redundant pipelines with high computation cost and degraded performance for\ncomplex scenes (e.g., occluded person instances). In this work, we present a\nsingle-stage model, Body Meshes as Points (BMP), to simplify the pipeline and\nlift both efficiency and performance. In particular, BMP adopts a new method\nthat represents multiple person instances as points in the spatial-depth space\nwhere each point is associated with one body mesh. Hinging on such\nrepresentations, BMP can directly predict body meshes for multiple persons in a\nsingle stage by concurrently localizing person instance points and estimating\nthe corresponding body meshes. To better reason about depth ordering of all the\npersons within the same scene, BMP designs a simple yet effective\ninter-instance ordinal depth loss to obtain depth-coherent body mesh\nestimation. BMP also introduces a novel keypoint-aware augmentation to enhance\nmodel robustness to occluded person instances. Comprehensive experiments on\nbenchmarks Panoptic, MuPoTS-3D and 3DPW clearly demonstrate the\nstate-of-the-art efficiency of BMP for multi-person body mesh estimation,\ntogether with outstanding accuracy. Code can be found at:\nhttps://github.com/jfzhang95/BMP.",
          "link": "http://arxiv.org/abs/2105.02467",
          "publishedOn": "2021-07-06T01:58:08.039Z",
          "wordCount": 672,
          "title": "Body Meshes as Points. (arXiv:2105.02467v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01988",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Louiset_R/0/1/0/all/0/1\">Robin Louiset</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dufumier_B/0/1/0/all/0/1\">Benoit Dufumier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Houenou_J/0/1/0/all/0/1\">Josselin Houenou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grigis_A/0/1/0/all/0/1\">Antoine Grigis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duchesnay_E/0/1/0/all/0/1\">Edouard Duchesnay</a>",
          "description": "Subtype Discovery consists in finding interpretable and consistent sub-parts\nof a dataset, which are also relevant to a certain supervised task. From a\nmathematical point of view, this can be defined as a clustering task driven by\nsupervised learning in order to uncover subgroups in line with the supervised\nprediction. In this paper, we propose a general Expectation-Maximization\nensemble framework entitled UCSL (Unsupervised Clustering driven by Supervised\nLearning). Our method is generic, it can integrate any clustering method and\ncan be driven by both binary classification and regression. We propose to\nconstruct a non-linear model by merging multiple linear estimators, one per\ncluster. Each hyperplane is estimated so that it correctly discriminates - or\npredict - only one cluster. We use SVC or Logistic Regression for\nclassification and SVR for regression. Furthermore, to perform cluster analysis\nwithin a more suitable space, we also propose a dimension-reduction algorithm\nthat projects the data onto an orthonormal space relevant to the supervised\ntask. We analyze the robustness and generalization capability of our algorithm\nusing synthetic and experimental datasets. In particular, we validate its\nability to identify suitable consistent sub-types by conducting a\npsychiatric-diseases cluster analysis with known ground-truth labels. The gain\nof the proposed method over previous state-of-the-art techniques is about +1.9\npoints in terms of balanced accuracy. Finally, we make codes and examples\navailable in a scikit-learn-compatible Python package at\nhttps://github.com/neurospin-projects/2021_rlouiset_ucsl",
          "link": "http://arxiv.org/abs/2107.01988",
          "publishedOn": "2021-07-06T01:58:08.032Z",
          "wordCount": 699,
          "title": "UCSL : A Machine Learning Expectation-Maximization framework for Unsupervised Clustering driven by Supervised Learning. (arXiv:2107.01988v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2009.02755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lorbeer_B/0/1/0/all/0/1\">Boris Lorbeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botler_M/0/1/0/all/0/1\">Max Botler</a>",
          "description": "In this paper, we propose POTATOES (Partitioning OverfiTting AuTOencoder\nEnSemble), a new method for unsupervised outlier detection (UOD). More\nprecisely, given any autoencoder for UOD, this technique can be used to improve\nits accuracy while at the same time removing the burden of tuning its\nregularization. The idea is to not regularize at all, but to rather randomly\npartition the data into sufficiently many equally sized parts, overfit each\npart with its own autoencoder, and to use the maximum over all autoencoder\nreconstruction errors as the anomaly score. We apply our model to various\nrealistic datasets and show that if the set of inliers is dense enough, our\nmethod indeed improves the UOD performance of a given autoencoder\nsignificantly. For reproducibility, the code is made available on github so the\nreader can recreate the results in this paper as well as apply the method to\nother autoencoders and datasets.",
          "link": "http://arxiv.org/abs/2009.02755",
          "publishedOn": "2021-07-06T01:58:08.012Z",
          "wordCount": 647,
          "title": "Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles. (arXiv:2009.02755v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Recasens_D/0/1/0/all/0/1\">David Recasens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamarca_J/0/1/0/all/0/1\">Jos&#xe9; Lamarca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facil_J/0/1/0/all/0/1\">Jos&#xe9; M. F&#xe1;cil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montiel_J/0/1/0/all/0/1\">J. M. M. Montiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>",
          "description": "Estimating a scene reconstruction and the camera motion from in-body videos\nis challenging due to several factors, e.g. the deformation of in-body cavities\nor the lack of texture. In this paper we present Endo-Depth-and-Motion, a\npipeline that estimates the 6-degrees-of-freedom camera pose and dense 3D scene\nmodels from monocular endoscopic videos. Our approach leverages recent advances\nin self-supervised depth networks to generate pseudo-RGBD frames, then tracks\nthe camera pose using photometric residuals and fuses the registered depth maps\nin a volumetric representation. We present an extensive experimental evaluation\nin the public dataset Hamlyn, showing high-quality results and comparisons\nagainst relevant baselines. We also release all models and code for future\ncomparisons.",
          "link": "http://arxiv.org/abs/2103.16525",
          "publishedOn": "2021-07-06T01:58:08.006Z",
          "wordCount": 600,
          "title": "Endo-Depth-and-Motion: Reconstruction and Tracking in Endoscopic Videos using Depth Networks and Photometric Constraints. (arXiv:2103.16525v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongxiang Lin</a>",
          "description": "For artificial learning systems, continual learning over time from a stream\nof data is essential. The burgeoning studies on supervised continual learning\nhave achieved great progress, while the study of catastrophic forgetting in\nunsupervised learning is still blank. Among unsupervised learning methods,\nself-supervise learning method shows tremendous potential on visual\nrepresentation without any labeled data at scale. To improve the visual\nrepresentation of self-supervised learning, larger and more varied data is\nneeded. In the real world, unlabeled data is generated at all times. This\ncircumstance provides a huge advantage for the learning of the self-supervised\nmethod. However, in the current paradigm, packing previous data and current\ndata together and training it again is a waste of time and resources. Thus, a\ncontinual self-supervised learning method is badly needed. In this paper, we\nmake the first attempt to implement the continual contrastive self-supervised\nlearning by proposing a rehearsal method, which keeps a few exemplars from the\nprevious data. Instead of directly combining saved exemplars with the current\ndata set for training, we leverage self-supervised knowledge distillation to\ntransfer contrastive information among previous data to the current network by\nmimicking similarity score distribution inferred by the old network over a set\nof saved exemplars. Moreover, we build an extra sample queue to assist the\nnetwork to distinguish between previous and current data and prevent mutual\ninterference while learning their own feature representation. Experimental\nresults show that our method performs well on CIFAR100 and ImageNet-Sub.\nCompared with self-supervised baselines, which learning tasks one by one\nwithout taking any technique, we improve the image classification top-1\naccuracy by 1.60% on CIFAR100 and 2.86% on ImageNet-Sub under 10 incremental\nsteps setting.",
          "link": "http://arxiv.org/abs/2107.01776",
          "publishedOn": "2021-07-06T01:58:07.999Z",
          "wordCount": 711,
          "title": "Continual Contrastive Self-supervised Learning for Image Classification. (arXiv:2107.01776v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kerroumi_M/0/1/0/all/0/1\">Mohamed Kerroumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayem_O/0/1/0/all/0/1\">Othmane Sayem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabou_A/0/1/0/all/0/1\">Aymen Shabou</a>",
          "description": "We introduce a novel approach for scanned document representation to perform\nfield extraction. It allows the simultaneous encoding of the textual, visual\nand layout information in a 3-axis tensor used as an input to a segmentation\nmodel. We improve the recent Chargrid and Wordgrid \\cite{chargrid} models in\nseveral ways, first by taking into account the visual modality, then by\nboosting its robustness in regards to small datasets while keeping the\ninference time low. Our approach is tested on public and private document-image\ndatasets, showing higher performances compared to the recent state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2010.02358",
          "publishedOn": "2021-07-06T01:58:07.990Z",
          "wordCount": 585,
          "title": "VisualWordGrid: Information Extraction From Scanned Documents Using A Multimodal Approach. (arXiv:2010.02358v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manigrasso_F/0/1/0/all/0/1\">Francesco Manigrasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miro_F/0/1/0/all/0/1\">Filomeno Davide Miro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morra_L/0/1/0/all/0/1\">Lia Morra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamberti_F/0/1/0/all/0/1\">Fabrizio Lamberti</a>",
          "description": "The detection of semantic relationships between objects represented in an\nimage is one of the fundamental challenges in image interpretation.\nNeural-Symbolic techniques, such as Logic Tensor Networks (LTNs), allow the\ncombination of semantic knowledge representation and reasoning with the ability\nto efficiently learn from examples typical of neural networks. We here propose\nFaster-LTN, an object detector composed of a convolutional backbone and an LTN.\nTo the best of our knowledge, this is the first attempt to combine both\nframeworks in an end-to-end training setting. This architecture is trained by\noptimizing a grounded theory which combines labelled examples with prior\nknowledge, in the form of logical axioms. Experimental comparisons show\ncompetitive performance with respect to the traditional Faster R-CNN\narchitecture.",
          "link": "http://arxiv.org/abs/2107.01877",
          "publishedOn": "2021-07-06T01:58:07.971Z",
          "wordCount": 571,
          "title": "Faster-LTN: a neuro-symbolic, end-to-end object detection architecture. (arXiv:2107.01877v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khodabakhsh_A/0/1/0/all/0/1\">Ali Khodabakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zahid Akhtar</a>",
          "description": "Despite the impressive progress in the field of presentation attack detection\nand multimedia forensics over the last decade, these systems are still\nvulnerable to attacks in real-life settings. Some of the challenges for\nexisting solutions are the detection of unknown attacks, the ability to perform\nin adversarial settings, few-shot learning, and explainability. In this study,\nthese limitations are approached by reliance on a game-theoretic view for\nmodeling the interactions between the attacker and the detector. Consequently,\na new optimization criterion is proposed and a set of requirements are defined\nfor improving the performance of these systems in real-life settings.\nFurthermore, a novel detection technique is proposed using generator-based\nfeature sets that are not biased towards any specific attack species. To\nfurther optimize the performance on known attacks, a new loss function coined\ncategorical margin maximization loss (C-marmax) is proposed which gradually\nimproves the performance against the most powerful attack. The proposed\napproach provides a more balanced performance across known and unknown attacks\nand achieves state-of-the-art performance in known and unknown attack detection\ncases against rational attackers. Lastly, the few-shot learning potential of\nthe proposed approach is studied as well as its ability to provide pixel-level\nexplainability.",
          "link": "http://arxiv.org/abs/2010.01592",
          "publishedOn": "2021-07-06T01:58:07.965Z",
          "wordCount": 668,
          "title": "Unknown Presentation Attack Detection against Rational Attackers. (arXiv:2010.01592v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bojian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>",
          "description": "It is important to learn joint embedding for 3D shapes and text in different\nshape understanding tasks, such as shape-text matching, retrieval, and shape\ncaptioning. Current multi-view based methods learn a mapping from multiple\nrendered views to text. However, these methods can not analyze 3D shapes well\ndue to the self-occlusion and limitation of learning manifolds. To resolve this\nissue, we propose a method to learn joint embedding of point clouds and text by\nmatching parts from shapes to words from sentences in a common space.\nSpecifically, we first learn segmentation prior to segment point clouds into\nparts. Then, we map parts and words into an optimized space, where the parts\nand words can be matched with each other. In the optimized space, we represent\na part by aggregating features of all points within the part, while\nrepresenting each word with its context information, where we train our network\nto minimize the triplet ranking loss. Moreover, we also introduce cross-modal\nattention to capture the relationship of part-word in this matching procedure,\nwhich enhances joint embedding learning. Our experimental results outperform\nthe state-of-the-art in multi-modal retrieval under the widely used benchmark.",
          "link": "http://arxiv.org/abs/2107.01872",
          "publishedOn": "2021-07-06T01:58:07.959Z",
          "wordCount": 640,
          "title": "Part2Word: Learning Joint Embedding of Point Clouds and Text by Matching Parts to Words. (arXiv:2107.01872v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.01543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sterneck_R/0/1/0/all/0/1\">Rachel Sterneck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1\">Abhishek Moitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1\">Priyadarshini Panda</a>",
          "description": "Neural networks have achieved remarkable performance in computer vision,\nhowever they are vulnerable to adversarial examples. Adversarial examples are\ninputs that have been carefully perturbed to fool classifier networks, while\nappearing unchanged to humans. Based on prior works on detecting adversaries,\nwe propose a structured methodology of augmenting a deep neural network (DNN)\nwith a detector subnetwork. We use $\\textit{Adversarial Noise Sensitivity}$\n(ANS), a novel metric for measuring the adversarial gradient contribution of\ndifferent intermediate layers of a network. Based on the ANS value, we append a\ndetector to the most sensitive layer. In prior works, more complex detectors\nwere added to a DNN, increasing the inference computational cost of the model.\nIn contrast, our structured and strategic addition of a detector to a DNN\nreduces the complexity of the model while making the overall network\nadversarially resilient. Through comprehensive white-box and black-box\nexperiments on MNIST, CIFAR-10, and CIFAR-100, we show that our method improves\nstate-of-the-art detector robustness against adversarial examples. Furthermore,\nwe validate the energy efficiency of our proposed adversarial detection\nmethodology through an extensive energy analysis on various hardware scalable\nCMOS accelerator platforms. We also demonstrate the effects of quantization on\nour detector-appended networks.",
          "link": "http://arxiv.org/abs/2101.01543",
          "publishedOn": "2021-07-06T01:58:07.952Z",
          "wordCount": 678,
          "title": "Noise Sensitivity-Based Energy Efficient and Robust Adversary Detection in Neural Networks. (arXiv:2101.01543v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.04463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kangle Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Aayush Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>",
          "description": "We present an unsupervised approach that converts the input speech of any\nindividual into audiovisual streams of potentially-infinitely many output\nspeakers. Our approach builds on simple autoencoders that project out-of-sample\ndata onto the distribution of the training set. We use Exemplar Autoencoders to\nlearn the voice, stylistic prosody, and visual appearance of a specific target\nexemplar speech. In contrast to existing methods, the proposed approach can be\neasily extended to an arbitrarily large number of speakers and styles using\nonly 3 minutes of target audio-video data, without requiring {\\em any} training\ndata for the input speaker. To do so, we learn audiovisual bottleneck\nrepresentations that capture the structured linguistic content of speech. We\noutperform prior approaches on both audio and video synthesis, and provide\nextensive qualitative analysis on our project page --\nhttps://www.cs.cmu.edu/~exemplar-ae/.",
          "link": "http://arxiv.org/abs/2001.04463",
          "publishedOn": "2021-07-06T01:58:07.945Z",
          "wordCount": 626,
          "title": "Unsupervised Audiovisual Synthesis via Exemplar Autoencoders. (arXiv:2001.04463v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Enhua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Transformer is a new kind of neural architecture which encodes the input data\nas powerful features via the attention mechanism. Basically, the visual\ntransformers first divide the input images into several local patches and then\ncalculate both representations and their relationship. Since natural images are\nof high complexity with abundant detail and color information, the granularity\nof the patch dividing is not fine enough for excavating features of objects in\ndifferent scales and locations. In this paper, we point out that the attention\ninside these local patches are also essential for building visual transformers\nwith high performance and we explore a new architecture, namely, Transformer iN\nTransformer (TNT). Specifically, we regard the local patches (e.g.,\n16$\\times$16) as \"visual sentences\" and present to further divide them into\nsmaller patches (e.g., 4$\\times$4) as \"visual words\". The attention of each\nword will be calculated with other words in the given visual sentence with\nnegligible computational costs. Features of both words and sentences will be\naggregated to enhance the representation ability. Experiments on several\nbenchmarks demonstrate the effectiveness of the proposed TNT architecture,\ne.g., we achieve an $81.5%$ top-1 accuracy on the ImageNet, which is about\n$1.7%$ higher than that of the state-of-the-art visual transformer with similar\ncomputational cost. The PyTorch code is available at\nhttps://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch, and the\nMindSpore code is at\nhttps://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT.",
          "link": "http://arxiv.org/abs/2103.00112",
          "publishedOn": "2021-07-06T01:58:07.937Z",
          "wordCount": 695,
          "title": "Transformer in Transformer. (arXiv:2103.00112v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sateesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haresh_S/0/1/0/all/0/1\">Sanjay Haresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Awais Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konin_A/0/1/0/all/0/1\">Andrey Konin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zia_M/0/1/0/all/0/1\">M. Zeeshan Zia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quoc-Huy Tran</a>",
          "description": "We present a novel approach for unsupervised activity segmentation, which\nuses video frame clustering as a pretext task and simultaneously performs\nrepresentation learning and online clustering. This is in contrast with prior\nworks where representation learning and clustering are often performed\nsequentially. We leverage temporal information in videos by employing temporal\noptimal transport and temporal coherence loss. In particular, we incorporate a\ntemporal regularization term into the standard optimal transport module, which\npreserves the temporal order of the activity, yielding the temporal optimal\ntransport module for computing pseudo-label cluster assignments. Next, the\ntemporal coherence loss encourages neighboring video frames to be mapped to\nnearby points while distant video frames are mapped to farther away points in\nthe embedding space. The combination of these two components results in\neffective representations for unsupervised activity segmentation. Furthermore,\nprevious methods require storing learned features for the entire dataset before\nclustering them in an offline manner, whereas our approach processes one\nmini-batch at a time in an online manner. Extensive evaluations on three public\ndatasets, i.e. 50-Salads, YouTube Instructions, and Breakfast, and our dataset,\ni.e., Desktop Assembly, show that our approach performs on par or better than\nprevious methods for unsupervised activity segmentation, despite having\nsignificantly less memory constraints.",
          "link": "http://arxiv.org/abs/2105.13353",
          "publishedOn": "2021-07-06T01:58:07.930Z",
          "wordCount": 690,
          "title": "Unsupervised Activity Segmentation by Joint Representation Learning and Online Clustering. (arXiv:2105.13353v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yingxue Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianxin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>",
          "description": "Image-to-image translation (I2I) aims to transfer images from a source domain\nto a target domain while preserving the content representations. I2I has drawn\nincreasing attention and made tremendous progress in recent years because of\nits wide range of applications in many computer vision and image processing\nproblems, such as image synthesis, segmentation, style transfer, restoration,\nand pose estimation. In this paper, we provide an overview of the I2I works\ndeveloped in recent years. We will analyze the key techniques of the existing\nI2I works and clarify the main progress the community has made. Additionally,\nwe will elaborate on the effect of I2I on the research and industry community\nand point out remaining challenges in related fields.",
          "link": "http://arxiv.org/abs/2101.08629",
          "publishedOn": "2021-07-06T01:58:07.922Z",
          "wordCount": 579,
          "title": "Image-to-Image Translation: Methods and Applications. (arXiv:2101.08629v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hager_J/0/1/0/all/0/1\">Janik Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_R/0/1/0/all/0/1\">Ruben Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toussaint_M/0/1/0/all/0/1\">Marc Toussaint</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mainprice_J/0/1/0/all/0/1\">Jim Mainprice</a>",
          "description": "In this paper, we introduce a Grasp Manifold Estimator (GraspME) to detect\ngrasp affordances for objects directly in 2D camera images. To perform\nmanipulation tasks autonomously it is crucial for robots to have such\ngraspability models of the surrounding objects. Grasp manifolds have the\nadvantage of providing continuously infinitely many grasps, which is not the\ncase when using other grasp representations such as predefined grasp points.\nFor instance, this property can be leveraged in motion optimization to define\ngoal sets as implicit surface constraints in the robot configuration space. In\nthis work, we restrict ourselves to the case of estimating possible\nend-effector positions directly from 2D camera images. To this extend, we\ndefine grasp manifolds via a set of key points and locate them in images using\na Mask R-CNN backbone. Using learned features allows generalizing to different\nview angles, with potentially noisy images, and objects that were not part of\nthe training set. We rely on simulation data only and perform experiments on\nsimple and complex objects, including unseen ones. Our framework achieves an\ninference speed of 11.5 fps on a GPU, an average precision for keypoint\nestimation of 94.5% and a mean pixel distance of only 1.29. This shows that we\ncan estimate the objects very well via bounding boxes and segmentation masks as\nwell as approximate the correct grasp manifold's keypoint coordinates.",
          "link": "http://arxiv.org/abs/2107.01836",
          "publishedOn": "2021-07-06T01:58:07.874Z",
          "wordCount": 662,
          "title": "GraspME -- Grasp Manifold Estimator. (arXiv:2107.01836v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14711",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Ce Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hui_Y/0/1/0/all/0/1\">Yuan Hui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_S/0/1/0/all/0/1\">Shiwei Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_M/0/1/0/all/0/1\">Mengke Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quan_Q/0/1/0/all/0/1\">Quan Quan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shuxin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hao_Y/0/1/0/all/0/1\">You Hao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Pengbo Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_H/0/1/0/all/0/1\">Honghu Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1\">Chunpeng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xinbao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>",
          "description": "Spine-related diseases have high morbidity and cause a huge burden of social\ncost. Spine imaging is an essential tool for noninvasively visualizing and\nassessing spinal pathology. Segmenting vertebrae in computed tomography (CT)\nimages is the basis of quantitative medical image analysis for clinical\ndiagnosis and surgery planning of spine diseases. Current publicly available\nannotated datasets on spinal vertebrae are small in size. Due to the lack of a\nlarge-scale annotated spine image dataset, the mainstream deep learning-based\nsegmentation methods, which are data-driven, are heavily restricted. In this\npaper, we introduce a large-scale spine CT dataset, called CTSpine1K, curated\nfrom multiple sources for vertebra segmentation, which contains 1,005 CT\nvolumes with over 11,100 labeled vertebrae belonging to different spinal\nconditions. Based on this dataset, we conduct several spinal vertebrae\nsegmentation experiments to set the first benchmark. We believe that this\nlarge-scale dataset will facilitate further research in many spine-related\nimage analysis tasks, including but not limited to vertebrae segmentation,\nlabeling, 3D spine reconstruction from biplanar radiographs, image\nsuper-resolution, and enhancement.",
          "link": "http://arxiv.org/abs/2105.14711",
          "publishedOn": "2021-07-06T01:58:07.857Z",
          "wordCount": 668,
          "title": "CTSpine1K: A Large-Scale Dataset for Spinal Vertebrae Segmentation in Computed Tomography. (arXiv:2105.14711v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10762",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "Random field and random cluster theory are used to describe certain\nmathematical results concerning the probability distribution of image pixel\nintensities characterized as generic $2D$ integer arrays. The size of the\nsmallest bounded region within an image is estimated for segmenting an image,\nfrom which, the equilibrium distribution of intensities can be recovered. From\nthe estimated bounded regions, properties of the sub-optimal and equilibrium\ndistributions of intensities are derived, which leads to an image compression\nmethodology whereby only slightly more than half of all pixels are required for\na worst-case reconstruction of the original image. A custom deep belief network\nand heuristic allows for the unsupervised segmentation, detection and\nlocalization of objects in an image. An example illustrates the mathematical\nresults.",
          "link": "http://arxiv.org/abs/2104.10762",
          "publishedOn": "2021-07-06T01:58:07.850Z",
          "wordCount": 670,
          "title": "Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v10 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.00113",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kelly_D/0/1/0/all/0/1\">Damien Kelly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dorado_I/0/1/0/all/0/1\">Ignacio Garcia Dorado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>",
          "description": "Could we compress images via standard codecs while avoiding visible\nartifacts? The answer is obvious -- this is doable as long as the bit budget is\ngenerous enough. What if the allocated bit-rate for compression is\ninsufficient? Then unfortunately, artifacts are a fact of life. Many attempts\nwere made over the years to fight this phenomenon, with various degrees of\nsuccess. In this work we aim to break the unholy connection between bit-rate\nand image quality, and propose a way to circumvent compression artifacts by\npre-editing the incoming image and modifying its content to fit the given bits.\nWe design this editing operation as a learned convolutional neural network, and\nformulate an optimization problem for its training. Our loss takes into account\na proximity between the original image and the edited one, a bit-budget penalty\nover the proposed image, and a no-reference image quality measure for forcing\nthe outcome to be visually pleasing. The proposed approach is demonstrated on\nthe popular JPEG compression, showing savings in bits and/or improvements in\nvisual quality, obtained with intricate editing effects.",
          "link": "http://arxiv.org/abs/2002.00113",
          "publishedOn": "2021-07-06T01:58:07.843Z",
          "wordCount": 640,
          "title": "Better Compression with Deep Pre-Editing. (arXiv:2002.00113v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mai Zhu</a>",
          "description": "The attributes of object contours has great significance for instance\nsegmentation task. However, most of the current popular deep neural networks do\nnot pay much attention to the object edge information. Inspired by the human\nannotation process when making instance segmentation datasets, in this paper,\nwe propose Mask Point R-CNN aiming at promoting the neural network's attention\nto the object boundary. Specifically, we innovatively extend the original human\nkeypoint detection task to the contour point detection of any object. Based on\nthis analogy, we present an contour point detection auxiliary task to Mask\nR-CNN, which can boost the gradient flow between different tasks by effectively\nusing feature fusion strategies and multi-task joint training. As a\nconsequence, the model will be more sensitive to the edges of the object and\ncan capture more geometric features. Quantitatively, the experimental results\nshow that our approach outperforms vanilla Mask R-CNN by 3.8\\% on Cityscapes\ndataset and 0.8\\% on COCO dataset.",
          "link": "http://arxiv.org/abs/2008.00460",
          "publishedOn": "2021-07-06T01:58:07.835Z",
          "wordCount": 628,
          "title": "Joint Object Contour Points and Semantics for Instance Segmentation. (arXiv:2008.00460v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiarpoor_R/0/1/0/all/0/1\">Reza Esfandiarpoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_A/0/1/0/all/0/1\">Amy Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajabdollahi_M/0/1/0/all/0/1\">Mohsen Hajabdollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>",
          "description": "In many practical few-shot learning problems, even though labeled examples\nare scarce, there are abundant auxiliary datasets that potentially contain\nuseful information. We propose the problem of extended few-shot learning to\nstudy these scenarios. We then introduce a framework to address the challenges\nof efficiently selecting and effectively using auxiliary data in few-shot image\nclassification. Given a large auxiliary dataset and a notion of semantic\nsimilarity among classes, we automatically select pseudo shots, which are\nlabeled examples from other classes related to the target task. We show that\nnaive approaches, such as (1) modeling these additional examples the same as\nthe target task examples or (2) using them to learn features via transfer\nlearning, only increase accuracy by a modest amount. Instead, we propose a\nmasking module that adjusts the features of auxiliary data to be more similar\nto those of the target classes. We show that this masking module performs\nbetter than naively modeling the support examples and transfer learning by 4.68\nand 6.03 percentage points, respectively.",
          "link": "http://arxiv.org/abs/2012.07176",
          "publishedOn": "2021-07-06T01:58:07.816Z",
          "wordCount": 656,
          "title": "Extended Few-Shot Learning: Exploiting Existing Resources for Novel Tasks. (arXiv:2012.07176v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi-Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Generalizable person Re-Identification (ReID) has attracted growing attention\nin recent computer vision community. In this work, we construct a structural\ncausal model among identity labels, identity-specific factors (clothes/shoes\ncolor etc), and domain-specific factors (background, viewpoints etc). According\nto the causal analysis, we propose a novel Domain Invariant Representation\nLearning for generalizable person Re-Identification (DIR-ReID) framework.\nSpecifically, we first propose to disentangle the identity-specific and\ndomain-specific feature spaces, based on which we propose an effective\nalgorithmic implementation for backdoor adjustment, essentially serving as a\ncausal intervention towards the SCM. Extensive experiments have been conducted,\nshowing that DIR-ReID outperforms state-of-the-art methods on large-scale\ndomain generalization ReID benchmarks.",
          "link": "http://arxiv.org/abs/2103.15890",
          "publishedOn": "2021-07-06T01:58:07.809Z",
          "wordCount": 581,
          "title": "Learning Domain Invariant Representations for Generalizable Person Re-Identification. (arXiv:2103.15890v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.10621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosenfeld_J/0/1/0/all/0/1\">Jonathan S. Rosenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1\">Jonathan Frankle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbin_M/0/1/0/all/0/1\">Michael Carbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavit_N/0/1/0/all/0/1\">Nir Shavit</a>",
          "description": "We show that the error of iteratively magnitude-pruned networks empirically\nfollows a scaling law with interpretable coefficients that depend on the\narchitecture and task. We functionally approximate the error of the pruned\nnetworks, showing it is predictable in terms of an invariant tying width,\ndepth, and pruning level, such that networks of vastly different pruned\ndensities are interchangeable. We demonstrate the accuracy of this\napproximation over orders of magnitude in depth, width, dataset size, and\ndensity. We show that the functional form holds (generalizes) for large scale\ndata (e.g., ImageNet) and architectures (e.g., ResNets). As neural networks\nbecome ever larger and costlier to train, our findings suggest a framework for\nreasoning conceptually and analytically about a standard method for\nunstructured pruning.",
          "link": "http://arxiv.org/abs/2006.10621",
          "publishedOn": "2021-07-06T01:58:07.800Z",
          "wordCount": 599,
          "title": "On the Predictability of Pruning Across Scales. (arXiv:2006.10621v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tritrong_N/0/1/0/all/0/1\">Nontawat Tritrong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rewatbowornwong_P/0/1/0/all/0/1\">Pitchaporn Rewatbowornwong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suwajanakorn_S/0/1/0/all/0/1\">Supasorn Suwajanakorn</a>",
          "description": "While GANs have shown success in realistic image generation, the idea of\nusing GANs for other tasks unrelated to synthesis is underexplored. Do GANs\nlearn meaningful structural parts of objects during their attempt to reproduce\nthose objects? In this work, we test this hypothesis and propose a simple and\neffective approach based on GANs for semantic part segmentation that requires\nas few as one label example along with an unlabeled dataset. Our key idea is to\nleverage a trained GAN to extract pixel-wise representation from the input\nimage and use it as feature vectors for a segmentation network. Our experiments\ndemonstrate that GANs representation is \"readily discriminative\" and produces\nsurprisingly good results that are comparable to those from supervised\nbaselines trained with significantly more labels. We believe this novel\nrepurposing of GANs underlies a new class of unsupervised representation\nlearning that is applicable to many other tasks. More results are available at\nhttps://repurposegans.github.io/.",
          "link": "http://arxiv.org/abs/2103.04379",
          "publishedOn": "2021-07-06T01:58:07.792Z",
          "wordCount": 648,
          "title": "Repurposing GANs for One-shot Semantic Part Segmentation. (arXiv:2103.04379v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07279",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Iraji_M/0/1/0/all/0/1\">Mohammad Saber Iraji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanha_J/0/1/0/all/0/1\">Jafar Tanha</a>",
          "description": "The new Coronavirus is spreading rapidly, and it has taken the lives of many\npeople so far. The virus has destructive effects on the human lung, and early\ndetection is very important. Deep Convolution neural networks are such powerful\ntools in classifying images. Therefore, in this paper, a hybrid approach based\non a deep network is presented. Feature vectors were extracted by applying a\ndeep convolution neural network on the images, and useful features were\nselected by the binary differential meta-heuristic algorithm. These optimized\nfeatures were given to the SVM classifier. A database consisting of three\ncategories of images such as COVID-19, pneumonia, and healthy included in 1092\nX-ray samples was considered. The proposed method achieved an accuracy of\n99.43%, a sensitivity of 99.16%, and a specificity of 99.57%. Our results\ndemonstrate that the suggested approach is better than recent studies on\nCOVID-19 detection with X-ray images.",
          "link": "http://arxiv.org/abs/2104.07279",
          "publishedOn": "2021-07-06T01:58:07.785Z",
          "wordCount": 679,
          "title": "COVID-19 detection using deep convolutional neural networks and binary-differential-algorithm-based feature selection on X-ray images. (arXiv:2104.07279v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangtong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>",
          "description": "Image composition aims to generate realistic composite image by inserting an\nobject from one image into another background image, where the placement (e.g.,\nlocation, size, occlusion) of inserted object may be unreasonable, which would\nsignificantly degrade the quality of the composite image. Although some works\nattempted to learn object placement to create realistic composite images, they\ndid not focus on assessing the plausibility of object placement. In this paper,\nwe focus on object placement assessment task, which verifies whether a\ncomposite image is plausible in terms of the object placement. To accomplish\nthis task, we construct the first Object Placement Assessment (OPA) dataset\nconsisting of composite images and their rationality labels. Dataset is\navailable at https://github.com/bcmi/Object-Placement-Assessment-Dataset-OPA.",
          "link": "http://arxiv.org/abs/2107.01889",
          "publishedOn": "2021-07-06T01:58:07.765Z",
          "wordCount": 552,
          "title": "OPA: Object Placement Assessment Dataset. (arXiv:2107.01889v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08727",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zimmer_V/0/1/0/all/0/1\">Veronika A. Zimmer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>",
          "description": "Left atrial (LA) segmentation from late gadolinium enhanced magnetic\nresonance imaging (LGE MRI) is a crucial step needed for planning the treatment\nof atrial fibrillation. However, automatic LA segmentation from LGE MRI is\nstill challenging, due to the poor image quality, high variability in LA\nshapes, and unclear LA boundary. Though deep learning-based methods can provide\npromising LA segmentation results, they often generalize poorly to unseen\ndomains, such as data from different scanners and/or sites. In this work, we\ncollect 210 LGE MRIs from different centers with different levels of image\nquality. To evaluate the domain generalization ability of models on the LA\nsegmentation task, we employ four commonly used semantic segmentation networks\nfor the LA segmentation from multi-center LGE MRIs. Besides, we investigate\nthree domain generalization strategies, i.e., histogram matching, mutual\ninformation based disentangled representation, and random style transfer, where\na simple histogram matching is proved to be most effective.",
          "link": "http://arxiv.org/abs/2106.08727",
          "publishedOn": "2021-07-06T01:58:07.758Z",
          "wordCount": 635,
          "title": "AtrialGeneral: Domain Generalization for Left Atrial Segmentation of Multi-Center LGE MRIs. (arXiv:2106.08727v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balayn_A/0/1/0/all/0/1\">Agathe Balayn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulynych_B/0/1/0/all/0/1\">Bogdan Kulynych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerses_S/0/1/0/all/0/1\">Seda Guerses</a>",
          "description": "Researchers have identified datasets used for training computer vision (CV)\nmodels as an important source of hazardous outcomes, and continue to examine\npopular CV datasets to expose their harms. These works tend to treat datasets\nas objects, or focus on particular steps in data production pipelines. We argue\nhere that we could further systematize our analysis of harms by examining CV\ndata pipelines through a process-oriented lens that captures the creation, the\nevolution and use of these datasets. As a step towards cultivating a\nprocess-oriented lens, we embarked on an empirical study of CV data pipelines\ninformed by the field of method engineering. We present here a preliminary\nresult: a reference model of CV data pipelines. Besides exploring the questions\nthat this endeavor raises, we discuss how the process lens could support\nresearchers in discovering understudied issues, and could help practitioners in\nmaking their processes more transparent.",
          "link": "http://arxiv.org/abs/2107.01824",
          "publishedOn": "2021-07-06T01:58:07.751Z",
          "wordCount": 599,
          "title": "Exploring Data Pipelines through the Process Lens: a Reference Model forComputer Vision. (arXiv:2107.01824v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minkyo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoonho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>",
          "description": "This paper studies probability distributions ofpenultimate activations of\nclassification networks.We show that, when a classification network istrained\nwith the cross-entropy loss, its final classi-fication layer forms\naGenerative-Discriminativepairwith a generative classifier based on a\nspecificdistribution of penultimate activations. More im-portantly, the\ndistribution is parameterized by theweights of the final fully-connected layer,\nand canbe considered as a generative model that synthe-sizes the penultimate\nactivations without feedinginput data. We empirically demonstrate that\nthisgenerative model enables stable knowledge dis-tillation in the presence of\ndomain shift, and cantransfer knowledge from a classifier to\nvariationalautoencoders and generative adversarial networksfor\nclass-conditional image generation.",
          "link": "http://arxiv.org/abs/2107.01900",
          "publishedOn": "2021-07-06T01:58:07.744Z",
          "wordCount": 538,
          "title": "On The Distribution of Penultimate Activations of Classification Networks. (arXiv:2107.01900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Youwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "Graph learning has emerged as a promising technique for multi-view clustering\nwith its ability to learn a unified and robust graph from multiple views.\nHowever, existing graph learning methods mostly focus on the multi-view\nconsistency issue, yet often neglect the inconsistency across multiple views,\nwhich makes them vulnerable to possibly low-quality or noisy datasets. To\novercome this limitation, we propose a new multi-view graph learning framework,\nwhich for the first time simultaneously and explicitly models multi-view\nconsistency and multi-view inconsistency in a unified objective function,\nthrough which the consistent and inconsistent parts of each single-view graph\nas well as the unified graph that fuses the consistent parts can be iteratively\nlearned. Though optimizing the objective function is NP-hard, we design a\nhighly efficient optimization algorithm which is able to obtain an approximate\nsolution with linear time complexity in the number of edges in the unified\ngraph. Furthermore, our multi-view graph learning approach can be applied to\nboth similarity graphs and dissimilarity graphs, which lead to two graph\nfusion-based variants in our framework. Experiments on twelve multi-view\ndatasets have demonstrated the robustness and efficiency of the proposed\napproach.",
          "link": "http://arxiv.org/abs/2008.10208",
          "publishedOn": "2021-07-06T01:58:07.737Z",
          "wordCount": 671,
          "title": "Multi-view Graph Learning by Joint Modeling of Consistency and Inconsistency. (arXiv:2008.10208v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhishan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dawei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>",
          "description": "As a critical component for online advertising and marking, click-through\nrate (CTR) prediction has draw lots of attentions from both industry and\nacademia field. Recently, the deep learning has become the mainstream\nmethodological choice for CTR. Despite of sustainable efforts have been made,\nexisting approaches still pose several challenges. On the one hand, high-order\ninteraction between the features is under-explored. On the other hand,\nhigh-order interactions may neglect the semantic information from the low-order\nfields. In this paper, we proposed a novel prediction method, named FINT, that\nemploys the Field-aware INTeraction layer which captures high-order feature\ninteractions while retaining the low-order field information. To empirically\ninvestigate the effectiveness and robustness of the FINT, we perform extensive\nexperiments on the three realistic databases: KDD2012, Criteo and Avazu. The\nobtained results demonstrate that the FINT can significantly improve the\nperformance compared to the existing methods, without increasing the amount of\ncomputation required. Moreover, the proposed method brought about 2.72\\%\nincrease to the advertising revenue of a big online video app through A/B\ntesting. To better promote the research in CTR field, we will release our code\nas well as reference implementation of those baseline models in the final\nversion.",
          "link": "http://arxiv.org/abs/2107.01999",
          "publishedOn": "2021-07-06T01:58:07.719Z",
          "wordCount": 642,
          "title": "FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Popli_A/0/1/0/all/0/1\">Additya Popli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_S/0/1/0/all/0/1\">Saraansh Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1\">Joshua J. Engelsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoe_N/0/1/0/all/0/1\">Naoyuki Onoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okubo_A/0/1/0/all/0/1\">Atsushi Okubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_A/0/1/0/all/0/1\">Anoop Namboodiri</a>",
          "description": "Typical fingerprint recognition systems are comprised of a spoof detection\nmodule and a subsequent recognition module, running one after the other. In\nthis paper, we reformulate the workings of a typical fingerprint recognition\nsystem. In particular, we posit that both spoof detection and fingerprint\nrecognition are correlated tasks. Therefore, rather than performing the two\ntasks separately, we propose a joint model for spoof detection and matching to\nsimultaneously perform both tasks without compromising the accuracy of either\ntask. We demonstrate the capability of our joint model to obtain an\nauthentication accuracy (1:1 matching) of TAR = 100% @ FAR = 0.1% on the FVC\n2006 DB2A dataset while achieving a spoof detection ACE of 1.44% on the LiveDet\n2015 dataset, both maintaining the performance of stand-alone methods. In\npractice, this reduces the time and memory requirements of the fingerprint\nrecognition system by 50% and 40%, respectively; a significant advantage for\nrecognition systems running on resource-constrained devices and communication\nchannels.",
          "link": "http://arxiv.org/abs/2104.03255",
          "publishedOn": "2021-07-06T01:58:07.703Z",
          "wordCount": 637,
          "title": "A Unified Model for Fingerprint Authentication and Presentation Attack Detection. (arXiv:2104.03255v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bi&#x27;an Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>",
          "description": "Point clouds have attracted increasing attention as a natural representation\nof 3D shapes. Significant progress has been made in developing methods for\npoint cloud analysis, which often requires costly human annotation as\nsupervision in practice. To address this issue, we propose a novel\nself-contrastive learning for self-supervised point cloud representation\nlearning, aiming to capture both local geometric patterns and nonlocal semantic\nprimitives based on the nonlocal self-similarity of point clouds. The\ncontributions are two-fold: on the one hand, instead of contrasting among\ndifferent point clouds as commonly employed in contrastive learning, we exploit\nself-similar point cloud patches within a single point cloud as positive\nsamples and otherwise negative ones to facilitate the task of contrastive\nlearning. Such self-contrastive learning is well aligned with the emerging\nparadigm of self-supervised learning for point cloud analysis. On the other\nhand, we actively learn hard negative samples that are close to positive\nsamples in the representation space for discriminative feature learning, which\nare sampled conditional on each anchor patch leveraging on the degree of\nself-similarity. Experimental results show that the proposed method achieves\nstate-of-the-art performance on widely used benchmark datasets for\nself-supervised point cloud segmentation and transfer learning for\nclassification.",
          "link": "http://arxiv.org/abs/2107.01886",
          "publishedOn": "2021-07-06T01:58:07.696Z",
          "wordCount": 644,
          "title": "Self-Contrastive Learning with Hard Negative Sampling for Self-supervised Point Cloud Learning. (arXiv:2107.01886v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeeseung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Younggeun Kim</a>",
          "description": "We propose Styleformer, which is a style-based generator for GAN\narchitecture, but a convolution-free transformer-based generator. In our paper,\nwe explain how a transformer can generate high-quality images, overcoming the\ndisadvantage that convolution operations are difficult to capture global\nfeatures in an image. Furthermore, we change the demodulation of StyleGAN2 and\nmodify the existing transformer structure (e.g., residual connection, layer\nnormalization) to create a strong style-based generator with a convolution-free\nstructure. We also make Styleformer lighter by applying Linformer, enabling\nStyleformer to generate higher resolution images and result in improvements in\nterms of speed and memory. We experiment with the low-resolution image dataset\nsuch as CIFAR-10, as well as the high-resolution image dataset like\nLSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark\ndataset, which is comparable performance to the current state-of-the-art and\noutperforms all GAN-based generative models, including StyleGAN2-ADA with fewer\nparameters on the unconditional setting. We also both achieve new\nstate-of-the-art with FID 15.17, IS 11.01, and FID 3.66, respectively on STL-10\nand CelebA. We release our code at\nhttps://github.com/Jeeseung-Park/Styleformer.",
          "link": "http://arxiv.org/abs/2106.07023",
          "publishedOn": "2021-07-06T01:58:07.688Z",
          "wordCount": 641,
          "title": "Styleformer: Transformer based Generative Adversarial Networks with Style Vector. (arXiv:2106.07023v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongbao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>",
          "description": "In real applications, new object classes often emerge after the detection\nmodel has been trained on a prepared dataset with fixed classes. Due to the\nstorage burden and the privacy of old data, sometimes it is impractical to\ntrain the model from scratch with both old and new data. Fine-tuning the old\nmodel with only new data will lead to a well-known phenomenon of catastrophic\nforgetting, which severely degrades the performance of modern object detectors.\nIn this paper, we propose a novel \\textbf{M}ulti-\\textbf{V}iew\n\\textbf{C}orrelation \\textbf{D}istillation (MVCD) based incremental object\ndetection method, which explores the correlations in the feature space of the\ntwo-stage object detector (Faster R-CNN). To better transfer the knowledge\nlearned from the old classes and maintain the ability to learn new classes, we\ndesign correlation distillation losses from channel-wise, point-wise and\ninstance-wise views to regularize the learning of the incremental model. A new\nmetric named Stability-Plasticity-mAP is proposed to better evaluate both the\nstability for old classes and the plasticity for new classes in incremental\nobject detection. The extensive experiments conducted on VOC2007 and COCO\ndemonstrate that MVCD can effectively learn to detect objects of new classes\nand mitigate the problem of catastrophic forgetting.",
          "link": "http://arxiv.org/abs/2107.01787",
          "publishedOn": "2021-07-06T01:58:07.681Z",
          "wordCount": 628,
          "title": "Multi-View Correlation Distillation for Incremental Object Detection. (arXiv:2107.01787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.03572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhaohui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Dongwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rongguang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>",
          "description": "Deep learning-based object detection and instance segmentation have achieved\nunprecedented progress. In this paper, we propose Complete-IoU (CIoU) loss and\nCluster-NMS for enhancing geometric factors in both bounding box regression and\nNon-Maximum Suppression (NMS), leading to notable gains of average precision\n(AP) and average recall (AR), without the sacrifice of inference efficiency. In\nparticular, we consider three geometric factors, i.e., overlap area, normalized\ncentral point distance and aspect ratio, which are crucial for measuring\nbounding box regression in object detection and instance segmentation. The\nthree geometric factors are then incorporated into CIoU loss for better\ndistinguishing difficult regression cases. The training of deep models using\nCIoU loss results in consistent AP and AR improvements in comparison to widely\nadopted $\\ell_n$-norm loss and IoU-based loss. Furthermore, we propose\nCluster-NMS, where NMS during inference is done by implicitly clustering\ndetected boxes and usually requires less iterations. Cluster-NMS is very\nefficient due to its pure GPU implementation, and geometric factors can be\nincorporated to improve both AP and AR. In the experiments, CIoU loss and\nCluster-NMS have been applied to state-of-the-art instance segmentation (e.g.,\nYOLACT and BlendMask-RT), and object detection (e.g., YOLO v3, SSD and Faster\nR-CNN) models. Taking YOLACT on MS COCO as an example, our method achieves\nperformance gains as +1.7 AP and +6.2 AR$_{100}$ for object detection, and +0.9\nAP and +3.5 AR$_{100}$ for instance segmentation, with 27.1 FPS on one NVIDIA\nGTX 1080Ti GPU. All the source code and trained models are available at\nhttps://github.com/Zzh-tju/CIoU",
          "link": "http://arxiv.org/abs/2005.03572",
          "publishedOn": "2021-07-06T01:58:07.655Z",
          "wordCount": 771,
          "title": "Enhancing Geometric Factors in Model Learning and Inference for Object Detection and Instance Segmentation. (arXiv:2005.03572v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica I Aviles-Rivero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellars_P/0/1/0/all/0/1\">Philip Sellars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>",
          "description": "Can one learn to diagnose COVID-19 under extreme minimal supervision? Since\nthe outbreak of the novel COVID-19 there has been a rush for developing\nArtificial Intelligence techniques for expert-level disease identification on\nChest X-ray data. In particular, the use of deep supervised learning has become\nthe go-to paradigm. However, the performance of such models is heavily\ndependent on the availability of a large and representative labelled dataset.\nThe creation of which is a heavily expensive and time consuming task, and\nespecially imposes a great challenge for a novel disease. Semi-supervised\nlearning has shown the ability to match the incredible performance of\nsupervised models whilst requiring a small fraction of the labelled examples.\nThis makes the semi-supervised paradigm an attractive option for identifying\nCOVID-19. In this work, we introduce a graph based deep semi-supervised\nframework for classifying COVID-19 from chest X-rays. Our framework introduces\nan optimisation model for graph diffusion that reinforces the natural relation\namong the tiny labelled set and the vast unlabelled data. We then connect the\ndiffusion prediction output as pseudo-labels that are used in an iterative\nscheme in a deep net. We demonstrate, through our experiments, that our model\nis able to outperform the current leading supervised model with a tiny fraction\nof the labelled examples. Finally, we provide attention maps to accommodate the\nradiologist's mental model, better fitting their perceptual and cognitive\nabilities. These visualisation aims to assist the radiologist in judging\nwhether the diagnostic is correct or not, and in consequence to accelerate the\ndecision.",
          "link": "http://arxiv.org/abs/2010.00378",
          "publishedOn": "2021-07-06T01:58:07.645Z",
          "wordCount": 777,
          "title": "GraphXCOVID: Explainable Deep Graph Diffusion Pseudo-Labelling for Identifying COVID-19 on Chest X-rays. (arXiv:2010.00378v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1810.04320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woodford_O/0/1/0/all/0/1\">Oliver J. Woodford</a>",
          "description": "Direct methods are widely used for alignment of models to images, due to\ntheir accuracy, since they minimize errors in the domain of measurement noise.\nThey have leveraged least squares minimizations, for simple, efficient,\nvariational optimization, since the seminal 1981 work of Lucas & Kanade, and\nnormalized cross correlation (NCC), for robustness to intensity variations,\nsince at least 1972. Despite the complementary benefits of these two well known\nmethods, they have not been effectively combined to address local variations in\nintensity. Many ad-hoc NCC frameworks, sub-optimal least squares methods and\nimage transformation approaches have thus been proposed instead, each with\ntheir own limitations. This work shows that a least squares optimization of NCC\nwithout approximation is not only possible, but straightforward and efficient.\nA robust, locally normalized formulation is introduced to mitigate local\nintensity variations and partial occlusions. Finally, sparse features with\noriented patches are proposed for further efficiency. The resulting framework\nis simple to implement, computationally efficient and robust to local intensity\nvariations. It is evaluated on the image alignment problem, showing\nimprovements in both convergence rate and computation time over existing\nlighting invariant methods.",
          "link": "http://arxiv.org/abs/1810.04320",
          "publishedOn": "2021-07-06T01:58:07.637Z",
          "wordCount": 650,
          "title": "Least Squares Normalized Cross Correlation. (arXiv:1810.04320v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10488",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_T/0/1/0/all/0/1\">Taeeon Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_B/0/1/0/all/0/1\">Byeongjoon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baek_J/0/1/0/all/0/1\">Jongduk Baek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We tackle a challenging blind image denoising problem, in which only single\ndistinct noisy images are available for training a denoiser, and no information\nabout noise is known, except for it being zero-mean, additive, and independent\nof the clean image. In such a setting, which often occurs in practice, it is\nnot possible to train a denoiser with the standard discriminative training or\nwith the recently developed Noise2Noise (N2N) training; the former requires the\nunderlying clean image for the given noisy image, and the latter requires two\nindependently realized noisy image pair for a clean image. To that end, we\npropose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise)\nmethod that first learns a generative model that can 1) simulate the noise in\nthe given noisy images and 2) generate a rough, noisy estimates of the clean\nimages, then 3) iteratively trains a denoiser with subsequently synthesized\nnoisy image pairs (as in N2N), obtained from the generative model. In results,\nwe show the denoiser trained with our GAN2GAN achieves an impressive denoising\nperformance on both synthetic and real-world datasets for the blind denoising\nsetting; it almost approaches the performance of the standard\ndiscriminatively-trained or N2N-trained models that have more information than\nours, and it significantly outperforms the recent baseline for the same\nsetting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one,\nBM3D. The official code of our method is available at\nhttps://github.com/csm9493/GAN2GAN.",
          "link": "http://arxiv.org/abs/1905.10488",
          "publishedOn": "2021-07-06T01:58:07.618Z",
          "wordCount": 738,
          "title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images. (arXiv:1905.10488v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Suman Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1\">Menelaos Kanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We present an approach for encoding visual task relationships to improve\nmodel performance in an Unsupervised Domain Adaptation (UDA) setting. Semantic\nsegmentation and monocular depth estimation are shown to be complementary\ntasks; in a multi-task learning setting, a proper encoding of their\nrelationships can further improve performance on both tasks. Motivated by this\nobservation, we propose a novel Cross-Task Relation Layer (CTRL), which encodes\ntask dependencies between the semantic and depth predictions. To capture the\ncross-task relationships, we propose a neural network architecture that\ncontains task-specific and cross-task refinement heads. Furthermore, we propose\nan Iterative Self-Learning (ISL) training scheme, which exploits semantic\npseudo-labels to provide extra supervision on the target domain. We\nexperimentally observe improvements in both tasks' performance because the\ncomplementary information present in these tasks is better captured.\nSpecifically, we show that: (1) our approach improves performance on all tasks\nwhen they are complementary and mutually dependent; (2) the CTRL helps to\nimprove both semantic segmentation and depth estimation tasks performance in\nthe challenging UDA setting; (3) the proposed ISL training scheme further\nimproves the semantic segmentation performance. The implementation is available\nat https://github.com/susaha/ctrl-uda.",
          "link": "http://arxiv.org/abs/2105.07830",
          "publishedOn": "2021-07-06T01:58:07.594Z",
          "wordCount": 684,
          "title": "Learning to Relate Depth and Semantics for Unsupervised Domain Adaptation. (arXiv:2105.07830v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiabei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yunjia Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>",
          "description": "This paper presents a method for gaze estimation according to face images. We\ntrain several gaze estimators adopting four different network architectures,\nincluding an architecture designed for gaze estimation (i.e.,iTracker-MHSA) and\nthree originally designed for general computer vision tasks(i.e., BoTNet,\nHRNet, ResNeSt). Then, we select the best six estimators and ensemble their\npredictions through a linear combination. The method ranks the first on the\nleader-board of ETH-XGaze Competition, achieving an average angular error of\n$3.11^{\\circ}$ on the ETH-XGaze test set.",
          "link": "http://arxiv.org/abs/2107.01980",
          "publishedOn": "2021-07-06T01:58:07.586Z",
          "wordCount": 528,
          "title": "Gaze Estimation with an Ensemble of Four Architectures. (arXiv:2107.01980v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1\">Felix Leeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanzillotta_G/0/1/0/all/0/1\">Guilia Lanzillotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annadani_Y/0/1/0/all/0/1\">Yashas Annadani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "We study the problem of self-supervised structured representation learning\nusing autoencoders for generative modeling. Unlike most methods which rely on\nmatching an arbitrary, relatively unstructured, prior distribution for\nsampling, we propose a sampling technique that relies solely on the\nindependence of latent variables, thereby avoiding the trade-off between\nreconstruction quality and generative performance inherent to VAEs. We design a\nnovel autoencoder architecture capable of learning a structured representation\nwithout the need for aggressive regularization. Our structural decoders learn a\nhierarchy of latent variables, akin to structural causal models, thereby\nordering the information without any additional regularization. We demonstrate\nhow these models learn a representation that improves results in a variety of\ndownstream tasks including generation, disentanglement, and extrapolation using\nseveral challenging and natural image datasets.",
          "link": "http://arxiv.org/abs/2006.07796",
          "publishedOn": "2021-07-06T01:58:07.579Z",
          "wordCount": 613,
          "title": "Structure by Architecture: Disentangled Representations without Regularization. (arXiv:2006.07796v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liulei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bredell_G/0/1/0/all/0/1\">Gustav Bredell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>",
          "description": "Despite recent progress of automatic medical image segmentation techniques,\nfully automatic results usually fail to meet the clinical use and typically\nrequire further refinement. In this work, we propose a quality-aware memory\nnetwork for interactive segmentation of 3D medical images. Provided by user\nguidance on an arbitrary slice, an interaction network is firstly employed to\nobtain an initial 2D segmentation. The quality-aware memory network\nsubsequently propagates the initial segmentation estimation bidirectionally\nover the entire volume. Subsequent refinement based on additional user guidance\non other slices can be incorporated in the same manner. To further facilitate\ninteractive segmentation, a quality assessment module is introduced to suggest\nthe next slice to segment based on the current segmentation quality of each\nslice. The proposed network has two appealing characteristics: 1) The\nmemory-augmented network offers the ability to quickly encode past segmentation\ninformation, which will be retrieved for the segmentation of other slices; 2)\nThe quality assessment module enables the model to directly estimate the\nqualities of segmentation predictions, which allows an active learning paradigm\nwhere users preferentially label the lowest-quality slice for multi-round\nrefinement. The proposed network leads to a robust interactive segmentation\nengine, which can generalize well to various types of user annotations (e.g.,\nscribbles, boxes). Experimental results on various medical datasets demonstrate\nthe superiority of our approach in comparison with existing techniques.",
          "link": "http://arxiv.org/abs/2106.10686",
          "publishedOn": "2021-07-06T01:58:07.570Z",
          "wordCount": 694,
          "title": "Quality-Aware Memory Network for Interactive Volumetric Image Segmentation. (arXiv:2106.10686v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1\">Haocong Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bin Hu</a>",
          "description": "Person re-identification (Re-ID) via gait features within 3D skeleton\nsequences is a newly-emerging topic with several advantages. Existing solutions\neither rely on hand-crafted descriptors or supervised gait representation\nlearning. This paper proposes a self-supervised gait encoding approach that can\nleverage unlabeled skeleton data to learn gait representations for person\nRe-ID. Specifically, we first create self-supervision by learning to\nreconstruct unlabeled skeleton sequences reversely, which involves richer\nhigh-level semantics to obtain better gait representations. Other pretext tasks\nare also explored to further improve self-supervised learning. Second, inspired\nby the fact that motion's continuity endows adjacent skeletons in one skeleton\nsequence and temporally consecutive skeleton sequences with higher correlations\n(referred as locality in 3D skeleton data), we propose a locality-aware\nattention mechanism and a locality-aware contrastive learning scheme, which aim\nto preserve locality-awareness on intra-sequence level and inter-sequence level\nrespectively during self-supervised learning. Last, with context vectors\nlearned by our locality-aware attention mechanism and contrastive learning\nscheme, a novel feature named Constrastive Attention-based Gait Encodings\n(CAGEs) is designed to represent gait effectively. Empirical evaluations show\nthat our approach significantly outperforms skeleton-based counterparts by\n15-40% Rank-1 accuracy, and it even achieves superior performance to numerous\nmulti-modal methods with extra RGB or depth information. Our codes are\navailable at https://github.com/Kali-Hac/Locality-Awareness-SGE.",
          "link": "http://arxiv.org/abs/2009.03671",
          "publishedOn": "2021-07-06T01:58:07.562Z",
          "wordCount": 742,
          "title": "A Self-Supervised Gait Encoding Approach with Locality-Awareness for 3D Skeleton Based Person Re-Identification. (arXiv:2009.03671v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zixin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>",
          "description": "How can neural networks trained by contrastive learning extract features from\nthe unlabeled data? Why does contrastive learning usually need much stronger\ndata augmentations than supervised learning to ensure good representations?\nThese questions involve both the optimization and statistical aspects of deep\nlearning, but can hardly be answered by analyzing supervised learning, where\nthe target functions are the highest pursuit. Indeed, in self-supervised\nlearning, it is inevitable to relate to the optimization/generalization of\nneural networks to how they can encode the latent structures in the data, which\nwe refer to as the feature learning process.\n\nIn this work, we formally study how contrastive learning learns the feature\nrepresentations for neural networks by analyzing its feature learning process.\nWe consider the case where our data are comprised of two types of features: the\nmore semantically aligned sparse features which we want to learn from, and the\nother dense features we want to avoid. Theoretically, we prove that contrastive\nlearning using $\\mathbf{ReLU}$ networks provably learns the desired sparse\nfeatures if proper augmentations are adopted. We present an underlying\nprinciple called $\\textbf{feature decoupling}$ to explain the effects of\naugmentations, where we theoretically characterize how augmentations can reduce\nthe correlations of dense features between positive samples while keeping the\ncorrelations of sparse features intact, thereby forcing the neural networks to\nlearn from the self-supervision of sparse features. Empirically, we verified\nthat the feature decoupling principle matches the underlying mechanism of\ncontrastive learning in practice.",
          "link": "http://arxiv.org/abs/2105.15134",
          "publishedOn": "2021-07-06T01:58:07.536Z",
          "wordCount": 725,
          "title": "Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning. (arXiv:2105.15134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bian_W/0/1/0/all/0/1\">Wenjing Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kejie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Adrian Prisacariu</a>",
          "description": "We propose Ray-ONet to reconstruct detailed 3D models from monocular images\nefficiently. By predicting a series of occupancy probabilities along a ray that\nis back-projected from a pixel in the camera coordinate, our method Ray-ONet\nimproves the reconstruction accuracy in comparison with Occupancy Networks\n(ONet), while reducing the network inference complexity to O($N^2$). As a\nresult, Ray-ONet achieves state-of-the-art performance on the ShapeNet\nbenchmark with more than 20$\\times$ speed-up at $128^3$ resolution and\nmaintains a similar memory footprint during inference.",
          "link": "http://arxiv.org/abs/2107.01899",
          "publishedOn": "2021-07-06T01:58:07.516Z",
          "wordCount": 518,
          "title": "Ray-ONet: Efficient 3D Reconstruction From A Single RGB Image. (arXiv:2107.01899v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haohang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiemin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenrui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Recent advances in self-supervised learning have experienced remarkable\nprogress, especially for contrastive learning based methods, which regard each\nimage as well as its augmentations as an individual class and try to\ndistinguish them from all other images. However, due to the large quantity of\nexemplars, this kind of pretext task intrinsically suffers from slow\nconvergence and is hard for optimization. This is especially true for small\nscale models, which we find the performance drops dramatically comparing with\nits supervised counterpart. In this paper, we propose a simple but effective\ndistillation strategy for unsupervised learning. The highlight is that the\nrelationship among similar samples counts and can be seamlessly transferred to\nthe student to boost the performance. Our method, termed as BINGO, which is\nshort for \\textbf{B}ag of \\textbf{I}nsta\\textbf{N}ces\na\\textbf{G}gregati\\textbf{O}n, targets at transferring the relationship learned\nby the teacher to the student. Here bag of instances indicates a set of similar\nsamples constructed by the teacher and are grouped within a bag, and the goal\nof distillation is to aggregate compact representations over the student with\nrespect to instances in a bag. Notably, BINGO achieves new state-of-the-art\nperformance on small scale models, \\emph{i.e.}, 65.5% and 68.9% top-1\naccuracies with linear evaluation on ImageNet, using ResNet-18 and ResNet-34 as\nbackbone, respectively, surpassing baselines (52.5% and 57.4% top-1 accuracies)\nby a significant margin. The code will be available at\n\\url{https://github.com/haohang96/bingo}.",
          "link": "http://arxiv.org/abs/2107.01691",
          "publishedOn": "2021-07-06T01:58:07.319Z",
          "wordCount": 671,
          "title": "Bag of Instances Aggregation Boosts Self-supervised Learning. (arXiv:2107.01691v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_I/0/1/0/all/0/1\">Indu Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utkarsh_A/0/1/0/all/0/1\">Ayush Utkarsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_R/0/1/0/all/0/1\">Riya Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod K Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1\">Antitza Dantcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sumantra Dutta Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalra_P/0/1/0/all/0/1\">Prem Kumar Kalra</a>",
          "description": "A fingerprint region of interest (roi) segmentation algorithm is designed to\nseparate the foreground fingerprint from the background noise. All the learning\nbased state-of-the-art fingerprint roi segmentation algorithms proposed in the\nliterature are benchmarked on scenarios when both training and testing\ndatabases consist of fingerprint images acquired from the same sensors.\nHowever, when testing is conducted on a different sensor, the segmentation\nperformance obtained is often unsatisfactory. As a result, every time a new\nfingerprint sensor is used for testing, the fingerprint roi segmentation model\nneeds to be re-trained with the fingerprint image acquired from the new sensor\nand its corresponding manually marked ROI. Manually marking fingerprint ROI is\nexpensive because firstly, it is time consuming and more importantly, requires\ndomain expertise. In order to save the human effort in generating annotations\nrequired by state-of-the-art, we propose a fingerprint roi segmentation model\nwhich aligns the features of fingerprint images derived from the unseen sensor\nsuch that they are similar to the ones obtained from the fingerprints whose\nground truth roi masks are available for training. Specifically, we propose a\nrecurrent adversarial learning based feature alignment network that helps the\nfingerprint roi segmentation model to learn sensor-invariant features.\nConsequently, sensor-invariant features learnt by the proposed roi segmentation\nmodel help it to achieve improved segmentation performance on fingerprints\nacquired from the new sensor. Experiments on publicly available FVC databases\ndemonstrate the efficacy of the proposed work.",
          "link": "http://arxiv.org/abs/2107.01361",
          "publishedOn": "2021-07-06T01:58:07.307Z",
          "wordCount": 691,
          "title": "Sensor-invariant Fingerprint ROI Segmentation Using Recurrent Adversarial Learning. (arXiv:2107.01361v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_R/0/1/0/all/0/1\">Robin Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">David Robert Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_S/0/1/0/all/0/1\">Simon Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Kazuya Takeda</a>",
          "description": "Interconnected road lanes are a central concept for navigating urban roads.\nCurrently, most autonomous vehicles rely on preconstructed lane maps as\ndesigning an algorithmic model is difficult. However, the generation and\nmaintenance of such maps is costly and hinders large-scale adoption of\nautonomous vehicle technology. This paper presents the first self-supervised\nlearning method to train a model to infer a spatially grounded lane-level road\nnetwork graph based on a dense segmented representation of the road scene\ngenerated from onboard sensors. A formal road lane network model is presented\nand proves that any structured road scene can be represented by a directed\nacyclic graph of at most depth three while retaining the notion of intersection\nregions, and that this is the most compressed representation. The formal model\nis implemented by a hybrid neural and search-based model, utilizing a novel\nbarrier function loss formulation for robust learning from partial labels.\nExperiments are conducted for all common road intersection layouts. Results\nshow that the model can generalize to new road layouts, unlike previous\napproaches, demonstrating its potential for real-world application as a\npractical learning-based lane-level map generator.",
          "link": "http://arxiv.org/abs/2107.01784",
          "publishedOn": "2021-07-06T01:58:07.297Z",
          "wordCount": 650,
          "title": "Learning a Model for Inferring a Spatial Road Lane Network Graph using Self-Supervision. (arXiv:2107.01784v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01422",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Xu_S/0/1/0/all/0/1\">Shiqi Xu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1\">Wenhui Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jonsson_J/0/1/0/all/0/1\">Joakim Jonsson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Qian_R/0/1/0/all/0/1\">Ruobing Qian</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Konda_P/0/1/0/all/0/1\">Pavan Chandra Konda</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhou_K/0/1/0/all/0/1\">Kevin C. Zhou</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Berrocal_E/0/1/0/all/0/1\">Edouard Berrocal</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Horstmeyer_R/0/1/0/all/0/1\">Roarke Horstmeyer</a>",
          "description": "Noninvasive optical imaging through dynamic scattering media has numerous\nimportant biomedical applications but still remains a challenging task. While\nstandard methods aim to form images based upon optical absorption or\nfluorescent emission, it is also well-established that the temporal correlation\nof scattered coherent light diffuses through tissue much like optical\nintensity. Few works to date, however, have aimed to experimentally measure and\nprocess such data to demonstrate deep-tissue imaging of decorrelation dynamics.\nIn this work, we take advantage of a single-photon avalanche diode (SPAD) array\ncamera, with over one thousand detectors, to simultaneously detect speckle\nfluctuations at the single-photon level from 12 different phantom tissue\nsurface locations delivered via a customized fiber bundle array. We then apply\na deep neural network to convert the acquired single-photon measurements into\nvideo of scattering dynamics beneath rapidly decorrelating liquid tissue\nphantoms. We demonstrate the ability to record video of dynamic events\noccurring 5-8 mm beneath a decorrelating tissue phantom with mm-scale\nresolution and at a 2.5-10 Hz frame rate.",
          "link": "http://arxiv.org/abs/2107.01422",
          "publishedOn": "2021-07-06T01:58:07.271Z",
          "wordCount": 631,
          "title": "Imaging dynamics beneath turbid media via parallelized single-photon detection. (arXiv:2107.01422v1 [physics.optics])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01502",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xinglong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_N/0/1/0/all/0/1\">Ning Huang</a>",
          "description": "Pulmonary vessel segmentation is important for clinical diagnosis of\npulmonary diseases, while is also challenging due to the complicated structure.\nIn this work, we present an effective framework and refinement process of\npulmonary vessel segmentation from chest computed tomographic (CT) images. The\nkey to our approach is a 2.5D segmentation network applied from three\northogonal axes, which presents a robust and fully automated pulmonary vessel\nsegmentation result with lower network complexity and memory usage compared to\n3D networks. The slice radius is introduced to convolve the adjacent\ninformation of the center slice and the multi-planar fusion optimizes the\npresentation of intra- and inter- slice features. Besides, the tree-like\nstructure of the pulmonary vessel is extracted in the post-processing process,\nwhich is used for segmentation refining and pruning. In the evaluation\nexperiments, three fusion methods are tested and the most promising one is\ncompared with the state-of-the-art 2D and 3D structures on 300 cases of lung\nimages randomly selected from LIDC dataset. Our method outperforms other\nnetwork structures by a large margin and achieves by far the highest average\nDICE score of 0.9272 and precision of 0.9310, as per our knowledge from the\npulmonary vessel segmentation models available in the literature.",
          "link": "http://arxiv.org/abs/2107.01502",
          "publishedOn": "2021-07-06T01:58:07.263Z",
          "wordCount": 677,
          "title": "Pulmonary Vessel Segmentation based on Orthogonal Fused U-Net++ of Chest CT Images. (arXiv:2107.01502v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_I/0/1/0/all/0/1\">Indu Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utkarsh_A/0/1/0/all/0/1\">Ayush Utkarsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_R/0/1/0/all/0/1\">Riya Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod K Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1\">Antitza Dantcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sumantra Dutta Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalra_P/0/1/0/all/0/1\">Prem Kumar Kalra</a>",
          "description": "The effectiveness of fingerprint-based authentication systems on good quality\nfingerprints is established long back. However, the performance of standard\nfingerprint matching systems on noisy and poor quality fingerprints is far from\nsatisfactory. Towards this, we propose a data uncertainty-based framework which\nenables the state-of-the-art fingerprint preprocessing models to quantify noise\npresent in the input image and identify fingerprint regions with background\nnoise and poor ridge clarity. Quantification of noise helps the model two\nfolds: firstly, it makes the objective function adaptive to the noise in a\nparticular input fingerprint and consequently, helps to achieve robust\nperformance on noisy and distorted fingerprint regions. Secondly, it provides a\nnoise variance map which indicates noisy pixels in the input fingerprint image.\nThe predicted noise variance map enables the end-users to understand erroneous\npredictions due to noise present in the input image. Extensive experimental\nevaluation on 13 publicly available fingerprint databases, across different\narchitectural choices and two fingerprint processing tasks demonstrate\neffectiveness of the proposed framework.",
          "link": "http://arxiv.org/abs/2107.01248",
          "publishedOn": "2021-07-06T01:58:07.247Z",
          "wordCount": 613,
          "title": "Data Uncertainty Guided Noise-aware Preprocessing Of Fingerprints. (arXiv:2107.01248v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Briq_R/0/1/0/all/0/1\">Rania Briq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochar_P/0/1/0/all/0/1\">Pratika Kochar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>",
          "description": "This paper proposes an approach that generates multiple 3D human meshes from\ntext. The human shapes are represented by 3D meshes based on the SMPL model.\nThe model's performance is evaluated on the COCO dataset, which contains\nchallenging human shapes and intricate interactions between individuals. The\nmodel is able to capture the dynamics of the scene and the interactions between\nindividuals based on text. We further show how using such a shape as input to\nimage synthesis frameworks helps to constrain the network to synthesize humans\nwith realistic human shapes.",
          "link": "http://arxiv.org/abs/2107.01869",
          "publishedOn": "2021-07-06T01:58:07.241Z",
          "wordCount": 525,
          "title": "Towards Better Adversarial Synthesis of Human Images from Text. (arXiv:2107.01869v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01456",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Trinh_Q/0/1/0/all/0/1\">Quoc Huy Trinh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Van Nguyen</a>",
          "description": "3D CT-scan base on chest is one of the controversial topisc of the researcher\nnowadays. There are many tasks to diagnose the disease through CT-scan images,\ninclude Covid19. In this paper, we propose a method that custom and combine\nDeep Neural Network to classify the series of 3D CT-scans chest images. In our\nmethods, we experiment with 2 backbones is DenseNet 121 and ResNet 101. In this\nproposal, we separate the experiment into 2 tasks, one is for 2 backbones\ncombination of ResNet and DenseNet, one is for DenseNet backbones combination.",
          "link": "http://arxiv.org/abs/2107.01456",
          "publishedOn": "2021-07-06T01:58:07.222Z",
          "wordCount": 582,
          "title": "Custom Deep Neural Network for 3D Covid Chest CT-scan Classification. (arXiv:2107.01456v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01327",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Hoang C. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_T/0/1/0/all/0/1\">Tung T. Le</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>",
          "description": "We introduce a new benchmark dataset, namely VinDr-RibCXR, for automatic\nsegmentation and labeling of individual ribs from chest X-ray (CXR) scans. The\nVinDr-RibCXR contains 245 CXRs with corresponding ground truth annotations\nprovided by human experts. A set of state-of-the-art segmentation models are\ntrained on 196 images from the VinDr-RibCXR to segment and label 20 individual\nribs. Our best performing model obtains a Dice score of 0.834 (95% CI,\n0.810--0.853) on an independent test set of 49 images. Our study, therefore,\nserves as a proof of concept and baseline performance for future research.",
          "link": "http://arxiv.org/abs/2107.01327",
          "publishedOn": "2021-07-06T01:58:07.215Z",
          "wordCount": 576,
          "title": "VinDr-RibCXR: A Benchmark Dataset for Automatic Segmentation and Labeling of Individual Ribs on Chest X-rays. (arXiv:2107.01327v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Linqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In this paper, we propose a similarity-aware fusion network (SAFNet) to\nadaptively fuse 2D images and 3D point clouds for 3D semantic segmentation.\nExisting fusion-based methods achieve remarkable performances by integrating\ninformation from multiple modalities. However, they heavily rely on the\ncorrespondence between 2D pixels and 3D points by projection and can only\nperform the information fusion in a fixed manner, and thus their performances\ncannot be easily migrated to a more realistic scenario where the collected data\noften lack strict pair-wise features for prediction. To address this, we employ\na late fusion strategy where we first learn the geometric and contextual\nsimilarities between the input and back-projected (from 2D pixels) point clouds\nand utilize them to guide the fusion of two modalities to further exploit\ncomplementary information. Specifically, we employ a geometric similarity\nmodule (GSM) to directly compare the spatial coordinate distributions of\npair-wise 3D neighborhoods, and a contextual similarity module (CSM) to\naggregate and compare spatial contextual information of corresponding central\npoints. The two proposed modules can effectively measure how much image\nfeatures can help predictions, enabling the network to adaptively adjust the\ncontributions of two modalities to the final prediction of each point.\nExperimental results on the ScanNetV2 benchmark demonstrate that SAFNet\nsignificantly outperforms existing state-of-the-art fusion-based approaches\nacross various data integrity.",
          "link": "http://arxiv.org/abs/2107.01579",
          "publishedOn": "2021-07-06T01:58:07.193Z",
          "wordCount": 663,
          "title": "Similarity-Aware Fusion Network for 3D Semantic Segmentation. (arXiv:2107.01579v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01392",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kumar_P/0/1/0/all/0/1\">Peeyush Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gangal_A/0/1/0/all/0/1\">Ayushe Gangal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumari_S/0/1/0/all/0/1\">Sunita Kumari</a>",
          "description": "Coronavirus is a large virus family consisting of diverse viruses, some of\nwhich disseminate among mammals and others cause sickness among humans.\nCOVID-19 is highly contagious and is rapidly spreading, rendering its early\ndiagnosis of preeminent status. Researchers, medical specialists and\norganizations all over the globe have been working tirelessly to combat this\nvirus and help in its containment. In this paper, a novel neural network called\nWisdomNet has been proposed, for the diagnosis of COVID-19 using chest X-rays.\nThe WisdomNet uses the concept of Wisdom of Crowds as its founding idea. It is\na two-layered convolutional Neural Network (CNN), which takes chest x-ray\nimages as input. Both layers of the proposed neural network consist of a number\nof neural networks each. The dataset used for this study consists of chest\nx-ray images of COVID-19 positive patients, compiled and shared by Dr. Cohen on\nGitHub, and the chest x-ray images of healthy lungs and lungs affected by viral\nand bacterial pneumonia were obtained from Kaggle. The network not only\npinpoints the presence of COVID-19, but also gives the probability of the\ndisease maturing into Acute Respiratory Distress Syndrome (ARDS). Thus,\npredicting the progression of the disease in the COVID-19 positive patients.\nThe network also slender the occurrences of false negative cases by employing a\nhigh threshold value, thus aids in curbing the spread of the disease and gives\nan accuracy of 100% for successfully predicting COVID-19 among the chest x-rays\nof patients affected with COVID-19, bacterial and viral pneumonia.",
          "link": "http://arxiv.org/abs/2107.01392",
          "publishedOn": "2021-07-06T01:58:07.185Z",
          "wordCount": 795,
          "title": "WisdomNet: Prognosis of COVID-19 with Slender Prospect of False Negative Cases and Vaticinating the Probability of Maturation to ARDS using Posteroanterior Chest X-Rays. (arXiv:2107.01392v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yifan Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tianjun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1\">David Wipf Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a hierarchical graph neural network (GNN) model that learns how to\ncluster a set of images into an unknown number of identities using a training\nset of images annotated with labels belonging to a disjoint set of identities.\nOur hierarchical GNN uses a novel approach to merge connected components\npredicted at each level of the hierarchy to form a new graph at the next level.\nUnlike fully unsupervised hierarchical clustering, the choice of grouping and\ncomplexity criteria stems naturally from supervision in the training set. The\nresulting method, Hi-LANDER, achieves an average of 54% improvement in F-score\nand 8% increase in Normalized Mutual Information (NMI) relative to current\nGNN-based clustering algorithms. Additionally, state-of-the-art GNN-based\nmethods rely on separate models to predict linkage probabilities and node\ndensities as intermediate steps of the clustering process. In contrast, our\nunified framework achieves a seven-fold decrease in computational cost. We\nrelease our training and inference code at\nhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.",
          "link": "http://arxiv.org/abs/2107.01319",
          "publishedOn": "2021-07-06T01:58:07.165Z",
          "wordCount": 608,
          "title": "Learning Hierarchical Graph Neural Networks for Image Clustering. (arXiv:2107.01319v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01748",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ONeil_A/0/1/0/all/0/1\">Alison O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "Acquiring annotated data at scale with rare diseases or conditions remains a\nchallenge. It would be extremely useful to have a method that controllably\nsynthesizes images that can correct such underrepresentation. Assuming a proper\nlatent representation, the idea of a \"latent vector arithmetic\" could offer the\nmeans of achieving such synthesis. A proper representation must encode the\nfidelity of the input data, preserve invariance and equivariance, and permit\narithmetic operations. Motivated by the ability to disentangle images into\nspatial anatomy (tensor) factors and accompanying imaging (vector)\nrepresentations, we propose a framework termed \"disentangled anatomy\narithmetic\", in which a generative model learns to combine anatomical factors\nof different input images such that when they are re-entangled with the desired\nimaging modality (e.g. MRI), plausible new cardiac images are created with the\ntarget characteristics. To encourage a realistic combination of anatomy factors\nafter the arithmetic step, we propose a localized noise injection network that\nprecedes the generator. Our model is used to generate realistic images,\npathology labels, and segmentation masks that are used to augment the existing\ndatasets and subsequently improve post-hoc classification and segmentation\ntasks. Code is publicly available at https://github.com/vios-s/DAA-GAN.",
          "link": "http://arxiv.org/abs/2107.01748",
          "publishedOn": "2021-07-06T01:58:07.158Z",
          "wordCount": 642,
          "title": "Controllable cardiac synthesis via disentangled anatomy arithmetic. (arXiv:2107.01748v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01318",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Toledo_M/0/1/0/all/0/1\">Marcelo Toledo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lima_D/0/1/0/all/0/1\">Daniel Lima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krieger_J/0/1/0/all/0/1\">Jos&#xe9; Krieger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gutierrez_M/0/1/0/all/0/1\">Marco Gutierrez</a>",
          "description": "CNN (Convolutional Neural Network) models have been successfully used for\nsegmentation of the left ventricle (LV) in cardiac MRI (Magnetic Resonance\nImaging), providing clinical measurements.In practice, two questions arise with\ndeployment of CNNs: 1) when is it better to use a shallow model instead of a\ndeeper one? 2) how the size of a dataset might change the network performance?\nWe propose a framework to answer them, by experimenting with deep and shallow\nversions of three U-Net families, trained from scratch in six subsets varying\nfrom 100 to 10,000 images, different network sizes, learning rates and\nregularization values. 1620 models were evaluated using 5-foldcross-validation\nby loss and DICE. The results indicate that: sample size affects performance\nmore than architecture or hyper-parameters; in small samples the performance is\nmore sensitive to hyper-parameters than architecture; the performance\ndifference between shallow and deeper networks is not the same across families.",
          "link": "http://arxiv.org/abs/2107.01318",
          "publishedOn": "2021-07-06T01:58:07.144Z",
          "wordCount": 623,
          "title": "A study of CNN capacity applied to Left Venticle Segmentation in Cardiac MRI. (arXiv:2107.01318v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pricope_T/0/1/0/all/0/1\">Tidor-Vlad Pricope</a>",
          "description": "Classifying hand-written digits and letters has taken a big leap with the\nintroduction of ConvNets. However, on very constrained hardware the time\nnecessary to train such models would be high. Our main contribution is twofold.\nFirst, we extensively test an end-to-end vanilla neural network (MLP) approach\nin pure numpy without any pre-processing or feature extraction done beforehand.\nSecond, we show that basic data mining operations can significantly improve the\nperformance of the models in terms of computational time, without sacrificing\nmuch accuracy. We illustrate our claims on a simpler variant of the Extended\nMNIST dataset, called Balanced EMNIST dataset. Our experiments show that,\nwithout any data mining, we get increased generalization performance when using\nmore hidden layers and regularization techniques, the best model achieving\n84.83% accuracy on a test dataset. Using dimensionality reduction done by PCA\nwe were able to increase that figure to 85.08% with only 10% of the original\nfeature space, reducing the memory size needed by 64%. Finally, adding methods\nto remove possibly harmful training samples like deviation from the mean helped\nus to still achieve over 84% test accuracy but with only 32.8% of the original\nmemory size for the training set. This compares favorably to the majority of\nliterature results obtained through similar architectures. Although this\napproach gets outshined by state-of-the-art models, it does scale to some\n(AlexNet, VGGNet) trained on 50% of the same dataset.",
          "link": "http://arxiv.org/abs/2107.01782",
          "publishedOn": "2021-07-06T01:58:07.103Z",
          "wordCount": 675,
          "title": "A contextual analysis of multi-layer perceptron models in classifying hand-written digits and letters: limited resources. (arXiv:2107.01782v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Naftali Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_S/0/1/0/all/0/1\">Srijan Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balch_T/0/1/0/all/0/1\">Tucker Balch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1\">Manuela Veloso</a>",
          "description": "In this work, we address time-series forecasting as a computer vision task.\nWe capture input data as an image and train a model to produce the subsequent\nimage. This approach results in predicting distributions as opposed to\npointwise values. To assess the robustness and quality of our approach, we\nexamine various datasets and multiple evaluation metrics. Our experiments show\nthat our forecasting tool is effective for cyclic data but somewhat less for\nirregular data such as stock prices. Importantly, when using image-based\nevaluation metrics, we find our method to outperform various baselines,\nincluding ARIMA, and a numerical variation of our deep learning approach.",
          "link": "http://arxiv.org/abs/2107.01273",
          "publishedOn": "2021-07-06T01:58:07.095Z",
          "wordCount": 554,
          "title": "Visual Time Series Forecasting: An Image-driven Approach. (arXiv:2107.01273v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_F/0/1/0/all/0/1\">Fatemeh Mahdavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajabi_R/0/1/0/all/0/1\">Roozbeh Rajabi</a>",
          "description": "In image processing, it is essential to detect and track air targets,\nespecially UAVs. In this paper, we detect the flying drone using a fisheye\ncamera. In the field of diagnosis and classification of objects, there are\nalways many problems that prevent the development of rapid and significant\nprogress in this area. During the previous decades, a couple of advanced\nclassification methods such as convolutional neural networks and support vector\nmachines have been developed. In this study, the drone was detected using three\nmethods of classification of convolutional neural network (CNN), support vector\nmachine (SVM), and nearest neighbor. The outcomes show that CNN, SVM, and\nnearest neighbor have total accuracy of 95%, 88%, and 80%, respectively.\nCompared with other classifiers with the same experimental conditions, the\naccuracy of the convolutional neural network classifier is satisfactory.",
          "link": "http://arxiv.org/abs/2107.01435",
          "publishedOn": "2021-07-06T01:58:07.085Z",
          "wordCount": 567,
          "title": "Drone Detection Using Convolutional Neural Networks. (arXiv:2107.01435v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>",
          "description": "Visual Commonsense Reasoning (VCR) predicts an answer with corresponding\nrationale, given a question-image input. VCR is a recently introduced visual\nscene understanding task with a wide range of applications, including visual\nquestion answering, automated vehicle systems, and clinical decision support.\nPrevious approaches to solving the VCR task generally rely on pre-training or\nexploiting memory with long dependency relationship encoded models. However,\nthese approaches suffer from a lack of generalizability and prior knowledge. In\nthis paper we propose a dynamic working memory based cognitive VCR network,\nwhich stores accumulated commonsense between sentences to provide prior\nknowledge for inference. Extensive experiments show that the proposed model\nyields significant improvements over existing methods on the benchmark VCR\ndataset. Moreover, the proposed model provides intuitive interpretation into\nvisual commonsense reasoning. A Python implementation of our mechanism is\npublicly available at https://github.com/tanjatang/DMVCR",
          "link": "http://arxiv.org/abs/2107.01671",
          "publishedOn": "2021-07-06T01:58:07.075Z",
          "wordCount": 568,
          "title": "Cognitive Visual Commonsense Reasoning Using Dynamic Working Memory. (arXiv:2107.01671v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>",
          "description": "Single-pixel imaging is a novel imaging scheme that has gained popularity due\nto its huge computational gain and potential for a low-cost alternative to\nimaging beyond the visible spectrum. The traditional reconstruction methods\nstruggle to produce a clear recovery when one limits the number of illumination\npatterns from a spatial light modulator. As a remedy, several\ndeep-learning-based solutions have been proposed which lack good generalization\nability due to the architectural setup and loss functions. In this paper, we\npropose a generative adversarial network-based reconstruction framework for\nsingle-pixel imaging, referred to as SPI-GAN. Our method can reconstruct images\nwith 17.92 dB PSNR and 0.487 SSIM, even if the sampling ratio drops to 5%. This\nfacilitates much faster reconstruction making our method suitable for\nsingle-pixel video. Furthermore, our ResNet-like architecture for the generator\nleads to useful representation learning that allows us to reconstruct\ncompletely unseen objects. The experimental results demonstrate that SPI-GAN\nachieves significant performance gain, e.g. near 3dB PSNR gain, over the\ncurrent state-of-the-art method.",
          "link": "http://arxiv.org/abs/2107.01330",
          "publishedOn": "2021-07-06T01:58:07.024Z",
          "wordCount": 611,
          "title": "SPI-GAN: Towards Single-Pixel Imaging through Generative Adversarial Network. (arXiv:2107.01330v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jareno_S/0/1/0/all/0/1\">Santos J. N&#xfa;&#xf1;ez Jare&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helden_D/0/1/0/all/0/1\">Dani&#xeb;l P. van Helden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirkes_E/0/1/0/all/0/1\">Evgeny M. Mirkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1\">Ivan Y. Tyukin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allison_P/0/1/0/all/0/1\">Penelope M. Allison</a>",
          "description": "In this article we consider a version of the challenging problem of learning\nfrom datasets whose size is too limited to allow generalisation beyond the\ntraining set. To address the challenge we propose to use a transfer learning\napproach whereby the model is first trained on a synthetic dataset replicating\nfeatures of the original objects. In this study the objects were smartphone\nphotographs of near-complete Roman terra sigillata pottery vessels from the\ncollection of the Museum of London. Taking the replicated features from\npublished profile drawings of pottery forms allowed the integration of expert\nknowledge into the process through our synthetic data generator. After this\nfirst initial training the model was fine-tuned with data from photographs of\nreal vessels. We show, through exhaustive experiments across several popular\ndeep learning architectures, different test priors, and considering the impact\nof the photograph viewpoint and excessive damage to the vessels, that the\nproposed hybrid approach enables the creation of classifiers with appropriate\ngeneralisation performance. This performance is significantly better than that\nof classifiers trained exclusively on the original data which shows the promise\nof the approach to alleviate the fundamental issue of learning from small\ndatasets.",
          "link": "http://arxiv.org/abs/2107.01401",
          "publishedOn": "2021-07-06T01:58:07.017Z",
          "wordCount": 652,
          "title": "Learning from scarce information: using synthetic data to classify Roman fine ware pottery. (arXiv:2107.01401v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01527",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Enshaei_N/0/1/0/all/0/1\">Nastaran Enshaei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oikonomou_A/0/1/0/all/0/1\">Anastasia Oikonomou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rafiee_M/0/1/0/all/0/1\">Moezedin Javad Rafiee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Afshar_P/0/1/0/all/0/1\">Parnian Afshar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heidarian_S/0/1/0/all/0/1\">Shahin Heidarian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naderkhani_F/0/1/0/all/0/1\">Farnoosh Naderkhani</a>",
          "description": "Novel Coronavirus disease (COVID-19) is a highly contagious respiratory\ninfection that has had devastating effects on the world. Recently, new COVID-19\nvariants are emerging making the situation more challenging and threatening.\nEvaluation and quantification of COVID-19 lung abnormalities based on chest\nComputed Tomography (CT) scans can help determining the disease stage,\nefficiently allocating limited healthcare resources, and making informed\ntreatment decisions. During pandemic era, however, visual assessment and\nquantification of COVID-19 lung lesions by expert radiologists become expensive\nand prone to error, which raises an urgent quest to develop practical\nautonomous solutions. In this context, first, the paper introduces an open\naccess COVID-19 CT segmentation dataset containing 433 CT images from 82\npatients that have been annotated by an expert radiologist. Second, a Deep\nNeural Network (DNN)-based framework is proposed, referred to as the\nCOVID-Rate, that autonomously segments lung abnormalities associated with\nCOVID-19 from chest CT scans. Performance of the proposed COVID-Rate framework\nis evaluated through several experiments based on the introduced and external\ndatasets. The results show a dice score of 0:802 and specificity and\nsensitivity of 0:997 and 0:832, respectively. Furthermore, the results indicate\nthat the COVID-Rate model can efficiently segment COVID-19 lesions in both 2D\nCT images and whole lung volumes. Results on the external dataset illustrate\ngeneralization capabilities of the COVID-Rate model to CT images obtained from\na different scanner.",
          "link": "http://arxiv.org/abs/2107.01527",
          "publishedOn": "2021-07-06T01:58:06.983Z",
          "wordCount": 733,
          "title": "COVID-Rate: An Automated Framework for Segmentation of COVID-19 Lesions from Chest CT Scans. (arXiv:2107.01527v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1\">Haocong Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bin Hu</a>",
          "description": "Person re-identification via 3D skeletons is an emerging topic with great\npotential in security-critical applications. Existing methods typically learn\nbody and motion features from the body-joint trajectory, whereas they lack a\nsystematic way to model body structure and underlying relations of body\ncomponents beyond the scale of body joints. In this paper, we for the first\ntime propose a Self-supervised Multi-scale Skeleton Graph Encoding (SM-SGE)\nframework that comprehensively models human body, component relations, and\nskeleton dynamics from unlabeled skeleton graphs of various scales to learn an\neffective skeleton representation for person Re-ID. Specifically, we first\ndevise multi-scale skeleton graphs with coarse-to-fine human body partitions,\nwhich enables us to model body structure and skeleton dynamics at multiple\nlevels. Second, to mine inherent correlations between body components in\nskeletal motion, we propose a multi-scale graph relation network to learn\nstructural relations between adjacent body-component nodes and collaborative\nrelations among nodes of different scales, so as to capture more discriminative\nskeleton graph features. Last, we propose a novel multi-scale skeleton\nreconstruction mechanism to enable our framework to encode skeleton dynamics\nand high-level semantics from unlabeled skeleton graphs, which encourages\nlearning a discriminative skeleton representation for person Re-ID. Extensive\nexperiments show that SM-SGE outperforms most state-of-the-art skeleton-based\nmethods. We further demonstrate its effectiveness on 3D skeleton data estimated\nfrom large-scale RGB videos. Our codes are open at\nhttps://github.com/Kali-Hac/SM-SGE.",
          "link": "http://arxiv.org/abs/2107.01903",
          "publishedOn": "2021-07-06T01:58:06.957Z",
          "wordCount": 691,
          "title": "SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification. (arXiv:2107.01903v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keren Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qijun Zhao</a>",
          "description": "RGB-D salient object detection (SOD) recently has attracted increasing\nresearch interest by benefiting conventional RGB SOD with extra depth\ninformation. However, existing RGB-D SOD models often fail to perform well in\nterms of both efficiency and accuracy, which hinders their potential\napplications on mobile devices and real-world problems. An underlying challenge\nis that the model accuracy usually degrades when the model is simplified to\nhave few parameters. To tackle this dilemma and also inspired by the fact that\ndepth quality is a key factor influencing the accuracy, we propose a novel\ndepth quality-inspired feature manipulation (DQFM) process, which is efficient\nitself and can serve as a gating mechanism for filtering depth features to\ngreatly boost the accuracy. DQFM resorts to the alignment of low-level RGB and\ndepth features, as well as holistic attention of the depth stream to explicitly\ncontrol and enhance cross-modal fusion. We embed DQFM to obtain an efficient\nlight-weight model called DFM-Net, where we also design a tailored depth\nbackbone and a two-stage decoder for further efficiency consideration.\nExtensive experimental results demonstrate that our DFM-Net achieves\nstate-of-the-art accuracy when comparing to existing non-efficient models, and\nmeanwhile runs at 140ms on CPU (2.2$\\times$ faster than the prior fastest\nefficient model) with only $\\sim$8.5Mb model size (14.9% of the prior\nlightest). Our code will be made publicly available.",
          "link": "http://arxiv.org/abs/2107.01779",
          "publishedOn": "2021-07-06T01:58:06.949Z",
          "wordCount": 660,
          "title": "Depth Quality-Inspired Feature Manipulation for Efficient RGB-D Salient Object Detection. (arXiv:2107.01779v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Stephen Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Saurajit Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phadke_U/0/1/0/all/0/1\">Unmesh Phadke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tingting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junwon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yada_R/0/1/0/all/0/1\">Ravi Theja Yada</a>",
          "description": "In this paper, we present Generic Object Detection (GenOD), one of the\nlargest object detection systems deployed to a web-scale general visual search\nengine that can detect over 900 categories for all Microsoft Bing Visual Search\nqueries in near real-time. It acts as a fundamental visual query understanding\nservice that provides object-centric information and shows gains in multiple\nproduction scenarios, improving upon domain-specific models. We discuss the\nchallenges of collecting data, training, deploying and updating such a\nlarge-scale object detection model with multiple dependencies. We discuss a\ndata collection pipeline that reduces per-bounding box labeling cost by 81.5%\nand latency by 61.2% while improving on annotation quality. We show that GenOD\ncan improve weighted average precision by over 20% compared to multiple\ndomain-specific models. We also improve the model update agility by nearly 2\ntimes with the proposed disjoint detector training compared to joint\nfine-tuning. Finally we demonstrate how GenOD benefits visual search\napplications by significantly improving object-level search relevance by 54.9%\nand user engagement by 59.9%.",
          "link": "http://arxiv.org/abs/2107.01814",
          "publishedOn": "2021-07-06T01:58:06.905Z",
          "wordCount": 630,
          "title": "Web-Scale Generic Object Detection at Microsoft Bing. (arXiv:2107.01814v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01682",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1\">Xiaohong Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1\">Yu Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_A/0/1/0/all/0/1\">Alice Gao</a>",
          "description": "This paper is responding to the MIA-COV19 challenge to classify COVID from\nnon-COVID based on CT lung images. The COVID-19 virus has devastated the world\nin the last eighteen months by infecting more than 182 million people and\ncausing over 3.9 million deaths. The overarching aim is to predict the\ndiagnosis of the COVID-19 virus from chest radiographs, through the development\nof explainable vision transformer deep learning techniques, leading to\npopulation screening in a more rapid, accurate and transparent way. In this\ncompetition, there are 5381 three-dimensional (3D) datasets in total, including\n1552 for training, 374 for evaluation and 3455 for testing. While most of the\ndata volumes are in axial view, there are a number of subjects' data are in\ncoronal or sagittal views with 1 or 2 slices are in axial view. Hence, while 3D\ndata based classification is investigated, in this competition, 2D images\nremains the main focus. Two deep learning methods are studied, which are vision\ntransformer (ViT) based on attention models and DenseNet that is built upon\nconventional convolutional neural network (CNN). Initial evaluation results\nbased on validation datasets whereby the ground truth is known indicate that\nViT performs better than DenseNet with F1 scores being 0.76 and 0.72\nrespectively. Codes are available at GitHub at\n<https://github/xiaohong1/COVID-ViT>.",
          "link": "http://arxiv.org/abs/2107.01682",
          "publishedOn": "2021-07-06T01:58:06.878Z",
          "wordCount": 709,
          "title": "COVID-VIT: Classification of COVID-19 from CT chest images based on vision transformer models. (arXiv:2107.01682v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yajie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangbo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shengang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yu-an Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanxin Zhang</a>",
          "description": "Deep neural networks (DNNs) have been found to be vulnerable to adversarial\nexamples. Adversarial examples are malicious images with visually imperceptible\nperturbations. While these carefully crafted perturbations restricted with\ntight $\\Lp$ norm bounds are small, they are still easily perceivable by humans.\nThese perturbations also have limited success rates when attacking black-box\nmodels or models with defenses like noise reduction filters. To solve these\nproblems, we propose Demiguise Attack, crafting ``unrestricted'' perturbations\nwith Perceptual Similarity. Specifically, we can create powerful and\nphotorealistic adversarial examples by manipulating semantic information based\non Perceptual Similarity. Adversarial examples we generate are friendly to the\nhuman visual system (HVS), although the perturbations are of large magnitudes.\nWe extend widely-used attacks with our approach, enhancing adversarial\neffectiveness impressively while contributing to imperceptibility. Extensive\nexperiments show that the proposed method not only outperforms various\nstate-of-the-art attacks in terms of fooling rate, transferability, and\nrobustness against defenses but can also improve attacks effectively. In\naddition, we also notice that our implementation can simulate illumination and\ncontrast changes that occur in real-world scenarios, which will contribute to\nexposing the blind spots of DNNs.",
          "link": "http://arxiv.org/abs/2107.01396",
          "publishedOn": "2021-07-06T01:58:06.849Z",
          "wordCount": 632,
          "title": "Demiguise Attack: Crafting Invisible Semantic Adversarial Perturbations with Perceptual Similarity. (arXiv:2107.01396v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niloy_F/0/1/0/all/0/1\">Fahim Faisal Niloy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arif/0/1/0/all/0/1\">Arif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayem_A/0/1/0/all/0/1\">Abu Bakar Siddik Nayem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_A/0/1/0/all/0/1\">Anis Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_O/0/1/0/all/0/1\">Ovi Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">M. Ashraful Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Amin Ahsan Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaber_M/0/1/0/all/0/1\">Moinul Islam Zaber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">AKM Mahbubur Rahman</a>",
          "description": "The advancement of deep learning technology has enabled us to develop systems\nthat outperform any other classification technique. However, success of any\nempirical system depends on the quality and diversity of the data available to\ntrain the proposed system. In this research, we have carefully accumulated a\nrelatively challenging dataset that contains images collected from various\nsources for three different disasters: fire, water and land. Besides this, we\nhave also collected images for various damaged infrastructure due to natural or\nman made calamities and damaged human due to war or accidents. We have also\naccumulated image data for a class named non-damage that contains images with\nno such disaster or sign of damage in them. There are 13,720 manually annotated\nimages in this dataset, each image is annotated by three individuals. We are\nalso providing discriminating image class information annotated manually with\nbounding box for a set of 200 test images. Images are collected from different\nnews portals, social media, and standard datasets made available by other\nresearchers. A three layer attention model (TLAM) is trained and average five\nfold validation accuracy of 95.88% is achieved. Moreover, on the 200 unseen\ntest images this accuracy is 96.48%. We also generate and compare attention\nmaps for these test images to determine the characteristics of the trained\nattention model. Our dataset is available at\nhttps://niloy193.github.io/Disaster-Dataset",
          "link": "http://arxiv.org/abs/2107.01284",
          "publishedOn": "2021-07-06T01:58:06.839Z",
          "wordCount": 688,
          "title": "A Novel Disaster Image Dataset and Characteristics Analysis using Attention Model. (arXiv:2107.01284v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xiaopeng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yunfeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yihong Gong</a>",
          "description": "Traditional crowd counting approaches usually use Gaussian assumption to\ngenerate pseudo density ground truth, which suffers from problems like\ninaccurate estimation of the Gaussian kernel sizes. In this paper, we propose a\nnew measure-based counting approach to regress the predicted density maps to\nthe scattered point-annotated ground truth directly. First, crowd counting is\nformulated as a measure matching problem. Second, we derive a semi-balanced\nform of Sinkhorn divergence, based on which a Sinkhorn counting loss is\ndesigned for measure matching. Third, we propose a self-supervised mechanism by\ndevising a Sinkhorn scale consistency loss to resist scale changes. Finally, an\nefficient optimization method is provided to minimize the overall loss\nfunction. Extensive experiments on four challenging crowd counting datasets\nnamely ShanghaiTech, UCF-QNRF, JHU++, and NWPU have validated the proposed\nmethod.",
          "link": "http://arxiv.org/abs/2107.01558",
          "publishedOn": "2021-07-06T01:58:06.829Z",
          "wordCount": 577,
          "title": "Direct Measure Matching for Crowd Counting. (arXiv:2107.01558v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01337",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Selim_M/0/1/0/all/0/1\">Md Selim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fei_B/0/1/0/all/0/1\">Baowei Fei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guo-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jin Chen</a>",
          "description": "While remarkable advances have been made in Computed Tomography (CT),\ncapturing CT images with non-standardized protocols causes low reproducibility\nregarding radiomic features, forming a barrier on CT image analysis in a large\nscale. RadiomicGAN is developed to effectively mitigate the discrepancy caused\nby using non-standard reconstruction kernels. RadiomicGAN consists of hybrid\nneural blocks including both pre-trained and trainable layers adopted to learn\nradiomic feature distributions efficiently. A novel training approach, called\nDynamic Window-based Training, has been developed to smoothly transform the\npre-trained model to the medical imaging domain. Model performance evaluated\nusing 1401 radiomic features show that RadiomicGAN clearly outperforms the\nstate-of-art image standardization models.",
          "link": "http://arxiv.org/abs/2107.01337",
          "publishedOn": "2021-07-06T01:58:06.817Z",
          "wordCount": 553,
          "title": "CT Image Harmonization for Enhancing Radiomics Studies. (arXiv:2107.01337v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1\">Mingbo Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuiwang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feiyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qijun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>",
          "description": "With the increasing demand for search and rescue, it is highly demanded to\ndetect objects of interest in large-scale images captured by Unmanned Aerial\nVehicles (UAVs), which is quite challenging due to extremely small scales of\nobjects. Most existing methods employed Feature Pyramid Network (FPN) to enrich\nshallow layers' features by combing deep layers' contextual features. However,\nunder the limitation of the inconsistency in gradient computation across\ndifferent layers, the shallow layers in FPN are not fully exploited to detect\ntiny objects. In this paper, we propose a Scale Selection Pyramid network\n(SSPNet) for tiny person detection, which consists of three components: Context\nAttention Module (CAM), Scale Enhancement Module (SEM), and Scale Selection\nModule (SSM). CAM takes account of context information to produce hierarchical\nattention heatmaps. SEM highlights features of specific scales at different\nlayers, leading the detector to focus on objects of specific scales instead of\nvast backgrounds. SSM exploits adjacent layers' relationships to fulfill\nsuitable feature sharing between deep layers and shallow layers, thereby\navoiding the inconsistency in gradient computation across different layers.\nBesides, we propose a Weighted Negative Sampling (WNS) strategy to guide the\ndetector to select more representative samples. Experiments on the TinyPerson\nbenchmark show that our method outperforms other state-of-the-art (SOTA)\ndetectors.",
          "link": "http://arxiv.org/abs/2107.01548",
          "publishedOn": "2021-07-06T01:58:06.781Z",
          "wordCount": 654,
          "title": "SSPNet: Scale Selection Pyramid Network for Tiny Person Detection from UAV Images. (arXiv:2107.01548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiangyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincentelli_A/0/1/0/all/0/1\">Alberto Sangiovanni Vincentelli</a>",
          "description": "Object detection is essential to safe autonomous or assisted driving.\nPrevious works usually utilize RGB images or LiDAR point clouds to identify and\nlocalize multiple objects in self-driving. However, cameras tend to fail in bad\ndriving conditions, e.g. bad weather or weak lighting, while LiDAR scanners are\ntoo expensive to get widely deployed in commercial applications. Radar has been\ndrawing more and more attention due to its robustness and low cost. In this\npaper, we propose a scene-aware radar learning framework for accurate and\nrobust object detection. First, the learning framework contains branches\nconditioning on the scene category of the radar sequence; with each branch\noptimized for a specific type of scene. Second, three different 3D\nautoencoder-based architectures are proposed for radar object detection and\nensemble learning is performed over the different architectures to further\nboost the final performance. Third, we propose novel scene-aware sequence mix\naugmentation (SceneMix) and scene-specific post-processing to generate more\nrobust detection results. In the ROD2021 Challenge, we achieved a final result\nof average precision of 75.0% and an average recall of 81.0%. Moreover, in the\nparking lot scene, our framework ranks first with an average precision of 97.8%\nand an average recall of 98.6%, which demonstrates the effectiveness of our\nframework.",
          "link": "http://arxiv.org/abs/2107.01469",
          "publishedOn": "2021-07-06T01:58:06.752Z",
          "wordCount": 641,
          "title": "Scene-aware Learning Network for Radar Object Detection. (arXiv:2107.01469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eungyeup Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeonghoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Somi Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1\">Choonghyun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Deep image colorization networks often suffer from the color-bleeding\nartifact, a problematic color spreading near the boundaries between adjacent\nobjects. The color-bleeding artifacts debase the reality of generated outputs,\nlimiting the applicability of colorization models on a practical application.\nAlthough previous approaches have tackled this problem in an automatic manner,\nthey often generate imperfect outputs because their enhancements are available\nonly in limited cases, such as having a high contrast of gray-scale value in an\ninput image. Instead, leveraging user interactions would be a promising\napproach, since it can help the edge correction in the desired regions. In this\npaper, we propose a novel edge-enhancing framework for the regions of interest,\nby utilizing user scribbles that indicate where to enhance. Our method requires\nminimal user effort to obtain satisfactory enhancements. Experimental results\non various datasets demonstrate that our interactive approach has outstanding\nperformance in improving color-bleeding artifacts against the existing\nbaselines.",
          "link": "http://arxiv.org/abs/2107.01619",
          "publishedOn": "2021-07-06T01:58:06.735Z",
          "wordCount": 588,
          "title": "Deep Edge-Aware Interactive Colorization against Color-Bleeding Effects. (arXiv:2107.01619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01351",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>",
          "description": "The precise detection of blood vessels in retinal images is crucial to the\nearly diagnosis of the retinal vascular diseases, e.g., diabetic, hypertensive\nand solar retinopathies. Existing works often fail in predicting the abnormal\nareas, e.g, sudden brighter and darker areas and are inclined to predict a\npixel to background due to the significant class imbalance, leading to high\naccuracy and specificity while low sensitivity. To that end, we propose a novel\nerror attention refining network (ERA-Net) that is capable of learning and\npredicting the potential false predictions in a two-stage manner for effective\nretinal vessel segmentation. The proposed ERA-Net in the refine stage drives\nthe model to focus on and refine the segmentation errors produced in the\ninitial training stage. To achieve this, unlike most previous attention\napproaches that run in an unsupervised manner, we introduce a novel error\nattention mechanism which considers the differences between the ground truth\nand the initial segmentation masks as the ground truth to supervise the\nattention map learning. Experimental results demonstrate that our method\nachieves state-of-the-art performance on two common retinal blood vessel\ndatasets.",
          "link": "http://arxiv.org/abs/2107.01351",
          "publishedOn": "2021-07-06T01:58:06.346Z",
          "wordCount": 631,
          "title": "EAR-NET: Error Attention Refining Network For Retinal Vessel Segmentation. (arXiv:2107.01351v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yanwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_H/0/1/0/all/0/1\">Haixu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fazheng Wang</a>",
          "description": "Offline Chinese handwriting text recognition is a long-standing research\ntopic in the field of pattern recognition. In previous studies, text detection\nand recognition are separated, which leads to the fact that text recognition is\nhighly dependent on the detection results. In this paper, we propose a robust\nend-to-end Chinese text page spotter framework. It unifies text detection and\ntext recognition with text kernel that integrates global text feature\ninformation to optimize the recognition from multiple scales, which reduces the\ndependence of detection and improves the robustness of the system. Our method\nachieves state-of-the-art results on the CASIA-HWDB2.0-2.2 dataset and\nICDAR-2013 competition dataset. Without any language model, the correct rates\nare 99.12% and 94.27% for line-level recognition, and 99.03% and 94.20% for\npage-level recognition, respectively.",
          "link": "http://arxiv.org/abs/2107.01547",
          "publishedOn": "2021-07-06T01:58:06.338Z",
          "wordCount": 567,
          "title": "Robust End-to-End Offline Chinese Handwriting Text Page Spotter with Text Kernel. (arXiv:2107.01547v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagar_S/0/1/0/all/0/1\">Sandeep Nagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufraisse_M/0/1/0/all/0/1\">Marius Dufraisse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_G/0/1/0/all/0/1\">Girish Varma</a>",
          "description": "Normalizing flows are an essential alternative to GANs for generative\nmodelling, which can be optimized directly on the maximum likelihood of the\ndataset. They also allow computation of the exact latent vector corresponding\nto an image since they are composed of invertible transformations. However, the\nrequirement of invertibility of the transformation prevents standard and\nexpressive neural network models such as CNNs from being directly used.\nEmergent convolutions were proposed to construct an invertible 3$\\times$3 CNN\nlayer using a pair of masked CNN layers, making them inefficient. We study\nconditions such that 3$\\times$3 CNNs are invertible, allowing them to construct\nexpressive normalizing flows. We derive necessary and sufficient conditions on\na padded CNN for it to be invertible. Our conditions for invertibility are\nsimple, can easily be maintained during the training process. Since we require\nonly a single CNN layer for every effective invertible CNN layer, our approach\nis more efficient than emerging convolutions. We also proposed a coupling\nmethod, Quad-coupling. We benchmark our approach and show similar performance\nresults to emergent convolutions while improving the model's efficiency.",
          "link": "http://arxiv.org/abs/2107.01358",
          "publishedOn": "2021-07-06T01:58:06.267Z",
          "wordCount": 623,
          "title": "CInC Flow: Characterizable Invertible 3x3 Convolution. (arXiv:2107.01358v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Yeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Wan Choi</a>",
          "description": "Continual learning has been a major problem in the deep learning community,\nwhere the main challenge is how to effectively learn a series of newly arriving\ntasks without forgetting the knowledge of previous tasks. Initiated by Learning\nwithout Forgetting (LwF), many of the existing works report that knowledge\ndistillation is effective to preserve the previous knowledge, and hence they\ncommonly use a soft label for the old task, namely a knowledge distillation\n(KD) loss, together with a class label for the new task, namely a cross entropy\n(CE) loss, to form a composite loss for a single neural network. However, this\napproach suffers from learning the knowledge by a CE loss as a KD loss often\nmore strongly influences the objective function when they are in a competitive\nsituation within a single network. This could be a critical problem\nparticularly in a class incremental scenario, where the knowledge across tasks\nas well as within the new task, both of which can only be acquired by a CE\nloss, is essentially learned due to the existence of a unified classifier. In\nthis paper, we propose a novel continual learning method, called\nSplit-and-Bridge, which can successfully address the above problem by partially\nsplitting a neural network into two partitions for training the new task\nseparated from the old task and re-connecting them for learning the knowledge\nacross tasks. In our thorough experimental analysis, our Split-and-Bridge\nmethod outperforms the state-of-the-art competitors in KD-based continual\nlearning.",
          "link": "http://arxiv.org/abs/2107.01349",
          "publishedOn": "2021-07-06T01:58:06.257Z",
          "wordCount": 704,
          "title": "Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network. (arXiv:2107.01349v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1\">Ding Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "This paper studies the model compression problem of vision transformers.\nBenefit from the self-attention module, transformer architectures have shown\nextraordinary performance on many computer vision tasks. Although the network\nperformance is boosted, transformers are often required more computational\nresources including memory usage and the inference complexity. Compared with\nthe existing knowledge distillation approaches, we propose to excavate useful\ninformation from the teacher transformer through the relationship between\nimages and the divided patches. We then explore an efficient fine-grained\nmanifold distillation approach that simultaneously calculates cross-images,\ncross-patch, and random-selected manifolds in teacher and student models.\nExperimental results conducted on several benchmarks demonstrate the\nsuperiority of the proposed algorithm for distilling portable transformer\nmodels with higher performance. For example, our approach achieves 75.06% Top-1\naccuracy on the ImageNet-1k dataset for training a DeiT-Tiny model, which\noutperforms other ViT distillation methods.",
          "link": "http://arxiv.org/abs/2107.01378",
          "publishedOn": "2021-07-06T01:58:06.166Z",
          "wordCount": 578,
          "title": "Efficient Vision Transformers via Fine-Grained Manifold Distillation. (arXiv:2107.01378v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiulong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>",
          "description": "Joint Energy-based Model (JEM) of Grathwohl et al. shows that a standard\nsoftmax classifier can be reinterpreted as an energy-based model (EBM) for the\njoint distribution p(x,y); the resulting model can be optimized to improve\ncalibration, robustness, and out-of-distribution detection, while generating\nsamples rivaling the quality of recent GAN-based approaches. However, the\nsoftmax classifier that JEM exploits is inherently discriminative and its\nlatent feature space is not well formulated as probabilistic distributions,\nwhich may hinder its potential for image generation and incur training\ninstability. We hypothesize that generative classifiers, such as Linear\nDiscriminant Analysis (LDA), might be more suitable for image generation since\ngenerative classifiers model the data generation process explicitly. This paper\ntherefore investigates an LDA classifier for image classification and\ngeneration. In particular, the Max-Mahalanobis Classifier (MMC), a special case\nof LDA, fits our goal very well. We show that our Generative MMC (GMMC) can be\ntrained discriminatively, generatively, or jointly for image classification and\ngeneration. Extensive experiments on multiple datasets show that GMMC achieves\nstate-of-the-art discriminative and generative performances, while\noutperforming JEM in calibration, adversarial robustness, and\nout-of-distribution detection by a significant margin. Our source code is\navailable at https://github.com/sndnyang/GMMC.",
          "link": "http://arxiv.org/abs/2101.00122",
          "publishedOn": "2021-07-05T01:54:58.207Z",
          "wordCount": 692,
          "title": "Generative Max-Mahalanobis Classifiers for Image Classification, Generation and More. (arXiv:2101.00122v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jerrick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkawhich_N/0/1/0/all/0/1\">Nathan Inkawhich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nina_O/0/1/0/all/0/1\">Oliver Nina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yuru Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songzheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mengru Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Huanqia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengxue Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_S/0/1/0/all/0/1\">Sol Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_C/0/1/0/all/0/1\">Casian Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasarica_A/0/1/0/all/0/1\">Alexandru Pasarica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hung-Min Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiarui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chia-Ying Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_M/0/1/0/all/0/1\">Michael Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Z/0/1/0/all/0/1\">Zhongkai Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yifei_X/0/1/0/all/0/1\">Xu Yifei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Min Feng</a>",
          "description": "In this paper, we introduce the first Challenge on Multi-modal Aerial View\nObject Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at\nCVPR. This challenge is composed of two different tracks using EO andSAR\nimagery. Both EO and SAR sensors possess different advantages and drawbacks.\nThe purpose of this competition is to analyze how to use both sets of sensory\ninformation in complementary ways. We discuss the top methods submitted for\nthis competition and evaluate their results on our blind test set. Our\nchallenge results show significant improvement of more than 15% accuracy from\nour current baselines for each track of the competition",
          "link": "http://arxiv.org/abs/2107.01189",
          "publishedOn": "2021-07-05T01:54:58.200Z",
          "wordCount": 632,
          "title": "NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Video segmentation, i.e., partitioning video frames into multiple segments or\nobjects, plays a critical role in a broad range of practical applications,\ne.g., visual effect assistance in movie, scene understanding in autonomous\ndriving, and virtual background creation in video conferencing, to name a few.\nRecently, due to the renaissance of connectionism in computer vision, there has\nbeen an influx of numerous deep learning based approaches that have been\ndedicated to video segmentation and delivered compelling performance. In this\nsurvey, we comprehensively review two basic lines of research in this area,\ni.e., generic object segmentation (of unknown categories) in videos and video\nsemantic segmentation, by introducing their respective task settings,\nbackground concepts, perceived need, development history, and main challenges.\nWe also provide a detailed overview of representative literature on both\nmethods and datasets. Additionally, we present quantitative performance\ncomparisons of the reviewed methods on benchmark datasets. At last, we point\nout a set of unsolved open issues in this field, and suggest possible\nopportunities for further research.",
          "link": "http://arxiv.org/abs/2107.01153",
          "publishedOn": "2021-07-05T01:54:58.182Z",
          "wordCount": 607,
          "title": "A Survey on Deep Learning Technique for Video Segmentation. (arXiv:2107.01153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.03409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1\">Gabriel Henrique de Almeida Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusioka_A/0/1/0/all/0/1\">Andr&#xe9; Minoro Fusioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassu_B/0/1/0/all/0/1\">Bogdan Tomoyuki Nassu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1\">Rodrigo Minetto</a>",
          "description": "Active fire detection in satellite imagery is of critical importance to the\nmanagement of environmental conservation policies, supporting decision-making\nand law enforcement. This is a well established field, with many techniques\nbeing proposed over the years, usually based on pixel or region-level\ncomparisons involving sensor-specific thresholds and neighborhood statistics.\nIn this paper, we address the problem of active fire detection using deep\nlearning techniques. In recent years, deep learning techniques have been\nenjoying an enormous success in many fields, but their use for active fire\ndetection is relatively new, with open questions and demand for datasets and\narchitectures for evaluation. This paper addresses these issues by introducing\na new large-scale dataset for active fire detection, with over 150,000 image\npatches (more than 200 GB of data) extracted from Landsat-8 images captured\naround the world in August and September 2020, containing wildfires in several\nlocations. The dataset was split in two parts, and contains 10-band spectral\nimages with associated outputs, produced by three well known handcrafted\nalgorithms for active fire detection in the first part, and manually annotated\nmasks in the second part. We also present a study on how different\nconvolutional neural network architectures can be used to approximate these\nhandcrafted algorithms, and how models trained on automatically segmented\npatches can be combined to achieve better performance than the original\nalgorithms - with the best combination having 87.2% precision and 92.4% recall\non our manually annotated dataset. The proposed dataset, source codes and\ntrained models are available on Github\n(https://github.com/pereira-gha/activefire), creating opportunities for further\nadvances in the field",
          "link": "http://arxiv.org/abs/2101.03409",
          "publishedOn": "2021-07-05T01:54:58.151Z",
          "wordCount": 747,
          "title": "Active Fire Detection in Landsat-8 Imagery: a Large-Scale Dataset and a Deep-Learning Study. (arXiv:2101.03409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1\">Sushrut Karmalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jessica Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>",
          "description": "This work tackles the issue of fairness in the context of generative\nprocedures, such as image super-resolution, which entail different definitions\nfrom the standard classification setting. Moreover, while traditional group\nfairness definitions are typically defined with respect to specified protected\ngroups -- camouflaging the fact that these groupings are artificial and carry\nhistorical and political motivations -- we emphasize that there are no ground\ntruth identities. For instance, should South and East Asians be viewed as a\nsingle group or separate groups? Should we consider one race as a whole or\nfurther split by gender? Choosing which groups are valid and who belongs in\nthem is an impossible dilemma and being \"fair\" with respect to Asians may\nrequire being \"unfair\" with respect to South Asians. This motivates the\nintroduction of definitions that allow algorithms to be \\emph{oblivious} to the\nrelevant groupings.\n\nWe define several intuitive notions of group fairness and study their\nincompatibilities and trade-offs. We show that the natural extension of\ndemographic parity is strongly dependent on the grouping, and \\emph{impossible}\nto achieve obliviously. On the other hand, the conceptually new definition we\nintroduce, Conditional Proportional Representation, can be achieved obliviously\nthrough Posterior Sampling. Our experiments validate our theoretical results\nand achieve fair image reconstruction using state-of-the-art generative models.",
          "link": "http://arxiv.org/abs/2106.12182",
          "publishedOn": "2021-07-05T01:54:58.144Z",
          "wordCount": 685,
          "title": "Fairness for Image Generation with Uncertain Sensitive Attributes. (arXiv:2106.12182v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12917",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohler_G/0/1/0/all/0/1\">Gregor K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jager_P/0/1/0/all/0/1\">Paul F. J&#xe4;ger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zimmerer_D/0/1/0/all/0/1\">David Zimmerer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neuberger_U/0/1/0/all/0/1\">Ulf Neuberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wick_W/0/1/0/all/0/1\">Wolfgang Wick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Debus_J/0/1/0/all/0/1\">J&#xfc;rgen Debus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heiland_S/0/1/0/all/0/1\">Sabine Heiland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bendszus_M/0/1/0/all/0/1\">Martin Bendszus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vollmuth_P/0/1/0/all/0/1\">Philipp Vollmuth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1\">Klaus H. Maier-Hein</a>",
          "description": "The ability to estimate how a tumor might evolve in the future could have\ntremendous clinical benefits, from improved treatment decisions to better dose\ndistribution in radiation therapy. Recent work has approached the glioma growth\nmodeling problem via deep learning and variational inference, thus learning\ngrowth dynamics entirely from a real patient data distribution. So far, this\napproach was constrained to predefined image acquisition intervals and\nsequences of fixed length, which limits its applicability in more realistic\nscenarios. We overcome these limitations by extending Neural Processes, a class\nof conditional generative models for stochastic time series, with a\nhierarchical multi-scale representation encoding including a spatio-temporal\nattention mechanism. The result is a learned growth model that can be\nconditioned on an arbitrary number of observations, and that can produce a\ndistribution of temporally consistent growth trajectories on a continuous time\naxis. On a dataset of 379 patients, the approach successfully captures both\nglobal and finer-grained variations in the images, exhibiting superior\nperformance compared to other learned growth models.",
          "link": "http://arxiv.org/abs/2106.12917",
          "publishedOn": "2021-07-05T01:54:58.137Z",
          "wordCount": 643,
          "title": "Continuous-Time Deep Glioma Growth Models. (arXiv:2106.12917v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Road-boundary detection is important for autonomous driving. It can be used\nto constrain autonomous vehicles running on road areas to ensure driving\nsafety. Compared with online road-boundary detection using on-vehicle\ncameras/Lidars, offline detection using aerial images could alleviate the\nsevere occlusion issue. Moreover, the offline detection results can be directly\nemployed to annotate high-definition (HD) maps. In recent years, deep-learning\ntechnologies have been used in offline detection. But there still lacks a\npublicly available dataset for this task, which hinders the research progress\nin this area. So in this paper, we propose a new benchmark dataset, named\n\\textit{Topo-boundary}, for offline topological road-boundary detection. The\ndataset contains 25,295 $1000\\times1000$-sized 4-channel aerial images. Each\nimage is provided with 8 training labels for different sub-tasks. We also\ndesign a new entropy-based metric for connectivity evaluation, which could\nbetter handle noises or outliers. We implement and evaluate 3\nsegmentation-based baselines and 5 graph-based baselines using the dataset. We\nalso propose a new imitation-learning-based baseline which is enhanced from our\nprevious work. The superiority of our enhancement is demonstrated from the\ncomparison. The dataset and our-implemented code for the baselines are\navailable at \\texttt{\\url{https://tonyxuqaq.github.io/Topo-boundary/}}.",
          "link": "http://arxiv.org/abs/2103.17119",
          "publishedOn": "2021-07-05T01:54:58.117Z",
          "wordCount": 687,
          "title": "Topo-boundary: A Benchmark Dataset on Topological Road-boundary Detection Using Aerial Images for Autonomous Driving. (arXiv:2103.17119v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jameel Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1\">Soshi Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhayek_A/0/1/0/all/0/1\">Ahmed Elhayek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Sk Aziz Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>",
          "description": "3D hand shape and pose estimation from a single depth map is a new and\nchallenging computer vision problem with many applications. Existing methods\naddressing it directly regress hand meshes via 2D convolutional neural\nnetworks, which leads to artifacts due to perspective distortions in the\nimages. To address the limitations of the existing methods, we develop\nHandVoxNet++, i.e., a voxel-based deep network with 3D and graph convolutions\ntrained in a fully supervised manner. The input to our network is a 3D\nvoxelized-depth-map-based on the truncated signed distance function (TSDF).\nHandVoxNet++ relies on two hand shape representations. The first one is the 3D\nvoxelized grid of hand shape, which does not preserve the mesh topology and\nwhich is the most accurate representation. The second representation is the\nhand surface that preserves the mesh topology. We combine the advantages of\nboth representations by aligning the hand surface to the voxelized hand shape\neither with a new neural Graph-Convolutions-based Mesh Registration\n(GCN-MeshReg) or classical segment-wise Non-Rigid Gravitational Approach\n(NRGA++) which does not rely on training data. In extensive evaluations on\nthree public benchmarks, i.e., SynHand5M, depth-based HANDS19 challenge and\nHO-3D, the proposed HandVoxNet++ achieves the state-of-the-art performance. In\nthis journal extension of our previous approach presented at CVPR 2020, we gain\n41.09% and 13.7% higher shape alignment accuracy on SynHand5M and HANDS19\ndatasets, respectively. Our method is ranked first on the HANDS19 challenge\ndataset (Task 1: Depth-Based 3D Hand Pose Estimation) at the moment of the\nsubmission of our results to the portal in August 2020.",
          "link": "http://arxiv.org/abs/2107.01205",
          "publishedOn": "2021-07-05T01:54:58.108Z",
          "wordCount": 721,
          "title": "HandVoxNet++: 3D Hand Shape and Pose Estimation using Voxel-Based Neural Networks. (arXiv:2107.01205v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "We introduce Neural Marching Cubes (NMC), a data-driven approach for\nextracting a triangle mesh from a discretized implicit field. Classical MC is\ndefined by coarse tessellation templates isolated to individual cubes. While\nmore refined tessellations have been proposed, they all make heuristic\nassumptions, such as trilinearity, when determining the vertex positions and\nlocal mesh topologies in each cube. In principle, none of these approaches can\nreconstruct geometric features that reveal coherence or dependencies between\nnearby cubes (e.g., a sharp edge), as such information is unaccounted for,\nresulting in poor estimates of the true underlying implicit field. To tackle\nthese challenges, we re-cast MC from a deep learning perspective, by designing\ntessellation templates more apt at preserving geometric features, and learning\nthe vertex positions and mesh topologies from training meshes, to account for\ncontextual information from nearby cubes. We develop a compact per-cube\nparameterization to represent the output triangle mesh, while being compatible\nwith neural processing, so that a simple 3D convolutional network can be\nemployed for the training. We show that all topological cases in each cube that\nare applicable to our design can be easily derived using our representation,\nand the resulting tessellations can also be obtained naturally and efficiently\nby following a few design guidelines. In addition, our network learns local\nfeatures with limited receptive fields, hence it generalizes well to new shapes\nand new datasets. We evaluate our neural MC approach by quantitative and\nqualitative comparisons to all well-known MC variants. In particular, we\ndemonstrate the ability of our network to recover sharp features such as edges\nand corners, a long-standing issue of MC and its variants. Our network also\nreconstructs local mesh topologies more accurately than previous approaches.",
          "link": "http://arxiv.org/abs/2106.11272",
          "publishedOn": "2021-07-05T01:54:58.101Z",
          "wordCount": 738,
          "title": "Neural Marching Cubes. (arXiv:2106.11272v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korschens_M/0/1/0/all/0/1\">Matthias K&#xf6;rschens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodesheim_P/0/1/0/all/0/1\">Paul Bodesheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romermann_C/0/1/0/all/0/1\">Christine R&#xf6;mermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_S/0/1/0/all/0/1\">Solveig Franziska Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migliavacca_M/0/1/0/all/0/1\">Mirco Migliavacca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulrich_J/0/1/0/all/0/1\">Josephine Ulrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "Monitoring the responses of plants to environmental changes is essential for\nplant biodiversity research. This, however, is currently still being done\nmanually by botanists in the field. This work is very laborious, and the data\nobtained is, though following a standardized method to estimate plant coverage,\nusually subjective and has a coarse temporal resolution. To remedy these\ncaveats, we investigate approaches using convolutional neural networks (CNNs)\nto automatically extract the relevant data from images, focusing on plant\ncommunity composition and species coverages of 9 herbaceous plant species. To\nthis end, we investigate several standard CNN architectures and different\npretraining methods. We find that we outperform our previous approach at higher\nimage resolutions using a custom CNN with a mean absolute error of 5.16%. In\naddition to these investigations, we also conduct an error analysis based on\nthe temporal aspect of the plant cover images. This analysis gives insight into\nwhere problems for automatic approaches lie, like occlusion and likely\nmisclassifications caused by temporal changes.",
          "link": "http://arxiv.org/abs/2106.11154",
          "publishedOn": "2021-07-05T01:54:58.093Z",
          "wordCount": 635,
          "title": "Automatic Plant Cover Estimation with Convolutional Neural Networks. (arXiv:2106.11154v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Li Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "We aim to tackle the challenging yet practical scenery image outpainting task\nin this work. Recently, generative adversarial learning has significantly\nadvanced the image outpainting by producing semantic consistent content for the\ngiven image. However, the existing methods always suffer from the blurry\ntexture and the artifacts of the generative part, making the overall\noutpainting results lack authenticity. To overcome the weakness, this work\ninvestigates a principle way to synthesize texture-rich results by borrowing\npixels from its neighbors (\\ie, reference images), named\n\\textbf{Re}ference-\\textbf{G}uided \\textbf{O}utpainting (ReGO). Particularly,\nthe ReGO designs an Adaptive Content Selection (ACS) module to transfer the\npixel of reference images for texture compensating of the target one. To\nprevent the style of the generated part from being affected by the reference\nimages, a style ranking loss is further proposed to augment the ReGO to\nsynthesize style-consistent results. Extensive experiments on two popular\nbenchmarks, NS6K~\\cite{yangzx} and NS8K~\\cite{wang}, well demonstrate the\neffectiveness of our ReGO.",
          "link": "http://arxiv.org/abs/2106.10601",
          "publishedOn": "2021-07-05T01:54:58.084Z",
          "wordCount": 623,
          "title": "ReGO: Reference-Guided Outpainting for Scenery Image. (arXiv:2106.10601v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>",
          "description": "Image reconstruction is likely the most predominant auxiliary task for image\nclassification, but we would like to think twice about this convention. In this\npaper, we investigated \"approximating the Fourier Transform of the input image\"\nas a potential alternative, in the hope that it may further boost the\nperformances on the primary task or introduce novel constraints not well\ncovered by image reconstruction. We experimented with five popular\nclassification architectures on the CIFAR-10 dataset, and the empirical results\nindicated that our proposed auxiliary task generally improves the\nclassification accuracy. More notably, the results showed that in certain cases\nour proposed auxiliary task may enhance the classifiers' resistance to\nadversarial attacks generated using the fast gradient sign method.",
          "link": "http://arxiv.org/abs/2106.11478",
          "publishedOn": "2021-07-05T01:54:58.075Z",
          "wordCount": 610,
          "title": "Fourier Transform Approximation as an Auxiliary Task for Image Classification. (arXiv:2106.11478v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gallardo_J/0/1/0/all/0/1\">Jhair Gallardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>",
          "description": "In continual learning, a system must incrementally learn from a\nnon-stationary data stream without catastrophic forgetting. Recently, multiple\nmethods have been devised for incrementally learning classes on large-scale\nimage classification tasks, such as ImageNet. State-of-the-art continual\nlearning methods use an initial supervised pre-training phase, in which the\nfirst 10% - 50% of the classes in a dataset are used to learn representations\nin an offline manner before continual learning of new classes begins. We\nhypothesize that self-supervised pre-training could yield features that\ngeneralize better than supervised learning, especially when the number of\nsamples used for pre-training is small. We test this hypothesis using the\nself-supervised MoCo-V2, Barlow Twins, and SwAV algorithms. On ImageNet, we\nfind that these methods outperform supervised pre-training considerably for\nonline continual learning, and the gains are larger when fewer samples are\navailable. Our findings are consistent across three online continual learning\nalgorithms. Our best system achieves a 14.95% relative increase in top-1\naccuracy on class incremental ImageNet over the prior state of the art for\nonline continual learning.",
          "link": "http://arxiv.org/abs/2103.14010",
          "publishedOn": "2021-07-05T01:54:58.055Z",
          "wordCount": 633,
          "title": "Self-Supervised Training Enhances Online Continual Learning. (arXiv:2103.14010v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08028",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kapse_S/0/1/0/all/0/1\">Saarthak Kapse</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gattu_R/0/1/0/all/0/1\">Rishabh Gattu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Syed Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shah_N/0/1/0/all/0/1\">Neal Shah</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Marshall_C/0/1/0/all/0/1\">Colin Marshall</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pierce_J/0/1/0/all/0/1\">Jonathan Pierce</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Phatak_T/0/1/0/all/0/1\">Tej Phatak</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gupta_A/0/1/0/all/0/1\">Amit Gupta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Green_J/0/1/0/all/0/1\">Jeremy Green</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Madan_N/0/1/0/all/0/1\">Nikhil Madan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>",
          "description": "We predict mechanical ventilation requirement and mortality using\ncomputational modeling of chest radiographs (CXRs) for coronavirus disease 2019\n(COVID-19) patients. This two-center, retrospective study analyzed 530\ndeidentified CXRs from 515 COVID-19 patients treated at Stony Brook University\nHospital and Newark Beth Israel Medical Center between March and August 2020.\nDL and machine learning classifiers to predict mechanical ventilation\nrequirement and mortality were trained and evaluated using patient CXRs. A\nnovel radiomic embedding framework was also explored for outcome prediction.\nAll results are compared against radiologist grading of CXRs (zone-wise expert\nseverity scores). Radiomic and DL classification models had mAUCs of\n0.78+/-0.02 and 0.81+/-0.04, compared with expert scores mAUCs of 0.75+/-0.02\nand 0.79+/-0.05 for mechanical ventilation requirement and mortality\nprediction, respectively. Combined classifiers using both radiomics and expert\nseverity scores resulted in mAUCs of 0.79+/-0.04 and 0.83+/-0.04 for each\nprediction task, demonstrating improvement over either artificial intelligence\nor radiologist interpretation alone. Our results also suggest instances where\ninclusion of radiomic features in DL improves model predictions, something that\nmight be explored in other pathologies. The models proposed in this study and\nthe prognostic information they provide might aid physician decision making and\nresource allocation during the COVID-19 pandemic.",
          "link": "http://arxiv.org/abs/2007.08028",
          "publishedOn": "2021-07-05T01:54:58.049Z",
          "wordCount": 757,
          "title": "Predicting Clinical Outcomes in COVID-19 using Radiomics and Deep Learning on Chest Radiographs: A Multi-Institutional Study. (arXiv:2007.08028v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antonante_P/0/1/0/all/0/1\">Pasquale Antonante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzoumas_V/0/1/0/all/0/1\">Vasileios Tzoumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>",
          "description": "Nonlinear estimation in robotics and vision is typically plagued with\noutliers due to wrong data association, or to incorrect detections from signal\nprocessing and machine learning methods. This paper introduces two unifying\nformulations for outlier-robust estimation, Generalized Maximum Consensus\n(G-MC) and Generalized Truncated Least Squares (G-TLS), and investigates\nfundamental limits, practical algorithms, and applications. Our first\ncontribution is a proof that outlier-robust estimation is inapproximable: in\nthe worst case, it is impossible to (even approximately) find the set of\noutliers, even with slower-than-polynomial-time algorithms (particularly,\nalgorithms running in quasi-polynomial time). As a second contribution, we\nreview and extend two general-purpose algorithms. The first, Adaptive Trimming\n(ADAPT), is combinatorial, and is suitable for G-MC; the second, Graduated\nNon-Convexity (GNC), is based on homotopy methods, and is suitable for G-TLS.\nWe extend ADAPT and GNC to the case where the user does not have prior\nknowledge of the inlier-noise statistics (or the statistics may vary over time)\nand is unable to guess a reasonable threshold to separate inliers from outliers\n(as the one commonly used in RANSAC). We propose the first minimally tuned\nalgorithms for outlier rejection, that dynamically decide how to separate\ninliers from outliers. Our third contribution is an evaluation of the proposed\nalgorithms on robot perception problems: mesh registration, image-based object\ndetection (shape alignment), and pose graph optimization. ADAPT and GNC execute\nin real-time, are deterministic, outperform RANSAC, and are robust up to 80-90%\noutliers. Their minimally tuned versions also compare favorably with the state\nof the art, even though they do not rely on a noise bound for the inliers.",
          "link": "http://arxiv.org/abs/2007.15109",
          "publishedOn": "2021-07-05T01:54:58.041Z",
          "wordCount": 739,
          "title": "Outlier-Robust Estimation: Hardness, Minimally Tuned Algorithms, and Applications. (arXiv:2007.15109v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07770",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Topiwala_P/0/1/0/all/0/1\">Pankaj Topiwala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pian_J/0/1/0/all/0/1\">Jiangfeng Pian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biondi_K/0/1/0/all/0/1\">Katalina Biondi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krovvidi_A/0/1/0/all/0/1\">Arvind Krovvidi</a>",
          "description": "Video quality assessment (VQA) is now a fastgrowing subject, beginning to\nmature in the full reference (FR) case, while the burgeoning no reference (NR)\ncase remains challenging. We investigate variants of the popular VMAF video\nquality assessment algorithm for the FR case, using support vector regression\nand feedforward neural networks, and extend it to the NR case, using the same\nlearning architectures, to develop a partially unified framework for VQA. When\nheavily trained, algorithms such as VMAF perform well on test datasets, with\n90%+ match; but predicting performance in the wild is better done by\ntraining/testing from scratch, as we do. Even from scratch, we achieve 90%+\nperformance in FR, with gains over VMAF. And we greatly reduce complexity vs.\nleading recent NR algorithms, VIDEVAL, RAPIQUE, yet exceed 80% in SRCC. In our\npreliminary testing, we find the improvements in trainability, while also\nconstraining computational complexity, as quite encouraging, suggesting further\nstudy and analysis.",
          "link": "http://arxiv.org/abs/2103.07770",
          "publishedOn": "2021-07-05T01:54:58.034Z",
          "wordCount": 625,
          "title": "VMAF And Variants: Towards A Unified VQA. (arXiv:2103.07770v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>",
          "description": "3D point cloud classification has many safety-critical applications such as\nautonomous driving and robotic grasping. However, several studies showed that\nit is vulnerable to adversarial attacks. In particular, an attacker can make a\nclassifier predict an incorrect label for a 3D point cloud via carefully\nmodifying, adding, and/or deleting a small number of its points. Randomized\nsmoothing is state-of-the-art technique to build certifiably robust 2D image\nclassifiers. However, when applied to 3D point cloud classification, randomized\nsmoothing can only certify robustness against adversarially modified points.\n\nIn this work, we propose PointGuard, the first defense that has provable\nrobustness guarantees against adversarially modified, added, and/or deleted\npoints. Specifically, given a 3D point cloud and an arbitrary point cloud\nclassifier, our PointGuard first creates multiple subsampled point clouds, each\nof which contains a random subset of the points in the original point cloud;\nthen our PointGuard predicts the label of the original point cloud as the\nmajority vote among the labels of the subsampled point clouds predicted by the\npoint cloud classifier. Our first major theoretical contribution is that we\nshow PointGuard provably predicts the same label for a 3D point cloud when the\nnumber of adversarially modified, added, and/or deleted points is bounded. Our\nsecond major theoretical contribution is that we prove the tightness of our\nderived bound when no assumptions on the point cloud classifier are made.\nMoreover, we design an efficient algorithm to compute our certified robustness\nguarantees. We also empirically evaluate PointGuard on ModelNet40 and ScanNet\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2103.03046",
          "publishedOn": "2021-07-05T01:54:58.017Z",
          "wordCount": 732,
          "title": "PointGuard: Provably Robust 3D Point Cloud Classification. (arXiv:2103.03046v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1\">Giorgos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzi_E/0/1/0/all/0/1\">Ed Pizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papakipos_Z/0/1/0/all/0/1\">Zo&#xeb; Papakipos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_L/0/1/0/all/0/1\">Lowik Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenicek_T/0/1/0/all/0/1\">Tomas Jenicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maximov_M/0/1/0/all/0/1\">Maxim Maximov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_O/0/1/0/all/0/1\">Ond&#x159;ej Chum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>",
          "description": "This paper introduces a new benchmark for large-scale image similarity\ndetection. This benchmark is used for the Image Similarity Challenge at\nNeurIPS'21 (ISC2021). The goal is to determine whether a query image is a\nmodified copy of any image in a reference corpus of size 1~million. The\nbenchmark features a variety of image transformations such as automated\ntransformations, hand-crafted image edits and machine-learning based\nmanipulations. This mimics real-life cases appearing in social media, for\nexample for integrity-related problems dealing with misinformation and\nobjectionable content. The strength of the image manipulations, and therefore\nthe difficulty of the benchmark, is calibrated according to the performance of\na set of baseline approaches. Both the query and reference set contain a\nmajority of \"distractor\" images that do not match, which corresponds to a\nreal-life needle-in-haystack setting, and the evaluation metric reflects that.\nWe expect the DISC21 benchmark to promote image copy detection as an important\nand challenging computer vision task and refresh the state of the art.",
          "link": "http://arxiv.org/abs/2106.09672",
          "publishedOn": "2021-07-05T01:54:58.004Z",
          "wordCount": 645,
          "title": "The 2021 Image Similarity Dataset and Challenge. (arXiv:2106.09672v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the development set,\nthe achieved CCC is 0.410 for valence and 0.661 for arousal, which\nsignificantly outperforms the baseline method with the corresponding CCC of\n0.210 and 0.230 for valence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.",
          "link": "http://arxiv.org/abs/2107.01175",
          "publishedOn": "2021-07-05T01:54:57.997Z",
          "wordCount": 611,
          "title": "Audio-visual Attentive Fusion for Continuous Emotion Recognition. (arXiv:2107.01175v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1\">David Bull</a>",
          "description": "This paper reviews the current state of the art in Artificial Intelligence\n(AI) technologies and applications in the context of the creative industries. A\nbrief background of AI, and specifically Machine Learning (ML) algorithms, is\nprovided including Convolutional Neural Network (CNNs), Generative Adversarial\nNetworks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement\nLearning (DRL). We categorise creative applications into five groups related to\nhow AI technologies are used: i) content creation, ii) information analysis,\niii) content enhancement and post production workflows, iv) information\nextraction and enhancement, and v) data compression. We critically examine the\nsuccesses and limitations of this rapidly advancing technology in each of these\nareas. We further differentiate between the use of AI as a creative tool and\nits potential as a creator in its own right. We foresee that, in the near\nfuture, machine learning-based AI will be adopted widely as a tool or\ncollaborative assistant for creativity. In contrast, we observe that the\nsuccesses of machine learning in domains with fewer constraints, where AI is\nthe `creator', remain modest. The potential of AI (or its developers) to win\nawards for its original creations in competition with human creatives is also\nlimited, based on contemporary technologies. We therefore conclude that, in the\ncontext of creative industries, maximum benefit from AI will be derived where\nits focus is human centric -- where it is designed to augment, rather than\nreplace, human creativity.",
          "link": "http://arxiv.org/abs/2007.12391",
          "publishedOn": "2021-07-05T01:54:57.982Z",
          "wordCount": 746,
          "title": "Artificial Intelligence in the Creative Industries: A Review. (arXiv:2007.12391v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mi_L/0/1/0/all/0/1\">Li Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yangjun Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "Real-world scenarios often require the anticipation of object interactions in\nunknown future, which would assist the decision-making process of both humans\nand agents. To meet this challenge, we present a new task named Visual\nRelationship Forecasting (VRF) in videos to explore the prediction of visual\nrelationships in a reasoning manner. Specifically, given a subject-object pair\nwith H existing frames, VRF aims to predict their future interactions for the\nnext T frames without visual evidence. To evaluate the VRF task, we introduce\ntwo video datasets named VRF-AG and VRF-VidOR, with a series of\nspatio-temporally localized visual relation annotations in a video. These two\ndatasets densely annotate 13 and 35 visual relationships in 1923 and 13447\nvideo clips, respectively. In addition, we present a novel Graph Convolutional\nTransformer (GCT) framework, which captures both object-level and frame-level\ndependencies by spatio-temporal Graph Convolution Network and Transformer.\nExperimental results on both VRF-AG and VRF-VidOR datasets demonstrate that GCT\noutperforms the state-of-the-art sequence modelling methods on visual\nrelationship forecasting.",
          "link": "http://arxiv.org/abs/2107.01181",
          "publishedOn": "2021-07-05T01:54:57.974Z",
          "wordCount": 595,
          "title": "Visual Relationship Forecasting in Videos. (arXiv:2107.01181v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_C/0/1/0/all/0/1\">Christian Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argus_M/0/1/0/all/0/1\">Max Argus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>",
          "description": "This work presents improvements in monocular hand shape estimation by\nbuilding on top of recent advances in unsupervised learning. We extend momentum\ncontrastive learning and contribute a structured collection of hand images,\nwell suited for visual representation learning, which we call HanCo. We find\nthat the representation learned by established contrastive learning methods can\nbe improved significantly by exploiting advanced background removal techniques\nand multi-view information. These allow us to generate more diverse instance\npairs than those obtained by augmentations commonly used in exemplar based\napproaches. Our method leads to a more suitable representation for the hand\nshape estimation task and shows a 4.7% reduction in mesh error and a 3.6%\nimprovement in F-score compared to an ImageNet pretrained baseline. We make our\nbenchmark dataset publicly available, to encourage further research into this\ndirection.",
          "link": "http://arxiv.org/abs/2106.04324",
          "publishedOn": "2021-07-05T01:54:57.968Z",
          "wordCount": 593,
          "title": "Contrastive Representation Learning for Hand Shape Estimation. (arXiv:2106.04324v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1\">Davood Zabihzadeh</a>",
          "description": "Deep Metric Learning (DML) learns a non-linear semantic embedding from input\ndata that brings similar pairs together while keeps dissimilar data away from\neach other. To this end, many different methods are proposed in the last decade\nwith promising results in various applications. The success of a DML algorithm\ngreatly depends on its loss function. However, no loss function is perfect, and\nit deals only with some aspects of an optimal similarity embedding. Besides,\nthe generalizability of the DML on unseen categories during the test stage is\nan important matter that is not considered by existing loss functions. To\naddress these challenges, we propose novel approaches to combine different\nlosses built on top of a shared deep feature extractor. The proposed ensemble\nof losses enforces the deep model to extract features that are consistent with\nall losses. Since the selected losses are diverse and each emphasizes different\naspects of an optimal semantic embedding, our effective combining methods yield\na considerable improvement over any individual loss and generalize well on\nunseen categories. Here, there is no limitation in choosing loss functions, and\nour methods can work with any set of existing ones. Besides, they can optimize\neach loss function as well as its weight in an end-to-end paradigm with no need\nto adjust any hyper-parameter. We evaluate our methods on some popular datasets\nfrom the machine vision domain in conventional Zero-Shot-Learning (ZSL)\nsettings. The results are very encouraging and show that our methods outperform\nall baseline losses by a large margin in all datasets.",
          "link": "http://arxiv.org/abs/2107.01130",
          "publishedOn": "2021-07-05T01:54:57.951Z",
          "wordCount": 713,
          "title": "Ensemble of Loss Functions to Improve Generalizability of Deep Metric Learning methods. (arXiv:2107.01130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhengyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>",
          "description": "Contrastive learning applied to self-supervised representation learning has\nseen a resurgence in deep models. In this paper, we find that existing\ncontrastive learning based solutions for self-supervised video recognition\nfocus on inter-variance encoding but ignore the intra-variance existing in\nclips within the same video. We thus propose to learn dual representations for\neach clip which (\\romannumeral 1) encode intra-variance through a shuffle-rank\npretext task; (\\romannumeral 2) encode inter-variance through a temporal\ncoherent contrastive loss. Experiment results show that our method plays an\nessential role in balancing inter and intra variances and brings consistent\nperformance gains on multiple backbones and contrastive learning frameworks.\nIntegrated with SimCLR and pretrained on Kinetics-400, our method achieves\n$\\textbf{82.0\\%}$ and $\\textbf{51.2\\%}$ downstream classification accuracy on\nUCF101 and HMDB51 test sets respectively and $\\textbf{46.1\\%}$ video retrieval\naccuracy on UCF101, outperforming both pretext-task based and contrastive\nlearning based counterparts.",
          "link": "http://arxiv.org/abs/2107.01194",
          "publishedOn": "2021-07-05T01:54:57.944Z",
          "wordCount": 595,
          "title": "How Incomplete is Contrastive Learning? AnInter-intra Variant Dual Representation Method forSelf-supervised Video Recognition. (arXiv:2107.01194v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Conghao Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>",
          "description": "It is essential but challenging to predict future trajectories of various\nagents in complex scenes. Whether it is internal personality factors of agents,\ninteractive behavior of the neighborhood, or the influence of surroundings, it\nwill have an impact on their future behavior styles. It means that even for the\nsame physical type of agents, there are huge differences in their behavior\npreferences. Although recent works have made significant progress in studying\nagents' multi-modal plannings, most of them still apply the same prediction\nstrategy to all agents, which makes them difficult to fully show the multiple\nstyles of vast agents. In this paper, we propose the Multi-Style Network (MSN)\nto focus on this problem by divide agents' preference styles into several\nhidden behavior categories adaptively and train each category's prediction\nnetwork separately, therefore giving agents all styles of predictions\nsimultaneously. Experiments demonstrate that our deterministic MSN-D and\ngenerative MSN-G outperform many recent state-of-the-art methods and show\nbetter multi-style characteristics in the visualized results.",
          "link": "http://arxiv.org/abs/2107.00932",
          "publishedOn": "2021-07-05T01:54:57.937Z",
          "wordCount": 594,
          "title": "MSN: Multi-Style Network for Trajectory Prediction. (arXiv:2107.00932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01125",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zenglin Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "The deep image prior has demonstrated the remarkable ability that untrained\nnetworks can address inverse imaging problems, such as denoising, inpainting\nand super-resolution, by optimizing on just a single degraded image. Despite\nits promise, it suffers from two limitations. First, it remains unclear how one\ncan control the prior beyond the choice of the network architecture. Second, it\nrequires an oracle to determine when to stop the optimization as the\nperformance degrades after reaching a peak. In this paper, we study the deep\nimage prior from a spectral bias perspective to address these problems. By\nintroducing a frequency-band correspondence measure, we observe that deep image\npriors for inverse imaging exhibit a spectral bias during optimization, where\nlow-frequency image signals are learned faster and better than high-frequency\nnoise signals. This pinpoints why degraded images can be denoised or inpainted\nwhen the optimization is stopped at the right time. Based on our observations,\nwe propose to control the spectral bias in the deep image prior to prevent\nperformance degradation and to speed up optimization convergence. We do so in\nthe two core layer types of inverse imaging networks: the convolution layer and\nthe upsampling layer. We present a Lipschitz-controlled approach for the\nconvolution and a Gaussian-controlled approach for the upsampling layer. We\nfurther introduce a stopping criterion to avoid superfluous computation. The\nexperiments on denoising, inpainting and super-resolution show that our method\nno longer suffers from performance degradation during optimization, relieving\nus from the need for an oracle criterion to stop early. We further outline a\nstopping criterion to avoid superfluous computation. Finally, we show that our\napproach obtains favorable restoration results compared to current approaches,\nacross all tasks.",
          "link": "http://arxiv.org/abs/2107.01125",
          "publishedOn": "2021-07-05T01:54:57.860Z",
          "wordCount": 739,
          "title": "On Measuring and Controlling the Spectral Bias of the Deep Image Prior. (arXiv:2107.01125v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01152",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1\">Junya Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1\">Xuan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1\">Liqun Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_S/0/1/0/all/0/1\">Shuyang Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lu_W/0/1/0/all/0/1\">Wenlian Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>",
          "description": "InfoNCE-based contrastive representation learners, such as SimCLR, have been\ntremendously successful in recent years. However, these contrastive schemes are\nnotoriously resource demanding, as their effectiveness breaks down with\nsmall-batch training (i.e., the log-K curse, whereas K is the batch-size). In\nthis work, we reveal mathematically why contrastive learners fail in the\nsmall-batch-size regime, and present a novel simple, non-trivial contrastive\nobjective named FlatNCE, which fixes this issue. Unlike InfoNCE, our FlatNCE no\nlonger explicitly appeals to a discriminative classification goal for\ncontrastive learning. Theoretically, we show FlatNCE is the mathematical dual\nformulation of InfoNCE, thus bridging the classical literature on energy\nmodeling; and empirically, we demonstrate that, with minimal modification of\ncode, FlatNCE enables immediate performance boost independent of the\nsubject-matter engineering efforts. The significance of this work is furthered\nby the powerful generalization of contrastive learning techniques, and the\nintroduction of new tools to monitor and diagnose contrastive training. We\nsubstantiate our claims with empirical evidence on CIFAR10, ImageNet, and other\ndatasets, where FlatNCE consistently outperforms InfoNCE.",
          "link": "http://arxiv.org/abs/2107.01152",
          "publishedOn": "2021-07-05T01:54:57.853Z",
          "wordCount": 644,
          "title": "Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE. (arXiv:2107.01152v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhmetyanov_A/0/1/0/all/0/1\">Azat Akhmetyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornilova_A/0/1/0/all/0/1\">Anastasiia Kornilova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faizullin_M/0/1/0/all/0/1\">Marsel Faizullin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pozo_D/0/1/0/all/0/1\">David Pozo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1\">Gonzalo Ferrer</a>",
          "description": "This paper addresses the problem of building an affordable easy-to-setup\nsynchronized multi-view camera system, which is in demand for many Computer\nVision and Robotics applications in high-dynamic environments. In our work, we\npropose a solution for this problem - a publicly-available Android application\nfor synchronized video recording on multiple smartphones with sub-millisecond\naccuracy. We present a generalized mathematical model of timestamping for\nAndroid smartphones and prove its applicability on 47 different physical\ndevices. Also, we estimate the time drift parameter for those smartphones,\nwhich is less than 1.2 millisecond per minute for most of the considered\ndevices, that makes smartphones' camera system a worthy analog for professional\nmulti-view systems. Finally, we demonstrate Android-app performance on the\ncamera system built from Android smartphones quantitatively, showing less than\n300 microseconds synchronization error, and qualitatively - on panorama\nstitching task.",
          "link": "http://arxiv.org/abs/2107.00987",
          "publishedOn": "2021-07-05T01:54:57.845Z",
          "wordCount": 572,
          "title": "Sub-millisecond Video Synchronization of Multiple Android Smartphones. (arXiv:2107.00987v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Airaksinen_M/0/1/0/all/0/1\">Manu Airaksinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhatalo_S/0/1/0/all/0/1\">Sampsa Vanhatalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>",
          "description": "Infant motility assessment using intelligent wearables is a promising new\napproach for assessment of infant neurophysiological development, and where\nefficient signal analysis plays a central role. This study investigates the use\nof different end-to-end neural network architectures for processing infant\nmotility data from wearable sensors. We focus on the performance and\ncomputational burden of alternative sensor encoder and time-series modelling\nmodules and their combinations. In addition, we explore the benefits of data\naugmentation methods in ideal and non-ideal recording conditions. The\nexperiments are conducted using a data-set of multi-sensor movement recordings\nfrom 7-month-old infants, as captured by a recently proposed smart jumpsuit for\ninfant motility assessment. Our results indicate that the choice of the encoder\nmodule has a major impact on classifier performance. For sensor encoders, the\nbest performance was obtained with parallel 2-dimensional convolutions for\nintra-sensor channel fusion with shared weights for all sensors. The results\nalso indicate that a relatively compact feature representation is obtainable\nfor within-sensor feature extraction without a drastic loss to classifier\nperformance. Comparison of time-series models revealed that feed-forward\ndilated convolutions with residual and skip connections outperformed all\nRNN-based models in performance, training time, and training stability. The\nexperiments also indicate that data augmentation improves model robustness in\nsimulated packet loss or sensor dropout scenarios. In particular, signal- and\nsensor-dropout-based augmentation strategies provided considerable boosts to\nperformance without negatively affecting the baseline performance. Overall the\nresults provide tangible suggestions on how to optimize end-to-end neural\nnetwork training for multi-channel movement sensor data.",
          "link": "http://arxiv.org/abs/2107.01086",
          "publishedOn": "2021-07-05T01:54:57.833Z",
          "wordCount": 705,
          "title": "Comparison of end-to-end neural network architectures and data augmentation methods for automatic infant motility assessment using wearable sensors. (arXiv:2107.01086v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongji Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiufan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yingying Zhu</a>",
          "description": "In this work, we address the problem of cross-view geo-localization, which\nestimates the geospatial location of a street view image by matching it with a\ndatabase of geo-tagged aerial images. The cross-view matching task is extremely\nchallenging due to drastic appearance and geometry differences across views.\nUnlike existing methods that predominantly fall back on CNN, here we devise a\nnovel evolving geo-localization Transformer (EgoTR) that utilizes the\nproperties of self-attention in Transformer to model global dependencies, thus\nsignificantly decreasing visual ambiguities in cross-view geo-localization. We\nalso exploit the positional encoding of Transformer to help the EgoTR\nunderstand and correspond geometric configurations between ground and aerial\nimages. Compared to state-of-the-art methods that impose strong assumption on\ngeometry knowledge, the EgoTR flexibly learns the positional embeddings through\nthe training objective and hence becomes more practical in many real-world\nscenarios. Although Transformer is well suited to our task, its vanilla\nself-attention mechanism independently interacts within image patches in each\nlayer, which overlooks correlations between layers. Instead, this paper propose\na simple yet effective self-cross attention mechanism to improve the quality of\nlearned representations. The self-cross attention models global dependencies\nbetween adjacent layers, which relates between image patches while modeling how\nfeatures evolve in the previous layer. As a result, the proposed self-cross\nattention leads to more stable training, improves the generalization ability\nand encourages representations to keep evolving as the network goes deeper.\nExtensive experiments demonstrate that our EgoTR performs favorably against\nstate-of-the-art methods on standard, fine-grained and cross-dataset cross-view\ngeo-localization tasks.",
          "link": "http://arxiv.org/abs/2107.00842",
          "publishedOn": "2021-07-05T01:54:57.826Z",
          "wordCount": 679,
          "title": "Cross-view Geo-localization with Evolving Transformer. (arXiv:2107.00842v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suwannaphong_T/0/1/0/all/0/1\">Thanaphon Suwannaphong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavana_S/0/1/0/all/0/1\">Sawaphob Chavana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tongsom_S/0/1/0/all/0/1\">Sahapol Tongsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palasuwan_D/0/1/0/all/0/1\">Duangdao Palasuwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalidabhongse_T/0/1/0/all/0/1\">Thanarat H. Chalidabhongse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>",
          "description": "Intestinal parasitic infection leads to several morbidities to humans\nworldwide, especially in tropical countries. The traditional diagnosis usually\nrelies on manual analysis from microscopic images which is prone to human error\ndue to morphological similarity of different parasitic eggs and abundance of\nimpurities in a sample. Many studies have developed automatic systems for\nparasite egg detection to reduce human workload. However, they work with high\nquality microscopes, which unfortunately remain unaffordable in some rural\nareas. Our work thus exploits a benefit of a low-cost USB microscope. This\ninstrument however provides poor quality of images due to limitation of\nmagnification (10x), causing difficulty in parasite detection and species\nclassification. In this paper, we propose a CNN-based technique using transfer\nlearning strategy to enhance the efficiency of automatic parasite\nclassification in poor-quality microscopic images. The patch-based technique\nwith sliding window is employed to search for location of the eggs. Two\nnetworks, AlexNet and ResNet50, are examined with a trade-off between\narchitecture size and classification performance. The results show that our\nproposed framework outperforms the state-of-the-art object recognition methods.\nOur system combined with final decision from an expert may improve the real\nfaecal examination with low-cost microscopes.",
          "link": "http://arxiv.org/abs/2107.00968",
          "publishedOn": "2021-07-05T01:54:57.820Z",
          "wordCount": 654,
          "title": "Parasitic Egg Detection and Classification in Low-cost Microscopic Images using Transfer Learning. (arXiv:2107.00968v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>",
          "description": "As a fundamental problem for Artificial Intelligence, multi-agent system\n(MAS) is making rapid progress, mainly driven by multi-agent reinforcement\nlearning (MARL) techniques. However, previous MARL methods largely focused on\ngrid-world like or game environments; MAS in visually rich environments has\nremained less explored. To narrow this gap and emphasize the crucial role of\nperception in MAS, we propose a large-scale 3D dataset, CollaVN, for\nmulti-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed\nto cooperatively navigate across photo-realistic environments to reach target\nlocations. Diverse MAVN variants are explored to make our problem more general.\nMoreover, a memory-augmented communication framework is proposed. Each agent is\nequipped with a private, external memory to persistently store communication\ninformation. This allows agents to make better use of their past communication\ninformation, enabling more efficient collaboration and robust long-term\nplanning. In our experiments, several baselines and evaluation metrics are\ndesigned. We also empirically verify the efficacy of our proposed MARL approach\nacross different MAVN task settings.",
          "link": "http://arxiv.org/abs/2107.01151",
          "publishedOn": "2021-07-05T01:54:57.814Z",
          "wordCount": 596,
          "title": "Collaborative Visual Navigation. (arXiv:2107.01151v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanam_Z/0/1/0/all/0/1\">Zeba Khanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usmani_A/0/1/0/all/0/1\">Atiya Usmani</a>",
          "description": "Braille has empowered visually challenged community to read and write. But at\nthe same time, it has created a gap due to widespread inability of non-Braille\nusers to understand Braille scripts. This gap has fuelled researchers to\npropose Optical Braille Recognition techniques to convert Braille documents to\nnatural language. The main motivation of this work is to cement the\ncommunication gap at academic institutions by translating personal documents of\nblind students. This has been accomplished by proposing an economical and\neffective technique which digitizes Braille documents using a smartphone\ncamera. For any given Braille image, a dot detection mechanism based on Hough\ntransform is proposed which is invariant to skewness, noise and other\ndeterrents. The detected dots are then clustered into Braille cells using\ndistance-based clustering algorithm. In succession, the standard physical\nparameters of each Braille cells are estimated for feature extraction and\nclassification as natural language characters. The comprehensive evaluation of\nthis technique on the proposed dataset of 54 Braille scripts has yielded into\naccuracy of 98.71%.",
          "link": "http://arxiv.org/abs/2107.00993",
          "publishedOn": "2021-07-05T01:54:57.793Z",
          "wordCount": 598,
          "title": "Optical Braille Recognition using Circular Hough Transform. (arXiv:2107.00993v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammernik_K/0/1/0/all/0/1\">Kerstin Hammernik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>",
          "description": "Deep learning-based segmentation methods are vulnerable to unforeseen data\ndistribution shifts during deployment, e.g. change of image appearances or\ncontrasts caused by different scanners, unexpected imaging artifacts etc. In\nthis paper, we present a cooperative framework for training image segmentation\nmodels and a latent space augmentation method for generating hard examples.\nBoth contributions improve model generalization and robustness with limited\ndata. The cooperative training framework consists of a fast-thinking network\n(FTN) and a slow-thinking network (STN). The FTN learns decoupled image\nfeatures and shape features for image reconstruction and segmentation tasks.\nThe STN learns shape priors for segmentation correction and refinement. The two\nnetworks are trained in a cooperative manner. The latent space augmentation\ngenerates challenging examples for training by masking the decoupled latent\nspace in both channel-wise and spatial-wise manners. We performed extensive\nexperiments on public cardiac imaging datasets. Using only 10 subjects from a\nsingle site for training, we demonstrated improved cross-site segmentation\nperformance and increased robustness against various unforeseen imaging\nartifacts compared to strong baseline methods. Particularly, cooperative\ntraining with latent space data augmentation yields 15% improvement in terms of\naverage Dice score when compared to a standard training method.",
          "link": "http://arxiv.org/abs/2107.01079",
          "publishedOn": "2021-07-05T01:54:57.734Z",
          "wordCount": 657,
          "title": "Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation. (arXiv:2107.01079v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karmanov_I/0/1/0/all/0/1\">Ilia Karmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanjani_F/0/1/0/all/0/1\">Farhad G. Zanjani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlin_S/0/1/0/all/0/1\">Simone Merlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadampot_I/0/1/0/all/0/1\">Ishaque Kadampot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijkman_D/0/1/0/all/0/1\">Daniel Dijkman</a>",
          "description": "We introduce WiCluster, a new machine learning (ML) approach for passive\nindoor positioning using radio frequency (RF) channel state information (CSI).\nWiCluster can predict both a zone-level position and a precise 2D or 3D\nposition, without using any precise position labels during training. Prior\nCSI-based indoor positioning work has relied on non-parametric approaches using\ndigital signal-processing (DSP) and, more recently, parametric approaches\n(e.g., fully supervised ML methods). However these do not handle the complexity\nof real-world environments well and do not meet requirements for large-scale\ncommercial deployments: the accuracy of DSP-based method deteriorates\nsignificantly in non-line-of-sight conditions, while supervised ML methods need\nlarge amounts of hard-to-acquire centimeter accuracy position labels. In\ncontrast, WiCluster is both precise and requires weaker label-information that\ncan be easily collected. Our first contribution is a novel dimensionality\nreduction method for charting. It combines a triplet-loss with a multi-scale\nclustering-loss to map the high-dimensional CSI representation to a 2D/3D\nlatent space. Our second contribution is two weakly supervised losses that map\nthis latent space into a Cartesian map, resulting in meter-accuracy position\nresults. These losses only require simple to acquire priors: a sketch of the\nfloorplan, approximate location of access-point locations and a few CSI packets\nthat are labeled with the corresponding zone in the floorplan. Thirdly, we\nreport results and a robustness study for 2D positioning in a single-floor\noffice building and 3D positioning in a two-floor home to show the robustness\nof our method.",
          "link": "http://arxiv.org/abs/2107.01002",
          "publishedOn": "2021-07-05T01:54:57.709Z",
          "wordCount": 696,
          "title": "WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise Labels. (arXiv:2107.01002v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafeiropoulos_C/0/1/0/all/0/1\">Charalampos Zafeiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzortzis_I/0/1/0/all/0/1\">Ioannis N. Tzortzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallis_I/0/1/0/all/0/1\">Ioannis Rallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_A/0/1/0/all/0/1\">Anastasios Doulamis</a>",
          "description": "In this paper, we scrutinize the effectiveness of various clustering\ntechniques, investigating their applicability in Cultural Heritage monitoring\napplications. In the context of this paper, we detect the level of\ndecomposition and corrosion on the walls of Saint Nicholas fort in Rhodes\nutilizing hyperspectral images. A total of 6 different clustering approaches\nhave been evaluated over a set of 14 different orthorectified hyperspectral\nimages. Experimental setup in this study involves K-means, Spectral, Meanshift,\nDBSCAN, Birch and Optics algorithms. For each of these techniques we evaluate\nits performance by the use of performance metrics such as Calinski-Harabasz,\nDavies-Bouldin indexes and Silhouette value. In this approach, we evaluate the\noutcomes of the clustering methods by comparing them with a set of annotated\nimages which denotes the ground truth regarding the decomposition and/or\ncorrosion area of the original images. The results depict that a few clustering\ntechniques applied on the given dataset succeeded decent accuracy, precision,\nrecall and f1 scores. Eventually, it was observed that the deterioration was\ndetected quite accurately.",
          "link": "http://arxiv.org/abs/2107.00964",
          "publishedOn": "2021-07-05T01:54:57.695Z",
          "wordCount": 616,
          "title": "Evaluating the Usefulness of Unsupervised monitoring in Cultural Heritage Monuments. (arXiv:2107.00964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00875",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ghamsarian_N/0/1/0/all/0/1\">Negin Ghamsarian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taschwer_M/0/1/0/all/0/1\">Mario Taschwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Putzgruber_Adamitsch_D/0/1/0/all/0/1\">Doris Putzgruber-Adamitsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarny_S/0/1/0/all/0/1\">Stephanie Sarny</a>, <a href=\"http://arxiv.org/find/eess/1/au:+El_Shabrawi_Y/0/1/0/all/0/1\">Yosuf El-Shabrawi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schoeffmann_K/0/1/0/all/0/1\">Klaus Schoeffmann</a>",
          "description": "A critical complication after cataract surgery is the dislocation of the lens\nimplant leading to vision deterioration and eye trauma. In order to reduce the\nrisk of this complication, it is vital to discover the risk factors during the\nsurgery. However, studying the relationship between lens dislocation and its\nsuspicious risk factors using numerous videos is a time-extensive procedure.\nHence, the surgeons demand an automatic approach to enable a larger-scale and,\naccordingly, more reliable study. In this paper, we propose a novel framework\nas the major step towards lens irregularity detection. In particular, we\npropose (I) an end-to-end recurrent neural network to recognize the\nlens-implantation phase and (II) a novel semantic segmentation network to\nsegment the lens and pupil after the implantation phase. The phase recognition\nresults reveal the effectiveness of the proposed surgical phase recognition\napproach. Moreover, the segmentation results confirm the proposed segmentation\nnetwork's effectiveness compared to state-of-the-art rival approaches.",
          "link": "http://arxiv.org/abs/2107.00875",
          "publishedOn": "2021-07-05T01:54:57.683Z",
          "wordCount": 628,
          "title": "LensID: A CNN-RNN-Based Framework Towards Lens Irregularity Detection in Cataract Surgery Videos. (arXiv:2107.00875v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1\">Chen Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Christopher M. Jermaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "We propose {\\rm \\texttt{ResIST}}, a novel distributed training protocol for\nResidual Networks (ResNets). {\\rm \\texttt{ResIST}} randomly decomposes a global\nResNet into several shallow sub-ResNets that are trained independently in a\ndistributed manner for several local iterations, before having their updates\nsynchronized and aggregated into the global model. In the next round, new\nsub-ResNets are randomly generated and the process repeats. By construction,\nper iteration, {\\rm \\texttt{ResIST}} communicates only a small portion of\nnetwork parameters to each machine and never uses the full model during\ntraining. Thus, {\\rm \\texttt{ResIST}} reduces the communication, memory, and\ntime requirements of ResNet training to only a fraction of the requirements of\nprevious methods. In comparison to common protocols like data-parallel training\nand data-parallel training with local SGD, {\\rm \\texttt{ResIST}} yields a\ndecrease in wall-clock training time, while being competitive with respect to\nmodel performance.",
          "link": "http://arxiv.org/abs/2107.00961",
          "publishedOn": "2021-07-05T01:54:57.676Z",
          "wordCount": 593,
          "title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training. (arXiv:2107.00961v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zongsheng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianwen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "While the researches on single image super-resolution (SISR), especially\nequipped with deep neural networks (DNNs), have achieved tremendous successes\nrecently, they still suffer from two major limitations. Firstly, the real image\ndegradation is usually unknown and highly variant from one to another, making\nit extremely hard to train a single model to handle the general SISR task.\nSecondly, most of current methods mainly focus on the downsampling process of\nthe degradation, but ignore or underestimate the inevitable noise\ncontamination. For example, the commonly-used independent and identically\ndistributed (i.i.d.) Gaussian noise distribution always largely deviates from\nthe real image noise (e.g., camera sensor noise), which limits their\nperformance in real scenarios. To address these issues, this paper proposes a\nmodel-based unsupervised SISR method to deal with the general SISR task with\nunknown degradations. Instead of the traditional i.i.d. Gaussian noise\nassumption, a novel patch-based non-i.i.d. noise modeling method is proposed to\nfit the complex real noise. Besides, a deep generator parameterized by a DNN is\nused to map the latent variable to the high-resolution image, and the\nconventional hyper-Laplacian prior is also elaborately embedded into such\ngenerator to further constrain the image gradients. Finally, a Monte Carlo EM\nalgorithm is designed to solve our model, which provides a general inference\nframework to update the image generator both w.r.t. the latent variable and the\nnetwork parameters. Comprehensive experiments demonstrate that the proposed\nmethod can evidently surpass the current state of the art (SotA) method (about\n1dB PSNR) not only with a slighter model (0.34M vs. 2.40M) but also faster\nspeed.",
          "link": "http://arxiv.org/abs/2107.00986",
          "publishedOn": "2021-07-05T01:54:57.667Z",
          "wordCount": 696,
          "title": "Unsupervised Single Image Super-resolution Under Complex Noise. (arXiv:2107.00986v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reynaud_H/0/1/0/all/0/1\">Hadrien Reynaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlontzos_A/0/1/0/all/0/1\">Athanasios Vlontzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Benjamin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beqiri_A/0/1/0/all/0/1\">Arian Beqiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leeson_P/0/1/0/all/0/1\">Paul Leeson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>",
          "description": "Cardiac ultrasound imaging is used to diagnose various heart diseases. Common\nanalysis pipelines involve manual processing of the video frames by expert\nclinicians. This suffers from intra- and inter-observer variability. We propose\na novel approach to ultrasound video analysis using a transformer architecture\nbased on a Residual Auto-Encoder Network and a BERT model adapted for token\nclassification. This enables videos of any length to be processed. We apply our\nmodel to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection\nand the automated computation of the left ventricular ejection fraction. We\nachieve an average frame distance of 3.36 frames for the ES and 7.17 frames for\nthe ED on videos of arbitrary length. Our end-to-end learnable approach can\nestimate the ejection fraction with a MAE of 5.95 and $R^2$ of 0.52 in 0.15s\nper video, showing that segmentation is not the only way to predict ejection\nfraction. Code and models are available at https://github.com/HReynaud/UVT.",
          "link": "http://arxiv.org/abs/2107.00977",
          "publishedOn": "2021-07-05T01:54:57.648Z",
          "wordCount": 602,
          "title": "Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation. (arXiv:2107.00977v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyung_E/0/1/0/all/0/1\">Eunyoung Hyung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Despite the success of recent Neural Architecture Search (NAS) methods on\nvarious tasks which have shown to output networks that largely outperform\nhuman-designed networks, conventional NAS methods have mostly tackled the\noptimization of searching for the network architecture for a single task\n(dataset), which does not generalize well across multiple tasks (datasets).\nMoreover, since such task-specific methods search for a neural architecture\nfrom scratch for every given task, they incur a large computational cost, which\nis problematic when the time and monetary budget are limited. In this paper, we\npropose an efficient NAS framework that is trained once on a database\nconsisting of datasets and pretrained networks and can rapidly search for a\nneural architecture for a novel dataset. The proposed MetaD2A (Meta\nDataset-to-Architecture) model can stochastically generate graphs\n(architectures) from a given set (dataset) via a cross-modal latent space\nlearned with amortized meta-learning. Moreover, we also propose a\nmeta-performance predictor to estimate and select the best architecture without\ndirect training on target datasets. The experimental results demonstrate that\nour model meta-learned on subsets of ImageNet-1K and architectures from\nNAS-Bench 201 search space successfully generalizes to multiple unseen datasets\nincluding CIFAR-10 and CIFAR-100, with an average search time of 33 GPU\nseconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than\nNSGANetV2, a transferable NAS method, with comparable performance. We believe\nthat the MetaD2A proposes a new research direction for rapid NAS as well as\nways to utilize the knowledge from rich databases of datasets and architectures\naccumulated over the past years. Code is available at\nhttps://github.com/HayeonLee/MetaD2A.",
          "link": "http://arxiv.org/abs/2107.00860",
          "publishedOn": "2021-07-05T01:54:57.638Z",
          "wordCount": 705,
          "title": "Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets. (arXiv:2107.00860v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaodi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Q/0/1/0/all/0/1\">Qi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>",
          "description": "Weak supervision learning on classification labels has demonstrated high\nperformance in various tasks. When a few pixel-level fine annotations are also\naffordable, it is natural to leverage both of the pixel-level (e.g.,\nsegmentation) and image level (e.g., classification) annotation to further\nimprove the performance. In computational pathology, however, such weak or\nmixed supervision learning is still a challenging task, since the high\nresolution of whole slide images makes it unattainable to perform end-to-end\ntraining of classification models. An alternative approach is to analyze such\ndata by patch-base model training, i.e., using self-supervised learning to\ngenerate pixel-level pseudo labels for patches. However, such methods usually\nhave model drifting issues, i.e., hard to converge, because the noise\naccumulates during the self-training process. To handle those problems, we\npropose a mixed supervision learning framework for super high-resolution images\nto effectively utilize their various labels (e.g., sufficient image-level\ncoarse annotations and a few pixel-level fine labels). During the patch\ntraining stage, this framework can make use of coarse image-level labels to\nrefine self-supervised learning and generate high-quality pixel-level pseudo\nlabels. A comprehensive strategy is proposed to suppress pixel-level false\npositives and false negatives. Three real-world datasets with very large number\nof images (i.e., more than 10,000 whole slide images) and various types of\nlabels are used to evaluate the effectiveness of mixed supervision learning. We\nreduced the false positive rate by around one third compared to state of the\nart while retaining 100\\% sensitivity, in the task of image-level\nclassification.",
          "link": "http://arxiv.org/abs/2107.00934",
          "publishedOn": "2021-07-05T01:54:57.630Z",
          "wordCount": 690,
          "title": "Mixed Supervision Learning for Whole Slide Image Classification. (arXiv:2107.00934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01063",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yibao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xingru Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>",
          "description": "The classification of histopathological images is of great value in both\ncancer diagnosis and pathological studies. However, multiple reasons, such as\nvariations caused by magnification factors and class imbalance, make it a\nchallenging task where conventional methods that learn from image-label\ndatasets perform unsatisfactorily in many cases. We observe that tumours of the\nsame class often share common morphological patterns. To exploit this fact, we\npropose an approach that learns similarity-based multi-scale embeddings (SMSE)\nfor magnification-independent histopathological image classification. In\nparticular, a pair loss and a triplet loss are leveraged to learn\nsimilarity-based embeddings from image pairs or image triplets. The learned\nembeddings provide accurate measurements of similarities between images, which\nare regarded as a more effective form of representation for histopathological\nmorphology than normal image features. Furthermore, in order to ensure the\ngenerated models are magnification-independent, images acquired at different\nmagnification factors are simultaneously fed to networks during training for\nlearning multi-scale embeddings. In addition to the SMSE, to eliminate the\nimpact of class imbalance, instead of using the hard sample mining strategy\nthat intuitively discards some easy samples, we introduce a new reinforced\nfocal loss to simultaneously punish hard misclassified samples while\nsuppressing easy well-classified samples. Experimental results show that the\nSMSE improves the performance for histopathological image classification tasks\nfor both breast and liver cancers by a large margin compared to previous\nmethods. In particular, the SMSE achieves the best performance on the BreakHis\nbenchmark with an improvement ranging from 5% to 18% compared to previous\nmethods using traditional features.",
          "link": "http://arxiv.org/abs/2107.01063",
          "publishedOn": "2021-07-05T01:54:57.623Z",
          "wordCount": 690,
          "title": "Magnification-independent Histopathological Image Classification with Similarity-based Multi-scale Embeddings. (arXiv:2107.01063v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cote_Allard_U/0/1/0/all/0/1\">Ulysse C&#xf4;t&#xe9;-Allard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakobsen_P/0/1/0/all/0/1\">Petter Jakobsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stautland_A/0/1/0/all/0/1\">Andrea Stautland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nordgreen_T/0/1/0/all/0/1\">Tine Nordgreen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasmer_O/0/1/0/all/0/1\">Ole Bernt Fasmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oedegaard_K/0/1/0/all/0/1\">Ketil Joachim Oedegaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresen_J/0/1/0/all/0/1\">Jim Torresen</a>",
          "description": "Manic episodes of bipolar disorder can lead to uncritical behaviour and\ndelusional psychosis, often with destructive consequences for those affected\nand their surroundings. Early detection and intervention of a manic episode are\ncrucial to prevent escalation, hospital admission and premature death. However,\npeople with bipolar disorder may not recognize that they are experiencing a\nmanic episode and symptoms such as euphoria and increased productivity can also\ndeter affected individuals from seeking help. This work proposes to perform\nuser-independent, automatic mood-state detection based on actigraphy and\nelectrodermal activity acquired from a wrist-worn device during mania and after\nrecovery (euthymia). This paper proposes a new deep learning-based ensemble\nmethod leveraging long (20h) and short (5 minutes) time-intervals to\ndiscriminate between the mood-states. When tested on 47 bipolar patients, the\nproposed classification scheme achieves an average accuracy of 91.59% in\neuthymic/manic mood-state recognition.",
          "link": "http://arxiv.org/abs/2107.00710",
          "publishedOn": "2021-07-05T01:54:57.615Z",
          "wordCount": 609,
          "title": "Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hampali_S/0/1/0/all/0/1\">Shreyas Hampali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sayan Deb Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>",
          "description": "HO-3D is a dataset providing image sequences of various hand-object\ninteraction scenarios annotated with the 3D pose of the hand and the object and\nwas originally introduced as HO-3D_v2. The annotations were obtained\nautomatically using an optimization method, 'HOnnotate', introduced in the\noriginal paper. HO-3D_v3 provides more accurate annotations for both the hand\nand object poses thus resulting in better estimates of contact regions between\nthe hand and the object. In this report, we elaborate on the improvements to\nthe HOnnotate method and provide evaluations to compare the accuracy of\nHO-3D_v2 and HO-3D_v3. HO-3D_v3 results in 4mm higher accuracy compared to\nHO-3D_v2 for hand poses while exhibiting higher contact regions with the object\nsurface.",
          "link": "http://arxiv.org/abs/2107.00887",
          "publishedOn": "2021-07-05T01:54:57.597Z",
          "wordCount": 559,
          "title": "HO-3D_v3: Improving the Accuracy of Hand-Object Annotations of the HO-3D Dataset. (arXiv:2107.00887v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_S/0/1/0/all/0/1\">Shintaro Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>",
          "description": "Currently, domestic service robots have an insufficient ability to interact\nnaturally through language. This is because understanding human instructions is\ncomplicated by various ambiguities and missing information. In existing\nmethods, the referring expressions that specify the relationships between\nobjects are insufficiently modeled. In this paper, we propose Target-dependent\nUNITER, which learns the relationship between the target object and other\nobjects directly by focusing on the relevant regions within an image, rather\nthan the whole image. Our method is an extension of the UNITER-based\nTransformer that can be pretrained on general-purpose datasets. We extend the\nUNITER approach by introducing a new architecture for handling the target\ncandidates. Our model is validated on two standard datasets, and the results\nshow that Target-dependent UNITER outperforms the baseline method in terms of\nclassification accuracy.",
          "link": "http://arxiv.org/abs/2107.00811",
          "publishedOn": "2021-07-05T01:54:57.588Z",
          "wordCount": 580,
          "title": "Target-dependent UNITER: A Transformer-Based Multimodal Language Comprehension Model for Domestic Service Robots. (arXiv:2107.00811v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kambara_M/0/1/0/all/0/1\">Motonari Kambara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>",
          "description": "There have been many studies in robotics to improve the communication skills\nof domestic service robots. Most studies, however, have not fully benefited\nfrom recent advances in deep neural networks because the training datasets are\nnot large enough. In this paper, our aim is to augment the datasets based on a\ncrossmodal language generation model. We propose the Case Relation Transformer\n(CRT), which generates a fetching instruction sentence from an image, such as\n\"Move the blue flip-flop to the lower left box.\" Unlike existing methods, the\nCRT uses the Transformer to integrate the visual features and geometry features\nof objects in the image. The CRT can handle the objects because of the Case\nRelation Block. We conducted comparison experiments and a human evaluation. The\nexperimental results show the CRT outperforms baseline methods.",
          "link": "http://arxiv.org/abs/2107.00789",
          "publishedOn": "2021-07-05T01:54:57.581Z",
          "wordCount": 580,
          "title": "Case Relation Transformer: A Crossmodal Language Generation Model for Fetching Instructions. (arXiv:2107.00789v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruzhansky_M/0/1/0/all/0/1\">Michael Ruzhansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haihui Wang</a>",
          "description": "This paper presents a novel intrinsic image transfer (IIT) algorithm for\nillumination manipulation, which creates a local image translation between two\nillumination surfaces. This model is built on an optimization-based framework\nconsisting of three photo-realistic losses defined on the sub-layers factorized\nby an intrinsic image decomposition. We illustrate that all losses can be\nreduced without the necessity of taking an intrinsic image decomposition under\nthe well-known spatial-varying illumination illumination-invariant reflectance\nprior knowledge. Moreover, with a series of relaxations, all of them can be\ndirectly defined on images, giving a closed-form solution for image\nillumination manipulation. This new paradigm differs from the prevailing\nRetinex-based algorithms, as it provides an implicit way to deal with the\nper-pixel image illumination. We finally demonstrate its versatility and\nbenefits to the illumination-related tasks such as illumination compensation,\nimage enhancement, and high dynamic range (HDR) image compression, and show the\nhigh-quality results on natural image datasets.",
          "link": "http://arxiv.org/abs/2107.00704",
          "publishedOn": "2021-07-05T01:54:57.572Z",
          "wordCount": 581,
          "title": "Intrinsic Image Transfer for Illumination Manipulation. (arXiv:2107.00704v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoni Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>",
          "description": "Hierarchical classification is significant for complex tasks by providing\nmulti-granular predictions and encouraging better mistakes. As the label\nstructure decides its performance, many existing approaches attempt to\nconstruct an excellent label structure for promoting the classification\nresults. In this paper, we consider that different label structures provide a\nvariety of prior knowledge for category recognition, thus fusing them is\nhelpful to achieve better hierarchical classification results. Furthermore, we\npropose a multi-task multi-structure fusion model to integrate different label\nstructures. It contains two kinds of branches: one is the traditional\nclassification branch to classify the common subclasses, the other is\nresponsible for identifying the heterogeneous superclasses defined by different\nlabel structures. Besides the effect of multiple label structures, we also\nexplore the architecture of the deep model for better hierachical\nclassification and adjust the hierarchical evaluation metrics for multiple\nlabel structures. Experimental results on CIFAR100 and Car196 show that our\nmethod obtains significantly better results than using a flat classifier or a\nhierarchical classifier with any single label structure.",
          "link": "http://arxiv.org/abs/2107.00808",
          "publishedOn": "2021-07-05T01:54:57.476Z",
          "wordCount": 609,
          "title": "MMF: Multi-Task Multi-Structure Fusion for Hierarchical Image Classification. (arXiv:2107.00808v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shanu Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod Kumar Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Praphul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P Namboodiri</a>",
          "description": "Understanding unsupervised domain adaptation has been an important task that\nhas been well explored. However, the wide variety of methods have not analyzed\nthe role of a classifier's performance in detail. In this paper, we thoroughly\nexamine the role of a classifier in terms of matching source and target\ndistributions. We specifically investigate the classifier ability by matching\na) the distribution of features, b) probabilistic uncertainty for samples and\nc) certainty activation mappings. Our analysis suggests that using these three\ndistributions does result in a consistently improved performance on all the\ndatasets. Our work thus extends present knowledge on the role of the various\ndistributions obtained from the classifier towards solving unsupervised domain\nadaptation.",
          "link": "http://arxiv.org/abs/2107.00727",
          "publishedOn": "2021-07-05T01:54:57.469Z",
          "wordCount": 552,
          "title": "Mitigating Uncertainty of Classifier for Unsupervised Domain Adaptation. (arXiv:2107.00727v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rebol_M/0/1/0/all/0/1\">Manuel Rebol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutl_C/0/1/0/all/0/1\">Christian G&#xfc;tl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietroszek_K/0/1/0/all/0/1\">Krzysztof Pietroszek</a>",
          "description": "In real life, people communicate using both speech and non-verbal signals\nsuch as gestures, face expression or body pose. Non-verbal signals impact the\nmeaning of the spoken utterance in an abundance of ways. An absence of\nnon-verbal signals impoverishes the process of communication. Yet, when users\nare represented as avatars, it is difficult to translate non-verbal signals\nalong with the speech into the virtual world without specialized motion-capture\nhardware. In this paper, we propose a novel, data-driven technique for\ngenerating gestures directly from speech. Our approach is based on the\napplication of Generative Adversarial Neural Networks (GANs) to model the\ncorrelation rather than causation between speech and gestures. This approach\napproximates neuroscience findings on how non-verbal communication and speech\nare correlated. We create a large dataset which consists of speech and\ncorresponding gestures in a 3D human pose format from which our model learns\nthe speaker-specific correlation. We evaluate the proposed technique in a user\nstudy that is inspired by the Turing test. For the study, we animate the\ngenerated gestures on a virtual character. We find that users are not able to\ndistinguish between the generated and the recorded gestures. Moreover, users\nare able to identify our synthesized gestures as related or not related to a\ngiven utterance.",
          "link": "http://arxiv.org/abs/2107.00712",
          "publishedOn": "2021-07-05T01:54:57.461Z",
          "wordCount": 662,
          "title": "Passing a Non-verbal Turing Test: Evaluating Gesture Animations Generated from Speech. (arXiv:2107.00712v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengcheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lingqiao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>",
          "description": "In this technical report, we briefly introduce the solution of our team\n\"TAL-ai\" for (Semi-) supervised Face detection in the low light condition in\nUG2+ Challenge in CVPR 2021. By conducting several experiments with popular\nimage enhancement methods and image transfer methods, we pulled the low light\nimage and the normal image to a more closer domain. And it is observed that\nusing these data to training can achieve better performance. We also adapt\nseveral popular object detection frameworks, e.g., DetectoRS, Cascade-RCNN, and\nlarge backbone like Swin-transformer. Finally, we ensemble several models which\nachieved mAP 74.89 on the testing set, ranking 1st on the final leaderboard.",
          "link": "http://arxiv.org/abs/2107.00818",
          "publishedOn": "2021-07-05T01:54:57.455Z",
          "wordCount": 559,
          "title": "1st Place Solutions for UG2+ Challenge 2021 -- (Semi-)supervised Face detection in the low light condition. (arXiv:2107.00818v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Glaser_N/0/1/0/all/0/1\">Nathaniel Glaser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yen-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "In this paper, we address the multi-robot collaborative perception problem,\nspecifically in the context of multi-view infilling for distributed semantic\nsegmentation. This setting entails several real-world challenges, especially\nthose relating to unregistered multi-agent image data. Solutions must\neffectively leverage multiple, non-static, and intermittently-overlapping RGB\nperspectives. To this end, we propose the Multi-Agent Infilling Network: an\nextensible neural architecture that can be deployed (in a distributed manner)\nto each agent in a robotic swarm. Specifically, each robot is in charge of\nlocally encoding and decoding visual information, and an extensible neural\nmechanism allows for an uncertainty-aware and context-based exchange of\nintermediate features. We demonstrate improved performance on a realistic\nmulti-robot AirSim dataset.",
          "link": "http://arxiv.org/abs/2107.00769",
          "publishedOn": "2021-07-05T01:54:57.448Z",
          "wordCount": 572,
          "title": "Enhancing Multi-Robot Perception via Learned Data Association. (arXiv:2107.00769v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kothawade_S/0/1/0/all/0/1\">Suraj Kothawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1\">Nathan Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1\">Krishnateja Killamsetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "Active learning has proven to be useful for minimizing labeling costs by\nselecting the most informative samples. However, existing active learning\nmethods do not work well in realistic scenarios such as imbalance or rare\nclasses, out-of-distribution data in the unlabeled set, and redundancy. In this\nwork, we propose SIMILAR (Submodular Information Measures based actIve\nLeARning), a unified active learning framework using recently proposed\nsubmodular information measures (SIM) as acquisition functions. We argue that\nSIMILAR not only works in standard active learning, but also easily extends to\nthe realistic settings considered above and acts as a one-stop solution for\nactive learning that is scalable to large real-world datasets. Empirically, we\nshow that SIMILAR significantly outperforms existing active learning algorithms\nby as much as ~5% - 18% in the case of rare classes and ~5% - 10% in the case\nof out-of-distribution data on several image classification tasks like\nCIFAR-10, MNIST, and ImageNet.",
          "link": "http://arxiv.org/abs/2107.00717",
          "publishedOn": "2021-07-05T01:54:57.432Z",
          "wordCount": 592,
          "title": "SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios. (arXiv:2107.00717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mirsadeghi_S/0/1/0/all/0/1\">S. Ehsan Mirsadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Royat_A/0/1/0/all/0/1\">Ali Royat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>",
          "description": "Semantic segmentation is one of the basic, yet essential scene understanding\ntasks for an autonomous agent. The recent developments in supervised machine\nlearning and neural networks have enjoyed great success in enhancing the\nperformance of the state-of-the-art techniques for this task. However, their\nsuperior performance is highly reliant on the availability of a large-scale\nannotated dataset. In this paper, we propose a novel fully unsupervised\nsemantic segmentation method, the so-called Information Maximization and\nAdversarial Regularization Segmentation (InMARS). Inspired by human perception\nwhich parses a scene into perceptual groups, rather than analyzing each pixel\nindividually, our proposed approach first partitions an input image into\nmeaningful regions (also known as superpixels). Next, it utilizes\nMutual-Information-Maximization followed by an adversarial training strategy to\ncluster these regions into semantically meaningful classes. To customize an\nadversarial training scheme for the problem, we incorporate adversarial pixel\nnoise along with spatial perturbations to impose photometrical and geometrical\ninvariance on the deep neural network. Our experiments demonstrate that our\nmethod achieves the state-of-the-art performance on two commonly used\nunsupervised semantic segmentation datasets, COCO-Stuff, and Potsdam.",
          "link": "http://arxiv.org/abs/2107.00691",
          "publishedOn": "2021-07-05T01:54:57.425Z",
          "wordCount": 623,
          "title": "Unsupervised Image Segmentation by Mutual Information Maximization and Adversarial Regularization. (arXiv:2107.00691v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>",
          "description": "Transformer architecture has emerged to be successful in a number of natural\nlanguage processing tasks. However, its applications to medical vision remain\nlargely unexplored. In this study, we present UTNet, a simple yet powerful\nhybrid Transformer architecture that integrates self-attention into a\nconvolutional neural network for enhancing medical image segmentation. UTNet\napplies self-attention modules in both encoder and decoder for capturing\nlong-range dependency at different scales with minimal overhead. To this end,\nwe propose an efficient self-attention mechanism along with relative position\nencoding that reduces the complexity of self-attention operation significantly\nfrom $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also\nproposed to recover fine-grained details from the skipped connections in the\nencoder. Our approach addresses the dilemma that Transformer requires huge\namounts of data to learn vision inductive bias. Our hybrid layer design allows\nthe initialization of Transformer into convolutional networks without a need of\npre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac\nmagnetic resonance imaging cohort. UTNet demonstrates superior segmentation\nperformance and robustness against the state-of-the-art approaches, holding the\npromise to generalize well on other medical image segmentations.",
          "link": "http://arxiv.org/abs/2107.00781",
          "publishedOn": "2021-07-05T01:54:57.418Z",
          "wordCount": 623,
          "title": "UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation. (arXiv:2107.00781v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huajun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fuqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xinyi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>",
          "description": "Pixel-wise regression is probably the most common problem in fine-grained\ncomputer vision tasks, such as estimating keypoint heatmaps and segmentation\nmasks. These regression problems are very challenging particularly because they\nrequire, at low computation overheads, modeling long-range dependencies on\nhigh-resolution inputs/outputs to estimate the highly nonlinear pixel-wise\nsemantics. While attention mechanisms in Deep Convolutional Neural\nNetworks(DCNNs) has become popular for boosting long-range dependencies,\nelement-specific attention, such as Nonlocal blocks, is highly complex and\nnoise-sensitive to learn, and most of simplified attention hybrids try to reach\nthe best compromise among multiple types of tasks. In this paper, we present\nthe Polarized Self-Attention(PSA) block that incorporates two critical designs\ntowards high-quality pixel-wise regression: (1) Polarized filtering: keeping\nhigh internal resolution in both channel and spatial attention computation\nwhile completely collapsing input tensors along their counterpart dimensions.\n(2) Enhancement: composing non-linearity that directly fits the output\ndistribution of typical fine-grained regression, such as the 2D Gaussian\ndistribution (keypoint heatmaps), or the 2D Binormial distribution (binary\nsegmentation masks). PSA appears to have exhausted the representation capacity\nwithin its channel-only and spatial-only branches, such that there is only\nmarginal metric differences between its sequential and parallel layouts.\nExperimental results show that PSA boosts standard baselines by $2-4$ points,\nand boosts state-of-the-arts by $1-2$ points on 2D pose estimation and semantic\nsegmentation benchmarks.",
          "link": "http://arxiv.org/abs/2107.00782",
          "publishedOn": "2021-07-05T01:54:57.411Z",
          "wordCount": 648,
          "title": "Polarized Self-Attention: Towards High-quality Pixel-wise Regression. (arXiv:2107.00782v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>",
          "description": "Image super-resolution (SR) research has witnessed impressive progress thanks\nto the advance of convolutional neural networks (CNNs) in recent years.\nHowever, most existing SR methods are non-blind and assume that degradation has\na single fixed and known distribution (e.g., bicubic) which struggle while\nhandling degradation in real-world data that usually follows a multi-modal,\nspatially variant, and unknown distribution. The recent blind SR studies\naddress this issue via degradation estimation, but they do not generalize well\nto multi-source degradation and cannot handle spatially variant degradation. We\ndesign CRL-SR, a contrastive representation learning network that focuses on\nblind SR of images with multi-modal and spatially variant distributions. CRL-SR\naddresses the blind SR challenges from two perspectives. The first is\ncontrastive decoupling encoding which introduces contrastive learning to\nextract resolution-invariant embedding and discard resolution-variant embedding\nunder the guidance of a bidirectional contrastive loss. The second is\ncontrastive feature refinement which generates lost or corrupted high-frequency\ndetails under the guidance of a conditional contrastive loss. Extensive\nexperiments on synthetic datasets and real images show that the proposed CRL-SR\ncan handle multi-modal and spatially variant degradation effectively under\nblind settings and it also outperforms state-of-the-art SR methods\nqualitatively and quantitatively.",
          "link": "http://arxiv.org/abs/2107.00708",
          "publishedOn": "2021-07-05T01:54:57.395Z",
          "wordCount": 629,
          "title": "Blind Image Super-Resolution via Contrastive Representation Learning. (arXiv:2107.00708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Glaser_N/0/1/0/all/0/1\">Nathaniel Glaser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yen-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "In this paper, we address bandwidth-limited and obstruction-prone\ncollaborative perception, specifically in the context of multi-agent semantic\nsegmentation. This setting presents several key challenges, including\nprocessing and exchanging unregistered robotic swarm imagery. To be successful,\nsolutions must effectively leverage multiple non-static and\nintermittently-overlapping RGB perspectives, while heeding bandwidth\nconstraints and overcoming unwanted foreground obstructions. As such, we\npropose an end-to-end learn-able Multi-Agent Spatial Handshaking network (MASH)\nto process, compress, and propagate visual information across a robotic swarm.\nOur distributed communication module operates directly (and exclusively) on raw\nimage data, without additional input requirements such as pose, depth, or\nwarping data. We demonstrate superior performance of our model compared against\nseveral baselines in a photo-realistic multi-robot AirSim environment,\nespecially in the presence of image occlusions. Our method achieves an absolute\n11% IoU improvement over strong baselines.",
          "link": "http://arxiv.org/abs/2107.00771",
          "publishedOn": "2021-07-05T01:54:57.385Z",
          "wordCount": 576,
          "title": "Overcoming Obstructions via Bandwidth-Limited Multi-Agent Spatial Handshaking. (arXiv:2107.00771v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngjoo Kim</a>",
          "description": "This paper proposes a novel approach to map-based navigation system for\nunmanned aircraft. The proposed system attempts label-to-label matching, not\nimage-to-image matching between aerial images and a map database. By using\nsemantic segmentation, the ground objects are labelled and the configuration of\nthe objects is used to find the corresponding location in the map database. The\nuse of the deep learning technique as a tool for extracting high-level features\nreduces the image-based localization problem to a pattern matching problem.\nThis paper proposes a pattern matching algorithm which does not require\naltitude information or a camera model to estimate the absolute horizontal\nposition. The feasibility analysis with simulated images shows the proposed\nmap-based navigation can be realized with the proposed pattern matching\nalgorithm and it is able to provide positions given the labelled objects.",
          "link": "http://arxiv.org/abs/2107.00689",
          "publishedOn": "2021-07-05T01:54:57.312Z",
          "wordCount": 572,
          "title": "Aerial Map-Based Navigation Using Semantic Segmentation and Pattern Matching. (arXiv:2107.00689v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/1705.03439",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1\">David M. Blei</a>",
          "description": "A key challenge for modern Bayesian statistics is how to perform scalable\ninference of posterior distributions. To address this challenge, variational\nBayes (VB) methods have emerged as a popular alternative to the classical\nMarkov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while\nachieving comparable predictive performance. However, there are few theoretical\nresults around VB. In this paper, we establish frequentist consistency and\nasymptotic normality of VB methods. Specifically, we connect VB methods to\npoint estimates based on variational approximations, called frequentist\nvariational approximations, and we use the connection to prove a variational\nBernstein-von Mises theorem. The theorem leverages the theoretical\ncharacterizations of frequentist variational approximations to understand\nasymptotic properties of VB. In summary, we prove that (1) the VB posterior\nconverges to the Kullback-Leibler (KL) minimizer of a normal distribution,\ncentered at the truth and (2) the corresponding variational expectation of the\nparameter is consistent and asymptotically normal. As applications of the\ntheorem, we derive asymptotic properties of VB posteriors in Bayesian mixture\nmodels, Bayesian generalized linear mixed models, and Bayesian stochastic block\nmodels. We conduct a simulation study to illustrate these theoretical results.",
          "link": "http://arxiv.org/abs/1705.03439",
          "publishedOn": "2021-07-09T01:58:29.204Z",
          "wordCount": 658,
          "title": "Frequentist Consistency of Variational Bayes. (arXiv:1705.03439v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1\">Akshay Mehra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamm_J/0/1/0/all/0/1\">Jihun Hamm</a>",
          "description": "Unsupervised domain adaptation (UDA) enables cross-domain learning without\ntarget domain labels by transferring knowledge from a labeled source domain\nwhose distribution differs from the target. However, UDA is not always\nsuccessful and several accounts of \"negative transfer\" have been reported in\nthe literature. In this work, we prove a simple lower bound on the target\ndomain error that complements the existing upper bound. Our bound shows the\ninsufficiency of minimizing source domain error and marginal distribution\nmismatch for a guaranteed reduction in the target domain error, due to the\npossible increase of induced labeling function mismatch. This insufficiency is\nfurther illustrated through simple distributions for which the same UDA\napproach succeeds, fails, and may succeed or fail with an equal chance.\nMotivated from this, we propose novel data poisoning attacks to fool UDA\nmethods into learning representations that produce large target domain errors.\nWe evaluate the effect of these attacks on popular UDA methods using benchmark\ndatasets where they have been previously shown to be successful. Our results\nshow that poisoning can significantly decrease the target domain accuracy,\ndropping it to almost 0\\% in some cases, with the addition of only 10\\%\npoisoned data in the source domain. The failure of UDA methods demonstrates the\nlimitations of UDA at guaranteeing cross-domain generalization consistent with\nthe lower bound. Thus, evaluation of UDA methods in adversarial settings such\nas data poisoning can provide a better sense of their robustness in scenarios\nunfavorable for UDA.",
          "link": "http://arxiv.org/abs/2107.03919",
          "publishedOn": "2021-07-09T01:58:29.191Z",
          "wordCount": 675,
          "title": "Understanding the Limits of Unsupervised Domain Adaptation via Data Poisoning. (arXiv:2107.03919v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03985",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shor_J/0/1/0/all/0/1\">Joel Shor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plakal_M/0/1/0/all/0/1\">Manoj Plakal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tobin_J/0/1/0/all/0/1\">Jimmy Tobin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Green_J/0/1/0/all/0/1\">Jordan R. Green</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brenner_M/0/1/0/all/0/1\">Michael P. Brenner</a>",
          "description": "Automatic classification of disordered speech can provide an objective tool\nfor identifying the presence and severity of speech impairment. Classification\napproaches can also help identify hard-to-recognize speech samples to teach ASR\nsystems about the variable manifestations of impaired speech. Here, we develop\nand compare different deep learning techniques to classify the intelligibility\nof disordered speech on selected phrases. We collected samples from a diverse\nset of 661 speakers with a variety of self-reported disorders speaking 29 words\nor phrases, which were rated by speech-language pathologists for their overall\nintelligibility using a five-point Likert scale. We then evaluated classifiers\ndeveloped using 3 approaches: (1) a convolutional neural network (CNN) trained\nfor the task, (2) classifiers trained on non-semantic speech representations\nfrom CNNs that used an unsupervised objective [1], and (3) classifiers trained\non the acoustic (encoder) embeddings from an ASR system trained on typical\nspeech [2]. We found that the ASR encoder's embeddings considerably outperform\nthe other two on detecting and classifying disordered speech. Further analysis\nshows that the ASR embeddings cluster speech by the spoken phrase, while the\nnon-semantic embeddings cluster speech by speaker. Also, longer phrases are\nmore indicative of intelligibility deficits than single words.",
          "link": "http://arxiv.org/abs/2107.03985",
          "publishedOn": "2021-07-09T01:58:29.177Z",
          "wordCount": 669,
          "title": "Comparing Supervised Models And Learned Speech Representations For Classifying Intelligibility Of Disordered Speech On Selected Phrases. (arXiv:2107.03985v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1\">Vincent Sitzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Pulkit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>",
          "description": "Humans have a strong intuitive understanding of the 3D environment around us.\nThe mental model of the physics in our brain applies to objects of different\nmaterials and enables us to perform a wide range of manipulation tasks that are\nfar beyond the reach of current robots. In this work, we desire to learn models\nfor dynamic 3D scenes purely from 2D visual observations. Our model combines\nNeural Radiance Fields (NeRF) and time contrastive learning with an\nautoencoding framework, which learns viewpoint-invariant 3D-aware scene\nrepresentations. We show that a dynamics model, constructed over the learned\nrepresentation space, enables visuomotor control for challenging manipulation\ntasks involving both rigid bodies and fluids, where the target is specified in\na viewpoint different from what the robot operates on. When coupled with an\nauto-decoding framework, it can even support goal specification from camera\nviewpoints that are outside the training distribution. We further demonstrate\nthe richness of the learned 3D dynamics model by performing future prediction\nand novel view synthesis. Finally, we provide detailed ablation studies\nregarding different system designs and qualitative analysis of the learned\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.04004",
          "publishedOn": "2021-07-09T01:58:29.146Z",
          "wordCount": 632,
          "title": "3D Neural Scene Representations for Visuomotor Control. (arXiv:2107.04004v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sibendu Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Kunal Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coviello_G/0/1/0/all/0/1\">Giuseppe Coviello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaradas_M/0/1/0/all/0/1\">Murugan Sankaradas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_O/0/1/0/all/0/1\">Oliver Po</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Y. Charlie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1\">Srimat T. Chakradhar</a>",
          "description": "Complex sensors like video cameras include tens of configurable parameters,\nwhich can be set by end-users to customize the sensors to specific application\nscenarios. Although parameter settings significantly affect the quality of the\nsensor output and the accuracy of insights derived from sensor data, most\nend-users use a fixed parameter setting because they lack the skill or\nunderstanding to appropriately configure these parameters. We propose CamTuner,\nwhich is a system to automatically, and dynamically adapt the complex sensor to\nchanging environments. CamTuner includes two key components. First, a bespoke\nanalytics quality estimator, which is a deep-learning model to automatically\nand continuously estimate the quality of insights from an analytics unit as the\nenvironment around a sensor change. Second, a reinforcement learning (RL)\nmodule, which reacts to the changes in quality, and automatically adjusts the\ncamera parameters to enhance the accuracy of insights. We improve the training\ntime of the RL module by an order of magnitude by designing virtual models to\nmimic essential behavior of the camera: we design virtual knobs that can be set\nto different values to mimic the effects of assigning different values to the\ncamera's configurable parameters, and we design a virtual camera model that\nmimics the output from a video camera at different times of the day. These\nvirtual models significantly accelerate training because (a) frame rates from a\nreal camera are limited to 25-30 fps while the virtual models enable processing\nat 300 fps, (b) we do not have to wait until the real camera sees different\nenvironments, which could take weeks or months, and (c) virtual knobs can be\nupdated instantly, while it can take 200-500 ms to change the camera parameter\nsettings. Our dynamic tuning approach results in up to 12% improvement in the\naccuracy of insights from several video analytics tasks.",
          "link": "http://arxiv.org/abs/2107.03964",
          "publishedOn": "2021-07-09T01:58:29.130Z",
          "wordCount": 751,
          "title": "CamTuner: Reinforcement-Learning based System for Camera Parameter Tuning to enhance Analytics. (arXiv:2107.03964v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_A/0/1/0/all/0/1\">Alexander Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nosovskiy_G/0/1/0/all/0/1\">Gleb Nosovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chekunov_A/0/1/0/all/0/1\">Alexey Chekunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedoseev_D/0/1/0/all/0/1\">Denis Fedoseev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kibkalo_V/0/1/0/all/0/1\">Vladislav Kibkalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikulin_M/0/1/0/all/0/1\">Mikhail Nikulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popelenskiy_F/0/1/0/all/0/1\">Fedor Popelenskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komkov_S/0/1/0/all/0/1\">Stepan Komkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazurenko_I/0/1/0/all/0/1\">Ivan Mazurenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1\">Aleksandr Petiushko</a>",
          "description": "Manifold hypothesis states that data points in high-dimensional space\nactually lie in close vicinity of a manifold of much lower dimension. In many\ncases this hypothesis was empirically verified and used to enhance unsupervised\nand semi-supervised learning. Here we present new approach to manifold\nhypothesis checking and underlying manifold dimension estimation. In order to\ndo it we use two very different methods simultaneously - one geometric, another\nprobabilistic - and check whether they give the same result. Our geometrical\nmethod is a modification for sparse data of a well-known box-counting algorithm\nfor Minkowski dimension calculation. The probabilistic method is new. Although\nit exploits standard nearest neighborhood distance, it is different from\nmethods which were previously used in such situations. This method is robust,\nfast and includes special preliminary data transformation. Experiments on real\ndatasets show that the suggested approach based on two methods combination is\npowerful and effective.",
          "link": "http://arxiv.org/abs/2107.03903",
          "publishedOn": "2021-07-09T01:58:29.096Z",
          "wordCount": 603,
          "title": "Manifold Hypothesis in Data Analysis: Double Geometrically-Probabilistic Approach to Manifold Dimension Estimation. (arXiv:2107.03903v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huazhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "We propose to address quadrupedal locomotion tasks using Reinforcement\nLearning (RL) with a Transformer-based model that learns to combine\nproprioceptive information and high-dimensional depth sensor inputs. While\nlearning-based locomotion has made great advances using RL, most methods still\nrely on domain randomization for training blind agents that generalize to\nchallenging terrains. Our key insight is that proprioceptive states only offer\ncontact measurements for immediate reaction, whereas an agent equipped with\nvisual sensory observations can learn to proactively maneuver environments with\nobstacles and uneven terrain by anticipating changes in the environment many\nsteps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL\nmethod for quadrupedal locomotion that leverages a Transformer-based model for\nfusing proprioceptive states and visual observations. We evaluate our method in\nchallenging simulated environments with different obstacles and uneven terrain.\nWe show that our method obtains significant improvements over policies with\nonly proprioceptive state inputs, and that Transformer-based models further\nimprove generalization across environments. Our project page with videos is at\nhttps://RchalYang.github.io/LocoTransformer .",
          "link": "http://arxiv.org/abs/2107.03996",
          "publishedOn": "2021-07-09T01:58:29.088Z",
          "wordCount": 619,
          "title": "Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers. (arXiv:2107.03996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blinov_P/0/1/0/all/0/1\">Pavel Blinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokh_V/0/1/0/all/0/1\">Vladimir Kokh</a>",
          "description": "The paper researches the problem of concept and patient representations in\nthe medical domain. We present the patient histories from Electronic Health\nRecords (EHRs) as temporal sequences of ICD concepts for which embeddings are\nlearned in an unsupervised setup with a transformer-based neural network model.\nThe model training was performed on the collection of one million patients'\nhistories in 6 years. The predictive power of such a model is assessed in\ncomparison with several baseline methods. A series of experiments on the\nMIMIC-III data show the advantage of the presented model compared to a similar\nsystem. Further, we analyze the obtained embedding space with regards to\nconcept relations and show how knowledge from the medical domain can be\nsuccessfully transferred to the practical task of insurance scoring in the form\nof patient embeddings.",
          "link": "http://arxiv.org/abs/2107.03913",
          "publishedOn": "2021-07-09T01:58:29.082Z",
          "wordCount": 564,
          "title": "Patient Embeddings in Healthcare and Insurance Applications. (arXiv:2107.03913v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zipeng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>",
          "description": "Successful real-world deployment of legged robots would require them to adapt\nin real-time to unseen scenarios like changing terrains, changing payloads,\nwear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to\nsolve this problem of real-time online adaptation in quadruped robots. RMA\nconsists of two components: a base policy and an adaptation module. The\ncombination of these components enables the robot to adapt to novel situations\nin fractions of a second. RMA is trained completely in simulation without using\nany domain knowledge like reference trajectories or predefined foot trajectory\ngenerators and is deployed on the A1 robot without any fine-tuning. We train\nRMA on a varied terrain generator using bioenergetics-inspired rewards and\ndeploy it on a variety of difficult terrains including rocky, slippery,\ndeformable surfaces in environments with grass, long vegetation, concrete,\npebbles, stairs, sand, etc. RMA shows state-of-the-art performance across\ndiverse real-world as well as simulation experiments. Video results at\nhttps://ashish-kmr.github.io/rma-legged-robots/",
          "link": "http://arxiv.org/abs/2107.04034",
          "publishedOn": "2021-07-09T01:58:29.075Z",
          "wordCount": 606,
          "title": "RMA: Rapid Motor Adaptation for Legged Robots. (arXiv:2107.04034v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03901",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walsh_S/0/1/0/all/0/1\">Sean Walsh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gkontra_P/0/1/0/all/0/1\">Polyxeni Gkontra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>",
          "description": "Deep learning models can enable accurate and efficient disease diagnosis, but\nhave thus far been hampered by the data scarcity present in the medical world.\nAutomated diagnosis studies have been constrained by underpowered single-center\ndatasets, and although some results have shown promise, their generalizability\nto other institutions remains questionable as the data heterogeneity between\ninstitutions is not taken into account. By allowing models to be trained in a\ndistributed manner that preserves patients' privacy, federated learning\npromises to alleviate these issues, by enabling diligent multi-center studies.\nWe present the first federated learning study on the modality of cardiovascular\nmagnetic resonance (CMR) and use four centers derived from subsets of the M\\&M\nand ACDC datasets, focusing on the diagnosis of hypertrophic cardiomyopathy\n(HCM). We adapt a 3D-CNN network pretrained on action recognition and explore\ntwo different ways of incorporating shape prior information to the model, and\nfour different data augmentation set-ups, systematically analyzing their impact\non the different collaborative learning choices. We show that despite the small\nsize of data (180 subjects derived from four centers), the privacy preserving\nfederated learning achieves promising results that are competitive with\ntraditional centralized learning. We further find that federatively trained\nmodels exhibit increased robustness and are more sensitive to domain shift\neffects.",
          "link": "http://arxiv.org/abs/2107.03901",
          "publishedOn": "2021-07-09T01:58:29.068Z",
          "wordCount": 675,
          "title": "Federated Learning for Multi-Center Imaging Diagnostics: A Study in Cardiovascular Disease. (arXiv:2107.03901v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+NOR_A/0/1/0/all/0/1\">Ahmad Kamal BIN MOHD NOR</a>, <a href=\"http://arxiv.org/find/cs/1/au:+PEDAPATI_S/0/1/0/all/0/1\">Srinivasa Rao PEDAPATI</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MUHAMMAD_M/0/1/0/all/0/1\">Masdi MUHAMMAD</a>",
          "description": "A state-of-the-art systematic review on XAI applied to Prognostic and Health\nManagement (PHM) of industrial asset is presented. The work attempts to provide\nan overview of the general trend of XAI in PHM, answers the question of\naccuracy versus explainability, investigates the extent of human role,\nexplainability evaluation and uncertainty management in PHM XAI. Research\narticles linked to PHM XAI, in English language, from 2015 to 2021 are selected\nfrom IEEE Xplore, ScienceDirect, SpringerLink, ACM Digital Library and Scopus\ndatabases using PRISMA guidelines. Data was extracted from 35 selected articles\nand examined using MS. Excel. Several findings were synthesized. Firstly, while\nthe discipline is still young, the analysis indicates the growing acceptance of\nXAI in PHM domain. Secondly, XAI functions as a double edge sword, where it is\nassimilated as a tool to execute PHM tasks as well as a mean of explanation, in\nparticular in diagnostic and anomaly detection. There is thus a need for XAI in\nPHM. Thirdly, the review shows that PHM XAI papers produce either good or\nexcellent results in general, suggesting that PHM performance is unaffected by\nXAI. Fourthly, human role, explainability metrics and uncertainty management\nare areas requiring further attention by the PHM community. Adequate\nexplainability metrics to cater for PHM need are urgently needed. Finally, most\ncase study featured on the accepted articles are based on real, indicating that\navailable AI and XAI approaches are equipped to solve complex real-world\nchallenges, increasing the confidence of AI model adoption in the industry.\nThis work is funded by the Universiti Teknologi Petronas Foundation.",
          "link": "http://arxiv.org/abs/2107.03869",
          "publishedOn": "2021-07-09T01:58:29.049Z",
          "wordCount": 705,
          "title": "Explainable AI (XAI) for PHM of Industrial Asset: A State-of-The-Art, PRISMA-Compliant Systematic Review. (arXiv:2107.03869v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03383",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Prosperi_M/0/1/0/all/0/1\">Mattia Prosperi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Marini_S/0/1/0/all/0/1\">Simone Marini</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Boucher_C/0/1/0/all/0/1\">Christina Boucher</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>",
          "description": "Whole genome sequencing (WGS) is quickly becoming the customary means for\nidentification of antimicrobial resistance (AMR) due to its ability to obtain\nhigh resolution information about the genes and mechanisms that are causing\nresistance and driving pathogen mobility. By contrast, traditional phenotypic\n(antibiogram) testing cannot easily elucidate such information. Yet development\nof AMR prediction tools from genotype-phenotype data can be biased, since\nsampling is non-randomized. Sample provenience, period of collection, and\nspecies representation can confound the association of genetic traits with AMR.\nThus, prediction models can perform poorly on new data with sampling\ndistribution shifts. In this work -- under an explicit set of causal\nassumptions -- we evaluate the effectiveness of propensity-based rebalancing\nand confounding adjustment on AMR prediction using genotype-phenotype AMR data\nfrom the Pathosystems Resource Integration Center (PATRIC). We select bacterial\ngenotypes (encoded as k-mer signatures, i.e. DNA fragments of length k),\ncountry, year, species, and AMR phenotypes for the tetracycline drug class,\npreparing test data with recent genomes coming from a single country. We test\nboosted logistic regression (BLR) and random forests (RF) with/without\nbias-handling. On 10,936 instances, we find evidence of species, location and\nyear imbalance with respect to the AMR phenotype. The crude versus\nbias-adjusted change in effect of genetic signatures on AMR varies but only\nmoderately (selecting the top 20,000 out of 40+ million k-mers). The area under\nthe receiver operating characteristic (AUROC) of the RF (0.95) is comparable to\nthat of BLR (0.94) on both out-of-bag samples from bootstrap and the external\ntest (n=1,085), where AUROCs do not decrease. We observe a 1%-5% gain in AUROC\nwith bias-handling compared to the sole use of genetic signatures. ...",
          "link": "http://arxiv.org/abs/2107.03383",
          "publishedOn": "2021-07-09T01:58:29.038Z",
          "wordCount": 748,
          "title": "Assessing putative bias in prediction of anti-microbial resistance from real-world genotyping data under explicit causal assumptions. (arXiv:2107.03383v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E_W/0/1/0/all/0/1\">Weinan E</a>",
          "description": "The generative adversarial network (GAN) is a well-known model for learning\nhigh-dimensional distributions, but the mechanism for its generalization\nability is not understood. In particular, GAN is vulnerable to the memorization\nphenomenon, the eventual convergence to the empirical distribution. We consider\na simplified GAN model with the generator replaced by a density, and analyze\nhow the discriminator contributes to generalization. We show that with early\nstopping, the generalization error measured by Wasserstein metric escapes from\nthe curse of dimensionality, despite that in the long term, memorization is\ninevitable. In addition, we present a hardness of learning result for WGAN.",
          "link": "http://arxiv.org/abs/2107.03633",
          "publishedOn": "2021-07-09T01:58:29.031Z",
          "wordCount": 535,
          "title": "Generalization Error of GAN from the Discriminator's Perspective. (arXiv:2107.03633v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03836",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lazarsfeld_J/0/1/0/all/0/1\">John Lazarsfeld</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Johnson_A/0/1/0/all/0/1\">Aaron Johnson</a>",
          "description": "The Maximal Information Coefficient (MIC) of Reshef et al. (Science, 2011) is\na statistic for measuring dependence between variable pairs in large datasets.\nIn this note, we prove that MIC is a consistent estimator of the corresponding\npopulation statistic MIC$_*$. This corrects an error in an argument of Reshef\net al. (JMLR, 2016), which we describe.",
          "link": "http://arxiv.org/abs/2107.03836",
          "publishedOn": "2021-07-09T01:58:29.024Z",
          "wordCount": 493,
          "title": "Consistency of the Maximal Information Coefficient Estimator. (arXiv:2107.03836v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Luong-Ha Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulet_J/0/1/0/all/0/1\">James-A. Goulet</a>",
          "description": "With few exceptions, neural networks have been relying on backpropagation and\ngradient descent as the inference engine in order to learn the model\nparameters, because the closed-form Bayesian inference for neural networks has\nbeen considered to be intractable. In this paper, we show how we can leverage\nthe tractable approximate Gaussian inference's (TAGI) capabilities to infer\nhidden states, rather than only using it for inferring the network's\nparameters. One novel aspect it allows is to infer hidden states through the\nimposition of constraints designed to achieve specific objectives, as\nillustrated through three examples: (1) the generation of adversarial-attack\nexamples, (2) the usage of a neural network as a black-box optimization method,\nand (3) the application of inference on continuous-action reinforcement\nlearning. These applications showcase how tasks that were previously reserved\nto gradient-based optimization approaches can now be approached with\nanalytically tractable inference",
          "link": "http://arxiv.org/abs/2107.03759",
          "publishedOn": "2021-07-09T01:58:28.984Z",
          "wordCount": 577,
          "title": "Analytically Tractable Hidden-States Inference in Bayesian Neural Networks. (arXiv:2107.03759v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ancha_S/0/1/0/all/0/1\">Siddharth Ancha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_G/0/1/0/all/0/1\">Gaurav Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_S/0/1/0/all/0/1\">Srinivasa G. Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>",
          "description": "To safely navigate unknown environments, robots must accurately perceive\ndynamic obstacles. Instead of directly measuring the scene depth with a LiDAR\nsensor, we explore the use of a much cheaper and higher resolution sensor:\nprogrammable light curtains. Light curtains are controllable depth sensors that\nsense only along a surface that a user selects. We use light curtains to\nestimate the safety envelope of a scene: a hypothetical surface that separates\nthe robot from all obstacles. We show that generating light curtains that sense\nrandom locations (from a particular distribution) can quickly discover the\nsafety envelope for scenes with unknown objects. Importantly, we produce\ntheoretical safety guarantees on the probability of detecting an obstacle using\nrandom curtains. We combine random curtains with a machine learning based model\nthat forecasts and tracks the motion of the safety envelope efficiently. Our\nmethod accurately estimates safety envelopes while providing probabilistic\nsafety guarantees that can be used to certify the efficacy of a robot\nperception system to detect and avoid dynamic obstacles. We evaluate our\napproach in a simulated urban driving environment and a real-world environment\nwith moving pedestrians using a light curtain device and show that we can\nestimate safety envelopes efficiently and effectively. Project website:\nhttps://siddancha.github.io/projects/active-safety-envelopes-with-guarantees",
          "link": "http://arxiv.org/abs/2107.04000",
          "publishedOn": "2021-07-09T01:58:28.964Z",
          "wordCount": 665,
          "title": "Active Safety Envelopes using Light Curtains with Probabilistic Guarantees. (arXiv:2107.04000v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vartholomaios_A/0/1/0/all/0/1\">Argyrios Vartholomaios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlos_S/0/1/0/all/0/1\">Stamatis Karlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouloumpris_E/0/1/0/all/0/1\">Eleftherios Kouloumpris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>",
          "description": "Energy production using renewable sources exhibits inherent uncertainties due\nto their intermittent nature. Nevertheless, the unified European energy market\npromotes the increasing penetration of renewable energy sources (RES) by the\nregional energy system operators. Consequently, RES forecasting can assist in\nthe integration of these volatile energy sources, since it leads to higher\nreliability and reduced ancillary operational costs for power systems. This\npaper presents a new dataset for solar and wind energy generation forecast in\nGreece and introduces a feature engineering pipeline that enriches the\ndimensional space of the dataset. In addition, we propose a novel method that\nutilizes the innovative Prophet model, an end-to-end forecasting tool that\nconsiders several kinds of nonlinear trends in decomposing the energy time\nseries before a tree-based ensemble provides short-term predictions. The\nperformance of the system is measured through representative evaluation\nmetrics, and by estimating the model's generalization under an industryprovided\nscheme of absolute error thresholds. The proposed hybrid model competes with\nbaseline persistence models, tree-based regression ensembles, and the Prophet\nmodel, managing to outperform them, presenting both lower error rates and more\nfavorable error distribution.",
          "link": "http://arxiv.org/abs/2107.03825",
          "publishedOn": "2021-07-09T01:58:28.957Z",
          "wordCount": 628,
          "title": "Short-term Renewable Energy Forecasting in Greece using Prophet Decomposition and Tree-based Ensembles. (arXiv:2107.03825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byungsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hangyeol Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Dongmin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Youngduck Choi</a>",
          "description": "The needs for precisely estimating a student's academic performance have been\nemphasized with an increasing amount of attention paid to Intelligent Tutoring\nSystem (ITS). However, since labels for academic performance, such as test\nscores, are collected from outside of ITS, obtaining the labels is costly,\nleading to label-scarcity problem which brings challenge in taking machine\nlearning approaches for academic performance prediction. To this end, inspired\nby the recent advancement of pre-training method in natural language processing\ncommunity, we propose DPA, a transfer learning framework with Discriminative\nPre-training tasks for Academic performance prediction. DPA pre-trains two\nmodels, a generator and a discriminator, and fine-tunes the discriminator on\nacademic performance prediction. In DPA's pre-training phase, a sequence of\ninteractions where some tokens are masked is provided to the generator which is\ntrained to reconstruct the original sequence. Then, the discriminator takes an\ninteraction sequence where the masked tokens are replaced by the generator's\noutputs, and is trained to predict the originalities of all tokens in the\nsequence. Compared to the previous state-of-the-art generative pre-training\nmethod, DPA is more sample efficient, leading to fast convergence to lower\nacademic performance prediction error. We conduct extensive experimental\nstudies on a real-world dataset obtained from a multi-platform ITS application\nand show that DPA outperforms the previous state-of-the-art generative\npre-training method with a reduction of 4.05% in mean absolute error and more\nrobust to increased label-scarcity.",
          "link": "http://arxiv.org/abs/2107.04009",
          "publishedOn": "2021-07-09T01:58:28.939Z",
          "wordCount": 674,
          "title": "Knowledge Transfer by Discriminative Pre-training for Academic Performance Prediction. (arXiv:2107.04009v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polyvyanyy_A/0/1/0/all/0/1\">Artem Polyvyanyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moffat_A/0/1/0/all/0/1\">Alistair Moffat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Banuelos_L/0/1/0/all/0/1\">Luciano Garc&#xed;a-Ba&#xf1;uelos</a>",
          "description": "Process mining studies ways to derive value from process executions recorded\nin event logs of IT-systems, with process discovery the task of inferring a\nprocess model for an event log emitted by some unknown system. One quality\ncriterion for discovered process models is generalization. Generalization seeks\nto quantify how well the discovered model describes future executions of the\nsystem, and is perhaps the least understood quality criterion in process\nmining. The lack of understanding is primarily a consequence of generalization\nseeking to measure properties over the entire future behavior of the system,\nwhen the only available sample of behavior is that provided by the event log\nitself. In this paper, we draw inspiration from computational statistics, and\nemploy a bootstrap approach to estimate properties of a population based on a\nsample. Specifically, we define an estimator of the model's generalization\nbased on the event log it was discovered from, and then use bootstrapping to\nmeasure the generalization of the model with respect to the system, and its\nstatistical significance. Experiments demonstrate the feasibility of the\napproach in industrial settings.",
          "link": "http://arxiv.org/abs/2107.03876",
          "publishedOn": "2021-07-09T01:58:28.917Z",
          "wordCount": 629,
          "title": "Bootstrapping Generalization of Process Models Discovered From Event Data. (arXiv:2107.03876v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bandi_H/0/1/0/all/0/1\">Hari Bandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertsimas_D/0/1/0/all/0/1\">Dimitris Bertsimas</a>",
          "description": "Systemic bias with respect to gender, race and ethnicity, often unconscious,\nis prevalent in datasets involving choices among individuals. Consequently,\nsociety has found it challenging to alleviate bias and achieve diversity in a\nway that maintains meritocracy in such settings. We propose (a) a novel\noptimization approach based on optimally flipping outcome labels and training\nclassification models simultaneously to discover changes to be made in the\nselection process so as to achieve diversity without significantly affecting\nmeritocracy, and (b) a novel implementation tool employing optimal\nclassification trees to provide insights on which attributes of individuals\nlead to flipping of their labels, and to help make changes in the current\nselection processes in a manner understandable by human decision makers. We\npresent case studies on three real-world datasets consisting of parole,\nadmissions to the bar and lending decisions, and demonstrate that the price of\ndiversity is low and sometimes negative, that is we can modify our selection\nprocesses in a way that enhances diversity without affecting meritocracy\nsignificantly, and sometimes improving it.",
          "link": "http://arxiv.org/abs/2107.03900",
          "publishedOn": "2021-07-09T01:58:28.910Z",
          "wordCount": 600,
          "title": "The Price of Diversity. (arXiv:2107.03900v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03940",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Butucea_C/0/1/0/all/0/1\">Cristina Butucea</a>, <a href=\"http://arxiv.org/find/math/1/au:+Issartel_Y/0/1/0/all/0/1\">Yann Issartel</a>",
          "description": "We study the problem of estimating non-linear functionals of discrete\ndistributions in the context of local differential privacy. The initial data\n$x_1,\\ldots,x_n \\in [K]$ are supposed i.i.d. and distributed according to an\nunknown discrete distribution $p = (p_1,\\ldots,p_K)$. Only $\\alpha$-locally\ndifferentially private (LDP) samples $z_1,...,z_n$ are publicly available,\nwhere the term 'local' means that each $z_i$ is produced using one individual\nattribute $x_i$. We exhibit privacy mechanisms (PM) that are interactive (i.e.\nthey are allowed to use already published confidential data) or\nnon-interactive. We describe the behavior of the quadratic risk for estimating\nthe power sum functional $F_{\\gamma} = \\sum_{k=1}^K p_k^{\\gamma}$, $\\gamma >0$\nas a function of $K, \\, n$ and $\\alpha$. In the non-interactive case, we study\ntwo plug-in type estimators of $F_{\\gamma}$, for all $\\gamma >0$, that are\nsimilar to the MLE analyzed by Jiao et al. (2017) in the multinomial model.\nHowever, due to the privacy constraint the rates we attain are slower and\nsimilar to those obtained in the Gaussian model by Collier et al. (2020). In\nthe interactive case, we introduce for all $\\gamma >1$ a two-step procedure\nwhich attains the faster parametric rate $(n \\alpha^2)^{-1/2}$ when $\\gamma\n\\geq 2$. We give lower bounds results over all $\\alpha$-LDP mechanisms and all\nestimators using the private samples.",
          "link": "http://arxiv.org/abs/2107.03940",
          "publishedOn": "2021-07-09T01:58:28.891Z",
          "wordCount": 655,
          "title": "Locally differentially private estimation of nonlinear functionals of discrete distributions. (arXiv:2107.03940v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03926",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Dolphin_R/0/1/0/all/0/1\">Rian Dolphin</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Smyth_B/0/1/0/all/0/1\">Barry Smyth</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>",
          "description": "Forecasting stock returns is a challenging problem due to the highly\nstochastic nature of the market and the vast array of factors and events that\ncan influence trading volume and prices. Nevertheless it has proven to be an\nattractive target for machine learning research because of the potential for\neven modest levels of prediction accuracy to deliver significant benefits. In\nthis paper, we describe a case-based reasoning approach to predicting stock\nmarket returns using only historical pricing data. We argue that one of the\nimpediments for case-based stock prediction has been the lack of a suitable\nsimilarity metric when it comes to identifying similar pricing histories as the\nbasis for a future prediction -- traditional Euclidean and correlation based\napproaches are not effective for a variety of reasons -- and in this regard, a\nkey contribution of this work is the development of a novel similarity metric\nfor comparing historical pricing data. We demonstrate the benefits of this\nmetric and the case-based approach in a real-world application in comparison to\na variety of conventional benchmarks.",
          "link": "http://arxiv.org/abs/2107.03926",
          "publishedOn": "2021-07-09T01:58:28.883Z",
          "wordCount": 636,
          "title": "Measuring Financial Time Series Similarity With a View to Identifying Profitable Stock Market Opportunities. (arXiv:2107.03926v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lofqvist_M/0/1/0/all/0/1\">Martina Lofqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1\">Jos&#xe9; Cano</a>",
          "description": "There is a proliferation in the number of satellites launched each year,\nresulting in downlinking of terabytes of data each day. The data received by\nground stations is often unprocessed, making this an expensive process\nconsidering the large data sizes and that not all of the data is useful. This,\ncoupled with the increasing demand for real-time data processing, has led to a\ngrowing need for on-orbit processing solutions. In this work, we investigate\nthe performance of CNN-based object detectors on constrained devices by\napplying different image compression techniques to satellite data. We examine\nthe capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier;\nlow-power, high-performance computers, with integrated GPUs, small enough to\nfit on-board a nanosatellite. We take a closer look at object detection\nnetworks, including the Single Shot MultiBox Detector (SSD) and Region-based\nFully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a\nLarge Scale Dataset for Object Detection in Aerial Images. The performance is\nmeasured in terms of execution time, memory consumption, and accuracy, and are\ncompared against a baseline containing a server with two powerful GPUs. The\nresults show that by applying image compression techniques, we are able to\nimprove the execution time and memory consumption, achieving a fully runnable\ndataset. A lossless compression technique achieves roughly a 10% reduction in\nexecution time and about a 3% reduction in memory consumption, with no impact\non the accuracy. While a lossy compression technique improves the execution\ntime by up to 144% and the memory consumption is reduced by as much as 97%.\nHowever, it has a significant impact on accuracy, varying depending on the\ncompression ratio. Thus the application and ratio of these compression\ntechniques may differ depending on the required level of accuracy for a\nparticular task.",
          "link": "http://arxiv.org/abs/2107.03774",
          "publishedOn": "2021-07-09T01:58:28.875Z",
          "wordCount": 777,
          "title": "Optimizing Data Processing in Space for Object Detection in Satellite Imagery. (arXiv:2107.03774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinhyung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1\">Xinshuo Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Man_Y/0/1/0/all/0/1\">Yunze Man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>",
          "description": "Point clouds and RGB images are naturally complementary modalities for 3D\nvisual understanding - the former provides sparse but accurate locations of\npoints on objects, while the latter contains dense color and texture\ninformation. Despite this potential for close sensor fusion, many methods train\ntwo models in isolation and use simple feature concatenation to represent 3D\nsensor data. This separated training scheme results in potentially sub-optimal\nperformance and prevents 3D tasks from being used to benefit 2D tasks that are\noften useful on their own. To provide a more integrated approach, we propose a\nnovel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box\nproposals to improve 2D segmentation predictions, which are then used to\nfurther refine the 3D boxes. We show that including a 2D network between two\nstages of 3D modules significantly improves both 2D and 3D task performance.\nMoreover, to prevent the 3D module from over-relying on the overfitted 2D\npredictions, we propose a dual-head 2D segmentation training and inference\nscheme, allowing the 2nd 3D module to learn to interpret imperfect 2D\nsegmentation predictions. Evaluating our model on the challenging SUN RGB-D\ndataset, we improve upon state-of-the-art results of both single modality and\nfusion networks by a large margin ($\\textbf{+3.8}$ mAP@0.5). Code will be\nreleased $\\href{https://github.com/Divadi/MTC_RCNN}{\\text{here.}}$",
          "link": "http://arxiv.org/abs/2107.04013",
          "publishedOn": "2021-07-09T01:58:28.868Z",
          "wordCount": 656,
          "title": "Multi-Modality Task Cascade for 3D Object Detection. (arXiv:2107.04013v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biggs_F/0/1/0/all/0/1\">Felix Biggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guedj_B/0/1/0/all/0/1\">Benjamin Guedj</a>",
          "description": "We develop a framework for derandomising PAC-Bayesian generalisation bounds\nachieving a margin on training data, relating this process to the\nconcentration-of-measure phenomenon. We apply these tools to linear prediction,\nsingle-hidden-layer neural networks with an unusual erf activation function,\nand deep ReLU networks, obtaining new bounds. The approach is also extended to\nthe idea of \"partial-derandomisation\" where only some layers are derandomised\nand the others are stochastic. This allows empirical evaluation of\nsingle-hidden-layer networks on more complex datasets, and helps bridge the gap\nbetween generalisation bounds for non-stochastic deep networks and those for\nrandomised deep networks as generally examined in PAC-Bayes.",
          "link": "http://arxiv.org/abs/2107.03955",
          "publishedOn": "2021-07-09T01:58:28.861Z",
          "wordCount": 528,
          "title": "On Margins and Derandomisation in PAC-Bayes. (arXiv:2107.03955v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kudari_S/0/1/0/all/0/1\">Shashidhar Veerappa Kudari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunari_A/0/1/0/all/0/1\">Akshaykumar Gunari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamadandi_A/0/1/0/all/0/1\">Adarsh Jamadandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabib_R/0/1/0/all/0/1\">Ramesh Ashok Tabib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mudenagudi_U/0/1/0/all/0/1\">Uma Mudenagudi</a>",
          "description": "In this paper, we propose a strategy to mitigate the problem of inefficient\nclustering performance by introducing data augmentation as an auxiliary\nplug-in. Classical clustering techniques such as K-means, Gaussian mixture\nmodel and spectral clustering are central to many data-driven applications.\nHowever, recently unsupervised simultaneous feature learning and clustering\nusing neural networks also known as Deep Embedded Clustering (DEC) has gained\nprominence. Pioneering works on deep feature clustering focus on defining\nrelevant clustering loss function and choosing the right neural network for\nextracting features. A central problem in all these cases is data sparsity\naccompanied by high intra-class and low inter-class variance, which\nsubsequently leads to poor clustering performance and erroneous candidate\nassignments. Towards this, we employ data augmentation techniques to improve\nthe density of the clusters, thus improving the overall performance. We train a\nvariant of Convolutional Autoencoder (CAE) with augmented data to construct the\ninitial feature space as a novel model for deep clustering. We demonstrate the\nresults of proposed strategy on crowdsourced Indian Heritage dataset. Extensive\nexperiments show consistent improvements over existing works.",
          "link": "http://arxiv.org/abs/2107.03852",
          "publishedOn": "2021-07-09T01:58:28.841Z",
          "wordCount": 618,
          "title": "Augmented Data as an Auxiliary Plug-in Towards Categorization of Crowdsourced Heritage Data. (arXiv:2107.03852v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tashiro_Y/0/1/0/all/0/1\">Yusuke Tashiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "The imputation of missing values in time series has many applications in\nhealthcare and finance. While autoregressive models are natural candidates for\ntime series imputation, score-based diffusion models have recently outperformed\nexisting counterparts including autoregressive models in many tasks such as\nimage generation and audio synthesis, and would be promising for time series\nimputation. In this paper, we propose Conditional Score-based Diffusion models\nfor Imputation (CSDI), a novel time series imputation method that utilizes\nscore-based diffusion models conditioned on observed data. Unlike existing\nscore-based approaches, the conditional diffusion model is explicitly trained\nfor imputation and can exploit correlations between observed values. On\nhealthcare and environmental data, CSDI improves by 40-70% over existing\nprobabilistic imputation methods on popular performance metrics. In addition,\ndeterministic imputation by CSDI reduces the error by 5-20% compared to the\nstate-of-the-art deterministic imputation methods. Furthermore, CSDI can also\nbe applied to time series interpolation and probabilistic forecasting, and is\ncompetitive with existing baselines.",
          "link": "http://arxiv.org/abs/2107.03502",
          "publishedOn": "2021-07-09T01:58:28.805Z",
          "wordCount": 595,
          "title": "CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation. (arXiv:2107.03502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaoliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wulong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>",
          "description": "Shift neural networks reduce computation complexity by removing expensive\nmultiplication operations and quantizing continuous weights into low-bit\ndiscrete values, which are fast and energy efficient compared to conventional\nneural networks. However, existing shift networks are sensitive to the weight\ninitialization, and also yield a degraded performance caused by vanishing\ngradient and weight sign freezing problem. To address these issues, we propose\nS low-bit re-parameterization, a novel technique for training low-bit shift\nnetworks. Our method decomposes a discrete parameter in a sign-sparse-shift\n3-fold manner. In this way, it efficiently learns a low-bit network with a\nweight dynamics similar to full-precision networks and insensitive to weight\ninitialization. Our proposed training method pushes the boundaries of shift\nneural networks and shows 3-bit shift networks out-performs their\nfull-precision counterparts in terms of top-1 accuracy on ImageNet.",
          "link": "http://arxiv.org/abs/2107.03453",
          "publishedOn": "2021-07-09T01:58:28.783Z",
          "wordCount": 581,
          "title": "$S^3$: Sign-Sparse-Shift Reparametrization for Effective Training of Low-bit Shift Networks. (arXiv:2107.03453v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.17171",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pohjonen_J/0/1/0/all/0/1\">Joona Pohjonen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sturenberg_C/0/1/0/all/0/1\">Carolin St&#xfc;renberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rannikko_A/0/1/0/all/0/1\">Antti Rannikko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mirtti_T/0/1/0/all/0/1\">Tuomas Mirtti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pitkanen_E/0/1/0/all/0/1\">Esa Pitk&#xe4;nen</a>",
          "description": "Many current neural networks for medical imaging generalise poorly to data\nunseen during training. Such behaviour can be caused by networks overfitting\neasy-to-learn, or statistically dominant, features while disregarding other\npotentially informative features. For example, indistinguishable differences in\nthe sharpness of the images from two different scanners can degrade the\nperformance of the network significantly. All neural networks intended for\nclinical practice need to be robust to variation in data caused by differences\nin imaging equipment, sample preparation and patient populations.\n\nTo address these challenges, we evaluate the utility of spectral decoupling\nas an implicit bias mitigation method. Spectral decoupling encourages the\nneural network to learn more features by simply regularising the networks'\nunnormalised prediction scores with an L2 penalty, thus having no added\ncomputational costs.\n\nWe show that spectral decoupling allows training neural networks on datasets\nwith strong spurious correlations. Networks trained without spectral decoupling\ndo not learn the original task and appear to make false predictions based on\nthe spurious correlations. Spectral decoupling also increases networks'\nrobustness for data distribution shifts. To validate our findings, we train\nnetworks with and without spectral decoupling to detect prostate cancer tissue\nslides and COVID-19 in chest radiographs. Networks trained with spectral\ndecoupling achieve substantially higher performance on all evaluation datasets.\n\nOur results show that spectral decoupling helps with generalisation issues\nassociated with neural networks. We recommend using spectral decoupling as an\nimplicit bias mitigation method in any neural network intended for clinical\nuse.",
          "link": "http://arxiv.org/abs/2103.17171",
          "publishedOn": "2021-07-09T01:58:28.572Z",
          "wordCount": 784,
          "title": "Spectral decoupling allows training transferable neural networks in medical imaging. (arXiv:2103.17171v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02287",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Ronaghi_F/0/1/0/all/0/1\">Farnoush Ronaghi</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Salimibeni_M/0/1/0/all/0/1\">Mohammad Salimibeni</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Naderkhani_F/0/1/0/all/0/1\">Farnoosh Naderkhani</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>",
          "description": "The novel of coronavirus (COVID-19) has suddenly and abruptly changed the\nworld as we knew at the start of the 3rd decade of the 21st century.\nParticularly, COVID-19 pandemic has negatively affected financial econometrics\nand stock markets across the globe. Artificial Intelligence (AI) and Machine\nLearning (ML)-based prediction models, especially Deep Neural Network (DNN)\narchitectures, have the potential to act as a key enabling factor to reduce the\nadverse effects of the COVID-19 pandemic and future possible ones on financial\nmarkets. In this regard, first, a unique COVID-19 related PRIce MOvement\nprediction (COVID19 PRIMO) dataset is introduced in this paper, which\nincorporates effects of social media trends related to COVID-19 on stock market\nprice movements. Afterwards, a novel hybrid and parallel DNN-based framework is\nproposed that integrates different and diversified learning architectures.\nReferred to as the COVID-19 adopted Hybrid and Parallel deep fusion framework\nfor Stock price Movement Prediction (COVID19-HPSMP), innovative fusion\nstrategies are used to combine scattered social media news related to COVID-19\nwith historical mark data. The proposed COVID19-HPSMP consists of two parallel\npaths (hence hybrid), one based on Convolutional Neural Network (CNN) with\nLocal/Global Attention modules, and one integrated CNN and Bi-directional Long\nShort term Memory (BLSTM) path. The two parallel paths are followed by a\nmultilayer fusion layer acting as a fusion centre that combines localized\nfeatures. Performance evaluations are performed based on the introduced COVID19\nPRIMO dataset illustrating superior performance of the proposed framework.",
          "link": "http://arxiv.org/abs/2101.02287",
          "publishedOn": "2021-07-09T01:58:28.564Z",
          "wordCount": 758,
          "title": "COVID19-HPSMP: COVID-19 Adopted Hybrid and Parallel Deep Information Fusion Framework for Stock Price Movement Prediction. (arXiv:2101.02287v2 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erdelyi_T/0/1/0/all/0/1\">Tam&#xe1;s Erd&#xe9;lyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Cameron Musco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Christopher Musco</a>",
          "description": "We prove new explicit upper bounds on the leverage scores of Fourier sparse\nfunctions under both the Gaussian and Laplace measures. In particular, we study\n$s$-sparse functions of the form $f(x) = \\sum_{j=1}^s a_j e^{i \\lambda_j x}$\nfor coefficients $a_j \\in \\mathbb{C}$ and frequencies $\\lambda_j \\in\n\\mathbb{R}$. Bounding Fourier sparse leverage scores under various measures is\nof pure mathematical interest in approximation theory, and our work extends\nexisting results for the uniform measure [Erd17,CP19a]. Practically, our bounds\nare motivated by two important applications in machine learning:\n\n1. Kernel Approximation. They yield a new random Fourier features algorithm\nfor approximating Gaussian and Cauchy (rational quadratic) kernel matrices. For\nlow-dimensional data, our method uses a near optimal number of features, and\nits runtime is polynomial in the $statistical\\ dimension$ of the approximated\nkernel matrix. It is the first \"oblivious sketching method\" with this property\nfor any kernel besides the polynomial kernel, resolving an open question of\n[AKM+17,AKK+20b].\n\n2. Active Learning. They can be used as non-uniform sampling distributions\nfor robust active learning when data follows a Gaussian or Laplace\ndistribution. Using the framework of [AKM+19], we provide essentially optimal\nresults for bandlimited and multiband interpolation, and Gaussian process\nregression. These results generalize existing work that only applies to\nuniformly distributed data.",
          "link": "http://arxiv.org/abs/2006.07340",
          "publishedOn": "2021-07-09T01:58:28.546Z",
          "wordCount": 688,
          "title": "Fourier Sparse Leverage Scores and Approximate Kernel Learning. (arXiv:2006.07340v3 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.08964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1\">Matthew Stevenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mues_C/0/1/0/all/0/1\">Christophe Mues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1\">Cristi&#xe1;n Bravo</a>",
          "description": "Compared to consumer lending, Micro, Small and Medium Enterprise (mSME)\ncredit risk modelling is particularly challenging, as, often, the same sources\nof information are not available. Therefore, it is standard policy for a loan\nofficer to provide a textual loan assessment to mitigate limited data\navailability. In turn, this statement is analysed by a credit expert alongside\nany available standard credit data. In our paper, we exploit recent advances\nfrom the field of Deep Learning and Natural Language Processing (NLP),\nincluding the BERT (Bidirectional Encoder Representations from Transformers)\nmodel, to extract information from 60 000 textual assessments provided by a\nlender. We consider the performance in terms of the AUC (Area Under the\nreceiver operating characteristic Curve) and Brier Score metrics and find that\nthe text alone is surprisingly effective for predicting default. However, when\ncombined with traditional data, it yields no additional predictive capability,\nwith performance dependent on the text's length. Our proposed deep learning\nmodel does, however, appear to be robust to the quality of the text and\ntherefore suitable for partly automating the mSME lending process. We also\ndemonstrate how the content of loan assessments influences performance, leading\nus to a series of recommendations on a new strategy for collecting future mSME\nloan assessments.",
          "link": "http://arxiv.org/abs/2003.08964",
          "publishedOn": "2021-07-09T01:58:28.540Z",
          "wordCount": 712,
          "title": "The value of text for small business default prediction: A deep learning approach. (arXiv:2003.08964v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yaofeng Desmond Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_B/0/1/0/all/0/1\">Biswadip Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Amit Chakraborty</a>",
          "description": "The incorporation of appropriate inductive bias plays a critical role in\nlearning dynamics from data. A growing body of work has been exploring ways to\nenforce energy conservation in the learned dynamics by encoding Lagrangian or\nHamiltonian dynamics into the neural network architecture. These existing\napproaches are based on differential equations, which do not allow\ndiscontinuity in the states and thereby limit the class of systems one can\nlearn. However, in reality, most physical systems, such as legged robots and\nrobotic manipulators, involve contacts and collisions, which introduce\ndiscontinuities in the states. In this paper, we introduce a differentiable\ncontact model, which can capture contact mechanics: frictionless/frictional, as\nwell as elastic/inelastic. This model can also accommodate inequality\nconstraints, such as limits on the joint angles. The proposed contact model\nextends the scope of Lagrangian and Hamiltonian neural networks by allowing\nsimultaneous learning of contact and system properties. We demonstrate this\nframework on a series of challenging 2D and 3D physical systems with different\ncoefficients of restitution and friction. The learned dynamics can be used as a\ndifferentiable physics simulator for downstream gradient-based optimization\ntasks, such as planning and control.",
          "link": "http://arxiv.org/abs/2102.06794",
          "publishedOn": "2021-07-09T01:58:28.533Z",
          "wordCount": 656,
          "title": "Extending Lagrangian and Hamiltonian Neural Networks with Differentiable Contact Models. (arXiv:2102.06794v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elsken_T/0/1/0/all/0/1\">Thomas Elsken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staffler_B/0/1/0/all/0/1\">Benedikt Staffler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zela_A/0/1/0/all/0/1\">Arber Zela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1\">Jan Hendrik Metzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "While neural architecture search methods have been successful in previous\nyears and led to new state-of-the-art performance on various problems, they\nhave also been criticized for being unstable, being highly sensitive with\nrespect to their hyperparameters, and often not performing better than random\nsearch. To shed some light on this issue, we discuss some practical\nconsiderations that help improve the stability, efficiency and overall\nperformance.",
          "link": "http://arxiv.org/abs/2107.03719",
          "publishedOn": "2021-07-09T01:58:28.498Z",
          "wordCount": 505,
          "title": "Bag of Tricks for Neural Architecture Search. (arXiv:2107.03719v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lechner_T/0/1/0/all/0/1\">Tosca Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_S/0/1/0/all/0/1\">Shai Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sushant Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananthakrishnan_N/0/1/0/all/0/1\">Nivasini Ananthakrishnan</a>",
          "description": "With the growing awareness to fairness in machine learning and the\nrealization of the central role that data representation has in data processing\ntasks, there is an obvious interest in notions of fair data representations.\nThe goal of such representations is that a model trained on data under the\nrepresentation (e.g., a classifier) will be guaranteed to respect some fairness\nconstraints.\n\nSuch representations are useful when they can be fixed for training models on\nvarious different tasks and also when they serve as data filtering between the\nraw data (known to the representation designer) and potentially malicious\nagents that use the data under the representation to learn predictive models\nand make decisions.\n\nA long list of recent research papers strive to provide tools for achieving\nthese goals.\n\nHowever, we prove that this is basically a futile effort. Roughly stated, we\nprove that no representation can guarantee the fairness of classifiers for\ndifferent tasks trained using it; even the basic goal of achieving\nlabel-independent Demographic Parity fairness fails once the marginal data\ndistribution shifts. More refined notions of fairness, like Odds Equality,\ncannot be guaranteed by a representation that does not take into account the\ntask specific labeling rule with respect to which such fairness will be\nevaluated (even if the marginal data distribution is known a priory).\nFurthermore, except for trivial cases, no representation can guarantee Odds\nEquality fairness for any two different tasks, while allowing accurate label\npredictions for both.\n\nWhile some of our conclusions are intuitive, we formulate (and prove) crisp\nstatements of such impossibilities, often contrasting impressions conveyed by\nmany recent works on fair representations.",
          "link": "http://arxiv.org/abs/2107.03483",
          "publishedOn": "2021-07-09T01:58:28.468Z",
          "wordCount": 701,
          "title": "Impossibility results for fair representations. (arXiv:2107.03483v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08733",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Amini_H/0/1/0/all/0/1\">Hamed Amini</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Feinstein_Z/0/1/0/all/0/1\">Zachary Feinstein</a>",
          "description": "This paper introduces a formulation of the optimal network compression\nproblem for financial systems. This general formulation is presented for\ndifferent levels of network compression or rerouting allowed from the initial\ninterbank network. We prove that this problem is, generically, NP-hard. We\nfocus on objective functions generated by systemic risk measures under\nsystematic shocks to the financial network. We conclude by studying the optimal\ncompression problem for specific networks; this permits us to study the\nso-called robust fragility of certain network topologies more generally as well\nas the potential benefits and costs of network compression. In particular,\nunder systematic shocks and heterogeneous financial networks the typical\nheuristics of robust fragility no longer hold generally.",
          "link": "http://arxiv.org/abs/2008.08733",
          "publishedOn": "2021-07-09T01:58:28.448Z",
          "wordCount": 574,
          "title": "Optimal Network Compression. (arXiv:2008.08733v3 [q-fin.RM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.11355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vavasis_S/0/1/0/all/0/1\">Stephen Vavasis</a>",
          "description": "Sum-of-norms clustering is a clustering formulation based on convex\noptimization that automatically induces hierarchy. Multiple algorithms have\nbeen proposed to solve the optimization problem: subgradient descent by Hocking\net al., ADMM and ADA by Chi and Lange, stochastic incremental algorithm by\nPanahi et al. and semismooth Newton-CG augmented Lagrangian method by Sun et\nal. All algorithms yield approximate solutions, even though an exact solution\nis demanded to determine the correct cluster assignment. The purpose of this\npaper is to close the gap between the output from existing algorithms and the\nexact solution to the optimization problem. We present a clustering test that\nidentifies and certifies the correct cluster assignment from an approximate\nsolution yielded by any primal-dual algorithm. Our certification validates\nclustering for both unit and multiplicative weights. The test may not succeed\nif the approximation is inaccurate. However, we show the correct cluster\nassignment is guaranteed to be certified by a primal-dual path following\nalgorithm after sufficiently many iterations, provided that the model parameter\n$\\lambda$ avoids a finite number of bad values. Numerical experiments are\nconducted on Gaussian mixture and half-moon data, which indicate that carefully\nchosen multiplicative weights increase the recovery power of sum-of-norms\nclustering.",
          "link": "http://arxiv.org/abs/2006.11355",
          "publishedOn": "2021-07-09T01:58:28.441Z",
          "wordCount": 656,
          "title": "Certifying clusters from sum-of-norms clustering. (arXiv:2006.11355v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sumon Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1\">Hridesh Rajan</a>",
          "description": "In recent years, many incidents have been reported where machine learning\nmodels exhibited discrimination among people based on race, sex, age, etc.\nResearch has been conducted to measure and mitigate unfairness in machine\nlearning models. For a machine learning task, it is a common practice to build\na pipeline that includes an ordered set of data preprocessing stages followed\nby a classifier. However, most of the research on fairness has considered a\nsingle classifier based prediction task. What are the fairness impacts of the\npreprocessing stages in machine learning pipeline? Furthermore, studies showed\nthat often the root cause of unfairness is ingrained in the data itself, rather\nthan the model. But no research has been conducted to measure the unfairness\ncaused by a specific transformation made in the data preprocessing stage. In\nthis paper, we introduced the causal method of fairness to reason about the\nfairness impact of data preprocessing stages in ML pipeline. We leveraged\nexisting metrics to define the fairness measures of the stages. Then we\nconducted a detailed fairness evaluation of the preprocessing stages in 37\npipelines collected from three different sources. Our results show that certain\ndata transformers are causing the model to exhibit unfairness. We identified a\nnumber of fairness patterns in several categories of data transformers.\nFinally, we showed how the local fairness of a preprocessing stage composes in\nthe global fairness of the pipeline. We used the fairness composition to choose\nappropriate downstream transformer that mitigates unfairness in the machine\nlearning pipeline.",
          "link": "http://arxiv.org/abs/2106.06054",
          "publishedOn": "2021-07-09T01:58:28.434Z",
          "wordCount": 764,
          "title": "Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mantovani_R/0/1/0/all/0/1\">Rafael Gomes Mantovani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_A/0/1/0/all/0/1\">Andr&#xe9; Luis Debiaso Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcobaca_E/0/1/0/all/0/1\">Edesio Alcoba&#xe7;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gertrudes_J/0/1/0/all/0/1\">Jadson Castro Gertrudes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_S/0/1/0/all/0/1\">Sylvio Barbon Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1\">Andr&#xe9; Carlos Ponce de Leon Ferreira de Carvalho</a>",
          "description": "Machine Learning (ML) algorithms have been increasingly applied to problems\nfrom several different areas. Despite their growing popularity, their\npredictive performance is usually affected by the values assigned to their\nhyperparameters (HPs). As consequence, researchers and practitioners face the\nchallenge of how to set these values. Many users have limited knowledge about\nML algorithms and the effect of their HP values and, therefore, do not take\nadvantage of suitable settings. They usually define the HP values by trial and\nerror, which is very subjective, not guaranteed to find good values and\ndependent on the user experience. Tuning techniques search for HP values able\nto maximize the predictive performance of induced models for a given dataset,\nbut have the drawback of a high computational cost. Thus, practitioners use\ndefault values suggested by the algorithm developer or by tools implementing\nthe algorithm. Although default values usually result in models with acceptable\npredictive performance, different implementations of the same algorithm can\nsuggest distinct default values. To maintain a balance between tuning and using\ndefault values, we propose a strategy to generate new optimized default values.\nOur approach is grounded on a small set of optimized values able to obtain\npredictive performance values better than default settings provided by popular\ntools. After performing a large experiment and a careful analysis of the\nresults, we concluded that our approach delivers better default values.\nBesides, it leads to competitive solutions when compared to tuned values,\nmaking it easier to use and having a lower cost. We also extracted simple rules\nto guide practitioners in deciding whether to use our new methodology or a HP\ntuning approach.",
          "link": "http://arxiv.org/abs/2008.00025",
          "publishedOn": "2021-07-09T01:58:28.426Z",
          "wordCount": 776,
          "title": "Rethinking Default Values: a Low Cost and Efficient Strategy to Define Hyperparameters. (arXiv:2008.00025v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14257",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Gupta_K/0/1/0/all/0/1\">Kanhaiya Gupta</a>",
          "description": "In recent years, artificial neural networks (ANNs) have won numerous contests\nin pattern recognition and machine learning. ANNS have been applied to problems\nranging from speech recognition to prediction of protein secondary structure,\nclassification of cancers, and gene prediction. Here, we intend to maximize the\nchances of finding the Higgs boson decays to two $\\tau$ leptons in the pseudo\ndataset using a Machine Learning technique to classify the recorded events as\nsignal or background.",
          "link": "http://arxiv.org/abs/2106.14257",
          "publishedOn": "2021-07-09T01:58:28.418Z",
          "wordCount": 546,
          "title": "Use of Machine Learning Technique to maximize the signal over background for $H \\rightarrow \\tau \\tau$. (arXiv:2106.14257v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.13298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhoushi_M/0/1/0/all/0/1\">Mostafa Elhoushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiq_F/0/1/0/all/0/1\">Farhan Shafiq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Henry Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Joey Yiwei Li</a>",
          "description": "The high computation, memory, and power budgets of inferring convolutional\nneural networks (CNNs) are major bottlenecks of model deployment to edge\ncomputing platforms, e.g., mobile devices and IoT. Moreover, training CNNs is\ntime and energy-intensive even on high-grade servers. Convolution layers and\nfully connected layers, because of their intense use of multiplications, are\nthe dominant contributor to this computation budget.\n\nWe propose to alleviate this problem by introducing two new operations:\nconvolutional shifts and fully-connected shifts which replace multiplications\nwith bitwise shift and sign flipping during both training and inference. During\ninference, both approaches require only 5 bits (or less) to represent the\nweights. This family of neural network architectures (that use convolutional\nshifts and fully connected shifts) is referred to as DeepShift models. We\npropose two methods to train DeepShift models: DeepShift-Q which trains regular\nweights constrained to powers of 2, and DeepShift-PS that trains the values of\nthe shifts and sign flips directly.\n\nVery close accuracy, and in some cases higher accuracy, to baselines are\nachieved. Converting pre-trained 32-bit floating-point baseline models of\nResNet18, ResNet50, VGG16, and GoogleNet to DeepShift and training them for 15\nto 30 epochs, resulted in Top-1/Top-5 accuracies higher than that of the\noriginal model.\n\nLast but not least, we implemented the convolutional shifts and fully\nconnected shift GPU kernels and showed a reduction in latency time of 25% when\ninferring ResNet18 compared to unoptimized multiplication-based GPU kernels.\nThe code can be found at https://github.com/mostafaelhoushi/DeepShift.",
          "link": "http://arxiv.org/abs/1905.13298",
          "publishedOn": "2021-07-09T01:58:28.398Z",
          "wordCount": 817,
          "title": "DeepShift: Towards Multiplication-Less Neural Networks. (arXiv:1905.13298v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klink_P/0/1/0/all/0/1\">Pascal Klink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulsamad_H/0/1/0/all/0/1\">Hany Abdulsamad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belousov_B/0/1/0/all/0/1\">Boris Belousov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DEramo_C/0/1/0/all/0/1\">Carlo D&#x27;Eramo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1\">Jan Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajarinen_J/0/1/0/all/0/1\">Joni Pajarinen</a>",
          "description": "Across machine learning, the use of curricula has shown strong empirical\npotential to improve learning from data by avoiding local optima of training\nobjectives. For reinforcement learning (RL), curricula are especially\ninteresting, as the underlying optimization has a strong tendency to get stuck\nin local optima due to the exploration-exploitation trade-off. Recently, a\nnumber of approaches for an automatic generation of curricula for RL have been\nshown to increase performance while requiring less expert knowledge compared to\nmanually designed curricula. However, these approaches are seldomly\ninvestigated from a theoretical perspective, preventing a deeper understanding\nof their mechanics. In this paper, we present an approach for automated\ncurriculum generation in RL with a clear theoretical underpinning. More\nprecisely, we formalize the well-known self-paced learning paradigm as inducing\na distribution over training tasks, which trades off between task complexity\nand the objective to match a desired task distribution. Experiments show that\ntraining on this induced distribution helps to avoid poor local optima across\nRL algorithms in different tasks with uninformative rewards and challenging\nexploration requirements.",
          "link": "http://arxiv.org/abs/2102.13176",
          "publishedOn": "2021-07-09T01:58:28.391Z",
          "wordCount": 642,
          "title": "A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning. (arXiv:2102.13176v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stadler_T/0/1/0/all/0/1\">Theresa Stadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oprisanu_B/0/1/0/all/0/1\">Bristena Oprisanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troncoso_C/0/1/0/all/0/1\">Carmela Troncoso</a>",
          "description": "Synthetic data has been advertised as a silver-bullet solution to\nprivacy-preserving data publishing that addresses the shortcomings of\ntraditional anonymisation techniques. The promise is that synthetic data drawn\nfrom generative models preserves the statistical properties of the original\ndataset but, at the same time, provides perfect protection against privacy\nattacks. In this work, we present the first quantitative evaluation of the\nprivacy gain of synthetic data publishing and compare it to that of previous\nanonymisation techniques.\n\nOur evaluation of a wide range of state-of-the-art generative models\ndemonstrates that synthetic data either does not prevent inference attacks or\ndoes not retain data utility. In other words, we empirically show that\nsynthetic data suffers from the same limitations as traditional anonymisation\ntechniques.\n\nFurthermore, we find that, in contrast to traditional anonymisation, the\nprivacy-utility tradeoff of synthetic data publishing is hard to predict.\nBecause it is impossible to predict what signals a synthetic dataset will\npreserve and what information will be lost, synthetic data leads to a highly\nvariable privacy gain and unpredictable utility loss. In summary, we find that\nsynthetic data is far from the holy grail of privacy-preserving data\npublishing.",
          "link": "http://arxiv.org/abs/2011.07018",
          "publishedOn": "2021-07-09T01:58:28.384Z",
          "wordCount": 668,
          "title": "Synthetic Data -- Anonymisation Groundhog Day. (arXiv:2011.07018v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenshuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Queralta_J/0/1/0/all/0/1\">Jorge Pe&#xf1;a Queralta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westerlund_T/0/1/0/all/0/1\">Tomi Westerlund</a>",
          "description": "Deep reinforcement learning has recently seen huge success across multiple\nareas in the robotics domain. Owing to the limitations of gathering real-world\ndata, i.e., sample inefficiency and the cost of collecting it, simulation\nenvironments are utilized for training the different agents. This not only aids\nin providing a potentially infinite data source, but also alleviates safety\nconcerns with real robots. Nonetheless, the gap between the simulated and real\nworlds degrades the performance of the policies once the models are transferred\ninto real robots. Multiple research efforts are therefore now being directed\ntowards closing this sim-to-real gap and accomplish more efficient policy\ntransfer. Recent years have seen the emergence of multiple methods applicable\nto different domains, but there is a lack, to the best of our knowledge, of a\ncomprehensive review summarizing and putting into context the different\nmethods. In this survey paper, we cover the fundamental background behind\nsim-to-real transfer in deep reinforcement learning and overview the main\nmethods being utilized at the moment: domain randomization, domain adaptation,\nimitation learning, meta-learning and knowledge distillation. We categorize\nsome of the most relevant recent works, and outline the main application\nscenarios. Finally, we discuss the main opportunities and challenges of the\ndifferent approaches and point to the most promising directions.",
          "link": "http://arxiv.org/abs/2009.13303",
          "publishedOn": "2021-07-09T01:58:28.378Z",
          "wordCount": 695,
          "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey. (arXiv:2009.13303v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dornheim_J/0/1/0/all/0/1\">Johannes Dornheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morand_L/0/1/0/all/0/1\">Lukas Morand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeitvogel_S/0/1/0/all/0/1\">Samuel Zeitvogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iraki_T/0/1/0/all/0/1\">Tarek Iraki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Link_N/0/1/0/all/0/1\">Norbert Link</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helm_D/0/1/0/all/0/1\">Dirk Helm</a>",
          "description": "A major goal of materials design is to find material structures with desired\nproperties and in a second step to find a processing path to reach one of these\nstructures. In this paper, we propose and investigate a deep reinforcement\nlearning approach for the optimization of processing paths. The goal is to find\noptimal processing paths in the material structure space that lead to\ntarget-structures, which have been identified beforehand to result in desired\nmaterial properties. There exists a target set containing one or multiple\ndifferent structures. Our proposed methods can find an optimal path from a\nstart structure to a single target structure, or optimize the processing paths\nto one of the equivalent target-structures in the set. In the latter case, the\nalgorithm learns during processing to simultaneously identify the best\nreachable target structure and the optimal path to it. The proposed methods\nbelong to the family of model-free deep reinforcement learning algorithms. They\nare guided by structure representations as features of the process state and by\na reward signal, which is formulated based on a distance function in the\nstructure space. Model-free reinforcement learning algorithms learn through\ntrial and error while interacting with the process. Thereby, they are not\nrestricted to information from a priori sampled processing data and are able to\nadapt to the specific process. The optimization itself is model-free and does\nnot require any prior knowledge about the process itself. We instantiate and\nevaluate the proposed methods by optimizing paths of a generic metal forming\nprocess. We show the ability of both methods to find processing paths leading\nclose to target structures and the ability of the extended method to identify\ntarget-structures that can be reached effectively and efficiently and to focus\non these targets for sample efficient processing path optimization.",
          "link": "http://arxiv.org/abs/2009.09706",
          "publishedOn": "2021-07-09T01:58:28.368Z",
          "wordCount": 806,
          "title": "Deep Reinforcement Learning Methods for Structure-Guided Processing Path Optimization. (arXiv:2009.09706v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1\">Ekdeep Singh Lubana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1\">Hidenori Tanaka</a>",
          "description": "Inspired by BatchNorm, there has been an explosion of normalization layers\nfor deep neural networks (DNNs). However, these alternative normalization\nlayers have seen minimal use, partially due to a lack of guiding principles\nthat can help identify when these layers can serve as a replacement for\nBatchNorm. To address this problem, we take a theoretical approach,\ngeneralizing the known beneficial mechanisms of BatchNorm to several recently\nproposed normalization techniques. Our generalized theory leads to the\nfollowing set of principles: (i) similar to BatchNorm, activations-based\nnormalization layers can prevent exponential growth of activations in ResNets,\nbut parametric layers require explicit remedies; (ii) use of GroupNorm can\nensure informative forward propagation, with different samples being assigned\ndissimilar activations, but increasing group size results in increasingly\nindistinguishable activations for different samples, explaining slow\nconvergence speed in models with LayerNorm; (iii) small group sizes result in\nlarge gradient norm in earlier layers, hence explaining training instability\nissues in Instance Normalization and illustrating a speed-stability tradeoff in\nGroupNorm. Overall, our analysis reveals a unified set of mechanisms that\nunderpin the success of normalization methods in deep learning, providing us\nwith a compass to systematically explore the vast design space of DNN\nnormalization layers.",
          "link": "http://arxiv.org/abs/2106.05956",
          "publishedOn": "2021-07-09T01:58:28.349Z",
          "wordCount": 669,
          "title": "Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning. (arXiv:2106.05956v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smirnov_O/0/1/0/all/0/1\">Oleg Smirnov</a>",
          "description": "The adoption of neural networks and deep learning in non-Euclidean domains\nhas been hindered until recently by the lack of scalable and efficient learning\nframeworks. Existing toolboxes in this space were mainly motivated by research\nand education use cases, whereas practical aspects, such as deploying and\nmaintaining machine learning models, were often overlooked.\n\nWe attempt to bridge this gap by proposing TensorFlow RiemOpt, a Python\nlibrary for optimization on Riemannian manifolds in TensorFlow. The library is\ndesigned with the aim for a seamless integration with the TensorFlow ecosystem,\ntargeting not only research, but also streamlining production machine learning\npipelines.",
          "link": "http://arxiv.org/abs/2105.13921",
          "publishedOn": "2021-07-09T01:58:28.343Z",
          "wordCount": 568,
          "title": "TensorFlow RiemOpt: a library for optimization on Riemannian manifolds. (arXiv:2105.13921v2 [cs.MS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>",
          "description": "We show that cascaded diffusion models are capable of generating high\nfidelity images on the class-conditional ImageNet generation challenge, without\nany assistance from auxiliary image classifiers to boost sample quality. A\ncascaded diffusion model comprises a pipeline of multiple diffusion models that\ngenerate images of increasing resolution, beginning with a standard diffusion\nmodel at the lowest resolution, followed by one or more super-resolution\ndiffusion models that successively upsample the image and add higher resolution\ndetails. We find that the sample quality of a cascading pipeline relies\ncrucially on conditioning augmentation, our proposed method of data\naugmentation of the lower resolution conditioning inputs to the\nsuper-resolution models. Our experiments show that conditioning augmentation\nprevents compounding error during sampling in a cascaded model, helping us to\ntrain cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at\n128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and\nclassification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256,\noutperforming VQ-VAE-2.",
          "link": "http://arxiv.org/abs/2106.15282",
          "publishedOn": "2021-07-09T01:58:28.336Z",
          "wordCount": 624,
          "title": "Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1\">Michal Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Krishna Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>",
          "description": "Knowledge distillation is widely used as a means of improving the performance\nof a relatively simple student model using the predictions from a complex\nteacher model. Several works have shown that distillation significantly boosts\nthe student's overall performance; however, are these gains uniform across all\ndata subgroups? In this paper, we show that distillation can harm performance\non certain subgroups, e.g., classes with few associated samples. We trace this\nbehaviour to errors made by the teacher distribution being transferred to and\namplified by the student model. To mitigate this problem, we present techniques\nwhich soften the teacher influence for subgroups where it is less reliable.\nExperiments on several image classification benchmarks show that these\nmodifications of distillation maintain boost in overall accuracy, while\nadditionally ensuring improvement in subgroup performance.",
          "link": "http://arxiv.org/abs/2106.10494",
          "publishedOn": "2021-07-09T01:58:28.330Z",
          "wordCount": 595,
          "title": "Teacher's pet: understanding and mitigating biases in distillation. (arXiv:2106.10494v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yan Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhiwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuaiji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongtu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jieping Ye</a>",
          "description": "We present a new practical framework based on deep reinforcement learning and\ndecision-time planning for real-world vehicle repositioning on ride-hailing (a\ntype of mobility-on-demand, MoD) platforms. Our approach learns the\nspatiotemporal state-value function using a batch training algorithm with deep\nvalue networks. The optimal repositioning action is generated on-demand through\nvalue-based policy search, which combines planning and bootstrapping with the\nvalue networks. For the large-fleet problems, we develop several algorithmic\nfeatures that we incorporate into our framework and that we demonstrate to\ninduce coordination among the algorithmically-guided vehicles. We benchmark our\nalgorithm with baselines in a ride-hailing simulation environment to\ndemonstrate its superiority in improving income efficiency meausred by\nincome-per-hour. We have also designed and run a real-world experiment program\nwith regular drivers on a major ride-hailing platform. We have observed\nsignificantly positive results on key metrics comparing our method with\nexperienced drivers who performed idle-time repositioning based on their own\nexpertise.",
          "link": "http://arxiv.org/abs/2103.04555",
          "publishedOn": "2021-07-09T01:58:28.323Z",
          "wordCount": 635,
          "title": "Real-world Ride-hailing Vehicle Repositioning using Deep Reinforcement Learning. (arXiv:2103.04555v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Le Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1\">Tao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chaochun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Liefeng Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Wen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>",
          "description": "We investigate large-scale latent variable models (LVMs) for neural story\ngeneration -- an under-explored application for open-domain long text -- with\nobjectives in two threads: generation effectiveness and controllability. LVMs,\nespecially the variational autoencoder (VAE), have achieved both effective and\ncontrollable generation through exploiting flexible distributional latent\nrepresentations. Recently, Transformers and its variants have achieved\nremarkable effectiveness without explicit latent representation learning, thus\nlack satisfying controllability in generation. In this paper, we advocate to\nrevive latent variable modeling, essentially the power of representation\nlearning, in the era of Transformers to enhance controllability without hurting\nstate-of-the-art generation effectiveness. Specifically, we integrate latent\nrepresentation vectors with a Transformer-based pre-trained architecture to\nbuild conditional variational autoencoder (CVAE). Model components such as\nencoder, decoder and the variational posterior are all built on top of\npre-trained language models -- GPT2 specifically in this paper. Experiments\ndemonstrate state-of-the-art conditional generation ability of our model, as\nwell as its excellent representation learning capability and controllability.",
          "link": "http://arxiv.org/abs/2101.00828",
          "publishedOn": "2021-07-09T01:58:28.305Z",
          "wordCount": 632,
          "title": "Transformer-based Conditional Variational Autoencoder for Controllable Story Generation. (arXiv:2101.00828v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Adversarial robustness of deep neural networks has been actively\ninvestigated. However, most existing defense approaches are limited to a\nspecific type of adversarial perturbations. Specifically, they often fail to\noffer resistance to multiple attack types simultaneously, i.e., they lack\nmulti-perturbation robustness. Furthermore, compared to image recognition\nproblems, the adversarial robustness of video recognition models is relatively\nunexplored. While several studies have proposed how to generate adversarial\nvideos, only a handful of approaches about the defense strategies have been\npublished in the literature. In this paper, we propose one of the first defense\nstrategies against multiple types of adversarial videos for video recognition.\nThe proposed method, referred to as MultiBN, performs adversarial training on\nmultiple adversarial video types using multiple independent batch normalization\n(BN) layers with a learning-based BN selection module. With a multiple BN\nstructure, each BN brach is responsible for learning the distribution of a\nsingle perturbation type and thus provides more precise distribution\nestimations. This mechanism benefits dealing with multiple perturbation types.\nThe BN selection module detects the attack type of an input video and sends it\nto the corresponding BN branch, making MultiBN fully automatic and allow\nend-to-end training. Compared to present adversarial training approaches, the\nproposed MultiBN exhibits stronger multi-perturbation robustness against\ndifferent and even unforeseen adversarial video types, ranging from Lp-bounded\nattacks and physically realizable attacks. This holds true on different\ndatasets and target models. Moreover, we conduct an extensive analysis to study\nthe properties of the multiple BN structure.",
          "link": "http://arxiv.org/abs/2009.05244",
          "publishedOn": "2021-07-09T01:58:28.295Z",
          "wordCount": 710,
          "title": "Defending Against Multiple and Unforeseen Adversarial Videos. (arXiv:2009.05244v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vacher_J/0/1/0/all/0/1\">Jonathan Vacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_C/0/1/0/all/0/1\">Claire Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coen_Cagli_R/0/1/0/all/0/1\">Ruben Coen-Cagli</a>",
          "description": "Probabilistic finite mixture models are widely used for unsupervised\nclustering. These models can often be improved by adapting them to the topology\nof the data. For instance, in order to classify spatially adjacent data points\nsimilarly, it is common to introduce a Laplacian constraint on the posterior\nprobability that each data point belongs to a class. Alternatively, the mixing\nprobabilities can be treated as free parameters, while assuming Gauss-Markov or\nmore complex priors to regularize those mixing probabilities. However, these\napproaches are constrained by the shape of the prior and often lead to\ncomplicated or intractable inference. Here, we propose a new parametrization of\nthe Dirichlet distribution to flexibly regularize the mixing probabilities of\nover-parametrized mixture distributions. Using the Expectation-Maximization\nalgorithm, we show that our approach allows us to define any linear update rule\nfor the mixing probabilities, including spatial smoothing regularization as a\nspecial case. We then show that this flexible design can be extended to share\nclass information between multiple mixture models. We apply our algorithm to\nartificial and natural image segmentation tasks, and we provide quantitative\nand qualitative comparison of the performance of Gaussian and Student-t\nmixtures on the Berkeley Segmentation Dataset. We also demonstrate how to\npropagate class information across the layers of deep convolutional neural\nnetworks in a probabilistically optimal way, suggesting a new interpretation\nfor feedback signals in biological visual systems. Our flexible approach can be\neasily generalized to adapt probabilistic mixture models to arbitrary data\ntopologies.",
          "link": "http://arxiv.org/abs/1905.10629",
          "publishedOn": "2021-07-09T01:58:28.288Z",
          "wordCount": 731,
          "title": "Flexibly Regularized Mixture Models and Application to Image Segmentation. (arXiv:1905.10629v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.09744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_K/0/1/0/all/0/1\">Kai Ming Ting</a>",
          "description": "This paper presents a new insight into improving the performance of\nStochastic Neighbour Embedding (t-SNE) by using Isolation kernel instead of\nGaussian kernel. Isolation kernel outperforms Gaussian kernel in two aspects.\nFirst, the use of Isolation kernel in t-SNE overcomes the drawback of\nmisrepresenting some structures in the data, which often occurs when Gaussian\nkernel is applied in t-SNE. This is because Gaussian kernel determines each\nlocal bandwidth based on one local point only, while Isolation kernel is\nderived directly from the data based on space partitioning. Second, the use of\nIsolation kernel yields a more efficient similarity computation because\ndata-dependent Isolation kernel has only one parameter that needs to be tuned.\nIn contrast, the use of data-independent Gaussian kernel increases the\ncomputational cost by determining n bandwidths for a dataset of n points. As\nthe root cause of these deficiencies in t-SNE is Gaussian kernel, we show that\nsimply replacing Gaussian kernel with Isolation kernel in t-SNE significantly\nimproves the quality of the final visualisation output (without creating\nmisrepresented structures) and removes one key obstacle that prevents t-SNE\nfrom processing large datasets. Moreover, Isolation kernel enables t-SNE to\ndeal with large-scale datasets in less runtime without trading off accuracy,\nunlike existing methods in speeding up t-SNE.",
          "link": "http://arxiv.org/abs/1906.09744",
          "publishedOn": "2021-07-09T01:58:28.281Z",
          "wordCount": 696,
          "title": "Improving the Effectiveness and Efficiency of Stochastic Neighbour Embedding with Isolation Kernel. (arXiv:1906.09744v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Luwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "Although many techniques have been applied to matrix factorization (MF), they\nmay not fully exploit the feature structure. In this paper, we incorporate the\ngrouping effect into MF and propose a novel method called Robust Matrix\nFactorization with Grouping effect (GRMF). The grouping effect is a\ngeneralization of the sparsity effect, which conducts denoising by clustering\nsimilar values around multiple centers instead of just around 0. Compared with\nexisting algorithms, the proposed GRMF can automatically learn the grouping\nstructure and sparsity in MF without prior knowledge, by introducing a\nnaturally adjustable non-convex regularization to achieve simultaneous sparsity\nand grouping effect. Specifically, GRMF uses an efficient alternating\nminimization framework to perform MF, in which the original non-convex problem\nis first converted into a convex problem through Difference-of-Convex (DC)\nprogramming, and then solved by Alternating Direction Method of Multipliers\n(ADMM). In addition, GRMF can be easily extended to the Non-negative Matrix\nFactorization (NMF) settings. Extensive experiments have been conducted using\nreal-world data sets with outliers and contaminated noise, where the\nexperimental results show that GRMF has promoted performance and robustness,\ncompared to five benchmark algorithms.",
          "link": "http://arxiv.org/abs/2106.13681",
          "publishedOn": "2021-07-09T01:58:28.273Z",
          "wordCount": 654,
          "title": "Robust Matrix Factorization with Grouping Effect. (arXiv:2106.13681v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Regatti_J/0/1/0/all/0/1\">Jayanth Reddy Regatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1\">Aniket Anand Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manavoglu_E/0/1/0/all/0/1\">Eren Manavoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogan_U/0/1/0/all/0/1\">Urun Dogan</a>",
          "description": "Recent advances in deep clustering and unsupervised representation learning\nare based on the idea that different views of an input image (generated through\ndata augmentation techniques) must either be closer in the representation\nspace, or have a similar cluster assignment. Bootstrap Your Own Latent (BYOL)\nis one such representation learning algorithm that has achieved\nstate-of-the-art results in self-supervised image classification on ImageNet\nunder the linear evaluation protocol. However, the utility of the learnt\nfeatures of BYOL to perform clustering is not explored. In this work, we study\nthe clustering ability of BYOL and observe that features learnt using BYOL may\nnot be optimal for clustering. We propose a novel consensus clustering based\nloss function, and train BYOL with the proposed loss in an end-to-end way that\nimproves the clustering ability and outperforms similar clustering based\nmethods on some popular computer vision datasets.",
          "link": "http://arxiv.org/abs/2010.01245",
          "publishedOn": "2021-07-09T01:58:28.252Z",
          "wordCount": 623,
          "title": "Consensus Clustering With Unsupervised Representation Learning. (arXiv:2010.01245v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wangyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "There is a prevailing trend to study urban morphology quantitatively thanks\nto the growing accessibility to various forms of spatial big data, increasing\ncomputing power, and use cases benefiting from such information. The methods\ndeveloped up to now measure urban morphology with numerical indices describing\ndensity, proportion, and mixture, but they do not directly represent\nmorphological features from the human's visual and intuitive perspective. We\ntake the first step to bridge the gap by proposing a deep learning-based\ntechnique to automatically classify road networks into four classes on a visual\nbasis. The method is implemented by generating an image of the street network\n(Colored Road Hierarchy Diagram), which we introduce in this paper, and\nclassifying it using a deep convolutional neural network (ResNet-34). The model\nachieves an overall classification accuracy of 0.875. Nine cities around the\nworld are selected as the study areas with their road networks acquired from\nOpenStreetMap. Latent subgroups among the cities are uncovered through\nclustering on the percentage of each road network category. In the subsequent\npart of the paper, we focus on the usability of such classification: we apply\nour method in a case study of urban vitality prediction. An advanced tree-based\nregression model (LightGBM) is for the first time designated to establish the\nrelationship between morphological indices and vitality indicators. The effect\nof road network classification is found to be small but positively associated\nwith urban vitality. This work expands the toolkit of quantitative urban\nmorphology study with new techniques, supporting further studies in the future.",
          "link": "http://arxiv.org/abs/2105.09908",
          "publishedOn": "2021-07-09T01:58:28.245Z",
          "wordCount": 726,
          "title": "Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>",
          "description": "Recent studies have shown that providing personalized explanations alongside\nrecommendations increases trust and perceived quality. Furthermore, it gives\nusers an opportunity to refine the recommendations by critiquing parts of the\nexplanations. On one hand, current recommender systems model the\nrecommendation, explanation, and critiquing objectives jointly, but this\ncreates an inherent trade-off between their respective performance. On the\nother hand, although recent latent linear critiquing approaches are built upon\nan existing recommender system, they suffer from computational inefficiency at\ninference due to the objective optimized at each conversation's turn. We\naddress these deficiencies with M&Ms-VAE, a novel variational autoencoder for\nrecommendation and explanation that is based on multimodal modeling\nassumptions. We train the model under a weak supervision scheme to simulate\nboth fully and partially observed variables. Then, we leverage the\ngeneralization ability of a trained M&Ms-VAE model to embed the user preference\nand the critique separately. Our work's most important innovation is our\ncritiquing module, which is built upon and trained in a self-supervised manner\nwith a simple ranking objective. Experiments on four real-world datasets\ndemonstrate that among state-of-the-art models, our system is the first to\ndominate or match the performance in terms of recommendation, explanation, and\nmulti-step critiquing. Moreover, M&Ms-VAE processes the critiques up to 25.6x\nfaster than the best baselines. Finally, we show that our model infers coherent\njoint and cross generation, even under weak supervision, thanks to our\nmultimodal-based modeling and training scheme.",
          "link": "http://arxiv.org/abs/2105.00774",
          "publishedOn": "2021-07-09T01:58:28.228Z",
          "wordCount": 706,
          "title": "Fast Multi-Step Critiquing for VAE-based Recommender Systems. (arXiv:2105.00774v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xiaomin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lihang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jieqiong Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Donglong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanzhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>",
          "description": "Effective molecular representation learning is of great importance to\nfacilitate molecular property prediction, which is a fundamental task for the\ndrug and material industry. Recent advances in graph neural networks (GNNs)\nhave shown great promise in applying GNNs for molecular representation\nlearning. Moreover, a few recent studies have also demonstrated successful\napplications of self-supervised learning methods to pre-train the GNNs to\novercome the problem of insufficient labeled molecules. However, existing GNNs\nand pre-training strategies usually treat molecules as topological graph data\nwithout fully utilizing the molecular geometry information. Whereas, the\nthree-dimensional (3D) spatial structure of a molecule, a.k.a molecular\ngeometry, is one of the most critical factors for determining molecular\nphysical, chemical, and biological properties. To this end, we propose a novel\nGeometry Enhanced Molecular representation learning method (GEM) for Chemical\nRepresentation Learning (ChemRL). At first, we design a geometry-based GNN\narchitecture that simultaneously models atoms, bonds, and bond angles in a\nmolecule. To be specific, we devised double graphs for a molecule: The first\none encodes the atom-bond relations; The second one encodes bond-angle\nrelations. Moreover, on top of the devised GNN architecture, we propose several\nnovel geometry-level self-supervised learning strategies to learn spatial\nknowledge by utilizing the local and global molecular 3D structures. We compare\nChemRL-GEM with various state-of-the-art (SOTA) baselines on different\nmolecular benchmarks and exhibit that ChemRL-GEM can significantly outperform\nall baselines in both regression and classification tasks. For example, the\nexperimental results show an overall improvement of 8.8% on average compared to\nSOTA baselines on the regression tasks, demonstrating the superiority of the\nproposed method.",
          "link": "http://arxiv.org/abs/2106.06130",
          "publishedOn": "2021-07-09T01:58:28.202Z",
          "wordCount": 742,
          "title": "ChemRL-GEM: Geometry Enhanced Molecular Representation Learning for Property Prediction. (arXiv:2106.06130v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.11612",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Erdogdu_M/0/1/0/all/0/1\">Murat A. Erdogdu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hosseinzadeh_R/0/1/0/all/0/1\">Rasa Hosseinzadeh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1\">Matthew S. Zhang</a>",
          "description": "We study sampling from a target distribution $\\nu_* = e^{-f}$ using the\nunadjusted Langevin Monte Carlo (LMC) algorithm when the potential $f$\nsatisfies a strong dissipativity condition and it is first-order smooth with a\nLipschitz gradient. We prove that, initialized with a Gaussian random vector\nthat has sufficiently small variance, iterating the LMC algorithm for\n$\\widetilde{\\mathcal{O}}(\\lambda^2 d\\epsilon^{-1})$ steps is sufficient to\nreach $\\epsilon$-neighborhood of the target in both Chi-squared and Renyi\ndivergence, where $\\lambda$ is the logarithmic Sobolev constant of $\\nu_*$. Our\nresults do not require warm-start to deal with the exponential dimension\ndependency in Chi-squared divergence at initialization. In particular, for\nstrongly convex and first-order smooth potentials, we show that the LMC\nalgorithm achieves the rate estimate $\\widetilde{\\mathcal{O}}(d\\epsilon^{-1})$\nwhich improves the previously known rates in both of these metrics, under the\nsame assumptions. Translating this rate to other metrics, our results also\nrecover the state-of-the-art rate estimates in KL divergence, total variation\nand $2$-Wasserstein distance in the same setup. Finally, as we rely on the\nlogarithmic Sobolev inequality, our framework covers a range of non-convex\npotentials that are first-order smooth and exhibit strong convexity outside of\na compact region.",
          "link": "http://arxiv.org/abs/2007.11612",
          "publishedOn": "2021-07-09T01:58:28.182Z",
          "wordCount": 730,
          "title": "Convergence of Langevin Monte Carlo in Chi-Squared and Renyi Divergence. (arXiv:2007.11612v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.08307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertugli_A/0/1/0/all/0/1\">Alessia Bertugli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1\">Simone Calderara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coscia_P/0/1/0/all/0/1\">Pasquale Coscia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Anticipating human motion in crowded scenarios is essential for developing\nintelligent transportation systems, social-aware robots and advanced video\nsurveillance applications. A key component of this task is represented by the\ninherently multi-modal nature of human paths which makes socially acceptable\nmultiple futures when human interactions are involved. To this end, we propose\na generative architecture for multi-future trajectory predictions based on\nConditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning\nmainly relies on prior belief maps, representing most likely moving directions\nand forcing the model to consider past observed dynamics in generating future\npositions. Human interactions are modeled with a graph-based attention\nmechanism enabling an online attentive hidden state refinement of the recurrent\nestimation. To corroborate our model, we perform extensive experiments on\npublicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS\nSportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its\neffectiveness in crowded scenes compared to several state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2005.08307",
          "publishedOn": "2021-07-09T01:58:28.173Z",
          "wordCount": 628,
          "title": "AC-VRNN: Attentive Conditional-VRNN for Multi-Future Trajectory Prediction. (arXiv:2005.08307v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ai_D/0/1/0/all/0/1\">Dige Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>",
          "description": "In practice, the problems encountered in Neural Architecture Search (NAS)\ntraining are not simple problems, but often a series of difficult combinations\n(wrong compensation estimation, curse of dimension, overfitting, high\ncomplexity, etc.). In this paper, we propose a framework to decouple network\nstructure from operator search space, and use two BOHBs to search\nalternatively. Considering that activation function and initialization are also\nimportant parts of neural network, the generalization ability of the model will\nbe affected. We introduce an activation function and an initialization method\ndomain, and add them into the operator search space to form a generalized\nsearch space, so as to improve the generalization ability of the child model.\nWe then trained a GCN-based predictor using feedback from the child model. This\ncan not only improve the search efficiency, but also solve the problem of\ndimension curse. Next, unlike other NAS studies, we used predictors to analyze\nthe stability of different network structures. Finally, we applied our\nframework to neural structure search and achieved significant improvements on\nmultiple datasets.",
          "link": "http://arxiv.org/abs/2103.11820",
          "publishedOn": "2021-07-09T01:58:27.802Z",
          "wordCount": 676,
          "title": "GPNAS: A Neural Network Architecture Search Framework Based on Graphical Predictor. (arXiv:2103.11820v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03863",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rios_F/0/1/0/all/0/1\">Felix L. Rios</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moffa_G/0/1/0/all/0/1\">Giusi Moffa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kuipers_J/0/1/0/all/0/1\">Jack Kuipers</a>",
          "description": "Describing the relationship between the variables in a study domain and\nmodelling the data generating mechanism is a fundamental problem in many\nempirical sciences. Probabilistic graphical models are one common approach to\ntackle the problem. Learning the graphical structure is computationally\nchallenging and a fervent area of current research with a plethora of\nalgorithms being developed. To facilitate the benchmarking of different\nmethods, we present a novel automated workflow, called benchpress for producing\nscalable, reproducible, and platform-independent benchmarks of structure\nlearning algorithms for probabilistic graphical models. Benchpress is\ninterfaced via a simple JSON-file, which makes it accessible for all users,\nwhile the code is designed in a fully modular fashion to enable researchers to\ncontribute additional methodologies. Benchpress currently provides an interface\nto a large number of state-of-the-art algorithms from libraries such as BiDAG,\nbnlearn, GOBNILP, pcalg, r.blip, scikit-learn, TETRAD, and trilearn as well as\na variety of methods for data generating models and performance evaluation.\nAlongside user-defined models and randomly generated datasets, the software\ntool also includes a number of standard datasets and graphical models from the\nliterature, which may be included in a benchmarking workflow. We demonstrate\nthe applicability of this workflow for learning Bayesian networks in four\ntypical data scenarios. The source code and documentation is publicly available\nfrom this http URL",
          "link": "http://arxiv.org/abs/2107.03863",
          "publishedOn": "2021-07-09T01:58:27.796Z",
          "wordCount": 673,
          "title": "Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models. (arXiv:2107.03863v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03588",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lantian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanlong Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1\">Lei Guo</a>",
          "description": "Dynamical systems with binary-valued observations are widely used in\ninformation industry, technology of biological pharmacy and other fields.\nThough there have been much efforts devoted to the identification of such\nsystems, most of the previous investigations are based on first-order gradient\nalgorithm which usually has much slower convergence rate than the Quasi-Newton\nalgorithm. Moreover, persistence of excitation(PE) conditions are usually\nrequired to guarantee consistent parameter estimates in the existing\nliterature, which are hard to be verified or guaranteed for feedback control\nsystems. In this paper, we propose an online projected Quasi-Newton type\nalgorithm for parameter estimation of stochastic regression models with\nbinary-valued observations and varying thresholds. By using both the stochastic\nLyapunov function and martingale estimation methods, we establish the strong\nconsistency of the estimation algorithm and provide the convergence rate, under\na signal condition which is considerably weaker than the traditional PE\ncondition and coincides with the weakest possible excitation known for the\nclassical least square algorithm of stochastic regression models. Convergence\nof adaptive predictors and their applications in adaptive control are also\ndiscussed.",
          "link": "http://arxiv.org/abs/2107.03588",
          "publishedOn": "2021-07-09T01:58:27.770Z",
          "wordCount": 627,
          "title": "Identification and Adaptation with Binary-Valued Observations under Non-Persistent Excitation Condition. (arXiv:2107.03588v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03738",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Papanikolaou_S/0/1/0/all/0/1\">Stefanos Papanikolaou</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Alava_M/0/1/0/all/0/1\">Mikko J. Alava</a>",
          "description": "Plastic yielding in solids strongly depends on various conditions, such as\ntemperature and loading rate and indeed, sample-dependent knowledge of yield\npoints in structural materials promotes reliability in mechanical behavior.\nCommonly, yielding is measured through controlled mechanical testing at small\nor large scales, in ways that either distinguish elastic (stress) from total\ndeformation measurements, or by identifying plastic slip contributions. In this\npaper we argue that instead of separate elastic/plastic measurements, yielding\ncan be unraveled through statistical analysis of total strain fluctuations\nduring the evolution sequence of profiles measured in-situ, through digital\nimage correlation. We demonstrate two distinct ways of precisely quantifying\nyield locations in widely applicable crystal plasticity models, that apply in\npolycrystalline solids, either by using principal component analysis or\ndiscrete wavelet transforms. We test and compare these approaches in synthetic\ndata of polycrystal simulations and a variety of yielding responses, through\nchanges of the applied loading rates and the strain-rate sensitivity exponents.",
          "link": "http://arxiv.org/abs/2107.03738",
          "publishedOn": "2021-07-09T01:58:27.755Z",
          "wordCount": 604,
          "title": "Direct detection of plasticity onset through total-strain profile evolution. (arXiv:2107.03738v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1\">Vitchyr H. Pong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Ashvin Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1\">Laura Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Catherine Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Meta-reinforcement learning (RL) can be used to train policies that quickly\nadapt to new tasks with orders of magnitude less data than standard RL, but\nthis fast adaptation often comes at the cost of greatly increasing the amount\nof reward supervision during meta-training time. Offline meta-RL removes the\nneed to continuously provide reward supervision because rewards must only be\nprovided once when the offline dataset is generated. In addition to the\nchallenges of offline RL, a unique distribution shift is present in meta RL:\nagents learn exploration strategies that can gather the experience needed to\nlearn a new task, and also learn adaptation strategies that work well when\npresented with the trajectories in the dataset, but the adaptation strategies\nare not adapted to the data distribution that the learned exploration\nstrategies collect. Unlike the online setting, the adaptation and exploration\nstrategies cannot effectively adapt to each other, resulting in poor\nperformance. In this paper, we propose a hybrid offline meta-RL algorithm,\nwhich uses offline data with rewards to meta-train an adaptive policy, and then\ncollects additional unsupervised online data, without any ground truth reward\nlabels, to bridge this distribution shift problem. Our method uses the offline\ndata to learn the distribution of reward functions, which is then sampled to\nself-supervise reward labels for the additional online data. By removing the\nneed to provide reward labels for the online experience, our approach can be\nmore practical to use in settings where reward supervision would otherwise be\nprovided manually. We compare our method to prior work on offline meta-RL on\nsimulated robot locomotion and manipulation tasks and find that using\nadditional data and self-generated rewards significantly improves an agent's\nability to generalize.",
          "link": "http://arxiv.org/abs/2107.03974",
          "publishedOn": "2021-07-09T01:58:27.748Z",
          "wordCount": 722,
          "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision. (arXiv:2107.03974v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trindade_S/0/1/0/all/0/1\">Silvana Trindade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bittencourt_L/0/1/0/all/0/1\">Luiz F. Bittencourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_N/0/1/0/all/0/1\">Nelson L. S. da Fonseca</a>",
          "description": "Federated learning has been explored as a promising solution for training at\nthe edge, where end devices collaborate to train models without sharing data\nwith other entities. Since the execution of these learning models occurs at the\nedge, where resources are limited, new solutions must be developed. In this\npaper, we describe the recent work on resource management at the edge, and\nexplore the challenges and future directions to allow the execution of\nfederated learning at the edge. Some of the problems of this management, such\nas discovery of resources, deployment, load balancing, migration, and energy\nefficiency will be discussed in the paper.",
          "link": "http://arxiv.org/abs/2107.03428",
          "publishedOn": "2021-07-09T01:58:27.731Z",
          "wordCount": 557,
          "title": "Management of Resource at the Network Edge for Federated Learning. (arXiv:2107.03428v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Napoles_G/0/1/0/all/0/1\">Gonzalo N&#xe1;poles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salgueiro_Y/0/1/0/all/0/1\">Yamisleydi Salgueiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grau_I/0/1/0/all/0/1\">Isel Grau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_M/0/1/0/all/0/1\">Maikel Leon Espinosa</a>",
          "description": "Machine learning solutions for pattern classification problems are nowadays\nwidely deployed in society and industry. However, the lack of transparency and\naccountability of most accurate models often hinders their meaningful and safe\nuse. Thus, there is a clear need for developing explainable artificial\nintelligence mechanisms. There exist model-agnostic methods that summarize\nfeature contributions, but their interpretability is limited to specific\npredictions made by black-box models. An open challenge is to develop models\nthat have intrinsic interpretability and produce their own explanations, even\nfor classes of models that are traditionally considered black boxes like\n(recurrent) neural networks. In this paper, we propose an LTCN-based model for\ninterpretable pattern classification of structured data. Our method brings its\nown mechanism for providing explanations by quantifying the relevance of each\nfeature in the decision process. For supporting the interpretability without\naffecting the performance, the model incorporates more flexibility through a\nquasi-nonlinear reasoning rule that allows controlling nonlinearity. Besides,\nwe propose a recurrence-aware decision model that evades the issues posed by\nunique fixed points while introducing a deterministic learning method to\ncompute the learnable parameters. The simulations show that our interpretable\nmodel obtains competitive performance when compared to the state-of-the-art\nwhite and black boxes.",
          "link": "http://arxiv.org/abs/2107.03423",
          "publishedOn": "2021-07-09T01:58:27.725Z",
          "wordCount": 629,
          "title": "Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification. (arXiv:2107.03423v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1\">Chris Chafe</a>",
          "description": "This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet. This is fully probabilistic,\nauto-regressive, and causal, i.e. each sample generated depends only on the\npreviously observed samples. Our approach outperforms a widely used wavenet\narchitecture by up to 9% on a similar dataset for predicting the next step.\nUsing the attention mechanism, we enable the architecture to learn which audio\nsamples are important for the prediction of the future sample. We show how\ncausal transformer generative models can be used for raw waveform synthesis. We\nalso show that this performance can be improved by another 2% by conditioning\nsamples over a wider context. The flexibility of the current model to\nsynthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.",
          "link": "http://arxiv.org/abs/2106.16036",
          "publishedOn": "2021-07-09T01:58:27.719Z",
          "wordCount": 646,
          "title": "A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03520",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Al_Abiad_M/0/1/0/all/0/1\">Mohammed S. Al-Abiad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1\">Md. Zoheb Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Jahangir Hossain</a>",
          "description": "We investigate resource allocation scheme to reduce the energy consumption of\nfederated learning (FL) in the integrated fog-cloud computing enabled\nInternet-of-things (IoT) networks. In the envisioned system, IoT devices are\nconnected with the centralized cloud server (CS) via multiple fog access points\n(F-APs). We consider two different scenarios for training the local models. In\nthe first scenario, local models are trained at the IoT devices and the F-APs\nupload the local model parameters to the CS. In the second scenario, local\nmodels are trained at the F-APs based on the collected data from the IoT\ndevices and the F-APs collaborate with the CS for updating the model\nparameters. Our objective is to minimize the overall energy-consumption of both\nscenarios subject to FL time constraint. Towards this goal, we devise a joint\noptimization of scheduling of IoT devices with the F-APs, transmit power\nallocation, computation frequency allocation at the devices and F-APs and\ndecouple it into two subproblems. In the first subproblem, we optimize the IoT\ndevice scheduling and power allocation, while in the second subproblem, we\noptimize the computation frequency allocation. For each scenario, we develop a\nconflict graph based solution to iteratively solve the two subproblems.\nSimulation results show that the proposed two schemes achieve a considerable\nperformance gain in terms of the energy consumption minimization. The presented\nsimulation results interestingly reveal that for a large number of IoT devices\nand large data sizes, it is more energy efficient to train the local models at\nthe IoT devices instead of the F-APs.",
          "link": "http://arxiv.org/abs/2107.03520",
          "publishedOn": "2021-07-09T01:58:27.699Z",
          "wordCount": 702,
          "title": "Energy Efficient Federated Learning in Integrated Fog-Cloud Computing Enabled Internet-of-Things Networks. (arXiv:2107.03520v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plank_P/0/1/0/all/0/1\">Philipp Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Arjun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wild_A/0/1/0/all/0/1\">Andreas Wild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1\">Wolfgang Maass</a>",
          "description": "In spite of intensive efforts it has remained an open problem to what extent\ncurrent Artificial Intelligence (AI) methods that employ Deep Neural Networks\n(DNNs) can be implemented more energy-efficiently on spike-based neuromorphic\nhardware. This holds in particular for AI methods that solve sequence\nprocessing tasks, a primary application target for spike-based neuromorphic\nhardware. One difficulty is that DNNs for such tasks typically employ Long\nShort-Term Memory (LSTM) units. Yet an efficient emulation of these units in\nspike-based hardware has been missing. We present a biologically inspired\nsolution that solves this problem. This solution enables us to implement a\nmajor class of DNNs for sequence processing tasks such as time series\nclassification and question answering with substantial energy savings on\nneuromorphic hardware. In fact, the Relational Network for reasoning about\nrelations between objects that we use for question answering is the first\nexample of a large DNN that carries out a sequence processing task with\nsubstantial energy-saving on neuromorphic hardware.",
          "link": "http://arxiv.org/abs/2107.03992",
          "publishedOn": "2021-07-09T01:58:27.686Z",
          "wordCount": 623,
          "title": "A Long Short-Term Memory for AI Applications in Spike-based Neuromorphic Hardware. (arXiv:2107.03992v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03920",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dalmasso_N/0/1/0/all/0/1\">Niccol&#xf2; Dalmasso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1\">David Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1\">Rafael Izbicki</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1\">Ann B. Lee</a>",
          "description": "Many areas of science make extensive use of computer simulators that\nimplicitly encode likelihood functions for complex systems. Classical\nstatistical methods are poorly suited for these so-called likelihood-free\ninference (LFI) settings, outside the asymptotic and low-dimensional regimes.\nAlthough new machine learning methods, such as normalizing flows, have\nrevolutionized the sample efficiency and capacity of LFI methods, it remains an\nopen question whether they produce reliable measures of uncertainty. In this\npaper, we present a statistical framework for LFI that unifies classical\nstatistics with modern machine learning to: (1) construct frequentist\nconfidence sets and hypothesis tests with finite-sample guarantees of nominal\ncoverage (type I error control) and power, and (2) provide rigorous diagnostics\nfor assessing empirical coverage over the entire parameter space. We refer to\nour framework as likelihood-free frequentist inference (LF2I). Any method that\nestimates a test statistic, such as the likelihood ratio, can be plugged into\nour framework to create powerful tests and confidence sets with correct\ncoverage. In this work, we specifically study two test statistics (ACORE and\nBFF), which, respectively, maximize versus integrate an odds function over the\nparameter space. Our theoretical and empirical results offer multifaceted\nperspectives on error sources and challenges in likelihood-free frequentist\ninference.",
          "link": "http://arxiv.org/abs/2107.03920",
          "publishedOn": "2021-07-09T01:58:27.643Z",
          "wordCount": 657,
          "title": "Likelihood-Free Frequentist Inference: Bridging Classical Statistics and Machine Learning in Simulation and Uncertainty Quantification. (arXiv:2107.03920v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1909.11201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengjiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shusen Wang</a>",
          "description": "Collaborative learning allows participants to jointly train a model without\ndata sharing. To update the model parameters, the central server broadcasts\nmodel parameters to the clients, and the clients send updating directions such\nas gradients to the server. While data do not leave a client device, the\ncommunicated gradients and parameters will leak a client's privacy. Attacks\nthat infer clients' privacy from gradients and parameters have been developed\nby prior work. Simple defenses such as dropout and differential privacy either\nfail to defend the attacks or seriously hurt test accuracy.\n\nWe propose a practical defense which we call Double-Blind Collaborative\nLearning (DBCL). The high-level idea is to apply random matrix sketching to the\nparameters (aka weights) and re-generate random sketching after each iteration.\nDBCL prevents clients from conducting gradient-based privacy inferences which\nare the most effective attacks. DBCL works because from the attacker's\nperspective, sketching is effectively random noise that outweighs the signal.\nNotably, DBCL does not much increase computation and communication costs and\ndoes not hurt test accuracy at all.",
          "link": "http://arxiv.org/abs/1909.11201",
          "publishedOn": "2021-07-09T01:58:27.634Z",
          "wordCount": 658,
          "title": "Matrix Sketching for Secure Collaborative Machine Learning. (arXiv:1909.11201v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1\">Amin Nikanjam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>",
          "description": "Nowadays, we are witnessing an increasing adoption of Deep Learning (DL)\nbased software systems in many industries. Designing a DL program requires\nconstructing a deep neural network (DNN) and then training it on a dataset.\nThis process requires that developers make multiple architectural (e.g., type,\nsize, number, and order of layers) and configuration (e.g., optimizer,\nregularization methods, and activation functions) choices that affect the\nquality of the DL models, and consequently software quality. An under-specified\nor poorly-designed DL model may train successfully but is likely to perform\npoorly when deployed in production. Design smells in DL programs are poor\ndesign and-or configuration decisions taken during the development of DL\ncomponents, that are likely to have a negative impact on the performance (i.e.,\nprediction accuracy) and then quality of DL based software systems. In this\npaper, we present a catalogue of 8 design smells for a popular DL architecture,\nnamely deep Feedforward Neural Networks which is widely employed in industrial\napplications. The design smells were identified through a review of the\nexisting literature on DL design and a manual inspection of 659 DL programs\nwith performance issues and design inefficiencies. The smells are specified by\ndescribing their context, consequences, and recommended refactorings. To\nprovide empirical evidence on the relevance and perceived impact of the\nproposed design smells, we conducted a survey with 81 DL developers. In\ngeneral, the developers perceived the proposed design smells as reflective of\ndesign or implementation problems, with agreement levels varying between 47\\%\nand 68\\%.",
          "link": "http://arxiv.org/abs/2107.02279",
          "publishedOn": "2021-07-09T01:58:27.622Z",
          "wordCount": 704,
          "title": "Design Smells in Deep Learning Programs: An Empirical Study. (arXiv:2107.02279v2 [cs.SE] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asam_M/0/1/0/all/0/1\">Muhammad Asam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Saddam Hussain Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamal_T/0/1/0/all/0/1\">Tauseef Jamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahoora_U/0/1/0/all/0/1\">Umme Zahoora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asifullah Khan</a>",
          "description": "Malicious activities in cyberspace have gone further than simply hacking\nmachines and spreading viruses. It has become a challenge for a nations\nsurvival and hence has evolved to cyber warfare. Malware is a key component of\ncyber-crime, and its analysis is the first line of defence against attack. This\nwork proposes a novel deep boosted hybrid learning-based malware classification\nframework and named as Deep boosted Feature Space-based Malware classification\n(DFS-MC). In the proposed framework, the discrimination power is enhanced by\nfusing the feature spaces of the best performing customized CNN architectures\nmodels and its discrimination by an SVM for classification. The discrimination\ncapacity of the proposed classification framework is assessed by comparing it\nagainst the standard customized CNNs. The customized CNN models are implemented\nin two ways: softmax classifier and deep hybrid learning-based malware\nclassification. In the hybrid learning, Deep features are extracted from\ncustomized CNN architectures and fed into the conventional machine learning\nclassifier to improve the classification performance. We also introduced the\nconcept of transfer learning in a customized CNN architecture based malware\nclassification framework through fine-tuning. The performance of the proposed\nmalware classification approaches are validated on the MalImg malware dataset\nusing the hold-out cross-validation technique. Experimental comparisons were\nconducted by employing innovative, customized CNN, trained from scratch and\nfine-tuning the customized CNN using transfer learning. The proposed\nclassification framework DFS-MC showed improved results, Accuracy: 98.61%,\nF-score: 0.96, Precision: 0.96, and Recall: 0.96.",
          "link": "http://arxiv.org/abs/2107.04008",
          "publishedOn": "2021-07-09T01:58:27.569Z",
          "wordCount": 677,
          "title": "Malware Classification Using Deep Boosted Learning. (arXiv:2107.04008v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wehbi_M/0/1/0/all/0/1\">Mohamad Wehbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamann_T/0/1/0/all/0/1\">Tim Hamann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_J/0/1/0/all/0/1\">Jens Barth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskofier_B/0/1/0/all/0/1\">Bjoern Eskofier</a>",
          "description": "Online handwriting recognition has been studied for a long time with only few\npracticable results when writing on normal paper. Previous approaches using\nsensor-based devices encountered problems that limited the usage of the\ndeveloped systems in real-world applications. This paper presents a\nwriter-independent system that recognizes characters written on plain paper\nwith the use of a sensor-equipped pen. This system is applicable in real-world\napplications and requires no user-specific training for recognition. The pen\nprovides linear acceleration, angular velocity, magnetic field, and force\napplied by the user, and acts as a digitizer that transforms the analogue\nsignals of the sensors into timeseries data while writing on regular paper. The\ndataset we collected with this pen consists of Latin lower-case and upper-case\nalphabets. We present the results of a convolutional neural network model for\nletter classification and show that this approach is practical and achieves\npromising results for writer-independent character recognition. This work aims\nat providing a realtime handwriting recognition system to be used for writing\non normal paper.",
          "link": "http://arxiv.org/abs/2107.03704",
          "publishedOn": "2021-07-09T01:58:27.552Z",
          "wordCount": 614,
          "title": "Digitizing Handwriting with a Sensor Pen: A Writer-Independent Recognizer. (arXiv:2107.03704v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen-Yu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>",
          "description": "We propose a black-box reduction that turns a certain reinforcement learning\nalgorithm with optimal regret in a (near-)stationary environment into another\nalgorithm with optimal dynamic regret in a non-stationary environment,\nimportantly without any prior knowledge on the degree of non-stationarity. By\nplugging different algorithms into our black-box, we provide a list of examples\nshowing that our approach not only recovers recent results for (contextual)\nmulti-armed bandits achieved by very specialized algorithms, but also\nsignificantly improves the state of the art for (generalized) linear bandits,\nepisodic MDPs, and infinite-horizon MDPs in various ways. Specifically, in most\ncases our algorithm achieves the optimal dynamic regret\n$\\widetilde{\\mathcal{O}}(\\min\\{\\sqrt{LT}, \\Delta^{1/3}T^{2/3}\\})$ where $T$ is\nthe number of rounds and $L$ and $\\Delta$ are the number and amount of changes\nof the world respectively, while previous works only obtain suboptimal bounds\nand/or require the knowledge of $L$ and $\\Delta$.",
          "link": "http://arxiv.org/abs/2102.05406",
          "publishedOn": "2021-07-09T01:58:27.546Z",
          "wordCount": 607,
          "title": "Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach. (arXiv:2102.05406v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.08100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kherad_M/0/1/0/all/0/1\">Mahdi Kherad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bidgoly_A/0/1/0/all/0/1\">Amir Jalaly Bidgoly</a>",
          "description": "When a user connects to the Internet to fulfill his needs, he often\nencounters a huge amount of related information. Recommender systems are the\ntechniques for massively filtering information and offering the items that\nusers find them satisfying and interesting. The advances in machine learning\nmethods, especially deep learning, have led to great achievements in\nrecommender systems, although these systems still suffer from challenges such\nas cold-start and sparsity problems. To solve these problems, context\ninformation such as user communication network is usually used. In this paper,\nwe have proposed a novel recommendation method based on Matrix Factorization\nand graph analysis methods. In addition, we leverage deep Autoencoders to\ninitialize users and items latent factors, and deep embedding method gathers\nusers' latent factors from the user trust graph. The proposed method is\nimplemented on two standard datasets. The experimental results and comparisons\ndemonstrate that the proposed approach is superior to the existing\nstate-of-the-art recommendation methods. Our approach outperforms other\ncomparative methods and achieves great improvements. This work has been\nsubmitted to the IEEE for possible publication. Copyright may be transferred\nwithout notice, after which this version may no longer be accessible",
          "link": "http://arxiv.org/abs/2004.08100",
          "publishedOn": "2021-07-09T01:58:27.539Z",
          "wordCount": 719,
          "title": "Recommendation system using a deep learning and graph analysis approach. (arXiv:2004.08100v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cantador_I/0/1/0/all/0/1\">Iv&#xe1;n Cantador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1\">Andr&#xe9;s Carvallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diez_F/0/1/0/all/0/1\">Fernando Diez</a>",
          "description": "The success of neural network embeddings has entailed a renewed interest in\nusing knowledge graphs for a wide variety of machine learning and information\nretrieval tasks. In particular, recent recommendation methods based on graph\nembeddings have shown state-of-the-art performance. In general, these methods\nencode latent rating patterns and content features. Differently from previous\nwork, in this paper, we propose to exploit embeddings extracted from graphs\nthat combine information from ratings and aspect-based opinions expressed in\ntextual reviews. We then adapt and evaluate state-of-the-art graph embedding\ntechniques over graphs generated from Amazon and Yelp reviews on six domains,\noutperforming baseline recommenders. Additionally, our method has the advantage\nof providing explanations that involve the coverage of aspect-based opinions\ngiven by users about recommended items.",
          "link": "http://arxiv.org/abs/2107.03385",
          "publishedOn": "2021-07-09T01:58:27.533Z",
          "wordCount": 568,
          "title": "Rating and aspect-based opinion graph embeddings for explainable recommendations. (arXiv:2107.03385v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Midtfjord_A/0/1/0/all/0/1\">Alise Danielle Midtfjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_R/0/1/0/all/0/1\">Riccardo De Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huseby_A/0/1/0/all/0/1\">Arne Bang Huseby</a>",
          "description": "The presence of snow and ice on runway surfaces reduces the available\ntire-pavement friction needed for retardation and directional control and\ncauses potential economic and safety threats for the aviation industry during\nthe winter seasons. To activate appropriate safety procedures, pilots need\naccurate and timely information on the actual runway surface conditions. In\nthis study, XGBoost is used to create a combined runway assessment system,\nwhich includes a classifcation model to predict slippery conditions and a\nregression model to predict the level of slipperiness. The models are trained\non weather data and data from runway reports. The runway surface conditions are\nrepresented by the tire-pavement friction coefficient, which is estimated from\nflight sensor data from landing aircrafts. To evaluate the performance of the\nmodels, they are compared to several state-of-the-art runway assessment\nmethods. The XGBoost models identify slippery runway conditions with a ROC AUC\nof 0.95, predict the friction coefficient with a MAE of 0.0254, and outperforms\nall the previous methods. The results show the strong abilities of machine\nlearning methods to model complex, physical phenomena with a good accuracy when\ndomain knowledge is used in the variable extraction. The XGBoost models are\ncombined with SHAP (SHapley Additive exPlanations) approximations to provide a\ncomprehensible decision support system for airport operators and pilots, which\ncan contribute to safer and more economic operations of airport runways.",
          "link": "http://arxiv.org/abs/2107.04010",
          "publishedOn": "2021-07-09T01:58:27.508Z",
          "wordCount": 682,
          "title": "A Machine Learning Approach to Safer Airplane Landings: Predicting Runway Conditions using Weather and Flight Data. (arXiv:2107.04010v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03402",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Katsnelson_M/0/1/0/all/0/1\">Mikhail I. Katsnelson</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Vanchurin_V/0/1/0/all/0/1\">Vitaly Vanchurin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Westerhout_T/0/1/0/all/0/1\">Tom Westerhout</a>",
          "description": "We demonstrate, both analytically and numerically, that learning dynamics of\nneural networks is generically attracted towards a self-organized critical\nstate. The effect can be modeled with quartic interactions between\nnon-trainable variables (e.g. states of neurons) and trainable variables (e.g.\nweight matrix). Non-trainable variables are rapidly driven towards stochastic\nequilibrium and trainable variables are slowly driven towards learning\nequilibrium described by a scale-invariant distribution on a wide range of\nscales. Our results suggest that the scale invariance observed in many physical\nand biological systems might be due to some kind of learning dynamics and\nsupport the claim that the universe might be a neural network.",
          "link": "http://arxiv.org/abs/2107.03402",
          "publishedOn": "2021-07-09T01:58:27.501Z",
          "wordCount": 544,
          "title": "Self-organized criticality in neural networks. (arXiv:2107.03402v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kundu/0/1/0/all/0/1\">Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debasish/0/1/0/all/0/1\">Debasish</a>",
          "description": "This paper presents a multiple learner algorithm called the 'Three Ensemble\nClustering 3EC' algorithm that classifies unlabeled data into quality clusters\nas a part of unsupervised learning. It offers the flexibility to explore the\ncontext of new clusters formed by an ensemble of algorithms based on internal\nvalidation indices.\n\nIt is worth mentioning that the input data set is considered to be a cluster\nof clusters. An anomaly can possibly manifest as a cluster as well. Each\npartitioned cluster is considered to be a new data set and is a candidate to\nexplore the most optimal algorithm and its number of partition splits until a\npredefined stopping criteria is met. The algorithms independently partition the\ndata set into clusters and the quality of the partitioning is assessed by an\nensemble of internal cluster validation indices. The 3EC algorithm presents the\nvalidation index scores from a choice of algorithms and its configuration of\npartitions and it is called the Tau Grid. 3EC chooses the most optimal score.\nThe 3EC algorithm owes its name to the two input ensembles of algorithms and\ninternal validation indices and an output ensemble of final clusters.\n\nQuality plays an important role in this clustering approach and it also acts\nas a stopping criteria from further partitioning. Quality is determined based\non the quality of the clusters provided by an algorithm and its optimal number\nof splits. The 3EC algorithm determines this from the score of the ensemble of\nvalidation indices. The user can configure the stopping criteria by providing\nquality thresholds for the score range of each of the validation indices and\nthe optimal size of the output cluster. The users can experiment with different\nsets of stopping criteria and choose the most 'sensible group' of quality\nclusters",
          "link": "http://arxiv.org/abs/2107.03729",
          "publishedOn": "2021-07-09T01:58:27.494Z",
          "wordCount": 725,
          "title": "The Three Ensemble Clustering (3EC) Algorithm for Pattern Discovery in Unsupervised Learning. (arXiv:2107.03729v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Daniel Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Haidar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Azer Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gittens_A/0/1/0/all/0/1\">Alex Gittens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yener_B/0/1/0/all/0/1\">B&#xfc;lent Yener</a>",
          "description": "Adversarial examples pose a threat to deep neural network models in a variety\nof scenarios, from settings where the adversary has complete knowledge of the\nmodel in a \"white box\" setting and to the opposite in a \"black box\" setting. In\nthis paper, we explore the use of output randomization as a defense against\nattacks in both the black box and white box models and propose two defenses. In\nthe first defense, we propose output randomization at test time to thwart\nfinite difference attacks in black box settings. Since this type of attack\nrelies on repeated queries to the model to estimate gradients, we investigate\nthe use of randomization to thwart such adversaries from successfully creating\nadversarial examples. We empirically show that this defense can limit the\nsuccess rate of a black box adversary using the Zeroth Order Optimization\nattack to 0%. Secondly, we propose output randomization training as a defense\nagainst white box adversaries. Unlike prior approaches that use randomization,\nour defense does not require its use at test time, eliminating the Backward\nPass Differentiable Approximation attack, which was shown to be effective\nagainst other randomization defenses. Additionally, this defense has low\noverhead and is easily implemented, allowing it to be used together with other\ndefenses across various model architectures. We evaluate output randomization\ntraining against the Projected Gradient Descent attacker and show that the\ndefense can reduce the PGD attack's success rate down to 12% when using\ncross-entropy loss.",
          "link": "http://arxiv.org/abs/2107.03806",
          "publishedOn": "2021-07-09T01:58:27.476Z",
          "wordCount": 697,
          "title": "Output Randomization: A Novel Defense for both White-box and Black-box Adversarial Models. (arXiv:2107.03806v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1904.05981",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Pal_S/0/1/0/all/0/1\">Soumik Pal</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhu_Y/0/1/0/all/0/1\">Yizhe Zhu</a>",
          "description": "We consider the community detection problem in sparse random hypergraphs.\nAngelini et al. (2015) conjectured the existence of a sharp threshold on model\nparameters for community detection in sparse hypergraphs generated by a\nhypergraph stochastic block model. We solve the positive part of the conjecture\nfor the case of two blocks: above the threshold, there is a spectral algorithm\nwhich asymptotically almost surely constructs a partition of the hypergraph\ncorrelated with the true partition. Our method is a generalization to random\nhypergraphs of the method developed by Massouli\\'{e} (2014) for sparse random\ngraphs.",
          "link": "http://arxiv.org/abs/1904.05981",
          "publishedOn": "2021-07-09T01:58:27.468Z",
          "wordCount": 597,
          "title": "Community detection in the sparse hypergraph stochastic block model. (arXiv:1904.05981v6 [math.PR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulsky_Y/0/1/0/all/0/1\">Yury Sulsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1\">Arun Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruce_J/0/1/0/all/0/1\">Jake Bruce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1\">Rob Fergus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wayne_G/0/1/0/all/0/1\">Greg Wayne</a>",
          "description": "Imitation learning enables agents to reuse and adapt the hard-won expertise\nof others, offering a solution to several key challenges in learning behavior.\nAlthough it is easy to observe behavior in the real-world, the underlying\nactions may not be accessible. We present a new method for imitation solely\nfrom observations that achieves comparable performance to experts on\nchallenging continuous control tasks while also exhibiting robustness in the\npresence of observations unrelated to the task. Our method, which we call FORM\n(for \"Future Observation Reward Model\") is derived from an inverse RL objective\nand imitates using a model of expert behavior learned by generative modelling\nof the expert's observations, without needing ground truth actions. We show\nthat FORM performs comparably to a strong baseline IRL method (GAIL) on the\nDeepMind Control Suite benchmark, while outperforming GAIL in the presence of\ntask-irrelevant features.",
          "link": "http://arxiv.org/abs/2107.03851",
          "publishedOn": "2021-07-09T01:58:27.456Z",
          "wordCount": 574,
          "title": "Imitation by Predicting Observations. (arXiv:2107.03851v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1710.09064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kankanahalli_S/0/1/0/all/0/1\">Srihari Kankanahalli</a>",
          "description": "Modern compression algorithms are often the result of laborious\ndomain-specific research; industry standards such as MP3, JPEG, and AMR-WB took\nyears to develop and were largely hand-designed. We present a deep neural\nnetwork model which optimizes all the steps of a wideband speech coding\npipeline (compression, quantization, entropy coding, and decompression)\nend-to-end directly from raw speech data -- no manual feature engineering\nnecessary, and it trains in hours. In testing, our DNN-based coder performs on\npar with the AMR-WB standard at a variety of bitrates (~9kbps up to ~24kbps).\nIt also runs in realtime on a 3.8GhZ Intel CPU.",
          "link": "http://arxiv.org/abs/1710.09064",
          "publishedOn": "2021-07-09T01:58:27.445Z",
          "wordCount": 581,
          "title": "End-to-End Optimized Speech Coding with Deep Neural Networks. (arXiv:1710.09064v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03673",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_L/0/1/0/all/0/1\">Lulu Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyu Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ma_Z/0/1/0/all/0/1\">Zheng Ma</a>",
          "description": "In this paper, we propose a model-operator-data network (MOD-Net) for solving\nPDEs. A MOD-Net is driven by a model to solve PDEs based on operator\nrepresentation with regularization from data. In this work, we use a deep\nneural network to parameterize the Green's function. The empirical risk\nconsists of the mean square of the governing equation, boundary conditions, and\na few labels, which are numerically computed by traditional schemes on coarse\ngrid points with cheap computation cost. With only the labeled dataset or only\nthe model constraints, it is insufficient to accurately train a MOD-Net for\ncomplicate problems. Intuitively, the labeled dataset works as a regularization\nin addition to the model constraints. The MOD-Net is much efficient than\noriginal neural operator because the MOD-Net also uses the information of\ngoverning equation and the boundary conditions of the PDE rather than purely\nthe expensive labels. Since the MOD-Net learns the Green's function of a PDE,\nit solves a type of PDEs but not a specific case. We numerically show MOD-Net\nis very efficient in solving Poisson equation and one-dimensional Boltzmann\nequation. For non-linear PDEs, where the concept of the Green's function does\nnot apply, the non-linear MOD-Net can be similarly used as an ansatz for\nsolving non-linear PDEs.",
          "link": "http://arxiv.org/abs/2107.03673",
          "publishedOn": "2021-07-09T01:58:27.432Z",
          "wordCount": 650,
          "title": "MOD-Net: A Machine Learning Approach via Model-Operator-Data Network for Solving PDEs. (arXiv:2107.03673v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuzhamuratov_A/0/1/0/all/0/1\">Arsen Kuzhamuratov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokin_D/0/1/0/all/0/1\">Dmitry Sorokin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulanov_A/0/1/0/all/0/1\">Alexander Ulanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lvovsky_A/0/1/0/all/0/1\">A. I. Lvovsky</a>",
          "description": "Animals have remarkable abilities to adapt locomotion to different terrains\nand tasks. However, robots trained by means of reinforcement learning are\ntypically able to solve only a single task and a transferred policy is usually\ninferior to that trained from scratch. In this work, we demonstrate that\nmeta-reinforcement learning can be used to successfully train a robot capable\nto solve a wide range of locomotion tasks. The performance of the meta-trained\nrobot is similar to that of a robot that is trained on a single task.",
          "link": "http://arxiv.org/abs/2107.03741",
          "publishedOn": "2021-07-09T01:58:27.327Z",
          "wordCount": 521,
          "title": "Adaptation of Quadruped Robot Locomotion with Meta-Learning. (arXiv:2107.03741v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gouttes_A/0/1/0/all/0/1\">Ad&#xe8;le Gouttes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasul_K/0/1/0/all/0/1\">Kashif Rasul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_M/0/1/0/all/0/1\">Mateusz Koren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stephan_J/0/1/0/all/0/1\">Johannes Stephan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naghibi_T/0/1/0/all/0/1\">Tofigh Naghibi</a>",
          "description": "Here, we propose a general method for probabilistic time series forecasting.\nWe combine an autoregressive recurrent neural network to model temporal\ndynamics with Implicit Quantile Networks to learn a large class of\ndistributions over a time-series target. When compared to other probabilistic\nneural forecasting models on real- and simulated data, our approach is\nfavorable in terms of point-wise prediction accuracy as well as on estimating\nthe underlying temporal distribution.",
          "link": "http://arxiv.org/abs/2107.03743",
          "publishedOn": "2021-07-09T01:58:27.320Z",
          "wordCount": 513,
          "title": "Probabilistic Time Series Forecasting with Implicit Quantile Networks. (arXiv:2107.03743v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jetchev_N/0/1/0/all/0/1\">Nikolay Jetchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_G/0/1/0/all/0/1\">G&#xf6;khan Yildirim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bracher_C/0/1/0/all/0/1\">Christian Bracher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollgraf_R/0/1/0/all/0/1\">Roland Vollgraf</a>",
          "description": "Attention is a general reasoning mechanism than can flexibly deal with image\ninformation, but its memory requirements had made it so far impractical for\nhigh resolution image generation. We present Grid Partitioned Attention (GPA),\na new approximate attention algorithm that leverages a sparse inductive bias\nfor higher computational and memory efficiency in image domains: queries attend\nonly to few keys, spatially close queries attend to close keys due to\ncorrelations. Our paper introduces the new attention layer, analyzes its\ncomplexity and how the trade-off between memory usage and model power can be\ntuned by the hyper-parameters.We will show how such attention enables novel\ndeep learning architectures with copying modules that are especially useful for\nconditional image generation tasks like pose morphing. Our contributions are\n(i) algorithm and code1of the novel GPA layer, (ii) a novel deep\nattention-copying architecture, and (iii) new state-of-the art experimental\nresults in human pose morphing generation benchmarks.",
          "link": "http://arxiv.org/abs/2107.03742",
          "publishedOn": "2021-07-09T01:58:27.228Z",
          "wordCount": 607,
          "title": "Grid Partitioned Attention: Efficient TransformerApproximation with Inductive Bias for High Resolution Detail Generation. (arXiv:2107.03742v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1\">Alexandra Peste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1\">Christoph H. Lampert</a>",
          "description": "The availability of large amounts of user-provided data has been key to the\nsuccess of machine learning for many real-world tasks. Recently, an increasing\nawareness has emerged that users should be given more control about how their\ndata is used. In particular, users should have the right to prohibit the use of\ntheir data for training machine learning systems, and to have it erased from\nalready trained systems. While several sample erasure methods have been\nproposed, all of them have drawbacks which have prevented them from gaining\nwidespread adoption. Most methods are either only applicable to very specific\nfamilies of models, sacrifice too much of the original model's accuracy, or\nthey have prohibitive memory or computational requirements. In this paper, we\npropose an efficient and effective algorithm, SSSE, for samples erasure, that\nis applicable to a wide class of machine learning models. From a second-order\nanalysis of the model's loss landscape we derive a closed-form update step of\nthe model parameters that only requires access to the data to be erased, not to\nthe original training set. Experiments on three datasets, CelebFaces attributes\n(CelebA), Animals with Attributes 2 (AwA2) and CIFAR10, show that in certain\ncases SSSE can erase samples almost as well as the optimal, yet impractical,\ngold standard of training a new model from scratch with only the permitted\ndata.",
          "link": "http://arxiv.org/abs/2107.03860",
          "publishedOn": "2021-07-09T01:58:27.174Z",
          "wordCount": 658,
          "title": "SSSE: Efficiently Erasing Samples from Trained Machine Learning Models. (arXiv:2107.03860v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03730",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Qoku_A/0/1/0/all/0/1\">Arber Qoku</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Buettner_F/0/1/0/all/0/1\">Florian Buettner</a>",
          "description": "Latent variable models are powerful statistical tools that can uncover\nrelevant variation between patients or cells, by inferring unobserved hidden\nstates from observable high-dimensional data. A major shortcoming of current\nmethods, however, is their inability to learn sparse and interpretable hidden\nstates. Additionally, in settings where partial knowledge on the latent\nstructure of the data is readily available, a statistically sound integration\nof prior information into current methods is challenging. To address these\nissues, we propose spex-LVM, a factorial latent variable model with sparse\npriors to encourage the inference of explainable factors driven by\ndomain-relevant information. spex-LVM utilizes existing knowledge of curated\nbiomedical pathways to automatically assign annotated attributes to latent\nfactors, yielding interpretable results tailored to the corresponding domain of\ninterest. Evaluations on simulated and real single-cell RNA-seq datasets\ndemonstrate that our model robustly identifies relevant structure in an\ninherently explainable manner, distinguishes technical noise from sources of\nbiomedical variation, and provides dataset-specific adaptations of existing\npathway annotations. Implementation is available at\nhttps://github.com/MLO-lab/spexlvm.",
          "link": "http://arxiv.org/abs/2107.03730",
          "publishedOn": "2021-07-09T01:58:27.163Z",
          "wordCount": 621,
          "title": "Encoding Domain Information with Sparse Priors for Inferring Explainable Latent Variables. (arXiv:2107.03730v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03607",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Dodia_H/0/1/0/all/0/1\">Hrithika Dodia</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Tandel_H/0/1/0/all/0/1\">Himanshu Tandel</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+DMello_L/0/1/0/all/0/1\">Lynette D&#x27;Mello</a>",
          "description": "Gravitational waves are ripples in the fabric of space-time that travel at\nthe speed of light. The detection of gravitational waves by LIGO is a major\nbreakthrough in the field of astronomy. Deep Learning has revolutionized many\nindustries including health care, finance and education. Deep Learning\ntechniques have also been explored for detection of gravitational waves to\novercome the drawbacks of traditional matched filtering method. However, in\nseveral researches, the training phase of neural network is very time consuming\nand hardware devices with large memory are required for the task. In order to\nreduce the extensive amount of hardware resources and time required in training\na neural network for detecting gravitational waves, we made SpecGrav. We use 2D\nConvolutional Neural Network and spectrograms of gravitational waves embedded\nin noise to detect gravitational waves from binary black hole merger and binary\nneutron star merger. The training phase of our neural network was of about just\n19 minutes on a 2GB GPU.",
          "link": "http://arxiv.org/abs/2107.03607",
          "publishedOn": "2021-07-09T01:58:27.110Z",
          "wordCount": 598,
          "title": "SpecGrav -- Detection of Gravitational Waves using Deep Learning. (arXiv:2107.03607v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghanathe_N/0/1/0/all/0/1\">Nikhil Pratap Ghanathe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_V/0/1/0/all/0/1\">Vivek Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rahul Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilton_S/0/1/0/all/0/1\">Steve Wilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aayan Kumar</a>",
          "description": "Recent breakthroughs in ML have produced new classes of models that allow ML\ninference to run directly on milliwatt-powered IoT devices. On one hand,\nexisting ML-to-FPGA compilers are designed for deep neural-networks on large\nFPGAs. On the other hand, general-purpose HLS tools fail to exploit properties\nspecific to ML inference, thereby resulting in suboptimal performance. We\npropose MAFIA, a tool to compile ML inference on small form-factor FPGAs for\nIoT applications. MAFIA provides native support for linear algebra operations\nand can express a variety of ML algorithms, including state-of-the-art models.\nWe show that MAFIA-generated programs outperform best-performing variant of a\ncommercial HLS compiler by 2.5x on average.",
          "link": "http://arxiv.org/abs/2107.03653",
          "publishedOn": "2021-07-09T01:58:27.104Z",
          "wordCount": 571,
          "title": "MAFIA: Machine Learning Acceleration on FPGAs for IoT Applications. (arXiv:2107.03653v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fenaux_L/0/1/0/all/0/1\">Lucas Fenaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quintero_M/0/1/0/all/0/1\">Maria Juliana Quintero</a>",
          "description": "We will introduce BumbleBee, a transformer model that will generate MIDI\nmusic data . We will tackle the issue of transformers applied to long sequences\nby implementing a longformer generative model that uses dilating sliding\nwindows to compute the attention layers. We will compare our results to that of\nthe music transformer and Long-Short term memory (LSTM) to benchmark our\nresults. This analysis will be performed using piano MIDI files, in particular\n, the JSB Chorales dataset that has already been used for other research works\n(Huang et al., 2018)",
          "link": "http://arxiv.org/abs/2107.03443",
          "publishedOn": "2021-07-09T01:58:27.098Z",
          "wordCount": 531,
          "title": "BumbleBee: A Transformer for Music. (arXiv:2107.03443v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Ningyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xuefeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>",
          "description": "We study the model-based undiscounted reinforcement learning for partially\nobservable Markov decision processes (POMDPs). The oracle we consider is the\noptimal policy of the POMDP with a known environment in terms of the average\nreward over an infinite horizon. We propose a learning algorithm for this\nproblem, building on spectral method-of-moments estimations for hidden Markov\nmodels, the belief error control in POMDPs and upper-confidence-bound methods\nfor online learning. We establish a regret bound of $O(T^{2/3}\\sqrt{\\log T})$\nfor the proposed learning algorithm where $T$ is the learning horizon. This is,\nto the best of our knowledge, the first algorithm achieving sublinear regret\nwith respect to our oracle for learning general POMDPs.",
          "link": "http://arxiv.org/abs/2107.03635",
          "publishedOn": "2021-07-09T01:58:27.091Z",
          "wordCount": 540,
          "title": "Sublinear Regret for Learning POMDPs. (arXiv:2107.03635v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianwen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhihua Lin</a>",
          "description": "In existing deep learning methods, almost all loss functions assume that\nsample data values used to be predicted are the only correct ones. This\nassumption does not hold for laboratory test data. Test results are often\nwithin tolerable or imprecision ranges, with all values in the ranges\nacceptable. By considering imprecision samples, we propose an imprecision range\nloss (IR loss) method and incorporate it into Long Short Term Memory (LSTM)\nmodel for disease progress prediction. In this method, each sample in\nimprecision range space has a certain probability to be the real value,\nparticipating in the loss calculation. The loss is defined as the integral of\nthe error of each point in the impression range space. A sampling method for\nimprecision space is formulated. The continuous imprecision space is\ndiscretized, and a sequence of imprecise data sets are obtained, which is\nconvenient for gradient descent learning. A heuristic learning algorithm is\ndeveloped to learn the model parameters based on the imprecise data sets.\nExperimental results on real data show that the prediction method based on IR\nloss can provide more stable and consistent prediction result when test samples\nare generated from imprecision range.",
          "link": "http://arxiv.org/abs/2107.03620",
          "publishedOn": "2021-07-09T01:58:27.069Z",
          "wordCount": 621,
          "title": "Predicting Disease Progress with Imprecise Lab Test Results. (arXiv:2107.03620v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03846",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aertsen_M/0/1/0/all/0/1\">Michael Aertsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Emam_D/0/1/0/all/0/1\">Doaa Emam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mufti_N/0/1/0/all/0/1\">Nada Mufti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guffens_F/0/1/0/all/0/1\">Frederic Guffens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_T/0/1/0/all/0/1\">Thomas Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demaerel_P/0/1/0/all/0/1\">Philippe Demaerel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Melbourne_A/0/1/0/all/0/1\">Andrew Melbourne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1\">Jam Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>",
          "description": "Deep neural networks have increased the accuracy of automatic segmentation,\nhowever, their accuracy depends on the availability of a large number of fully\nsegmented images. Methods to train deep neural networks using images for which\nsome, but not all, regions of interest are segmented are necessary to make\nbetter use of partially annotated datasets. In this paper, we propose the first\naxiomatic definition of label-set loss functions that are the loss functions\nthat can handle partially segmented images. We prove that there is one and only\none method to convert a classical loss function for fully segmented images into\na proper label-set loss function. Our theory also allows us to define the\nleaf-Dice loss, a label-set generalization of the Dice loss particularly suited\nfor partial supervision with only missing labels. Using the leaf-Dice loss, we\nset a new state of the art in partially supervised learning for fetal brain 3D\nMRI segmentation. We achieve a deep neural network able to segment white\nmatter, ventricles, cerebellum, extra-ventricular CSF, cortical gray matter,\ndeep gray matter, brainstem, and corpus callosum based on fetal brain 3D MRI of\nanatomically normal fetuses or with open spina bifida. Our implementation of\nthe proposed label-set loss functions is available at\nhttps://github.com/LucasFidon/label-set-loss-functions",
          "link": "http://arxiv.org/abs/2107.03846",
          "publishedOn": "2021-07-09T01:58:27.054Z",
          "wordCount": 687,
          "title": "Label-set Loss Functions for Partial Supervision: Application to Fetal Brain 3D MRI Parcellation. (arXiv:2107.03846v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_B/0/1/0/all/0/1\">Bong-Chul Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "Effective environmental planning and management to address climate change\ncould be achieved through extensive environmental modeling with machine\nlearning and conventional physical models. In order to develop and improve\nthese models, practitioners and researchers need comprehensive benchmark\ndatasets that are prepared and processed with environmental expertise that they\ncan rely on. This study presents an extensive dataset of rainfall events for\nthe state of Iowa (2016-2019) acquired from the National Weather Service Next\nGeneration Weather Radar (NEXRAD) system and processed by a quantitative\nprecipitation estimation system. The dataset presented in this study could be\nused for better disaster monitoring, response and recovery by paving the way\nfor both predictive and prescriptive modeling.",
          "link": "http://arxiv.org/abs/2107.03432",
          "publishedOn": "2021-07-09T01:58:27.048Z",
          "wordCount": 572,
          "title": "IowaRain: A Statewide Rain Event Dataset Based on Weather Radars and Quantitative Precipitation Estimation. (arXiv:2107.03432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goucher_A/0/1/0/all/0/1\">Adam P. Goucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troll_R/0/1/0/all/0/1\">Rajan Troll</a>",
          "description": "We introduce a differentiable random access memory module with $O(1)$\nperformance regardless of size, scaling to billions of entries. The design\nstores entries on points of a chosen lattice to calculate nearest neighbours of\narbitrary points efficiently by exploiting symmetries. Augmenting a standard\nneural network architecture with a single memory layer based on this, we can\nscale the parameter count up to memory limits with negligible computational\noverhead, giving better accuracy at similar cost. On large language modelling\ntasks, these enhanced models with larger capacity significantly outperform the\nunmodified transformer baseline. We found continued scaling with memory size up\nto the limits tested.",
          "link": "http://arxiv.org/abs/2107.03474",
          "publishedOn": "2021-07-09T01:58:27.041Z",
          "wordCount": 535,
          "title": "Differentiable Random Access Memory using Lattices. (arXiv:2107.03474v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03442",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hamghalam_M/0/1/0/all/0/1\">Mohammad Hamghalam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F. Frangi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simpson_A/0/1/0/all/0/1\">Amber L. Simpson</a>",
          "description": "In large studies involving multi protocol Magnetic Resonance Imaging (MRI),\nit can occur to miss one or more sub-modalities for a given patient owing to\npoor quality (e.g. imaging artifacts), failed acquisitions, or hallway\ninterrupted imaging examinations. In some cases, certain protocols are\nunavailable due to limited scan time or to retrospectively harmonise the\nimaging protocols of two independent studies. Missing image modalities pose a\nchallenge to segmentation frameworks as complementary information contributed\nby the missing scans is then lost. In this paper, we propose a novel model,\nMulti-modal Gaussian Process Prior Variational Autoencoder (MGP-VAE), to impute\none or more missing sub-modalities for a patient scan. MGP-VAE can leverage the\nGaussian Process (GP) prior on the Variational Autoencoder (VAE) to utilize the\nsubjects/patients and sub-modalities correlations. Instead of designing one\nnetwork for each possible subset of present sub-modalities or using frameworks\nto mix feature maps, missing data can be generated from a single model based on\nall the available samples. We show the applicability of MGP-VAE on brain tumor\nsegmentation where either, two, or three of four sub-modalities may be missing.\nOur experiments against competitive segmentation baselines with missing\nsub-modality on BraTS'19 dataset indicate the effectiveness of the MGP-VAE\nmodel for segmentation tasks.",
          "link": "http://arxiv.org/abs/2107.03442",
          "publishedOn": "2021-07-09T01:58:27.021Z",
          "wordCount": 666,
          "title": "Modality Completion via Gaussian Process Prior Variational Autoencoders for Multi-Modal Glioma Segmentation. (arXiv:2107.03442v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gui_X/0/1/0/all/0/1\">Xingtai Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiyang Zhang</a>",
          "description": "Intelligent diagnosis method based on data-driven and deep learning is an\nattractive and meaningful field in recent years. However, in practical\napplication scenarios, the imbalance of time-series fault is an urgent problem\nto be solved. From the perspective of Bayesian probability, this paper analyzes\nhow to improve the performance of imbalanced classification by adjusting the\ndistance between classes and the distribution within a class and proposes a\ntime-series fault diagnosis model based on deep metric learning. As a core of\ndeep metric learning, a novel quadruplet data pair design considering imbalance\nclass is proposed with reference to traditional deep metric learning. Based on\nsuch data pair, this paper proposes a quadruplet loss function which takes into\naccount the inter-class distance and the intra-class data distribution, and\npays special attention to imbalanced sample pairs. The reasonable combination\nof quadruplet loss and softmax loss function can reduce the impact of\nimbalance. Experiments on two open datasets are carried out to verify the\neffectiveness and robustness of the model. Experimental results show that the\nproposed method can effectively improve the performance of imbalanced\nclassification.",
          "link": "http://arxiv.org/abs/2107.03786",
          "publishedOn": "2021-07-09T01:58:27.013Z",
          "wordCount": 615,
          "title": "Quadruplet Deep Metric Learning Model for Imbalanced Time-series Fault Diagnosis. (arXiv:2107.03786v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yikang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhao Zhong</a>",
          "description": "In this paper, we propose a Collaboration of Experts (CoE) framework to pool\ntogether the expertise of multiple networks towards a common aim. Each expert\nis an individual network with expertise on a unique portion of the dataset,\nwhich enhances the collective capacity. Given a sample, an expert is selected\nby the delegator, which simultaneously outputs a rough prediction to support\nearly termination. To fulfill this framework, we propose three modules to impel\neach model to play its role, namely weight generation module (WGM), label\ngeneration module (LGM) and variance calculation module (VCM). Our method\nachieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy\nwith 194M FLOPs. Combined with PWLU activation function and CondConv, CoE\nfurther achieves the accuracy of 80.0% with only 100M FLOPs for the first time.\nMore importantly, our method is hardware friendly and achieves a 3-6x speedup\ncompared with some existing conditional computation approaches.",
          "link": "http://arxiv.org/abs/2107.03815",
          "publishedOn": "2021-07-09T01:58:27.006Z",
          "wordCount": 594,
          "title": "Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs. (arXiv:2107.03815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03455",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1\">Avishek Ghosh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sankararaman_A/0/1/0/all/0/1\">Abishek Sankararaman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>",
          "description": "We consider the problem of model selection for the general stochastic\ncontextual bandits under the realizability assumption. We propose a successive\nrefinement based algorithm called Adaptive Contextual Bandit ({\\ttfamily ACB}),\nthat works in phases and successively eliminates model classes that are too\nsimple to fit the given instance. We prove that this algorithm is adaptive,\ni.e., the regret rate order-wise matches that of {\\ttfamily FALCON}, the\nstate-of-art contextual bandit algorithm of Levi et. al '20, that needs\nknowledge of the true model class. The price of not knowing the correct model\nclass is only an additive term contributing to the second order term in the\nregret bound. This cost possess the intuitive property that it becomes smaller\nas the model class becomes easier to identify, and vice-versa. We then show\nthat a much simpler explore-then-commit (ETC) style algorithm also obtains a\nregret rate of matching that of {\\ttfamily FALCON}, despite not knowing the\ntrue model class. However, the cost of model selection is higher in ETC as\nopposed to in {\\ttfamily ACB}, as expected. Furthermore, {\\ttfamily ACB}\napplied to the linear bandit setting with unknown sparsity, order-wise recovers\nthe model selection guarantees previously established by algorithms tailored to\nthe linear setting.",
          "link": "http://arxiv.org/abs/2107.03455",
          "publishedOn": "2021-07-09T01:58:26.990Z",
          "wordCount": 648,
          "title": "Model Selection for Generic Contextual Bandits. (arXiv:2107.03455v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1\">Michael A. Hedderich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alex Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>",
          "description": "Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning,\nco-located with ICLR 2021. In this workshop, we want to advance theory, methods\nand tools for allowing experts to express prior coded knowledge for automatic\ndata annotations that can be used to train arbitrary deep neural networks for\nprediction. The ICLR 2021 Workshop on Weak Supervision aims at advancing\nmethods that help modern machine-learning methods to generalize from knowledge\nprovided by experts, in interaction with observable (unlabeled) data. In total,\n15 papers were accepted. All the accepted contributions are listed in these\nProceedings.",
          "link": "http://arxiv.org/abs/2107.03690",
          "publishedOn": "2021-07-09T01:58:26.960Z",
          "wordCount": 534,
          "title": "Proceedings of the First Workshop on Weakly Supervised Learning (WeaSuL). (arXiv:2107.03690v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moldoveanu_M/0/1/0/all/0/1\">Matei Moldoveanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_A/0/1/0/all/0/1\">Abdellatif Zaidi</a>",
          "description": "It is widely perceived that leveraging the success of modern machine learning\ntechniques to mobile devices and wireless networks has the potential of\nenabling important new services. This, however, poses significant challenges,\nessentially due to that both data and processing power are highly distributed\nin a wireless network. In this paper, we develop a learning algorithm and an\narchitecture that make use of multiple data streams and processing units, not\nonly during the training phase but also during the inference phase. In\nparticular, the analysis reveals how inference propagates and fuses across a\nnetwork. We study the design criterion of our proposed method and its bandwidth\nrequirements. Also, we discuss implementation aspects using neural networks in\ntypical wireless radio access; and provide experiments that illustrate benefits\nover state-of-the-art techniques.",
          "link": "http://arxiv.org/abs/2107.03433",
          "publishedOn": "2021-07-09T01:58:26.946Z",
          "wordCount": 594,
          "title": "In-Network Learning: Distributed Training and Inference in Networks. (arXiv:2107.03433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03770",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mehrjou_A/0/1/0/all/0/1\">Arash Mehrjou</a>",
          "description": "We establish a connection between federated learning, a concept from machine\nlearning, and mean-field games, a concept from game theory and control theory.\nIn this analogy, the local federated learners are considered as the players and\nthe aggregation of the gradients in a central server is the mean-field effect.\nWe present federated learning as a differential game and discuss the properties\nof the equilibrium of this game. We hope this novel view to federated learning\nbrings together researchers from these two distinct areas to work on\nfundamental problems of large-scale distributed and privacy-preserving learning\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.03770",
          "publishedOn": "2021-07-09T01:58:26.939Z",
          "wordCount": 531,
          "title": "Federated Learning as a Mean-Field Game. (arXiv:2107.03770v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ravindranath_S/0/1/0/all/0/1\">Sai Srivatsa Ravindranath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shira Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jonathan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kominers_S/0/1/0/all/0/1\">Scott D. Kominers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1\">David C. Parkes</a>",
          "description": "We initiate the use of a multi-layer neural network to model two-sided\nmatching and to explore the design space between strategy-proofness and\nstability. It is well known that both properties cannot be achieved\nsimultaneously but the efficient frontier in this design space is not\nunderstood. We show empirically that it is possible to achieve a good\ncompromise between stability and strategy-proofness-substantially better than\nthat achievable through a convex combination of deferred acceptance (stable and\nstrategy-proof for only one side of the market) and randomized serial\ndictatorship (strategy-proof but not stable).",
          "link": "http://arxiv.org/abs/2107.03427",
          "publishedOn": "2021-07-09T01:58:26.924Z",
          "wordCount": 536,
          "title": "Deep Learning for Two-Sided Matching. (arXiv:2107.03427v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1\">Seyed Mojtaba Marvasti-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaghani_J/0/1/0/all/0/1\">Javad Khaghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanei_Yakhdan_H/0/1/0/all/0/1\">Hossein Ghanei-Yakhdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1\">Shohreh Kasaei</a>",
          "description": "A strong visual object tracker nowadays relies on its well-crafted modules,\nwhich typically consist of manually-designed network architectures to deliver\nhigh-quality tracking results. Not surprisingly, the manual design process\nbecomes a particularly challenging barrier, as it demands sufficient prior\nexperience, enormous effort, intuition and perhaps some good luck. Meanwhile,\nneural architecture search has gaining grounds in practical applications such\nas image segmentation, as a promising method in tackling the issue of automated\nsearch of feasible network structures. In this work, we propose a novel\ncell-level differentiable architecture search mechanism to automate the network\ndesign of the tracking module, aiming to adapt backbone features to the\nobjective of a tracking network during offline training. The proposed approach\nis simple, efficient, and with no need to stack a series of modules to\nconstruct a network. Our approach is easy to be incorporated into existing\ntrackers, which is empirically validated using different differentiable\narchitecture search-based methods and tracking objectives. Extensive\nexperimental evaluations demonstrate the superior performance of our approach\nover five commonly-used benchmarks. Meanwhile, our automated searching process\ntakes 41 (18) hours for the second (first) order DARTS method on the\nTrackingNet dataset.",
          "link": "http://arxiv.org/abs/2107.03463",
          "publishedOn": "2021-07-09T01:58:26.905Z",
          "wordCount": 652,
          "title": "CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search. (arXiv:2107.03463v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03651",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bar_David_D/0/1/0/all/0/1\">Daniel Bar-David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bar_David_L/0/1/0/all/0/1\">Laura Bar-David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shapira_Y/0/1/0/all/0/1\">Yinon Shapira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leibu_R/0/1/0/all/0/1\">Rina Leibu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dori_D/0/1/0/all/0/1\">Dalia Dori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schneor_R/0/1/0/all/0/1\">Ronit Schneor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischer_A/0/1/0/all/0/1\">Anath Fischer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soudry_S/0/1/0/all/0/1\">Shiri Soudry</a>",
          "description": "To explore the clinical validity of elastic deformation of optical coherence\ntomography (OCT) images for data augmentation in the development of\ndeep-learning model for detection of diabetic macular edema (DME).",
          "link": "http://arxiv.org/abs/2107.03651",
          "publishedOn": "2021-07-09T01:58:26.899Z",
          "wordCount": 508,
          "title": "Elastic deformation of optical coherence tomography images of diabetic macular edema for deep-learning models training: how far to go?. (arXiv:2107.03651v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+El_Awady_K/0/1/0/all/0/1\">Khalid El-Awady</a>",
          "description": "We demonstrate the use of Adaptive Stress Testing to detect and address\npotential vulnerabilities in a financial environment. We develop a simplified\nmodel for credit card fraud detection that utilizes a linear regression\nclassifier based on historical payment transaction data coupled with business\nrules. We then apply the reinforcement learning model known as Adaptive Stress\nTesting to train an agent, that can be thought of as a potential fraudster, to\nfind the most likely path to system failure -- successfully defrauding the\nsystem. We show the connection between this most likely failure path and the\nlimits of the classifier and discuss how the fraud detection system's business\nrules can be further augmented to mitigate these failure modes.",
          "link": "http://arxiv.org/abs/2107.03577",
          "publishedOn": "2021-07-09T01:58:26.891Z",
          "wordCount": 553,
          "title": "Adaptive Stress Testing for Adversarial Learning in a Financial Environment. (arXiv:2107.03577v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03387",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Cvetko_T/0/1/0/all/0/1\">Tim Cvetko</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robek_T/0/1/0/all/0/1\">Tinkara Robek</a>",
          "description": "In this paper, we propose a novel method and a practical approach to\npredicting early onsets of sleep syndromes, including restless leg syndrome,\ninsomnia, based on an algorithm that is comprised of two modules. A Fast\nFourier Transform is applied to 30 seconds long epochs of EEG recordings to\nprovide localized time-frequency information, and a deep convolutional LSTM\nneural network is trained for sleep stage classification. Automating sleep\nstages detection from EEG data offers great potential to tackling sleep\nirregularities on a daily basis. Thereby, a novel approach for sleep stage\nclassification is proposed which combines the best of signal processing and\nstatistics. In this study, we used the PhysioNet Sleep European Data Format\n(EDF) Database. The code evaluation showed impressive results, reaching an\naccuracy of 86.43, precision of 77.76, recall of 93,32, F1-score of 89.12 with\nthe final mean false error loss of 0.09.",
          "link": "http://arxiv.org/abs/2107.03387",
          "publishedOn": "2021-07-09T01:58:26.885Z",
          "wordCount": 591,
          "title": "Sleep syndromes onset detection based on automatic sleep staging algorithm. (arXiv:2107.03387v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1\">Martin Knoche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1\">Stefan H&#xf6;rmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>",
          "description": "Face recognition approaches often rely on equal image resolution for\nverification faces on two images. However, in practical applications, those\nimage resolutions are usually not in the same range due to different image\ncapture mechanisms or sources. In this work, we first analyze the impact of\nimage resolutions on the face verification performance with a state-of-the-art\nface recognition model. For images, synthetically reduced to $5\\, \\times 5\\,\n\\mathrm{px}$ resolution, the verification performance drops from $99.23\\%$\nincreasingly down to almost $55\\%$. Especially, for cross-resolution image\npairs (one high- and one low-resolution image), the verification accuracy\ndecreases even further. We investigate this behavior more in-depth by looking\nat the feature distances for every 2-image test pair. To tackle this problem,\nwe propose the following two methods: 1) Train a state-of-the-art\nface-recognition model straightforward with $50\\%$ low-resolution images\ndirectly within each batch. \\\\ 2) Train a siamese-network structure and adding\na cosine distance feature loss between high- and low-resolution features. Both\nmethods show an improvement for cross-resolution scenarios and can increase the\naccuracy at very low resolution to approximately $70\\%$. However, a\ndisadvantage is that a specific model needs to be trained for every\nresolution-pair ...",
          "link": "http://arxiv.org/abs/2107.03769",
          "publishedOn": "2021-07-09T01:58:26.861Z",
          "wordCount": 636,
          "title": "Image Resolution Susceptibility of Face Recognition Models. (arXiv:2107.03769v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvir Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1\">Janntatul Tajrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naira Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>",
          "description": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.",
          "link": "http://arxiv.org/abs/2107.03844",
          "publishedOn": "2021-07-09T01:58:26.838Z",
          "wordCount": 691,
          "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03645",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Heindel_L/0/1/0/all/0/1\">Leonhard Heindel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hantschke_P/0/1/0/all/0/1\">Peter Hantschke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kastner_M/0/1/0/all/0/1\">Markus K&#xe4;stner</a>",
          "description": "Modern Internet of Things solutions are used in a variety of different areas,\nranging from connected vehicles and healthcare to industrial applications. They\nrely on a large amount of interconnected sensors, which can lead to both\ntechnical and economical challenges. Virtual sensing techniques aim to reduce\nthe number of physical sensors in a system by using data from available\nmeasurements to estimate additional unknown quantities of interest. Successful\nmodel-based solutions include Kalman filters or the combination of finite\nelement models and modal analysis, while many data-driven methods rely on\nmachine learning algorithms. The presented hybrid virtual sensing approach\ncombines Long Short-Term Memory networks with frequency response function\nmodels in order to estimate the behavior of non-linear dynamic systems with\nmultiple input and output channels. Network training and prediction make use of\nshort signal subsequences, which are later recombined by applying a windowing\ntechnique. The frequency response function model acts as a baseline estimate\nwhich perfectly captures linear dynamic systems and is augmented by the\nnon-linear Long Short-Term Memory network following two different hybrid\nmodeling strategies. The approach is tested using a non-linear experimental\ndataset, which results from measurements of a three-component servo-hydraulic\nfatigue test bench. A variety of metrics in time and frequency domains, as well\nas fatigue strength under variable amplitudes are used to evaluate the\napproximation quality of the proposed method. In addition to virtual sensing,\nthe algorithm is also applied to a forward prediction task. Synthetic data are\nused in a separate study to estimate the prediction quality on datasets of\ndifferent size.",
          "link": "http://arxiv.org/abs/2107.03645",
          "publishedOn": "2021-07-09T01:58:26.831Z",
          "wordCount": 708,
          "title": "A hybrid virtual sensing approach for approximating non-linear dynamic system behavior using LSTM networks. (arXiv:2107.03645v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_N/0/1/0/all/0/1\">Nihal Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Soumya Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>",
          "description": "We study a version of the contextual bandit problem where an agent is given\nsoft control of a node in a graph-structured environment through a set of\nstochastic expert policies. The agent interacts with the environment over\nepisodes, with each episode having different context distributions; this\nresults in the `best expert' changing across episodes. Our goal is to develop\nan agent that tracks the best expert over episodes. We introduce the Empirical\nDivergence-based UCB (ED-UCB) algorithm in this setting where the agent does\nnot have any knowledge of the expert policies or changes in context\ndistributions. With mild assumptions, we show that bootstrapping from\n$\\tilde{O}(N\\log(NT^2\\sqrt{E}))$ samples results in a regret of\n$\\tilde{O}(E(N+1) + \\frac{N\\sqrt{E}}{T^2})$. If the expert policies are known\nto the agent a priori, then we can improve the regret to $\\tilde{O}(EN)$\nwithout requiring any bootstrapping. Our analysis also tightens pre-existing\nlogarithmic regret bounds to a problem-dependent constant in the non-episodic\nsetting when expert policies are known. We finally empirically validate our\nfindings through simulations.",
          "link": "http://arxiv.org/abs/2107.03263",
          "publishedOn": "2021-07-08T01:58:00.342Z",
          "wordCount": 590,
          "title": "Episodic Bandits with Stochastic Experts. (arXiv:2107.03263v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davtyan_A/0/1/0/all/0/1\">Aram Davtyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sameni_S/0/1/0/all/0/1\">Sepehr Sameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerkezi_L/0/1/0/all/0/1\">Llukman Cerkezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meishvilli_G/0/1/0/all/0/1\">Givi Meishvilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielski_A/0/1/0/all/0/1\">Adam Bielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>",
          "description": "Optimization is often cast as a deterministic problem, where the solution is\nfound through some iterative procedure such as gradient descent. However, when\ntraining neural networks the loss function changes over (iteration) time due to\nthe randomized selection of a subset of the samples. This randomization turns\nthe optimization problem into a stochastic one. We propose to consider the loss\nas a noisy observation with respect to some reference optimum. This\ninterpretation of the loss allows us to adopt Kalman filtering as an optimizer,\nas its recursive formulation is designed to estimate unknown parameters from\nnoisy measurements. Moreover, we show that the Kalman Filter dynamical model\nfor the evolution of the unknown parameters can be used to capture the gradient\ndynamics of advanced methods such as Momentum and Adam. We call this stochastic\noptimization method KaFiStO. KaFiStO is an easy to implement, scalable, and\nefficient method to train neural networks. We show that it also yields\nparameter estimates that are on par with or better than existing optimization\nalgorithms across several neural network architectures and machine learning\ntasks, such as computer vision and language modeling.",
          "link": "http://arxiv.org/abs/2107.03331",
          "publishedOn": "2021-07-08T01:58:00.335Z",
          "wordCount": 637,
          "title": "KaFiStO: A Kalman Filtering Framework for Stochastic Optimization. (arXiv:2107.03331v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maschler_B/0/1/0/all/0/1\">Benjamin Maschler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tatiyosyan_S/0/1/0/all/0/1\">Sophia Tatiyosyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyrich_M/0/1/0/all/0/1\">Michael Weyrich</a>",
          "description": "In recent years, the use of lithium-ion batteries has greatly expanded into\nproducts from many industrial sectors, e.g. cars, power tools or medical\ndevices. An early prediction and robust understanding of battery faults could\ntherefore greatly increase product quality in those fields. While current\napproaches for data-driven fault prediction provide good results on the exact\nprocesses they were trained on, they often lack the ability to flexibly adapt\nto changes, e.g. in operational or environmental parameters. Continual learning\npromises such flexibility, allowing for an automatic adaption of previously\nlearnt knowledge to new tasks. Therefore, this article discusses different\ncontinual learning approaches from the group of regularization strategies,\nwhich are implemented, evaluated and compared based on a real battery wear\ndataset. Online elastic weight consolidation delivers the best results, but, as\nwith all examined approaches, its performance appears to be strongly dependent\non task characteristics and task sequence.",
          "link": "http://arxiv.org/abs/2107.03336",
          "publishedOn": "2021-07-08T01:58:00.328Z",
          "wordCount": 600,
          "title": "Regularization-based Continual Learning for Fault Prediction in Lithium-Ion Batteries. (arXiv:2107.03336v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.02790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Evan Zheran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Aditi Raghunathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "The goal of meta-reinforcement learning (meta-RL) is to build agents that can\nquickly learn new tasks by leveraging prior experience on related tasks.\nLearning a new task often requires both exploring to gather task-relevant\ninformation and exploiting this information to solve the task. In principle,\noptimal exploration and exploitation can be learned end-to-end by simply\nmaximizing task performance. However, such meta-RL approaches struggle with\nlocal optima due to a chicken-and-egg problem: learning to explore requires\ngood exploitation to gauge the exploration's utility, but learning to exploit\nrequires information gathered via exploration. Optimizing separate objectives\nfor exploration and exploitation can avoid this problem, but prior meta-RL\nexploration objectives yield suboptimal policies that gather information\nirrelevant to the task. We alleviate both concerns by constructing an\nexploitation objective that automatically identifies task-relevant information\nand an exploration objective to recover only this information. This avoids\nlocal optima in end-to-end training, without sacrificing optimal exploration.\nEmpirically, DREAM substantially outperforms existing approaches on complex\nmeta-RL problems, such as sparse-reward 3D visual navigation. Videos of DREAM:\nhttps://ezliu.github.io/dream/",
          "link": "http://arxiv.org/abs/2008.02790",
          "publishedOn": "2021-07-08T01:58:00.302Z",
          "wordCount": 662,
          "title": "Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices. (arXiv:2008.02790v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Holden Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pabbaraju_C/0/1/0/all/0/1\">Chirag Pabbaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevekari_A/0/1/0/all/0/1\">Anish Sevekari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1\">Andrej Risteski</a>",
          "description": "Normalizing flows are a widely used class of latent-variable generative\nmodels with a tractable likelihood. Affine-coupling (Dinh et al, 2014-16)\nmodels are a particularly common type of normalizing flows, for which the\nJacobian of the latent-to-observable-variable transformation is triangular,\nallowing the likelihood to be computed in linear time. Despite the widespread\nusage of affine couplings, the special structure of the architecture makes\nunderstanding their representational power challenging. The question of\nuniversal approximation was only recently resolved by three parallel papers\n(Huang et al.,2020;Zhang et al.,2020;Koehler et al.,2020) -- who showed\nreasonably regular distributions can be approximated arbitrarily well using\naffine couplings -- albeit with networks with a nearly-singular Jacobian. As\nill-conditioned Jacobians are an obstacle for likelihood-based training, the\nfundamental question remains: which distributions can be approximated using\nwell-conditioned affine coupling flows?\n\nIn this paper, we show that any log-concave distribution can be approximated\nusing well-conditioned affine-coupling flows. In terms of proof techniques, we\nuncover and leverage deep connections between affine coupling architectures,\nunderdamped Langevin dynamics (a stochastic differential equation often used to\nsample from Gibbs measures) and H\\'enon maps (a structured dynamical system\nthat appears in the study of symplectic diffeomorphisms). Our results also\ninform the practice of training affine couplings: we approximate a padded\nversion of the input distribution with iid Gaussians -- a strategy which\nKoehler et al.(2020) empirically observed to result in better-conditioned\nflows, but had hitherto no theoretical grounding. Our proof can thus be seen as\nproviding theoretical evidence for the benefits of Gaussian padding when\ntraining normalizing flows.",
          "link": "http://arxiv.org/abs/2107.02951",
          "publishedOn": "2021-07-08T01:58:00.183Z",
          "wordCount": 697,
          "title": "Universal Approximation for Log-concave Distributions using Well-conditioned Normalizing Flows. (arXiv:2107.02951v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blum_A/0/1/0/all/0/1\">Avrim Blum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1\">Steve Hanneke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jian Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Han Shao</a>",
          "description": "We study the problem of robust learning under clean-label data-poisoning\nattacks, where the attacker injects (an arbitrary set of) correctly-labeled\nexamples to the training set to fool the algorithm into making mistakes on\nspecific test instances at test time. The learning goal is to minimize the\nattackable rate (the probability mass of attackable test instances), which is\nmore difficult than optimal PAC learning. As we show, any robust algorithm with\ndiminishing attackable rate can achieve the optimal dependence on $\\epsilon$ in\nits PAC sample complexity, i.e., $O(1/\\epsilon)$. On the other hand, the\nattackable rate might be large even for some optimal PAC learners, e.g., SVM\nfor linear classifiers. Furthermore, we show that the class of linear\nhypotheses is not robustly learnable when the data distribution has zero margin\nand is robustly learnable in the case of positive margin but requires sample\ncomplexity exponential in the dimension. For a general hypothesis class with\nbounded VC dimension, if the attacker is limited to add at most $t>0$ poison\nexamples, the optimal robust learning sample complexity grows almost linearly\nwith $t$.",
          "link": "http://arxiv.org/abs/2103.00671",
          "publishedOn": "2021-07-08T01:58:00.012Z",
          "wordCount": 649,
          "title": "Robust learning under clean-label attack. (arXiv:2103.00671v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeghidour_N/0/1/0/all/0/1\">Neil Zeghidour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luebs_A/0/1/0/all/0/1\">Alejandro Luebs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omran_A/0/1/0/all/0/1\">Ahmed Omran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skoglund_J/0/1/0/all/0/1\">Jan Skoglund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_M/0/1/0/all/0/1\">Marco Tagliasacchi</a>",
          "description": "We present SoundStream, a novel neural audio codec that can efficiently\ncompress speech, music and general audio at bitrates normally targeted by\nspeech-tailored codecs. SoundStream relies on a model architecture composed by\na fully convolutional encoder/decoder network and a residual vector quantizer,\nwhich are trained jointly end-to-end. Training leverages recent advances in\ntext-to-speech and speech enhancement, which combine adversarial and\nreconstruction losses to allow the generation of high-quality audio content\nfrom quantized embeddings. By training with structured dropout applied to\nquantizer layers, a single model can operate across variable bitrates from\n3kbps to 18kbps, with a negligible quality loss when compared with models\ntrained at fixed bitrates. In addition, the model is amenable to a low latency\nimplementation, which supports streamable inference and runs in real time on a\nsmartphone CPU. In subjective evaluations using audio at 24kHz sampling rate,\nSoundStream at 3kbps outperforms Opus at 12kbps and approaches EVS at 9.6kbps.\nMoreover, we are able to perform joint compression and enhancement either at\nthe encoder or at the decoder side with no additional latency, which we\ndemonstrate through background noise suppression for speech.",
          "link": "http://arxiv.org/abs/2107.03312",
          "publishedOn": "2021-07-08T01:58:00.005Z",
          "wordCount": 621,
          "title": "SoundStream: An End-to-End Neural Audio Codec. (arXiv:2107.03312v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fuentes_B/0/1/0/all/0/1\">Benoit Fuentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1\">Ga&#xeb;l Richard</a>",
          "description": "We present a new probabilistic model to address semi-nonnegative matrix\nfactorization (SNMF), called Skellam-SNMF. It is a hierarchical generative\nmodel consisting of prior components, Skellam-distributed hidden variables and\nobserved data. Two inference algorithms are derived: Expectation-Maximization\n(EM) algorithm for maximum \\emph{a posteriori} estimation and Variational Bayes\nEM (VBEM) for full Bayesian inference, including the estimation of parameters\nprior distribution. From this Skellam-based model, we also introduce a new\ndivergence $\\mathcal{D}$ between a real-valued target data $x$ and two\nnonnegative parameters $\\lambda_{0}$ and $\\lambda_{1}$ such that\n$\\mathcal{D}\\left(x\\mid\\lambda_{0},\\lambda_{1}\\right)=0\\Leftrightarrow\nx=\\lambda_{0}-\\lambda_{1}$, which is a generalization of the Kullback-Leibler\n(KL) divergence. Finally, we conduct experimental studies on those new\nalgorithms in order to understand their behavior and prove that they can\noutperform the classic SNMF approach on real data in a task of automatic\nclustering.",
          "link": "http://arxiv.org/abs/2107.03317",
          "publishedOn": "2021-07-08T01:57:59.999Z",
          "wordCount": 569,
          "title": "Probabilistic semi-nonnegative matrix factorization: a Skellam-based framework. (arXiv:2107.03317v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloem_Reddy_B/0/1/0/all/0/1\">Benjamin Bloem-Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1\">Karen Ullrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1\">Chris J. Maddison</a>",
          "description": "Most data is automatically collected and only ever \"seen\" by algorithms. Yet,\ndata compressors preserve perceptual fidelity rather than just the information\nneeded by algorithms performing downstream tasks. In this paper, we\ncharacterize the bit-rate required to ensure high performance on all predictive\ntasks that are invariant under a set of transformations, such as data\naugmentations. Based on our theory, we design unsupervised objectives for\ntraining neural compressors. Using these objectives, we train a generic image\ncompressor that achieves substantial rate savings (more than $1000\\times$ on\nImageNet) compared to JPEG on 8 datasets, without decreasing downstream\nclassification performance.",
          "link": "http://arxiv.org/abs/2106.10800",
          "publishedOn": "2021-07-08T01:57:59.980Z",
          "wordCount": 563,
          "title": "Lossy Compression for Lossless Prediction. (arXiv:2106.10800v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lienen_J/0/1/0/all/0/1\">Julian Lienen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nommensen_N/0/1/0/all/0/1\">Nils Nommensen</a>",
          "description": "In many real-world applications, the relative depth of objects in an image is\ncrucial for scene understanding. Recent approaches mainly tackle the problem of\ndepth prediction in monocular images by treating the problem as a regression\ntask. Yet, being interested in an order relation in the first place, ranking\nmethods suggest themselves as a natural alternative to regression, and indeed,\nranking approaches leveraging pairwise comparisons as training information\n(\"object A is closer to the camera than B\") have shown promising performance on\nthis problem. In this paper, we elaborate on the use of so-called listwise\nranking as a generalization of the pairwise approach. Our method is based on\nthe Plackett-Luce (PL) model, a probability distribution on rankings, which we\ncombine with a state-of-the-art neural network architecture and a simple\nsampling strategy to reduce training complexity. Moreover, taking advantage of\nthe representation of PL as a random utility model, the proposed predictor\noffers a natural way to recover (shift-invariant) metric depth information from\nranking-only data provided at training time. An empirical evaluation on several\nbenchmark datasets in a \"zero-shot\" setting demonstrates the effectiveness of\nour approach compared to existing ranking and regression methods.",
          "link": "http://arxiv.org/abs/2010.13118",
          "publishedOn": "2021-07-08T01:57:59.974Z",
          "wordCount": 713,
          "title": "Monocular Depth Estimation via Listwise Ranking using the Plackett-Luce Model. (arXiv:2010.13118v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sajiv Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>",
          "description": "Modeling of non-rigid object launching and manipulation is complex\nconsidering the wide range of dynamics affecting trajectory, many of which may\nbe unknown. Using physics models can be inaccurate because they cannot account\nfor unknown factors and the effects of the deformation of the object as it is\nlaunched; moreover, deriving force coefficients for these models is not\npossible without extensive experimental testing. Recently, advancements in\ndata-powered artificial intelligence methods have allowed learnable models and\nsystems to emerge. It is desirable to train a model for launch prediction on a\nrobot, as deep neural networks can account for immeasurable dynamics. However,\nthe inability to collect large amounts of experimental data decreases\nperformance of deep neural networks. Through estimating force coefficients, the\naccepted physics models can be leveraged to produce adequate supplemental data\nto artificially increase the size of the training set, yielding improved neural\nnetworks. In this paper, we introduce a new framework for algorithmic\nestimation of force coefficients for non-rigid object launching, which can be\ngeneralized to other domains, in order to generate large datasets. We implement\na novel training algorithm and objective for our deep neural network to\naccurately model launch trajectory of non-rigid objects and predict whether\nthey will hit a series of targets. Our experimental results demonstrate the\neffectiveness of using simulated data from force coefficient estimation and\nshows the importance of simulated data for training an effective neural\nnetwork.",
          "link": "http://arxiv.org/abs/2105.12833",
          "publishedOn": "2021-07-08T01:57:59.967Z",
          "wordCount": 721,
          "title": "Simulated Data Generation Through Algorithmic Force Coefficient Estimation for AI-Based Robotic Projectile Launch Modeling. (arXiv:2105.12833v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sajadmanesh_S/0/1/0/all/0/1\">Sina Sajadmanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatica_Perez_D/0/1/0/all/0/1\">Daniel Gatica-Perez</a>",
          "description": "Graph Neural Networks (GNNs) have demonstrated superior performance in\nlearning node representations for various graph inference tasks. However,\nlearning over graph data can raise privacy concerns when nodes represent people\nor human-related variables that involve sensitive or personal information.\nWhile numerous techniques have been proposed for privacy-preserving deep\nlearning over non-relational data, there is less work addressing the privacy\nissues pertained to applying deep learning algorithms on graphs. In this paper,\nwe study the problem of node data privacy, where graph nodes have potentially\nsensitive data that is kept private, but they could be beneficial for a central\nserver for training a GNN over the graph. To address this problem, we develop a\nprivacy-preserving, architecture-agnostic GNN learning algorithm with formal\nprivacy guarantees based on Local Differential Privacy (LDP). Specifically, we\npropose an LDP encoder and an unbiased rectifier, by which the server can\ncommunicate with the graph nodes to privately collect their data and\napproximate the GNN's first layer. To further reduce the effect of the injected\nnoise, we propose to prepend a simple graph convolution layer, called KProp,\nwhich is based on the multi-hop aggregation of the nodes' features acting as a\ndenoising mechanism. Finally, we propose a robust training framework, in which\nwe benefit from KProp's denoising capability to increase the accuracy of\ninference in the presence of noisy labels. Extensive experiments conducted over\nreal-world datasets demonstrate that our method can maintain a satisfying level\nof accuracy with low privacy loss.",
          "link": "http://arxiv.org/abs/2006.05535",
          "publishedOn": "2021-07-08T01:57:59.961Z",
          "wordCount": 769,
          "title": "Locally Private Graph Neural Networks. (arXiv:2006.05535v9 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongde Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel E. O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingming Liu</a>",
          "description": "Accurately forecasting transportation demand is crucial for efficient urban\ntraffic guidance, control and management. One solution to enhance the level of\nprediction accuracy is to leverage graph convolutional networks (GCN), a neural\nnetwork based modelling approach with the ability to process data contained in\ngraph based structures. As a powerful extension of GCN, a spatial-temporal\ngraph convolutional network (ST-GCN) aims to capture the relationship of data\ncontained in the graphical nodes across both spatial and temporal dimensions,\nwhich presents a novel deep learning paradigm for the analysis of complex\ntime-series data that also involves spatial information as present in\ntransportation use cases. In this paper, we present an Attention-based ST-GCN\n(AST-GCN) for predicting the number of available bikes in bike-sharing systems\nin cities, where the attention-based mechanism is introduced to further improve\nthe performance of an ST-GCN. Furthermore, we also discuss the impacts of\ndifferent modelling methods of adjacency matrices on the proposed architecture.\nOur experimental results are presented using two real-world datasets,\nDublinbikes and NYC-Citi Bike, to illustrate the efficacy of our proposed model\nwhich outperforms the majority of existing approaches.",
          "link": "http://arxiv.org/abs/2104.10644",
          "publishedOn": "2021-07-08T01:57:59.952Z",
          "wordCount": 675,
          "title": "A Comparative Study of Using Spatial-Temporal Graph Convolutional Networks for Predicting Availability in Bike Sharing Schemes. (arXiv:2104.10644v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.06674",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Na_S/0/1/0/all/0/1\">Sen Na</a>, <a href=\"http://arxiv.org/find/math/1/au:+Shin_S/0/1/0/all/0/1\">Sungho Shin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Anitescu_M/0/1/0/all/0/1\">Mihai Anitescu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zavala_V/0/1/0/all/0/1\">Victor M. Zavala</a>",
          "description": "We study the convergence properties of an overlapping Schwarz\ndecomposition~algorithm for solving nonlinear optimal control problems (OCPs).\nThe approach decomposes the time domain into a set of overlapping subdomains,\nand solves subproblems defined over such subdomains in parallel. Convergence is\nattained by updating primal-dual information at the boundaries of the\noverlapping regions. We show that the algorithm exhibits local linear\nconvergence and that the convergence rate improves exponentially with the\noverlap size. Our convergence results rely on a sensitivity result for OCPs\nthat we call \"exponential decay of sensitivity\" (EDS). Intuitively, EDS states\nthat the impact of parametric perturbations at the boundaries of the domain\n(initial and final time) decays exponentially as one moves into the domain. We\nshow that EDS holds for nonlinear OCPs under a uniform second-order sufficient\ncondition, a controllability condition, and a uniform boundedness condition. We\nconduct numerical experiments using a quadrotor motion planning problem and a\nPDE control problem; and show that the approach is significantly more efficient\nthan ADMM and as efficient as the centralized solver Ipopt.",
          "link": "http://arxiv.org/abs/2005.06674",
          "publishedOn": "2021-07-08T01:57:59.934Z",
          "wordCount": 658,
          "title": "On the Convergence of Overlapping Schwarz Decomposition for Nonlinear Optimal Control. (arXiv:2005.06674v4 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1\">Amin Nikanjam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morovati_M/0/1/0/all/0/1\">Mohammad Mehdi Morovati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braiek_H/0/1/0/all/0/1\">Houssem Ben Braiek</a>",
          "description": "A growing demand is witnessed in both industry and academia for employing\nDeep Learning (DL) in various domains to solve real-world problems. Deep\nReinforcement Learning (DRL) is the application of DL in the domain of\nReinforcement Learning (RL). Like any software systems, DRL applications can\nfail because of faults in their programs. In this paper, we present the first\nattempt to categorize faults occurring in DRL programs. We manually analyzed\n761 artifacts of DRL programs (from Stack Overflow posts and GitHub issues)\ndeveloped using well-known DRL frameworks (OpenAI Gym, Dopamine, Keras-rl,\nTensorforce) and identified faults reported by developers/users. We labeled and\ntaxonomized the identified faults through several rounds of discussions. The\nresulting taxonomy is validated using an online survey with 19\ndevelopers/researchers. To allow for the automatic detection of faults in DRL\nprograms, we have defined a meta-model of DRL programs and developed DRLinter,\na model-based fault detection approach that leverages static analysis and graph\ntransformations. The execution flow of DRLinter consists in parsing a DRL\nprogram to generate a model conforming to our meta-model and applying detection\nrules on the model to identify faults occurrences. The effectiveness of\nDRLinter is evaluated using 15 synthetic DRLprograms in which we injected\nfaults observed in the analyzed artifacts of the taxonomy. The results show\nthat DRLinter can successfully detect faults in all synthetic faulty programs.",
          "link": "http://arxiv.org/abs/2101.00135",
          "publishedOn": "2021-07-08T01:57:59.927Z",
          "wordCount": 695,
          "title": "Faults in Deep Reinforcement Learning Programs: A Taxonomy and A Detection Approach. (arXiv:2101.00135v2 [cs.SE] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02990",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kamulete_V/0/1/0/all/0/1\">Vathy M. Kamulete</a>",
          "description": "Statistical tests for dataset shift are susceptible to false alarms: they are\nsensitive to minor differences where there is in fact adequate sample coverage\nand predictive performance. We propose instead a robust framework for tests of\ndataset shift based on outlier scores, D-SOS for short. D-SOS detects adverse\nshifts and can identify false alarms caused by benign ones. It posits that a\nnew (test) sample is not substantively worse than an old (training) sample, and\nnot that the two are equal. The key idea is to reduce observations to outlier\nscores and compare contamination rates. Beyond comparing distributions, users\ncan define what worse means in terms of predictive performance and other\nrelevant notions. We show how versatile and practical D-SOS is for a wide range\nof real and simulated datasets. Unlike tests of equal distribution and of\ngoodness-of-fit, the D-SOS tests are uniquely tailored to serve as robust\nperformance metrics to monitor model drift and dataset shift.",
          "link": "http://arxiv.org/abs/2107.02990",
          "publishedOn": "2021-07-08T01:57:59.920Z",
          "wordCount": 588,
          "title": "Test for non-negligible adverse shifts. (arXiv:2107.02990v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burkhalter_L/0/1/0/all/0/1\">Lukas Burkhalter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nijeholt_H/0/1/0/all/0/1\">Hidde Lycklama &#xe0; Nijeholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viand_A/0/1/0/all/0/1\">Alexander Viand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuchler_N/0/1/0/all/0/1\">Nicolas K&#xfc;chler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hithnawi_A/0/1/0/all/0/1\">Anwar Hithnawi</a>",
          "description": "Federated Learning is an emerging decentralized machine learning paradigm\nthat allows a large number of clients to train a joint model without the need\nto share their private data. Participants instead only share ephemeral updates\nnecessary to train the model. To ensure the confidentiality of the client\nupdates, Federated Learning systems employ secure aggregation; clients encrypt\ntheir gradient updates, and only the aggregated model is revealed to the\nserver. Achieving this level of data protection, however, presents new\nchallenges to the robustness of Federated Learning, i.e., the ability to\ntolerate failures and attacks. Unfortunately, in this setting, a malicious\nclient can now easily exert influence on the model behavior without being\ndetected. As Federated Learning is being deployed in practice in a range of\nsensitive applications, its robustness is growing in importance. In this paper,\nwe take a step towards understanding and improving the robustness of secure\nFederated Learning. We start this paper with a systematic study that evaluates\nand analyzes existing attack vectors and discusses potential defenses and\nassesses their effectiveness. We then present RoFL, a secure Federated Learning\nsystem that improves robustness against malicious clients through input checks\non the encrypted model updates. RoFL extends Federated Learning's secure\naggregation protocol to allow expressing a variety of properties and\nconstraints on model updates using zero-knowledge proofs. To enable RoFL to\nscale to typical Federated Learning settings, we introduce several ML and\ncryptographic optimizations specific to Federated Learning. We implement and\nevaluate a prototype of RoFL and show that realistic ML models can be trained\nin a reasonable time while improving robustness.",
          "link": "http://arxiv.org/abs/2107.03311",
          "publishedOn": "2021-07-08T01:57:59.914Z",
          "wordCount": 707,
          "title": "RoFL: Attestable Robustness for Secure Federated Learning. (arXiv:2107.03311v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00685",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Marvin_D/0/1/0/all/0/1\">Dario Marvin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nespoli_L/0/1/0/all/0/1\">Lorenzo Nespoli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Strepparava_D/0/1/0/all/0/1\">Davide Strepparava</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Medici_V/0/1/0/all/0/1\">Vasco Medici</a>",
          "description": "The ability to forecast the concentration of air pollutants in an urban\nregion is crucial for decision-makers wishing to reduce the impact of pollution\non public health through active measures (e.g. temporary traffic closures). In\nthis study, we present a machine learning approach applied to the forecast of\nthe day-ahead maximum value of the ozone concentration for several geographical\nlocations in southern Switzerland. Due to the low density of measurement\nstations and to the complex orography of the use case terrain, we adopted\nfeature selection methods instead of explicitly restricting relevant features\nto a neighbourhood of the prediction sites, as common in spatio-temporal\nforecasting methods. We then used Shapley values to assess the explainability\nof the learned models in terms of feature importance and feature interactions\nin relation to ozone predictions; our analysis suggests that the trained models\neffectively learned explanatory cross-dependencies among atmospheric variables.\nFinally, we show how weighting observations helps in increasing the accuracy of\nthe forecasts for specific ranges of ozone's daily peak values.",
          "link": "http://arxiv.org/abs/2012.00685",
          "publishedOn": "2021-07-08T01:57:59.907Z",
          "wordCount": 644,
          "title": "A data-driven approach to the forecasting of ground-level ozone concentration. (arXiv:2012.00685v4 [physics.ao-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03280",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+LeRoy_B/0/1/0/all/0/1\">Benjamin LeRoy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1\">David Zhao</a>",
          "description": "Quantifying uncertainty in model predictions is a common goal for\npractitioners seeking more than just point predictions. One tool for\nuncertainty quantification that requires minimal assumptions is conformal\ninference, which can help create probabilistically valid prediction regions for\nblack box models. Classical conformal prediction only provides marginal\nvalidity, whereas in many situations locally valid prediction regions are\ndesirable. Deciding how best to partition the feature space X when applying\nlocalized conformal prediction is still an open question. We present MD-split+,\na practical local conformal approach that creates X partitions based on\nlocalized model performance of conditional density estimation models. Our\nmethod handles complex real-world data settings where such models may be\nmisspecified, and scales to high-dimensional inputs. We discuss how our local\npartitions philosophically align with expected behavior from an unattainable\nconditional conformal inference approach. We also empirically compare our\nmethod against other local conformal approaches.",
          "link": "http://arxiv.org/abs/2107.03280",
          "publishedOn": "2021-07-08T01:57:59.887Z",
          "wordCount": 585,
          "title": "MD-split+: Practical Local Conformal Inference in High Dimensions. (arXiv:2107.03280v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03361",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Venkatesh_T/0/1/0/all/0/1\">T.S.Sachin Venkatesh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Srivastava_R/0/1/0/all/0/1\">Rajat Srivastava</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bhatt_P/0/1/0/all/0/1\">Pratyush Bhatt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tyagi_P/0/1/0/all/0/1\">Prince Tyagi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Singh_R/0/1/0/all/0/1\">Raj Kumar Singh</a>",
          "description": "Super-resolution is an innovative technique that upscales the resolution of\nan image or a video and thus enables us to reconstruct high-fidelity images\nfrom low-resolution data. This study performs super-resolution analysis on\nturbulent flow fields spatially and temporally using various state-of-the-art\nmachine learning techniques like ESPCN, ESRGAN and TecoGAN to reconstruct\nhigh-resolution flow fields from low-resolution flow field data, especially\nkeeping in mind the need for low resource consumption and rapid results\nproduction/verification. The dataset used for this study is extracted from the\n'isotropic 1024 coarse' dataset which is a part of Johns Hopkins Turbulence\nDatabases (JHTDB). We have utilized pre-trained models and fine tuned them to\nour needs, so as to minimize the computational resources and the time required\nfor the implementation of the super-resolution models. The advantages presented\nby this method far exceed the expectations and the outcomes of regular single\nstructure models. The results obtained through these models are then compared\nusing MSE, PSNR, SAM, VIF and SCC metrics in order to evaluate the upscaled\nresults, find the balance between computational power and output quality, and\nthen identify the most accurate and efficient model for spatial and temporal\nsuper-resolution of turbulent flow fields.",
          "link": "http://arxiv.org/abs/2107.03361",
          "publishedOn": "2021-07-08T01:57:59.880Z",
          "wordCount": 660,
          "title": "A comparative study of various Deep Learning techniques for spatio-temporal Super-Resolution reconstruction of Forced Isotropic Turbulent flows. (arXiv:2107.03361v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhaoyang_Z/0/1/0/all/0/1\">Zhang Zhaoyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenqi_S/0/1/0/all/0/1\">Shao Wenqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jinwei_G/0/1/0/all/0/1\">Gu Jinwei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiaogang_W/0/1/0/all/0/1\">Wang Xiaogang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_L/0/1/0/all/0/1\">Luo Ping</a>",
          "description": "Model quantization is challenging due to many tedious hyper-parameters such\nas precision (bitwidth), dynamic range (minimum and maximum discrete values)\nand stepsize (interval between discrete values). Unlike prior arts that\ncarefully tune these values, we present a fully differentiable approach to\nlearn all of them, named Differentiable Dynamic Quantization (DDQ), which has\nseveral benefits. (1) DDQ is able to quantize challenging lightweight\narchitectures like MobileNets, where different layers prefer different\nquantization parameters. (2) DDQ is hardware-friendly and can be easily\nimplemented using low-precision matrix-vector multiplication, making it capable\nin many hardware such as ARM. (3) Extensive experiments show that DDQ\noutperforms prior arts on many networks and benchmarks, especially when models\nare already efficient and compact. e.g., DDQ is the first approach that\nachieves lossless 4-bit quantization for MobileNetV2 on ImageNet.",
          "link": "http://arxiv.org/abs/2106.02295",
          "publishedOn": "2021-07-08T01:57:59.874Z",
          "wordCount": 598,
          "title": "Differentiable Dynamic Quantization with Mixed Precision and Adaptive Resolution. (arXiv:2106.02295v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.04388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1\">Grgur Kova&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laversanne_Finot_A/0/1/0/all/0/1\">Adrien Laversanne-Finot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Designing agents, capable of learning autonomously a wide range of skills is\ncritical in order to increase the scope of reinforcement learning. It will both\nincrease the diversity of learned skills and reduce the burden of manually\ndesigning reward functions for each skill. Self-supervised agents, setting\ntheir own goals, and trying to maximize the diversity of those goals have shown\ngreat promise towards this end. However, a currently known limitation of agents\ntrying to maximize the diversity of sampled goals is that they tend to get\nattracted to noise or more generally to parts of the environments that cannot\nbe controlled (distractors). When agents have access to predefined goal\nfeatures or expert knowledge, absolute Learning Progress (ALP) provides a way\nto distinguish between regions that can be controlled and those that cannot.\nHowever, those methods often fall short when the agents are only provided with\nraw sensory inputs such as images. In this work we extend those concepts to\nunsupervised image-based goal exploration. We propose a framework that allows\nagents to autonomously identify and ignore noisy distracting regions while\nsearching for novelty in the learnable regions to both improve overall\nperformance and avoid catastrophic forgetting. Our framework can be combined\nwith any state-of-the-art novelty seeking goal exploration approaches. We\nconstruct a rich 3D image based environment with distractors. Experiments on\nthis environment show that agents using our framework successfully identify\ninteresting regions of the environment, resulting in drastically improved\nperformances. The source code is available at\nhttps://sites.google.com/view/grimgep.",
          "link": "http://arxiv.org/abs/2008.04388",
          "publishedOn": "2021-07-08T01:57:59.867Z",
          "wordCount": 728,
          "title": "GRIMGEP: Learning Progress for Robust Goal Sampling in Visual Deep Reinforcement Learning. (arXiv:2008.04388v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03337",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Harbrecht_H/0/1/0/all/0/1\">Helmut Harbrecht</a>, <a href=\"http://arxiv.org/find/math/1/au:+Multerer_M/0/1/0/all/0/1\">Michael Multerer</a>",
          "description": "In this article, we introduce the novel concept of samplets by transferring\nthe construction of Tausch-White wavelets to the realm of data. This way we\nobtain a multilevel representation of discrete data which directly enables data\ncompression, detection of singularities and adaptivity. Applying samplets to\nrepresent kernel matrices, as they arise in kernel based learning or Gaussian\nprocess regression, we end up with quasi-sparse matrices. By thresholding small\nentries, these matrices are compressible to O(N log N) relevant entries, where\nN is the number of data points. This feature allows for the use of fill-in\nreducing reorderings to obtain a sparse factorization of the compressed\nmatrices. Besides the comprehensive introduction to samplets and their\nproperties, we present extensive numerical studies to benchmark the approach.\nOur results demonstrate that samplets mark a considerable step in the direction\nof making large data sets accessible for analysis.",
          "link": "http://arxiv.org/abs/2107.03337",
          "publishedOn": "2021-07-08T01:57:59.857Z",
          "wordCount": 581,
          "title": "Samplets: A new paradigm for data compression. (arXiv:2107.03337v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1\">Seyyedali Hosseinalipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorlatova_M/0/1/0/all/0/1\">Maria Gorlatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1\">Christopher G. Brinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1\">Mung Chiang</a>",
          "description": "We consider distributed machine learning (ML) through unmanned aerial\nvehicles (UAVs) for geo-distributed device clusters. We propose five new\ntechnologies/techniques: (i) stratified UAV swarms with leader, worker, and\ncoordinator UAVs, (ii) hierarchical nested personalized federated learning\n(HN-PFL): a holistic distributed ML framework for personalized model training\nacross the worker-leader-core network hierarchy, (iii) cooperative UAV resource\npooling for distributed ML using the UAVs' local computational capabilities,\n(iv) aerial data caching and relaying for efficient data relaying to conduct\nML, and (v) concept/model drift, capturing online data variations at the\ndevices. We split the UAV-enabled model training problem as two parts. (a)\nNetwork-aware HN-PFL, where we optimize a tradeoff between energy consumption\nand ML model performance by configuring data offloading among devices-UAVs and\nUAV-UAVs, UAVs' CPU frequencies, and mini-batch sizes subject to\ncommunication/computation network heterogeneity. We tackle this optimization\nproblem via the method of posynomial condensation and propose a distributed\nalgorithm with a performance guarantee. (b) Macro-trajectory and learning\nduration design, which we formulate as a sequential decision making problem,\ntackled via deep reinforcement learning. Our simulations demonstrate the\nsuperiority of our methodology with regards to the distributed ML performance,\nthe optimization of network resources, and the swarm trajectory efficiency.",
          "link": "http://arxiv.org/abs/2106.15734",
          "publishedOn": "2021-07-08T01:57:59.838Z",
          "wordCount": 673,
          "title": "UAV-assisted Online Machine Learning over Multi-Tiered Networks: A Hierarchical Nested Personalized Federated Learning Approach. (arXiv:2106.15734v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhicheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chenglei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Sidan Du</a>",
          "description": "Regularization plays a vital role in machine learning optimization. One novel\nregularization method called flooding makes the training loss fluctuate around\nthe flooding level. It intends to make the model continue to random walk until\nit comes to a flat loss landscape to enhance generalization. However, the\nhyper-parameter flooding level of the flooding method fails to be selected\nproperly and uniformly. We propose a novel method called Jitter to improve it.\nJitter is essentially a kind of random loss function. Before training, we\nrandomly sample the Jitter Point from a specific probability distribution. The\nflooding level should be replaced by Jitter point to obtain a new target\nfunction and train the model accordingly. As Jitter point acting as a random\nfactor, we actually add some randomness to the loss function, which is\nconsistent with the fact that there exists innumerable random behaviors in the\nlearning process of the machine learning model and is supposed to make the\nmodel more robust. In addition, Jitter performs random walk randomly which\ndivides the loss curve into small intervals and then flipping them over,\nideally making the loss curve much flatter and enhancing generalization\nability. Moreover, Jitter can be a domain-, task-, and model-independent\nregularization method and train the model effectively after the training error\nreduces to zero. Our experimental results show that Jitter method can improve\nmodel performance more significantly than the previous flooding method and make\nthe test loss curve descend twice.",
          "link": "http://arxiv.org/abs/2106.13749",
          "publishedOn": "2021-07-08T01:57:59.831Z",
          "wordCount": 689,
          "title": "Jitter: Random Jittering Loss Function. (arXiv:2106.13749v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12711",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Staerman_G/0/1/0/all/0/1\">Guillaume Staerman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mozharovskyi_P/0/1/0/all/0/1\">Pavlo Mozharovskyi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Clemencon_S/0/1/0/all/0/1\">St&#xe9;phan Cl&#xe9;men&#xe7;on</a>, <a href=\"http://arxiv.org/find/stat/1/au:+dAlche_Buc_F/0/1/0/all/0/1\">Florence d&#x27;Alch&#xe9;-Buc</a>",
          "description": "Data depth is a non parametric statistical tool that measures centrality of\nany element $x\\in\\mathbb{R}^d$ with respect to (w.r.t.) a probability\ndistribution or a data set. It is a natural median-oriented extension of the\ncumulative distribution function (cdf) to the multivariate case. Consequently,\nits upper level sets -- the depth-trimmed regions -- give rise to a definition\nof multivariate quantiles. In this work, we propose two new pseudo-metrics\nbetween continuous probability measures based on data depth and its associated\ncentral regions. The first one is constructed as the Lp-distance between data\ndepth w.r.t. each distribution while the second one relies on the Hausdorff\ndistance between their quantile regions. It can further be seen as an original\nway to extend the one-dimensional formulae of the Wasserstein distance, which\ninvolves quantiles and cdfs, to the multivariate space. After discussing the\nproperties of these pseudo-metrics and providing conditions under which they\ndefine a distance, we highlight similarities with the Wasserstein distance.\nInterestingly, the derived non-asymptotic bounds show that in contrast to the\nWasserstein distance, the proposed pseudo-metrics do not suffer from the curse\nof dimensionality. Moreover, based on the support function of a convex body, we\npropose an efficient approximation possessing linear time complexity w.r.t. the\nsize of the data set and its dimension. The quality of this approximation as\nwell as the performance of the proposed approach are illustrated in\nexperiments. Furthermore, by construction the regions-based pseudo-metric\nappears to be robust w.r.t. both outliers and heavy tails, a behavior witnessed\nin the numerical experiments.",
          "link": "http://arxiv.org/abs/2103.12711",
          "publishedOn": "2021-07-08T01:57:59.825Z",
          "wordCount": 704,
          "title": "A Pseudo-Metric between Probability Distributions based on Depth-Trimmed Regions. (arXiv:2103.12711v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03051",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gentzel_A/0/1/0/all/0/1\">Amanda Gentzel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pruthi_P/0/1/0/all/0/1\">Purva Pruthi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jensen_D/0/1/0/all/0/1\">David Jensen</a>",
          "description": "Methods that infer causal dependence from observational data are central to\nmany areas of science, including medicine, economics, and the social sciences.\nA variety of theoretical properties of these methods have been proven, but\nempirical evaluation remains a challenge, largely due to the lack of\nobservational data sets for which treatment effect is known. We describe and\nanalyze observational sampling from randomized controlled trials (OSRCT), a\nmethod for evaluating causal inference methods using data from randomized\ncontrolled trials (RCTs). This method can be used to create constructed\nobservational data sets with corresponding unbiased estimates of treatment\neffect, substantially increasing the number of data sets available for\nempirical evaluation of causal inference methods. We show that, in expectation,\nOSRCT creates data sets that are equivalent to those produced by randomly\nsampling from empirical data sets in which all potential outcomes are\navailable. We then perform a large-scale evaluation of seven causal inference\nmethods over 37 data sets, drawn from RCTs, as well as simulators, real-world\ncomputational systems, and observational data sets augmented with a synthetic\nresponse variable. We find notable performance differences when comparing\nacross data from different sources, demonstrating the importance of using data\nfrom a variety of sources when evaluating any causal inference method.",
          "link": "http://arxiv.org/abs/2010.03051",
          "publishedOn": "2021-07-08T01:57:59.817Z",
          "wordCount": 677,
          "title": "How and Why to Use Experimental Data to Evaluate Methods for Observational Causal Inference. (arXiv:2010.03051v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gawlikowski_J/0/1/0/all/0/1\">Jakob Gawlikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tassi_C/0/1/0/all/0/1\">Cedrique Rovile Njieutcheu Tassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsin Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humt_M/0/1/0/all/0/1\">Matthias Humt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruspe_A/0/1/0/all/0/1\">Anna Kruspe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1\">Rudolph Triebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_P/0/1/0/all/0/1\">Peter Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_M/0/1/0/all/0/1\">Muhammad Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamler_R/0/1/0/all/0/1\">Richard Bamler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Due to their increasing spread, confidence in neural network predictions\nbecame more and more important. However, basic neural networks do not deliver\ncertainty estimates or suffer from over or under confidence. Many researchers\nhave been working on understanding and quantifying uncertainty in a neural\nnetwork's prediction. As a result, different types and sources of uncertainty\nhave been identified and a variety of approaches to measure and quantify\nuncertainty in neural networks have been proposed. This work gives a\ncomprehensive overview of uncertainty estimation in neural networks, reviews\nrecent advances in the field, highlights current challenges, and identifies\npotential research opportunities. It is intended to give anyone interested in\nuncertainty estimation in neural networks a broad overview and introduction,\nwithout presupposing prior knowledge in this field. A comprehensive\nintroduction to the most crucial sources of uncertainty is given and their\nseparation into reducible model uncertainty and not reducible data uncertainty\nis presented. The modeling of these uncertainties based on deterministic neural\nnetworks, Bayesian neural networks, ensemble of neural networks, and test-time\ndata augmentation approaches is introduced and different branches of these\nfields as well as the latest developments are discussed. For a practical\napplication, we discuss different measures of uncertainty, approaches for the\ncalibration of neural networks and give an overview of existing baselines and\nimplementations. Different examples from the wide spectrum of challenges in\ndifferent fields give an idea of the needs and challenges regarding\nuncertainties in practical applications. Additionally, the practical\nlimitations of current methods for mission- and safety-critical real world\napplications are discussed and an outlook on the next steps towards a broader\nusage of such methods is given.",
          "link": "http://arxiv.org/abs/2107.03342",
          "publishedOn": "2021-07-08T01:57:59.810Z",
          "wordCount": 728,
          "title": "A Survey of Uncertainty in Deep Neural Networks. (arXiv:2107.03342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.13435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamabattula_S/0/1/0/all/0/1\">Sree Ram Kamabattula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devarajan_V/0/1/0/all/0/1\">Venkat Devarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazi_B/0/1/0/all/0/1\">Babak Namazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_G/0/1/0/all/0/1\">Ganesh Sankaranarayanan</a>",
          "description": "Training deep neural networks (DNNs) with noisy labels is a challenging\nproblem due to over-parameterization. DNNs tend to essentially fit on clean\nsamples at a higher rate in the initial stages, and later fit on the noisy\nsamples at a relatively lower rate. Thus, with a noisy dataset, the test\naccuracy increases initially and drops in the later stages. To find an early\nstopping point at the maximum obtainable test accuracy (MOTA), recent studies\nassume either that i) a clean validation set is available or ii) the noise\nratio is known, or, both. However, often a clean validation set is unavailable,\nand the noise estimation can be inaccurate. To overcome these issues, we\nprovide a novel training solution, free of these conditions. We analyze the\nrate of change of the training accuracy for different noise ratios under\ndifferent conditions to identify a training stop region. We further develop a\nheuristic algorithm based on a small-learning assumption to find a training\nstop point (TSP) at or close to MOTA. To the best of our knowledge, our method\nis the first to rely solely on the \\textit{training behavior}, while utilizing\nthe entire training set, to automatically find a TSP. We validated the\nrobustness of our algorithm (AutoTSP) through several experiments on CIFAR-10,\nCIFAR-100, and a real-world noisy dataset for different noise ratios, noise\ntypes, and architectures.",
          "link": "http://arxiv.org/abs/2012.13435",
          "publishedOn": "2021-07-08T01:57:59.790Z",
          "wordCount": 704,
          "title": "Identifying Training Stop Point with Noisy Labeled Data. (arXiv:2012.13435v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baranwal_A/0/1/0/all/0/1\">Aseem Baranwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fountoulakis_K/0/1/0/all/0/1\">Kimon Fountoulakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagannath_A/0/1/0/all/0/1\">Aukosh Jagannath</a>",
          "description": "Recently there has been increased interest in semi-supervised classification\nin the presence of graphical information. A new class of learning models has\nemerged that relies, at its most basic level, on classifying the data after\nfirst applying a graph convolution. To understand the merits of this approach,\nwe study the classification of a mixture of Gaussians, where the data\ncorresponds to the node attributes of a stochastic block model. We show that\ngraph convolution extends the regime in which the data is linearly separable by\na factor of roughly $1/\\sqrt{D}$, where $D$ is the expected degree of a node,\nas compared to the mixture model data on its own. Furthermore, we find that the\nlinear classifier obtained by minimizing the cross-entropy loss after the graph\nconvolution generalizes to out-of-distribution data where the unseen data can\nhave different intra- and inter-class edge probabilities from the training\ndata.",
          "link": "http://arxiv.org/abs/2102.06966",
          "publishedOn": "2021-07-08T01:57:59.783Z",
          "wordCount": 628,
          "title": "Graph Convolution for Semi-Supervised Classification: Improved Linear Separability and Out-of-Distribution Generalization. (arXiv:2102.06966v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03323",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cvetko_T/0/1/0/all/0/1\">Tim Cvetko</a>",
          "description": "Brain tumor segmentation is a challenging problem in medical image analysis.\nThe endpoint is to generate the salient masks that accurately identify brain\ntumor regions in an fMRI screening. In this paper, we propose a novel attention\ngate (AG model) for brain tumor segmentation that utilizes both the edge\ndetecting unit and the attention gated network to highlight and segment the\nsalient regions from fMRI images. This feature enables us to eliminate the\nnecessity of having to explicitly point towards the damaged area(external\ntissue localization) and classify(classification) as per classical computer\nvision techniques. AGs can easily be integrated within the deep convolutional\nneural networks(CNNs). Minimal computional overhead is required while the AGs\nincrease the sensitivity scores significantly. We show that the edge detector\nalong with an attention gated mechanism provide a sufficient enough method for\nbrain segmentation reaching an IOU of 0.78",
          "link": "http://arxiv.org/abs/2107.03323",
          "publishedOn": "2021-07-08T01:57:59.776Z",
          "wordCount": 594,
          "title": "AGD-Autoencoder: Attention Gated Deep Convolutional Autoencoder for Brain Tumor Segmentation. (arXiv:2107.03323v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hande Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Keping Yang</a>",
          "description": "Recommender systems rely on user behavior data like ratings and clicks to\nbuild personalization model. However, the collected data is observational\nrather than experimental, causing various biases in the data which\nsignificantly affect the learned model. Most existing work for recommendation\ndebiasing, such as the inverse propensity scoring and imputation approaches,\nfocuses on one or two specific biases, lacking the universal capacity that can\naccount for mixed or even unknown biases in the data.\n\nTowards this research gap, we first analyze the origin of biases from the\nperspective of \\textit{risk discrepancy} that represents the difference between\nthe expectation empirical risk and the true risk. Remarkably, we derive a\ngeneral learning framework that well summarizes most existing debiasing\nstrategies by specifying some parameters of the general framework. This\nprovides a valuable opportunity to develop a universal solution for debiasing,\ne.g., by learning the debiasing parameters from data. However, the training\ndata lacks important signal of how the data is biased and what the unbiased\ndata looks like. To move this idea forward, we propose \\textit{AotoDebias} that\nleverages another (small) set of uniform data to optimize the debiasing\nparameters by solving the bi-level optimization problem with meta-learning.\nThrough theoretical analyses, we derive the generalization bound for AutoDebias\nand prove its ability to acquire the appropriate debiasing strategy. Extensive\nexperiments on two real datasets and a simulated dataset demonstrated\neffectiveness of AutoDebias. The code is available at\n\\url{https://github.com/DongHande/AutoDebias}.",
          "link": "http://arxiv.org/abs/2105.04170",
          "publishedOn": "2021-07-08T01:57:59.769Z",
          "wordCount": 713,
          "title": "AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>",
          "description": "This paper is concerned with multi-modal data fusion (MMDF) under unexpected\nmodality failures in nonlinear non-Gaussian dynamic processes. An efficient\nframework to tackle this problem is proposed. In particular, a notion termed\nmodality \"\\emph{usefulness}\", which takes a value of 1 or 0, is used for\nindicating whether the observation of this modality is useful or not. For $n$\nmodalities involved, $2^n$ combinations of their \"\\emph{usefulness}\" values\nexist. Each combination defines one hypothetical model of the true data\ngenerative process. Then the problem of concern is formalized as a task of\nnonlinear non-Gaussian state filtering under model uncertainty, which is\naddressed by a dynamic model averaging (DMA) based particle filter (PF)\nalgorithm. This DMA algorithm employs $2^n$ models, while all models share the\nsame state-transition function and a unique set of particle values. That makes\nthe computational complexity of this algorithm only slightly larger than a\nsingle model based PF algorithm, especially for scenarios in which $n$ is\nsmall. Experimental results show that the proposed solution outperforms\nremarkably state-of-the-art methods. Code and data are available at\nhttps://github.com/robinlau1981/fusion.",
          "link": "http://arxiv.org/abs/2105.06018",
          "publishedOn": "2021-07-08T01:57:59.761Z",
          "wordCount": 640,
          "title": "Robust Dynamic Multi-Modal Data Fusion: A Model Uncertainty Perspective. (arXiv:2105.06018v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07801",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lin_S/0/1/0/all/0/1\">Shanny Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>",
          "description": "Enhancing the spatio-temporal observability of distributed energy resources\n(DERs) is crucial for achieving secure and efficient operations in distribution\ngrids. This paper puts forth a joint recovery framework for residential loads\nby leveraging the complimentary strengths of heterogeneous types of\nmeasurements. The proposed approaches integrate the low-resolution smart meter\ndata collected for every load node with the fast-sampled feeder-level\nmeasurements provided by limited number of phasor measurement units. To address\nthe lack of data, we exploit two key characteristics for the loads and DERs,\nnamely the sparse changes due to infrequent activities of appliances and\nelectric vehicles (EVs) and the locational dependence of solar photovoltaic\n(PV) generation. Accordingly, meaningful regularization terms are introduced to\ncast a convex load recovery problem, which will be further simplified to reduce\ncomputational complexity. The load recovery solutions can be utilized to\nidentify the EV charging events at each load node and to infer the total\nbehind-the-meter PV output. Numerical tests using real-world data have\ndemonstrated the effectiveness of the proposed approaches in enhancing the\nvisibility of these grid-edge DERs.",
          "link": "http://arxiv.org/abs/2102.07801",
          "publishedOn": "2021-07-08T01:57:59.742Z",
          "wordCount": 636,
          "title": "Enhancing the Spatio-temporal Observability of Grid-Edge Resources in Distribution Grids. (arXiv:2102.07801v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13867",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1\">Jiameng Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "We propose POLAR, a \\textbf{pol}ynomial \\textbf{ar}ithmetic framework that\nleverages polynomial overapproximations with interval remainders for\nbounded-time reachability analysis of neural network-controlled systems\n(NNCSs). Compared with existing arithmetic approaches that use standard Taylor\nmodels, our framework uses a novel approach to iteratively overapproximate the\nneuron output ranges layer-by-layer with a combination of Bernstein polynomial\ninterpolation for continuous activation functions and Taylor model arithmetic\nfor the other operations. This approach can overcome the main drawback in the\nstandard Taylor model arithmetic, i.e. its inability to handle functions that\ncannot be well approximated by Taylor polynomials, and significantly improve\nthe accuracy and efficiency of reachable states computation for NNCSs. To\nfurther tighten the overapproximation, our method keeps the Taylor model\nremainders symbolic under the linear mappings when estimating the output range\nof a neural network. We show that POLAR can be seamlessly integrated with\nexisting Taylor model flowpipe construction techniques, and demonstrate that\nPOLAR significantly outperforms the current state-of-the-art techniques on a\nsuite of benchmarks.",
          "link": "http://arxiv.org/abs/2106.13867",
          "publishedOn": "2021-07-08T01:57:59.736Z",
          "wordCount": 634,
          "title": "POLAR: A Polynomial Arithmetic Framework for Verifying Neural-Network Controlled Systems. (arXiv:2106.13867v3 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengli Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zavala_V/0/1/0/all/0/1\">Victor M. Zavala</a>",
          "description": "In this paper we review the mathematical foundations of convolutional neural\nnets (CNNs) with the goals of: i) highlighting connections with techniques from\nstatistics, signal processing, linear algebra, differential equations, and\noptimization, ii) demystifying underlying computations, and iii) identifying\nnew types of applications. CNNs are powerful machine learning models that\nhighlight features from grid data to make predictions (regression and\nclassification). The grid data object can be represented as vectors (in 1D),\nmatrices (in 2D), or tensors (in 3D or higher dimensions) and can incorporate\nmultiple channels (thus providing high flexibility in the input data\nrepresentation). CNNs highlight features from the grid data by performing\nconvolution operations with different types of operators. The operators\nhighlight different types of features (e.g., patterns, gradients, geometrical\nfeatures) and are learned by using optimization techniques. In other words,\nCNNs seek to identify optimal operators that best map the input data to the\noutput data. A common misconception is that CNNs are only capable of processing\nimage or video data but their application scope is much wider; specifically,\ndatasets encountered in diverse applications can be expressed as grid data.\nHere, we show how to apply CNNs to new types of applications such as optimal\ncontrol, flow cytometry, multivariate process monitoring, and molecular\nsimulations.",
          "link": "http://arxiv.org/abs/2101.04869",
          "publishedOn": "2021-07-08T01:57:59.728Z",
          "wordCount": 678,
          "title": "Convolutional Neural Nets in Chemical Engineering: Foundations, Computations, and Applications. (arXiv:2101.04869v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Serra_T/0/1/0/all/0/1\">Thiago Serra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1\">Srikumar Ramalingam</a>",
          "description": "We can compress a neural network while exactly preserving its underlying\nfunctionality with respect to a given input domain if some of its neurons are\nstable. However, current approaches to determine the stability of neurons with\nRectified Linear Unit (ReLU) activations require solving or finding a good\napproximation to multiple discrete optimization problems. In this work, we\nintroduce an algorithm based on solving a single optimization problem to\nidentify all stable neurons. Our approach is on median 100 times faster than\nthe state-of-art method, which allows us to explore exact compression on deeper\n(5 x 100) and wider (2 x 800) networks within minutes. For classifiers trained\nunder an amount of L1 regularization that does not worsen accuracy, we can\nremove up to 40% of the connections",
          "link": "http://arxiv.org/abs/2102.07804",
          "publishedOn": "2021-07-08T01:57:59.720Z",
          "wordCount": 593,
          "title": "Scaling Up Exact Neural Network Compression by ReLU Stability. (arXiv:2102.07804v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garbacea_C/0/1/0/all/0/1\">Cristina Garbacea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mengtian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carton_S/0/1/0/all/0/1\">Samuel Carton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1\">Qiaozhu Mei</a>",
          "description": "Text simplification reduces the language complexity of professional content\nfor accessibility purposes. End-to-end neural network models have been widely\nadopted to directly generate the simplified version of input text, usually\nfunctioning as a blackbox. We show that text simplification can be decomposed\ninto a compact pipeline of tasks to ensure the transparency and explainability\nof the process. The first two steps in this pipeline are often neglected: 1) to\npredict whether a given piece of text needs to be simplified, and 2) if yes, to\nidentify complex parts of the text. The two tasks can be solved separately\nusing either lexical or deep learning methods, or solved jointly. Simply\napplying explainable complexity prediction as a preliminary step, the\nout-of-sample text simplification performance of the state-of-the-art,\nblack-box simplification models can be improved by a large margin.",
          "link": "http://arxiv.org/abs/2007.15823",
          "publishedOn": "2021-07-08T01:57:59.713Z",
          "wordCount": 612,
          "title": "Explainable Prediction of Text Complexity: The Missing Preliminaries for Text Simplification. (arXiv:2007.15823v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_A/0/1/0/all/0/1\">Abhinav Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasiviswanathan_S/0/1/0/all/0/1\">Shiva Prasad Kasiviswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zekun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feyisetan_O/0/1/0/all/0/1\">Oluwaseyi Feyisetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teissier_N/0/1/0/all/0/1\">Nathanael Teissier</a>",
          "description": "Machine learning classifiers rely on loss functions for performance\nevaluation, often on a private (hidden) dataset. Label inference was recently\nintroduced as the problem of reconstructing the ground truth labels of this\nprivate dataset from just the (possibly perturbed) loss function values\nevaluated at chosen prediction vectors, without any other access to the hidden\ndataset. Existing results have demonstrated this inference is possible on\nspecific loss functions like the cross-entropy loss. In this paper, we\nintroduce the notion of codomain separability to formally study the necessary\nand sufficient conditions under which label inference is possible from any\n(noisy) loss function values. Using this notion, we show that for many commonly\nused loss functions, including multiclass cross-entropy with common activation\nfunctions and some Bregman divergence-based losses, it is possible to design\nlabel inference attacks for arbitrary noise levels. We demonstrate that these\nattacks can also be carried out through actual neural network models, and\nargue, both formally and empirically, the role of finite precision arithmetic\nin this setting.",
          "link": "http://arxiv.org/abs/2107.03022",
          "publishedOn": "2021-07-08T01:57:59.694Z",
          "wordCount": 603,
          "title": "On Codomain Separability and Label Inference from (Noisy) Loss Functions. (arXiv:2107.03022v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03145",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Umer_R/0/1/0/all/0/1\">Rao Muhammad Umer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munir_A/0/1/0/all/0/1\">Asad Munir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Micheloni_C/0/1/0/all/0/1\">Christian Micheloni</a>",
          "description": "Recently, most of state-of-the-art single image super-resolution (SISR)\nmethods have attained impressive performance by using deep convolutional neural\nnetworks (DCNNs). The existing SR methods have limited performance due to a\nfixed degradation settings, i.e. usually a bicubic downscaling of\nlow-resolution (LR) image. However, in real-world settings, the LR degradation\nprocess is unknown which can be bicubic LR, bilinear LR, nearest-neighbor LR,\nor real LR. Therefore, most SR methods are ineffective and inefficient in\nhandling more than one degradation settings within a single network. To handle\nthe multiple degradation, i.e. refers to multi-domain image super-resolution,\nwe propose a deep Super-Resolution Residual StarGAN (SR2*GAN), a novel and\nscalable approach that super-resolves the LR images for the multiple LR domains\nusing only a single model. The proposed scheme is trained in a StarGAN like\nnetwork topology with a single generator and discriminator networks. We\ndemonstrate the effectiveness of our proposed approach in quantitative and\nqualitative experiments compared to other state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.03145",
          "publishedOn": "2021-07-08T01:57:59.687Z",
          "wordCount": 632,
          "title": "A Deep Residual Star Generative Adversarial Network for multi-domain Image Super-Resolution. (arXiv:2107.03145v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12108",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ober_S/0/1/0/all/0/1\">Sebastian W. Ober</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rasmussen_C/0/1/0/all/0/1\">Carl E. Rasmussen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wilk_M/0/1/0/all/0/1\">Mark van der Wilk</a>",
          "description": "Deep kernel learning (DKL) and related techniques aim to combine the\nrepresentational power of neural networks with the reliable uncertainty\nestimates of Gaussian processes. One crucial aspect of these models is an\nexpectation that, because they are treated as Gaussian process models optimized\nusing the marginal likelihood, they are protected from overfitting. However, we\nidentify situations where this is not the case. We explore this behavior,\nexplain its origins and consider how it applies to real datasets. Through\ncareful experimentation on the UCI, CIFAR-10, and the UTKFace datasets, we find\nthat the overfitting from overparameterized maximum marginal likelihood, in\nwhich the model is \"somewhat Bayesian\", can in certain scenarios be worse than\nthat from not being Bayesian at all. We explain how and when DKL can still be\nsuccessful by investigating optimization dynamics. We also find that failures\nof DKL can be rectified by a fully Bayesian treatment, which leads to the\ndesired performance improvements over standard neural networks and Gaussian\nprocesses.",
          "link": "http://arxiv.org/abs/2102.12108",
          "publishedOn": "2021-07-08T01:57:59.657Z",
          "wordCount": 630,
          "title": "The Promises and Pitfalls of Deep Kernel Learning. (arXiv:2102.12108v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yonetani_R/0/1/0/all/0/1\">Ryo Yonetani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniai_T/0/1/0/all/0/1\">Tatsunori Taniai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barekatain_M/0/1/0/all/0/1\">Mohammadamin Barekatain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_M/0/1/0/all/0/1\">Mai Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanezaki_A/0/1/0/all/0/1\">Asako Kanezaki</a>",
          "description": "We present Neural A*, a novel data-driven search method for path planning\nproblems. Despite the recent increasing attention to data-driven path planning,\nmachine learning approaches to search-based planning are still challenging due\nto the discrete nature of search algorithms. In this work, we reformulate a\ncanonical A* search algorithm to be differentiable and couple it with a\nconvolutional encoder to form an end-to-end trainable neural network planner.\nNeural A* solves a path planning problem by encoding a problem instance to a\nguidance map and then performing the differentiable A* search with the guidance\nmap. By learning to match the search results with ground-truth paths provided\nby experts, Neural A* can produce a path consistent with the ground truth\naccurately and efficiently. Our extensive experiments confirmed that Neural A*\noutperformed state-of-the-art data-driven planners in terms of the search\noptimality and efficiency trade-off. Furthermore, Neural A* successfully\npredicted realistic human trajectories by directly performing search-based\nplanning on natural image inputs. Project page:\nhttps://omron-sinicx.github.io/neural-astar/",
          "link": "http://arxiv.org/abs/2009.07476",
          "publishedOn": "2021-07-08T01:57:59.650Z",
          "wordCount": 650,
          "title": "Path Planning using Neural A* Search. (arXiv:2009.07476v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03323",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Podkopaev_A/0/1/0/all/0/1\">Aleksandr Podkopaev</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya Ramdas</a>",
          "description": "Trustworthy deployment of ML models requires a proper measure of uncertainty,\nespecially in safety-critical applications. We focus on uncertainty\nquantification (UQ) for classification problems via two avenues -- prediction\nsets using conformal prediction and calibration of probabilistic predictors by\npost-hoc binning -- since these possess distribution-free guarantees for i.i.d.\ndata. Two common ways of generalizing beyond the i.i.d. setting include\nhandling covariate and label shift. Within the context of distribution-free UQ,\nthe former has already received attention, but not the latter. It is known that\nlabel shift hurts prediction, and we first argue that it also hurts UQ, by\nshowing degradation in coverage and calibration. Piggybacking on recent\nprogress in addressing label shift (for better prediction), we examine the\nright way to achieve UQ by reweighting the aforementioned conformal and\ncalibration procedures whenever some unlabeled data from the target\ndistribution is available. We examine these techniques theoretically in a\ndistribution-free framework and demonstrate their excellent practical\nperformance.",
          "link": "http://arxiv.org/abs/2103.03323",
          "publishedOn": "2021-07-08T01:57:59.588Z",
          "wordCount": 622,
          "title": "Distribution-free uncertainty quantification for classification under label shift. (arXiv:2103.03323v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1\">David Evans</a>",
          "description": "A fundamental question in adversarial machine learning is whether a robust\nclassifier exists for a given task. A line of research has made progress\ntowards this goal by studying concentration of measure, but without considering\ndata labels. We argue that the standard concentration fails to fully\ncharacterize the intrinsic robustness of a classification problem, since it\nignores data labels which are essential to any classification task. Building on\na novel definition of label uncertainty, we empirically demonstrate that error\nregions induced by state-of-the-art models tend to have much higher label\nuncertainty compared with randomly-selected subsets. This observation motivates\nus to adapt a concentration estimation algorithm to account for label\nuncertainty, resulting in more accurate intrinsic robustness measures for\nbenchmark image classification problems. We further provide empirical evidence\nshowing that adding an abstain option for classifiers based on label\nuncertainty can help improve both the clean and robust accuracies of models.",
          "link": "http://arxiv.org/abs/2107.03250",
          "publishedOn": "2021-07-08T01:57:59.567Z",
          "wordCount": 585,
          "title": "Incorporating Label Uncertainty in Understanding Adversarial Robustness. (arXiv:2107.03250v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lingfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1\">Xue-Cheng Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiang Yang</a>",
          "description": "We study gradient-based regularization methods for neural networks. We mainly\nfocus on two regularization methods: the total variation and the Tikhonov\nregularization. Applying these methods is equivalent to using neural networks\nto solve some partial differential equations, mostly in high dimensions in\npractical applications. In this work, we introduce a general framework to\nanalyze the generalization error of regularized networks. The error estimate\nrelies on two assumptions on the approximation error and the quadrature error.\nMoreover, we conduct some experiments on the image classification tasks to show\nthat gradient-based methods can significantly improve the generalization\nability and adversarial robustness of neural networks. A graphical extension of\nthe gradient-based methods are also considered in the experiments.",
          "link": "http://arxiv.org/abs/2107.02797",
          "publishedOn": "2021-07-08T01:57:59.555Z",
          "wordCount": 554,
          "title": "Generalization Error Analysis of Neural networks with Gradient Based Regularization. (arXiv:2107.02797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mavridis_C/0/1/0/all/0/1\">Christos Mavridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baras_J/0/1/0/all/0/1\">John Baras</a>",
          "description": "Inherent in virtually every iterative machine learning algorithm is the\nproblem of hyper-parameter tuning, which includes three major design\nparameters: (a) the complexity of the model, e.g., the number of neurons in a\nneural network, (b) the initial conditions, which heavily affect the behavior\nof the algorithm, and (c) the dissimilarity measure used to quantify its\nperformance. We introduce an online prototype-based learning algorithm that can\nbe viewed as a progressively growing competitive-learning neural network\narchitecture for classification and clustering. The learning rule of the\nproposed approach is formulated as an online gradient-free stochastic\napproximation algorithm that solves a sequence of appropriately defined\noptimization problems, simulating an annealing process. The annealing nature of\nthe algorithm contributes to avoiding poor local minima, offers robustness with\nrespect to the initial conditions, and provides a means to progressively\nincrease the complexity of the learning model, through an intuitive bifurcation\nphenomenon. The proposed approach is interpretable, requires minimal\nhyper-parameter tuning, and allows online control over the\nperformance-complexity trade-off. Finally, we show that Bregman divergences\nappear naturally as a family of dissimilarity measures that play a central role\nin both the performance and the computational complexity of the learning\nalgorithm. Experimental results illustrate the properties and evaluate the\nperformance of the proposed learning algorithm.",
          "link": "http://arxiv.org/abs/2102.05836",
          "publishedOn": "2021-07-08T01:57:59.428Z",
          "wordCount": 659,
          "title": "Online Deterministic Annealing for Classification and Clustering. (arXiv:2102.05836v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.06060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tong Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1\">Jascha Sohl-Dickstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1\">Hugo Larochelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1\">Liam Paull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>",
          "description": "We show that the sum of the implicit generator log-density $\\log p_g$ of a\nGAN with the logit score of the discriminator defines an energy function which\nyields the true data density when the generator is imperfect but the\ndiscriminator is optimal, thus making it possible to improve on the typical\ngenerator (with implicit density $p_g$). To make that practical, we show that\nsampling from this modified density can be achieved by sampling in latent space\naccording to an energy-based model induced by the sum of the latent prior\nlog-density and the discriminator output score. This can be achieved by running\na Langevin MCMC in latent space and then applying the generator function, which\nwe call Discriminator Driven Latent Sampling~(DDLS). We show that DDLS is\nhighly efficient compared to previous methods which work in the\nhigh-dimensional pixel space and can be applied to improve on previously\ntrained GANs of many types. We evaluate DDLS on both synthetic and real-world\ndatasets qualitatively and quantitatively. On CIFAR-10, DDLS substantially\nimproves the Inception Score of an off-the-shelf pre-trained\nSN-GAN~\\citep{sngan} from $8.22$ to $9.09$ which is even comparable to the\nclass-conditional BigGAN~\\citep{biggan} model. This achieves a new\nstate-of-the-art in unconditional image synthesis setting without introducing\nextra parameters or additional training.",
          "link": "http://arxiv.org/abs/2003.06060",
          "publishedOn": "2021-07-08T01:57:59.349Z",
          "wordCount": 705,
          "title": "Your GAN is Secretly an Energy-based Model and You Should use Discriminator Driven Latent Sampling. (arXiv:2003.06060v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1\">Koushik Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sandeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Ashish Kumar Pandey</a>",
          "description": "Tropical cyclones can be of varied intensity and cause a huge loss of lives\nand property if the intensity is high enough. Therefore, the prediction of the\nintensity of tropical cyclones advance in time is of utmost importance. We\npropose a novel stacked bidirectional long short-term memory network (BiLSTM)\nbased model architecture to predict the intensity of a tropical cyclone in\nterms of Maximum surface sustained wind speed (MSWS). The proposed model can\npredict MSWS well advance in time (up to 72 h) with very high accuracy. We have\napplied the model on tropical cyclones in the North Indian Ocean from 1982 to\n2018 and checked its performance on two recent tropical cyclones, namely, Fani\nand Vayu. The model predicts MSWS (in knots) for the next 3, 12, 24, 36, 48,\n60, and 72 hours with a mean absolute error of 1.52, 3.66, 5.88, 7.42, 8.96,\n10.15, and 11.92, respectively.",
          "link": "http://arxiv.org/abs/2107.03187",
          "publishedOn": "2021-07-08T01:57:59.304Z",
          "wordCount": 589,
          "title": "Intensity Prediction of Tropical Cyclones using Long Short-Term Memory Network. (arXiv:2107.03187v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1\">Amin Nikanjam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braiek_H/0/1/0/all/0/1\">Houssem Ben Braiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morovati_M/0/1/0/all/0/1\">Mohammad Mehdi Morovati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>",
          "description": "Nowadays, we are witnessing an increasing demand in both corporates and\nacademia for exploiting Deep Learning (DL) to solve complex real-world\nproblems. A DL program encodes the network structure of a desirable DL model\nand the process by which the model learns from the training dataset. Like any\nsoftware, a DL program can be faulty, which implies substantial challenges of\nsoftware quality assurance, especially in safety-critical domains. It is\ntherefore crucial to equip DL development teams with efficient fault detection\ntechniques and tools. In this paper, we propose NeuraLint, a model-based fault\ndetection approach for DL programs, using meta-modelling and graph\ntransformations. First, we design a meta-model for DL programs that includes\ntheir base skeleton and fundamental properties. Then, we construct a\ngraph-based verification process that covers 23 rules defined on top of the\nmeta-model and implemented as graph transformations to detect faults and design\ninefficiencies in the generated models (i.e., instances of the meta-model).\nFirst, the proposed approach is evaluated by finding faults and design\ninefficiencies in 28 synthesized examples built from common problems reported\nin the literature. Then NeuraLint successfully finds 64 faults and design\ninefficiencies in 34 real-world DL programs extracted from Stack Overflow posts\nand GitHub repositories. The results show that NeuraLint effectively detects\nfaults and design issues in both synthesized and real-world examples with a\nrecall of 70.5 % and a precision of 100 %. Although the proposed meta-model is\ndesigned for feedforward neural networks, it can be extended to support other\nneural network architectures such as recurrent neural networks. Researchers can\nalso expand our set of verification rules to cover more types of issues in DL\nprograms.",
          "link": "http://arxiv.org/abs/2105.08095",
          "publishedOn": "2021-07-08T01:57:59.286Z",
          "wordCount": 741,
          "title": "Automatic Fault Detection for Deep Learning Programs Using Graph Transformations. (arXiv:2105.08095v2 [cs.SE] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fouladvand_S/0/1/0/all/0/1\">Sajjad Fouladvand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbert_J/0/1/0/all/0/1\">Jeffery Talbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwoskin_L/0/1/0/all/0/1\">Linda P. Dwoskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bush_H/0/1/0/all/0/1\">Heather Bush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meadows_A/0/1/0/all/0/1\">Amy Lynn Meadows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peterson_L/0/1/0/all/0/1\">Lars E. Peterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jin Chen</a>",
          "description": "Opioid Use Disorder (OUD) is a public health crisis costing the US billions\nof dollars annually in healthcare, lost workplace productivity, and crime.\nAnalyzing longitudinal healthcare data is critical in addressing many\nreal-world problems in healthcare. Leveraging the real-world longitudinal\nhealthcare data, we propose a novel multi-stream transformer model called MUPOD\nfor OUD identification. MUPOD is designed to simultaneously analyze multiple\ntypes of healthcare data streams, such as medications and diagnoses, by\nattending to segments within and across these data streams. Our model tested on\nthe data from 392,492 patients with long-term back pain problems showed\nsignificantly better performance than the traditional models and recently\ndeveloped deep learning models.",
          "link": "http://arxiv.org/abs/2103.08800",
          "publishedOn": "2021-07-08T01:57:59.279Z",
          "wordCount": 603,
          "title": "Predicting Opioid Use Disorder from Longitudinal Healthcare Data using Multi-stream Transformer. (arXiv:2103.08800v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cantador_I/0/1/0/all/0/1\">Iv&#xe1;n Cantador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1\">Andr&#xe9;s Carvallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diez_F/0/1/0/all/0/1\">Fernando Diez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1\">Denis Parra</a>",
          "description": "The success of neural network embeddings has entailed a renewed interest in\nusing knowledge graphs for a wide variety of machine learning and information\nretrieval tasks. In particular, current recommendation methods based on graph\nembeddings have shown state-of-the-art performance. These methods commonly\nencode latent rating patterns and content features. Different from previous\nwork, in this paper, we propose to exploit embeddings extracted from graphs\nthat combine information from ratings and aspect-based opinions expressed in\ntextual reviews. We then adapt and evaluate state-of-the-art graph embedding\ntechniques over graphs generated from Amazon and Yelp reviews on six domains,\noutperforming baseline recommenders. Our approach has the advantage of\nproviding explanations which leverage aspect-based opinions given by users\nabout recommended items. Furthermore, we also provide examples of the\napplicability of recommendations utilizing aspect opinions as explanations in a\nvisualization dashboard, which allows obtaining information about the most and\nleast liked aspects of similar users obtained from the embeddings of an input\ngraph.",
          "link": "http://arxiv.org/abs/2107.03226",
          "publishedOn": "2021-07-08T01:57:59.268Z",
          "wordCount": 609,
          "title": "Graphing else matters: exploiting aspect opinions and ratings in explainable graph-based recommendations. (arXiv:2107.03226v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengyuan Zhou</a>",
          "description": "We design a simple reinforcement learning (RL) agent that implements an\noptimistic version of $Q$-learning and establish through regret analysis that\nthis agent can operate with some level of competence in any environment. While\nwe leverage concepts from the literature on provably efficient RL, we consider\na general agent-environment interface and provide a novel agent design and\nanalysis. This level of generality positions our results to inform the design\nof future agents for operation in complex real environments. We establish that,\nas time progresses, our agent performs competitively relative to policies that\nrequire longer times to evaluate. The time it takes to approach asymptotic\nperformance is polynomial in the complexity of the agent's state representation\nand the time required to evaluate the best policy that the agent can represent.\nNotably, there is no dependence on the complexity of the environment. The\nultimate per-period performance loss of the agent is bounded by a constant\nmultiple of a measure of distortion introduced by the agent's state\nrepresentation. This work is the first to establish that an algorithm\napproaches this asymptotic condition within a tractable time frame.",
          "link": "http://arxiv.org/abs/2102.05261",
          "publishedOn": "2021-07-08T01:57:59.262Z",
          "wordCount": 686,
          "title": "Simple Agent, Complex Environment: Efficient Reinforcement Learning with Agent States. (arXiv:2102.05261v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02919",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengyuan Zhou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mertikopoulos_P/0/1/0/all/0/1\">Panayotis Mertikopoulos</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bambos_N/0/1/0/all/0/1\">Nicholas Bambos</a>, <a href=\"http://arxiv.org/find/math/1/au:+Glynn_P/0/1/0/all/0/1\">Peter W. Glynn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1\">Yinyu Ye</a>",
          "description": "One of the most widely used methods for solving large-scale stochastic\noptimization problems is distributed asynchronous stochastic gradient descent\n(DASGD), a family of algorithms that result from parallelizing stochastic\ngradient descent on distributed computing architectures (possibly)\nasychronously. However, a key obstacle in the efficient implementation of DASGD\nis the issue of delays: when a computing node contributes a gradient update,\nthe global model parameter may have already been updated by other nodes several\ntimes over, thereby rendering this gradient information stale. These delays can\nquickly add up if the computational throughput of a node is saturated, so the\nconvergence of DASGD may be compromised in the presence of large delays. Our\nfirst contribution is that, by carefully tuning the algorithm's step-size,\nconvergence to the critical set is still achieved in mean square, even if the\ndelays grow unbounded at a polynomial rate. We also establish finer results in\na broad class of structured optimization problems (called variationally\ncoherent), where we show that DASGD converges to a global optimum with\nprobability $1$ under the same delay assumptions. Together, these results\ncontribute to the broad landscape of large-scale non-convex stochastic\noptimization by offering state-of-the-art theoretical guarantees and providing\ninsights for algorithm design.",
          "link": "http://arxiv.org/abs/2107.02919",
          "publishedOn": "2021-07-08T01:57:59.255Z",
          "wordCount": 656,
          "title": "Distributed stochastic optimization with large delays. (arXiv:2107.02919v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03248",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Venkataramanan_V/0/1/0/all/0/1\">Venkatesh Venkataramanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaza_S/0/1/0/all/0/1\">Sridevi Kaza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Annaswamy_A/0/1/0/all/0/1\">Anuradha M. Annaswamy</a>",
          "description": "With increasing penetration of Distributed Energy Resources (DERs) in grid\nedge including renewable generation, flexible loads, and storage, accurate\nprediction of distributed generation and consumption at the consumer level\nbecomes important. However, DER prediction based on the transmission of\ncustomer level data, either repeatedly or in large amounts, is not feasible due\nto privacy concerns. In this paper, a distributed machine learning approach,\nFederated Learning, is proposed to carry out DER forecasting using a network of\nIoT nodes, each of which transmits a model of the consumption and generation\npatterns without revealing consumer data. We consider a simulation study which\nincludes 1000 DERs, and show that our method leads to an accurate prediction of\npreserve consumer privacy, while still leading to an accurate forecast. We also\nevaluate grid-specific performance metrics such as load swings and load\ncurtailment and show that our FL algorithm leads to satisfactory performance.\nSimulations are also performed on the Pecan street dataset to demonstrate the\nvalidity of the proposed approach on real data.",
          "link": "http://arxiv.org/abs/2107.03248",
          "publishedOn": "2021-07-08T01:57:59.229Z",
          "wordCount": 612,
          "title": "DER Forecast using Privacy Preserving Federated Learning. (arXiv:2107.03248v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mark Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tworek_J/0/1/0/all/0/1\">Jerry Tworek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_H/0/1/0/all/0/1\">Heewoo Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1\">Qiming Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponde_H/0/1/0/all/0/1\">Henrique Ponde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_H/0/1/0/all/0/1\">Harri Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burda_Y/0/1/0/all/0/1\">Yura Burda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockman_G/0/1/0/all/0/1\">Greg Brockman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Alex Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Raul Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1\">Gretchen Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_M/0/1/0/all/0/1\">Michael Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khlaaf_H/0/1/0/all/0/1\">Heidy Khlaaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_G/0/1/0/all/0/1\">Girish Sastry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_P/0/1/0/all/0/1\">Pamela Mishkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_B/0/1/0/all/0/1\">Brooke Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_S/0/1/0/all/0/1\">Scott Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryder_N/0/1/0/all/0/1\">Nick Ryder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlov_M/0/1/0/all/0/1\">Mikhail Pavlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Power_A/0/1/0/all/0/1\">Alethea Power</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1\">Lukasz Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bavarian_M/0/1/0/all/0/1\">Mohammad Bavarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winter_C/0/1/0/all/0/1\">Clemens Winter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tillet_P/0/1/0/all/0/1\">Philippe Tillet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Such_F/0/1/0/all/0/1\">Felipe Such</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_D/0/1/0/all/0/1\">Dave Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plappert_M/0/1/0/all/0/1\">Matthias Plappert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chantzis_F/0/1/0/all/0/1\">Fotios Chantzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_E/0/1/0/all/0/1\">Elizabeth Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbert_Voss_A/0/1/0/all/0/1\">Ariel Herbert-Voss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guss_W/0/1/0/all/0/1\">Will Guss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nichol_A/0/1/0/all/0/1\">Alex Nichol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuschkin_I/0/1/0/all/0/1\">Igor Babuschkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaji_S/0/1/0/all/0/1\">Suchir Balaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shantanu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carr_A/0/1/0/all/0/1\">Andrew Carr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1\">Jan Leike</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achiam_J/0/1/0/all/0/1\">Josh Achiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1\">Vedant Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morikawa_E/0/1/0/all/0/1\">Evan Morikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1\">Alec Radford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_M/0/1/0/all/0/1\">Matthew Knight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1\">Miles Brundage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murati_M/0/1/0/all/0/1\">Mira Murati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_K/0/1/0/all/0/1\">Katie Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welinder_P/0/1/0/all/0/1\">Peter Welinder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGrew_B/0/1/0/all/0/1\">Bob McGrew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Dario Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutskever_I/0/1/0/all/0/1\">Ilya Sutskever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaremba_W/0/1/0/all/0/1\">Wojciech Zaremba</a>",
          "description": "We introduce Codex, a GPT language model fine-tuned on publicly available\ncode from GitHub, and study its Python code-writing capabilities. A distinct\nproduction version of Codex powers GitHub Copilot. On HumanEval, a new\nevaluation set we release to measure functional correctness for synthesizing\nprograms from docstrings, our model solves 28.8% of the problems, while GPT-3\nsolves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling\nfrom the model is a surprisingly effective strategy for producing working\nsolutions to difficult prompts. Using this method, we solve 70.2% of our\nproblems with 100 samples per problem. Careful investigation of our model\nreveals its limitations, including difficulty with docstrings describing long\nchains of operations and with binding operations to variables. Finally, we\ndiscuss the potential broader impacts of deploying powerful code generation\ntechnologies, covering safety, security, and economics.",
          "link": "http://arxiv.org/abs/2107.03374",
          "publishedOn": "2021-07-08T01:57:59.223Z",
          "wordCount": 663,
          "title": "Evaluating Large Language Models Trained on Code. (arXiv:2107.03374v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Markovic_D/0/1/0/all/0/1\">Dimitrije Markovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stojic_H/0/1/0/all/0/1\">Hrvoje Stojic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwoebel_S/0/1/0/all/0/1\">Sarah Schwoebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiebel_S/0/1/0/all/0/1\">Stefan J. Kiebel</a>",
          "description": "A key feature of sequential decision making under uncertainty is a need to\nbalance between exploiting--choosing the best action according to the current\nknowledge, and exploring--obtaining information about values of other actions.\nThe multi-armed bandit problem, a classical task that captures this trade-off,\nserved as a vehicle in machine learning for developing bandit algorithms that\nproved to be useful in numerous industrial applications. The active inference\nframework, an approach to sequential decision making recently developed in\nneuroscience for understanding human and animal behaviour, is distinguished by\nits sophisticated strategy for resolving the exploration-exploitation\ntrade-off. This makes active inference an exciting alternative to already\nestablished bandit algorithms. Here we derive an efficient and scalable\napproximate active inference algorithm and compare it to two state-of-the-art\nbandit algorithms: Bayesian upper confidence bound and optimistic Thompson\nsampling. This comparison is done on two types of bandit problems: a stationary\nand a dynamic switching bandit. Our empirical evaluation shows that the active\ninference algorithm does not produce efficient long-term behaviour in\nstationary bandits. However, in the more challenging switching bandit problem\nactive inference performs substantially better than the two state-of-the-art\nbandit algorithms. The results open exciting venues for further research in\ntheoretical and applied machine learning, as well as lend additional\ncredibility to active inference as a general framework for studying human and\nanimal behaviour.",
          "link": "http://arxiv.org/abs/2101.08699",
          "publishedOn": "2021-07-08T01:57:59.215Z",
          "wordCount": 688,
          "title": "An empirical evaluation of active inference in multi-armed bandits. (arXiv:2101.08699v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07290",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ramzi_Z/0/1/0/all/0/1\">Zaccharie Ramzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ciuciu_P/0/1/0/all/0/1\">Philippe Ciuciu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Starck_J/0/1/0/all/0/1\">Jean-Luc Starck</a>",
          "description": "We present a new neural network, the XPDNet, for MRI reconstruction from\nperiodically under-sampled multi-coil data. We inform the design of this\nnetwork by taking best practices from MRI reconstruction and computer vision.\nWe show that this network can achieve state-of-the-art reconstruction results,\nas shown by its ranking of second in the fastMRI 2020 challenge.",
          "link": "http://arxiv.org/abs/2010.07290",
          "publishedOn": "2021-07-08T01:57:59.206Z",
          "wordCount": 549,
          "title": "XPDNet for MRI Reconstruction: an application to the 2020 fastMRI challenge. (arXiv:2010.07290v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duong_Q/0/1/0/all/0/1\">Quan Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quoc Nguyen</a>",
          "description": "This study aims to propose effective modeling and approach for designing a\nlogistics network in the urban area in order to offer an efficient flow\ndistribution network as a competitive strategy in the logistics industry where\ndemand is sensitive to both price and time. A multi-stage approach is\nintroduced to select the number of hubs and allocate spokes to the hubs for\nflow distribution and hubs' location detection. Specifically, a fuzzy\nclustering model with the objective function is to minimize the approximate\ntransportation cost is employed, in the next phase is to focus on balancing the\ndemand capacity among the hubs with the help of domain experts, afterward, the\nfacility location vehicle routing problems within the network is introduced. To\ndemonstrate the approach's advantages, an experiment was performed on the\ndesigned network and its actual transportation cost for the real operational\ndata in which specific to the Ho Chi Minh city infrastructure conditions.\nAdditionally, we show the flexibility of the designed network in the flow\ndistribution and its computational experiments to develop the managerial\ninsights which contribute to the network design decision-making process.",
          "link": "http://arxiv.org/abs/2107.03080",
          "publishedOn": "2021-07-08T01:57:59.199Z",
          "wordCount": 625,
          "title": "Hub and Spoke Logistics Network Design for Urban Region with Clustering-Based Approach. (arXiv:2107.03080v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.12780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paterson_C/0/1/0/all/0/1\">Colin Paterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calinescu_R/0/1/0/all/0/1\">Radu Calinescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picardi_C/0/1/0/all/0/1\">Chiara Picardi</a>",
          "description": "Regions of high-dimensional input spaces that are underrepresented in\ntraining datasets reduce machine-learnt classifier performance, and may lead to\ncorner cases and unwanted bias for classifiers used in decision making systems.\nWhen these regions belong to otherwise well-represented classes, their presence\nand negative impact are very hard to identify. We propose an approach for the\ndetection and mitigation of such rare subclasses in deep neural network\nclassifiers. The new approach is underpinned by an easy-to-compute commonality\nmetric that supports the detection of rare subclasses, and comprises methods\nfor reducing the impact of these subclasses during both model training and\nmodel exploitation. We demonstrate our approach using two well-known datasets,\nMNIST's handwritten digits and Kaggle's cats/dogs, identifying rare subclasses\nand producing models which compensate for subclass rarity. In addition we\ndemonstrate how our run-time approach increases the ability of users to\nidentify samples likely to be misclassified at run-time.",
          "link": "http://arxiv.org/abs/1911.12780",
          "publishedOn": "2021-07-08T01:57:59.182Z",
          "wordCount": 628,
          "title": "Detection and Mitigation of Rare Subclasses in Deep Neural Network Classifiers. (arXiv:1911.12780v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03230",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Grbcic_L/0/1/0/all/0/1\">Luka Grb&#x10d;i&#x107;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Druzeta_S/0/1/0/all/0/1\">Sini&#x161;a Dru&#x17e;eta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mausa_G/0/1/0/all/0/1\">Goran Mau&#x161;a</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lipic_T/0/1/0/all/0/1\">Tomislav Lipi&#x107;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lusic_D/0/1/0/all/0/1\">Darija Vuki&#x107; Lu&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Alvir_M/0/1/0/all/0/1\">Marta Alvir</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lucin_I/0/1/0/all/0/1\">Ivana Lu&#x10d;in</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sikirica_A/0/1/0/all/0/1\">Ante Sikirica</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Davidovic_D/0/1/0/all/0/1\">Davor Davidovi&#x107;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Travas_V/0/1/0/all/0/1\">Vanja Trava&#x161;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kalafatovic_D/0/1/0/all/0/1\">Daniela Kalafatovi&#x107;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pikelj_K/0/1/0/all/0/1\">Kristina Pikelj</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fajkovic_H/0/1/0/all/0/1\">Hana Fajkovi&#x107;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kranjcevic_L/0/1/0/all/0/1\">Lado Kranj&#x10d;evi&#x107;</a>",
          "description": "Coastal water quality management is a public health concern, as poor coastal\nwater quality can harbor pathogens that are dangerous to human health.\nTourism-oriented countries need to actively monitor the condition of coastal\nwater at tourist popular sites during the summer season. In this study, routine\nmonitoring data of $Escherichia\\ Coli$ and enterococci across 15 public beaches\nin the city of Rijeka, Croatia, were used to build machine learning models for\npredicting their levels based on environmental parameters as well as to\ninvestigate their relationships with environmental stressors. Gradient Boosting\n(Catboost, Xgboost), Random Forests, Support Vector Regression and Artificial\nNeural Networks were trained with measurements from all sampling sites and used\nto predict $E.\\ Coli$ and enterococci values based on environmental features.\nThe evaluation of stability and generalizability with 10-fold cross validation\nanalysis of the machine learning models, showed that the Catboost algorithm\nperformed best with R$^2$ values of 0.71 and 0.68 for predicting $E.\\ Coli$ and\nenterococci, respectively, compared to other evaluated ML algorithms including\nXgboost, Random Forests, Support Vector Regression and Artificial Neural\nNetworks. We also use the SHapley Additive exPlanations technique to identify\nand interpret which features have the most predictive power. The results show\nthat site salinity measured is the most important feature for forecasting both\n$E.\\ Coli$ and enterococci levels. Finally, the spatial and temporal accuracy\nof both ML models were examined at sites with the lowest coastal water quality.\nThe spatial $E. Coli$ and enterococci models achieved strong R$^2$ values of\n0.85 and 0.83, while the temporal models achieved R$^2$ values of 0.74 and\n0.67. The temporal model also achieved moderate R$^2$ values of 0.44 and 0.46\nat a site with high coastal water quality.",
          "link": "http://arxiv.org/abs/2107.03230",
          "publishedOn": "2021-07-08T01:57:59.176Z",
          "wordCount": 752,
          "title": "Coastal water quality prediction based on machine learning with feature interpretation and spatio-temporal analysis. (arXiv:2107.03230v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03183",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Thommen_K/0/1/0/all/0/1\">Kaspar Thommen</a>",
          "description": "We derive the conjugate prior of the Dirichlet and beta distributions and\nexplore it with numerical examples to gain an intuitive understanding of the\ndistribution itself, its hyperparameters, and conditions concerning its\nconvergence. Due to the prior's intractability, we proceed to define and\nanalyze a closed-form approximation. Finally, we provide an algorithm\nimplementing this approximation that enables fully tractable Bayesian conjugate\ntreatment of Dirichlet and beta likelihoods without the need for Monte Carlo\nsimulations.",
          "link": "http://arxiv.org/abs/2107.03183",
          "publishedOn": "2021-07-08T01:57:59.167Z",
          "wordCount": 526,
          "title": "A Closed-Form Approximation to the Conjugate Prior of the Dirichlet and Beta Distributions. (arXiv:2107.03183v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00570",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Peng_T/0/1/0/all/0/1\">Tommy Peng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Malik_A/0/1/0/all/0/1\">Avinash Malik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bear_L/0/1/0/all/0/1\">Laura Bear</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Trew_M/0/1/0/all/0/1\">Mark L. Trew</a>",
          "description": "The proposed method re-frames traditional inverse problems of\nelectrocardiography into regression problems, constraining the solution space\nby decomposing signals with multidimensional Gaussian impulse basis functions.\nImpulse HSPs were generated with single Gaussian basis functions at discrete\nheart surface locations and projected to corresponding BSPs using a volume\nconductor torso model. Both BSP (inputs) and HSP (outputs) were mapped to\nregular 2D surface meshes and used to train a neural network. Predictive\ncapabilities of the network were tested with unseen synthetic and experimental\ndata. A dense full connected single hidden layer neural network was trained to\nmap body surface impulses to heart surface Gaussian basis functions for\nreconstructing HSP. Synthetic pulses moving across the heart surface were\npredicted from the neural network with root mean squared error of $9.1\\pm1.4$%.\nPredicted signals were robust to noise up to 20 dB and errors due to\ndisplacement and rotation of the heart within the torso were bounded and\npredictable. A shift of the heart 40 mm toward the spine resulted in a 4\\%\nincrease in signal feature localization error. The set of training impulse\nfunction data could be reduced and prediction error remained bounded. Recorded\nHSPs from in-vitro pig hearts were reliably decomposed using space-time\nGaussian basis functions. Predicted HSPs for left-ventricular pacing had a mean\nabsolute error of $10.4\\pm11.4$ ms. Other pacing scenarios were analyzed with\nsimilar success. Conclusion: Impulses from Gaussian basis functions are\npotentially an effective and robust way to train simple neural network data\nmodels for reconstructing HSPs from decomposed BSPs. The HSPs predicted by the\nneural network can be used to generate activation maps that non-invasively\nidentify features of cardiac electrical dysfunction and can guide subsequent\ntreatment options.",
          "link": "http://arxiv.org/abs/2102.00570",
          "publishedOn": "2021-07-08T01:57:59.159Z",
          "wordCount": 733,
          "title": "Impulse data models for the inverse problem of electrocardiography. (arXiv:2102.00570v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Timo M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindemann_B/0/1/0/all/0/1\">Benjamin Lindemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_T/0/1/0/all/0/1\">Tobias Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jazdi_N/0/1/0/all/0/1\">Nasser Jazdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyrich_M/0/1/0/all/0/1\">Michael Weyrich</a>",
          "description": "Shorter product life cycles and increasing individualization of production\nleads to an increased reconfiguration demand in the domain of industrial\nautomation systems, which will be dominated by cyber-physical production\nsystems in the future. In constantly changing systems, however, not all\nconfiguration alternatives of the almost infinite state space are fully\nunderstood. Thus, certain configurations can lead to process instability, a\nreduction in quality or machine failures. Therefore, this paper presents an\napproach that enhances an intelligent Digital Twin with a self-organized\nreconfiguration management based on adaptive process models in order to find\noptimized configurations more comprehensively.",
          "link": "http://arxiv.org/abs/2107.03324",
          "publishedOn": "2021-07-08T01:57:59.152Z",
          "wordCount": 564,
          "title": "Enhancing an Intelligent Digital Twin with a Self-organized Reconfiguration Management based on Adaptive Process Models. (arXiv:2107.03324v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2002.05426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leenings_R/0/1/0/all/0/1\">Ramona Leenings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winter_N/0/1/0/all/0/1\">Nils Ralf Winter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plagwitz_L/0/1/0/all/0/1\">Lucas Plagwitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holstein_V/0/1/0/all/0/1\">Vincent Holstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernsting_J/0/1/0/all/0/1\">Jan Ernsting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steenweg_J/0/1/0/all/0/1\">Jakob Steenweg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebker_J/0/1/0/all/0/1\">Julian Gebker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarink_K/0/1/0/all/0/1\">Kelvin Sarink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emden_D/0/1/0/all/0/1\">Daniel Emden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grotegerd_D/0/1/0/all/0/1\">Dominik Grotegerd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opel_N/0/1/0/all/0/1\">Nils Opel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risse_B/0/1/0/all/0/1\">Benjamin Risse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannlowski_U/0/1/0/all/0/1\">Udo Dannlowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_T/0/1/0/all/0/1\">Tim Hahn</a>",
          "description": "PHOTONAI is a high-level Python API designed to simplify and accelerate\nmachine learning model development. It functions as a unifying framework\nallowing the user to easily access and combine algorithms from different\ntoolboxes into custom algorithm sequences. It is especially designed to support\nthe iterative model development process and automates the repetitive training,\nhyperparameter optimization and evaluation tasks. Importantly, the workflow\nensures unbiased performance estimates while still allowing the user to fully\ncustomize the machine learning analysis. PHOTONAI extends existing solutions\nwith a novel pipeline implementation supporting more complex data streams,\nfeature combinations, and algorithm selection. Metrics and results can be\nconveniently visualized using the PHOTONAI Explorer and predictive models are\nshareable in a standardized format for further external validation or\napplication. A growing add-on ecosystem allows researchers to offer data\nmodality specific algorithms to the community and enhance machine learning in\nthe areas of the life sciences. Its practical utility is demonstrated on an\nexemplary medical machine learning problem, achieving a state-of-the-art\nsolution in few lines of code. Source code is publicly available on Github,\nwhile examples and documentation can be found at www.photon-ai.com.",
          "link": "http://arxiv.org/abs/2002.05426",
          "publishedOn": "2021-07-08T01:57:59.144Z",
          "wordCount": 705,
          "title": "PHOTONAI -- A Python API for Rapid Machine Learning Model Development. (arXiv:2002.05426v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tianze Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Y/0/1/0/all/0/1\">Yiping Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Sinno Jialin Pan</a>",
          "description": "Attributed event sequences are commonly encountered in practice. A recent\nresearch line focuses on incorporating neural networks with the statistical\nmodel -- marked point processes, which is the conventional tool for dealing\nwith attributed event sequences. Neural marked point processes possess good\ninterpretability of probabilistic models as well as the representational power\nof neural networks. However, we find that performance of neural marked point\nprocesses is not always increasing as the network architecture becomes more\ncomplicated and larger, which is what we call the performance saturation\nphenomenon. This is due to the fact that the generalization error of neural\nmarked point processes is determined by both the network representational\nability and the model specification at the same time. Therefore we can draw two\nmajor conclusions: first, simple network structures can perform no worse than\ncomplicated ones for some cases; second, using a proper probabilistic\nassumption is as equally, if not more, important as improving the complexity of\nthe network. Based on this observation, we propose a simple graph-based network\nstructure called GCHP, which utilizes only graph convolutional layers, thus it\ncan be easily accelerated by the parallel mechanism. We directly consider the\ndistribution of interarrival times instead of imposing a specific assumption on\nthe conditional intensity function, and propose to use a likelihood ratio loss\nwith a moment matching mechanism for optimization and model selection.\nExperimental results show that GCHP can significantly reduce training time and\nthe likelihood ratio loss with interarrival time probability assumptions can\ngreatly improve the model performance.",
          "link": "http://arxiv.org/abs/2107.03354",
          "publishedOn": "2021-07-08T01:57:59.125Z",
          "wordCount": 720,
          "title": "Mitigating Performance Saturation in Neural Marked Point Processes: Architectures and Loss Functions. (arXiv:2107.03354v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1908.04628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xindi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_O/0/1/0/all/0/1\">Onur Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eliassi_Rad_T/0/1/0/all/0/1\">Tina Eliassi-Rad</a>",
          "description": "Many real-world prediction tasks have outcome variables that have\ncharacteristic heavy-tail distributions. Examples include copies of books sold,\nauction prices of art pieces, demand for commodities in warehouses, etc. By\nlearning heavy-tailed distributions, \"big and rare\" instances (e.g., the\nbest-sellers) will have accurate predictions. Most existing approaches are not\ndedicated to learning heavy-tailed distribution; thus, they heavily\nunder-predict such instances. To tackle this problem, we introduce Learning to\nPlace (L2P), which exploits the pairwise relationships between instances for\nlearning. In its training phase, L2P learns a pairwise preference classifier:\nis instance A > instance B? In its placing phase, L2P obtains a prediction by\nplacing the new instance among the known instances. Based on its placement, the\nnew instance is then assigned a value for its outcome variable. Experiments on\nreal data show that L2P outperforms competing approaches in terms of accuracy\nand ability to reproduce heavy-tailed outcome distribution. In addition, L2P\nprovides an interpretable model by placing each predicted instance in relation\nto its comparable neighbors. Interpretable models are highly desirable when\nlives and treasure are at stake.",
          "link": "http://arxiv.org/abs/1908.04628",
          "publishedOn": "2021-07-08T01:57:59.118Z",
          "wordCount": 688,
          "title": "L2P: An Algorithm for Estimating Heavy-tailed Outcomes. (arXiv:1908.04628v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen-Yu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chung-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>",
          "description": "We study infinite-horizon discounted two-player zero-sum Markov games, and\ndevelop a decentralized algorithm that provably converges to the set of Nash\nequilibria under self-play. Our algorithm is based on running an Optimistic\nGradient Descent Ascent algorithm on each state to learn the policies, with a\ncritic that slowly learns the value of each state. To the best of our\nknowledge, this is the first algorithm in this setting that is simultaneously\nrational (converging to the opponent's best response when it uses a stationary\npolicy), convergent (converging to the set of Nash equilibria under self-play),\nagnostic (no need to know the actions played by the opponent), symmetric\n(players taking symmetric roles in the algorithm), and enjoying a finite-time\nlast-iterate convergence guarantee, all of which are desirable properties of\ndecentralized algorithms.",
          "link": "http://arxiv.org/abs/2102.04540",
          "publishedOn": "2021-07-08T01:57:59.111Z",
          "wordCount": 605,
          "title": "Last-iterate Convergence of Decentralized Optimistic Gradient Descent/Ascent in Infinite-horizon Competitive Markov Games. (arXiv:2102.04540v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07969",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rosca_M/0/1/0/all/0/1\">Mihaela Rosca</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Weber_T/0/1/0/all/0/1\">Theophane Weber</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1\">Arthur Gretton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mohamed_S/0/1/0/all/0/1\">Shakir Mohamed</a>",
          "description": "How sensitive should machine learning models be to input changes? We tackle\nthe question of model smoothness and show that it is a useful inductive bias\nwhich aids generalization, adversarial robustness, generative modeling and\nreinforcement learning. We explore current methods of imposing smoothness\nconstraints and observe they lack the flexibility to adapt to new tasks, they\ndon't account for data modalities, they interact with losses, architectures and\noptimization in ways not yet fully understood. We conclude that new advances in\nthe field are hinging on finding ways to incorporate data, tasks and learning\ninto our definitions of smoothness.",
          "link": "http://arxiv.org/abs/2012.07969",
          "publishedOn": "2021-07-08T01:57:59.089Z",
          "wordCount": 557,
          "title": "A case for new neural network smoothness constraints. (arXiv:2012.07969v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.07348",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Trosset_M/0/1/0/all/0/1\">Michael W. Trosset</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_M/0/1/0/all/0/1\">Mingyue Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tang_M/0/1/0/all/0/1\">Minh Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>",
          "description": "A random dot product graph (RDPG) is a generative model for networks in which\nvertices correspond to positions in a latent Euclidean space and edge\nprobabilities are determined by the dot products of the latent positions. We\nconsider RDPGs for which the latent positions are randomly sampled from an\nunknown $1$-dimensional submanifold of the latent space. In principle,\nrestricted inference, i.e., procedures that exploit the structure of the\nsubmanifold, should be more effective than unrestricted inference; however, it\nis not clear how to conduct restricted inference when the submanifold is\nunknown. We submit that techniques for manifold learning can be used to learn\nthe unknown submanifold well enough to realize benefit from restricted\ninference. To illustrate, we test $1$- and $2$-sample hypotheses about the\nFr\\'{e}chet means of small communities of vertices, using the complete set of\nvertices to infer latent structure. We propose test statistics that deploy the\nIsomap procedure for manifold learning, using shortest path distances on\nneighborhood graphs constructed from estimated latent positions to estimate arc\nlengths on the unknown $1$-dimensional submanifold. Unlike conventional\napplications of Isomap, the estimated latent positions do not lie on the\nsubmanifold of interest. We extend existing convergence results for Isomap to\nthis setting and use them to demonstrate that, as the number of auxiliary\nvertices increases, the power of our test converges to the power of the\ncorresponding test when the submanifold is known.",
          "link": "http://arxiv.org/abs/2004.07348",
          "publishedOn": "2021-07-08T01:57:59.082Z",
          "wordCount": 715,
          "title": "Learning 1-Dimensional Submanifolds for Subsequent Inference on Random Dot Product Graphs. (arXiv:2004.07348v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cleveston_I/0/1/0/all/0/1\">Iury Cleveston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombini_E/0/1/0/all/0/1\">Esther L. Colombini</a>",
          "description": "Building vehicles capable of operating without human supervision requires the\ndetermination of the agent's pose. Visual Odometry (VO) algorithms estimate the\negomotion using only visual changes from the input images. The most recent VO\nmethods implement deep-learning techniques using convolutional neural networks\n(CNN) extensively, which add a substantial cost when dealing with\nhigh-resolution images. Furthermore, in VO tasks, more input data does not mean\na better prediction; on the contrary, the architecture may filter out useless\ninformation. Therefore, the implementation of computationally efficient and\nlightweight architectures is essential. In this work, we propose the RAM-VO, an\nextension of the Recurrent Attention Model (RAM) for visual odometry tasks.\nRAM-VO improves the visual and temporal representation of information and\nimplements the Proximal Policy Optimization (PPO) algorithm to learn robust\npolicies. The results indicate that RAM-VO can perform regressions with six\ndegrees of freedom from monocular input images using approximately 3 million\nparameters. In addition, experiments on the KITTI dataset demonstrate that\nRAM-VO achieves competitive results using only 5.7% of the available visual\ninformation.",
          "link": "http://arxiv.org/abs/2107.02974",
          "publishedOn": "2021-07-08T01:57:59.075Z",
          "wordCount": 608,
          "title": "RAM-VO: Less is more in Visual Odometry. (arXiv:2107.02974v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1\">Georgios N. Yannakakis</a>",
          "description": "Search-based procedural content generation methods have recently been\nintroduced for the autonomous creation of bullet hell games. Search-based\nmethods, however, can hardly model patterns of danmakus -- the bullet hell\nshooting entity -- explicitly and the resulting levels often look\nnon-realistic. In this paper, we present a novel bullet hell game platform\nnamed Keiki, which allows the representation of danmakus as a parametric\nsequence which, in turn, can model the sequential behaviours of danmakus. We\nemploy three types of generative adversarial networks (GANs) and test Keiki\nacross three metrics designed to quantify the quality of the generated\ndanmakus. The time-series GAN and periodic spatial GAN show different yet\ncompetitive performance in terms of the evaluation metrics adopted, their\ndeviation from human-designed danmakus, and the diversity of generated\ndanmakus. The preliminary experimental studies presented here showcase that\npotential of time-series GANs for sequential content generation in games.",
          "link": "http://arxiv.org/abs/2107.02991",
          "publishedOn": "2021-07-08T01:57:59.068Z",
          "wordCount": 592,
          "title": "Keiki: Towards Realistic Danmaku Generation via Sequential GANs. (arXiv:2107.02991v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chia_P/0/1/0/all/0/1\">Patrick John Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bingqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>",
          "description": "Large eCommerce players introduced comparison tables as a new type of\nrecommendations. However, building comparisons at scale without pre-existing\ntraining/taxonomy data remains an open challenge, especially within the\noperational constraints of shops in the long tail. We present preliminary\nresults from building a comparison pipeline designed to scale in a multi-shop\nscenario: we describe our design choices and run extensive benchmarks on\nmultiple shops to stress-test it. Finally, we run a small user study on\nproperty selection and conclude by discussing potential improvements and\nhighlighting the questions that remain to be addressed.",
          "link": "http://arxiv.org/abs/2107.03256",
          "publishedOn": "2021-07-08T01:57:59.060Z",
          "wordCount": 534,
          "title": "\"Are you sure?\": Preliminary Insights from Scaling Product Comparisons to Multiple Shops. (arXiv:2107.03256v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/1910.10844",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Norton_M/0/1/0/all/0/1\">Matthew Norton</a>, <a href=\"http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1\">Johannes O. Royset</a>",
          "description": "The theoretical and empirical performance of Empirical Risk Minimization\n(ERM) often suffers when loss functions are poorly behaved with large Lipschitz\nmoduli and spurious sharp minimizers. We propose and analyze a counterpart to\nERM called Diametrical Risk Minimization (DRM), which accounts for worst-case\nempirical risks within neighborhoods in parameter space. DRM has generalization\nbounds that are independent of Lipschitz moduli for convex as well as nonconvex\nproblems and it can be implemented using a practical algorithm based on\nstochastic gradient descent. Numerical results illustrate the ability of DRM to\nfind quality solutions with low generalization error in sharp empirical risk\nlandscapes from benchmark neural network classification problems with corrupted\nlabels.",
          "link": "http://arxiv.org/abs/1910.10844",
          "publishedOn": "2021-07-08T01:57:59.053Z",
          "wordCount": 565,
          "title": "Diametrical Risk Minimization: Theory and Computations. (arXiv:1910.10844v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guillory_D/0/1/0/all/0/1\">Devin Guillory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1\">Vaishaal Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>",
          "description": "Recent work has shown that the performance of machine learning models can\nvary substantially when models are evaluated on data drawn from a distribution\nthat is close to but different from the training distribution. As a result,\npredicting model performance on unseen distributions is an important challenge.\nOur work connects techniques from domain adaptation and predictive uncertainty\nliterature, and allows us to predict model accuracy on challenging unseen\ndistributions without access to labeled data. In the context of distribution\nshift, distributional distances are often used to adapt models and improve\ntheir performance on new domains, however accuracy estimation, or other forms\nof predictive uncertainty, are often neglected in these investigations. Through\ninvestigating a wide range of established distributional distances, such as\nFrechet distance or Maximum Mean Discrepancy, we determine that they fail to\ninduce reliable estimates of performance under distribution shift. On the other\nhand, we find that the difference of confidences (DoC) of a classifier's\npredictions successfully estimates the classifier's performance change over a\nvariety of shifts. We specifically investigate the distinction between\nsynthetic and natural distribution shifts and observe that despite its\nsimplicity DoC consistently outperforms other quantifications of distributional\ndifference. $DoC$ reduces predictive error by almost half ($46\\%$) on several\nrealistic and challenging distribution shifts, e.g., on the ImageNet-Vid-Robust\nand ImageNet-Rendition datasets.",
          "link": "http://arxiv.org/abs/2107.03315",
          "publishedOn": "2021-07-08T01:57:59.047Z",
          "wordCount": 657,
          "title": "Predicting with Confidence on Unseen Distributions. (arXiv:2107.03315v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Alvin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1\">Ali Madani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_B/0/1/0/all/0/1\">Ben Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1\">Nikhil Naik</a>",
          "description": "Attribute extrapolation in sample generation is challenging for deep neural\nnetworks operating beyond the training distribution. We formulate a new task\nfor extrapolation in sequence generation, focusing on natural language and\nproteins, and propose GENhance, a generative framework that enhances attributes\nthrough a learned latent space. Trained on movie reviews and a computed protein\nstability dataset, GENhance can generate strongly-positive text reviews and\nhighly stable protein sequences without being exposed to similar data during\ntraining. We release our benchmark tasks and models to contribute to the study\nof generative modeling extrapolation and data-driven design in biology and\nchemistry.",
          "link": "http://arxiv.org/abs/2107.02968",
          "publishedOn": "2021-07-08T01:57:59.040Z",
          "wordCount": 532,
          "title": "Deep Extrapolation for Attribute-Enhanced Generation. (arXiv:2107.02968v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "Collaborative filtering (CF) is widely used to learn an informative latent\nrepresentation of a user or item from observed interactions. Existing CF-based\nmethods commonly adopt negative sampling to discriminate different items. That\nis, observed user-item pairs are treated as positive instances; unobserved\npairs are considered as negative instances and are sampled under a defined\ndistribution for training. Training with negative sampling on large datasets is\ncomputationally expensive. Further, negative items should be carefully sampled\nunder the defined distribution, in order to avoid selecting an observed\npositive item in the training dataset. Unavoidably, some negative items sampled\nfrom the training dataset could be positive in the test set. Recently,\nself-supervised learning (SSL) has emerged as a powerful tool to learn a model\nwithout negative samples. In this paper, we propose a self-supervised\ncollaborative filtering framework (SelfCF), that is specially designed for\nrecommender scenario with implicit feedback. The main idea of SelfCF is to\naugment the output embeddings generated by backbone networks, because it is\ninfeasible to augment raw input of user/item ids. We propose and study three\noutput perturbation techniques that can be applied to different types of\nbackbone networks including both traditional CF models and graph-based models.\nBy encapsulating two popular recommendation models into the framework, our\nexperiments on three datasets show that the best performance of our framework\nis comparable or better than the supervised counterpart. We also show that\nSelfCF can boost up the performance by up to 8.93\\% on average, compared with\nanother self-supervised framework as the baseline. Source codes are available\nat: https://github.com/enoche/SelfCF.",
          "link": "http://arxiv.org/abs/2107.03019",
          "publishedOn": "2021-07-08T01:57:59.034Z",
          "wordCount": 698,
          "title": "SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. (arXiv:2107.03019v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2103.02718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalin_J/0/1/0/all/0/1\">Josh Kalin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1\">Matthew Ciolino</a>",
          "description": "Machine learning models present a risk of adversarial attack when deployed in\nproduction. Quantifying the contributing factors and uncertainties using\nempirical measures could assist the industry with assessing the risk of\ndownloading and deploying common model types. This work proposes modifying the\ntraditional Drake Equation's formalism to estimate the number of potentially\nsuccessful adversarial attacks on a deployed model. The Drake Equation is\nfamously used for parameterizing uncertainties and it has been used in many\nresearch fields outside of its original intentions to estimate the number of\nradio-capable extra-terrestrial civilizations. While previous work has outlined\nmethods for discovering vulnerabilities in public model architectures, the\nproposed equation seeks to provide a semi-quantitative benchmark for evaluating\nand estimating the potential risk factors for adversarial attacks.",
          "link": "http://arxiv.org/abs/2103.02718",
          "publishedOn": "2021-07-08T01:57:58.989Z",
          "wordCount": 599,
          "title": "A Modified Drake Equation for Assessing Adversarial Risk to Machine Learning Models. (arXiv:2103.02718v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ankit Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_P/0/1/0/all/0/1\">Pritish Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craven_P/0/1/0/all/0/1\">Patrick Craven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_K/0/1/0/all/0/1\">Kevin Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oden_K/0/1/0/all/0/1\">Kevin Oden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Julie Shah</a>",
          "description": "When observing task demonstrations, human apprentices are able to identify\nwhether a given task is executed correctly long before they gain expertise in\nactually performing that task. Prior research into learning from demonstrations\n(LfD) has failed to capture this notion of the acceptability of a task's\nexecution; meanwhile, temporal logics provide a flexible language for\nexpressing task specifications. Inspired by this, we present Bayesian\nspecification inference, a probabilistic model for inferring task specification\nas a temporal logic formula. We incorporate methods from probabilistic\nprogramming to define our priors, along with a domain-independent likelihood\nfunction to enable sampling-based inference. We demonstrate the efficacy of our\nmodel for inferring specifications, with over 90% similarity observed between\nthe inferred specification and the ground truth, both within a synthetic domain\nand during a real-world table setting task.",
          "link": "http://arxiv.org/abs/2107.02912",
          "publishedOn": "2021-07-08T01:57:58.964Z",
          "wordCount": 572,
          "title": "Supervised Bayesian Specification Inference from Demonstrations. (arXiv:2107.02912v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2004.04120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galatolo_F/0/1/0/all/0/1\">Federico A. Galatolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cimino_M/0/1/0/all/0/1\">Mario G.C.A. Cimino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaglini_G/0/1/0/all/0/1\">Gigliola Vaglini</a>",
          "description": "In this research, some of the issues that arise from the scalarization of the\nmulti-objective optimization problem in the Advantage Actor Critic (A2C)\nreinforcement learning algorithm are investigated. The paper shows how a naive\nscalarization can lead to gradients overlapping. Furthermore, the possibility\nthat the entropy regularization term can be a source of uncontrolled noise is\ndiscussed. With respect to the above issues, a technique to avoid gradient\noverlapping is proposed, while keeping the same loss formulation. Moreover, a\nmethod to avoid the uncontrolled noise, by sampling the actions from\ndistributions with a desired minimum entropy, is investigated. Pilot\nexperiments have been carried out to show how the proposed method speeds up the\ntraining. The proposed approach can be applied to any Advantage-based\nReinforcement Learning algorithm.",
          "link": "http://arxiv.org/abs/2004.04120",
          "publishedOn": "2021-07-08T01:57:58.956Z",
          "wordCount": 606,
          "title": "Solving the scalarization issues of Advantage-based Reinforcement Learning Algorithms. (arXiv:2004.04120v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1\">Elias Frantar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtic_E/0/1/0/all/0/1\">Eldar Kurtic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>",
          "description": "Efficiently approximating local curvature information of the loss function is\na key tool for optimization and compression of deep neural networks. Yet, most\nexisting methods to approximate second-order information have high\ncomputational or storage costs, which can limit their practicality. In this\nwork, we investigate matrix-free, linear-time approaches for estimating\nInverse-Hessian Vector Products (IHVPs) for the case when the Hessian can be\napproximated as a sum of rank-one matrices, as in the classic approximation of\nthe Hessian by the empirical Fisher matrix. We propose two new algorithms as\npart of a framework called M-FAC: the first algorithm is tailored towards\nnetwork compression and can compute the IHVP for dimension $d$, if the Hessian\nis given as a sum of $m$ rank-one matrices, using $O(dm^2)$ precomputation,\n$O(dm)$ cost for computing the IHVP, and query cost $O(m)$ for any single\nelement of the inverse Hessian. The second algorithm targets an optimization\nsetting, where we wish to compute the product between the inverse Hessian,\nestimated over a sliding window of optimization steps, and a given gradient\ndirection, as required for preconditioned SGD. We give an algorithm with cost\n$O(dm + m^2)$ for computing the IHVP and $O(dm + m^3)$ for adding or removing\nany gradient from the sliding window. These two algorithms yield\nstate-of-the-art results for network pruning and optimization with lower\ncomputational overhead relative to existing second-order methods.\nImplementations are available at [10] and [18].",
          "link": "http://arxiv.org/abs/2107.03356",
          "publishedOn": "2021-07-08T01:57:58.937Z",
          "wordCount": 668,
          "title": "Efficient Matrix-Free Approximations of Second-Order Information, with Applications to Pruning and Optimization. (arXiv:2107.03356v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Letizia_N/0/1/0/all/0/1\">Nunzio A. Letizia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonello_A/0/1/0/all/0/1\">Andrea M. Tonello</a>",
          "description": "Channel capacity plays a crucial role in the development of modern\ncommunication systems as it represents the maximum rate at which information\ncan be reliably transmitted over a communication channel. Nevertheless, for the\nmajority of channels, finding a closed-form capacity expression remains an open\nchallenge. This is because it requires to carry out two formidable tasks a) the\ncomputation of the mutual information between the channel input and output, and\nb) its maximization with respect to the signal distribution at the channel\ninput. In this paper, we address both tasks. Inspired by implicit generative\nmodels, we propose a novel cooperative framework to automatically learn the\nchannel capacity, for any type of memory-less channel. In particular, we\nfirstly develop a new methodology to estimate the mutual information directly\nfrom a discriminator typically deployed to train adversarial networks, referred\nto as discriminative mutual information estimator (DIME). Secondly, we include\nthe discriminator in a cooperative channel capacity learning framework,\nreferred to as CORTICAL, where a discriminator learns to distinguish between\ndependent and independent channel input-output samples while a generator learns\nto produce the optimal channel input distribution for which the discriminator\nexhibits the best performance. Lastly, we prove that a particular choice of the\ncooperative value function solves the channel capacity estimation problem.\nSimulation results demonstrate that the proposed method offers high accuracy.",
          "link": "http://arxiv.org/abs/2107.03084",
          "publishedOn": "2021-07-08T01:57:58.909Z",
          "wordCount": 660,
          "title": "Discriminative Mutual Information Estimators for Channel Capacity Learning. (arXiv:2107.03084v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Huayi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1\">Xiaorong Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaofeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Ming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>",
          "description": "Multi-view clustering, a long-standing and important research problem,\nfocuses on mining complementary information from diverse views. However,\nexisting works often fuse multiple views' representations or handle clustering\nin a common feature space, which may result in their entanglement especially\nfor visual representations. To address this issue, we present a novel VAE-based\nmulti-view clustering framework (Multi-VAE) by learning disentangled visual\nrepresentations. Concretely, we define a view-common variable and multiple\nview-peculiar variables in the generative model. The prior of view-common\nvariable obeys approximately discrete Gumbel Softmax distribution, which is\nintroduced to extract the common cluster factor of multiple views. Meanwhile,\nthe prior of view-peculiar variable follows continuous Gaussian distribution,\nwhich is used to represent each view's peculiar visual factors. By controlling\nthe mutual information capacity to disentangle the view-common and\nview-peculiar representations, continuous visual information of multiple views\ncan be separated so that their common discrete cluster information can be\neffectively mined. Experimental results demonstrate that Multi-VAE enjoys the\ndisentangled and explainable visual representations, while obtaining superior\nclustering performance compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.11232",
          "publishedOn": "2021-07-08T01:57:58.646Z",
          "wordCount": 693,
          "title": "Multi-VAE: Learning Disentangled View-common and View-peculiar Visual Representations for Multi-view Clustering. (arXiv:2106.11232v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balzategui_J/0/1/0/all/0/1\">Julen Balzategui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eciolaza_L/0/1/0/all/0/1\">Luka Eciolaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maestro_Watson_D/0/1/0/all/0/1\">Daniel Maestro-Watson</a>",
          "description": "Quality inspection applications in industry are required to move towards a\nzero-defect manufacturing scenario, withnon-destructive inspection and\ntraceability of 100 % of produced parts. Developing robust fault detection and\nclassification modelsfrom the start-up of the lines is challenging due to the\ndifficulty in getting enough representative samples of the faulty patternsand\nthe need to manually label them. This work presents a methodology to develop a\nrobust inspection system, targeting thesepeculiarities, in the context of solar\ncell manufacturing. The methodology is divided into two phases: In the first\nphase, an anomalydetection model based on a Generative Adversarial Network\n(GAN) is employed. This model enables the detection and localizationof\nanomalous patterns within the solar cells from the beginning, using only\nnon-defective samples for training and without anymanual labeling involved. In\na second stage, as defective samples arise, the detected anomalies will be used\nas automaticallygenerated annotations for the supervised training of a Fully\nConvolutional Network that is capable of detecting multiple types offaults. The\nexperimental results using 1873 EL images of monocrystalline cells show that\n(a) the anomaly detection scheme can beused to start detecting features with\nvery little available data, (b) the anomaly detection may serve as automatic\nlabeling in order totrain a supervised model, and (c) segmentation and\nclassification results of supervised models trained with automatic labels\narecomparable to the ones obtained from the models trained with manual labels.",
          "link": "http://arxiv.org/abs/2103.03518",
          "publishedOn": "2021-07-08T01:57:58.583Z",
          "wordCount": 762,
          "title": "Anomaly detection and automatic labeling for solar cell quality inspection based on Generative Adversarial Network. (arXiv:2103.03518v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10316",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_W/0/1/0/all/0/1\">Weijun Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+LU_F/0/1/0/all/0/1\">Fengyuan LU</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+LI_E/0/1/0/all/0/1\">En LI</a>",
          "description": "How to accurately classify and diagnose whether an individual has Coronary\nStenosis (CS) without invasive physical examination? This problem has not been\nsolved satisfactorily. To this end, the four machine learning (ML) algorithms,\ni.e., Boosted Tree (BT), Decision Tree (DT), Logistic Regression (LR) and\nRandom Forest (RF) are employed in this paper. First, eleven features including\nbasic information of an individual, symptoms and results of routine physical\nexamination are selected, as well as one label is specified, indicating whether\nan individual suffers from different severity of coronary artery stenosis or\nnot. On the basis of it, a sample set is constructed. Second, each of these\nfour ML algorithms learns from the sample set to obtain the corresponding\noptimal classified results, respectively. The experimental results show that:\nRF performs better than other three algorithms, and the former algorithm\nclassifies whether an individual has CS with an accuracy of 95.7% (=90/94).",
          "link": "http://arxiv.org/abs/2007.10316",
          "publishedOn": "2021-07-08T01:57:58.575Z",
          "wordCount": 624,
          "title": "Auxiliary Diagnosing Coronary Stenosis Using Machine Learning. (arXiv:2007.10316v3 [q-bio.TO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06538",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rubio_Herrero_J/0/1/0/all/0/1\">Javier Rubio-Herrero</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Marrero_C/0/1/0/all/0/1\">Carlos Ortiz Marrero</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fan_W/0/1/0/all/0/1\">Wai-Tong Louis Fan</a>",
          "description": "Atmospheric modeling has recently experienced a surge with the advent of deep\nlearning. Most of these models, however, predict concentrations of pollutants\nfollowing a data-driven approach in which the physical laws that govern their\nbehaviors and relationships remain hidden. With the aid of real-world air\nquality data collected hourly in different stations throughout Madrid, we\npresent an empirical approach using data-driven techniques with the following\ngoals: (1) Find parsimonious systems of ordinary differential equations via\nsparse identification of nonlinear dynamics (SINDy) that model the\nconcentration of pollutants and their changes over time; (2) assess the\nperformance and limitations of our models using stability analysis; (3)\nreconstruct the time series of chemical pollutants not measured in certain\nstations using delay coordinate embedding results. Our results show that\nAkaike's Information Criterion can work well in conjunction with best subset\nregression as to find an equilibrium between sparsity and goodness of fit. We\nalso find that, due to the complexity of the chemical system under study,\nidentifying the dynamics of this system over longer periods of time require\nhigher levels of data filtering and smoothing. Stability analysis for the\nreconstructed ordinary differential equations (ODEs) reveals that more than\nhalf of the physically relevant critical points are saddle points, suggesting\nthat the system is unstable even under the idealized assumption that all\nenvironmental conditions are constant over time.",
          "link": "http://arxiv.org/abs/2010.06538",
          "publishedOn": "2021-07-08T01:57:58.548Z",
          "wordCount": 695,
          "title": "Modeling Atmospheric Data and Identifying Dynamics: Temporal Data-Driven Modeling of Air Pollutants. (arXiv:2010.06538v2 [stat.AP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_N/0/1/0/all/0/1\">Nicolo Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>",
          "description": "We propose a new gradient-based approach for extracting sub-architectures\nfrom a given large model. Contrarily to existing pruning methods, which are\nunable to disentangle the network architecture and the corresponding weights,\nour architecture-pruning scheme produces transferable new structures that can\nbe successfully retrained to solve different tasks. We focus on a\ntransfer-learning setup where architectures can be trained on a large data set\nbut very few data points are available for fine-tuning them on new tasks. We\ndefine a new gradient-based algorithm that trains architectures of arbitrarily\nlow complexity independently from the attached weights. Given a search space\ndefined by an existing large neural model, we reformulate the architecture\nsearch task as a complexity-penalized subset-selection problem and solve it\nthrough a two-temperature relaxation scheme. We provide theoretical convergence\nguarantees and validate the proposed transfer-learning strategy on real data.",
          "link": "http://arxiv.org/abs/2107.03375",
          "publishedOn": "2021-07-08T01:57:58.541Z",
          "wordCount": 590,
          "title": "Differentiable Architecture Pruning for Transfer Learning. (arXiv:2107.03375v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandle_S/0/1/0/all/0/1\">Sebastian Br&#xe4;ndle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanussek_M/0/1/0/all/0/1\">Marc Hanussek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blohm_M/0/1/0/all/0/1\">Matthias Blohm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1\">Maximilien Kintz</a>",
          "description": "Automated Machine Learning (AutoML) has gained increasing success on tabular\ndata in recent years. However, processing unstructured data like text is a\nchallenge and not widely supported by open-source AutoML tools. This work\ncompares three manually created text representations and text embeddings\nautomatically created by AutoML tools. Our benchmark includes four popular\nopen-source AutoML tools and eight datasets for text classification purposes.\nThe results show that straightforward text representations perform better than\nAutoML tools with automatically created text embeddings.",
          "link": "http://arxiv.org/abs/2106.12798",
          "publishedOn": "2021-07-08T01:57:58.535Z",
          "wordCount": 552,
          "title": "Evaluation of Representation Models for Text Classification with AutoML Tools. (arXiv:2106.12798v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03322",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunzhang Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_R/0/1/0/all/0/1\">Renxiong Liu</a>",
          "description": "We establish an equivalence between the $\\ell_2$-regularized solution path\nfor a convex loss function, and the solution of an ordinary differentiable\nequation (ODE). Importantly, this equivalence reveals that the solution path\ncan be viewed as the flow of a hybrid of gradient descent and Newton method\napplying to the empirical loss, which is similar to a widely used optimization\ntechnique called trust region method. This provides an interesting algorithmic\nview of $\\ell_2$ regularization, and is in contrast to the conventional view\nthat the $\\ell_2$ regularization solution path is similar to the gradient flow\nof the empirical loss.New path-following algorithms based on homotopy methods\nand numerical ODE solvers are proposed to numerically approximate the solution\npath. In particular, we consider respectively Newton method and gradient\ndescent method as the basis algorithm for the homotopy method, and establish\ntheir approximation error rates over the solution path. Importantly, our theory\nsuggests novel schemes to choose grid points that guarantee an arbitrarily\nsmall suboptimality for the solution path. In terms of computational cost, we\nprove that in order to achieve an $\\epsilon$-suboptimality for the entire\nsolution path, the number of Newton steps required for the Newton method is\n$\\mathcal O(\\epsilon^{-1/2})$, while the number of gradient steps required for\nthe gradient descent method is $\\mathcal O\\left(\\epsilon^{-1}\n\\ln(\\epsilon^{-1})\\right)$. Finally, we use $\\ell_2$-regularized logistic\nregression as an illustrating example to demonstrate the effectiveness of the\nproposed path-following algorithms.",
          "link": "http://arxiv.org/abs/2107.03322",
          "publishedOn": "2021-07-08T01:57:58.527Z",
          "wordCount": 672,
          "title": "An algorithmic view of $\\ell_2$ regularization and some path-following algorithms. (arXiv:2107.03322v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalra_B/0/1/0/all/0/1\">Bhavya Kalra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1\">Kulin Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manwani_N/0/1/0/all/0/1\">Naresh Manwani</a>",
          "description": "In this paper, we propose deep architectures for learning instance specific\nabstain (reject option) binary classifiers. The proposed approach uses double\nsigmoid loss function as described by Kulin Shah and Naresh Manwani in (\"Online\nActive Learning of Reject Option Classifiers\", AAAI, 2020), as a performance\nmeasure. We show that the double sigmoid loss is classification calibrated. We\nalso show that the excess risk of 0-d-1 loss is upper bounded by the excess\nrisk of double sigmoid loss. We derive the generalization error bounds for the\nproposed architecture for reject option classifiers. To show the effectiveness\nof the proposed approach, we experiment with several real world datasets. We\nobserve that the proposed approach not only performs comparable to the\nstate-of-the-art approaches, it is also robust against label noise. We also\nprovide visualizations to observe the important features learned by the network\ncorresponding to the abstaining decision.",
          "link": "http://arxiv.org/abs/2107.03090",
          "publishedOn": "2021-07-08T01:57:58.520Z",
          "wordCount": 573,
          "title": "RISAN: Robust Instance Specific Abstention Network. (arXiv:2107.03090v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03217",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Meng_Q/0/1/0/all/0/1\">Qun Meng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Songhao Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ng_S/0/1/0/all/0/1\">Szu Hui Ng</a>",
          "description": "Gaussian process (GP) model based optimization is widely applied in\nsimulation and machine learning. In general, it first estimates a GP model\nbased on a few observations from the true response and then employs this model\nto guide the search, aiming to quickly locate the global optimum. Despite its\nsuccessful applications, it has several limitations that may hinder its broader\nusage. First, building an accurate GP model can be difficult and\ncomputationally expensive, especially when the response function is multi-modal\nor varies significantly over the design space. Second, even with an appropriate\nmodel, the search process can be trapped in suboptimal regions before moving to\nthe global optimum due to the excessive effort spent around the current best\nsolution. In this work, we adopt the Additive Global and Local GP (AGLGP) model\nin the optimization framework. The model is rooted in the inducing-points-based\nGP sparse approximations and is combined with independent local models in\ndifferent regions. With these properties, the AGLGP model is suitable for\nmulti-modal responses with relatively large data sizes. Based on this AGLGP\nmodel, we propose a Combined Global and Local search for Optimization (CGLO)\nalgorithm. It first divides the whole design space into disjoint local regions\nand identifies a promising region with the global model. Next, a local model in\nthe selected region is fit to guide detailed search within this region. The\nalgorithm then switches back to the global step when a good local solution is\nfound. The global and local natures of CGLO enable it to enjoy the benefits of\nboth global and local search to efficiently locate the global optimum.",
          "link": "http://arxiv.org/abs/2107.03217",
          "publishedOn": "2021-07-08T01:57:58.489Z",
          "wordCount": 706,
          "title": "Combined Global and Local Search for Optimization with Gaussian Process Models. (arXiv:2107.03217v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2002.05582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pezzotti_N/0/1/0/all/0/1\">Nicola Pezzotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>",
          "description": "In healthcare applications, predictive uncertainty has been used to assess\npredictive accuracy. In this paper, we demonstrate that predictive uncertainty\nestimated by the current methods does not highly correlate with prediction\nerror by decomposing the latter into random and systematic errors, and showing\nthat the former is equivalent to the variance of the random error. In addition,\nwe observe that current methods unnecessarily compromise performance by\nmodifying the model and training loss to estimate the target and uncertainty\njointly. We show that estimating them separately without modifications improves\nperformance. Following this, we propose a novel method that estimates the\ntarget labels and magnitude of the prediction error in two steps. We\ndemonstrate this method on a large-scale MRI reconstruction task, and achieve\nsignificantly better results than the state-of-the-art uncertainty estimation\nmethods.",
          "link": "http://arxiv.org/abs/2002.05582",
          "publishedOn": "2021-07-08T01:57:58.483Z",
          "wordCount": 605,
          "title": "Learning to Predict Error for MRI Reconstruction. (arXiv:2002.05582v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Junior_C/0/1/0/all/0/1\">Celso A. M. Lopes Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_R/0/1/0/all/0/1\">Ricardo B. das Neves Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bezerra_B/0/1/0/all/0/1\">Byron L. D. Bezerra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toselli_A/0/1/0/all/0/1\">Alejandro H. Toselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Impedovo_D/0/1/0/all/0/1\">Donato Impedovo</a>",
          "description": "This paper describes the short-term competition on Components Segmentation\nTask of Document Photos that was prepared in the context of the 16th\nInternational Conference on Document Analysis and Recognition (ICDAR 2021).\nThis competition aims to bring together researchers working on the filed of\nidentification document image processing and provides them a suitable benchmark\nto compare their techniques on the component segmentation task of document\nimages. Three challenge tasks were proposed entailing different segmentation\nassignments to be performed on a provided dataset. The collected data are from\nseveral types of Brazilian ID documents, whose personal information was\nconveniently replaced. There were 16 participants whose results obtained for\nsome or all the three tasks show different rates for the adopted metrics, like\nDice Similarity Coefficient ranging from 0.06 to 0.99. Different Deep Learning\nmodels were applied by the entrants with diverse strategies to achieve the best\nresults in each of the tasks. Obtained results show that the current applied\nmethods for solving one of the proposed tasks (document boundary detection) are\nalready well stablished. However, for the other two challenge tasks (text zone\nand handwritten sign detection) research and development of more robust\napproaches are still required to achieve acceptable results.",
          "link": "http://arxiv.org/abs/2106.08499",
          "publishedOn": "2021-07-08T01:57:58.470Z",
          "wordCount": 676,
          "title": "ICDAR 2021 Competition on Components Segmentation Task of Document Photos. (arXiv:2106.08499v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03299",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Barlas_A/0/1/0/all/0/1\">Ali B. Barlas</a> (BBVA Research), <a href=\"http://arxiv.org/find/econ/1/au:+Mert_S/0/1/0/all/0/1\">Seda Guler Mert</a> (BBVA Research), <a href=\"http://arxiv.org/find/econ/1/au:+Isa_B/0/1/0/all/0/1\">Berk Orkun Isa</a> (BBVA Research) <a href=\"http://arxiv.org/find/econ/1/au:+Ortiz_A/0/1/0/all/0/1\">Alvaro Ortiz</a> (BBVA Research), <a href=\"http://arxiv.org/find/econ/1/au:+Rodrigo_T/0/1/0/all/0/1\">Tomasa Rodrigo</a> (BBVA Research), <a href=\"http://arxiv.org/find/econ/1/au:+Soybilgen_B/0/1/0/all/0/1\">Baris Soybilgen</a> (Bilgi University), <a href=\"http://arxiv.org/find/econ/1/au:+Yazgan_E/0/1/0/all/0/1\">Ege Yazgan</a> (Bilgi University)",
          "description": "We use the aggregate information from individual-to-firm and firm-to-firm in\nGaranti BBVA Bank transactions to mimic domestic private demand. Particularly,\nwe replicate the quarterly national accounts aggregate consumption and\ninvestment (gross fixed capital formation) and its bigger components (Machinery\nand Equipment and Construction) in real time for the case of Turkey. In order\nto validate the usefulness of the information derived from these indicators we\ntest the nowcasting ability of both indicators to nowcast the Turkish GDP using\ndifferent nowcasting models. The results are successful and confirm the\nusefulness of Consumption and Investment Banking transactions for nowcasting\npurposes. The value of the Big data information is more relevant at the\nbeginning of the nowcasting process, when the traditional hard data information\nis scarce. This makes this information specially relevant for those countries\nwhere statistical release lags are longer like the Emerging Markets.",
          "link": "http://arxiv.org/abs/2107.03299",
          "publishedOn": "2021-07-08T01:57:58.464Z",
          "wordCount": 617,
          "title": "Big Data Information and Nowcasting: Consumption and Investment from Bank Transactions in Turkey. (arXiv:2107.03299v1 [econ.EM])"
        },
        {
          "id": "http://arxiv.org/abs/2104.09343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lesage_Landry_A/0/1/0/all/0/1\">Antoine Lesage-Landry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callaway_D/0/1/0/all/0/1\">Duncan S. Callaway</a>",
          "description": "We formulate an efficient approximation for multi-agent batch reinforcement\nlearning, the approximate multi-agent fitted Q iteration (AMAFQI). We present a\ndetailed derivation of our approach. We propose an iterative policy search and\nshow that it yields a greedy policy with respect to multiple approximations of\nthe centralized, standard Q-function. In each iteration and policy evaluation,\nAMAFQI requires a number of computations that scales linearly with the number\nof agents whereas the analogous number of computations increase exponentially\nfor the fitted Q iteration (FQI), one of the most commonly used approaches in\nbatch reinforcement learning. This property of AMAFQI is fundamental for the\ndesign of a tractable multi-agent approach. We evaluate the performance of\nAMAFQI and compare it to FQI in numerical simulations. Numerical examples\nillustrate the significant computation time reduction when using AMAFQI instead\nof FQI in multi-agent problems and corroborate the similar decision-making\nperformance of both approaches.",
          "link": "http://arxiv.org/abs/2104.09343",
          "publishedOn": "2021-07-08T01:57:58.444Z",
          "wordCount": 604,
          "title": "Approximate Multi-Agent Fitted Q Iteration. (arXiv:2104.09343v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaoyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>",
          "description": "Deepfakes pose growing challenges to the trust of information on the\nInternet. Thus, detecting deepfakes has attracted increasing attentions from\nboth academia and industry. State-of-the-art deepfake detection methods consist\nof two key components, i.e., face extractor and face classifier, which extract\nthe face region in an image and classify it to be real/fake, respectively.\nExisting studies mainly focused on improving the detection performance in\nnon-adversarial settings, leaving security of deepfake detection in adversarial\nsettings largely unexplored. In this work, we aim to bridge the gap. In\nparticular, we perform a systematic measurement study to understand the\nsecurity of the state-of-the-art deepfake detection methods in adversarial\nsettings. We use two large-scale public deepfakes data sources including\nFaceForensics++ and Facebook Deepfake Detection Challenge, where the deepfakes\nare fake face images; and we train state-of-the-art deepfake detection methods.\nThese detection methods can achieve 0.94--0.99 accuracies in non-adversarial\nsettings on these datasets. However, our measurement results uncover multiple\nsecurity limitations of the deepfake detection methods in adversarial settings.\nFirst, we find that an attacker can evade a face extractor, i.e., the face\nextractor fails to extract the correct face regions, via adding small Gaussian\nnoise to its deepfake images. Second, we find that a face classifier trained\nusing deepfakes generated by one method cannot detect deepfakes generated by\nanother method, i.e., an attacker can evade detection via generating deepfakes\nusing a new method. Third, we find that an attacker can leverage backdoor\nattacks developed by the adversarial machine learning community to evade a face\nclassifier. Our results highlight that deepfake detection should consider the\nadversarial nature of the problem.",
          "link": "http://arxiv.org/abs/2107.02045",
          "publishedOn": "2021-07-08T01:57:58.438Z",
          "wordCount": 724,
          "title": "Understanding the Security of Deepfake Detection. (arXiv:2107.02045v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.06684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galatolo_F/0/1/0/all/0/1\">Federico A. Galatolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cimino_M/0/1/0/all/0/1\">Mario G.C.A. Cimino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaglini_G/0/1/0/all/0/1\">Gigliola Vaglini</a>",
          "description": "This paper proposes the Mesh Neural Network (MNN), a novel architecture which\nallows neurons to be connected in any topology, to efficiently route\ninformation. In MNNs, information is propagated between neurons throughout a\nstate transition function. State and error gradients are then directly computed\nfrom state updates without backward computation. The MNN architecture and the\nerror propagation schema is formalized and derived in tensor algebra. The\nproposed computational model can fully supply a gradient descent process, and\nis potentially suitable for very large scale sparse NNs, due to its\nexpressivity and training efficiency, with respect to NNs based on\nback-propagation and computational graphs.",
          "link": "http://arxiv.org/abs/1905.06684",
          "publishedOn": "2021-07-08T01:57:58.431Z",
          "wordCount": 615,
          "title": "Formal derivation of Mesh Neural Networks with their Forward-Only gradient Propagation. (arXiv:1905.06684v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14694",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_O/0/1/0/all/0/1\">Omair Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Saif_ur_Rehman_M/0/1/0/all/0/1\">Muhammad Saif-ur-Rehman</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dyck_S/0/1/0/all/0/1\">Susanne Dyck</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Glasmachers_T/0/1/0/all/0/1\">Tobias Glasmachers</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Iossifidis_I/0/1/0/all/0/1\">Ioannis Iossifidis</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Klaes_C/0/1/0/all/0/1\">Christian Klaes</a>",
          "description": "Brain-computer interfaces (BCIs) enable direct communication between humans\nand machines by translating brain activity into control commands. EEG is one of\nthe most common sources of neural signals because of its inexpensive and\nnon-invasive nature. However, interpretation of EEG signals is non-trivial\nbecause EEG signals have a low spatial resolution and are often distorted with\nnoise and artifacts. Therefore, it is possible that meaningful patterns for\nclassifying EEG signals are deeply hidden. Nowadays, state-of-the-art\ndeep-learning algorithms have proven to be quite efficient in learning hidden,\nmeaningful patterns. However, the performance of the deep learning algorithms\ndepends upon the quality and the amount of the provided training data. Hence, a\nbetter input formation (feature extraction) technique and a generative model to\nproduce high-quality data can enable the deep learning algorithms to adapt high\ngeneralization quality. In this study, we proposed a novel input formation\n(feature extraction) method in conjunction with a novel deep learning based\ngenerative model to harness new training examples. The feature vectors are\nextracted using a modified Short Time Fourier Transform (STFT) called\nanchored-STFT. Anchored-STFT, inspired by wavelet transform, tries to minimize\nthe tradeoff between time and frequency resolution. As a result, it extracts\nthe inputs (feature vectors) with better time and frequency resolution compared\nto the standard STFT. Secondly, we introduced a novel generative adversarial\ndata augmentation technique called gradient norm adversarial augmentation\n(GNAA) for generating more training data. Thirdly, we investigated the\nexistence and significance of adversarial inputs in EEG data. Our approach\nobtained the kappa value of 0.814 for BCI competition II dataset III and 0.755\nfor BCI competition IV dataset 2b for session-to-session transfer on test data.",
          "link": "http://arxiv.org/abs/2011.14694",
          "publishedOn": "2021-07-08T01:57:58.422Z",
          "wordCount": 757,
          "title": "Improving the performance of EEG decoding using anchored-STFT in conjunction with gradient norm adversarial augmentation. (arXiv:2011.14694v3 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02847",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_S/0/1/0/all/0/1\">Shaohan Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sahinidis_N/0/1/0/all/0/1\">Nikolaos V. Sahinidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_C/0/1/0/all/0/1\">Chuanhou Gao</a>",
          "description": "This paper investigates the effectiveness of transfer learning based on\nMallows' Cp. We propose a procedure that combines transfer learning with\nMallows' Cp (TLCp) and prove that it outperforms the conventional Mallows' Cp\ncriterion in terms of accuracy and stability. Our theoretical results indicate\nthat, for any sample size in the target domain, the proposed TLCp estimator\nperforms better than the Cp estimator by the mean squared error (MSE) metric in\nthe case of orthogonal predictors, provided that i) the dissimilarity between\nthe tasks from source domain and target domain is small, and ii) the procedure\nparameters (complexity penalties) are tuned according to certain explicit\nrules. Moreover, we show that our transfer learning framework can be extended\nto other feature selection criteria, such as the Bayesian information\ncriterion. By analyzing the solution of the orthogonalized Cp, we identify an\nestimator that asymptotically approximates the solution of the Cp criterion in\nthe case of non-orthogonal predictors. Similar results are obtained for the\nnon-orthogonal TLCp. Finally, simulation studies and applications with real\ndata demonstrate the usefulness of the TLCp scheme.",
          "link": "http://arxiv.org/abs/2107.02847",
          "publishedOn": "2021-07-08T01:57:58.414Z",
          "wordCount": 613,
          "title": "Transfer Learning in Information Criteria-based Feature Selection. (arXiv:2107.02847v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02926",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Patel_D/0/1/0/all/0/1\">Dhruv V Patel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ray_D/0/1/0/all/0/1\">Deep Ray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oberai_A/0/1/0/all/0/1\">Assad A Oberai</a>",
          "description": "Inverse problems are notoriously difficult to solve because they can have no\nsolutions, multiple solutions, or have solutions that vary significantly in\nresponse to small perturbations in measurements. Bayesian inference, which\nposes an inverse problem as a stochastic inference problem, addresses these\ndifficulties and provides quantitative estimates of the inferred field and the\nassociated uncertainty. However, it is difficult to employ when inferring\nvectors of large dimensions, and/or when prior information is available through\npreviously acquired samples. In this paper, we describe how deep generative\nadversarial networks can be used to represent the prior distribution in\nBayesian inference and overcome these challenges. We apply these ideas to\ninverse problems that are diverse in terms of the governing physical\nprinciples, sources of prior knowledge, type of measurement, and the extent of\navailable information about measurement noise. In each case we apply the\nproposed approach to infer the most likely solution and quantitative estimates\nof uncertainty.",
          "link": "http://arxiv.org/abs/2107.02926",
          "publishedOn": "2021-07-08T01:57:58.392Z",
          "wordCount": 604,
          "title": "Solution of Physics-based Bayesian Inverse Problems with Deep Generative Priors. (arXiv:2107.02926v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilder_B/0/1/0/all/0/1\">Bryan Wilder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suen_S/0/1/0/all/0/1\">Sze-chuan Suen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1\">Bistra Dilkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1\">Milind Tambe</a>",
          "description": "There is significant interest in learning and optimizing a complex system\ncomposed of multiple sub-components, where these components may be agents or\nautonomous sensors. Among the rich literature on this topic, agent-based and\ndomain-specific simulations can capture complex dynamics and subgroup\ninteraction, but optimizing over such simulations can be computationally and\nalgorithmically challenging. Bayesian approaches, such as Gaussian processes\n(GPs), can be used to learn a computationally tractable approximation to the\nunderlying dynamics but typically neglect the detailed information about\nsubgroups in the complicated system. We attempt to find the best of both worlds\nby proposing the idea of decomposed feedback, which captures group-based\nheterogeneity and dynamics. We introduce a novel decomposed GP regression to\nincorporate the subgroup decomposed feedback. Our modified regression has\nprovably lower variance -- and thus a more accurate posterior -- compared to\nprevious approaches; it also allows us to introduce a decomposed GP-UCB\noptimization algorithm that leverages subgroup feedback. The Bayesian nature of\nour method makes the optimization algorithm trackable with a theoretical\nguarantee on convergence and no-regret property. To demonstrate the wide\napplicability of this work, we execute our algorithm on two disparate social\nproblems: infectious disease control in a heterogeneous population and\nallocation of distributed weather sensors. Experimental results show that our\nnew method provides significant improvement compared to the state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.03003",
          "publishedOn": "2021-07-08T01:57:58.378Z",
          "wordCount": 660,
          "title": "Harnessing Heterogeneity: Learning from Decomposed Feedback in Bayesian Modeling. (arXiv:2107.03003v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trask_N/0/1/0/all/0/1\">Nat Trask</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulian_M/0/1/0/all/0/1\">Mamikon Gulian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1\">Andy Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kookjin Lee</a>",
          "description": "Partition of unity networks (POU-Nets) have been shown capable of realizing\nalgebraic convergence rates for regression and solution of PDEs, but require\nempirical tuning of training parameters. We enrich POU-Nets with a Gaussian\nnoise model to obtain a probabilistic generalization amenable to gradient-based\nminimization of a maximum likelihood loss. The resulting architecture provides\nspatial representations of both noiseless and noisy data as Gaussian mixtures\nwith closed form expressions for variance which provides an estimator of local\nerror. The training process yields remarkably sharp partitions of input space\nbased upon correlation of function values. This classification of training\npoints is amenable to a hierarchical refinement strategy that significantly\nimproves the localization of the regression, allowing for higher-order\npolynomial approximation to be utilized. The framework scales more favorably to\nlarge data sets as compared to Gaussian process regression and allows for\nspatially varying uncertainty, leveraging the expressive power of deep neural\nnetworks while bypassing expensive training associated with other probabilistic\ndeep learning methods. Compared to standard deep neural networks, the framework\ndemonstrates hp-convergence without the use of regularizers to tune the\nlocalization of partitions. We provide benchmarks quantifying performance in\nhigh/low-dimensions, demonstrating that convergence rates depend only on the\nlatent dimension of data within high-dimensional space. Finally, we introduce a\nnew open-source data set of PDE-based simulations of a semiconductor device and\nperform unsupervised extraction of a physically interpretable reduced-order\nbasis.",
          "link": "http://arxiv.org/abs/2107.03066",
          "publishedOn": "2021-07-08T01:57:58.371Z",
          "wordCount": 670,
          "title": "Probabilistic partition of unity networks: clustering based deep approximation. (arXiv:2107.03066v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_B/0/1/0/all/0/1\">Bowei Xi</a>",
          "description": "We provide a comprehensive overview of adversarial machine learning focusing\non two application domains, i.e., cybersecurity and computer vision. Research\nin adversarial machine learning addresses a significant threat to the wide\napplication of machine learning techniques -- they are vulnerable to carefully\ncrafted attacks from malicious adversaries. For example, deep neural networks\nfail to correctly classify adversarial images, which are generated by adding\nimperceptible perturbations to clean images.We first discuss three main\ncategories of attacks against machine learning techniques -- poisoning attacks,\nevasion attacks, and privacy attacks. Then the corresponding defense approaches\nare introduced along with the weakness and limitations of the existing defense\napproaches. We notice adversarial samples in cybersecurity and computer vision\nare fundamentally different. While adversarial samples in cybersecurity often\nhave different properties/distributions compared with training data,\nadversarial images in computer vision are created with minor input\nperturbations. This further complicates the development of robust learning\ntechniques, because a robust learning technique must withstand different types\nof attacks.",
          "link": "http://arxiv.org/abs/2107.02894",
          "publishedOn": "2021-07-08T01:57:58.357Z",
          "wordCount": 617,
          "title": "Adversarial Machine Learning for Cybersecurity and Computer Vision: Current Developments and Challenges. (arXiv:2107.02894v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gotovos_A/0/1/0/all/0/1\">Alkis Gotovos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burkholz_R/0/1/0/all/0/1\">Rebekka Burkholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quackenbush_J/0/1/0/all/0/1\">John Quackenbush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1\">Stefanie Jegelka</a>",
          "description": "Modeling the time evolution of discrete sets of items (e.g., genetic\nmutations) is a fundamental problem in many biomedical applications. We\napproach this problem through the lens of continuous-time Markov chains, and\nshow that the resulting learning task is generally underspecified in the usual\nsetting of cross-sectional data. We explore a perhaps surprising remedy:\nincluding a number of additional independent items can help determine time\norder, and hence resolve underspecification. This is in sharp contrast to the\ncommon practice of limiting the analysis to a small subset of relevant items,\nwhich is followed largely due to poor scaling of existing methods. To put our\ntheoretical insight into practice, we develop an approximate likelihood\nmaximization method for learning continuous-time Markov chains, which can scale\nto hundreds of items and is orders of magnitude faster than previous methods.\nWe demonstrate the effectiveness of our approach on synthetic and real cancer\ndata.",
          "link": "http://arxiv.org/abs/2107.02911",
          "publishedOn": "2021-07-08T01:57:58.338Z",
          "wordCount": 583,
          "title": "Scaling up Continuous-Time Markov Chains Helps Resolve Underspecification. (arXiv:2107.02911v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayyeri_M/0/1/0/all/0/1\">Mojtaba Nayyeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cil_G/0/1/0/all/0/1\">Gokce Muge Cil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdati_S/0/1/0/all/0/1\">Sahar Vahdati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osborne_F/0/1/0/all/0/1\">Francesco Osborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mahfuzur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angioni_S/0/1/0/all/0/1\">Simone Angioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salatino_A/0/1/0/all/0/1\">Angelo Salatino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1\">Diego Reforgiato Recupero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassilyeva_N/0/1/0/all/0/1\">Nadezhda Vassilyeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motta_E/0/1/0/all/0/1\">Enrico Motta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>",
          "description": "The incompleteness of Knowledge Graphs (KGs) is a crucial issue affecting the\nquality of AI-based services. In the scholarly domain, KGs describing research\npublications typically lack important information, hindering our ability to\nanalyse and predict research dynamics. In recent years, link prediction\napproaches based on Knowledge Graph Embedding models became the first aid for\nthis issue. In this work, we present Trans4E, a novel embedding model that is\nparticularly fit for KGs which include N to M relations with N$\\gg$M. This is\ntypical for KGs that categorize a large number of entities (e.g., research\narticles, patents, persons) according to a relatively small set of categories.\nTrans4E was applied on two large-scale knowledge graphs, the Academia/Industry\nDynAmics (AIDA) and Microsoft Academic Graph (MAG), for completing the\ninformation about Fields of Study (e.g., 'neural networks', 'machine learning',\n'artificial intelligence'), and affiliation types (e.g., 'education',\n'company', 'government'), improving the scope and accuracy of the resulting\ndata. We evaluated our approach against alternative solutions on AIDA, MAG, and\nfour other benchmarks (FB15k, FB15k-237, WN18, and WN18RR). Trans4E outperforms\nthe other models when using low embedding dimensions and obtains competitive\nresults in high dimensions.",
          "link": "http://arxiv.org/abs/2107.03297",
          "publishedOn": "2021-07-08T01:57:58.325Z",
          "wordCount": 648,
          "title": "Trans4E: Link Prediction on Scholarly Knowledge Graphs. (arXiv:2107.03297v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Waters_E/0/1/0/all/0/1\">Emily Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oghaz_M/0/1/0/all/0/1\">Mahdi Maktabdar Oghaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saheer_L/0/1/0/all/0/1\">Lakshmi Babu Saheer</a>",
          "description": "Urban trees help regulate temperature, reduce energy consumption, improve\nurban air quality, reduce wind speeds, and mitigating the urban heat island\neffect. Urban trees also play a key role in climate change mitigation and\nglobal warming by capturing and storing atmospheric carbon-dioxide which is the\nlargest contributor to greenhouse gases. Automated tree detection and species\nclassification using aerial imagery can be a powerful tool for sustainable\nforest and urban tree management. Hence, This study first offers a pipeline for\ngenerating labelled dataset of urban trees using Google Map's aerial images and\nthen investigates how state of the art deep Convolutional Neural Network models\nsuch as VGG and ResNet handle the classification problem of urban tree aerial\nimages under different parameters. Experimental results show our best model\nachieves an average accuracy of 60% over 6 tree species.",
          "link": "http://arxiv.org/abs/2107.03182",
          "publishedOn": "2021-07-08T01:57:58.311Z",
          "wordCount": 594,
          "title": "Urban Tree Species Classification Using Aerial Imagery. (arXiv:2107.03182v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03131",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Elbir_A/0/1/0/all/0/1\">Ahmet M. Elbir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mishra_K/0/1/0/all/0/1\">Kumar Vijay Mishra</a>",
          "description": "Cognitive communications have emerged as a promising solution to enhance,\nadapt, and invent new tools and capabilities that transcend conventional\nwireless networks. Deep learning (DL) is critical in enabling essential\nfeatures of cognitive systems because of its fast prediction performance,\nadaptive behavior, and model-free structure. These features are especially\nsignificant for multi-antenna wireless communications systems, which generate\nand handle massive data. Multiple antennas may provide multiplexing, diversity,\nor antenna gains that, respectively, improve the capacity, bit error rate, or\nthe signal-to-interference-plus-noise ratio. In practice, multi-antenna\ncognitive communications encounter challenges in terms of data complexity and\ndiversity, hardware complexity, and wireless channel dynamics. The DL-based\nsolutions tackle these problems at the various stages of communications\nprocessing such as channel estimation, hybrid beamforming, user localization,\nand sparse array design. There are research opportunities to address\nsignificant design challenges arising from insufficient data coverage, learning\nmodel complexity, and data transmission overheads. This article provides\nsynopses of various DL-based methods to impart cognitive behavior to\nmulti-antenna wireless communications.",
          "link": "http://arxiv.org/abs/2010.03131",
          "publishedOn": "2021-07-08T01:57:58.292Z",
          "wordCount": 647,
          "title": "Cognitive Learning-Aided Multi-Antenna Communications. (arXiv:2010.03131v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathelin_A/0/1/0/all/0/1\">Antoine de Mathelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deheeger_F/0/1/0/all/0/1\">Fran&#xe7;ois Deheeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1\">Guillaume Richard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mougeot_M/0/1/0/all/0/1\">Mathilde Mougeot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vayatis_N/0/1/0/all/0/1\">Nicolas Vayatis</a>",
          "description": "ADAPT is an open-source python library providing the implementation of\nseveral domain adaptation methods. The library is suited for scikit-learn\nestimator object (object which implement fit and predict methods) and\ntensorflow models. Most of the implemented methods are developed in an\nestimator agnostic fashion, offering various possibilities adapted to multiple\nusage. The library offers three modules corresponding to the three principal\nstrategies of domain adaptation: (i) feature-based containing methods\nperforming feature transformation; (ii) instance-based with the implementation\nof reweighting techniques and (iii) parameter-based proposing methods to adapt\npre-trained models to novel observations. A full documentation is proposed\nonline https://adapt-python.github.io/adapt/ with gallery of examples. Besides,\nthe library presents an high test coverage.",
          "link": "http://arxiv.org/abs/2107.03049",
          "publishedOn": "2021-07-08T01:57:58.284Z",
          "wordCount": 550,
          "title": "ADAPT : Awesome Domain Adaptation Python Toolbox. (arXiv:2107.03049v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modiri_A/0/1/0/all/0/1\">Arghavan Modiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garnett_R/0/1/0/all/0/1\">Roman Garnett</a>",
          "description": "Active search is a learning paradigm where we seek to identify as many\nmembers of a rare, valuable class as possible given a labeling budget. Previous\nwork on active search has assumed access to a faithful (and expensive) oracle\nreporting experimental results. However, some settings offer access to cheaper\nsurrogates such as computational simulation that may aid in the search. We\npropose a model of multifidelity active search, as well as a novel,\ncomputationally efficient policy for this setting that is motivated by\nstate-of-the-art classical policies. Our policy is nonmyopic and budget aware,\nallowing for a dynamic tradeoff between exploration and exploitation. We\nevaluate the performance of our solution on real-world datasets and demonstrate\nsignificantly better performance than natural benchmarks.",
          "link": "http://arxiv.org/abs/2106.06356",
          "publishedOn": "2021-07-08T01:57:58.278Z",
          "wordCount": 572,
          "title": "Nonmyopic Multifidelity Active Search. (arXiv:2106.06356v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ernie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_H/0/1/0/all/0/1\">Hui-Syuan Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>",
          "description": "Large-scale pretrained language models have led to dramatic improvements in\ntext generation. Impressive performance can be achieved by finetuning only on a\nsmall number of instances (few-shot setting). Nonetheless, almost all previous\nwork simply applies random sampling to select the few-shot training instances.\nLittle to no attention has been paid to the selection strategies and how they\nwould affect model performance. In this work, we present a study on training\ninstance selection in few-shot neural text generation. The selection decision\nis made based only on the unlabeled data so as to identify the most worthwhile\ndata points that should be annotated under some budget of labeling cost. Based\non the intuition that the few-shot training instances should be diverse and\nrepresentative of the entire data distribution, we propose a simple selection\nstrategy with K-means clustering. We show that even with the naive\nclustering-based approach, the generation models consistently outperform random\nsampling on three text generation tasks: data-to-text generation, document\nsummarization and question generation. We hope that this work will call for\nmore attention on this largely unexplored area.",
          "link": "http://arxiv.org/abs/2107.03176",
          "publishedOn": "2021-07-08T01:57:58.271Z",
          "wordCount": 620,
          "title": "On Training Instance Selection for Few-Shot Neural Text Generation. (arXiv:2107.03176v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velasco_Mata_J/0/1/0/all/0/1\">Javier Velasco-Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Castro_V/0/1/0/all/0/1\">V&#xed;ctor Gonz&#xe1;lez-Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidalgo_E/0/1/0/all/0/1\">Eduardo Fidalgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alegre_E/0/1/0/all/0/1\">Enrique Alegre</a>",
          "description": "Botnets are one of the online threats with the biggest presence, causing\nbillionaire losses to global economies. Nowadays, the increasing number of\ndevices connected to the Internet makes it necessary to analyze large amounts\nof network traffic data. In this work, we focus on increasing the performance\non botnet traffic classification by selecting those features that further\nincrease the detection rate. For this purpose we use two feature selection\ntechniques, Information Gain and Gini Importance, which led to three\npre-selected subsets of five, six and seven features. Then, we evaluate the\nthree feature subsets along with three models, Decision Tree, Random Forest and\nk-Nearest Neighbors. To test the performance of the three feature vectors and\nthe three models we generate two datasets based on the CTU-13 dataset, namely\nQB-CTU13 and EQB-CTU13. We measure the performance as the macro averaged F1\nscore over the computational time required to classify a sample. The results\nshow that the highest performance is achieved by Decision Trees using a five\nfeature set which obtained a mean F1 score of 85% classifying each sample in an\naverage time of 0.78 microseconds.",
          "link": "http://arxiv.org/abs/2107.02896",
          "publishedOn": "2021-07-08T01:57:58.264Z",
          "wordCount": 631,
          "title": "Efficient Detection of Botnet Traffic by features selection and Decision Trees. (arXiv:2107.02896v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02821",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kasieczka_G/0/1/0/all/0/1\">Gregor Kasieczka</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nachman_B/0/1/0/all/0/1\">Benjamin Nachman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shih_D/0/1/0/all/0/1\">David Shih</a>",
          "description": "The identification of anomalous overdensities in data - group or collective\nanomaly detection - is a rich problem with a large number of real world\napplications. However, it has received relatively little attention in the\nbroader ML community, as compared to point anomalies or other types of single\ninstance outliers. One reason for this is the lack of powerful benchmark\ndatasets. In this paper, we first explain how, after the Nobel-prize winning\ndiscovery of the Higgs boson, unsupervised group anomaly detection has become a\nnew frontier of fundamental physics (where the motivation is to find new\nparticles and forces). Then we propose a realistic synthetic benchmark dataset\n(LHCO2020) for the development of group anomaly detection algorithms. Finally,\nwe compare several existing statistically-sound techniques for unsupervised\ngroup anomaly detection, and demonstrate their performance on the LHCO2020\ndataset.",
          "link": "http://arxiv.org/abs/2107.02821",
          "publishedOn": "2021-07-08T01:57:58.237Z",
          "wordCount": 605,
          "title": "New Methods and Datasets for Group Anomaly Detection From Fundamental Physics. (arXiv:2107.02821v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_B/0/1/0/all/0/1\">Bowei Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_F/0/1/0/all/0/1\">Fan Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xinyan Deng</a>",
          "description": "The paper develops a new adversarial attack against deep neural networks\n(DNN), based on applying bio-inspired design to moving physical objects. To the\nbest of our knowledge, this is the first work to introduce physical attacks\nwith a moving object. Instead of following the dominating attack strategy in\nthe existing literature, i.e., to introduce minor perturbations to a digital\ninput or a stationary physical object, we show two new successful attack\nstrategies in this paper. We show by superimposing several patterns onto one\nphysical object, a DNN becomes confused and picks one of the patterns to assign\na class label. Our experiment with three flapping wing robots demonstrates the\npossibility of developing an adversarial camouflage to cause a targeted mistake\nby DNN. We also show certain motion can reduce the dependency among consecutive\nframes in a video and make an object detector \"blind\", i.e., not able to detect\nan object exists in the video. Hence in a successful physical attack against\nDNN, targeted motion against the system should also be considered.",
          "link": "http://arxiv.org/abs/2107.02895",
          "publishedOn": "2021-07-08T01:57:58.215Z",
          "wordCount": 627,
          "title": "Bio-Inspired Adversarial Attack Against Deep Neural Networks. (arXiv:2107.02895v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1\">Erin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>",
          "description": "Data imbalance is a ubiquitous problem in machine learning. In large scale\ncollected and annotated datasets, data imbalance is either mitigated manually\nby undersampling frequent classes and oversampling rare classes, or planned for\nwith imputation and augmentation techniques. In both cases balancing data\nrequires labels. In other words, only annotated data can be balanced.\nCollecting fully annotated datasets is challenging, especially for large scale\nsatellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.\nAlthough the NASA Earth Imagery dataset is unlabeled, there are implicit\nproperties of the data source that we can rely on to hypothesize about its\nimbalance, such as distribution of land and water in the case of the Earth's\nimagery. We present a new iterative method to balance unlabeled data. Our\nmethod utilizes image embeddings as a proxy for image labels that can be used\nto balance data, and ultimately when trained increases overall accuracy.",
          "link": "http://arxiv.org/abs/2107.03227",
          "publishedOn": "2021-07-08T01:57:58.196Z",
          "wordCount": 614,
          "title": "Scalable Data Balancing for Unlabeled Satellite Imagery. (arXiv:2107.03227v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02970",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sundaram_S/0/1/0/all/0/1\">Shobhita Sundaram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hulkund_N/0/1/0/all/0/1\">Neha Hulkund</a>",
          "description": "A common problem in computer vision -- particularly in medical applications\n-- is a lack of sufficiently diverse, large sets of training data. These\ndatasets often suffer from severe class imbalance. As a result, networks often\noverfit and are unable to generalize to novel examples. Generative Adversarial\nNetworks (GANs) offer a novel method of synthetic data augmentation. In this\nwork, we evaluate the use of GAN- based data augmentation to artificially\nexpand the CheXpert dataset of chest radiographs. We compare performance to\ntraditional augmentation and find that GAN-based augmentation leads to higher\ndownstream performance for underrepresented classes. Furthermore, we see that\nthis result is pronounced in low data regimens. This suggests that GAN-based\naugmentation a promising area of research to improve network performance when\ndata collection is prohibitively expensive.",
          "link": "http://arxiv.org/abs/2107.02970",
          "publishedOn": "2021-07-08T01:57:58.181Z",
          "wordCount": 588,
          "title": "GAN-based Data Augmentation for Chest X-ray Classification. (arXiv:2107.02970v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aafaq_N/0/1/0/all/0/1\">Nayyer Aafaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>",
          "description": "Deep learning is found to be vulnerable to adversarial examples. However, its\nadversarial susceptibility in image caption generation is under-explored. We\nstudy adversarial examples for vision and language models, which typically\nadopt an encoder-decoder framework consisting of two major components: a\nConvolutional Neural Network (i.e., CNN) for image feature extraction and a\nRecurrent Neural Network (RNN) for caption generation. In particular, we\ninvestigate attacks on the visual encoder's hidden layer that is fed to the\nsubsequent recurrent network. The existing methods either attack the\nclassification layer of the visual encoder or they back-propagate the gradients\nfrom the language model. In contrast, we propose a GAN-based algorithm for\ncrafting adversarial examples for neural image captioning that mimics the\ninternal representation of the CNN such that the resulting deep features of the\ninput image enable a controlled incorrect caption generation through the\nrecurrent network. Our contribution provides new insights for understanding\nadversarial attacks on vision systems with language component. The proposed\nmethod employs two strategies for a comprehensive evaluation. The first\nexamines if a neural image captioning system can be misled to output targeted\nimage captions. The second analyzes the possibility of keywords into the\npredicted captions. Experiments show that our algorithm can craft effective\nadversarial images based on the CNN hidden layers to fool captioning framework.\nMoreover, we discover the proposed attack to be highly transferable. Our work\nleads to new robustness implications for neural image captioning.",
          "link": "http://arxiv.org/abs/2107.03050",
          "publishedOn": "2021-07-08T01:57:58.174Z",
          "wordCount": 678,
          "title": "Controlled Caption Generation for Images Through Adversarial Attacks. (arXiv:2107.03050v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindsly_S/0/1/0/all/0/1\">Stephen Lindsly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stansbury_C/0/1/0/all/0/1\">Cooper Stansbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehemtulla_A/0/1/0/all/0/1\">Alnawaz Rehemtulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajapakse_I/0/1/0/all/0/1\">Indika Rajapakse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hero_A/0/1/0/all/0/1\">Alfred Hero</a>",
          "description": "Adversarial attacks against deep neural networks (DNNs) are continuously\nevolving, requiring increasingly powerful defense strategies. We develop a\nnovel adversarial defense framework inspired by the adaptive immune system: the\nRobust Adversarial Immune-inspired Learning System (RAILS). Initializing a\npopulation of exemplars that is balanced across classes, RAILS starts from a\nuniform label distribution that encourages diversity and debiases a potentially\ncorrupted initial condition. RAILS implements an evolutionary optimization\nprocess to adjust the label distribution and achieve specificity towards ground\ntruth. RAILS displays a tradeoff between robustness (diversity) and accuracy\n(specificity), providing a new immune-inspired perspective on adversarial\nlearning. We empirically validate the benefits of RAILS through several\nadversarial image classification experiments on MNIST, SVHN, and CIFAR-10\ndatasets. For the PGD attack, RAILS is found to improve the robustness over\nexisting methods by >= 5.62%, 12.5% and 10.32%, respectively, without\nappreciable loss of standard accuracy.",
          "link": "http://arxiv.org/abs/2107.02840",
          "publishedOn": "2021-07-08T01:57:58.167Z",
          "wordCount": 598,
          "title": "RAILS: A Robust Adversarial Immune-inspired Learning System. (arXiv:2107.02840v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Correa_J/0/1/0/all/0/1\">Juan D Correa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghack Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1\">Elias Bareinboim</a>",
          "description": "The Ladder of Causation describes three qualitatively different types of\nactivities an agent may be interested in engaging in, namely, seeing\n(observational), doing (interventional), and imagining (counterfactual) (Pearl\nand Mackenzie, 2018). The inferential challenge imposed by the causal hierarchy\nis that data is collected by an agent observing or intervening in a system\n(layers 1 and 2), while its goal may be to understand what would have happened\nhad it taken a different course of action, contrary to what factually ended up\nhappening (layer 3). While there exists a solid understanding of the conditions\nunder which cross-layer inferences are allowed from observations to\ninterventions, the results are somewhat scarcer when targeting counterfactual\nquantities. In this paper, we study the identification of nested\ncounterfactuals from an arbitrary combination of observations and experiments.\nSpecifically, building on a more explicit definition of nested counterfactuals,\nwe prove the counterfactual unnesting theorem (CUT), which allows one to map\narbitrary nested counterfactuals to unnested ones. For instance, applications\nin mediation and fairness analysis usually evoke notions of direct, indirect,\nand spurious effects, which naturally require nesting. Second, we introduce a\nsufficient and necessary graphical condition for counterfactual identification\nfrom an arbitrary combination of observational and experimental distributions.\nLastly, we develop an efficient and complete algorithm for identifying nested\ncounterfactuals; failure of the algorithm returning an expression for a query\nimplies it is not identifiable.",
          "link": "http://arxiv.org/abs/2107.03190",
          "publishedOn": "2021-07-08T01:57:58.144Z",
          "wordCount": 663,
          "title": "Nested Counterfactual Identification from Arbitrary Surrogate Experiments. (arXiv:2107.03190v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03144",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kassraie_P/0/1/0/all/0/1\">Parnian Kassraie</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>",
          "description": "Contextual bandits are a rich model for sequential decision making given side\ninformation, with important applications, e.g., in recommender systems. We\npropose novel algorithms for contextual bandits harnessing neural networks to\napproximate the unknown reward function. We resolve the open problem of proving\nsublinear regret bounds in this setting for general context sequences,\nconsidering both fully-connected and convolutional networks. To this end, we\nfirst analyze NTK-UCB, a kernelized bandit optimization algorithm employing the\nNeural Tangent Kernel (NTK), and bound its regret in terms of the NTK maximum\ninformation gain $\\gamma_T$, a complexity parameter capturing the difficulty of\nlearning. Our bounds on $\\gamma_T$ for the NTK may be of independent interest.\nWe then introduce our neural network based algorithm NN-UCB, and show that its\nregret closely tracks that of NTK-UCB. Under broad non-parametric assumptions\nabout the reward function, our approach converges to the optimal policy at a\n$\\tilde{\\mathcal{O}}(T^{-1/2d})$ rate, where $d$ is the dimension of the\ncontext.",
          "link": "http://arxiv.org/abs/2107.03144",
          "publishedOn": "2021-07-08T01:57:58.137Z",
          "wordCount": 588,
          "title": "Neural Contextual Bandits without Regret. (arXiv:2107.03144v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhidong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fang Chen</a>",
          "description": "The label bias and selection bias are acknowledged as two reasons in data\nthat will hinder the fairness of machine-learning outcomes. The label bias\noccurs when the labeling decision is disturbed by sensitive features, while the\nselection bias occurs when subjective bias exists during the data sampling.\nEven worse, models trained on such data can inherit or even intensify the\ndiscrimination. Most algorithmic fairness approaches perform an empirical risk\nminimization with predefined fairness constraints, which tends to trade-off\naccuracy for fairness. However, such methods would achieve the desired fairness\nlevel with the sacrifice of the benefits (receive positive outcomes) for\nindividuals affected by the bias. Therefore, we propose a\nBias-TolerantFAirRegularizedLoss (B-FARL), which tries to regain the benefits\nusing data affected by label bias and selection bias. B-FARL takes the biased\ndata as input, calls a model that approximates the one trained with fair but\nlatent data, and thus prevents discrimination without constraints required. In\naddition, we show the effective components by decomposing B-FARL, and we\nutilize the meta-learning framework for the B-FARL optimization. The\nexperimental results on real-world datasets show that our method is empirically\neffective in improving fairness towards the direction of true but latent\nlabels.",
          "link": "http://arxiv.org/abs/2107.03207",
          "publishedOn": "2021-07-08T01:57:58.104Z",
          "wordCount": 624,
          "title": "Bias-Tolerant Fair Classification. (arXiv:2107.03207v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02908",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Shimmin_C/0/1/0/all/0/1\">Chase Shimmin</a>",
          "description": "We introduce the Particle Convolution Network (PCN), a new type of\nequivariant neural network layer suitable for many tasks in jet physics. The\nparticle convolution layer can be viewed as an extension of Deep Sets and\nEnergy Flow network architectures, in which the permutation-invariant operator\nis promoted to a group convolution. While the PCN can be implemented for\nvarious kinds of symmetries, we consider the specific case of rotation about\nthe jet axis the $\\eta - \\phi$ plane. In two standard benchmark tasks, q/g\ntagging and top tagging, we show that the rotational PCN (rPCN) achieves\nperformance comparable to graph networks such as ParticleNet. Moreover, we show\nthat it is possible to implement an IRC-safe rPCN, which significantly\noutperforms existing IRC-safe tagging methods on both tasks. We speculate that\nby generalizing the PCN to include additional convolutional symmetries relevant\nto jet physics, it may outperform the current state-of-the-art set by graph\nnetworks, while offering a new degree of control over physically-motivated\ninductive biases.",
          "link": "http://arxiv.org/abs/2107.02908",
          "publishedOn": "2021-07-08T01:57:58.098Z",
          "wordCount": 605,
          "title": "Particle Convolution for High Energy Physics. (arXiv:2107.02908v1 [hep-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindsly_S/0/1/0/all/0/1\">Stephen Lindsly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stansbury_C/0/1/0/all/0/1\">Cooper Stansbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajapakse_I/0/1/0/all/0/1\">Indika Rajapakse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hero_A/0/1/0/all/0/1\">Alfred Hero</a>",
          "description": "Biomimetics has played a key role in the evolution of artificial neural\nnetworks. Thus far, in silico metaphors have been dominated by concepts from\nneuroscience and cognitive psychology. In this paper we introduce a different\ntype of biomimetic model, one that borrows concepts from the immune system, for\ndesigning robust deep neural networks. This immuno-mimetic model leads to a new\ncomputational biology framework for robustification of deep neural networks\nagainst adversarial attacks. Within this Immuno-Net framework we define a\nrobust adaptive immune-inspired learning system (Immuno-Net RAILS) that\nemulates, in silico, the adaptive biological mechanisms of B-cells that are\nused to defend a mammalian host against pathogenic attacks. When applied to\nimage classification tasks on benchmark datasets, we demonstrate that\nImmuno-net RAILS results in improvement of as much as 12.5% in adversarial\naccuracy of a baseline method, the DkNN-robustified CNN, without appreciable\nloss of accuracy on clean data.",
          "link": "http://arxiv.org/abs/2107.02842",
          "publishedOn": "2021-07-08T01:57:58.086Z",
          "wordCount": 587,
          "title": "Immuno-mimetic Deep Neural Networks (Immuno-Net). (arXiv:2107.02842v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brown_O/0/1/0/all/0/1\">Olivia Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curtis_A/0/1/0/all/0/1\">Andrew Curtis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodwin_J/0/1/0/all/0/1\">Justin Goodwin</a>",
          "description": "The Department of Defense (DoD) has significantly increased its investment in\nthe design, evaluation, and deployment of Artificial Intelligence and Machine\nLearning (AI/ML) capabilities to address national security needs. While there\nare numerous AI/ML successes in the academic and commercial sectors, many of\nthese systems have also been shown to be brittle and nonrobust. In a complex\nand ever-changing national security environment, it is vital that the DoD\nestablish a sound and methodical process to evaluate the performance and\nrobustness of AI/ML models before these new capabilities are deployed to the\nfield. This paper reviews the AI/ML development process, highlights common best\npractices for AI/ML model evaluation, and makes recommendations to DoD\nevaluators to ensure the deployment of robust AI/ML capabilities for national\nsecurity needs.",
          "link": "http://arxiv.org/abs/2107.02868",
          "publishedOn": "2021-07-08T01:57:58.015Z",
          "wordCount": 559,
          "title": "Principles for Evaluation of AI/ML Model Performance and Robustness. (arXiv:2107.02868v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Sihai Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yong Zhao</a>",
          "description": "In this paper, a family of novel diffusion adaptive estimation algorithm is\nproposed from the asymmetric cost function perspective by combining diffusion\nstrategy and the linear-linear cost (LLC), quadratic-quadratic cost (QQC), and\nlinear-exponential cost (LEC), at all distributed network nodes, and named\ndiffusion LLCLMS (DLLCLMS), diffusion QQCLMS (DQQCLMS), and diffusion LECLMS\n(DLECLMS), respectively. Then the stability of mean estimation error and\ncomputational complexity of those three diffusion algorithms are analyzed\ntheoretically. Finally, several experiment simulation results are designed to\nverify the superiority of those three proposed diffusion algorithms.\nExperimental simulation results show that DLLCLMS, DQQCLMS, and DLECLMS\nalgorithms are more robust to the input signal and impulsive noise than the\nDSELMS, DRVSSLMS, and DLLAD algorithms. In brief, theoretical analysis and\nexperiment results show that those proposed DLLCLMS, DQQCLMS, and DLECLMS\nalgorithms have superior performance when estimating the unknown linear system\nunder the changeable impulsive noise environments and different types of input\nsignals.",
          "link": "http://arxiv.org/abs/2107.03067",
          "publishedOn": "2021-07-08T01:57:57.991Z",
          "wordCount": 591,
          "title": "Distributed adaptive algorithm based on the asymmetric cost of error functions. (arXiv:2107.03067v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1\">Mahardhika Pratama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zain_C/0/1/0/all/0/1\">Choiru Za&#x27;in</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lughofer_E/0/1/0/all/0/1\">Edwin Lughofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardede_E/0/1/0/all/0/1\">Eric Pardede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahayu_D/0/1/0/all/0/1\">Dwi A. P. Rahayu</a>",
          "description": "The large-scale data stream problem refers to high-speed information flow\nwhich cannot be processed in scalable manner under a traditional computing\nplatform. This problem also imposes expensive labelling cost making the\ndeployment of fully supervised algorithms unfeasible. On the other hand, the\nproblem of semi-supervised large-scale data streams is little explored in the\nliterature because most works are designed in the traditional single-node\ncomputing environments while also being fully supervised approaches. This paper\noffers Weakly Supervised Scalable Teacher Forcing Network (WeScatterNet) to\ncope with the scarcity of labelled samples and the large-scale data streams\nsimultaneously. WeScatterNet is crafted under distributed computing platform of\nApache Spark with a data-free model fusion strategy for model compression after\nparallel computing stage. It features an open network structure to address the\nglobal and local drift problems while integrating a data augmentation,\nannotation and auto-correction ($DA^3$) method for handling partially labelled\ndata streams. The performance of WeScatterNet is numerically evaluated in the\nsix large-scale data stream problems with only $25\\%$ label proportions. It\nshows highly competitive performance even if compared with fully supervised\nlearners with $100\\%$ label proportions.",
          "link": "http://arxiv.org/abs/2107.02943",
          "publishedOn": "2021-07-08T01:57:57.978Z",
          "wordCount": 651,
          "title": "Scalable Teacher Forcing Network for Semi-Supervised Large Scale Data Streams. (arXiv:2107.02943v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1\">Daniel Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarlow_D/0/1/0/all/0/1\">Danny Tarlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1\">Rianne van den Berg</a>",
          "description": "Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown\nimpressive results on image and waveform generation in continuous state spaces.\nHere, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs),\ndiffusion-like generative models for discrete data that generalize the\nmultinomial diffusion model of Hoogeboom et al. 2021, by going beyond\ncorruption processes with uniform transition probabilities. This includes\ncorruption with transition matrices that mimic Gaussian kernels in continuous\nspace, matrices based on nearest neighbors in embedding space, and matrices\nthat introduce absorbing states. The third allows us to draw a connection\nbetween diffusion models and autoregressive and mask-based generative models.\nWe show that the choice of transition matrix is an important design decision\nthat leads to improved results in image and text domains. We also introduce a\nnew loss function that combines the variational lower bound with an auxiliary\ncross entropy loss. For text, this model class achieves strong results on\ncharacter-level text generation while scaling to large vocabularies on LM1B. On\nthe image dataset CIFAR-10, our models approach the sample quality and exceed\nthe log-likelihood of the continuous-space DDPM model.",
          "link": "http://arxiv.org/abs/2107.03006",
          "publishedOn": "2021-07-08T01:57:57.929Z",
          "wordCount": 641,
          "title": "Structured Denoising Diffusion Models in Discrete State-Spaces. (arXiv:2107.03006v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Billah_M/0/1/0/all/0/1\">Mustain Billah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1\">Adnan Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_Z/0/1/0/all/0/1\">Ziaur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galib_S/0/1/0/all/0/1\">Syed Md. Galib</a>",
          "description": "Accurate building energy prediction is useful in various applications\nstarting from building energy automation and management to optimal storage\ncontrol. However, vulnerabilities should be considered when designing building\nenergy prediction models, as intelligent attackers can deliberately influence\nthe model performance using sophisticated attack models. These may consequently\ndegrade the prediction accuracy, which may affect the efficiency and\nperformance of the building energy management systems. In this paper, we\ninvestigate the impact of bi-level poisoning attacks on regression models of\nenergy usage obtained from household appliances. Furthermore, an effective\ncountermeasure against the poisoning attacks on the prediction model is\nproposed in this paper. Attacks and defenses are evaluated on a benchmark\ndataset. Experimental results show that an intelligent cyber-attacker can\npoison the prediction model to manipulate the decision. However, our proposed\nsolution successfully ensures defense against such poisoning attacks\neffectively compared to other benchmark techniques.",
          "link": "http://arxiv.org/abs/2107.02897",
          "publishedOn": "2021-07-08T01:57:57.805Z",
          "wordCount": 607,
          "title": "Bi-Level Poisoning Attack Model and Countermeasure for Appliance Consumption Data of Smart Homes. (arXiv:2107.02897v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03220",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Multimodal brain networks characterize complex connectivities among different\nbrain regions from both structural and functional aspects and provide a new\nmeans for mental disease analysis. Recently, Graph Neural Networks (GNNs) have\nbecome a de facto model for analyzing graph-structured data. However, how to\nemploy GNNs to extract effective representations from brain networks in\nmultiple modalities remains rarely explored. Moreover, as brain networks\nprovide no initial node features, how to design informative node attributes and\nleverage edge weights for GNNs to learn is left unsolved. To this end, we\ndevelop a novel multiview GNN for multimodal brain networks. In particular, we\nregard each modality as a view for brain networks and employ contrastive\nlearning for multimodal fusion. Then, we propose a GNN model which takes\nadvantage of the message passing scheme by propagating messages based on degree\nstatistics and brain region connectivities. Extensive experiments on two\nreal-world disease datasets (HIV and Bipolar) demonstrate the effectiveness of\nour proposed method over state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2107.03220",
          "publishedOn": "2021-07-08T01:57:57.784Z",
          "wordCount": 637,
          "title": "Joint Embedding of Structural and Functional Brain Networks with Graph Neural Networks for Mental Illness Diagnosis. (arXiv:2107.03220v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuribayashi_M/0/1/0/all/0/1\">Minoru Kuribayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasui_T/0/1/0/all/0/1\">Tatsuya Yasui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1\">Asad Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funabiki_N/0/1/0/all/0/1\">Nobuo Funabiki</a>",
          "description": "To ensure protection of the intellectual property rights of DNN models,\nwatermarking techniques have been investigated to insert side-information into\nthe models without seriously degrading the performance of original task. One of\nthe threats for the DNN watermarking is the pruning attack such that less\nimportant neurons in the model are pruned to make it faster and more compact as\nwell as to remove the watermark. In this study, we investigate a channel coding\napproach to resist the pruning attack. As the channel model is completely\ndifferent from conventional models like digital images, it has been an open\nproblem what kind of encoding method is suitable for DNN watermarking. A novel\nencoding approach by using constant weight codes to immunize the effects of\npruning attacks is presented. To the best of our knowledge, this is the first\nstudy that introduces an encoding technique for DNN watermarking to make it\nrobust against pruning attacks.",
          "link": "http://arxiv.org/abs/2107.02961",
          "publishedOn": "2021-07-08T01:57:57.736Z",
          "wordCount": 595,
          "title": "Immunization of Pruning Attack in DNN Watermarking Using Constant Weight Code. (arXiv:2107.02961v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garau_Luis_J/0/1/0/all/0/1\">Juan Jose Garau-Luis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crawley_E/0/1/0/all/0/1\">Edward Crawley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cameron_B/0/1/0/all/0/1\">Bruce Cameron</a>",
          "description": "Deep Reinforcement Learning (DRL) is considered a potential framework to\nimprove many real-world autonomous systems; it has attracted the attention of\nmultiple and diverse fields. Nevertheless, the successful deployment in the\nreal world is a test most of DRL models still need to pass. In this work we\nfocus on this issue by reviewing and evaluating the research efforts from both\ndomain-agnostic and domain-specific communities. On one hand, we offer a\ncomprehensive summary of DRL challenges and summarize the different proposals\nto mitigate them; this helps identifying five gaps of domain-agnostic research.\nOn the other hand, from the domain-specific perspective, we discuss different\nsuccess stories and argue why other models might fail to be deployed. Finally,\nwe take up on ways to move forward accounting for both perspectives.",
          "link": "http://arxiv.org/abs/2107.03015",
          "publishedOn": "2021-07-08T01:57:57.701Z",
          "wordCount": 577,
          "title": "Evaluating the progress of Deep Reinforcement Learning in the real world: aligning domain-agnostic and domain-specific research. (arXiv:2107.03015v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shamszare_H/0/1/0/all/0/1\">Hamid Shamszare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saremi_R/0/1/0/all/0/1\">Razieh Saremi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jena_S/0/1/0/all/0/1\">Sanam Jena</a>",
          "description": "The success of software crowdsourcing depends on active and trustworthy pool\nof worker supply. The uncertainty of crowd workers' behaviors makes it\nchallenging to predict workers' success and plan accordingly. In a competitive\ncrowdsourcing marketplace, competition for success over shared tasks adds\nanother layer of uncertainty in crowd workers' decision-making process.\nPreliminary analysis on software worker behaviors reveals an alarming task\ndropping rate of 82.9%. These factors lead to the need for an automated\nrecommendation system for CSD workers to improve the visibility and\npredictability of their success in the competition. To that end, this paper\nproposes a collaborative recommendation system for crowd workers. The proposed\nrecommendation system method uses five input metrics based on workers'\ncollaboration history in the pool, workers' preferences in taking tasks in\nterms of monetary prize and duration, workers' specialty, and workers'\nproficiency. The proposed method then recommends the most suitable tasks for a\nworker to compete on based on workers' probability of success in the task.\nExperimental results on 260 active crowd workers demonstrate that just\nfollowing the top three success probabilities of task recommendations, workers\ncan achieve success up to 86%",
          "link": "http://arxiv.org/abs/2107.02890",
          "publishedOn": "2021-07-08T01:57:57.652Z",
          "wordCount": 649,
          "title": "From Zero to The Hero: A Collaborative Market Aware Recommendation System for Crowd Workers. (arXiv:2107.02890v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huiyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1\">Diego Klabjan</a>",
          "description": "We introduce a new, reliable, and agnostic uncertainty measure for\nclassification tasks called logit uncertainty. It is based on logit outputs of\nneural networks. We in particular show that this new uncertainty measure yields\na superior performance compared to existing uncertainty measures on different\ntasks, including out of sample detection and finding erroneous predictions. We\nanalyze theoretical foundations of the measure and explore a relationship with\nhigh density regions. We also demonstrate how to test uncertainty using\nintermediate outputs in training of generative adversarial networks. We propose\ntwo potential ways to utilize logit-based uncertainty in real world\napplications, and show that the uncertainty measure outperforms.",
          "link": "http://arxiv.org/abs/2107.02845",
          "publishedOn": "2021-07-08T01:57:57.537Z",
          "wordCount": 524,
          "title": "Logit-based Uncertainty Measure in Classification. (arXiv:2107.02845v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sugahara_S/0/1/0/all/0/1\">Shouta Sugahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ueno_M/0/1/0/all/0/1\">Maomi Ueno</a>",
          "description": "Earlier studies have shown that classification accuracies of Bayesian\nnetworks (BNs) obtained by maximizing the conditional log likelihood (CLL) of a\nclass variable, given the feature variables, were higher than those obtained by\nmaximizing the marginal likelihood (ML). However, differences between the\nperformances of the two scores in the earlier studies may be attributed to the\nfact that they used approximate learning algorithms, not exact ones. This paper\ncompares the classification accuracies of BNs with approximate learning using\nCLL to those with exact learning using ML. The results demonstrate that the\nclassification accuracies of BNs obtained by maximizing the ML are higher than\nthose obtained by maximizing the CLL for large data. However, the results also\ndemonstrate that the classification accuracies of exact learning BNs using the\nML are much worse than those of other methods when the sample size is small and\nthe class variable has numerous parents. To resolve the problem, we propose an\nexact learning augmented naive Bayes classifier (ANB), which ensures a class\nvariable with no parents. The proposed method is guaranteed to asymptotically\nestimate the identical class posterior to that of the exactly learned BN.\nComparison experiments demonstrated the superior performance of the proposed\nmethod.",
          "link": "http://arxiv.org/abs/2107.03018",
          "publishedOn": "2021-07-08T01:57:57.325Z",
          "wordCount": 635,
          "title": "Exact Learning Augmented Naive Bayes Classifier. (arXiv:2107.03018v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haselbeck_F/0/1/0/all/0/1\">Florian Haselbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimm_D/0/1/0/all/0/1\">Dominik G. Grimm</a>",
          "description": "Time series forecasting is a growing domain with diverse applications.\nHowever, changes of the system behavior over time due to internal or external\ninfluences are challenging. Therefore, predictions of a previously learned\nfore-casting model might not be useful anymore. In this paper, we present\nEVent-triggered Augmented Refitting of Gaussian Process Regression for Seasonal\nData (EVARS-GPR), a novel online algorithm that is able to handle sudden shifts\nin the target variable scale of seasonal data. For this purpose, EVARS-GPR\ncom-bines online change point detection with a refitting of the prediction\nmodel using data augmentation for samples prior to a change point. Our\nexperiments on sim-ulated data show that EVARS-GPR is applicable for a wide\nrange of output scale changes. EVARS-GPR has on average a 20.8 % lower RMSE on\ndifferent real-world datasets compared to methods with a similar computational\nresource con-sumption. Furthermore, we show that our algorithm leads to a\nsix-fold reduction of the averaged runtime in relation to all comparison\npartners with a periodical refitting strategy. In summary, we present a\ncomputationally efficient online fore-casting algorithm for seasonal time\nseries with changes of the target variable scale and demonstrate its\nfunctionality on simulated as well as real-world data. All code is publicly\navailable on GitHub: https://github.com/grimmlab/evars-gpr.",
          "link": "http://arxiv.org/abs/2107.02463",
          "publishedOn": "2021-07-07T01:57:14.137Z",
          "wordCount": 651,
          "title": "EVARS-GPR: EVent-triggered Augmented Refitting of Gaussian Process Regression for Seasonal Data. (arXiv:2107.02463v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunari_A/0/1/0/all/0/1\">Akshaykumar Gunari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudari_S/0/1/0/all/0/1\">Shashidhar Veerappa Kudari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadagadalli_S/0/1/0/all/0/1\">Sukanya Nadagadalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goudnaik_K/0/1/0/all/0/1\">Keerthi Goudnaik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabib_R/0/1/0/all/0/1\">Ramesh Ashok Tabib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mudenagudi_U/0/1/0/all/0/1\">Uma Mudenagudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamadandi_A/0/1/0/all/0/1\">Adarsh Jamadandi</a>",
          "description": "In this paper, we propose a methodology to improvise the technique of deep\ntransfer clustering (DTC) when applied to the less variant data distribution.\nClustering can be considered as the most important unsupervised learning\nproblem. A simple definition of clustering can be stated as \"the process of\norganizing objects into groups, whose members are similar in some way\". Image\nclustering is a crucial but challenging task in the domain machine learning and\ncomputer vision. We have discussed the clustering of the data collection where\nthe data is less variant. We have discussed the improvement by using\nattention-based classifiers rather than regular classifiers as the initial\nfeature extractors in the deep transfer clustering. We have enforced the model\nto learn only the required region of interest in the images to get the\ndifferentiable and robust features that do not take into account the\nbackground. This paper is the improvement of the existing deep transfer\nclustering for less variant data distribution.",
          "link": "http://arxiv.org/abs/2107.02415",
          "publishedOn": "2021-07-07T01:57:14.118Z",
          "wordCount": 590,
          "title": "Deep Visual Attention-Based Transfer Clustering. (arXiv:2107.02415v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaohui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Renjie Liao</a>",
          "description": "In this paper, we present a non-parametric structured latent variable model\nfor image generation, called NP-DRAW, which sequentially draws on a latent\ncanvas in a part-by-part fashion and then decodes the image from the canvas.\nOur key contributions are as follows. 1) We propose a non-parametric prior\ndistribution over the appearance of image parts so that the latent variable\n``what-to-draw'' per step becomes a categorical random variable. This improves\nthe expressiveness and greatly eases the learning compared to Gaussians used in\nthe literature. 2) We model the sequential dependency structure of parts via a\nTransformer, which is more powerful and easier to train compared to RNNs used\nin the literature. 3) We propose an effective heuristic parsing algorithm to\npre-train the prior. Experiments on MNIST, Omniglot, CIFAR-10, and CelebA show\nthat our method significantly outperforms previous structured image models like\nDRAW and AIR and is competitive to other generic generative models. Moreover,\nwe show that our model's inherent compositionality and interpretability bring\nsignificant benefits in the low-data learning regime and latent space editing.\nCode is available at https://github.com/ZENGXH/NPDRAW.",
          "link": "http://arxiv.org/abs/2106.13435",
          "publishedOn": "2021-07-07T01:57:14.112Z",
          "wordCount": 664,
          "title": "NP-DRAW: A Non-Parametric Structured Latent Variable Model for Image Generation. (arXiv:2106.13435v2 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02287",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Cardoso_N/0/1/0/all/0/1\">N. M. Cardoso</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Schwarz_G/0/1/0/all/0/1\">G. B. O. Schwarz</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dias_L/0/1/0/all/0/1\">L. O. Dias</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bom_C/0/1/0/all/0/1\">C. R. Bom</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sodre_L/0/1/0/all/0/1\">L. Sodr&#xe9; Jr.</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Oliveira_C/0/1/0/all/0/1\">C. Mendes de Oliveira</a>",
          "description": "The universe is composed of galaxies that have diverse shapes. Once the\nstructure of a galaxy is determined, it is possible to obtain important\ninformation about its formation and evolution. Morphologically classifying\ngalaxies means cataloging them according to their visual appearance and the\nclassification is linked to the physical properties of the galaxy. A\nmorphological classification made through visual inspection is subject to\nbiases introduced by subjective observations made by human volunteers. For this\nreason, systematic, objective and easily reproducible classification of\ngalaxies has been gaining importance since the astronomer Edwin Hubble created\nhis famous classification method. In this work, we combine accurate visual\nclassifications of the Galaxy Zoo project with \\emph {Deep Learning} methods.\nThe goal is to find an efficient technique at human performance level\nclassification, but in a systematic and automatic way, for classification of\nelliptical and spiral galaxies. For this, a neural network model was created\nthrough an Ensemble of four other convolutional models, allowing a greater\naccuracy in the classification than what would be obtained with any one\nindividual. Details of the individual models and improvements made are also\ndescribed. The present work is entirely based on the analysis of images (not\nparameter tables) from DR1 (www.datalab.noao.edu) of the Southern Photometric\nLocal Universe Survey (S-PLUS). In terms of classification, we achieved, with\nthe Ensemble, an accuracy of $\\approx 99 \\%$ in the test sample (using\npre-trained networks).",
          "link": "http://arxiv.org/abs/2107.02287",
          "publishedOn": "2021-07-07T01:57:14.058Z",
          "wordCount": 713,
          "title": "Morphological Classification of Galaxies in S-PLUS using an Ensemble of Convolutional Networks. (arXiv:2107.02287v1 [astro-ph.GA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sumegha Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_P/0/1/0/all/0/1\">Pravesh K. Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengda Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raz_R/0/1/0/all/0/1\">Ran Raz</a>",
          "description": "In this work, we show, for the well-studied problem of learning parity under\nnoise, where a learner tries to learn $x=(x_1,\\ldots,x_n) \\in \\{0,1\\}^n$ from a\nstream of random linear equations over $\\mathrm{F}_2$ that are correct with\nprobability $\\frac{1}{2}+\\varepsilon$ and flipped with probability\n$\\frac{1}{2}-\\varepsilon$, that any learning algorithm requires either a memory\nof size $\\Omega(n^2/\\varepsilon)$ or an exponential number of samples.\n\nIn fact, we study memory-sample lower bounds for a large class of learning\nproblems, as characterized by [GRT'18], when the samples are noisy. A matrix\n$M: A \\times X \\rightarrow \\{-1,1\\}$ corresponds to the following learning\nproblem with error parameter $\\varepsilon$: an unknown element $x \\in X$ is\nchosen uniformly at random. A learner tries to learn $x$ from a stream of\nsamples, $(a_1, b_1), (a_2, b_2) \\ldots$, where for every $i$, $a_i \\in A$ is\nchosen uniformly at random and $b_i = M(a_i,x)$ with probability\n$1/2+\\varepsilon$ and $b_i = -M(a_i,x)$ with probability $1/2-\\varepsilon$\n($0<\\varepsilon< \\frac{1}{2}$). Assume that $k,\\ell, r$ are such that any\nsubmatrix of $M$ of at least $2^{-k} \\cdot |A|$ rows and at least $2^{-\\ell}\n\\cdot |X|$ columns, has a bias of at most $2^{-r}$. We show that any learning\nalgorithm for the learning problem corresponding to $M$, with error, requires\neither a memory of size at least $\\Omega\\left(\\frac{k \\cdot \\ell}{\\varepsilon}\n\\right)$, or at least $2^{\\Omega(r)}$ samples. In particular, this shows that\nfor a large class of learning problems, same as those in [GRT'18], any learning\nalgorithm requires either a memory of size at least $\\Omega\\left(\\frac{(\\log\n|X|) \\cdot (\\log |A|)}{\\varepsilon}\\right)$ or an exponential number of noisy\nsamples.\n\nOur proof is based on adapting the arguments in [Raz'17,GRT'18] to the noisy\ncase.",
          "link": "http://arxiv.org/abs/2107.02320",
          "publishedOn": "2021-07-07T01:57:14.051Z",
          "wordCount": 729,
          "title": "Memory-Sample Lower Bounds for Learning Parity with Noise. (arXiv:2107.02320v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tissera_D/0/1/0/all/0/1\">Dumindu Tissera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vithanage_K/0/1/0/all/0/1\">Kasun Vithanage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijesinghe_R/0/1/0/all/0/1\">Rukshan Wijesinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xavier_A/0/1/0/all/0/1\">Alex Xavier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasena_S/0/1/0/all/0/1\">Sanath Jayasena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_S/0/1/0/all/0/1\">Subha Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>",
          "description": "Any clustering algorithm must synchronously learn to model the clusters and\nallocate data to those clusters in the absence of labels. Mixture model-based\nmethods model clusters with pre-defined statistical distributions and allocate\ndata to those clusters based on the cluster likelihoods. They iteratively\nrefine those distribution parameters and member assignments following the\nExpectation-Maximization (EM) algorithm. However, the cluster representability\nof such hand-designed distributions that employ a limited amount of parameters\nis not adequate for most real-world clustering tasks. In this paper, we realize\nmixture model-based clustering with a neural network where the final layer\nneurons, with the aid of an additional transformation, approximate cluster\ndistribution outputs. The network parameters pose as the parameters of those\ndistributions. The result is an elegant, much-generalized representation of\nclusters than a restricted mixture of hand-designed distributions. We train the\nnetwork end-to-end via batch-wise EM iterations where the forward pass acts as\nthe E-step and the backward pass acts as the M-step. In image clustering, the\nmixture-based EM objective can be used as the clustering objective along with\nexisting representation learning methods. In particular, we show that when\nmixture-EM optimization is fused with consistency optimization, it improves the\nsole consistency optimization performance in clustering. Our trained networks\noutperform single-stage deep clustering methods that still depend on k-means,\nwith unsupervised classification accuracy of 63.8% in STL10, 58% in CIFAR10,\n25.9% in CIFAR100, and 98.9% in MNIST.",
          "link": "http://arxiv.org/abs/2107.02453",
          "publishedOn": "2021-07-07T01:57:14.042Z",
          "wordCount": 688,
          "title": "Neural Mixture Models with Expectation-Maximization for End-to-end Deep Clustering. (arXiv:2107.02453v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.05495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhowmick_A/0/1/0/all/0/1\">Aritra Bhowmick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DSouza_M/0/1/0/all/0/1\">Meenakshi D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_G/0/1/0/all/0/1\">G. Srinivasa Raghavan</a>",
          "description": "The Lipschitz constant of neural networks plays an important role in several\ncontexts of deep learning ranging from robustness certification and\nregularization to stability analysis of systems with neural network\ncontrollers. Obtaining tight bounds of the Lipschitz constant is therefore\nimportant. We introduce LipBaB, a branch and bound framework to compute\ncertified bounds of the local Lipschitz constant of deep neural networks with\nReLU activation functions up to any desired precision. We achieve this by\nbounding the norm of the Jacobians, corresponding to different activation\npatterns of the network caused within the input domain. Our algorithm can\nprovide provably exact computation of the Lipschitz constant for any p-norm.",
          "link": "http://arxiv.org/abs/2105.05495",
          "publishedOn": "2021-07-07T01:57:14.019Z",
          "wordCount": 566,
          "title": "LipBaB: Computing exact Lipschitz constant of ReLU networks. (arXiv:2105.05495v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02363",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Davison_A/0/1/0/all/0/1\">Andrew Davison</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Austern_M/0/1/0/all/0/1\">Morgane Austern</a>",
          "description": "Network data are ubiquitous in modern machine learning, with tasks of\ninterest including node classification, node clustering and link prediction. A\nfrequent approach begins by learning an Euclidean embedding of the network, to\nwhich algorithms developed for vector-valued data are applied. For large\nnetworks, embeddings are learned using stochastic gradient methods where the\nsub-sampling scheme can be freely chosen. Despite the strong empirical\nperformance of such methods, they are not well understood theoretically. Our\nwork encapsulates representation methods using a subsampling approach, such as\nnode2vec, into a single unifying framework. We prove, under the assumption that\nthe graph is exchangeable, that the distribution of the learned embedding\nvectors asymptotically decouples. Moreover, we characterize the asymptotic\ndistribution and provided rates of convergence, in terms of the latent\nparameters, which includes the choice of loss function and the embedding\ndimension. This provides a theoretical foundation to understand what the\nembedding vectors represent and how well these methods perform on downstream\ntasks. Notably, we observe that typically used loss functions may lead to\nshortcomings, such as a lack of Fisher consistency.",
          "link": "http://arxiv.org/abs/2107.02363",
          "publishedOn": "2021-07-07T01:57:13.999Z",
          "wordCount": 619,
          "title": "Asymptotics of Network Embeddings Learned via Subsampling. (arXiv:2107.02363v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Praveer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L. Rubin</a>",
          "description": "Federated learning is an emerging research paradigm for enabling\ncollaboratively training deep learning models without sharing patient data.\nHowever, the data from different institutions are usually heterogeneous across\ninstitutions, which may reduce the performance of models trained using\nfederated learning. In this study, we propose a novel heterogeneity-aware\nfederated learning method, SplitAVG, to overcome the performance drops from\ndata heterogeneity in federated learning. Unlike previous federated methods\nthat require complex heuristic training or hyper parameter tuning, our SplitAVG\nleverages the simple network split and feature map concatenation strategies to\nencourage the federated model training an unbiased estimator of the target data\ndistribution. We compare SplitAVG with seven state-of-the-art federated\nlearning methods, using centrally hosted training data as the baseline on a\nsuite of both synthetic and real-world federated datasets. We find that the\nperformance of models trained using all the comparison federated learning\nmethods degraded significantly with the increasing degrees of data\nheterogeneity. In contrast, SplitAVG method achieves comparable results to the\nbaseline method under all heterogeneous settings, that it achieves 96.2% of the\naccuracy and 110.4% of the mean absolute error obtained by the baseline in a\ndiabetic retinopathy binary classification dataset and a bone age prediction\ndataset, respectively, on highly heterogeneous data partitions. We conclude\nthat SplitAVG method can effectively overcome the performance drops from\nvariability in data distributions across institutions. Experimental results\nalso show that SplitAVG can be adapted to different base networks and\ngeneralized to various types of medical imaging tasks.",
          "link": "http://arxiv.org/abs/2107.02375",
          "publishedOn": "2021-07-07T01:57:13.993Z",
          "wordCount": 689,
          "title": "SplitAVG: A heterogeneity-aware federated deep learning method for medical imaging. (arXiv:2107.02375v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.04026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chunbo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Peng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_G/0/1/0/all/0/1\">Geyong Min</a>",
          "description": "This paper proposes fractional order graph neural networks (FGNNs), optimized\nby the approximation strategy to address the challenges of local optimum of\nclassic and fractional graph neural networks which are specialised at\naggregating information from the feature and adjacent matrices of connected\nnodes and their neighbours to solve learning tasks on non-Euclidean data such\nas graphs. Meanwhile the approximate calculation of fractional order gradients\nalso overcomes the high computational complexity of fractional order\nderivations. We further prove that such an approximation is feasible and the\nFGNN is unbiased towards global optimization solution. Extensive experiments on\ncitation networks show that FGNN achieves great advantage over baseline models\nwhen selected appropriate fractional order.",
          "link": "http://arxiv.org/abs/2001.04026",
          "publishedOn": "2021-07-07T01:57:13.914Z",
          "wordCount": 604,
          "title": "Fractional order graph neural network. (arXiv:2001.04026v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14565",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mathur_A/0/1/0/all/0/1\">Anant Mathur</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moka_S/0/1/0/all/0/1\">Sarat Moka</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Botev_Z/0/1/0/all/0/1\">Zdravko Botev</a>",
          "description": "In addition to recent developments in computing speed and memory,\nmethodological advances have contributed to significant gains in the\nperformance of stochastic simulation. In this paper, we focus on variance\nreduction for matrix computations via matrix factorization. We provide insights\ninto existing variance reduction methods for estimating the entries of large\nmatrices. Popular methods do not exploit the reduction in variance that is\npossible when the matrix is factorized. We show how computing the square root\nfactorization of the matrix can achieve in some important cases arbitrarily\nbetter stochastic performance. In addition, we propose a factorized estimator\nfor the trace of a product of matrices and numerically demonstrate that the\nestimator can be up to 1,000 times more efficient on certain problems of\nestimating the log-likelihood of a Gaussian process. Additionally, we provide a\nnew estimator of the log-determinant of a positive semi-definite matrix where\nthe log-determinant is treated as a normalizing constant of a probability\ndensity.",
          "link": "http://arxiv.org/abs/2106.14565",
          "publishedOn": "2021-07-07T01:57:13.909Z",
          "wordCount": 617,
          "title": "Variance Reduction for Matrix Computations with Applications to Gaussian Processes. (arXiv:2106.14565v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.13611",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_J/0/1/0/all/0/1\">Junxiang Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yu_F/0/1/0/all/0/1\">Fuxun Yu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>",
          "description": "Alternating Direction Method of Multipliers (ADMM) has been used successfully\nin many conventional machine learning applications and is considered to be a\nuseful alternative to Stochastic Gradient Descent (SGD) as a deep learning\noptimizer. However, as an emerging domain, several challenges remain, including\n1) The lack of global convergence guarantees, 2) Slow convergence towards\nsolutions, and 3) Cubic time complexity with regard to feature dimensions. In\nthis paper, we propose a novel optimization framework for deep learning via\nADMM (dlADMM) to address these challenges simultaneously. The parameters in\neach layer are updated backward and then forward so that the parameter\ninformation in each layer is exchanged efficiently. The time complexity is\nreduced from cubic to quadratic in (latent) feature dimensions via a dedicated\nalgorithm design for subproblems that enhances them utilizing iterative\nquadratic approximations and backtracking. Finally, we provide the first proof\nof global convergence for an ADMM-based method (dlADMM) in a deep neural\nnetwork problem under mild conditions. Experiments on benchmark datasets\ndemonstrated that our proposed dlADMM algorithm outperforms most of the\ncomparison methods.",
          "link": "http://arxiv.org/abs/1905.13611",
          "publishedOn": "2021-07-07T01:57:13.892Z",
          "wordCount": 653,
          "title": "ADMM for Efficient Deep Learning with Global Convergence. (arXiv:1905.13611v4 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09460",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_K/0/1/0/all/0/1\">Ke Alexander Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "In many real-world problems, we want to infer some property of an expensive\nblack-box function $f$, given a budget of $T$ function evaluations. One example\nis budget constrained global optimization of $f$, for which Bayesian\noptimization is a popular method. Other properties of interest include local\noptima, level sets, integrals, or graph-structured information induced by $f$.\nOften, we can find an algorithm $\\mathcal{A}$ to compute the desired property,\nbut it may require far more than $T$ queries to execute. Given such an\n$\\mathcal{A}$, and a prior distribution over $f$, we refer to the problem of\ninferring the output of $\\mathcal{A}$ using $T$ evaluations as Bayesian\nAlgorithm Execution (BAX). To tackle this problem, we present a procedure,\nInfoBAX, that sequentially chooses queries that maximize mutual information\nwith respect to the algorithm's output. Applying this to Dijkstra's algorithm,\nfor instance, we infer shortest paths in synthetic and real-world graphs with\nblack-box edge costs. Using evolution strategies, we yield variants of Bayesian\noptimization that target local, rather than global, optima. On these problems,\nInfoBAX uses up to 500 times fewer queries to $f$ than required by the original\nalgorithm. Our method is closely connected to other Bayesian optimal\nexperimental design procedures such as entropy search methods and optimal\nsensor placement using Gaussian processes.",
          "link": "http://arxiv.org/abs/2104.09460",
          "publishedOn": "2021-07-07T01:57:13.886Z",
          "wordCount": 698,
          "title": "Bayesian Algorithm Execution: Estimating Computable Properties of Black-box Functions Using Mutual Information. (arXiv:2104.09460v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12071",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_R/0/1/0/all/0/1\">Ru Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_R/0/1/0/all/0/1\">Ruipeng Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xi_Y/0/1/0/all/0/1\">Yuanzhe Xi</a>",
          "description": "Multigrid methods are one of the most efficient techniques for solving linear\nsystems arising from Partial Differential Equations (PDEs) and graph Laplacians\nfrom machine learning applications. One of the key components of multigrid is\nsmoothing, which aims at reducing high-frequency errors on each grid level.\nHowever, finding optimal smoothing algorithms is problem-dependent and can\nimpose challenges for many problems. In this paper, we propose an efficient\nadaptive framework for learning optimized smoothers from operator stencils in\nthe form of convolutional neural networks (CNNs). The CNNs are trained on\nsmall-scale problems from a given type of PDEs based on a supervised loss\nfunction derived from multigrid convergence theories, and can be applied to\nlarge-scale problems of the same class of PDEs. Numerical results on\nanisotropic rotated Laplacian problems demonstrate improved convergence rates\nand solution time compared with classical hand-crafted relaxation methods.",
          "link": "http://arxiv.org/abs/2102.12071",
          "publishedOn": "2021-07-07T01:57:13.879Z",
          "wordCount": 588,
          "title": "Learning optimal multigrid smoothers via neural networks. (arXiv:2102.12071v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neely_M/0/1/0/all/0/1\">Michael Neely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schouten_S/0/1/0/all/0/1\">Stefan F. Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleeker_M/0/1/0/all/0/1\">Maurits J. R. Bleeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1\">Ana Lucic</a>",
          "description": "By computing the rank correlation between attention weights and\nfeature-additive explanation methods, previous analyses either invalidate or\nsupport the role of attention-based explanations as a faithful and plausible\nmeasure of salience. To investigate whether this approach is appropriate, we\ncompare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and\nattention-based explanations, applied to two neural architectures trained on\nsingle- and pair-sequence language tasks. In most cases, we find that none of\nour chosen methods agree. Based on our empirical observations and theoretical\nobjections, we conclude that rank correlation does not measure the quality of\nfeature-additive methods. Practitioners should instead use the numerous and\nrigorous diagnostic methods proposed by the community.",
          "link": "http://arxiv.org/abs/2105.03287",
          "publishedOn": "2021-07-07T01:57:13.872Z",
          "wordCount": 604,
          "title": "Order in the Court: Explainable AI Methods Prone to Disagreement. (arXiv:2105.03287v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreau_H/0/1/0/all/0/1\">Hugues Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassilev_A/0/1/0/all/0/1\">Andr&#xe9;a Vassilev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>",
          "description": "In Transport Mode Detection, a great diversity of methodologies exist\naccording to the choice made on sensors, preprocessing, model used, etc. In\nthis domain, the comparisons between each option are not always complete.\nExperiments on a public, real-life dataset are led here to evaluate carefully\neach of the choices that were made, with a specific emphasis on data fusion\nmethods. Our most surprising finding is that none of the methods we implemented\nfrom the literature is better than a simple late fusion. Two important\ndecisions are the choice of a sensor and the choice of a representation for the\ndata: we found that using 2D convolutions on spectrograms with a logarithmic\naxis for the frequencies was better than 1-dimensional temporal\nrepresentations.",
          "link": "http://arxiv.org/abs/2106.05876",
          "publishedOn": "2021-07-07T01:57:13.852Z",
          "wordCount": 618,
          "title": "Data Fusion for Deep Learning on Transport Mode Detection: A Case Study. (arXiv:2106.05876v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gangal_A/0/1/0/all/0/1\">Ayushe Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Peeyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumari_S/0/1/0/all/0/1\">Sunita Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aditya Kumar</a>",
          "description": "This chapter aims to provide next-level understanding of the problems of the\nworld and the solutions available to those problems, which lie very well within\nthe domain of neural computing, and at the same time are intelligent in their\napproach, to invoke a sense of innovation among the educationalists,\nresearchers, academic professionals, students and people concerned, by\nhighlighting the work done by major researchers and innovators in this field\nand thus, encouraging the readers to develop newer and more advanced techniques\nfor the same. By means of this chapter, the societal problems are discussed and\nvarious solutions are also given by means of the theories presented and\nresearches done so far. Different types of neural networks discovered so far\nand applications of some of those neural networks are focused on, apart from\ntheir theoretical understanding, the working and core concepts involved in the\napplications.",
          "link": "http://arxiv.org/abs/2107.02744",
          "publishedOn": "2021-07-07T01:57:13.836Z",
          "wordCount": 581,
          "title": "Neural Computing. (arXiv:2107.02744v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2007.02445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shevkunov_K/0/1/0/all/0/1\">Kirill Shevkunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1\">Liudmila Prokhorenkova</a>",
          "description": "Various non-trivial spaces are becoming popular for embedding structured data\nsuch as graphs, texts, or images. Following spherical and hyperbolic spaces,\nmore general product spaces have been proposed. However, searching for the best\nconfiguration of product space is a resource-intensive procedure, which reduces\nthe practical applicability of the idea. We generalize the concept of product\nspace and introduce an overlapping space that does not have the configuration\nsearch problem. The main idea is to allow subsets of coordinates to be shared\nbetween spaces of different types (Euclidean, hyperbolic, spherical). As a\nresult, parameter optimization automatically learns the optimal configuration.\nAdditionally, overlapping spaces allow for more compact representations since\ntheir geometry is more complex. Our experiments confirm that overlapping spaces\noutperform the competitors in graph embedding tasks. Here, we consider both\ndistortion setup, where the aim is to preserve distances, and ranking setup,\nwhere the relative order should be preserved. The proposed method effectively\nsolves the problem and outperforms the competitors in both settings. We also\nperform an empirical analysis in a realistic information retrieval task, where\nwe compare all spaces by incorporating them into DSSM. In this case, the\nproposed overlapping space consistently achieves nearly optimal results without\nany configuration tuning. This allows for reducing training time, which can be\nsignificant in large-scale applications.",
          "link": "http://arxiv.org/abs/2007.02445",
          "publishedOn": "2021-07-07T01:57:13.810Z",
          "wordCount": 674,
          "title": "Overlapping Spaces for Compact Graph Representations. (arXiv:2007.02445v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaffre_T/0/1/0/all/0/1\">Thomas Chaffre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moras_J/0/1/0/all/0/1\">Julien Moras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_Hon_Tong_A/0/1/0/all/0/1\">Adrien Chan-Hon-Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzat_J/0/1/0/all/0/1\">Julien Marzat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sammut_K/0/1/0/all/0/1\">Karl Sammut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenadec_G/0/1/0/all/0/1\">Gilles Le Chenadec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clement_B/0/1/0/all/0/1\">Benoit Clement</a>",
          "description": "Navigation problems under unknown varying conditions are among the most\nimportant and well-studied problems in the control field. Classic model-based\nadaptive control methods can be applied only when a convenient model of the\nplant or environment is provided. Recent model-free adaptive control methods\naim at removing this dependency by learning the physical characteristics of the\nplant and/or process directly from sensor feedback. Although there have been\nprior attempts at improving these techniques, it remains an open question as to\nwhether it is possible to cope with real-world uncertainties in a control\nsystem that is fully based on either paradigm. We propose a conceptually simple\nlearning-based approach composed of a full state feedback controller, tuned\nrobustly by a deep reinforcement learning framework based on the Soft\nActor-Critic algorithm. We compare it, in realistic simulations, to a\nmodel-free controller that uses the same deep reinforcement learning framework\nfor the control of a micro aerial vehicle under wind gust. The results indicate\nthe great potential of learning-based adaptive control methods in modern\ndynamical systems.",
          "link": "http://arxiv.org/abs/2101.12501",
          "publishedOn": "2021-07-07T01:57:13.804Z",
          "wordCount": 662,
          "title": "Learning-based vs Model-free Adaptive Control of a MAV under Wind Gust. (arXiv:2101.12501v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mulamba_M/0/1/0/all/0/1\">Maxime Mulamba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandi_J/0/1/0/all/0/1\">Jayanta Mandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diligenti_M/0/1/0/all/0/1\">Michelangelo Diligenti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombardi_M/0/1/0/all/0/1\">Michele Lombardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucarey_V/0/1/0/all/0/1\">Victor Bucarey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guns_T/0/1/0/all/0/1\">Tias Guns</a>",
          "description": "Many decision-making processes involve solving a combinatorial optimization\nproblem with uncertain input that can be estimated from historic data.\nRecently, problems in this class have been successfully addressed via\nend-to-end learning approaches, which rely on solving one optimization problem\nfor each training instance at every epoch. In this context, we provide two\ndistinct contributions. First, we use a Noise Contrastive approach to motivate\na family of surrogate loss functions, based on viewing non-optimal solutions as\nnegative examples. Second, we address a major bottleneck of all\npredict-and-optimize approaches, i.e. the need to frequently recompute optimal\nsolutions at training time. This is done via a solver-agnostic solution caching\nscheme, and by replacing optimization calls with a lookup in the solution\ncache. The method is formally based on an inner approximation of the feasible\nspace and, combined with a cache lookup strategy, provides a controllable\ntrade-off between training time and accuracy of the loss approximation. We\nempirically show that even a very slow growth rate is enough to match the\nquality of state-of-the-art methods, at a fraction of the computational cost.",
          "link": "http://arxiv.org/abs/2011.05354",
          "publishedOn": "2021-07-07T01:57:13.798Z",
          "wordCount": 648,
          "title": "Contrastive Losses and Solution Caching for Predict-and-Optimize. (arXiv:2011.05354v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01981",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengyang Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Luo_Y/0/1/0/all/0/1\">Youzhi Luo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_Z/0/1/0/all/0/1\">Zhao Xu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xie_Y/0/1/0/all/0/1\">Yaochen Xie</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_L/0/1/0/all/0/1\">Limei Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cai_L/0/1/0/all/0/1\">Lei Cai</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Qi_Q/0/1/0/all/0/1\">Qi Qi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhuoning Yuan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>",
          "description": "Properties of molecules are indicative of their functions and thus are useful\nin many applications. With the advances of deep learning methods, computational\napproaches for predicting molecular properties are gaining increasing momentum.\nHowever, there lacks customized and advanced methods and comprehensive tools\nfor this task currently. Here we develop a suite of comprehensive machine\nlearning methods and tools spanning different computational models, molecular\nrepresentations, and loss functions for molecular property prediction and drug\ndiscovery. Specifically, we represent molecules as both graphs and sequences.\nBuilt on these representations, we develop novel deep models for learning from\nmolecular graphs and sequences. In order to learn effectively from highly\nimbalanced datasets, we develop advanced loss functions that optimize areas\nunder precision-recall curves. Altogether, our work not only serves as a\ncomprehensive tool, but also contributes towards developing novel and advanced\ngraph and sequence learning methodologies. Results on both online and offline\nantibiotics discovery and molecular property prediction tasks show that our\nmethods achieve consistent improvements over prior methods. In particular, our\nmethods achieve #1 ranking in terms of both ROC-AUC and PRC-AUC on the AI Cures\nOpen Challenge for drug discovery related to COVID-19. Our software is released\nas part of the MoleculeX library under AdvProp.",
          "link": "http://arxiv.org/abs/2012.01981",
          "publishedOn": "2021-07-07T01:57:13.791Z",
          "wordCount": 739,
          "title": "Advanced Graph and Sequence Neural Networks for Molecular Property Prediction and Drug Discovery. (arXiv:2012.01981v3 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11748",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Charoenphakdee_N/0/1/0/all/0/1\">Nontawat Charoenphakdee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cui_Z/0/1/0/all/0/1\">Zhenghang Cui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1\">Yivan Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "The goal of classification with rejection is to avoid risky misclassification\nin error-critical applications such as medical diagnosis and product\ninspection. In this paper, based on the relationship between classification\nwith rejection and cost-sensitive classification, we propose a novel method of\nclassification with rejection by learning an ensemble of cost-sensitive\nclassifiers, which satisfies all the following properties: (i) it can avoid\nestimating class-posterior probabilities, resulting in improved classification\naccuracy, (ii) it allows a flexible choice of losses including non-convex ones,\n(iii) it does not require complicated modifications when using different\nlosses, (iv) it is applicable to both binary and multiclass cases, and (v) it\nis theoretically justifiable for any classification-calibrated loss.\nExperimental results demonstrate the usefulness of our proposed approach in\nclean-labeled, noisy-labeled, and positive-unlabeled classification.",
          "link": "http://arxiv.org/abs/2010.11748",
          "publishedOn": "2021-07-07T01:57:13.785Z",
          "wordCount": 596,
          "title": "Classification with Rejection Based on Cost-sensitive Classification. (arXiv:2010.11748v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Treutlein_J/0/1/0/all/0/1\">Johannes Treutlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dennis_M/0/1/0/all/0/1\">Michael Dennis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oesterheld_C/0/1/0/all/0/1\">Caspar Oesterheld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "In many coordination problems, independently reasoning humans are able to\ndiscover mutually compatible policies. In contrast, independently trained\nself-play policies are often mutually incompatible. Zero-shot coordination\n(ZSC) has recently been proposed as a new frontier in multi-agent reinforcement\nlearning to address this fundamental issue. Prior work approaches the ZSC\nproblem by assuming players can agree on a shared learning algorithm but not on\nlabels for actions and observations, and proposes other-play as an optimal\nsolution. However, until now, this \"label-free\" problem has only been\ninformally defined. We formalize this setting as the label-free coordination\n(LFC) problem by defining the label-free coordination game. We show that\nother-play is not an optimal solution to the LFC problem as it fails to\nconsistently break ties between incompatible maximizers of the other-play\nobjective. We introduce an extension of the algorithm, other-play with\ntie-breaking, and prove that it is optimal in the LFC problem and an\nequilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the\nZSC setting aims to prevent, we conclude that the LFC problem does not reflect\nthe aims of ZSC. To address this, we introduce an alternative informal\noperationalization of ZSC as a starting point for future work.",
          "link": "http://arxiv.org/abs/2106.06613",
          "publishedOn": "2021-07-07T01:57:13.767Z",
          "wordCount": 664,
          "title": "A New Formalism, Method and Open Issues for Zero-Shot Coordination. (arXiv:2106.06613v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">S&#xf6;ren Auer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersen_T/0/1/0/all/0/1\">Ted Pedersen</a>",
          "description": "There is currently a gap between the natural language expression of scholarly\npublications and their structured semantic content modeling to enable\nintelligent content search. With the volume of research growing exponentially\nevery year, a search feature operating over semantically structured content is\ncompelling. The SemEval-2021 Shared Task NLPContributionGraph (a.k.a. 'the NCG\ntask') tasks participants to develop automated systems that structure\ncontributions from NLP scholarly articles in the English language. Being the\nfirst-of-its-kind in the SemEval series, the task released structured data from\nNLP scholarly articles at three levels of information granularity, i.e. at\nsentence-level, phrase-level, and phrases organized as triples toward Knowledge\nGraph (KG) building. The sentence-level annotations comprised the few sentences\nabout the article's contribution. The phrase-level annotations were scientific\nterm and predicate phrases from the contribution sentences. Finally, the\ntriples constituted the research overview KG. For the Shared Task,\nparticipating systems were then expected to automatically classify contribution\nsentences, extract scientific terms and relations from the sentences, and\norganize them as KG triples.\n\nOverall, the task drew a strong participation demographic of seven teams and\n27 participants. The best end-to-end task system classified contribution\nsentences at 57.27% F1, phrases at 46.41% F1, and triples at 22.28% F1. While\nthe absolute performance to generate triples remains low, in the conclusion of\nthis article, the difficulty of producing such data and as a consequence of\nmodeling it is highlighted.",
          "link": "http://arxiv.org/abs/2106.07385",
          "publishedOn": "2021-07-07T01:57:13.761Z",
          "wordCount": 732,
          "title": "SemEval-2021 Task 11: NLPContributionGraph -- Structuring Scholarly NLP Contributions for a Research Knowledge Graph. (arXiv:2106.07385v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_T/0/1/0/all/0/1\">Tzu-Ming Harry Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yin-Chih Chelsea Wang</a>",
          "description": "Clinical finding summaries from an orthopantomogram, or a dental panoramic\nradiograph, have significant potential to improve patient communication and\nspeed up clinical judgments. While orthopantomogram is a first-line tool for\ndental examinations, no existing work has explored the summarization of\nfindings from it. A finding summary has to find teeth in the imaging study and\nlabel the teeth with several types of past treatments. To tackle the problem,\nwe developDeepOPG that breaks the summarization process into functional\nsegmentation and tooth localization, the latter of which is further refined by\na novel dental coherence module. We also leverage weak supervision labels to\nimprove detection results in a reinforcement learning scenario. Experiments\nshow high efficacy of DeepOPG on finding summarization, achieving an overall\nAUC of 88.2% in detecting six types of findings. The proposed dental coherence\nand weak supervision both are shown to improve DeepOPG by adding 5.9% and 0.4%\nto AP@IoU=0.5.",
          "link": "http://arxiv.org/abs/2103.08290",
          "publishedOn": "2021-07-07T01:57:13.755Z",
          "wordCount": 612,
          "title": "DeepOPG: Improving Orthopantomogram Finding Summarization with Weak Supervision. (arXiv:2103.08290v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05466",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Hie_B/0/1/0/all/0/1\">Brian L. Hie</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_K/0/1/0/all/0/1\">Kevin K. Yang</a>",
          "description": "Machine-learning models that learn from data to predict how protein sequence\nencodes function are emerging as a useful protein engineering tool. However,\nwhen using these models to suggest new protein designs, one must deal with the\nvast combinatorial complexity of protein sequences. Here, we review how to use\na sequence-to-function machine-learning surrogate model to select sequences for\nexperimental measurement. First, we discuss how to select sequences through a\nsingle round of machine-learning optimization. Then, we discuss sequential\noptimization, where the goal is to discover optimized sequences and improve the\nmodel across multiple rounds of training, optimization, and experimental\nmeasurement.",
          "link": "http://arxiv.org/abs/2106.05466",
          "publishedOn": "2021-07-07T01:57:13.750Z",
          "wordCount": 552,
          "title": "Adaptive machine learning for protein engineering. (arXiv:2106.05466v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bevilacqua_B/0/1/0/all/0/1\">Beatrice Bevilacqua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangze Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1\">Bruno Ribeiro</a>",
          "description": "In general, graph representation learning methods assume that the train and\ntest data come from the same distribution. In this work we consider an\nunderexplored area of an otherwise rapidly developing field of graph\nrepresentation learning: The task of out-of-distribution (OOD) graph\nclassification, where train and test data have different distributions, with\ntest data unavailable during training. Our work shows it is possible to use a\ncausal model to learn approximately invariant representations that better\nextrapolate between train and test data. Finally, we conclude with synthetic\nand real-world dataset experiments showcasing the benefits of representations\nthat are invariant to train/test distribution shifts.",
          "link": "http://arxiv.org/abs/2103.05045",
          "publishedOn": "2021-07-07T01:57:13.744Z",
          "wordCount": 558,
          "title": "Size-Invariant Graph Representations for Graph Classification Extrapolations. (arXiv:2103.05045v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1\">Thomas Bartz-Beielstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehbach_F/0/1/0/all/0/1\">Frederik Rehbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1\">Amrita Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaefferer_M/0/1/0/all/0/1\">Martin Zaefferer</a>",
          "description": "A surrogate model based hyperparameter tuning approach for deep learning is\npresented. This article demonstrates how the architecture-level parameters\n(hyperparameters) of deep learning models that were implemented in\nKeras/tensorflow can be optimized. The implementation of the tuning procedure\nis 100% accessible from R, the software environment for statistical computing.\nWith a few lines of code, existing R packages (tfruns and SPOT) can be combined\nto perform hyperparameter tuning. An elementary hyperparameter tuning task\n(neural network and the MNIST data) is used to exemplify this approach",
          "link": "http://arxiv.org/abs/2105.14625",
          "publishedOn": "2021-07-07T01:57:13.728Z",
          "wordCount": 566,
          "title": "Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT. (arXiv:2105.14625v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.13823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_H/0/1/0/all/0/1\">Hassam Ullah Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phielipp_M/0/1/0/all/0/1\">Mariano Phielipp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boloni_L/0/1/0/all/0/1\">Ladislau B&#xf6;l&#xf6;ni</a>",
          "description": "The classic DQN algorithm is limited by the overestimation bias of the\nlearned Q-function. Subsequent algorithms have proposed techniques to reduce\nthis problem, without fully eliminating it. Recently, the Maxmin and Ensemble\nQ-learning algorithms have used different estimates provided by the ensembles\nof learners to reduce the overestimation bias. Unfortunately, these learners\ncan converge to the same point in the parametric or representation space,\nfalling back to the classic single neural network DQN. In this paper, we\ndescribe a regularization technique to maximize ensemble diversity in these\nalgorithms. We propose and compare five regularization functions inspired from\neconomics theory and consensus optimization. We show that the regularized\napproach significantly outperforms the Maxmin and Ensemble Q-learning\nalgorithms as well as non-ensemble baselines.",
          "link": "http://arxiv.org/abs/2006.13823",
          "publishedOn": "2021-07-07T01:57:13.722Z",
          "wordCount": 592,
          "title": "Maximizing Ensemble Diversity in Deep Q-Learning. (arXiv:2006.13823v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>",
          "description": "A real-world graph has a complex topological structure, which is often formed\nby the interaction of different latent factors. Disentanglement of these latent\nfactors can effectively improve the robustness and expressiveness of node\nrepresentation of graph. However, most existing methods lack consideration of\nthe intrinsic differences in relations between nodes caused by factor\nentanglement. In this paper, we propose an Adversarial Disentangled Graph\nConvolutional Network (ADGCN) for disentangled graph representation learning.\nSpecifically, a component-specific aggregation approach is proposed to achieve\nmicro-disentanglement by inferring latent components that caused the links\nbetween nodes. On the basis of micro-disentanglement, we further propose a\nmacro-disentanglement adversarial regularizer to improve the separability among\ncomponent distributions, thus restricting the interdependence among components.\nAdditionally, to reveal the topological graph structure, a diversity-preserving\nnode sampling approach is proposed, by which the graph structure can be\nprogressively refined in a way of local structure awareness. The experimental\nresults on various real-world graph data verify that our ADGCN obtains more\nfavorable performance over currently available alternatives.",
          "link": "http://arxiv.org/abs/2103.07295",
          "publishedOn": "2021-07-07T01:57:13.713Z",
          "wordCount": 627,
          "title": "Adversarial Graph Disentanglement. (arXiv:2103.07295v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paria_D/0/1/0/all/0/1\">Debjit Paria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Abhishek Sinha</a>",
          "description": "We consider a set-valued online prediction problem in the context of network\ncaching. Assume that multiple users are connected to several caches via a\nbipartite network. At any time slot, each user requests an arbitrary file\nchosen from a large catalog. A user's request at a slot is met if the requested\nfile is cached in at least one of the caches connected to the user. Our\nobjective is to predict, prefetch, and optimally distribute the files on the\ncaches to maximize the total number of cache hits in an online setting. The\nproblem is non-trivial due to the non-convex and non-smooth nature of the\nobjective function. In this paper, we propose $\\texttt{LeadCache}$ - an online\ncaching policy based on the Follow-the-Perturbed-Leader paradigm. We show that\nthe policy is regret-optimal up to a factor of $\\tilde{O}(n^{3/8}),$ where $n$\nis the number of users. We design two efficient implementations of the\n$\\texttt{LeadCache}$ policy, one based on Pipage rounding and the other based\non Madow's sampling, each of which makes precisely one call to an LP-solver per\niteration. With a Strong-Law-type assumption, we show that the total number of\nfile fetches under $\\texttt{LeadCache}$ remains almost surely finite over an\ninfinite horizon. Finally, we derive a tight regret lower bound using results\nfrom graph coloring. We conclude that the learning-based $\\texttt{LeadCache}$\npolicy decisively outperforms the known caching policies both theoretically and\nempirically.",
          "link": "http://arxiv.org/abs/2009.08228",
          "publishedOn": "2021-07-07T01:57:13.706Z",
          "wordCount": 699,
          "title": "LeadCache: Regret-Optimal Caching in Networks. (arXiv:2009.08228v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_M/0/1/0/all/0/1\">Malcolm Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matt Botvinick</a>",
          "description": "Neural networks have achieved success in a wide array of perceptual tasks but\noften fail at tasks involving both perception and higher-level reasoning. On\nthese more challenging tasks, bespoke approaches (such as modular symbolic\ncomponents, independent dynamics models or semantic parsers) targeted towards\nthat specific type of task have typically performed better. The downside to\nthese targeted approaches, however, is that they can be more brittle than\ngeneral-purpose neural networks, requiring significant modification or even\nredesign according to the particular task at hand. Here, we propose a more\ngeneral neural-network-based approach to dynamic visual reasoning problems that\nobtains state-of-the-art performance on three different domains, in each case\noutperforming bespoke modular approaches tailored specifically to the task. Our\nmethod relies on learned object-centric representations, self-attention and\nself-supervised dynamics learning, and all three elements together are required\nfor strong performance to emerge. The success of this combination suggests that\nthere may be no need to trade off flexibility for performance on problems\ninvolving spatio-temporal or causal-style reasoning. With the right soft biases\nand learning objectives in a neural network we may be able to attain the best\nof both worlds.",
          "link": "http://arxiv.org/abs/2012.08508",
          "publishedOn": "2021-07-07T01:57:13.213Z",
          "wordCount": 676,
          "title": "Attention over learned object embeddings enables complex visual reasoning. (arXiv:2012.08508v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velu_A/0/1/0/all/0/1\">Akash Velu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinitsky_E/0/1/0/all/0/1\">Eugene Vinitsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayen_A/0/1/0/all/0/1\">Alexandre Bayen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>",
          "description": "Proximal Policy Optimization (PPO) is a popular on-policy reinforcement\nlearning algorithm but is significantly less utilized than off-policy learning\nalgorithms in multi-agent settings. This is often due the belief that on-policy\nmethods are significantly less sample efficient than their off-policy\ncounterparts in multi-agent problems. In this work, we investigate Multi-Agent\nPPO (MAPPO), a variant of PPO which is specialized for multi-agent settings.\nUsing a 1-GPU desktop, we show that MAPPO achieves surprisingly strong\nperformance in three popular multi-agent testbeds: the particle-world\nenvironments, the Starcraft multi-agent challenge, and the Hanabi challenge,\nwith minimal hyperparameter tuning and without any domain-specific algorithmic\nmodifications or architectures. In the majority of environments, we find that\ncompared to off-policy baselines, MAPPO achieves strong results while\nexhibiting comparable sample efficiency. Finally, through ablation studies, we\npresent the implementation and algorithmic factors which are most influential\nto MAPPO's practical performance.",
          "link": "http://arxiv.org/abs/2103.01955",
          "publishedOn": "2021-07-07T01:57:13.198Z",
          "wordCount": 617,
          "title": "The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games. (arXiv:2103.01955v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_D/0/1/0/all/0/1\">Devinder Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Shama Naz Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_M/0/1/0/all/0/1\">Md. Apel Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">ZhaoYang Dong</a>",
          "description": "Energy forecasting has a vital role to play in smart grid (SG) systems\ninvolving various applications such as demand-side management, load shedding,\nand optimum dispatch. Managing efficient forecasting while ensuring the least\npossible prediction error is one of the main challenges posed in the grid\ntoday, considering the uncertainty and granularity in SG data. This paper\npresents a comprehensive and application-oriented review of state-of-the-art\nforecasting methods for SG systems along with recent developments in\nprobabilistic deep learning (PDL) considering different models and\narchitectures. Traditional point forecasting methods including statistical,\nmachine learning (ML), and deep learning (DL) are extensively investigated in\nterms of their applicability to energy forecasting. In addition, the\nsignificance of hybrid and data pre-processing techniques to support\nforecasting performance is also studied. A comparative case study using the\nVictorian electricity consumption and American electric power (AEP) datasets is\nconducted to analyze the performance of point and probabilistic forecasting\nmethods. The analysis demonstrates higher accuracy of the long-short term\nmemory (LSTM) models with appropriate hyper-parameter tuning among point\nforecasting methods especially when sample sizes are larger and involve\nnonlinear patterns with long sequences. Furthermore, Bayesian bidirectional\nLSTM (BLSTM) as a probabilistic method exhibit the highest accuracy in terms of\nleast pinball score and root mean square error (RMSE).",
          "link": "http://arxiv.org/abs/2011.12598",
          "publishedOn": "2021-07-07T01:57:13.187Z",
          "wordCount": 674,
          "title": "Energy Forecasting in Smart Grid Systems: A Review of the State-of-the-art Techniques. (arXiv:2011.12598v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02780",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Agarwal_A/0/1/0/all/0/1\">Anish Agarwal</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Singh_R/0/1/0/all/0/1\">Rahul Singh</a>",
          "description": "Even the most carefully curated economic data sets have variables that are\nnoisy, missing, discretized, or privatized. The standard workflow for empirical\nresearch involves data cleaning followed by data analysis that typically\nignores the bias and variance consequences of data cleaning. We formulate a\nsemiparametric model for causal inference with corrupted data to encompass both\ndata cleaning and data analysis. We propose a new end-to-end procedure for data\ncleaning, estimation, and inference with data cleaning-adjusted confidence\nintervals. We prove root-n consistency, Gaussian approximation, and\nsemiparametric efficiency for our estimator of the causal parameter by finite\nsample arguments. Our key assumption is that the true covariates are\napproximately low rank. In our analysis, we provide nonasymptotic theoretical\ncontributions to matrix completion, statistical learning, and semiparametric\nstatistics. We verify the coverage of the data cleaning-adjusted confidence\nintervals in simulations.",
          "link": "http://arxiv.org/abs/2107.02780",
          "publishedOn": "2021-07-07T01:57:13.181Z",
          "wordCount": 593,
          "title": "Causal Inference with Corrupted Data: Measurement Error, Missing Values, Discretization, and Differential Privacy. (arXiv:2107.02780v1 [econ.EM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02597",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Sawant_N/0/1/0/all/0/1\">Nihar Sawant</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kramer_B/0/1/0/all/0/1\">Boris Kramer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Peherstorfer_B/0/1/0/all/0/1\">Benjamin Peherstorfer</a>",
          "description": "Operator inference learns low-dimensional dynamical-system models with\npolynomial nonlinear terms from trajectories of high-dimensional physical\nsystems (non-intrusive model reduction). This work focuses on the large class\nof physical systems that can be well described by models with quadratic\nnonlinear terms and proposes a regularizer for operator inference that induces\na stability bias onto quadratic models. The proposed regularizer is physics\ninformed in the sense that it penalizes quadratic terms with large norms and so\nexplicitly leverages the quadratic model form that is given by the underlying\nphysics. This means that the proposed approach judiciously learns from data and\nphysical insights combined, rather than from either data or physics alone.\nAdditionally, a formulation of operator inference is proposed that enforces\nmodel constraints for preserving structure such as symmetry and definiteness in\nthe linear terms. Numerical results demonstrate that models learned with\noperator inference and the proposed regularizer and structure preservation are\naccurate and stable even in cases where using no regularization or Tikhonov\nregularization leads to models that are unstable.",
          "link": "http://arxiv.org/abs/2107.02597",
          "publishedOn": "2021-07-07T01:57:13.175Z",
          "wordCount": 622,
          "title": "Physics-informed regularization and structure preservation for learning stable reduced models from data with operator inference. (arXiv:2107.02597v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsirtsis_S/0/1/0/all/0/1\">Stratis Tsirtsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Abir De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1\">Manuel Gomez-Rodriguez</a>",
          "description": "Methods to find counterfactual explanations have predominantly focused on one\nstep decision making processes. In this work, we initiate the development of\nmethods to find counterfactual explanations for decision making processes in\nwhich multiple, dependent actions are taken sequentially over time. We start by\nformally characterizing a sequence of actions and states using finite horizon\nMarkov decision processes and the Gumbel-Max structural causal model. Building\nupon this characterization, we formally state the problem of finding\ncounterfactual explanations for sequential decision making processes. In our\nproblem formulation, the counterfactual explanation specifies an alternative\nsequence of actions differing in at most k actions from the observed sequence\nthat could have led the observed process realization to a better outcome. Then,\nwe introduce a polynomial time algorithm based on dynamic programming to build\na counterfactual policy that is guaranteed to always provide the optimal\ncounterfactual explanation on every possible realization of the counterfactual\nenvironment dynamics. We validate our algorithm using both synthetic and real\ndata from cognitive behavioral therapy and show that the counterfactual\nexplanations our algorithm finds can provide valuable insights to enhance\nsequential decision making under uncertainty.",
          "link": "http://arxiv.org/abs/2107.02776",
          "publishedOn": "2021-07-07T01:57:13.153Z",
          "wordCount": 637,
          "title": "Counterfactual Explanations in Sequential Decision Making Under Uncertainty. (arXiv:2107.02776v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_H/0/1/0/all/0/1\">Hasan Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morshed_M/0/1/0/all/0/1\">Mashrur Mahmud Morshed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Kamrul Hasan</a>",
          "description": "Any spatio-temporal movement or reorientation of the hand, done with the\nintention of conveying a specific meaning, can be considered as a hand gesture.\nInputs to hand gesture recognition systems can be in several forms, such as\ndepth images, monocular RGB, or skeleton joint points. We observe that raw\ndepth images possess low contrasts in the hand regions of interest (ROI). They\ndo not highlight important details to learn, such as finger bending information\n(whether a finger is overlapping the palm, or another finger). Recently, in\ndeep-learning--based dynamic hand gesture recognition, researchers are tying to\nfuse different input modalities (e.g. RGB or depth images and hand skeleton\njoint points) to improve the recognition accuracy. In this paper, we focus on\ndynamic hand gesture (DHG) recognition using depth quantized image features and\nhand skeleton joint points. In particular, we explore the effect of using\ndepth-quantized features in Convolutional Neural Network (CNN) and Recurrent\nNeural Network (RNN) based multi-modal fusion networks. We find that our method\nimproves existing results on the SHREC-DHG-14 dataset. Furthermore, using our\nmethod, we show that it is possible to reduce the resolution of the input\nimages by more than four times and still obtain comparable or better accuracy\nto that of the resolutions used in previous methods.",
          "link": "http://arxiv.org/abs/2107.02543",
          "publishedOn": "2021-07-07T01:57:13.134Z",
          "wordCount": 655,
          "title": "A deep-learning--based multimodal depth-aware dynamic hand gesture recognition system. (arXiv:2107.02543v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zineng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Since visual perception can give rich information beyond text descriptions\nfor world understanding, there has been increasing interest in leveraging\nvisual grounding for language learning. Recently, vokenization has attracted\nattention by using the predictions of a text-to-image retrieval model as labels\nfor language model supervision. Despite its success, the method suffers from\napproximation error of using finite image labels and the lack of vocabulary\ndiversity of a small image-text dataset. To overcome these limitations, we\npresent VidLanKD, a video-language knowledge distillation method for improving\nlanguage understanding. We train a multi-modal teacher model on a video-text\ndataset, and then transfer its knowledge to a student language model with a\ntext dataset. To avoid approximation error, we propose to use different\nknowledge distillation objectives. In addition, the use of a large-scale\nvideo-text dataset helps learn diverse and richer vocabularies. In our\nexperiments, VidLanKD achieves consistent improvements over text-only language\nmodels and vokenization models, on several downstream language understanding\ntasks including GLUE, SQuAD, and SWAG. We also demonstrate the improved world\nknowledge, physical reasoning, and temporal reasoning capabilities of our model\nby evaluating on the GLUE-diagnostics, PIQA, and TRACIE datasets. Lastly, we\npresent comprehensive ablation studies as well as visualizations of the learned\ntext-to-video grounding results of our teacher and student language models. Our\ncode and models are available at: https://github.com/zinengtang/VidLanKD",
          "link": "http://arxiv.org/abs/2107.02681",
          "publishedOn": "2021-07-07T01:57:13.097Z",
          "wordCount": 674,
          "title": "VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer. (arXiv:2107.02681v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Hyperspectral pansharpening aims to synthesize a low-resolution hyperspectral\nimage (LR-HSI) with a registered panchromatic image (PAN) to generate an\nenhanced HSI with high spectral and spatial resolution. Recently proposed HS\npansharpening methods have obtained remarkable results using deep convolutional\nnetworks (ConvNets), which typically consist of three steps: (1) up-sampling\nthe LR-HSI, (2) predicting the residual image via a ConvNet, and (3) obtaining\nthe final fused HSI by adding the outputs from first and second steps. Recent\nmethods have leveraged Deep Image Prior (DIP) to up-sample the LR-HSI due to\nits excellent ability to preserve both spatial and spectral information,\nwithout learning from large data sets. However, we observed that the quality of\nup-sampled HSIs can be further improved by introducing an additional\nspatial-domain constraint to the conventional spectral-domain energy function.\nWe define our spatial-domain constraint as the $L_1$ distance between the\npredicted PAN image and the actual PAN image. To estimate the PAN image of the\nup-sampled HSI, we also propose a learnable spectral response function (SRF).\nMoreover, we noticed that the residual image between the up-sampled HSI and the\nreference HSI mainly consists of edge information and very fine structures. In\norder to accurately estimate fine information, we propose a novel over-complete\nnetwork, called HyperKite, which focuses on learning high-level features by\nconstraining the receptive from increasing in the deep layers. We perform\nexperiments on three HSI datasets to demonstrate the superiority of our\nDIP-HyperKite over the state-of-the-art pansharpening methods. The deployment\ncodes, pre-trained models, and final fusion outputs of our DIP-HyperKite and\nthe methods used for the comparisons will be publicly made available at\nhttps://github.com/wgcban/DIP-HyperKite.git.",
          "link": "http://arxiv.org/abs/2107.02630",
          "publishedOn": "2021-07-07T01:57:13.081Z",
          "wordCount": 726,
          "title": "Hyperspectral Pansharpening Based on Improved Deep Image Prior and Residual Reconstruction. (arXiv:2107.02630v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02355",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hossen_M/0/1/0/all/0/1\">Md Abir Hossen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diwaka_P/0/1/0/all/0/1\">Prasoon K Diwaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ragi_S/0/1/0/all/0/1\">Shankarachary Ragi</a>",
          "description": "Measuring soil health indicators is an important and challenging task that\naffects farmers' decisions on timing, placement, and quantity of fertilizers\napplied in the farms. Most existing methods to measure soil health indicators\n(SHIs) are in-lab wet chemistry or spectroscopy-based methods, which require\nsignificant human input and effort, time-consuming, costly, and are\nlow-throughput in nature. To address this challenge, we develop an artificial\nintelligence (AI)-driven near real-time unmanned aerial vehicle (UAV)-based\nmultispectral sensing (UMS) solution to estimate total nitrogen (TN) of the\nsoil, an important macro-nutrient or SHI that directly affects the crop health.\nAccurate prediction of soil TN can significantly increase crop yield through\ninformed decision making on the timing of seed planting, and fertilizer\nquantity and timing. We train two machine learning models including multi-layer\nperceptron and support vector machine to predict the soil nitrogen using a\nsuite of data classes including multispectral characteristics of the soil and\ncrops in red, near-infrared, and green spectral bands, computed vegetation\nindices, and environmental variables including air temperature and relative\nhumidity. To generate the ground-truth data or the training data for the\nmachine learning models, we measure the total nitrogen of the soil samples\n(collected from a farm) using laser-induced breakdown spectroscopy (LIBS).",
          "link": "http://arxiv.org/abs/2107.02355",
          "publishedOn": "2021-07-07T01:57:13.036Z",
          "wordCount": 670,
          "title": "Total Nitrogen Estimation in Agricultural Soils via Aerial Multispectral Imaging and LIBS. (arXiv:2107.02355v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09223",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Plassier_V/0/1/0/all/0/1\">Vincent Plassier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Portier_F/0/1/0/all/0/1\">Fran&#xe7;ois Portier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Segers_J/0/1/0/all/0/1\">Johan Segers</a>",
          "description": "Consider the problem of learning a large number of response functions\nsimultaneously based on the same input variables. The training data consist of\na single independent random sample of the input variables drawn from a common\ndistribution together with the associated responses. The input variables are\nmapped into a high-dimensional linear space, called the feature space, and the\nresponse functions are modelled as linear functionals of the mapped features,\nwith coefficients calibrated via ordinary least squares. We provide convergence\nguarantees on the worst-case excess prediction risk by controlling the\nconvergence rate of the excess risk uniformly in the response function. The\ndimension of the feature map is allowed to tend to infinity with the sample\nsize. The collection of response functions, although potentially infinite, is\nsupposed to have a finite Vapnik-Chervonenkis dimension. The bound derived can\nbe applied when building multiple surrogate models in a reasonable computing\ntime.",
          "link": "http://arxiv.org/abs/2006.09223",
          "publishedOn": "2021-07-07T01:57:13.024Z",
          "wordCount": 615,
          "title": "Risk bounds when learning infinitely many response functions by ordinary linear regression. (arXiv:2006.09223v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.13645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arechiga_N/0/1/0/all/0/1\">Nikos Ar&#xe9;chiga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Best_A/0/1/0/all/0/1\">Andrew Best</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1\">Jyotirmoy Deshmukh</a>",
          "description": "Autonomous systems such as self-driving cars and general-purpose robots are\nsafety-critical systems that operate in highly uncertain and dynamic\nenvironments. We propose an interactive multi-agent framework where the\nsystem-under-design is modeled as an ego agent and its environment is modeled\nby a number of adversarial (ado) agents. For example, a self-driving car is an\nego agent whose behavior is influenced by ado agents such as pedestrians,\nbicyclists, traffic lights, road geometry etc. Given a logical specification of\nthe correct behavior of the ego agent, and a set of constraints that encode\nreasonable adversarial behavior, our framework reduces the adversarial testing\nproblem to the problem of synthesizing controllers for (constrained) ado agents\nthat cause the ego agent to violate its specifications. Specifically, we\nexplore the use of tabular and deep reinforcement learning approaches for\nsynthesizing adversarial agents. We show that ado agents trained in this\nfashion are better than traditional falsification or testing techniques because\nthey can generalize to ego agents and environments that differ from the\noriginal ego agent. We demonstrate the efficacy of our technique on two\nreal-world case studies from the domain of self-driving cars.",
          "link": "http://arxiv.org/abs/1910.13645",
          "publishedOn": "2021-07-07T01:57:13.018Z",
          "wordCount": 662,
          "title": "Automatic Testing With Reusable Adversarial Agents. (arXiv:1910.13645v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lianmin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>",
          "description": "The increasing size of neural network models has been critical for\nimprovements in their accuracy, but device memory is not growing at the same\nrate. This creates fundamental challenges for training neural networks within\nlimited memory environments. In this work, we propose ActNN, a memory-efficient\ntraining framework that stores randomly quantized activations for back\npropagation. We prove the convergence of ActNN for general network\narchitectures, and we characterize the impact of quantization on the\nconvergence via an exact expression for the gradient variance. Using our\ntheory, we propose novel mixed-precision quantization strategies that exploit\nthe activation's heterogeneity across feature dimensions, samples, and layers.\nThese techniques can be readily applied to existing dynamic graph frameworks,\nsuch as PyTorch, simply by substituting the layers. We evaluate ActNN on\nmainstream computer vision models for classification, detection, and\nsegmentation tasks. On all these tasks, ActNN compresses the activation to 2\nbits on average, with negligible accuracy loss. ActNN reduces the memory\nfootprint of the activation by 12x, and it enables training with a 6.6x to 14x\nlarger batch size.",
          "link": "http://arxiv.org/abs/2104.14129",
          "publishedOn": "2021-07-07T01:57:13.012Z",
          "wordCount": 667,
          "title": "ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training. (arXiv:2104.14129v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10333",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Elesedy_B/0/1/0/all/0/1\">Bryn Elesedy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zaidi_S/0/1/0/all/0/1\">Sheheryar Zaidi</a>",
          "description": "It is widely believed that engineering a model to be invariant/equivariant\nimproves generalisation. Despite the growing popularity of this approach, a\nprecise characterisation of the generalisation benefit is lacking. By\nconsidering the simplest case of linear models, this paper provides the first\nprovably non-zero improvement in generalisation for invariant/equivariant\nmodels when the target distribution is invariant/equivariant with respect to a\ncompact group. Moreover, our work reveals an interesting relationship between\ngeneralisation, the number of training examples and properties of the group\naction. Our results rest on an observation of the structure of function spaces\nunder averaging operators which, along with its consequences for feature\naveraging, may be of independent interest.",
          "link": "http://arxiv.org/abs/2102.10333",
          "publishedOn": "2021-07-07T01:57:12.996Z",
          "wordCount": 562,
          "title": "Provably Strict Generalisation Benefit for Equivariant Models. (arXiv:2102.10333v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sourav Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_Casillas_P/0/1/0/all/0/1\">Peter Rivera-Casillas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cecil_O/0/1/0/all/0/1\">Orie M. Cecil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farthing_M/0/1/0/all/0/1\">Matthew W. Farthing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perracchione_E/0/1/0/all/0/1\">Emma Perracchione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putti_M/0/1/0/all/0/1\">Mario Putti</a>",
          "description": "Model reduction for fluid flow simulation continues to be of great interest\nacross a number of scientific and engineering fields. In a previous work\n[arXiv:2104.13962], we explored the use of Neural Ordinary Differential\nEquations (NODE) as a non-intrusive method for propagating the latent-space\ndynamics in reduced order models. Here, we investigate employing deep\nautoencoders for discovering the reduced basis representation, the dynamics of\nwhich are then approximated by NODE. The ability of deep autoencoders to\nrepresent the latent-space is compared to the traditional proper orthogonal\ndecomposition (POD) approach, again in conjunction with NODE for capturing the\ndynamics. Additionally, we compare their behavior with two classical\nnon-intrusive methods based on POD and radial basis function interpolation as\nwell as dynamic mode decomposition. The test problems we consider include\nincompressible flow around a cylinder as well as a real-world application of\nshallow water hydrodynamics in an estuarine system. Our findings indicate that\ndeep autoencoders can leverage nonlinear manifold learning to achieve a highly\nefficient compression of spatial information and define a latent-space that\nappears to be more suitable for capturing the temporal dynamics through the\nNODE framework.",
          "link": "http://arxiv.org/abs/2107.02784",
          "publishedOn": "2021-07-07T01:57:12.990Z",
          "wordCount": 669,
          "title": "Data-driven reduced order modeling of environmental hydrodynamics using deep autoencoders and neural ODEs. (arXiv:2107.02784v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.14062",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Abbe_E/0/1/0/all/0/1\">Emmanuel Abbe</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_J/0/1/0/all/0/1\">Jianqing Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_K/0/1/0/all/0/1\">Kaizheng Wang</a>",
          "description": "Principal Component Analysis (PCA) is a powerful tool in statistics and\nmachine learning. While existing study of PCA focuses on the recovery of\nprincipal components and their associated eigenvalues, there are few precise\ncharacterizations of individual principal component scores that yield\nlow-dimensional embedding of samples. That hinders the analysis of various\nspectral methods. In this paper, we first develop an $\\ell_p$ perturbation\ntheory for a hollowed version of PCA in Hilbert spaces which provably improves\nupon the vanilla PCA in the presence of heteroscedastic noises. Through a novel\n$\\ell_p$ analysis of eigenvectors, we investigate entrywise behaviors of\nprincipal component score vectors and show that they can be approximated by\nlinear functionals of the Gram matrix in $\\ell_p$ norm, which includes $\\ell_2$\nand $\\ell_\\infty$ as special examples. For sub-Gaussian mixture models, the\nchoice of $p$ giving optimal bounds depends on the signal-to-noise ratio, which\nfurther yields optimality guarantees for spectral clustering. For contextual\ncommunity detection, the $\\ell_p$ theory leads to a simple spectral algorithm\nthat achieves the information threshold for exact recovery. These also provide\noptimal recovery results for Gaussian mixture and stochastic block models as\nspecial cases.",
          "link": "http://arxiv.org/abs/2006.14062",
          "publishedOn": "2021-07-07T01:57:12.972Z",
          "wordCount": 670,
          "title": "An $\\ell_p$ theory of PCA and spectral clustering. (arXiv:2006.14062v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.04216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Habibi_J/0/1/0/all/0/1\">Jafar Habibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazelinia_A/0/1/0/all/0/1\">Amir Fazelinia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annamoradnejad_I/0/1/0/all/0/1\">Issa Annamoradnejad</a>",
          "description": "In machine learning tasks, especially in the tasks of prediction, scientists\ntend to rely solely on available historical data and disregard unproven\ninsights, such as experts' opinions, polls, and betting odds. In this paper, we\npropose a general three-step framework for utilizing experts' insights in\nmachine learning tasks and build four concrete models for a sports game\nprediction case study. For the case study, we have chosen the task of\npredicting NCAA Men's Basketball games, which has been the focus of a group of\nKaggle competitions in recent years. Results highly suggest that the good\nperformance and high scores of the past models are a result of chance, and not\nbecause of a good-performing and stable model. Furthermore, our proposed models\ncan achieve more steady results with lower log loss average (best at 0.489)\ncompared to the top solutions of the 2019 competition (>0.503), and reach the\ntop 1%, 10% and 1% in the 2017, 2018 and 2019 leaderboards, respectively.",
          "link": "http://arxiv.org/abs/2008.04216",
          "publishedOn": "2021-07-07T01:57:12.965Z",
          "wordCount": 628,
          "title": "Using Experts' Opinions in Machine Learning Tasks. (arXiv:2008.04216v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moin_A/0/1/0/all/0/1\">Armin Moin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badii_A/0/1/0/all/0/1\">Atta Badii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Models are used in both the Software Engineering (SE) and the Artificial\nIntelligence (AI) communities. In the former case, models of software, which\nmay specify the software system architecture on different levels of abstraction\ncould be used in various stages of the Software Development Life-Cycle (SDLC),\nfrom early conceptualization and design, to verification, implementation,\ntesting and evolution. However, in the latter case, i.e., AI, models may\nprovide smart capabilities, such as prediction and decision making support. For\ninstance, in Machine Learning (ML), which is the most popular sub-discipline of\nAI at the present time, mathematical models may learn useful patterns in the\nobserved data instances and can become capable of making better predictions or\nrecommendations in the future. The goal of this work is to create synergy by\nbringing models in the said communities together and proposing a holistic\napproach. We illustrate how software models can become capable of producing or\ndealing with data analytics and ML models. The main focus is on the Internet of\nThings (IoT) and smart Cyber-Physical Systems (CPS) use cases, where both ML\nand model-driven (model-based) SE play a key role. In particular, we implement\nthe proposed approach in an open source prototype and validate it using two use\ncases from the IoT/CPS domain.",
          "link": "http://arxiv.org/abs/2107.02689",
          "publishedOn": "2021-07-07T01:57:12.939Z",
          "wordCount": 651,
          "title": "A Model-Driven Engineering Approach to Machine Learning and Software Modeling. (arXiv:2107.02689v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vaezipoor_P/0/1/0/all/0/1\">Pashootan Vaezipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Andrew Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Icarte_R/0/1/0/all/0/1\">Rodrigo Toro Icarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1\">Sheila McIlraith</a>",
          "description": "We address the problem of teaching a deep reinforcement learning (RL) agent\nto follow instructions in multi-task environments. Instructions are expressed\nin a well-known formal language -- linear temporal logic (LTL) -- and can\nspecify a diversity of complex, temporally extended behaviours, including\nconditionals and alternative realizations. Our proposed learning approach\nexploits the compositional syntax and the semantics of LTL, enabling our RL\nagent to learn task-conditioned policies that generalize to new instructions,\nnot observed during training. To reduce the overhead of learning LTL semantics,\nwe introduce an environment-agnostic LTL pretraining scheme which improves\nsample-efficiency in downstream environments. Experiments on discrete and\ncontinuous domains target combinatorial task sets of up to $\\sim10^{39}$ unique\ntasks and demonstrate the strength of our approach in learning to solve\n(unseen) tasks, given LTL instructions.",
          "link": "http://arxiv.org/abs/2102.06858",
          "publishedOn": "2021-07-07T01:57:12.923Z",
          "wordCount": 612,
          "title": "LTL2Action: Generalizing LTL Instructions for Multi-Task RL. (arXiv:2102.06858v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbera_G/0/1/0/all/0/1\">Giammarco La Barbera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussaid_H/0/1/0/all/0/1\">Haithem Boussaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belucci_B/0/1/0/all/0/1\">Bruno Belucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delmonte_A/0/1/0/all/0/1\">Alessandro Delmonte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulin_J/0/1/0/all/0/1\">Jeanne Goulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarnacki_S/0/1/0/all/0/1\">Sabine Sarnacki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouet_L/0/1/0/all/0/1\">Laurence Rouet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloch_I/0/1/0/all/0/1\">Isabelle Bloch</a>",
          "description": "Due to a high heterogeneity in pose and size and to a limited number of\navailable data, segmentation of pediatric images is challenging for deep\nlearning methods. In this work, we propose a new CNN architecture that is pose\nand scale invariant thanks to the use of Spatial Transformer Network (STN). Our\narchitecture is composed of three sequential modules that are estimated\ntogether during training: (i) a regression module to estimate a similarity\nmatrix to normalize the input image to a reference one; (ii) a differentiable\nmodule to find the region of interest to segment; (iii) a segmentation module,\nbased on the popular UNet architecture, to delineate the object. Unlike the\noriginal UNet, which strives to learn a complex mapping, including pose and\nscale variations, from a finite training dataset, our segmentation module\nlearns a simpler mapping focusing on images with normalized pose and size.\nFurthermore, the use of an automatic bounding box detection through STN allows\nsaving time and especially memory, while keeping similar performance. We test\nthe proposed method in kidney and renal tumor segmentation on abdominal\npediatric CT scanners. Results indicate that the estimated STN homogenization\nof size and pose accelerates the segmentation (25h), compared to standard\ndata-augmentation (33h), while obtaining a similar quality for the kidney\n(88.01\\% of Dice score) and improving the renal tumor delineation (from 85.52\\%\nto 87.12\\%).",
          "link": "http://arxiv.org/abs/2107.02655",
          "publishedOn": "2021-07-07T01:57:12.917Z",
          "wordCount": 712,
          "title": "Automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric segmentation. (arXiv:2107.02655v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akagunduz_E/0/1/0/all/0/1\">Erdem Akag&#xfc;nd&#xfc;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cifdaloz_O/0/1/0/all/0/1\">Oguzhan Cifdaloz</a>",
          "description": "In this paper, we investigate the parameter identification problem in\ndynamical systems through a deep learning approach. Focusing mainly on\nsecond-order, linear time-invariant dynamical systems, the topic of damping\nfactor identification is studied. By utilizing a six-layer deep neural network\nwith different recurrent cells, namely GRUs, LSTMs or BiLSTMs; and by feeding\ninput-output sequence pairs captured from a dynamical system simulator, we\nsearch for an effective deep recurrent architecture in order to resolve damping\nfactor identification problem. Our study results show that, although previously\nnot utilized for this task in the literature, bidirectional gated recurrent\ncells (BiLSTMs) provide better parameter identification results when compared\nto unidirectional gated recurrent memory cells such as GRUs and LSTM. Thus,\nindicating that an input-output sequence pair of finite length, collected from\na dynamical system and when observed anachronistically, may carry information\nin both time directions for prediction of a dynamical systems parameter.",
          "link": "http://arxiv.org/abs/2107.02427",
          "publishedOn": "2021-07-07T01:57:12.910Z",
          "wordCount": 600,
          "title": "Dynamical System Parameter Identification using Deep Recurrent Cell Networks. (arXiv:2107.02427v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.00478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Putatunda_S/0/1/0/all/0/1\">Sayan Putatunda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ubrangala_D/0/1/0/all/0/1\">Dayananda Ubrangala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rama_K/0/1/0/all/0/1\">Kiran Rama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondapalli_R/0/1/0/all/0/1\">Ravi Kondapalli</a>",
          "description": "In recent years, the concept of automated machine learning has become very\npopular. Automated Machine Learning (AutoML) mainly refers to the automated\nmethods for model selection and hyper-parameter optimization of various\nalgorithms such as random forests, gradient boosting, neural networks, etc. In\nthis paper, we introduce a new package i.e. DriveML for automated machine\nlearning. DriveML helps in implementing some of the pillars of an automated\nmachine learning pipeline such as automated data preparation, feature\nengineering, model building and model explanation by running the function\ninstead of writing lengthy R codes. The DriveML package is available in CRAN.\nWe compare the DriveML package with other relevant packages in CRAN/Github and\nfind that DriveML performs the best across different parameters. We also\nprovide an illustration by applying the DriveML package with default\nconfiguration on a real world dataset. Overall, the main benefits of DriveML\nare in development time savings, reduce developer's errors, optimal tuning of\nmachine learning models and reproducibility.",
          "link": "http://arxiv.org/abs/2005.00478",
          "publishedOn": "2021-07-07T01:57:12.903Z",
          "wordCount": 657,
          "title": "DriveML: An R Package for Driverless Machine Learning. (arXiv:2005.00478v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaixiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Soo-Hyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "Graph neural networks (GNNs) integrate deep architectures and topological\nstructure modeling in an effective way. However, the performance of existing\nGNNs would decrease significantly when they stack many layers, because of the\nover-smoothing issue. Node embeddings tend to converge to similar vectors when\nGNNs keep recursively aggregating the representations of neighbors. To enable\ndeep GNNs, several methods have been explored recently. But they are developed\nfrom either techniques in convolutional neural networks or heuristic\nstrategies. There is no generalizable and theoretical principle to guide the\ndesign of deep GNNs. To this end, we analyze the bottleneck of deep GNNs by\nleveraging the Dirichlet energy of node embeddings, and propose a generalizable\nprinciple to guide the training of deep GNNs. Based on it, a novel deep GNN\nframework -- EGNN is designed. It could provide lower and upper constraints in\nterms of Dirichlet energy at each layer to avoid over-smoothing. Experimental\nresults demonstrate that EGNN achieves state-of-the-art performance by using\ndeep layers.",
          "link": "http://arxiv.org/abs/2107.02392",
          "publishedOn": "2021-07-07T01:57:12.896Z",
          "wordCount": 598,
          "title": "Dirichlet Energy Constrained Learning for Deep Graph Neural Networks. (arXiv:2107.02392v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02751",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Sasdelli_M/0/1/0/all/0/1\">Michele Sasdelli</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>",
          "description": "Quantum annealing is a promising paradigm for building practical quantum\ncomputers. Compared to other approaches, quantum annealing technology has been\nscaled up to a larger number of qubits. On the other hand, deep learning has\nbeen profoundly successful in pushing the boundaries of AI. It is thus natural\nto investigate potentially game changing technologies such as quantum annealers\nto augment the capabilities of deep learning. In this work, we explore binary\nneural networks, which are lightweight yet powerful models typically intended\nfor resource constrained devices. Departing from current training regimes for\nbinary networks that smooth/approximate the activation functions to make the\nnetwork differentiable, we devise a quadratic unconstrained binary optimization\nformulation for the training problem. While the problem is intractable, i.e.,\nthe cost to estimate the binary weights scales exponentially with network size,\nwe show how the problem can be optimized directly on a quantum annealer,\nthereby opening up to the potential gains of quantum computing. We\nexperimentally validated our formulation via simulation and testing on an\nactual quantum annealer (D-Wave Advantage), the latter to the extent allowable\nby the capacity of current technology.",
          "link": "http://arxiv.org/abs/2107.02751",
          "publishedOn": "2021-07-07T01:57:12.881Z",
          "wordCount": 615,
          "title": "Quantum Annealing Formulation for Binary Neural Networks. (arXiv:2107.02751v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tengyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingbin Liang</a>",
          "description": "General Value Function (GVF) is a powerful tool to represent both the {\\em\npredictive} and {\\em retrospective} knowledge in reinforcement learning (RL).\nIn practice, often multiple interrelated GVFs need to be evaluated jointly with\npre-collected off-policy samples. In the literature, the gradient temporal\ndifference (GTD) learning method has been adopted to evaluate GVFs in the\noff-policy setting, but such an approach may suffer from a large estimation\nerror even if the function approximation class is sufficiently expressive.\nMoreover, none of the previous work have formally established the convergence\nguarantee to the ground truth GVFs under the function approximation settings.\nIn this paper, we address both issues through the lens of a class of GVFs with\ncausal filtering, which cover a wide range of RL applications such as reward\nvariance, value gradient, cost in anomaly detection, stationary distribution\ngradient, etc. We propose a new algorithm called GenTD for off-policy GVFs\nevaluation and show that GenTD learns multiple interrelated multi-dimensional\nGVFs as efficiently as a single canonical scalar value function. We further\nshow that unlike GTD, the learned GVFs by GenTD are guaranteed to converge to\nthe ground truth GVFs as long as the function approximation power is\nsufficiently large. To our best knowledge, GenTD is the first off-policy GVF\nevaluation algorithm that has global optimality guarantee.",
          "link": "http://arxiv.org/abs/2107.02711",
          "publishedOn": "2021-07-07T01:57:12.874Z",
          "wordCount": 662,
          "title": "A Unified Off-Policy Evaluation Approach for General Value Function. (arXiv:2107.02711v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moin_A/0/1/0/all/0/1\">Armin Moin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mituca_A/0/1/0/all/0/1\">Andrei Mituca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badii_A/0/1/0/all/0/1\">Atta Badii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "In this paper, we present the novel early tool prototype of ML-Quadrat, which\nis an open source research prototype, based on the Eclipse Modeling Framework\n(EMF) and the state of the art in the literature of Model-Driven Software\nEngineering (MDSE) for smart Cyber-Physical Systems (CPS) and the Internet of\nThings (IoT). Its envisioned users are mostly software developers, who might\nnot have deep knowledge and skills in the heterogeneous IoT platforms and the\ndiverse Artificial Intelligence (AI) technologies, specifically regarding Data\nAnalytics and Machine Learning (DAML). ML-Quadrat is released under the terms\nof the Apache 2.0 license on Github: https://github.com/arminmoin/ML-Quadrat.\nAdditionally, the novel early tool prototype of DriotData, a Low-Code platform\ntargeting citizen data scientists and citizen/end-user software developers is\ndemonstrated. DriotData exploits and adopts ML-Quadrat and offers an extended\nversion of it as a web-based service to companies, especially Small- and\nMedium-Sized Enterprises (SME). A basic web-based demo of the Minimum Viable\nProduct (MVP) of DriotData is already available. Finally, a short video\ndemonstrating the tools is available on YouTube: https://youtu.be/YCNFfhmy_JY.",
          "link": "http://arxiv.org/abs/2107.02692",
          "publishedOn": "2021-07-07T01:57:12.868Z",
          "wordCount": 630,
          "title": "ML-Quadrat & DriotData: A Model-Driven Engineering Tool and a Low-Code Platform for Smart IoT Services. (arXiv:2107.02692v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1\">S&#xf6;ren Mindermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzak_M/0/1/0/all/0/1\">Muhammed Razzak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Winnie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1\">Andreas Kirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mrinank Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1\">Adrien Morisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1\">Aidan N. Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1\">Sebastian Farquhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>",
          "description": "We introduce Goldilocks Selection, a technique for faster model training\nwhich selects a sequence of training points that are \"just right\". We propose\nan information-theoretic acquisition function -- the reducible validation loss\n-- and compute it with a small proxy model -- GoldiProx -- to efficiently\nchoose training points that maximize information about a validation set. We\nshow that the \"hard\" (e.g. high loss) points usually selected in the\noptimization literature are typically noisy, while the \"easy\" (e.g. low noise)\nsamples often prioritized for curriculum learning confer less information.\nFurther, points with uncertain labels, typically targeted by active learning,\ntend to be less relevant to the task. In contrast, Goldilocks Selection chooses\npoints that are \"just right\" and empirically outperforms the above approaches.\nMoreover, the selected sequence can transfer to other architectures;\npractitioners can share and reuse it without the need to recreate it.",
          "link": "http://arxiv.org/abs/2107.02565",
          "publishedOn": "2021-07-07T01:57:12.862Z",
          "wordCount": 613,
          "title": "Prioritized training on points that are learnable, worth learning, and not yet learned. (arXiv:2107.02565v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Van-Dinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzinotas_S/0/1/0/all/0/1\">Symeon Chatzinotas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ottersten_B/0/1/0/all/0/1\">Bjorn Ottersten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_T/0/1/0/all/0/1\">Trung Q. Duong</a>",
          "description": "Federated learning (FL) is capable of performing large distributed machine\nlearning tasks across multiple edge users by periodically aggregating trained\nlocal parameters. To address key challenges of enabling FL over a wireless\nfog-cloud system (e.g., non-i.i.d. data, users' heterogeneity), we first\npropose an efficient FL algorithm (called FedFog) to perform the local\naggregation of gradient parameters at fog servers and global training update at\nthe cloud. Next, we employ FedFog in wireless fog-cloud systems by\ninvestigating a novel network-aware FL optimization problem that strikes the\nbalance between the global loss and completion time. An iterative algorithm is\nthen developed to obtain a precise measurement of the system performance, which\nhelps design an efficient stopping criteria to output an appropriate number of\nglobal rounds. To mitigate the straggler effect, we propose a flexible user\naggregation strategy that trains fast users first to obtain a certain level of\naccuracy before allowing slow users to join the global training updates.\nExtensive numerical results using several real-world FL tasks are provided to\nverify the theoretical convergence of FedFog. We also show that the proposed\nco-design of FL and communication is essential to substantially improve\nresource utilization while achieving comparable accuracy of the learning model.",
          "link": "http://arxiv.org/abs/2107.02755",
          "publishedOn": "2021-07-07T01:57:12.854Z",
          "wordCount": 673,
          "title": "FedFog: Network-Aware Optimization of Federated Learning over Wireless Fog-Cloud Systems. (arXiv:2107.02755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kunar_A/0/1/0/all/0/1\">Aditya Kunar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1\">Robert Birke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lydia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zilong Zhao</a>",
          "description": "Tabular generative adversarial networks (TGAN) have recently emerged to cater\nto the need of synthesizing tabular data -- the most widely used data format.\nWhile synthetic tabular data offers the advantage of complying with privacy\nregulations, there still exists a risk of privacy leakage via inference attacks\ndue to interpolating the properties of real data during training. Differential\nprivate (DP) training algorithms provide theoretical guarantees for training\nmachine learning models by injecting statistical noise to prevent privacy\nleaks. However, the challenges of applying DP on TGAN are to determine the most\noptimal framework (i.e., PATE/DP-SGD) and neural network (i.e.,\nGenerator/Discriminator)to inject noise such that the data utility is well\nmaintained under a given privacy guarantee. In this paper, we propose DTGAN, a\nnovel conditional Wasserstein tabular GAN that comes in two variants DTGAN_G\nand DTGAN_D, for providing a detailed comparison of tabular GANs trained using\nDP-SGD for the generator vs discriminator, respectively. We elicit the privacy\nanalysis associated with training the generator with complex loss functions\n(i.e., classification and information losses) needed for high quality tabular\ndata synthesis. Additionally, we rigorously evaluate the theoretical privacy\nguarantees offered by DP empirically against membership and attribute inference\nattacks. Our results on 3 datasets show that the DP-SGD framework is superior\nto PATE and that a DP discriminator is more optimal for training convergence.\nThus, we find (i) DTGAN_D is capable of maintaining the highest data utility\nacross 4 ML models by up to 18% in terms of the average precision score for a\nstrict privacy budget, epsilon = 1, as compared to the prior studies and (ii)\nDP effectively prevents privacy loss against inference attacks by restricting\nthe success probability of membership attacks to be close to 50%.",
          "link": "http://arxiv.org/abs/2107.02521",
          "publishedOn": "2021-07-07T01:57:12.840Z",
          "wordCount": 728,
          "title": "DTGAN: Differential Private Training for Tabular GANs. (arXiv:2107.02521v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+huang_T/0/1/0/all/0/1\">Tianjin huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1\">Yulong Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>",
          "description": "Adversarial training is an approach for increasing model's resilience against\nadversarial perturbations. Such approaches have been demonstrated to result in\nmodels with feature representations that generalize better. However, limited\nworks have been done on adversarial training of models on graph data. In this\npaper, we raise such a question { does adversarial training improve the\ngeneralization of graph representations. We formulate L2 and L1 versions of\nadversarial training in two powerful node embedding methods: graph autoencoder\n(GAE) and variational graph autoencoder (VGAE). We conduct extensive\nexperiments on three main applications, i.e. link prediction, node clustering,\ngraph anomaly detection of GAE and VGAE, and demonstrate that both L2 and L1\nadversarial training boost the generalization of GAE and VGAE.",
          "link": "http://arxiv.org/abs/2107.02658",
          "publishedOn": "2021-07-07T01:57:12.834Z",
          "wordCount": 557,
          "title": "On Generalization of Graph Autoencoders with Adversarial Training. (arXiv:2107.02658v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_J/0/1/0/all/0/1\">Jos&#xe9; Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1\">Ra&#xed;ssa Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1\">Ronnie Alves</a>",
          "description": "Strategies based on Explainable Artificial Intelligence - XAI have emerged in\ncomputing to promote a better understanding of predictions made by black box\nmodels. Most XAI-based tools used today explain these types of models,\ngenerating attribute rankings aimed at explaining the same, that is, the\nanalysis of Attribute Importance. There is no consensus on which XAI tool\ngenerates a general rank of explainability, for this reason, several proposals\nfor tools have emerged (Ciu, Dalex, Eli5, Lofo, Shap and Skater). Here, we\npresent an experimental benchmark of explainable AI techniques capable of\nproducing model-agnostic global explainability ranks based on tabular data\nrelated to different problems. Seeking to answer questions such as \"Are the\nexplanations generated by the different tools the same, similar or different?\"\nand \"How does data complexity play along model explainability?\". The results\nfrom the construction of 82 computational models and 592 ranks give us some\nlight on the other side of the problem of explainability: dataset complexity!",
          "link": "http://arxiv.org/abs/2107.02661",
          "publishedOn": "2021-07-07T01:57:12.827Z",
          "wordCount": 597,
          "title": "Does Dataset Complexity Matters for Model Explainers?. (arXiv:2107.02661v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Biwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chaochao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magliacane_S/0/1/0/all/0/1\">Sara Magliacane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>",
          "description": "Most approaches in reinforcement learning (RL) are data-hungry and specific\nto fixed environments. In this paper, we propose a principled framework for\nadaptive RL, called AdaRL, that adapts reliably to changes across domains.\nSpecifically, we construct a generative environment model for the structural\nrelationships among variables in the system and embed the changes in a compact\nway, which provides a clear and interpretable picture for locating what and\nwhere the changes are and how to adapt. Based on the environment model, we\ncharacterize a minimal set of representations, including both domain-specific\nfactors and domain-shared state representations, that suffice for reliable and\nlow-cost transfer. Moreover, we show that by explicitly leveraging a compact\nrepresentation to encode changes, we can adapt the policy with only a few\nsamples without further policy optimization in the target domain. We illustrate\nthe efficacy of AdaRL through a series of experiments that allow for changes in\ndifferent components of Cartpole and Atari games.",
          "link": "http://arxiv.org/abs/2107.02729",
          "publishedOn": "2021-07-07T01:57:12.821Z",
          "wordCount": 603,
          "title": "AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning. (arXiv:2107.02729v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lang_J/0/1/0/all/0/1\">Jana Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giese_M/0/1/0/all/0/1\">Martin A. Giese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synofzik_M/0/1/0/all/0/1\">Matthis Synofzik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilg_W/0/1/0/all/0/1\">Winfried Ilg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otte_S/0/1/0/all/0/1\">Sebastian Otte</a>",
          "description": "Motor disturbances can affect the interaction with dynamic objects, such as\ncatching a ball. A classification of clinical catching trials might give\ninsight into the existence of pathological alterations in the relation of arm\nand ball movements. Accurate, but also early decisions are required to classify\na catching attempt before the catcher's first ball contact. To obtain\nclinically valuable results, a significant decision confidence of at least 75%\nis required. Hence, three competing objectives have to be optimized at the same\ntime: accuracy, earliness and decision-making confidence. Here we propose a\ncoupled classification and prediction approach for early time series\nclassification: a predictive, generative recurrent neural network (RNN)\nforecasts the next data points of ball trajectories based on already available\nobservations; a discriminative RNN continuously generates classification\nguesses based on the available data points and the unrolled sequence\npredictions. We compare our approach, which we refer to as predictive\nsequential classification (PSC), to state-of-the-art sequence learners,\nincluding various RNN and temporal convolutional network (TCN) architectures.\nOn this hard real-world task we can consistently demonstrate the superiority of\nPSC over all other models in terms of accuracy and confidence with respect to\nearliness of recognition. Specifically, PSC is able to confidently classify the\nsuccess of catching trials as early as 123 milliseconds before the first ball\ncontact. We conclude that PSC is a promising approach for early time series\nclassification, when accurate and confident decisions are required.",
          "link": "http://arxiv.org/abs/2107.02442",
          "publishedOn": "2021-07-07T01:57:12.815Z",
          "wordCount": 692,
          "title": "Early Recognition of Ball Catching Success in Clinical Trials with RNN-Based Predictive Classification. (arXiv:2107.02442v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deka_D/0/1/0/all/0/1\">Deepjyoti Deka</a>",
          "description": "The rapid growth of distributed energy resources potentially increases power\ngrid instability. One promising strategy is to employ data in power grids to\nefficiently respond to abnormal events (e.g., faults) by detection and\nlocation. Unfortunately, most existing works lack physical interpretation and\nare vulnerable to the practical challenges: sparse observation, insufficient\nlabeled datasets, and stochastic environment. We propose a physics-informed\ngraph learning framework of two stages to handle these challenges when locating\nfaults. Stage- I focuses on informing a graph neural network (GNN) with the\ngeometrical structure of power grids; stage-II employs the physical similarity\nof labeled and unlabeled data samples to improve the location accuracy. We\nprovide a random walk-based the underpinning of designing our GNNs to address\nthe challenge of sparse observation and augment the correct prediction\nprobability. We compare our approach with three baselines in the IEEE 123-node\nbenchmark system, showing that the proposed method outperforms the others by\nsignificant margins, especially when label rates are low. Also, we validate the\nrobustness of our algorithms to out-of-distribution-data (ODD) due to topology\nchanges and load variations. Additionally, we adapt our graph learning\nframework to the IEEE 37-node test feeder and show high location performance\nwith the proposed training strategy.",
          "link": "http://arxiv.org/abs/2107.02275",
          "publishedOn": "2021-07-07T01:57:12.800Z",
          "wordCount": 644,
          "title": "Physics-Informed Graph Learning for Robust Fault Location in Distribution Systems. (arXiv:2107.02275v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maiti_A/0/1/0/all/0/1\">Aurghya Maiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1\">Vineet Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_G/0/1/0/all/0/1\">Gaurav Sinha</a>",
          "description": "We study the problem of determining the best intervention in a Causal\nBayesian Network (CBN) specified only by its causal graph. We model this as a\nstochastic multi-armed bandit (MAB) problem with side-information, where the\ninterventions correspond to the arms of the bandit instance. First, we propose\na simple regret minimization algorithm that takes as input a semi-Markovian\ncausal graph with atomic interventions and possibly unobservable variables, and\nachieves $\\tilde{O}(\\sqrt{M/T})$ expected simple regret, where $M$ is dependent\non the input CBN and could be very small compared to the number of arms. We\nalso show that this is almost optimal for CBNs described by causal graphs\nhaving an $n$-ary tree structure. Our simple regret minimization results, both\nupper and lower bound, subsume previous results in the literature, which\nassumed additional structural restrictions on the input causal graph. In\nparticular, our results indicate that the simple regret guarantee of our\nproposed algorithm can only be improved by considering more nuanced structural\nrestrictions on the causal graph. Next, we propose a cumulative regret\nminimization algorithm that takes as input a general causal graph with all\nobservable nodes and atomic interventions and performs better than the optimal\nMAB algorithm that does not take causal side-information into account. We also\nexperimentally compare both our algorithms with the best known algorithms in\nthe literature. To the best of our knowledge, this work gives the first simple\nand cumulative regret minimization algorithms for CBNs with general causal\ngraphs under atomic interventions and having unobserved confounders.",
          "link": "http://arxiv.org/abs/2107.02772",
          "publishedOn": "2021-07-07T01:57:12.794Z",
          "wordCount": 677,
          "title": "Causal Bandits on General Graphs. (arXiv:2107.02772v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02520",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Daesung Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hoon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Seok-Hwan Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_S/0/1/0/all/0/1\">Seung-Eun Hong</a>",
          "description": "Cooperative beamforming across access points (APs) and fronthaul quantization\nstrategies are essential for cloud radio access network (C-RAN) systems. The\nnonconvexity of the C-RAN optimization problems, which is stemmed from per-AP\npower and fronthaul capacity constraints, requires high computational\ncomplexity for executing iterative algorithms. To resolve this issue, we\ninvestigate a deep learning approach where the optimization module is replaced\nwith a well-trained deep neural network (DNN). An efficient learning solution\nis proposed which constructs a DNN to produce a low-dimensional representation\nof optimal beamforming and quantization strategies. Numerical results validate\nthe advantages of the proposed learning solution.",
          "link": "http://arxiv.org/abs/2107.02520",
          "publishedOn": "2021-07-07T01:57:12.788Z",
          "wordCount": 568,
          "title": "Deep Learning Methods for Joint Optimization of Beamforming and Fronthaul Quantization in Cloud Radio Access Networks. (arXiv:2107.02520v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karppa_M/0/1/0/all/0/1\">Matti Karppa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aumuller_M/0/1/0/all/0/1\">Martin Aum&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagh_R/0/1/0/all/0/1\">Rasmus Pagh</a>",
          "description": "Kernel Density Estimation (KDE) is a nonparametric method for estimating the\nshape of a density function, given a set of samples from the distribution.\nRecently, locality-sensitive hashing, originally proposed as a tool for nearest\nneighbor search, has been shown to enable fast KDE data structures. However,\nthese approaches do not take advantage of the many other advances that have\nbeen made in algorithms for nearest neighbor algorithms. We present an\nalgorithm called Density Estimation from Approximate Nearest Neighbors (DEANN)\nwhere we apply Approximate Nearest Neighbor (ANN) algorithms as a black box\nsubroutine to compute an unbiased KDE. The idea is to find points that have a\nlarge contribution to the KDE using ANN, compute their contribution exactly,\nand approximate the remainder with Random Sampling (RS). We present a\ntheoretical argument that supports the idea that an ANN subroutine can speed up\nthe evaluation. Furthermore, we provide a C++ implementation with a Python\ninterface that can make use of an arbitrary ANN implementation as a subroutine\nfor KDE evaluation. We show empirically that our implementation outperforms\nstate of the art implementations in all high dimensional datasets we\nconsidered, and matches the performance of RS in cases where the ANN yield no\ngains in performance.",
          "link": "http://arxiv.org/abs/2107.02736",
          "publishedOn": "2021-07-07T01:57:12.782Z",
          "wordCount": 651,
          "title": "DEANN: Speeding up Kernel-Density Estimation using Approximate Nearest Neighbor Search. (arXiv:2107.02736v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganev_I/0/1/0/all/0/1\">Iordan Ganev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1\">Robin Walters</a>",
          "description": "We provide a theoretical framework for neural networks in terms of the\nrepresentation theory of quivers, thus revealing symmetries of the parameter\nspace of neural networks. An exploitation of these symmetries leads to a model\ncompression algorithm for radial neural networks based on an analogue of the QR\ndecomposition. A projected version of backpropogation on the original model\nmatches usual backpropogation on the compressed model.",
          "link": "http://arxiv.org/abs/2107.02550",
          "publishedOn": "2021-07-07T01:57:12.776Z",
          "wordCount": 496,
          "title": "The QR decomposition for radial neural networks. (arXiv:2107.02550v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilager_S/0/1/0/all/0/1\">Shashikant Ilager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buyya_R/0/1/0/all/0/1\">Rajkumar Buyya</a>",
          "description": "This paper investigates the existing resource management approaches in Cloud\nData Centres for energy and thermal efficiency. It identifies the need for\nintegrated computing and cooling systems management and learning-based\nsolutions in resource management systems. A taxonomy on energy and thermal\nefficient resource management in data centres is proposed based on an in-depth\nanalysis of the literature. Furthermore, a detailed survey on existing\napproaches is conducted according to the taxonomy and recent advancements\nincluding machine learning-based resource management approaches and cooling\nmanagement technologies are discussed.",
          "link": "http://arxiv.org/abs/2107.02342",
          "publishedOn": "2021-07-07T01:57:12.769Z",
          "wordCount": 546,
          "title": "Energy and Thermal-aware Resource Management of Cloud Data Centres: A Taxonomy and Future Directions. (arXiv:2107.02342v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zixia Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>",
          "description": "This paper describes the system used in submission from SHANGHAITECH team to\nthe IWPT 2021 Shared Task. Our system is a graph-based parser with the\ntechnique of Automated Concatenation of Embeddings (ACE). Because recent work\nfound that better word representations can be obtained by concatenating\ndifferent types of embeddings, we use ACE to automatically find the better\nconcatenation of embeddings for the task of enhanced universal dependencies.\nAccording to official results averaged on 17 languages, our system ranks 2nd\nover 9 teams.",
          "link": "http://arxiv.org/abs/2107.02416",
          "publishedOn": "2021-07-07T01:57:12.754Z",
          "wordCount": 529,
          "title": "Enhanced Universal Dependency Parsing with Automated Concatenation of Embeddings. (arXiv:2107.02416v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuzi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "While recent text to speech (TTS) models perform very well in synthesizing\nreading-style (e.g., audiobook) speech, it is still challenging to synthesize\nspontaneous-style speech (e.g., podcast or conversation), mainly because of two\nreasons: 1) the lack of training data for spontaneous speech; 2) the difficulty\nin modeling the filled pauses (um and uh) and diverse rhythms in spontaneous\nspeech. In this paper, we develop AdaSpeech 3, an adaptive TTS system that\nfine-tunes a well-trained reading-style TTS model for spontaneous-style speech.\nSpecifically, 1) to insert filled pauses (FP) in the text sequence\nappropriately, we introduce an FP predictor to the TTS model; 2) to model the\nvarying rhythms, we introduce a duration predictor based on mixture of experts\n(MoE), which contains three experts responsible for the generation of fast,\nmedium and slow speech respectively, and fine-tune it as well as the pitch\npredictor for rhythm adaptation; 3) to adapt to other speaker timbre, we\nfine-tune some parameters in the decoder with few speech data. To address the\nchallenge of lack of training data, we mine a spontaneous speech dataset to\nsupport our research this work and facilitate future research on spontaneous\nTTS. Experiments show that AdaSpeech 3 synthesizes speech with natural FP and\nrhythms in spontaneous styles, and achieves much better MOS and SMOS scores\nthan previous adaptive TTS systems.",
          "link": "http://arxiv.org/abs/2107.02530",
          "publishedOn": "2021-07-07T01:57:12.749Z",
          "wordCount": 679,
          "title": "AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style. (arXiv:2107.02530v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vysogorets_A/0/1/0/all/0/1\">Artem Vysogorets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kempe_J/0/1/0/all/0/1\">Julia Kempe</a>",
          "description": "Neural network pruning is a fruitful area of research with surging interest\nin high sparsity regimes. Benchmarking in this domain heavily relies on\nfaithful representation of the sparsity of subnetworks, which has been\ntraditionally computed as the fraction of removed connections (direct\nsparsity). This definition, however, fails to recognize unpruned parameters\nthat detached from input or output layers of underlying subnetworks,\npotentially underestimating actual effective sparsity: the fraction of\ninactivated connections. While this effect might be negligible for moderately\npruned networks (up to 10-100 compression rates), we find that it plays an\nincreasing role for thinner subnetworks, greatly distorting comparison between\ndifferent pruning algorithms. For example, we show that effective compression\nof a randomly pruned LeNet-300-100 can be orders of magnitude larger than its\ndirect counterpart, while no discrepancy is ever observed when using SynFlow\nfor pruning [Tanaka et al., 2020]. In this work, we adopt the lens of effective\nsparsity to reevaluate several recent pruning algorithms on common benchmark\narchitectures (e.g., LeNet-300-100, VGG-19, ResNet-18) and discover that their\nabsolute and relative performance changes dramatically in this new and more\nappropriate framework. To aim for effective, rather than direct, sparsity, we\ndevelop a low-cost extension to most pruning algorithms. Further, equipped with\neffective sparsity as a reference frame, we partially reconfirm that random\npruning with appropriate sparsity allocation across layers performs as well or\nbetter than more sophisticated algorithms for pruning at initialization [Su et\nal., 2020]. In response to this observation, using a simple analogy of pressure\ndistribution in coupled cylinders from physics, we design novel layerwise\nsparsity quotas that outperform all existing baselines in the context of random\npruning.",
          "link": "http://arxiv.org/abs/2107.02306",
          "publishedOn": "2021-07-07T01:57:12.743Z",
          "wordCount": 710,
          "title": "Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity. (arXiv:2107.02306v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weiwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiayun Luo</a>",
          "description": "Drought is a serious natural disaster that has a long duration and a wide\nrange of influence. To decrease the drought-caused losses, drought prediction\nis the basis of making the corresponding drought prevention and disaster\nreduction measures. While this problem has been studied in the literature, it\nremains unknown whether drought can be precisely predicted or not with machine\nlearning models using weather data. To answer this question, a real-world\npublic dataset is leveraged in this study and different drought levels are\npredicted using the last 90 days of 18 meteorological indicators as the\npredictors. In a comprehensive approach, 16 machine learning models and 16 deep\nlearning models are evaluated and compared. The results show no single model\ncan achieve the best performance for all evaluation metrics simultaneously,\nwhich indicates the drought prediction problem is still challenging. As\nbenchmarks for further studies, the code and results are publicly available in\na Github repository.",
          "link": "http://arxiv.org/abs/2107.02517",
          "publishedOn": "2021-07-07T01:57:12.737Z",
          "wordCount": 603,
          "title": "An Evaluation of Machine Learning and Deep Learning Models for Drought Prediction using Weather Data. (arXiv:2107.02517v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arjevani_Y/0/1/0/all/0/1\">Yossi Arjevani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_M/0/1/0/all/0/1\">Michael Field</a>",
          "description": "Motivated by questions originating from the study of a class of shallow\nstudent-teacher neural networks, methods are developed for the analysis of\nspurious minima in classes of gradient equivariant dynamics related to neural\nnets. In the symmetric case, methods depend on the generic equivariant\nbifurcation theory of irreducible representations of the symmetric group on $n$\nsymbols, $S_n$; in particular, the standard representation of $S_n$. It is\nshown that spurious minima do not arise from spontaneous symmetry breaking but\nrather through a complex deformation of the landscape geometry that can be\nencoded by a generic $S_n$-equivariant bifurcation. We describe minimal models\nfor forced symmetry breaking that give a lower bound on the dynamic complexity\ninvolved in the creation of spurious minima when there is no symmetry. Results\non generic bifurcation when there are quadratic equivariants are also proved;\nthis work extends and clarifies results of Ihrig & Golubitsky and Chossat,\nLauterback & Melbourne on the instability of solutions when there are quadratic\nequivariants.",
          "link": "http://arxiv.org/abs/2107.02422",
          "publishedOn": "2021-07-07T01:57:12.722Z",
          "wordCount": 605,
          "title": "Equivariant bifurcation, quadratic equivariants, and symmetry breaking for the standard representation of $S_n$. (arXiv:2107.02422v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Charles Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemay_A/0/1/0/all/0/1\">Andreanne Lemay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>",
          "description": "As machine learning (ML) continue to be integrated into healthcare systems\nthat affect clinical decision making, new strategies will need to be\nincorporated in order to effectively detect and evaluate subgroup disparities\nto ensure accountability and generalizability in clinical workflows. In this\npaper, we explore how epistemic uncertainty can be used to evaluate disparity\nin patient demographics (race) and data acquisition (scanner) subgroups for\nbreast density assessment on a dataset of 108,190 mammograms collected from 33\nclinical sites. Our results show that even if aggregate performance is\ncomparable, the choice of uncertainty quantification metric can significantly\nthe subgroup level. We hope this analysis can promote further work on how\nuncertainty can be leveraged to increase transparency of machine learning\napplications for clinical deployment.",
          "link": "http://arxiv.org/abs/2107.02716",
          "publishedOn": "2021-07-07T01:57:12.713Z",
          "wordCount": 574,
          "title": "Evaluating subgroup disparity using epistemic uncertainty in mammography. (arXiv:2107.02716v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuntian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Baekjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ambuj Tewari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1\">Ness Shroff</a>",
          "description": "In this paper, we consider the Gaussian process (GP) bandit optimization\nproblem in a non-stationary environment. To capture external changes, the\nblack-box function is allowed to be time-varying within a reproducing kernel\nHilbert space (RKHS). To this end, we develop WGP-UCB, a novel UCB-type\nalgorithm based on weighted Gaussian process regression. A key challenge is how\nto cope with infinite-dimensional feature maps. To that end, we leverage kernel\napproximation techniques to prove a sublinear regret bound, which is the first\n(frequentist) sublinear regret guarantee on weighted time-varying bandits with\ngeneral nonlinear rewards. This result generalizes both non-stationary linear\nbandits and standard GP-UCB algorithms. Further, a novel concentration\ninequality is achieved for weighted Gaussian process regression with general\nweights. We also provide universal upper bounds and weight-dependent upper\nbounds for weighted maximum information gains. These results are potentially of\nindependent interest for applications such as news ranking and adaptive\npricing, where weights can be adopted to capture the importance or quality of\ndata. Finally, we conduct experiments to highlight the favorable gains of the\nproposed algorithm in many cases when compared to existing methods.",
          "link": "http://arxiv.org/abs/2107.02371",
          "publishedOn": "2021-07-07T01:57:12.707Z",
          "wordCount": 619,
          "title": "Weighted Gaussian Process Bandits for Non-stationary Environments. (arXiv:2107.02371v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02586",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Remerscheid_N/0/1/0/all/0/1\">Nicolas Remerscheid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus Makowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1\">Rickmer Braren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "Collaborative machine learning techniques such as federated learning (FL)\nenable the training of models on effectively larger datasets without data\ntransfer. Recent initiatives have demonstrated that segmentation models trained\nwith FL can achieve performance similar to locally trained models. However, FL\nis not a fully privacy-preserving technique and privacy-centred attacks can\ndisclose confidential patient data. Thus, supplementing FL with\nprivacy-enhancing technologies (PTs) such as differential privacy (DP) is a\nrequirement for clinical applications in a multi-institutional setting. The\napplication of PTs to FL in medical imaging and the trade-offs between privacy\nguarantees and model utility, the ramifications on training performance and the\nsusceptibility of the final models to attacks have not yet been conclusively\ninvestigated. Here we demonstrate the first application of differentially\nprivate gradient descent-based FL on the task of semantic segmentation in\ncomputed tomography. We find that high segmentation performance is possible\nunder strong privacy guarantees with an acceptable training time penalty. We\nfurthermore demonstrate the first successful gradient-based model inversion\nattack on a semantic segmentation model and show that the application of DP\nprevents it from divulging sensitive image features.",
          "link": "http://arxiv.org/abs/2107.02586",
          "publishedOn": "2021-07-07T01:57:12.699Z",
          "wordCount": 655,
          "title": "Differentially private federated deep learning for multi-site medical image segmentation. (arXiv:2107.02586v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiulong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1\">Martin Takac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunderraman_R/0/1/0/all/0/1\">Rajshekhar Sunderraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>",
          "description": "The goal of text-to-image synthesis is to generate a visually realistic image\nthat matches a given text description. In practice, the captions annotated by\nhumans for the same image have large variance in terms of contents and the\nchoice of words. The linguistic discrepancy between the captions of the\nidentical image leads to the synthetic images deviating from the ground truth.\nTo address this issue, we propose a contrastive learning approach to improve\nthe quality and enhance the semantic consistency of synthetic images. In the\npre-training stage, we utilize the contrastive learning approach to learn the\nconsistent textual representations for the captions corresponding to the same\nimage. Furthermore, in the following stage of GAN training, we employ the\ncontrastive learning method to enhance the consistency between the generated\nimages from the captions related to the same image. We evaluate our approach\nover two popular text-to-image synthesis models, AttnGAN and DM-GAN, on\ndatasets CUB and COCO, respectively. Experimental results have shown that our\napproach can effectively improve the quality of synthetic images in terms of\nthree metrics: IS, FID and R-precision. Especially, on the challenging COCO\ndataset, our approach boosts the FID significantly by 29.60% over AttnGAn and\nby 21.96% over DM-GAN.",
          "link": "http://arxiv.org/abs/2107.02423",
          "publishedOn": "2021-07-07T01:57:12.687Z",
          "wordCount": 628,
          "title": "Improving Text-to-Image Synthesis Using Contrastive Learning. (arXiv:2107.02423v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_P/0/1/0/all/0/1\">Pengpeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_F/0/1/0/all/0/1\">Feihu Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guohua Yang</a>",
          "description": "Graph representation learning has attracted a surge of interest recently,\nwhose target at learning discriminant embedding for each node in the graph.\nMost of these representation methods focus on supervised learning and heavily\ndepend on label information. However, annotating graphs are expensive to obtain\nin the real world, especially in specialized domains (i.e. biology), as it\nneeds the annotator to have the domain knowledge to label the graph. To\napproach this problem, self-supervised learning provides a feasible solution\nfor graph representation learning. In this paper, we propose a Multi-Level\nGraph Contrastive Learning (MLGCL) framework for learning robust representation\nof graph data by contrasting space views of graphs. Specifically, we introduce\na novel contrastive view - topological and feature space views. The original\ngraph is first-order approximation structure and contains uncertainty or error,\nwhile the $k$NN graph generated by encoding features preserves high-order\nproximity. Thus $k$NN graph generated by encoding features not only provide a\ncomplementary view, but is more suitable to GNN encoder to extract discriminant\nrepresentation. Furthermore, we develop a multi-level contrastive mode to\npreserve the local similarity and semantic similarity of graph-structured data\nsimultaneously. Extensive experiments indicate MLGCL achieves promising results\ncompared with the existing state-of-the-art graph representation learning\nmethods on seven datasets.",
          "link": "http://arxiv.org/abs/2107.02639",
          "publishedOn": "2021-07-07T01:57:12.681Z",
          "wordCount": 635,
          "title": "Multi-Level Graph Contrastive Learning. (arXiv:2107.02639v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_R/0/1/0/all/0/1\">Ricardo Luna Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonetti_M/0/1/0/all/0/1\">Matteo Leonetti</a>",
          "description": "In Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of\ntasks to prepare for and learn faster in new, unseen, but related tasks. The\ntraining tasks are usually hand-crafted to be representative of the expected\ndistribution of test tasks and hence all used in training. We show that given a\nset of training tasks, learning can be both faster and more effective (leading\nto better performance in the test tasks), if the training tasks are\nappropriately selected. We propose a task selection algorithm,\nInformation-Theoretic Task Selection (ITTS), based on information theory, which\noptimizes the set of tasks used for training in meta-RL, irrespectively of how\nthey are generated. The algorithm establishes which training tasks are both\nsufficiently relevant for the test tasks, and different enough from one\nanother. We reproduce different meta-RL experiments from the literature and\nshow that ITTS improves the final performance in all of them.",
          "link": "http://arxiv.org/abs/2107.02603",
          "publishedOn": "2021-07-07T01:57:12.667Z",
          "wordCount": 578,
          "title": "Meta-Reinforcement Learning for Heuristic Planning. (arXiv:2107.02603v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02293",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tayebi_R/0/1/0/all/0/1\">Rohollah Moosavi Tayebi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mu_Y/0/1/0/all/0/1\">Youqing Mu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dehkharghanian_T/0/1/0/all/0/1\">Taher Dehkharghanian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ross_C/0/1/0/all/0/1\">Catherine Ross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sur_M/0/1/0/all/0/1\">Monalisa Sur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Foley_R/0/1/0/all/0/1\">Ronan Foley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1\">Hamid R. Tizhoosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campbell_C/0/1/0/all/0/1\">Clinton JV Campbell</a>",
          "description": "Bone marrow cytology is required to make a hematological diagnosis,\ninfluencing critical clinical decision points in hematology. However, bone\nmarrow cytology is tedious, limited to experienced reference centers and\nassociated with high inter-observer variability. This may lead to a delayed or\nincorrect diagnosis, leaving an unmet need for innovative supporting\ntechnologies. We have developed the first ever end-to-end deep learning-based\ntechnology for automated bone marrow cytology. Starting with a bone marrow\naspirate digital whole slide image, our technology rapidly and automatically\ndetects suitable regions for cytology, and subsequently identifies and\nclassifies all bone marrow cells in each region. This collective\ncytomorphological information is captured in a novel representation called\nHistogram of Cell Types (HCT) quantifying bone marrow cell class probability\ndistribution and acting as a cytological \"patient fingerprint\". The approach\nachieves high accuracy in region detection (0.97 accuracy and 0.99 ROC AUC),\nand cell detection and cell classification (0.75 mAP, 0.78 F1-score,\nLog-average miss rate of 0.31). HCT has potential to revolutionize\nhematopathology diagnostic workflows, leading to more cost-effective, accurate\ndiagnosis and opening the door to precision medicine.",
          "link": "http://arxiv.org/abs/2107.02293",
          "publishedOn": "2021-07-07T01:57:12.652Z",
          "wordCount": 644,
          "title": "Histogram of Cell Types: Deep Learning for Automated Bone Marrow Cytology. (arXiv:2107.02293v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sirmacek_B/0/1/0/all/0/1\">Beril Sirmacek</a>",
          "description": "Urban areas are not only one of the biggest contributors to climate change,\nbut also they are one of the most vulnerable areas with high populations who\nwould together experience the negative impacts. In this paper, I address some\nof the opportunities brought by satellite remote sensing imaging and artificial\nintelligence (AI) in order to measure climate adaptation of cities\nautomatically. I propose an AI-based framework which might be useful for\nextracting indicators from remote sensing images and might help with predictive\nestimation of future states of these climate adaptation related indicators.\nWhen such models become more robust and used in real-life applications, they\nmight help decision makers and early responders to choose the best actions to\nsustain the wellbeing of society, natural resources and biodiversity. I\nunderline that this is an open field and an ongoing research for many\nscientists, therefore I offer an in depth discussion on the challenges and\nlimitations of AI-based methods and the predictive estimation models in\ngeneral.",
          "link": "http://arxiv.org/abs/2107.02693",
          "publishedOn": "2021-07-07T01:57:12.643Z",
          "wordCount": 610,
          "title": "Remote sensing, AI and innovative prediction methods for adapting cities to the impacts of the climate change. (arXiv:2107.02693v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vorontsov_E/0/1/0/all/0/1\">Eugene Vorontsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadoury_S/0/1/0/all/0/1\">Samuel Kadoury</a>",
          "description": "Imperfect labels limit the quality of predictions learned by deep neural\nnetworks. This is particularly relevant in medical image segmentation, where\nreference annotations are difficult to collect and vary significantly even\nacross expert annotators. Prior work on mitigating label noise focused on\nsimple models of mostly uniform noise. In this work, we explore biased and\nunbiased errors artificially introduced to brain tumour annotations on MRI\ndata. We found that supervised and semi-supervised segmentation methods are\nrobust or fairly robust to unbiased errors but sensitive to biased errors. It\nis therefore important to identify the sorts of errors expected in medical\nimage labels and especially mitigate the biased errors.",
          "link": "http://arxiv.org/abs/2107.02189",
          "publishedOn": "2021-07-07T01:57:12.618Z",
          "wordCount": 550,
          "title": "Label noise in segmentation networks : mitigation must deal with bias. (arXiv:2107.02189v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03308",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Natarovskii_V/0/1/0/all/0/1\">Viacheslav Natarovskii</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rudolf_D/0/1/0/all/0/1\">Daniel Rudolf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sprungk_B/0/1/0/all/0/1\">Bj&#xf6;rn Sprungk</a>",
          "description": "For Bayesian learning, given likelihood function and Gaussian prior, the\nelliptical slice sampler, introduced by Murray, Adams and MacKay 2010, provides\na tool for the construction of a Markov chain for approximate sampling of the\nunderlying posterior distribution. Besides of its wide applicability and\nsimplicity its main feature is that no tuning is necessary. Under weak\nregularity assumptions on the posterior density we show that the corresponding\nMarkov chain is geometrically ergodic and therefore yield qualitative\nconvergence guarantees. We illustrate our result for Gaussian posteriors as\nthey appear in Gaussian process regression, as well as in a setting of a\nmulti-modal distribution. Remarkably, our numerical experiments indicate a\ndimension-independent performance of elliptical slice sampling even in\nsituations where our ergodicity result does not apply.",
          "link": "http://arxiv.org/abs/2105.03308",
          "publishedOn": "2021-07-07T01:57:12.572Z",
          "wordCount": 599,
          "title": "Geometric convergence of elliptical slice sampling. (arXiv:2105.03308v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02643",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Budd_S/0/1/0/all/0/1\">Samuel Budd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinclair_M/0/1/0/all/0/1\">Matthew Sinclair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Day_T/0/1/0/all/0/1\">Thomas Day</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vlontzos_A/0/1/0/all/0/1\">Athanasios Vlontzos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_J/0/1/0/all/0/1\">Jeremy Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tianrui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matthew_J/0/1/0/all/0/1\">Jaqueline Matthew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Skelton_E/0/1/0/all/0/1\">Emily Skelton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simpson_J/0/1/0/all/0/1\">John Simpson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_E/0/1/0/all/0/1\">Emma C. Robinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>",
          "description": "Fetal ultrasound screening during pregnancy plays a vital role in the early\ndetection of fetal malformations which have potential long-term health impacts.\nThe level of skill required to diagnose such malformations from live ultrasound\nduring examination is high and resources for screening are often limited. We\npresent an interpretable, atlas-learning segmentation method for automatic\ndiagnosis of Hypo-plastic Left Heart Syndrome (HLHS) from a single `4 Chamber\nHeart' view image. We propose to extend the recently introduced\nImage-and-Spatial Transformer Networks (Atlas-ISTN) into a framework that\nenables sensitising atlas generation to disease. In this framework we can\njointly learn image segmentation, registration, atlas construction and disease\nprediction while providing a maximum level of clinical interpretability\ncompared to direct image classification methods. As a result our segmentation\nallows diagnoses competitive with expert-derived manual diagnosis and yields an\nAUC-ROC of 0.978 (1043 cases for training, 260 for validation and 325 for\ntesting).",
          "link": "http://arxiv.org/abs/2107.02643",
          "publishedOn": "2021-07-07T01:57:12.566Z",
          "wordCount": 629,
          "title": "Detecting Hypo-plastic Left Heart Syndrome in Fetal Ultrasound via Disease-specific Atlas Maps. (arXiv:2107.02643v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02476",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zaridis_D/0/1/0/all/0/1\">Dimitrios G. Zaridis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mylona_E/0/1/0/all/0/1\">Eugenia Mylona</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marias_K/0/1/0/all/0/1\">Kostas Marias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papanikolaou_N/0/1/0/all/0/1\">Nikolaos Papanikolaou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tachos_N/0/1/0/all/0/1\">Nikolaos S. Tachos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fotiadis_D/0/1/0/all/0/1\">Dimitrios I. Fotiadis</a>",
          "description": "Prostate segmentation from magnetic resonance imaging (MRI) is a challenging\ntask. In recent years, several network architectures have been proposed to\nautomate this process and alleviate the burden of manual annotation. Although\nthe performance of these models has achieved promising results, there is still\nroom for improvement before these models can be used safely and effectively in\nclinical practice. One of the major challenges in prostate MR image\nsegmentation is the presence of class imbalance in the image labels where the\nbackground pixels dominate over the prostate. In the present work we propose a\nDL-based pipeline for cropping the region around the prostate from MRI images\nto produce a more balanced distribution of the foreground pixels (prostate) and\nthe background pixels and improve segmentation accuracy. The effect of\nDL-cropping for improving the segmentation performance compared to standard\ncenter-cropping is assessed using five popular DL networks for prostate\nsegmentation, namely U-net, U-net+, Res Unet++, Bridge U-net and Dense U-net.\nThe proposed smart-cropping outperformed the standard center cropping in terms\nof segmentation accuracy for all the evaluated prostate segmentation networks.\nIn terms of Dice score, the highest improvement was achieved for the U-net+ and\nResU-net++ architectures corresponding to 8.9% and 8%, respectively.",
          "link": "http://arxiv.org/abs/2107.02476",
          "publishedOn": "2021-07-07T01:57:12.560Z",
          "wordCount": 668,
          "title": "A new smart-cropping pipeline for prostate segmentation using deep learning networks. (arXiv:2107.02476v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1\">En-Yu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun-Lin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hong-Liang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Duan-Bing Chen</a>",
          "description": "Many real-world systems can be expressed in temporal networks with nodes\nplaying far different roles in structure and function and edges representing\nthe relationships between nodes. Identifying critical nodes can help us control\nthe spread of public opinions or epidemics, predict leading figures in\nacademia, conduct advertisements for various commodities, and so on. However,\nit is rather difficult to identify critical nodes because the network structure\nchanges over time in temporal networks. In this paper, considering the sequence\ntopological information of temporal networks, a novel and effective learning\nframework based on the combination of special GCNs and RNNs is proposed to\nidentify nodes with the best spreading ability. The effectiveness of the\napproach is evaluated by weighted Susceptible-Infected-Recovered model.\nExperimental results on four real-world temporal networks demonstrate that the\nproposed method outperforms both traditional and deep learning benchmark\nmethods in terms of the Kendall $\\tau$ coefficient and top $k$ hit rate.",
          "link": "http://arxiv.org/abs/2106.10419",
          "publishedOn": "2021-07-07T01:57:12.554Z",
          "wordCount": 630,
          "title": "Predicting Critical Nodes in Temporal Networks by Dynamic Graph Convolutional Networks. (arXiv:2106.10419v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marwah_T/0/1/0/all/0/1\">Tanya Marwah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1\">Andrej Risteski</a>",
          "description": "Recent experiments have shown that deep networks can approximate solutions to\nhigh-dimensional PDEs, seemingly escaping the curse of dimensionality. However,\nquestions regarding the theoretical basis for such approximations, including\nthe required network size, remain open. In this paper, we investigate the\nrepresentational power of neural networks for approximating solutions to linear\nelliptic PDEs with Dirichlet boundary conditions. We prove that when a PDE's\ncoefficients are representable by small neural networks, the parameters\nrequired to approximate its solution scale polynomially with the input\ndimension $d$ and proportionally to the parameter counts of the coefficient\nnetworks. To this we end, we develop a proof technique that simulates gradient\ndescent (in an appropriate Hilbert space) by growing a neural network\narchitecture whose iterates each participate as sub-networks in their (slightly\nlarger) successors, and converge to the solution of the PDE. We bound the size\nof the solution, showing a polynomial dependence on $d$ and no dependence on\nthe volume of the domain.",
          "link": "http://arxiv.org/abs/2103.02138",
          "publishedOn": "2021-07-07T01:57:12.546Z",
          "wordCount": 631,
          "title": "Parametric Complexity Bounds for Approximating PDEs with Neural Networks. (arXiv:2103.02138v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_J/0/1/0/all/0/1\">Jun Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zongben Xu</a>",
          "description": "Meta learning has attracted much attention recently in machine learning\ncommunity. Contrary to conventional machine learning aiming to learn inherent\nprediction rules to predict labels for new query data, meta learning aims to\nlearn the learning methodology for machine learning from observed tasks, so as\nto generalize to new query tasks by leveraging the meta-learned learning\nmethodology. In this study, we interpret such learning methodology as learning\nan explicit hyperparameter prediction policy shared by all training tasks.\nSpecifically, this policy is represented as a parameterized function called\nmeta-learner, mapping from a training/test task to its suitable hyperparameter\nsetting, extracted from a pre-specified function set called meta learning\nmachine. Such setting guarantees that the meta-learned learning methodology is\nable to flexibly fit diverse query tasks, instead of only obtaining fixed\nhyperparameters by many current meta learning methods, with less adaptability\nto query task's variations. Such understanding of meta learning also makes it\neasily succeed from traditional learning theory for analyzing its\ngeneralization bounds with general losses/tasks/models. The theory naturally\nleads to some feasible controlling strategies for ameliorating the quality of\nthe extracted meta-learner, verified to be able to finely ameliorate its\ngeneralization capability in some typical meta learning applications, including\nfew-shot regression, few-shot classification and domain generalization.",
          "link": "http://arxiv.org/abs/2107.02378",
          "publishedOn": "2021-07-07T01:57:12.540Z",
          "wordCount": 638,
          "title": "Learning an Explicit Hyperparameter Prediction Policy Conditioned on Tasks. (arXiv:2107.02378v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhibin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenchao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "Hierarchical topic models such as the gamma belief network (GBN) have\ndelivered promising results in mining multi-layer document representations and\ndiscovering interpretable topic taxonomies. However, they often assume in the\nprior that the topics at each layer are independently drawn from the Dirichlet\ndistribution, ignoring the dependencies between the topics both at the same\nlayer and across different layers. To relax this assumption, we propose\nsawtooth factorial topic embedding guided GBN, a deep generative model of\ndocuments that captures the dependencies and semantic similarities between the\ntopics in the embedding space. Specifically, both the words and topics are\nrepresented as embedding vectors of the same dimension. The topic matrix at a\nlayer is factorized into the product of a factor loading matrix and a topic\nembedding matrix, the transpose of which is set as the factor loading matrix of\nthe layer above. Repeating this particular type of factorization, which shares\ncomponents between adjacent layers, leads to a structure referred to as\nsawtooth factorization. An auto-encoding variational inference network is\nconstructed to optimize the model parameter via stochastic gradient descent.\nExperiments on big corpora show that our models outperform other neural topic\nmodels on extracting deeper interpretable topics and deriving better document\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.02757",
          "publishedOn": "2021-07-07T01:57:12.532Z",
          "wordCount": 649,
          "title": "Sawtooth Factorial Topic Embeddings Guided Gamma Belief Network. (arXiv:2107.02757v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Douwes_C/0/1/0/all/0/1\">Constance Douwes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esling_P/0/1/0/all/0/1\">Philippe Esling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briot_J/0/1/0/all/0/1\">Jean-Pierre Briot</a>",
          "description": "In recent years, the deep learning community has largely focused on the\naccuracy of deep generative models, resulting in impressive improvements in\nseveral research fields. However, this scientific race for quality comes at a\ntremendous computational cost, which incurs vast energy consumption and\ngreenhouse gas emissions. If the current exponential growth of computational\nconsumption persists, Artificial Intelligence (AI) will sadly become a\nconsiderable contributor to global warming.\n\nAt the heart of this problem are the measures that we use as a scientific\ncommunity to evaluate our work. Currently, researchers in the field of AI judge\nscientific works mostly based on the improvement in accuracy, log-likelihood,\nreconstruction or opinion scores, all of which entirely obliterates the actual\ncomputational cost of generative models.\n\nIn this paper, we introduce the idea of relying on a multi-objective measure\nbased on Pareto optimality, which simultaneously integrates the models\naccuracy, as well as the environmental impact of their training. By applying\nthis measure on the current state-of-the-art in generative audio models, we\nshow that this measure drastically changes the perceived significance of the\nresults in the field, encouraging optimal training techniques and resource\nallocation. We hope that this type of measure will be widely adopted, in order\nto help the community to better evaluate the significance of their work, while\nbringing computational cost -- and in fine carbon emissions -- in the spotlight\nof AI research.",
          "link": "http://arxiv.org/abs/2107.02621",
          "publishedOn": "2021-07-07T01:57:12.508Z",
          "wordCount": 672,
          "title": "A Multi-Objective Approach for Sustainable Generative Audio Models. (arXiv:2107.02621v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.11202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keller_M/0/1/0/all/0/1\">Marcel Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Ke Sun</a>",
          "description": "Softmax is widely used in deep learning to map some representation to a\nprobability distribution. As it is based on exp/log functions that are\nrelatively expensive in multi-party computation, Mohassel and Zhang (2017)\nproposed a simpler replacement based on ReLU to be used in secure computation.\nHowever, we could not reproduce the accuracy they reported for training on\nMNIST with three fully connected layers. Later works (e.g., Wagh et al., 2019\nand 2021) used the softmax replacement not for computing the output probability\ndistribution but for approximating the gradient in back-propagation. In this\nwork, we analyze the two uses of the replacement and compare them to softmax,\nboth in terms of accuracy and cost in multi-party computation. We found that\nthe replacement only provides a significant speed-up for a one-layer network\nwhile it always reduces accuracy, sometimes significantly. Thus we conclude\nthat its usefulness is limited and one should use the original softmax function\ninstead.",
          "link": "http://arxiv.org/abs/2011.11202",
          "publishedOn": "2021-07-07T01:57:12.482Z",
          "wordCount": 620,
          "title": "Effectiveness of MPC-friendly Softmax Replacement. (arXiv:2011.11202v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_G/0/1/0/all/0/1\">Gokul Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1\">Sumit K. Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_C/0/1/0/all/0/1\">Chaitali Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jae-sun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogras_U/0/1/0/all/0/1\">Umit Y. Ogras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>",
          "description": "With the widespread use of Deep Neural Networks (DNNs), machine learning\nalgorithms have evolved in two diverse directions -- one with ever-increasing\nconnection density for better accuracy and the other with more compact sizing\nfor energy efficiency. The increase in connection density increases on-chip\ndata movement, which makes efficient on-chip communication a critical function\nof the DNN accelerator. The contribution of this work is threefold. First, we\nillustrate that the point-to-point (P2P)-based interconnect is incapable of\nhandling a high volume of on-chip data movement for DNNs. Second, we evaluate\nP2P and network-on-chip (NoC) interconnect (with a regular topology such as a\nmesh) for SRAM- and ReRAM-based in-memory computing (IMC) architectures for a\nrange of DNNs. This analysis shows the necessity for the optimal interconnect\nchoice for an IMC DNN accelerator. Finally, we perform an experimental\nevaluation for different DNNs to empirically obtain the performance of the IMC\narchitecture with both NoC-tree and NoC-mesh. We conclude that, at the tile\nlevel, NoC-tree is appropriate for compact DNNs employed at the edge, and\nNoC-mesh is necessary to accelerate DNNs with high connection density.\nFurthermore, we propose a technique to determine the optimal choice of\ninterconnect for any given DNN. In this technique, we use analytical models of\nNoC to evaluate end-to-end communication latency of any given DNN. We\ndemonstrate that the interconnect optimization in the IMC architecture results\nin up to 6$\\times$ improvement in energy-delay-area product for VGG-19\ninference compared to the state-of-the-art ReRAM-based IMC architectures.",
          "link": "http://arxiv.org/abs/2107.02358",
          "publishedOn": "2021-07-07T01:57:12.472Z",
          "wordCount": 695,
          "title": "Impact of On-Chip Interconnect on In-Memory Acceleration of Deep Neural Networks. (arXiv:2107.02358v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02480",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guitart_A/0/1/0/all/0/1\">Anna Guitart</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rio_A/0/1/0/all/0/1\">Ana Fern&#xe1;ndez del R&#xed;o</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Perianez_A/0/1/0/all/0/1\">&#xc1;frica Peri&#xe1;&#xf1;ez</a>",
          "description": "Every day, 800 women and 6,700 newborns die from complications related to\npregnancy or childbirth. A well-trained midwife can prevent most of these\nmaternal and newborn deaths. Data science models together with logs generated\nby users of online learning applications for midwives can help to improve their\nlearning competencies. The goal is to use these rich behavioral data to push\ndigital learning towards personalized content and to provide an adaptive\nlearning journey. In this work, we evaluate various forecasting methods to\ndetermine the interest of future users on the different kind of contents\navailable in the app, broken down by profession and region.",
          "link": "http://arxiv.org/abs/2107.02480",
          "publishedOn": "2021-07-07T01:57:12.458Z",
          "wordCount": 546,
          "title": "Midwifery Learning and Forecasting: Predicting Content Demand with User-Generated Logs. (arXiv:2107.02480v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Ye Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aker_A/0/1/0/all/0/1\">Ahmet Aker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1\">Kalina Bontcheva</a>",
          "description": "The spreading COVID-19 misinformation over social media already draws the\nattention of many researchers. According to Google Scholar, about 26000\nCOVID-19 related misinformation studies have been published to date. Most of\nthese studies focusing on 1) detect and/or 2) analysing the characteristics of\nCOVID-19 related misinformation. However, the study of the social behaviours\nrelated to misinformation is often neglected. In this paper, we introduce a\nfine-grained annotated misinformation tweets dataset including social\nbehaviours annotation (e.g. comment or question to the misinformation). The\ndataset not only allows social behaviours analysis but also suitable for both\nevidence-based or non-evidence-based misinformation classification task. In\naddition, we introduce leave claim out validation in our experiments and\ndemonstrate the misinformation classification performance could be\nsignificantly different when applying to real-world unseen misinformation.",
          "link": "http://arxiv.org/abs/2106.11702",
          "publishedOn": "2021-07-07T01:57:12.443Z",
          "wordCount": 660,
          "title": "Categorising Fine-to-Coarse Grained Misinformation: An Empirical Study of COVID-19 Infodemic. (arXiv:2106.11702v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhanghao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaiyuan Yang</a>",
          "description": "A compact, accurate, and bitwidth-programmable in-memory computing (IMC)\nstatic random-access memory (SRAM) macro, named CAP-RAM, is presented for\nenergy-efficient convolutional neural network (CNN) inference. It leverages a\nnovel charge-domain multiply-and-accumulate (MAC) mechanism and circuitry to\nachieve superior linearity under process variations compared to conventional\nIMC designs. The adopted semi-parallel architecture efficiently stores filters\nfrom multiple CNN layers by sharing eight standard 6T SRAM cells with one\ncharge-domain MAC circuit. Moreover, up to six levels of bit-width of weights\nwith two encoding schemes and eight levels of input activations are supported.\nA 7-bit charge-injection SAR (ciSAR) analog-to-digital converter (ADC) getting\nrid of sample and hold (S&H) and input/reference buffers further improves the\noverall energy efficiency and throughput. A 65-nm prototype validates the\nexcellent linearity and computing accuracy of CAP-RAM. A single 512x128 macro\nstores a complete pruned and quantized CNN model to achieve 98.8% inference\naccuracy on the MNIST data set and 89.0% on the CIFAR-10 data set, with a\n573.4-giga operations per second (GOPS) peak throughput and a 49.4-tera\noperations per second (TOPS)/W energy efficiency.",
          "link": "http://arxiv.org/abs/2107.02388",
          "publishedOn": "2021-07-07T01:57:12.427Z",
          "wordCount": 663,
          "title": "CAP-RAM: A Charge-Domain In-Memory Computing 6T-SRAM for Accurate and Precision-Programmable CNN Inference. (arXiv:2107.02388v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.08346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hatonen_V/0/1/0/all/0/1\">Vili H&#xe4;t&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melzer_F/0/1/0/all/0/1\">Fiona Melzer</a>",
          "description": "Decades of research on climate have provided a consensus that human activity\nhas changed the climate and we are currently heading into a climate crisis.\nWhile public discussion and research efforts on climate change mitigation have\nincreased, potential solutions need to not only be discussed but also\neffectively deployed. For preventing mismanagement and holding policy makers\naccountable, transparency and degree of information about government processes\nhave been shown to be crucial. However, currently the quantity of information\nabout climate change discussions and the range of sources make it increasingly\ndifficult for the public and civil society to maintain an overview to hold\npoliticians accountable.\n\nIn response, we propose a multi-source topic aggregation system (MuSTAS)\nwhich processes policy makers speech and rhetoric from several publicly\navailable sources into an easily digestible topic summary. MuSTAS uses novel\nmulti-source hybrid latent Dirichlet allocation to model topics from a variety\nof documents. This topic digest will serve the general public and civil society\nin assessing where, how, and when politicians talk about climate and climate\npolicies, enabling them to hold politicians accountable for their actions to\nmitigate climate change and lack thereof.",
          "link": "http://arxiv.org/abs/2010.08346",
          "publishedOn": "2021-07-07T01:57:12.412Z",
          "wordCount": 684,
          "title": "From Talk to Action with Accountability: Monitoring the Public Discussion of Policy Makers with Deep Neural Networks and Topic Modelling. (arXiv:2010.08346v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00335",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lou_A/0/1/0/all/0/1\">Aaron Lou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katsman_I/0/1/0/all/0/1\">Isay Katsman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jiang_Q/0/1/0/all/0/1\">Qingxuan Jiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>",
          "description": "Recent advances in deep representation learning on Riemannian manifolds\nextend classical deep learning operations to better capture the geometry of the\nmanifold. One possible extension is the Fr\\'echet mean, the generalization of\nthe Euclidean mean; however, it has been difficult to apply because it lacks a\nclosed form with an easily computable derivative. In this paper, we show how to\ndifferentiate through the Fr\\'echet mean for arbitrary Riemannian manifolds.\nThen, focusing on hyperbolic space, we derive explicit gradient expressions and\na fast, accurate, and hyperparameter-free Fr\\'echet mean solver. This fully\nintegrates the Fr\\'echet mean into the hyperbolic neural network pipeline. To\ndemonstrate this integration, we present two case studies. First, we apply our\nFr\\'echet mean to the existing Hyperbolic Graph Convolutional Network,\nreplacing its projected aggregation to obtain state-of-the-art results on\ndatasets with high hyperbolicity. Second, to demonstrate the Fr\\'echet mean's\ncapacity to generalize Euclidean neural network operations, we develop a\nhyperbolic batch normalization method that gives an improvement parallel to the\none observed in the Euclidean setting.",
          "link": "http://arxiv.org/abs/2003.00335",
          "publishedOn": "2021-07-07T01:57:12.406Z",
          "wordCount": 644,
          "title": "Differentiating through the Fr\\'echet Mean. (arXiv:2003.00335v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.10835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Steven W. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1\">Nikolay Atanasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vijay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morari_M/0/1/0/all/0/1\">Manfred Morari</a>",
          "description": "This work presents an explicit-implicit procedure to compute a model\npredictive control (MPC) law with guarantees on recursive feasibility and\nasymptotic stability. The approach combines an offline-trained fully-connected\nneural network with an online primal active set solver. The neural network\nprovides a control input initialization while the primal active set method\nensures recursive feasibility and asymptotic stability. The neural network is\ntrained with a primal-dual loss function, aiming to generate control sequences\nthat are primal feasible and meet a desired level of suboptimality. Since the\nneural network alone does not guarantee constraint satisfaction, its output is\nused to warm start the primal active set method online. We demonstrate that\nthis approach scales to large problems with thousands of optimization\nvariables, which are challenging for current approaches. Our method achieves a\n2x reduction in online inference time compared to the best method in a\nbenchmark suite of different solver and initialization strategies.",
          "link": "http://arxiv.org/abs/1910.10835",
          "publishedOn": "2021-07-07T01:57:12.400Z",
          "wordCount": 641,
          "title": "Large Scale Model Predictive Control with Neural Networks and Primal Active Sets. (arXiv:1910.10835v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Matt Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>",
          "description": "We present a scalable technique for upper bounding the Lipschitz constant of\ngenerative models. We relate this quantity to the maximal norm over the set of\nattainable vector-Jacobian products of a given generative model. We approximate\nthis set by layerwise convex approximations using zonotopes. Our approach\ngeneralizes and improves upon prior work using zonotope transformers and we\nextend to Lipschitz estimation of neural networks with large output dimension.\nThis provides efficient and tight bounds on small networks and can scale to\ngenerative models on VAE and DCGAN architectures.",
          "link": "http://arxiv.org/abs/2107.02732",
          "publishedOn": "2021-07-07T01:57:12.393Z",
          "wordCount": 520,
          "title": "Provable Lipschitz Certification for Generative Models. (arXiv:2107.02732v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianshen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azam_N/0/1/0/all/0/1\">Naveed Ahmed Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haraguchi_K/0/1/0/all/0/1\">Kazuya Haraguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagamochi_H/0/1/0/all/0/1\">Hiroshi Nagamochi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akutsu_T/0/1/0/all/0/1\">Tatsuya Akutsu</a>",
          "description": "Recently a novel framework has been proposed for designing the molecular\nstructure of chemical compounds using both artificial neural networks (ANNs)\nand mixed integer linear programming (MILP). In the framework, we first define\na feature vector $f(C)$ of a chemical graph $C$ and construct an ANN that maps\n$x=f(C)$ to a predicted value $\\eta(x)$ of a chemical property $\\pi$ to $C$.\nAfter this, we formulate an MILP that simulates the computation process of\n$f(C)$ from $C$ and that of $\\eta(x)$ from $x$. Given a target value $y^*$ of\nthe chemical property $\\pi$, we infer a chemical graph $C^\\dagger$ such that\n$\\eta(f(C^\\dagger))=y^*$ by solving the MILP. In this paper, we use linear\nregression to construct a prediction function $\\eta$ instead of ANNs. For this,\nwe derive an MILP formulation that simulates the computation process of a\nprediction function by linear regression. The results of computational\nexperiments suggest our method can infer chemical graphs with around up to 50\nnon-hydrogen atoms.",
          "link": "http://arxiv.org/abs/2107.02381",
          "publishedOn": "2021-07-07T01:57:12.378Z",
          "wordCount": 607,
          "title": "An Inverse QSAR Method Based on Linear Regression and Integer Programming. (arXiv:2107.02381v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1\">Joseph Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_T/0/1/0/all/0/1\">Talfan Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>",
          "description": "In this article, we present a visual introduction to Gaussian Belief\nPropagation (GBP), an approximate probabilistic inference algorithm that\noperates by passing messages between the nodes of arbitrarily structured factor\ngraphs. A special case of loopy belief propagation, GBP updates rely only on\nlocal information and will converge independently of the message schedule. Our\nkey argument is that, given recent trends in computing hardware, GBP has the\nright computational properties to act as a scalable distributed probabilistic\ninference framework for future machine learning systems.",
          "link": "http://arxiv.org/abs/2107.02308",
          "publishedOn": "2021-07-07T01:57:12.372Z",
          "wordCount": 534,
          "title": "A visual introduction to Gaussian Belief Propagation. (arXiv:2107.02308v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1\">Francesco Farina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_L/0/1/0/all/0/1\">Lawrence Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richmond_N/0/1/0/all/0/1\">Nicola J Richmond</a>",
          "description": "We introduce a framework for uncertainty estimation that both describes and\nextends many existing methods. We consider typical hyperparameters involved in\nclassical training as random variables and marginalise them out to capture\nvarious sources of uncertainty in the parameter space. We investigate which\nforms and combinations of marginalisation are most useful from a practical\npoint of view on standard benchmarking data sets. Moreover, we discuss how some\nmarginalisations may produce reliable estimates of uncertainty without the need\nfor extensive hyperparameter tuning and/or large-scale ensembling.",
          "link": "http://arxiv.org/abs/2107.02526",
          "publishedOn": "2021-07-07T01:57:12.366Z",
          "wordCount": 530,
          "title": "Intrinsic uncertainties and where to find them. (arXiv:2107.02526v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianqiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1\">Sameera Ramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>",
          "description": "It is well noted that coordinate based MLPs benefit greatly -- in terms of\npreserving high-frequency information -- through the encoding of coordinate\npositions as an array of Fourier features. Hitherto, the rationale for the\neffectiveness of these positional encodings has been solely studied through a\nFourier lens. In this paper, we strive to broaden this understanding by showing\nthat alternative non-Fourier embedding functions can indeed be used for\npositional encoding. Moreover, we show that their performance is entirely\ndetermined by a trade-off between the stable rank of the embedded matrix and\nthe distance preservation between embedded coordinates. We further establish\nthat the now ubiquitous Fourier feature mapping of position is a special case\nthat fulfills these conditions. Consequently, we present a more general theory\nto analyze positional encoding in terms of shifted basis functions. To this\nend, we develop the necessary theoretical formulae and empirically verify that\nour theoretical claims hold in practice. Codes available at\nhttps://github.com/osiriszjq/Rethinking-positional-encoding.",
          "link": "http://arxiv.org/abs/2107.02561",
          "publishedOn": "2021-07-07T01:57:12.361Z",
          "wordCount": 586,
          "title": "Rethinking Positional Encoding. (arXiv:2107.02561v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3$\\times$ as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results~(e.g., Top-1 accuracy 84.1% trained on 224$\\times$224\nImageNet-1K only), while being more scalable on high-resolution images. The\nmodels and source code will be released soon.",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-07T01:57:12.355Z",
          "wordCount": 671,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02283",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Zhu_L/0/1/0/all/0/1\">Liao Zhu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Sun_N/0/1/0/all/0/1\">Ningning Sun</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wells_M/0/1/0/all/0/1\">Martin T. Wells</a>",
          "description": "This paper builds the clustering model of measures of market microstructure\nfeatures which are popular in predicting the stock returns. In a 10-second time\nfrequency, we study the clustering structure of different measures to find out\nthe best ones for predicting. In this way, we can predict more accurately with\na limited number of predictors, which removes the noise and makes the model\nmore interpretable.",
          "link": "http://arxiv.org/abs/2107.02283",
          "publishedOn": "2021-07-07T01:57:12.332Z",
          "wordCount": 497,
          "title": "Clustering Structure of Microstructure Measures. (arXiv:2107.02283v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trizna_D/0/1/0/all/0/1\">Dmitrijs Trizna</a>",
          "description": "In this article, we present a Shell Language Preprocessing (SLP) library,\nwhich implements tokenization and encoding directed on the parsing of Unix and\nLinux shell commands. We describe the rationale behind the need for a new\napproach with specific examples when conventional Natural Language Processing\n(NLP) pipelines fail. Furthermore, we evaluate our methodology on a security\nclassification task against widely accepted information and communications\ntechnology (ICT) tokenization techniques and achieve significant improvement of\nan F1-score from 0.392 to 0.874.",
          "link": "http://arxiv.org/abs/2107.02438",
          "publishedOn": "2021-07-07T01:57:12.327Z",
          "wordCount": 515,
          "title": "Shell Language Processing: Unix command parsing for Machine Learning. (arXiv:2107.02438v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geiger_R/0/1/0/all/0/1\">R. Stuart Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cope_D/0/1/0/all/0/1\">Dominique Cope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ip_J/0/1/0/all/0/1\">Jamie Ip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotosh_M/0/1/0/all/0/1\">Marsha Lotosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Aayush Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Jenny Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Rebekah Tang</a>",
          "description": "Supervised machine learning, in which models are automatically derived from\nlabeled training data, is only as good as the quality of that data. This study\nbuilds on prior work that investigated to what extent 'best practices' around\nlabeling training data were followed in applied ML publications within a single\ndomain (social media platforms). In this paper, we expand by studying\npublications that apply supervised ML in a far broader spectrum of disciplines,\nfocusing on human-labeled data. We report to what extent a random sample of ML\napplication papers across disciplines give specific details about whether best\npractices were followed, while acknowledging that a greater range of\napplication fields necessarily produces greater diversity of labeling and\nannotation methods. Because much of machine learning research and education\nonly focuses on what is done once a \"ground truth\" or \"gold standard\" of\ntraining data is available, it is especially relevant to discuss issues around\nthe equally-important aspect of whether such data is reliable in the first\nplace. This determination becomes increasingly complex when applied to a\nvariety of specialized fields, as labeling can range from a task requiring\nlittle-to-no background knowledge to one that must be performed by someone with\ncareer expertise.",
          "link": "http://arxiv.org/abs/2107.02278",
          "publishedOn": "2021-07-07T01:57:12.307Z",
          "wordCount": 671,
          "title": "\"Garbage In, Garbage Out\" Revisited: What Do Machine Learning Application Papers Report About Human-Labeled Training Data?. (arXiv:2107.02278v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1\">Shashank Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanervisto_A/0/1/0/all/0/1\">Anssi Kanervisto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrenko_A/0/1/0/all/0/1\">Aleksei Petrenko</a>",
          "description": "Humans and other intelligent animals evolved highly sophisticated perception\nsystems that combine multiple sensory modalities. On the other hand,\nstate-of-the-art artificial agents rely mostly on visual inputs or structured\nlow-dimensional observations provided by instrumented environments. Learning to\nact based on combined visual and auditory inputs is still a new topic of\nresearch that has not been explored beyond simple scenarios. To facilitate\nprogress in this area we introduce a new version of VizDoom simulator to create\na highly efficient learning environment that provides raw audio observations.\nWe study the performance of different model architectures in a series of tasks\nthat require the agent to recognize sounds and execute instructions given in\nnatural language. Finally, we train our agent to play the full game of Doom and\nfind that it can consistently defeat a traditional vision-based adversary. We\nare currently in the process of merging the augmented simulator with the main\nViZDoom code repository. Video demonstrations and experiment code can be found\nat https://sites.google.com/view/sound-rl.",
          "link": "http://arxiv.org/abs/2107.02195",
          "publishedOn": "2021-07-07T01:57:12.300Z",
          "wordCount": 621,
          "title": "Agents that Listen: High-Throughput Reinforcement Learning with Multiple Sensory Systems. (arXiv:2107.02195v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_P/0/1/0/all/0/1\">Pingcheng Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huazhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "We address the problem of solving complex bimanual robot manipulation tasks\non multiple objects with sparse rewards. Such complex tasks can be decomposed\ninto sub-tasks that are accomplishable by different robots concurrently or\nsequentially for better efficiency. While previous reinforcement learning\napproaches primarily focus on modeling the compositionality of sub-tasks, two\nfundamental issues are largely ignored particularly when learning cooperative\nstrategies for two robots: (i) domination, i.e., one robot may try to solve a\ntask by itself and leaves the other idle; (ii) conflict, i.e., one robot can\neasily interrupt another's workspace when executing different sub-tasks\nsimultaneously. To tackle these two issues, we propose a novel technique called\ndisentangled attention, which provides an intrinsic regularization for two\nrobots to focus on separate sub-tasks and objects. We evaluate our method on\nfour bimanual manipulation tasks. Experimental results show that our proposed\nintrinsic regularization successfully avoids domination and reduces conflicts\nfor the policies, which leads to significantly more effective cooperative\nstrategies than all the baselines. Our project page with videos is at\nhttps://mehooz.github.io/bimanual-attention.",
          "link": "http://arxiv.org/abs/2106.05907",
          "publishedOn": "2021-07-07T01:57:12.262Z",
          "wordCount": 651,
          "title": "Disentangled Attention as Intrinsic Regularization for Bimanual Multi-Object Manipulation. (arXiv:2106.05907v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lim_D/0/1/0/all/0/1\">Derek Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohne_F/0/1/0/all/0/1\">Felix Hohne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>",
          "description": "Much data with graph structures satisfy the principle of homophily, meaning\nthat connected nodes tend to be similar with respect to a specific attribute.\nAs such, ubiquitous datasets for graph machine learning tasks have generally\nbeen highly homophilous, rewarding methods that leverage homophily as an\ninductive bias. Recent work has pointed out this particular focus, as new\nnon-homophilous datasets have been introduced and graph representation learning\nmodels better suited for low-homophily settings have been developed. However,\nthese datasets are small and poorly suited to truly testing the effectiveness\nof new methods in non-homophilous settings. We present a series of improved\ngraph datasets with node label relationships that do not satisfy the homophily\nprinciple. Along with this, we introduce a new measure of the presence or\nabsence of homophily that is better suited than existing measures in different\nregimes. We benchmark a range of simple methods and graph neural networks\nacross our proposed datasets, drawing new insights for further research. Data\nand codes can be found at https://github.com/CUAI/Non-Homophily-Benchmarks.",
          "link": "http://arxiv.org/abs/2104.01404",
          "publishedOn": "2021-07-07T01:57:12.180Z",
          "wordCount": 642,
          "title": "New Benchmarks for Learning on Non-Homophilous Graphs. (arXiv:2104.01404v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02495",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1\">Laurence Aitchison</a>",
          "description": "We show that a popular self-supervised learning method, InfoNCE, is a special\ncase of a new family of unsupervised learning methods, the self-supervised\nvariational autoencoder (SSVAE). SSVAEs circumvent the usual VAE requirement to\nreconstruct the data by using a carefully chosen implicit decoder. The InfoNCE\nobjective was motivated as a simplified parametric mutual information\nestimator. Under one choice of prior, the SSVAE objective (i.e. the ELBO) is\nexactly equal to the mutual information (up to constants). Under an alternative\nchoice of prior, the SSVAE objective is exactly equal to the simplified\nparametric mutual information estimator used in InfoNCE (up to constants).\nImportantly, the use of simplified parametric mutual information estimators is\nbelieved to be critical to obtain good high-level representations, and the\nSSVAE framework naturally provides a principled justification for using prior\ninformation to choose these estimators.",
          "link": "http://arxiv.org/abs/2107.02495",
          "publishedOn": "2021-07-07T01:57:12.128Z",
          "wordCount": 560,
          "title": "InfoNCE is a variational autoencoder. (arXiv:2107.02495v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_A/0/1/0/all/0/1\">Arun Narenthiran Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_S/0/1/0/all/0/1\">Sahil Modi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasparino_M/0/1/0/all/0/1\">Mateus Valverde Gasparino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_C/0/1/0/all/0/1\">Che Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1\">Andres Eduardo Baquero Velasquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1\">Girish Chowdhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Saurabh Gupta</a>",
          "description": "We describe a system for visually guided autonomous navigation of\nunder-canopy farm robots. Low-cost under-canopy robots can drive between crop\nrows under the plant canopy and accomplish tasks that are infeasible for\nover-the-canopy drones or larger agricultural equipment. However, autonomously\nnavigating them under the canopy presents a number of challenges: unreliable\nGPS and LiDAR, high cost of sensing, challenging farm terrain, clutter due to\nleaves and weeds, and large variability in appearance over the season and\nacross crop types. We address these challenges by building a modular system\nthat leverages machine learning for robust and generalizable perception from\nmonocular RGB images from low-cost cameras, and model predictive control for\naccurate control in challenging terrain. Our system, CropFollow, is able to\nautonomously drive 485 meters per intervention on average, outperforming a\nstate-of-the-art LiDAR based system (286 meters per intervention) in extensive\nfield testing spanning over 25 km.",
          "link": "http://arxiv.org/abs/2107.02792",
          "publishedOn": "2021-07-07T01:57:12.122Z",
          "wordCount": 609,
          "title": "Learned Visual Navigation for Under-Canopy Agricultural Robots. (arXiv:2107.02792v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1\">Pranav Jeevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a> (Indian Institute of Technology Bombay)",
          "description": "Linear attention mechanisms provide hope for overcoming the bottleneck of\nquadratic complexity which restricts application of transformer models in\nvision tasks. We modify the ViT architecture to work on longer sequence data by\nreplacing the quadratic attention with efficient transformers like Performer,\nLinformer and Nystr\\\"omformer of linear complexity creating Vision X-formers\n(ViX). We show that ViX performs better than ViT in image classification\nconsuming lesser computing resources. We further show that replacing the\nembedding linear layer by convolutional layers in ViX further increases their\nperformance. Our test on recent visions transformer models like LeViT and\nCompact Convolutional Transformer (CCT) show that replacing the attention with\nNystr\\\"omformer or Performer saves GPU usage and memory without deteriorating\nperformance. Incorporating these changes can democratize transformers by making\nthem accessible to those with limited data and computing resources.",
          "link": "http://arxiv.org/abs/2107.02239",
          "publishedOn": "2021-07-07T01:57:12.115Z",
          "wordCount": 598,
          "title": "Vision Xformers: Efficient Attention for Image Classification. (arXiv:2107.02239v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.08328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mollas_I/0/1/0/all/0/1\">Ioannis Mollas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysopoulou_Z/0/1/0/all/0/1\">Zoe Chrysopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlos_S/0/1/0/all/0/1\">Stamatis Karlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>",
          "description": "Online hate speech is a recent problem in our society that is rising at a\nsteady pace by leveraging the vulnerabilities of the corresponding regimes that\ncharacterise most social media platforms. This phenomenon is primarily fostered\nby offensive comments, either during user interaction or in the form of a\nposted multimedia context. Nowadays, giant corporations own platforms where\nmillions of users log in every day, and protection from exposure to similar\nphenomena appears to be necessary in order to comply with the corresponding\nlegislation and maintain a high level of service quality. A robust and reliable\nsystem for detecting and preventing the uploading of relevant content will have\na significant impact on our digitally interconnected society. Several aspects\nof our daily lives are undeniably linked to our social profiles, making us\nvulnerable to abusive behaviours. As a result, the lack of accurate hate speech\ndetection mechanisms would severely degrade the overall user experience,\nalthough its erroneous operation would pose many ethical concerns. In this\npaper, we present 'ETHOS', a textual dataset with two variants: binary and\nmulti-label, based on YouTube and Reddit comments validated using the\nFigure-Eight crowdsourcing platform. Furthermore, we present the annotation\nprotocol used to create this dataset: an active sampling procedure for\nbalancing our data in relation to the various aspects defined. Our key\nassumption is that, even gaining a small amount of labelled data from such a\ntime-consuming process, we can guarantee hate speech occurrences in the\nexamined material.",
          "link": "http://arxiv.org/abs/2006.08328",
          "publishedOn": "2021-07-07T01:57:12.109Z",
          "wordCount": 739,
          "title": "ETHOS: an Online Hate Speech Detection Dataset. (arXiv:2006.08328v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taborsky_P/0/1/0/all/0/1\">Petr Taborsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1\">Lars Kai Hansen</a>",
          "description": "We take a geometrical viewpoint and present a unifying view on supervised\ndeep learning with the Bregman divergence loss function - this entails frequent\nclassification and prediction tasks. Motivated by simulations we suggest that\nthere is principally no implicit bias of vanilla stochastic gradient descent\ntraining of deep models towards \"simpler\" functions. Instead, we show that good\ngeneralization may be instigated by bounded spectral products over layers\nleading to a novel geometric regularizer. It is revealed that in deep enough\nmodels such a regularizer enables both, extreme accuracy and generalization, to\nbe reached. We associate popular regularization techniques like weight decay,\ndrop out, batch normalization, and early stopping with this perspective. Backed\nup by theory we further demonstrate that \"generalization by design\" is\npractically possible and that good generalization may be encoded into the\nstructure of the network. We design two such easy-to-use structural\nregularizers that insert an additional \\textit{generalization layer} into a\nmodel architecture, one with a skip connection and another one with drop-out.\nWe verify our theoretical results in experiments on various feedforward and\nconvolutional architectures, including ResNets, and datasets (MNIST, CIFAR10,\nsynthetic data). We believe this work opens up new avenues of research towards\nbetter generalizing architectures.",
          "link": "http://arxiv.org/abs/2107.02253",
          "publishedOn": "2021-07-07T01:57:12.103Z",
          "wordCount": 643,
          "title": "Generalization by design: Shortcuts to Generalization in Deep Learning. (arXiv:2107.02253v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_A/0/1/0/all/0/1\">Azqa Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verwer_S/0/1/0/all/0/1\">Sicco Verwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moskal_S/0/1/0/all/0/1\">Stephen Moskal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shanchieh Jay Yang</a>",
          "description": "Attack graphs (AG) are used to assess pathways availed by cyber adversaries\nto penetrate a network. State-of-the-art approaches for AG generation focus\nmostly on deriving dependencies between system vulnerabilities based on network\nscans and expert knowledge. In real-world operations however, it is costly and\nineffective to rely on constant vulnerability scanning and expert-crafted AGs.\nWe propose to automatically learn AGs based on actions observed through\nintrusion alerts, without prior expert knowledge. Specifically, we develop an\nunsupervised sequence learning system, SAGE, that leverages the temporal and\nprobabilistic dependence between alerts in a suffix-based probabilistic\ndeterministic finite automaton (S-PDFA) -- a model that accentuates infrequent\nsevere alerts and summarizes paths leading to them. AGs are then derived from\nthe S-PDFA. Tested with intrusion alerts collected through Collegiate\nPenetration Testing Competition, SAGE produces AGs that reflect the strategies\nused by participating teams. The resulting AGs are succinct, interpretable, and\nenable analysts to derive actionable insights, e.g., attackers tend to follow\nshorter paths after they have discovered a longer one.",
          "link": "http://arxiv.org/abs/2107.02783",
          "publishedOn": "2021-07-07T01:57:12.087Z",
          "wordCount": 613,
          "title": "SAGE: Intrusion Alert-driven Attack Graph Extractor. (arXiv:2107.02783v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/1904.08084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nanni_L/0/1/0/all/0/1\">L. Nanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahnam_S/0/1/0/all/0/1\">S. Brahnam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidoni_S/0/1/0/all/0/1\">S. Ghidoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguolo_G/0/1/0/all/0/1\">G. Maguolo</a>",
          "description": "Bioimage classification plays a crucial role in many biological problems. In\nthis work, we present a new General Purpose (GenP) ensemble that boosts\nperformance by combining local features, dense sampling features, and deep\nlearning approaches. First, we introduce three new methods for data\naugmentation based on PCA/DCT; second, we show that different data augmentation\napproaches can boost the performance of an ensemble of CNNs; and, finally, we\npropose a set of handcrafted/learned descriptors that are highly generalizable.\nEach handcrafted descriptor is used to train a different Support Vector Machine\n(SVM), and the different SVMs are combined with the ensemble of CNNs. Our\nmethod is evaluated on a diverse set of bioimage classification problems.\nResults demonstrate that the proposed GenP bioimage ensemble obtains\nstate-of-the-art performance without any ad-hoc dataset tuning of parameters\n(thus avoiding the risk of overfitting/overtraining).",
          "link": "http://arxiv.org/abs/1904.08084",
          "publishedOn": "2021-07-07T01:57:12.081Z",
          "wordCount": 642,
          "title": "General Purpose (GenP) Bioimage Ensemble of Handcrafted and Learned Features with Data Augmentation. (arXiv:1904.08084v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_L/0/1/0/all/0/1\">Lee Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Kraepelin_U/0/1/0/all/0/1\">Ulrike Schmidt-Kraepelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1\">Yishay Mansour</a>",
          "description": "We introduce the dueling teams problem, a new online-learning setting in\nwhich the learner observes noisy comparisons of disjoint pairs of $k$-sized\nteams from a universe of $n$ players. The goal of the learner is to minimize\nthe number of duels required to identify, with high probability, a Condorcet\nwinning team, i.e., a team which wins against any other disjoint team (with\nprobability at least $1/2$). Noisy comparisons are linked to a total order on\nthe teams. We formalize our model by building upon the dueling bandits setting\n(Yue et al.2012) and provide several algorithms, both for stochastic and\ndeterministic settings. For the stochastic setting, we provide a reduction to\nthe classical dueling bandits setting, yielding an algorithm that identifies a\nCondorcet winning team within $\\mathcal{O}((n + k \\log (k)) \\frac{\\max(\\log\\log\nn, \\log k)}{\\Delta^2})$ duels, where $\\Delta$ is a gap parameter. For\ndeterministic feedback, we additionally present a gap-independent algorithm\nthat identifies a Condorcet winning team within $\\mathcal{O}(nk\\log(k)+k^5)$\nduels.",
          "link": "http://arxiv.org/abs/2107.02738",
          "publishedOn": "2021-07-07T01:57:12.073Z",
          "wordCount": 593,
          "title": "Dueling Bandits with Team Comparisons. (arXiv:2107.02738v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nye_M/0/1/0/all/0/1\">Maxwell Nye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1\">Brenden M. Lake</a>",
          "description": "Human reasoning can often be understood as an interplay between two systems:\nthe intuitive and associative (\"System 1\") and the deliberative and logical\n(\"System 2\"). Neural sequence models -- which have been increasingly successful\nat performing complex, structured tasks -- exhibit the advantages and failure\nmodes of System 1: they are fast and learn patterns from data, but are often\ninconsistent and incoherent. In this work, we seek a lightweight, training-free\nmeans of improving existing System 1-like sequence models by adding System\n2-inspired logical reasoning. We explore several variations on this theme in\nwhich candidate generations from a neural sequence model are examined for\nlogical consistency by a symbolic reasoning module, which can either accept or\nreject the generations. Our approach uses neural inference to mediate between\nthe neural System 1 and the logical System 2. Results in robust story\ngeneration and grounded instruction-following show that this approach can\nincrease the coherence and accuracy of neurally-based generations.",
          "link": "http://arxiv.org/abs/2107.02794",
          "publishedOn": "2021-07-07T01:57:12.067Z",
          "wordCount": 607,
          "title": "Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning. (arXiv:2107.02794v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalander_M/0/1/0/all/0/1\">Marcus Kalander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chanfei Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lujia Pan</a>",
          "description": "We consider the problem of training robust and accurate deep neural networks\n(DNNs) when subject to various proportions of noisy labels. Large-scale\ndatasets tend to contain mislabeled samples that can be memorized by DNNs,\nimpeding the performance. With appropriate handling, this degradation can be\nalleviated. There are two problems to consider: how to distinguish clean\nsamples and how to deal with noisy samples. In this paper, we present Ensemble\nNoise-robust K-fold Cross-Validation Selection (E-NKCVS) to effectively select\nclean samples from noisy data, solving the first problem. For the second\nproblem, we create a new pseudo label for any sample determined to have an\nuncertain or likely corrupt label. E-NKCVS obtains multiple predicted labels\nfor each sample and the entropy of these labels is used to tune the weight\ngiven to the pseudo label and the given label. Theoretical analysis and\nextensive verification of the algorithms in the noisy label setting are\nprovided. We evaluate our approach on various image and text classification\ntasks where the labels have been manually corrupted with different noise\nratios. Additionally, two large real-world noisy datasets are also used,\nClothing-1M and WebVision. E-NKCVS is empirically shown to be highly tolerant\nto considerable proportions of label noise and has a consistent improvement\nover state-of-the-art methods. Especially on more difficult datasets with\nhigher noise ratios, we can achieve a significant improvement over the\nsecond-best model. Moreover, our proposed approach can easily be integrated\ninto existing DNN methods to improve their robustness against label noise.",
          "link": "http://arxiv.org/abs/2107.02347",
          "publishedOn": "2021-07-07T01:57:12.061Z",
          "wordCount": 698,
          "title": "An Ensemble Noise-Robust K-fold Cross-Validation Selection Method for Noisy Labels. (arXiv:2107.02347v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nam Kyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hong Kook Kim</a>",
          "description": "This report proposes a polyphonic sound event detection (SED) method for the\nDCASE 2021 Challenge Task 4. The proposed SED model consists of two stages: a\nmean-teacher model for providing target labels regarding weakly labeled or\nunlabeled data and a self-training-based noisy student model for predicting\nstrong labels for sound events. The mean-teacher model, which is based on the\nresidual convolutional recurrent neural network (RCRNN) for the teacher and\nstudent model, is first trained using all the training data from a weakly\nlabeled dataset, an unlabeled dataset, and a strongly labeled synthetic\ndataset. Then, the trained mean-teacher model predicts the strong label to each\nof the weakly labeled and unlabeled datasets, which is brought to the noisy\nstudent model in the second stage of the proposed SED model. Here, the\nstructure of the noisy student model is identical to the RCRNN-based student\nmodel of the mean-teacher model in the first stage. Then, it is self-trained by\nadding feature noises, such as time-frequency shift, mixup, SpecAugment, and\ndropout-based model noise. In addition, a semi-supervised loss function is\napplied to train the noisy student model, which acts as label noise injection.\nThe performance of the proposed SED model is evaluated on the validation set of\nthe DCASE 2021 Challenge Task 4, and then, several ensemble models that combine\nfive-fold validation models with different hyperparameters of the\nsemi-supervised loss function are finally selected as our final models.",
          "link": "http://arxiv.org/abs/2107.02569",
          "publishedOn": "2021-07-07T01:57:12.046Z",
          "wordCount": 694,
          "title": "Self-training with noisy student model and semi-supervised loss function for dcase 2021 challenge task 4. (arXiv:2107.02569v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moin_A/0/1/0/all/0/1\">Armin Moin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badii_A/0/1/0/all/0/1\">Atta Badii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "In this paper, we propose a novel approach to support domain-specific\nModel-Driven Software Engineering (MDSE) for the real-world use-case scenarios\nof smart Cyber-Physical Systems (CPS) and the Internet of Things (IoT). We\nargue that the majority of available data in the nature for Artificial\nIntelligence (AI), specifically Machine Learning (ML) are unlabeled. Hence,\nunsupervised and/or semi-supervised ML approaches are the practical choices.\nHowever, prior work in the literature of MDSE has considered supervised ML\napproaches, which only work with labeled training data. Our proposed approach\nis fully implemented and integrated with an existing state-of-the-art MDSE tool\nto serve the CPS/IoT domain. Moreover, we validate the proposed approach using\na portion of the open data of the REFIT reference dataset for the smart energy\nsystems domain. Our model-to-code transformations (code generators) provide the\nfull source code of the desired IoT services out of the model instances in an\nautomated manner. Currently, we generate the source code in Java and Python.\nThe Python code is responsible for the ML functionalities and uses the APIs of\nseveral ML libraries and frameworks, namely Scikit-Learn, Keras and TensorFlow.\nFor unsupervised and semi-supervised learning, the APIs of Scikit-Learn are\ndeployed. In addition to the pure MDSE approach, where certain ML methods,\ne.g., K-Means, Mini-Batch K-Means, DB-SCAN, Spectral Clustering, Gaussian\nMixture Model, Self-Training, Label Propagation and Label Spreading are\nsupported, a more flexible, hybrid approach is also enabled to support the\npractitioner in deploying a pre-trained ML model with any arbitrary\narchitecture and learning algorithm.",
          "link": "http://arxiv.org/abs/2107.02690",
          "publishedOn": "2021-07-07T01:57:12.040Z",
          "wordCount": 692,
          "title": "Enabling Un-/Semi-Supervised Machine Learning for MDSE of the Real-World CPS/IoT Applications. (arXiv:2107.02690v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuowei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijun Zhang</a>",
          "description": "This paper develops simple feed-forward neural networks that achieve the\nuniversal approximation property for all continuous functions with a fixed\nfinite number of neurons. These neural networks are simple because they are\ndesigned with a simple and computable continuous activation function $\\sigma$\nleveraging a triangular-wave function and a softsign function. We prove that\n$\\sigma$-activated networks with width $36d(2d+1)$ and depth $11$ can\napproximate any continuous function on a $d$-dimensioanl hypercube within an\narbitrarily small error. Hence, for supervised learning and its related\nregression problems, the hypothesis space generated by these networks with a\nsize not smaller than $36d(2d+1)\\times 11$ is dense in the space of continuous\nfunctions. Furthermore, classification functions arising from image and signal\nclassification are in the hypothesis space generated by $\\sigma$-activated\nnetworks with width $36d(2d+1)$ and depth $12$, when there exist pairwise\ndisjoint closed bounded subsets of $\\mathbb{R}^d$ such that the samples of the\nsame class are located in the same subset.",
          "link": "http://arxiv.org/abs/2107.02397",
          "publishedOn": "2021-07-07T01:57:12.034Z",
          "wordCount": 592,
          "title": "Deep Network Approximation With Accuracy Independent of Number of Neurons. (arXiv:2107.02397v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02474",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Moens_V/0/1/0/all/0/1\">Vincent Moens</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sootla_A/0/1/0/all/0/1\">Aivar Sootla</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ammar_H/0/1/0/all/0/1\">Haitham Bou Ammar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "We present a method for conditional sampling with normalizing flows when only\npart of an observation is available. We rely on the following fact: if the\nflow's domain can be partitioned in such a way that the flow restrictions to\nsubdomains keep the bijectivity property, a lower bound to the conditioning\nvariable log-probability can be derived. Simulation from the variational\nconditional flow then amends to solving an equality constraint. Our\ncontribution is three-fold: a) we provide detailed insights on the choice of\nvariational distributions; b) we propose how to partition the input space of\nthe flow to preserve bijectivity property; c) we propose a set of methods to\noptimise the variational distribution in specific cases. Through extensive\nexperiments, we show that our sampling method can be applied with success to\ninvertible residual networks for inference and classification.",
          "link": "http://arxiv.org/abs/2107.02474",
          "publishedOn": "2021-07-07T01:57:12.028Z",
          "wordCount": 569,
          "title": "Implicit Variational Conditional Sampling with Normalizing Flows. (arXiv:2107.02474v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rogoz_A/0/1/0/all/0/1\">Ana-Cristina Rogoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muntean_R/0/1/0/all/0/1\">Radu Muntean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobeli_S/0/1/0/all/0/1\">Stefan Cobeli</a>",
          "description": "Detecting objects of interest in images was always a compelling task to\nautomate. In recent years this task was more and more explored using deep\nlearning techniques, mostly using region-based convolutional networks. In this\nproject we propose an alternative semantic segmentation technique making use of\nGenerative Adversarial Networks. We consider semantic segmentation to be a\ndomain transfer problem. Thus, we train a feed forward network (FFNN) to\nreceive as input a seed real image and generate as output its segmentation\nmask.",
          "link": "http://arxiv.org/abs/2107.02525",
          "publishedOn": "2021-07-07T01:57:12.007Z",
          "wordCount": 531,
          "title": "Semantic Segmentation Alternative Technique: Segmentation Domain Generation. (arXiv:2107.02525v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1\">Siddharth Karamcheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>",
          "description": "Active learning promises to alleviate the massive data needs of supervised\nmachine learning: it has successfully improved sample efficiency by an order of\nmagnitude on traditional tasks like topic classification and object\nrecognition. However, we uncover a striking contrast to this promise: across 5\nmodels and 4 datasets on the task of visual question answering, a wide variety\nof active learning approaches fail to outperform random selection. To\nunderstand this discrepancy, we profile 8 active learning methods on a\nper-example basis, and identify the problem as collective outliers -- groups of\nexamples that active learning methods prefer to acquire but models fail to\nlearn (e.g., questions that ask about text in images or require external\nknowledge). Through systematic ablation experiments and qualitative\nvisualizations, we verify that collective outliers are a general phenomenon\nresponsible for degrading pool-based active learning. Notably, we show that\nactive learning sample efficiency increases significantly as the number of\ncollective outliers in the active learning pool decreases. We conclude with a\ndiscussion and prescriptive recommendations for mitigating the effects of these\noutliers in future work.",
          "link": "http://arxiv.org/abs/2107.02331",
          "publishedOn": "2021-07-07T01:57:12.001Z",
          "wordCount": 650,
          "title": "Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering. (arXiv:2107.02331v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dianbo_Liu_D/0/1/0/all/0/1\">Dianbo Liu Dianbo_Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1\">Alex Lamb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Anirudh Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael Curtis Mozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>",
          "description": "Deep learning has advanced from fully connected architectures to structured\nmodels organized into components, e.g., the transformer composed of positional\nelements, modular architectures divided into slots, and graph neural nets made\nup of nodes. In structured models, an interesting question is how to conduct\ndynamic and possibly sparse communication among the separate components. Here,\nwe explore the hypothesis that restricting the transmitted information among\ncomponents to discrete representations is a beneficial bottleneck. The\nmotivating intuition is human language in which communication occurs through\ndiscrete symbols. Even though individuals have different understandings of what\na ``\"cat\" is based on their specific experiences, the shared discrete token\nmakes it possible for communication among individuals to be unimpeded by\nindividual differences in internal representation. To discretize the values of\nconcepts dynamically communicated among specialist components, we extend the\nquantization mechanism from the Vector-Quantized Variational Autoencoder to\nmulti-headed discretization with shared codebooks and use it for\ndiscrete-valued neural communication (DVNC). Our experiments show that DVNC\nsubstantially improves systematic generalization in a variety of architectures\n-- transformers, modular architectures, and graph neural networks. We also show\nthat the DVNC is robust to the choice of hyperparameters, making the method\nvery useful in practice. Moreover, we establish a theoretical justification of\nour discretization process, proving that it has the ability to increase noise\nrobustness and reduce the underlying dimensionality of the model.",
          "link": "http://arxiv.org/abs/2107.02367",
          "publishedOn": "2021-07-07T01:57:11.995Z",
          "wordCount": 658,
          "title": "Discrete-Valued Neural Communication. (arXiv:2107.02367v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kangle Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andrew Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>",
          "description": "One common failure mode of Neural Radiance Field (NeRF) models is fitting\nincorrect geometries when given an insufficient number of input views. We\npropose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning\nneural radiance fields that takes advantage of readily-available depth\nsupervision. Our key insight is that sparse depth supervision can be used to\nregularize the learned geometry, a crucial component for effectively rendering\nnovel views using NeRF. We exploit the fact that current NeRF pipelines require\nimages with known camera poses that are typically estimated by running\nstructure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that\ncan be used as ``free\" depth supervision during training: we simply add a loss\nto ensure that depth rendered along rays that intersect these 3D points is\nclose to the observed depth. We find that DS-NeRF can render more accurate\nimages given fewer training views while training 2-6x faster. With only two\ntraining views on real-world images, DS-NeRF significantly outperforms NeRF as\nwell as other sparse-view variants. We show that our loss is compatible with\nthese NeRF models, demonstrating that depth is a cheap and easily digestible\nsupervisory signal. Finally, we show that DS-NeRF supports other types of depth\nsupervision such as scanned depth sensors and RGBD reconstruction outputs.",
          "link": "http://arxiv.org/abs/2107.02791",
          "publishedOn": "2021-07-07T01:57:11.989Z",
          "wordCount": 662,
          "title": "Depth-supervised NeRF: Fewer Views and Faster Training for Free. (arXiv:2107.02791v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungyoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hoki Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaewook Lee</a>",
          "description": "Deep learning is vulnerable to adversarial examples. Many defenses based on\nrandomized neural networks have been proposed to solve the problem, but fail to\nachieve robustness against attacks using proxy gradients such as the\nExpectation over Transformation (EOT) attack. We investigate the effect of the\nadversarial attacks using proxy gradients on randomized neural networks and\ndemonstrate that it highly relies on the directional distribution of the loss\ngradients of the randomized neural network. We show in particular that proxy\ngradients are less effective when the gradients are more scattered. To this\nend, we propose Gradient Diversity (GradDiv) regularizations that minimize the\nconcentration of the gradients to build a robust randomized neural network. Our\nexperiments on MNIST, CIFAR10, and STL10 show that our proposed GradDiv\nregularizations improve the adversarial robustness of randomized neural\nnetworks against a variety of state-of-the-art attack methods. Moreover, our\nmethod efficiently reduces the transferability among sample models of\nrandomized neural networks.",
          "link": "http://arxiv.org/abs/2107.02425",
          "publishedOn": "2021-07-07T01:57:11.982Z",
          "wordCount": 587,
          "title": "GradDiv: Adversarial Robustness of Randomized Neural Networks via Gradient Diversity Regularization. (arXiv:2107.02425v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chari_S/0/1/0/all/0/1\">Shruthi Chari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Prithwish Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghalwash_M/0/1/0/all/0/1\">Mohamed Ghalwash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_O/0/1/0/all/0/1\">Oshani Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyigoz_E/0/1/0/all/0/1\">Elif K. Eyigoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruen_D/0/1/0/all/0/1\">Daniel M. Gruen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Ching-Hua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_P/0/1/0/all/0/1\">Pablo Meyer Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_D/0/1/0/all/0/1\">Deborah L. McGuinness</a>",
          "description": "Academic advances of AI models in high-precision domains, like healthcare,\nneed to be made explainable in order to enhance real-world adoption. Our past\nstudies and ongoing interactions indicate that medical experts can use AI\nsystems with greater trust if there are ways to connect the model inferences\nabout patients to explanations that are tied back to the context of use.\nSpecifically, risk prediction is a complex problem of diagnostic and\ninterventional importance to clinicians wherein they consult different sources\nto make decisions. To enable the adoption of the ever improving AI risk\nprediction models in practice, we have begun to explore techniques to\ncontextualize such models along three dimensions of interest: the patients'\nclinical state, AI predictions about their risk of complications, and\nalgorithmic explanations supporting the predictions. We validate the importance\nof these dimensions by implementing a proof-of-concept (POC) in type-2 diabetes\n(T2DM) use case where we assess the risk of chronic kidney disease (CKD) - a\ncommon T2DM comorbidity. Within the POC, we include risk prediction models for\nCKD, post-hoc explainers of the predictions, and other natural-language modules\nwhich operationalize domain knowledge and CPGs to provide context. With primary\ncare physicians (PCP) as our end-users, we present our initial results and\nclinician feedback in this paper. Our POC approach covers multiple knowledge\nsources and clinical scenarios, blends knowledge to explain data and\npredictions to PCPs, and received an enthusiastic response from our medical\nexpert.",
          "link": "http://arxiv.org/abs/2107.02359",
          "publishedOn": "2021-07-07T01:57:11.964Z",
          "wordCount": 717,
          "title": "Leveraging Clinical Context for User-Centered Explainability: A Diabetes Use Case. (arXiv:2107.02359v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shih_P/0/1/0/all/0/1\">Po-Kan Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moraffah_B/0/1/0/all/0/1\">Bahman Moraffah</a>",
          "description": "With the arrival of next generation wireless communication, a growing number\nof new applications like internet of things, autonomous driving systems, and\ndrone are crowding the unlicensed spectrum. Licensed network such as the\nlong-term evolution (LTE) also comes to the unlicensed spectrum for better\nproviding high-capacity contents with low cost. However, LTE was not designed\nto share resources with others. Previous solutions usually work on fixed\nscenarios. This work features a Nonparametric Bayesian reinforcement learning\nalgorithm to cope with the coexistence between Wi-Fi and LTE licensed assisted\naccess (LTE-LAA) agents in 5 GHz unlicensed spectrum. The coexistence problem\nis modeled as a decentralized partially-observable Markov decision process\n(Dec-POMDP) and Bayesian inference is adopted for policy learning with\nnonparametric prior to accommodate the uncertainty of policy for different\nagents. A fairness measure is introduced in the reward function to encourage\nfair sharing between agents. Variational inference for posterior model\napproximation is considered to make the algorithm computationally efficient.\nSimulation results demonstrate that this algorithm can reach high value with\ncompact policy representations in few learning iterations.",
          "link": "http://arxiv.org/abs/2107.02431",
          "publishedOn": "2021-07-07T01:57:11.958Z",
          "wordCount": 623,
          "title": "Bayesian Nonparametric Modelling for Model-Free Reinforcement Learning in LTE-LAA and Wi-Fi Coexistence. (arXiv:2107.02431v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soh_H/0/1/0/all/0/1\">Harold Soh</a>",
          "description": "This work focuses on learning useful and robust deep world models using\nmultiple, possibly unreliable, sensors. We find that current methods do not\nsufficiently encourage a shared representation between modalities; this can\ncause poor performance on downstream tasks and over-reliance on specific\nsensors. As a solution, we contribute a new multi-modal deep latent state-space\nmodel, trained using a mutual information lower-bound. The key innovation is a\nspecially-designed density ratio estimator that encourages consistency between\nthe latent codes of each modality. We tasked our method to learn policies (in a\nself-supervised manner) on multi-modal Natural MuJoCo benchmarks and a\nchallenging Table Wiping task. Experiments show our method significantly\noutperforms state-of-the-art deep reinforcement learning methods, particularly\nin the presence of missing observations.",
          "link": "http://arxiv.org/abs/2107.02339",
          "publishedOn": "2021-07-07T01:57:11.952Z",
          "wordCount": 567,
          "title": "Multi-Modal Mutual Information (MuMMI) Training for Robust Self-Supervised Deep Reinforcement Learning. (arXiv:2107.02339v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Losey_D/0/1/0/all/0/1\">Dylan P. Losey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajcsy_A/0/1/0/all/0/1\">Andrea Bajcsy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OMalley_M/0/1/0/all/0/1\">Marcia K. O&#x27;Malley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1\">Anca D. Dragan</a>",
          "description": "When a robot performs a task next to a human, physical interaction is\ninevitable: the human might push, pull, twist, or guide the robot. The\nstate-of-the-art treats these interactions as disturbances that the robot\nshould reject or avoid. At best, these robots respond safely while the human\ninteracts; but after the human lets go, these robots simply return to their\noriginal behavior. We recognize that physical human-robot interaction (pHRI) is\noften intentional -- the human intervenes on purpose because the robot is not\ndoing the task correctly. In this paper, we argue that when pHRI is intentional\nit is also informative: the robot can leverage interactions to learn how it\nshould complete the rest of its current task even after the person lets go. We\nformalize pHRI as a dynamical system, where the human has in mind an objective\nfunction they want the robot to optimize, but the robot does not get direct\naccess to the parameters of this objective -- they are internal to the human.\nWithin our proposed framework human interactions become observations about the\ntrue objective. We introduce approximations to learn from and respond to pHRI\nin real-time. We recognize that not all human corrections are perfect: often\nusers interact with the robot noisily, and so we improve the efficiency of\nrobot learning from pHRI by reducing unintended learning. Finally, we conduct\nsimulations and user studies on a robotic manipulator to compare our proposed\napproach to the state-of-the-art. Our results indicate that learning from pHRI\nleads to better task performance and improved human satisfaction.",
          "link": "http://arxiv.org/abs/2107.02349",
          "publishedOn": "2021-07-07T01:57:11.946Z",
          "wordCount": 705,
          "title": "Physical Interaction as Communication: Learning Robot Objectives Online from Human Corrections. (arXiv:2107.02349v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>",
          "description": "Over the last few decades, artificial intelligence research has made\ntremendous strides, but it still heavily relies on fixed datasets in stationary\nenvironments. Continual learning is a growing field of research that examines\nhow AI systems can learn sequentially from a continuous stream of linked data\nin the same way that biological systems do. Simultaneously, fake media such as\ndeepfakes and synthetic face images have emerged as significant to current\nmultimedia technologies. Recently, numerous method has been proposed which can\ndetect deepfakes with high accuracy. However, they suffer significantly due to\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\nthis work, we apply continuous learning to neural networks' learning dynamics,\nemphasizing its potential to increase data efficiency significantly. We propose\nContinual Representation using Distillation (CoReD) method that employs the\nconcept of Continual Learning (CoL), Representation Learning (ReL), and\nKnowledge Distillation (KD). We design CoReD to perform sequential domain\nadaptation tasks on new deepfake and GAN-generated synthetic face datasets,\nwhile effectively minimizing the catastrophic forgetting in a teacher-student\nmodel setting. Our extensive experimental results demonstrate that our method\nis efficient at domain adaptation to detect low-quality deepfakes videos and\nGAN-generated images from several datasets, outperforming the-state-of-art\nbaseline methods.",
          "link": "http://arxiv.org/abs/2107.02408",
          "publishedOn": "2021-07-07T01:57:11.940Z",
          "wordCount": 675,
          "title": "CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham M. Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>",
          "description": "Eluder dimension and information gain are two widely used methods of\ncomplexity measures in bandit and reinforcement learning. Eluder dimension was\noriginally proposed as a general complexity measure of function classes, but\nthe common examples of where it is known to be small are function spaces\n(vector spaces). In these cases, the primary tool to upper bound the eluder\ndimension is the elliptic potential lemma. Interestingly, the elliptic\npotential lemma also features prominently in the analysis of linear\nbandits/reinforcement learning and their nonparametric generalization, the\ninformation gain. We show that this is not a coincidence -- eluder dimension\nand information gain are equivalent in a precise sense for reproducing kernel\nHilbert spaces.",
          "link": "http://arxiv.org/abs/2107.02377",
          "publishedOn": "2021-07-07T01:57:11.925Z",
          "wordCount": 568,
          "title": "A Short Note on the Relationship of Information Gain and Eluder Dimension. (arXiv:2107.02377v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">J. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">X. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">S. Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">L. Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu%2A_H/0/1/0/all/0/1\">H. Liu*</a>",
          "description": "Drug combination therapy has become a increasingly promising method in the\ntreatment of cancer. However, the number of possible drug combinations is so\nhuge that it is hard to screen synergistic drug combinations through wet-lab\nexperiments. Therefore, computational screening has become an important way to\nprioritize drug combinations. Graph neural network have recently shown\nremarkable performance in the prediction of compound-protein interactions, but\nit has not been applied to the screening of drug combinations. In this paper,\nwe proposed a deep learning model based on graph neural networks and attention\nmechanism to identify drug combinations that can effectively inhibit the\nviability of specific cancer cells. The feature embeddings of drug molecule\nstructure and gene expression profiles were taken as input to multi-layer\nfeedforward neural network to identify the synergistic drug combinations. We\ncompared DeepDDS with classical machine learning methods and other deep\nlearning-based methods on benchmark data set, and the leave-one-out\nexperimental results showed that DeepDDS achieved better performance than\ncompetitive methods. Also, on an independent test set released by well-known\npharmaceutical enterprise AstraZeneca, DeepDDS was superior to competitive\nmethods by more than 16\\% predictive precision. Furthermore, we explored the\ninterpretability of the graph attention network, and found the correlation\nmatrix of atomic features revealed important chemical substructures of drugs.\nWe believed that DeepDDS is an effective tool that prioritized synergistic drug\ncombinations for further wet-lab experiment validation.",
          "link": "http://arxiv.org/abs/2107.02467",
          "publishedOn": "2021-07-07T01:57:11.919Z",
          "wordCount": 675,
          "title": "DeepDDS: deep graph neural network with attention mechanism to predict synergistic drug combinations. (arXiv:2107.02467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huber_C/0/1/0/all/0/1\">Christian Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_J/0/1/0/all/0/1\">Juan Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuker_S/0/1/0/all/0/1\">Sebastian St&#xfc;ker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>",
          "description": "Neural sequence-to-sequence systems deliver state-of-the-art performance for\nautomatic speech recognition (ASR). When using appropriate modeling units,\ne.g., byte-pair encoded characters, these systems are in principal open\nvocabulary systems. In practice, however, they often fail to recognize words\nnot seen during training, e.g., named entities, numbers or technical terms. To\nalleviate this problem we supplement an end-to-end ASR system with a\nword/phrase memory and a mechanism to access this memory to recognize the words\nand phrases correctly. After the training of the ASR system, and when it has\nalready been deployed, a relevant word can be added or subtracted instantly\nwithout the need for further training. In this paper we demonstrate that\nthrough this mechanism our system is able to recognize more than 85% of newly\nadded words that it previously failed to recognize compared to a strong\nbaseline.",
          "link": "http://arxiv.org/abs/2107.02268",
          "publishedOn": "2021-07-07T01:57:11.913Z",
          "wordCount": 582,
          "title": "Instant One-Shot Word-Learning for Context-Specific Neural Sequence-to-Sequence Speech Recognition. (arXiv:2107.02268v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leinen_F/0/1/0/all/0/1\">Fabian Leinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cozzolino_V/0/1/0/all/0/1\">Vittorio Cozzolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1\">Torsten Sch&#xf6;n</a>",
          "description": "Human body volume estimation from a single RGB image is a challenging problem\ndespite minimal attention from the research community. However VolNet, an\narchitecture leveraging 2D and 3D pose estimation, body part segmentation and\nvolume regression extracted from a single 2D RGB image combined with the\nsubject's body height can be used to estimate the total body volume. VolNet is\ndesigned to predict the 2D and 3D pose as well as the body part segmentation in\nintermediate tasks. We generated a synthetic, large-scale dataset of\nphoto-realistic images of human bodies with a wide range of body shapes and\nrealistic poses called SURREALvols. By using Volnet and combining multiple\nstacked hourglass networks together with ResNeXt, our model correctly predicted\nthe volume in ~82% of cases with a 10% tolerance threshold. This is a\nconsiderable improvement compared to state-of-the-art solutions such as BodyNet\nwith only a ~38% success rate.",
          "link": "http://arxiv.org/abs/2107.02259",
          "publishedOn": "2021-07-07T01:57:11.906Z",
          "wordCount": 596,
          "title": "VolNet: Estimating Human Body Part Volumes from a Single RGB Image. (arXiv:2107.02259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02345",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1\">Ricky Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_T/0/1/0/all/0/1\">Timothy T. Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_G/0/1/0/all/0/1\">Gavin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_D/0/1/0/all/0/1\">Da Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarunic_M/0/1/0/all/0/1\">Marinko V. Sarunic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beg_M/0/1/0/all/0/1\">Mirza Faisal Beg</a>",
          "description": "With the FDA approval of Artificial Intelligence (AI) for point-of-care\nclinical diagnoses, model generalizability is of the utmost importance as\nclinical decision-making must be domain-agnostic. A method of tackling the\nproblem is to increase the dataset to include images from a multitude of\ndomains; while this technique is ideal, the security requirements of medical\ndata is a major limitation. Additionally, researchers with developed tools\nbenefit from the addition of open-sourced data, but are limited by the\ndifference in domains. Herewith, we investigated the implementation of a\nCycle-Consistent Generative Adversarial Networks (CycleGAN) for the domain\nadaptation of Optical Coherence Tomography (OCT) volumes. This study was done\nin collaboration with the Biomedical Optics Research Group and Functional &\nAnatomical Imaging & Shape Analysis Lab at Simon Fraser University. In this\nstudy, we investigated a learning-based approach of adapting the domain of a\npublicly available dataset, UK Biobank dataset (UKB). To evaluate the\nperformance of domain adaptation, we utilized pre-existing retinal layer\nsegmentation tools developed on a different set of RETOUCH OCT data. This study\nprovides insight on state-of-the-art tools for domain adaptation compared to\ntraditional processing techniques as well as a pipeline for adapting publicly\navailable retinal data to the domains previously used by our collaborators.",
          "link": "http://arxiv.org/abs/2107.02345",
          "publishedOn": "2021-07-07T01:57:11.892Z",
          "wordCount": 675,
          "title": "Domain Adaptation via CycleGAN for Retina Segmentation in Optical Coherence Tomography. (arXiv:2107.02345v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sofianidis_G/0/1/0/all/0/1\">Georgios Sofianidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozanec_J/0/1/0/all/0/1\">Jo&#x17e;e M. Ro&#x17e;anec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mladenic_D/0/1/0/all/0/1\">Dunja Mladeni&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyriazis_D/0/1/0/all/0/1\">Dimosthenis Kyriazis</a>",
          "description": "The implementation of Artificial Intelligence (AI) systems in the\nmanufacturing domain enables higher production efficiency, outstanding\nperformance, and safer operations, leveraging powerful tools such as deep\nlearning and reinforcement learning techniques. Despite the high accuracy of\nthese models, they are mostly considered black boxes: they are unintelligible\nto the human. Opaqueness affects trust in the system, a factor that is critical\nin the context of decision-making. We present an overview of Explainable\nArtificial Intelligence (XAI) techniques as a means of boosting the\ntransparency of models. We analyze different metrics to evaluate these\ntechniques and describe several application scenarios in the manufacturing\ndomain.",
          "link": "http://arxiv.org/abs/2107.02295",
          "publishedOn": "2021-07-07T01:57:11.880Z",
          "wordCount": 537,
          "title": "A Review of Explainable Artificial Intelligence in Manufacturing. (arXiv:2107.02295v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1\">Dylan J. Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Akshay Krishnamurthy</a>",
          "description": "A recurring theme in statistical learning, online learning, and beyond is\nthat faster convergence rates are possible for problems with low noise, often\nquantified by the performance of the best hypothesis; such results are known as\nfirst-order or small-loss guarantees. While first-order guarantees are\nrelatively well understood in statistical and online learning, adapting to low\nnoise in contextual bandits (and more broadly, decision making) presents major\nalgorithmic challenges. In a COLT 2017 open problem, Agarwal, Krishnamurthy,\nLangford, Luo, and Schapire asked whether first-order guarantees are even\npossible for contextual bandits and -- if so -- whether they can be attained by\nefficient algorithms. We give a resolution to this question by providing an\noptimal and efficient reduction from contextual bandits to online regression\nwith the logarithmic (or, cross-entropy) loss. Our algorithm is simple and\npractical, readily accommodates rich function classes, and requires no\ndistributional assumptions beyond realizability. In a large-scale empirical\nevaluation, we find that our approach typically outperforms comparable\nnon-first-order methods.\n\nOn the technical side, we show that the logarithmic loss and an\ninformation-theoretic quantity called the triangular discrimination play a\nfundamental role in obtaining first-order guarantees, and we combine this\nobservation with new refinements to the regression oracle reduction framework\nof Foster and Rakhlin. The use of triangular discrimination yields novel\nresults even for the classical statistical learning model, and we anticipate\nthat it will find broader use.",
          "link": "http://arxiv.org/abs/2107.02237",
          "publishedOn": "2021-07-07T01:57:11.874Z",
          "wordCount": 670,
          "title": "Efficient First-Order Contextual Bandits: Prediction, Allocation, and Triangular Discrimination. (arXiv:2107.02237v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fazzini_P/0/1/0/all/0/1\">Paolo Fazzini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torre_M/0/1/0/all/0/1\">Marco Torre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizza_V/0/1/0/all/0/1\">Valeria Rizza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petracchini_F/0/1/0/all/0/1\">Francesco Petracchini</a>",
          "description": "Adaptive traffic signal control (ATSC) in urban traffic networks poses a\nchallenging task due to the complicated dynamics arising in traffic systems. In\nrecent years, several approaches based on multi-agent deep reinforcement\nlearning (MARL) have been studied experimentally. These approaches propose\ndistributed techniques in which each signalized intersection is seen as an\nagent in a stochastic game whose purpose is to optimize the flow of vehicles in\nits vicinity. In this setting, the systems evolves towards an equilibrium among\nthe agents that shows beneficial for the whole traffic network. A recently\ndeveloped multi-agent variant of the well-established advantage actor-critic\n(A2C) algorithm, called MA2C (multi-agent A2C) exploits the promising idea of\nsome communication among the agents. In this view,the agents share their\nstrategies with other neighbor agents, thereby stabilizing the learning process\neven when the agents grow in number and variety. We experimented MA2C in two\ntraffic networks located in Bologna (Italy) and found that its action\ntranslates into a significant decrease of the amount of pollutants released\ninto the environment.",
          "link": "http://arxiv.org/abs/2107.02361",
          "publishedOn": "2021-07-07T01:57:11.867Z",
          "wordCount": 622,
          "title": "Effects of Smart Traffic Signal Control on Air Quality. (arXiv:2107.02361v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02232",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Aguilar_X/0/1/0/all/0/1\">Xavier Aguilar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Markidis_S/0/1/0/all/0/1\">Stefano Markidis</a>",
          "description": "We design and develop a new Particle-in-Cell (PIC) method for plasma\nsimulations using Deep-Learning (DL) to calculate the electric field from the\nelectron phase space. We train a Multilayer Perceptron (MLP) and a\nConvolutional Neural Network (CNN) to solve the two-stream instability test. We\nverify that the DL-based MLP PIC method produces the correct results using the\ntwo-stream instability: the DL-based PIC provides the expected growth rate of\nthe two-stream instability. The DL-based PIC does not conserve the total energy\nand momentum. However, the DL-based PIC method is stable against the cold-beam\ninstability, affecting traditional PIC methods. This work shows that\nintegrating DL technologies into traditional computational methods is a viable\napproach for developing next-generation PIC algorithms.",
          "link": "http://arxiv.org/abs/2107.02232",
          "publishedOn": "2021-07-07T01:57:11.862Z",
          "wordCount": 560,
          "title": "A Deep Learning-Based Particle-in-Cell Method for Plasma Simulations. (arXiv:2107.02232v1 [physics.plasm-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bozic_A/0/1/0/all/0/1\">Alja&#x17e; Bo&#x17e;i&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palafox_P/0/1/0/all/0/1\">Pablo Palafox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>",
          "description": "We introduce TransformerFusion, a transformer-based 3D scene reconstruction\napproach. From an input monocular RGB video, the video frames are processed by\na transformer network that fuses the observations into a volumetric feature\ngrid representing the scene; this feature grid is then decoded into an implicit\n3D scene representation. Key to our approach is the transformer architecture\nthat enables the network to learn to attend to the most relevant image frames\nfor each 3D location in the scene, supervised only by the scene reconstruction\ntask. Features are fused in a coarse-to-fine fashion, storing fine-level\nfeatures only where needed, requiring lower memory storage and enabling fusion\nat interactive rates. The feature grid is then decoded to a higher-resolution\nscene reconstruction, using an MLP-based surface occupancy prediction from\ninterpolated coarse-to-fine 3D features. Our approach results in an accurate\nsurface reconstruction, outperforming state-of-the-art multi-view stereo depth\nestimation methods, fully-convolutional 3D reconstruction approaches, and\napproaches using LSTM- or GRU-based recurrent networks for video sequence\nfusion.",
          "link": "http://arxiv.org/abs/2107.02191",
          "publishedOn": "2021-07-07T01:57:11.856Z",
          "wordCount": 608,
          "title": "TransformerFusion: Monocular RGB Scene Reconstruction using Transformers. (arXiv:2107.02191v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobian_H/0/1/0/all/0/1\">Hamed Yaghoobian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabnia_H/0/1/0/all/0/1\">Hamid R. Arabnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_K/0/1/0/all/0/1\">Khaled Rasheed</a>",
          "description": "Sarcasm detection is the task of identifying irony containing utterances in\nsentiment-bearing text. However, the figurative and creative nature of sarcasm\nposes a great challenge for affective computing systems performing sentiment\nanalysis. This article compiles and reviews the salient work in the literature\nof automatic sarcasm detection. Thus far, three main paradigm shifts have\noccurred in the way researchers have approached this task: 1) semi-supervised\npattern extraction to identify implicit sentiment, 2) use of hashtag-based\nsupervision, and 3) incorporation of context beyond target text. In this\narticle, we provide a comprehensive review of the datasets, approaches, trends,\nand issues in sarcasm and irony detection.",
          "link": "http://arxiv.org/abs/2107.02276",
          "publishedOn": "2021-07-07T01:57:11.818Z",
          "wordCount": 533,
          "title": "Sarcasm Detection: A Comparative Study. (arXiv:2107.02276v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02266",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Khamaru_K/0/1/0/all/0/1\">Koulik Khamaru</a>, <a href=\"http://arxiv.org/find/math/1/au:+Deshpande_Y/0/1/0/all/0/1\">Yash Deshpande</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wainwright_M/0/1/0/all/0/1\">Martin J. Wainwright</a>",
          "description": "When data is collected in an adaptive manner, even simple methods like\nordinary least squares can exhibit non-normal asymptotic behavior. As an\nundesirable consequence, hypothesis tests and confidence intervals based on\nasymptotic normality can lead to erroneous results. We propose an online\ndebiasing estimator to correct these distributional anomalies in least squares\nestimation. Our proposed method takes advantage of the covariance structure\npresent in the dataset and provides sharper estimates in directions for which\nmore information has accrued. We establish an asymptotic normality property for\nour proposed online debiasing estimator under mild conditions on the data\ncollection process, and provide asymptotically exact confidence intervals. We\nadditionally prove a minimax lower bound for the adaptive linear regression\nproblem, thereby providing a baseline by which to compare estimators. There are\nvarious conditions under which our proposed estimator achieves the minimax\nlower bound up to logarithmic factors. We demonstrate the usefulness of our\ntheory via applications to multi-armed bandit, autoregressive time series\nestimation, and active learning with exploration.",
          "link": "http://arxiv.org/abs/2107.02266",
          "publishedOn": "2021-07-07T01:57:11.804Z",
          "wordCount": 606,
          "title": "Near-optimal inference in adaptive linear regression. (arXiv:2107.02266v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kristy Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Madeline Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "Density ratio estimation serves as an important technique in the unsupervised\nmachine learning toolbox. However, such ratios are difficult to estimate for\ncomplex, high-dimensional data, particularly when the densities of interest are\nsufficiently different. In our work, we propose to leverage an invertible\ngenerative model to map the two distributions into a common feature space prior\nto estimation. This featurization brings the densities closer together in\nlatent space, sidestepping pathological scenarios where the learned density\nratios in input space can be arbitrarily inaccurate. At the same time, the\ninvertibility of our feature map guarantees that the ratios computed in feature\nspace are equivalent to those in input space. Empirically, we demonstrate the\nefficacy of our approach in a variety of downstream tasks that require access\nto accurate density ratios such as mutual information estimation, targeted\nsampling in deep generative models, and classification with data augmentation.",
          "link": "http://arxiv.org/abs/2107.02212",
          "publishedOn": "2021-07-07T01:57:11.798Z",
          "wordCount": 575,
          "title": "Featurized Density Ratio Estimation. (arXiv:2107.02212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cahuantzi_R/0/1/0/all/0/1\">Roberto Cahuantzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinye Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guttel_S/0/1/0/all/0/1\">Stefan G&#xfc;ttel</a>",
          "description": "We explore relations between the hyper-parameters of a recurrent neural\nnetwork (RNN) and the complexity of string sequences it is able to memorize. We\ncompare long short-term memory (LSTM) networks and gated recurrent units\n(GRUs). We find that an increase of RNN depth does not necessarily result in\nbetter memorization capability when the training time is constrained. Our\nresults also indicate that the learning rate and the number of units per layer\nare among the most important hyper-parameters to be tuned. Generally, GRUs\noutperform LSTM networks on low complexity sequences while on high complexity\nsequences LSTMs perform better.",
          "link": "http://arxiv.org/abs/2107.02248",
          "publishedOn": "2021-07-07T01:57:11.775Z",
          "wordCount": 560,
          "title": "A comparison of LSTM and GRU networks for learning symbolic sequences. (arXiv:2107.02248v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aadirupa Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaillard_P/0/1/0/all/0/1\">Pierre Gaillard</a>",
          "description": "We introduce the problem of sleeping dueling bandits with stochastic\npreferences and adversarial availabilities (DB-SPAA). In almost all dueling\nbandit applications, the decision space often changes over time; eg, retail\nstore management, online shopping, restaurant recommendation, search engine\noptimization, etc. Surprisingly, this `sleeping aspect' of dueling bandits has\nnever been studied in the literature. Like dueling bandits, the goal is to\ncompete with the best arm by sequentially querying the preference feedback of\nitem pairs. The non-triviality however results due to the non-stationary item\nspaces that allow any arbitrary subsets items to go unavailable every round.\nThe goal is to find an optimal `no-regret' policy that can identify the best\navailable item at each round, as opposed to the standard `fixed best-arm regret\nobjective' of dueling bandits. We first derive an instance-specific lower bound\nfor DB-SPAA $\\Omega( \\sum_{i =1}^{K-1}\\sum_{j=i+1}^K \\frac{\\log\nT}{\\Delta(i,j)})$, where $K$ is the number of items and $\\Delta(i,j)$ is the\ngap between items $i$ and $j$. This indicates that the sleeping problem with\npreference feedback is inherently more difficult than that for classical\nmulti-armed bandits (MAB). We then propose two algorithms, with near optimal\nregret guarantees. Our results are corroborated empirically.",
          "link": "http://arxiv.org/abs/2107.02274",
          "publishedOn": "2021-07-07T01:57:11.752Z",
          "wordCount": 616,
          "title": "Dueling Bandits with Adversarial Sleeping. (arXiv:2107.02274v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Go_K/0/1/0/all/0/1\">Kyeongryeol Go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Seyoung Yun</a>",
          "description": "Meta-learning aims to learn a model that can handle multiple tasks generated\nfrom an unknown but shared distribution. However, typical meta-learning\nalgorithms have assumed the tasks to be similar such that a single meta-learner\nis sufficient to aggregate the variations in all aspects. In addition, there\nhas been less consideration on uncertainty when limited information is given as\ncontext. In this paper, we devise a novel meta-learning framework, called\nMeta-learning Amidst Heterogeneity and Ambiguity (MAHA), that outperforms\nprevious works in terms of prediction based on its ability on task\nidentification. By extensively conducting several experiments in regression and\nclassification, we demonstrate the validity of our model, which turns out to be\nrobust to both task heterogeneity and ambiguity.",
          "link": "http://arxiv.org/abs/2107.02228",
          "publishedOn": "2021-07-07T01:57:11.736Z",
          "wordCount": 542,
          "title": "Meta-learning Amidst Heterogeneity and Ambiguity. (arXiv:2107.02228v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cascarano_P/0/1/0/all/0/1\">Pasquale Cascarano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comes_M/0/1/0/all/0/1\">Maria Colomba Comes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_A/0/1/0/all/0/1\">Andrea Sebastiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mencattini_A/0/1/0/all/0/1\">Arianna Mencattini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccolomini_E/0/1/0/all/0/1\">Elena Loli Piccolomini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinelli_E/0/1/0/all/0/1\">Eugenio Martinelli</a>",
          "description": "In fluorescence microscopy, Single Molecule Localization Microscopy (SMLM)\ntechniques aim at localizing with high precision high density fluorescent\nmolecules by stochastically activating and imaging small subsets of blinking\nemitters. Super Resolution (SR) plays an important role in this field since it\nallows to go beyond the intrinsic light diffraction limit. In this work, we\npropose a deep learning-based algorithm for precise molecule localization of\nhigh density frames acquired by SMLM techniques whose $\\ell_{2}$-based loss\nfunction is regularized by positivity and $\\ell_{0}$-based constraints. The\n$\\ell_{0}$ is relaxed through its Continuous Exact $\\ell_{0}$ (CEL0)\ncounterpart. The arising approach, named DeepCEL0, is parameter-free, more\nflexible, faster and provides more precise molecule localization maps if\ncompared to the other state-of-the-art methods. We validate our approach on\nboth simulated and real fluorescence microscopy data.",
          "link": "http://arxiv.org/abs/2107.02281",
          "publishedOn": "2021-07-07T01:57:11.716Z",
          "wordCount": 594,
          "title": "DeepCEL0 for 2D Single Molecule Localization in Fluorescence Microscopy. (arXiv:2107.02281v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cachay_S/0/1/0/all/0/1\">Salva R&#xfc;hling Cachay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1\">Artur Dubrawski</a>",
          "description": "Aggregating multiple sources of weak supervision (WS) can ease the\ndata-labeling bottleneck prevalent in many machine learning applications, by\nreplacing the tedious manual collection of ground truth labels. Current state\nof the art approaches that do not use any labeled training data, however,\nrequire two separate modeling steps: Learning a probabilistic latent variable\nmodel based on the WS sources -- making assumptions that rarely hold in\npractice -- followed by downstream model training. Importantly, the first step\nof modeling does not consider the performance of the downstream model. To\naddress these caveats we propose an end-to-end approach for directly learning\nthe downstream model by maximizing its agreement with probabilistic labels\ngenerated by reparameterizing previous probabilistic posteriors with a neural\nnetwork. Our results show improved performance over prior work in terms of end\nmodel performance on downstream test sets, as well as in terms of improved\nrobustness to dependencies among weak supervision sources.",
          "link": "http://arxiv.org/abs/2107.02233",
          "publishedOn": "2021-07-07T01:57:11.710Z",
          "wordCount": 580,
          "title": "End-to-End Weak Supervision. (arXiv:2107.02233v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02211",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Peciulis_R/0/1/0/all/0/1\">Rokas Pe&#x10d;iulis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lukosevicius_M/0/1/0/all/0/1\">Mantas Luko&#x161;evi&#x10d;ius</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krisciukaitis_A/0/1/0/all/0/1\">Algimantas Kri&#x161;&#x10d;iukaitis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petrolis_R/0/1/0/all/0/1\">Robertas Petrolis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buteikiene_D/0/1/0/all/0/1\">Dovil&#x117; Buteikien&#x117;</a>",
          "description": "This work aims to research an automatic method for detecting Age-related\nMacular Degeneration (AMD) lesions in RGB eye fundus images. For this, we align\ninvasively obtained eye fundus contrast images (the \"golden standard\"\ndiagnostic) to the RGB ones and use them to hand-annotate the lesions. This is\ndone using our custom-made tool. Using the data, we train and test five\ndifferent convolutional neural networks: a custom one to classify healthy and\nAMD-affected eye fundi, and four well-known networks: ResNet50, ResNet101,\nMobileNetV3, and UNet to segment (localize) the AMD lesions in the affected eye\nfundus images. We achieve 93.55% accuracy or 69.71% Dice index as the\npreliminary best results in segmentation with MobileNetV3.",
          "link": "http://arxiv.org/abs/2107.02211",
          "publishedOn": "2021-07-07T01:57:11.625Z",
          "wordCount": 575,
          "title": "Automated age-related macular degeneration area estimation -- first results. (arXiv:2107.02211v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_D/0/1/0/all/0/1\">Dustin Hayes</a>",
          "description": "This paper contributes to interpretable machine learning via visual knowledge\ndiscovery in parallel coordinates. The concepts of hypercubes and hyper-blocks\nare used as easily understandable by end-users in the visual form in parallel\ncoordinates. The Hyper algorithm for classification with mixed and pure\nhyper-blocks (HBs) is proposed to discover hyper-blocks interactively and\nautomatically in individual, multiple, overlapping, and non-overlapping\nsetting. The combination of hyper-blocks with linguistic description of visual\npatterns is presented too. It is shown that Hyper models generalize decision\ntrees. The Hyper algorithm was tested on the benchmark data from UCI ML\nrepository. It allowed discovering pure and mixed HBs with all data and then\nwith 10-fold cross validation. The links between hyper-blocks, dimension\nreduction and visualization are established. Major benefits of hyper-block\ntechnology and the Hyper algorithm are in their ability to discover and observe\nhyper-blocks by end-users including side by side visualizations making patterns\nvisible for all classes. Another advantage of sets of HBs relative to the\ndecision trees is the ability to avoid both data overgeneralization and\noverfitting.",
          "link": "http://arxiv.org/abs/2106.07474",
          "publishedOn": "2021-07-06T01:58:11.980Z",
          "wordCount": 629,
          "title": "Discovering Interpretable Machine Learning Models in Parallel Coordinates. (arXiv:2106.07474v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazrae_P/0/1/0/all/0/1\">Pooya Rostami Mazrae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1\">Maliheh Izadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heydarnoori_A/0/1/0/all/0/1\">Abbas Heydarnoori</a>",
          "description": "An issue documents discussions around required changes in issue-tracking\nsystems, while a commit contains the change itself in the version control\nsystems. Recovering links between issues and commits can facilitate many\nsoftware evolution tasks such as bug localization, and software documentation.\nA previous study on over half a million issues from GitHub reports only about\n42.2% of issues are manually linked by developers to their pertinent commits.\nAutomating the linking of commit-issue pairs can contribute to the improvement\nof the said tasks. By far, current state-of-the-art approaches for automated\ncommit-issue linking suffer from low precision, leading to unreliable results,\nsometimes to the point that imposes human supervision on the predicted links.\nThe low performance gets even more severe when there is a lack of textual\ninformation in either commits or issues. Current approaches are also proven\ncomputationally expensive.\n\nWe propose Hybrid-Linker to overcome such limitations by exploiting two\ninformation channels; (1) a non-textual-based component that operates on\nnon-textual, automatically recorded information of the commit-issue pairs to\npredict a link, and (2) a textual-based one which does the same using textual\ninformation of the commit-issue pairs. Then, combining the results from the two\nclassifiers, Hybrid-Linker makes the final prediction. Thus, every time one\ncomponent falls short in predicting a link, the other component fills the gap\nand improves the results. We evaluate Hybrid-Linker against competing\napproaches, namely FRLink and DeepLink on a dataset of 12 projects.\nHybrid-Linker achieves 90.1%, 87.8%, and 88.9% based on recall, precision, and\nF-measure, respectively. It also outperforms FRLink and DeepLink by 31.3%, and\n41.3%, regarding the F-measure. Moreover, Hybrid-Linker exhibits extensive\nimprovements in terms of performance as well.",
          "link": "http://arxiv.org/abs/2107.01894",
          "publishedOn": "2021-07-06T01:58:11.963Z",
          "wordCount": 728,
          "title": "Automated Recovery of Issue-Commit Links Leveraging Both Textual and Non-textual Data. (arXiv:2107.01894v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1\">Poojan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindagi_V/0/1/0/all/0/1\">Vishwanath A. Sindagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1\">Vibashan VS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Recent advances in deep learning have led to the development of accurate and\nefficient models for various computer vision applications such as\nclassification, segmentation, and detection. However, learning highly accurate\nmodels relies on the availability of large-scale annotated datasets. Due to\nthis, model performance drops drastically when evaluated on label-scarce\ndatasets having visually distinct images, termed as domain adaptation problem.\nThere is a plethora of works to adapt classification and segmentation models to\nlabel-scarce target datasets through unsupervised domain adaptation.\nConsidering that detection is a fundamental task in computer vision, many\nrecent works have focused on developing novel domain adaptive detection\ntechniques. Here, we describe in detail the domain adaptation problem for\ndetection and present an extensive survey of the various methods. Furthermore,\nwe highlight strategies proposed and the associated shortcomings. Subsequently,\nwe identify multiple aspects of the problem that are most promising for future\nresearch. We believe that this survey shall be valuable to the pattern\nrecognition experts working in the fields of computer vision, biometrics,\nmedical imaging, and autonomous navigation by introducing them to the problem,\nand familiarizing them with the current status of the progress while providing\npromising directions for future research.",
          "link": "http://arxiv.org/abs/2105.13502",
          "publishedOn": "2021-07-06T01:58:11.957Z",
          "wordCount": 667,
          "title": "Unsupervised Domain Adaptation of Object Detectors: A Survey. (arXiv:2105.13502v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagaraju_R/0/1/0/all/0/1\">Rakesh Nagaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "Generative adversarial networks (GAN) are a class of powerful machine\nlearning techniques, where both a generative and discriminative model are\ntrained simultaneously. GANs have been used, for example, to successfully\ngenerate \"deep fake\" images. A recent trend in malware research consists of\ntreating executables as images and employing image-based analysis techniques.\nIn this research, we generate fake malware images using auxiliary classifier\nGANs (AC-GAN), and we consider the effectiveness of various techniques for\nclassifying the resulting images. Our results indicate that the resulting\nmulticlass classification problem is challenging, yet we can obtain strong\nresults when restricting the problem to distinguishing between real and fake\nsamples. While the AC-GAN generated images often appear to be very similar to\nreal malware images, we conclude that from a deep learning perspective, the\nAC-GAN generated samples do not rise to the level of deep fake malware images.",
          "link": "http://arxiv.org/abs/2107.01620",
          "publishedOn": "2021-07-06T01:58:11.950Z",
          "wordCount": 568,
          "title": "Auxiliary-Classifier GAN for Malware Analysis. (arXiv:2107.01620v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erven_T/0/1/0/all/0/1\">Tim van Erven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachs_S/0/1/0/all/0/1\">Sarah Sachs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koolen_W/0/1/0/all/0/1\">Wouter M. Koolen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotlowski_W/0/1/0/all/0/1\">Wojciech Kot&#x142;owski</a>",
          "description": "We consider online convex optimization when a number k of data points are\noutliers that may be corrupted. We model this by introducing the notion of\nrobust regret, which measures the regret only on rounds that are not outliers.\nThe aim for the learner is to achieve small robust regret, without knowing\nwhere the outliers are. If the outliers are chosen adversarially, we show that\na simple filtering strategy on extreme gradients incurs O(k) additive overhead\ncompared to the usual regret bounds, and that this is unimprovable, which means\nthat k needs to be sublinear in the number of rounds. We further ask which\nadditional assumptions would allow for a linear number of outliers. It turns\nout that the usual benign cases of independently, identically distributed\n(i.i.d.) observations or strongly convex losses are not sufficient. However,\ncombining i.i.d. observations with the assumption that outliers are those\nobservations that are in an extreme quantile of the distribution, does lead to\nsublinear robust regret, even though the expected number of outliers is linear.",
          "link": "http://arxiv.org/abs/2107.01881",
          "publishedOn": "2021-07-06T01:58:11.944Z",
          "wordCount": 607,
          "title": "Robust Online Convex Optimization in the Presence of Outliers. (arXiv:2107.01881v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01906",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Azizian_W/0/1/0/all/0/1\">Wa&#xef;ss Azizian</a>, <a href=\"http://arxiv.org/find/math/1/au:+Iutzeler_F/0/1/0/all/0/1\">Franck Iutzeler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Malick_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Malick</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mertikopoulos_P/0/1/0/all/0/1\">Panayotis Mertikopoulos</a>",
          "description": "In this paper, we analyze the local convergence rate of optimistic mirror\ndescent methods in stochastic variational inequalities, a class of optimization\nproblems with important applications to learning theory and machine learning.\nOur analysis reveals an intricate relation between the algorithm's rate of\nconvergence and the local geometry induced by the method's underlying Bregman\nfunction. We quantify this relation by means of the Legendre exponent, a notion\nthat we introduce to measure the growth rate of the Bregman divergence relative\nto the ambient norm near a solution. We show that this exponent determines both\nthe optimal step-size policy of the algorithm and the optimal rates attained,\nexplaining in this way the differences observed for some popular Bregman\nfunctions (Euclidean projection, negative entropy, fractional power, etc.).",
          "link": "http://arxiv.org/abs/2107.01906",
          "publishedOn": "2021-07-06T01:58:11.935Z",
          "wordCount": 597,
          "title": "The Last-Iterate Convergence Rate of Optimistic Mirror Descent in Stochastic Variational Inequalities. (arXiv:2107.01906v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2005.09310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas Lane</a>",
          "description": "Knowledge distillation has been widely used to compress existing deep\nlearning models while preserving the performance on a wide range of\napplications. In the specific context of Automatic Speech Recognition (ASR),\ndistillation from ensembles of acoustic models has recently shown promising\nresults in increasing recognition performance. In this paper, we propose an\nextension of multi-teacher distillation methods to joint CTC-attention\nend-to-end ASR systems. We also introduce three novel distillation strategies.\nThe core intuition behind them is to integrate the error rate metric to the\nteacher selection rather than solely focusing on the observed losses. In this\nway, we directly distill and optimize the student toward the relevant metric\nfor speech recognition. We evaluate these strategies under a selection of\ntraining procedures on different datasets (TIMIT, Librispeech, Common Voice)\nand various languages (English, French, Italian). In particular,\nstate-of-the-art error rates are reported on the Common Voice French, Italian\nand TIMIT datasets.",
          "link": "http://arxiv.org/abs/2005.09310",
          "publishedOn": "2021-07-06T01:58:11.917Z",
          "wordCount": 641,
          "title": "Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition. (arXiv:2005.09310v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicek_S/0/1/0/all/0/1\">Safa Cicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We present a method for inferring dense depth maps from images and sparse\ndepth measurements by leveraging synthetic data to learn the association of\nsparse point clouds with dense natural shapes, and using the image as evidence\nto validate the predicted depth map. Our learned prior for natural shapes uses\nonly sparse depth as input, not images, so the method is not affected by the\ncovariate shift when attempting to transfer learned models from synthetic data\nto real ones. This allows us to use abundant synthetic data with ground truth\nto learn the most difficult component of the reconstruction process, which is\ntopology estimation, and use the image to refine the prediction based on\nphotometric evidence. Our approach uses fewer parameters than previous methods,\nyet, achieves the state of the art on both indoor and outdoor benchmark\ndatasets. Code available at:\nhttps://github.com/alexklwong/learning-topology-synthetic-data.",
          "link": "http://arxiv.org/abs/2106.02994",
          "publishedOn": "2021-07-06T01:58:11.910Z",
          "wordCount": 614,
          "title": "Learning Topology from Synthetic Data for Unsupervised Depth Completion. (arXiv:2106.02994v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10488",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_T/0/1/0/all/0/1\">Taeeon Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_B/0/1/0/all/0/1\">Byeongjoon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baek_J/0/1/0/all/0/1\">Jongduk Baek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We tackle a challenging blind image denoising problem, in which only single\ndistinct noisy images are available for training a denoiser, and no information\nabout noise is known, except for it being zero-mean, additive, and independent\nof the clean image. In such a setting, which often occurs in practice, it is\nnot possible to train a denoiser with the standard discriminative training or\nwith the recently developed Noise2Noise (N2N) training; the former requires the\nunderlying clean image for the given noisy image, and the latter requires two\nindependently realized noisy image pair for a clean image. To that end, we\npropose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise)\nmethod that first learns a generative model that can 1) simulate the noise in\nthe given noisy images and 2) generate a rough, noisy estimates of the clean\nimages, then 3) iteratively trains a denoiser with subsequently synthesized\nnoisy image pairs (as in N2N), obtained from the generative model. In results,\nwe show the denoiser trained with our GAN2GAN achieves an impressive denoising\nperformance on both synthetic and real-world datasets for the blind denoising\nsetting; it almost approaches the performance of the standard\ndiscriminatively-trained or N2N-trained models that have more information than\nours, and it significantly outperforms the recent baseline for the same\nsetting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one,\nBM3D. The official code of our method is available at\nhttps://github.com/csm9493/GAN2GAN.",
          "link": "http://arxiv.org/abs/1905.10488",
          "publishedOn": "2021-07-06T01:58:11.904Z",
          "wordCount": 738,
          "title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images. (arXiv:1905.10488v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Recasens_D/0/1/0/all/0/1\">David Recasens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamarca_J/0/1/0/all/0/1\">Jos&#xe9; Lamarca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facil_J/0/1/0/all/0/1\">Jos&#xe9; M. F&#xe1;cil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montiel_J/0/1/0/all/0/1\">J. M. M. Montiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>",
          "description": "Estimating a scene reconstruction and the camera motion from in-body videos\nis challenging due to several factors, e.g. the deformation of in-body cavities\nor the lack of texture. In this paper we present Endo-Depth-and-Motion, a\npipeline that estimates the 6-degrees-of-freedom camera pose and dense 3D scene\nmodels from monocular endoscopic videos. Our approach leverages recent advances\nin self-supervised depth networks to generate pseudo-RGBD frames, then tracks\nthe camera pose using photometric residuals and fuses the registered depth maps\nin a volumetric representation. We present an extensive experimental evaluation\nin the public dataset Hamlyn, showing high-quality results and comparisons\nagainst relevant baselines. We also release all models and code for future\ncomparisons.",
          "link": "http://arxiv.org/abs/2103.16525",
          "publishedOn": "2021-07-06T01:58:11.896Z",
          "wordCount": 600,
          "title": "Endo-Depth-and-Motion: Reconstruction and Tracking in Endoscopic Videos using Depth Networks and Photometric Constraints. (arXiv:2103.16525v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baby_D/0/1/0/all/0/1\">Dheeraj Baby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>",
          "description": "We consider the problem of the Zinkevich (2003)-style dynamic regret\nminimization in online learning with exp-concave losses. We show that whenever\nimproper learning is allowed, a Strongly Adaptive online learner achieves the\ndynamic regret of $\\tilde O^*(n^{1/3}C_n^{2/3} \\vee 1)$ where $C_n$ is the\ntotal variation (a.k.a. path length) of the an arbitrary sequence of\ncomparators that may not be known to the learner ahead of time. Achieving this\nrate was highly nontrivial even for squared losses in 1D where the best known\nupper bound was $O(\\sqrt{nC_n} \\vee \\log n)$ (Yuan and Lamperski, 2019). Our\nnew proof techniques make elegant use of the intricate structures of the primal\nand dual variables imposed by the KKT conditions and could be of independent\ninterest. Finally, we apply our results to the classical statistical problem of\nlocally adaptive non-parametric regression (Mammen, 1991; Donoho and Johnstone,\n1998) and obtain a stronger and more flexible algorithm that do not require any\nstatistical assumptions or any hyperparameter tuning.",
          "link": "http://arxiv.org/abs/2104.11824",
          "publishedOn": "2021-07-06T01:58:11.888Z",
          "wordCount": 634,
          "title": "Optimal Dynamic Regret in Exp-Concave Online Learning. (arXiv:2104.11824v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_R/0/1/0/all/0/1\">Robin Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">David Robert Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_S/0/1/0/all/0/1\">Simon Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Kazuya Takeda</a>",
          "description": "Interconnected road lanes are a central concept for navigating urban roads.\nCurrently, most autonomous vehicles rely on preconstructed lane maps as\ndesigning an algorithmic model is difficult. However, the generation and\nmaintenance of such maps is costly and hinders large-scale adoption of\nautonomous vehicle technology. This paper presents the first self-supervised\nlearning method to train a model to infer a spatially grounded lane-level road\nnetwork graph based on a dense segmented representation of the road scene\ngenerated from onboard sensors. A formal road lane network model is presented\nand proves that any structured road scene can be represented by a directed\nacyclic graph of at most depth three while retaining the notion of intersection\nregions, and that this is the most compressed representation. The formal model\nis implemented by a hybrid neural and search-based model, utilizing a novel\nbarrier function loss formulation for robust learning from partial labels.\nExperiments are conducted for all common road intersection layouts. Results\nshow that the model can generalize to new road layouts, unlike previous\napproaches, demonstrating its potential for real-world application as a\npractical learning-based lane-level map generator.",
          "link": "http://arxiv.org/abs/2107.01784",
          "publishedOn": "2021-07-06T01:58:11.869Z",
          "wordCount": 650,
          "title": "Learning a Model for Inferring a Spatial Road Lane Network Graph using Self-Supervision. (arXiv:2107.01784v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Little_M/0/1/0/all/0/1\">Max A. Little</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kayas_U/0/1/0/all/0/1\">Ugur Kayas</a>",
          "description": "Dynamic programming (DP) is a broadly applicable algorithmic design paradigm\nfor the efficient, exact solution of otherwise intractable, combinatorial\nproblems. However, the design of such algorithms is often presented informally\nin an ad-hoc manner, and as a result is often difficult to apply correctly. In\nthis paper, we present a rigorous algebraic formalism for systematically\nderiving novel DP algorithms, either from existing DP algorithms or from simple\nfunctional recurrences. These derivations lead to algorithms which are provably\ncorrect and polymorphic over any semiring, which means that they can be applied\nto the full scope of combinatorial problems expressible in terms of semirings.\nThis includes, for example: optimization, optimal probability and Viterbi\ndecoding, probabilistic marginalization, logical inference, fuzzy sets,\ndifferentiable softmax, and relational and provenance queries. The approach,\nbuilding on many ideas from the existing literature on constructive\nalgorithmics, exploits generic properties of (semiring) polymorphic functions,\ntupling and formal sums (lifting), and algebraic simplifications arising from\nconstraint algebras. We demonstrate the effectiveness of this formalism for\nsome example applications arising in signal processing, bioinformatics and\nreliability engineering.",
          "link": "http://arxiv.org/abs/2107.01752",
          "publishedOn": "2021-07-06T01:58:11.863Z",
          "wordCount": 613,
          "title": "Polymorphic dynamic programming by algebraic shortcut fusion. (arXiv:2107.01752v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosenfeld_J/0/1/0/all/0/1\">Jonathan S. Rosenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1\">Jonathan Frankle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbin_M/0/1/0/all/0/1\">Michael Carbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavit_N/0/1/0/all/0/1\">Nir Shavit</a>",
          "description": "We show that the error of iteratively magnitude-pruned networks empirically\nfollows a scaling law with interpretable coefficients that depend on the\narchitecture and task. We functionally approximate the error of the pruned\nnetworks, showing it is predictable in terms of an invariant tying width,\ndepth, and pruning level, such that networks of vastly different pruned\ndensities are interchangeable. We demonstrate the accuracy of this\napproximation over orders of magnitude in depth, width, dataset size, and\ndensity. We show that the functional form holds (generalizes) for large scale\ndata (e.g., ImageNet) and architectures (e.g., ResNets). As neural networks\nbecome ever larger and costlier to train, our findings suggest a framework for\nreasoning conceptually and analytically about a standard method for\nunstructured pruning.",
          "link": "http://arxiv.org/abs/2006.10621",
          "publishedOn": "2021-07-06T01:58:11.855Z",
          "wordCount": 599,
          "title": "On the Predictability of Pruning Across Scales. (arXiv:2006.10621v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Samarth Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1\">Ajay Mandlekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>",
          "description": "Offline reinforcement learning proposes to learn policies from large\ncollected datasets without interacting with the physical environment. These\nalgorithms have made it possible to learn useful skills from data that can then\nbe deployed in the environment in real-world settings where interactions may be\ncostly or dangerous, such as autonomous driving or factories. However, current\nalgorithms overfit to the dataset they are trained on and exhibit poor\nout-of-distribution generalization to the environment when deployed. In this\npaper, we study the effectiveness of performing data augmentations on the state\nspace, and study 7 different augmentation schemes and how they behave with\nexisting offline RL algorithms. We then combine the best data performing\naugmentation scheme with a state-of-the-art Q-learning technique, and improve\nthe function approximation of the Q-networks by smoothening out the learned\nstate-action space. We experimentally show that using this Surprisingly Simple\nSelf-Supervision technique in RL (S4RL), we significantly improve over the\ncurrent state-of-the-art algorithms on offline robot learning environments such\nas MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4].",
          "link": "http://arxiv.org/abs/2103.06326",
          "publishedOn": "2021-07-06T01:58:11.848Z",
          "wordCount": 628,
          "title": "S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning. (arXiv:2103.06326v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rohin Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wild_C/0/1/0/all/0/1\">Cody Wild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Steven H. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alex_N/0/1/0/all/0/1\">Neel Alex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houghton_B/0/1/0/all/0/1\">Brandon Houghton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guss_W/0/1/0/all/0/1\">William Guss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1\">Sharada Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanervisto_A/0/1/0/all/0/1\">Anssi Kanervisto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1\">Stephanie Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topin_N/0/1/0/all/0/1\">Nicholay Topin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1\">Stuart Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1\">Anca Dragan</a>",
          "description": "The last decade has seen a significant increase of interest in deep learning\nresearch, with many public successes that have demonstrated its potential. As\nsuch, these systems are now being incorporated into commercial products. With\nthis comes an additional challenge: how can we build AI systems that solve\ntasks where there is not a crisp, well-defined specification? While multiple\nsolutions have been proposed, in this competition we focus on one in\nparticular: learning from human feedback. Rather than training AI systems using\na predefined reward function or using a labeled dataset with a predefined set\nof categories, we instead train the AI system using a learning signal derived\nfrom some form of human feedback, which can evolve over time as the\nunderstanding of the task changes, or as the capabilities of the AI system\nimprove.\n\nThe MineRL BASALT competition aims to spur forward research on this important\nclass of techniques. We design a suite of four tasks in Minecraft for which we\nexpect it will be hard to write down hardcoded reward functions. These tasks\nare defined by a paragraph of natural language: for example, \"create a\nwaterfall and take a scenic picture of it\", with additional clarifying details.\nParticipants must train a separate agent for each task, using any method they\nwant. Agents are then evaluated by humans who have read the task description.\nTo help participants get started, we provide a dataset of human demonstrations\non each of the four tasks, as well as an imitation learning baseline that\nleverages these demonstrations.\n\nOur hope is that this competition will improve our ability to build AI\nsystems that do what their designers intend them to do, even when the intent\ncannot be easily formalized. Besides allowing AI to solve more tasks, this can\nalso enable more effective regulation of AI systems, as well as making progress\non the value alignment problem.",
          "link": "http://arxiv.org/abs/2107.01969",
          "publishedOn": "2021-07-06T01:58:11.842Z",
          "wordCount": 772,
          "title": "The MineRL BASALT Competition on Learning from Human Feedback. (arXiv:2107.01969v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01285",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hillman_J/0/1/0/all/0/1\">Jonathan Hillman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hocking_T/0/1/0/all/0/1\">Toby Dylan Hocking</a>",
          "description": "Receiver Operating Characteristic (ROC) curves are plots of true positive\nrate versus false positive rate which are useful for evaluating binary\nclassification models, but difficult to use for learning since the Area Under\nthe Curve (AUC) is non-convex. ROC curves can also be used in other problems\nthat have false positive and true positive rates such as changepoint detection.\nWe show that in this more general context, the ROC curve can have loops, points\nwith highly sub-optimal error rates, and AUC greater than one. This observation\nmotivates a new optimization objective: rather than maximizing the AUC, we\nwould like a monotonic ROC curve with AUC=1 that avoids points with large\nvalues for Min(FP,FN). We propose a convex relaxation of this objective that\nresults in a new surrogate loss function called the AUM, short for Area Under\nMin(FP, FN). Whereas previous loss functions are based on summing over all\nlabeled examples or pairs, the AUM requires a sort and a sum over the sequence\nof points on the ROC curve. We show that AUM directional derivatives can be\nefficiently computed and used in a gradient descent learning algorithm. In our\nempirical study of supervised binary classification and changepoint detection\nproblems, we show that our new AUM minimization learning algorithm results in\nimproved AUC and comparable speed relative to previous baselines.",
          "link": "http://arxiv.org/abs/2107.01285",
          "publishedOn": "2021-07-06T01:58:11.826Z",
          "wordCount": 663,
          "title": "Optimizing ROC Curves with a Sort-Based Surrogate Loss Function for Binary Classification and Changepoint Detection. (arXiv:2107.01285v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berns_S/0/1/0/all/0/1\">Sebastian Berns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broad_T/0/1/0/all/0/1\">Terence Broad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guckelsberger_C/0/1/0/all/0/1\">Christian Guckelsberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colton_S/0/1/0/all/0/1\">Simon Colton</a>",
          "description": "We present a framework for automating generative deep learning with a\nspecific focus on artistic applications. The framework provides opportunities\nto hand over creative responsibilities to a generative system as targets for\nautomation. For the definition of targets, we adopt core concepts from\nautomated machine learning and an analysis of generative deep learning\npipelines, both in standard and artistic settings. To motivate the framework,\nwe argue that automation aligns well with the goal of increasing the creative\nresponsibility of a generative system, a central theme in computational\ncreativity research. We understand automation as the challenge of granting a\ngenerative system more creative autonomy, by framing the interaction between\nthe user and the system as a co-creative process. The development of the\nframework is informed by our analysis of the relationship between automation\nand creative autonomy. An illustrative example shows how the framework can give\ninspiration and guidance in the process of handing over creative\nresponsibility.",
          "link": "http://arxiv.org/abs/2107.01858",
          "publishedOn": "2021-07-06T01:58:11.819Z",
          "wordCount": 588,
          "title": "Automating Generative Deep Learning for Artistic Purposes: Challenges and Opportunities. (arXiv:2107.01858v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yulin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuni Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaifa Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiapu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingquan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kai Zhou</a>",
          "description": "Graph-based Anomaly Detection (GAD) is becoming prevalent due to the powerful\nrepresentation abilities of graphs as well as recent advances in graph mining\ntechniques. These GAD tools, however, expose a new attacking surface,\nironically due to their unique advantage of being able to exploit the relations\namong data. That is, attackers now can manipulate those relations (i.e., the\nstructure of the graph) to allow some target nodes to evade detection. In this\npaper, we exploit this vulnerability by designing a new type of targeted\nstructural poisoning attacks to a representative regression-based GAD system\ntermed OddBall. Specially, we formulate the attack against OddBall as a\nbi-level optimization problem, where the key technical challenge is to\nefficiently solve the problem in a discrete domain. We propose a novel attack\nmethod termed BinarizedAttack based on gradient descent. Comparing to prior\narts, BinarizedAttack can better use the gradient information, making it\nparticularly suitable for solving combinatorial optimization problems.\nFurthermore, we investigate the attack transferability of BinarizedAttack by\nemploying it to attack other representation-learning-based GAD systems. Our\ncomprehensive experiments demonstrate that BinarizedAttack is very effective in\nenabling target nodes to evade graph-based anomaly detection tools with limited\nattackers' budget, and in the black-box transfer attack setting,\nBinarizedAttack is also tested effective and in particular, can significantly\nchange the node embeddings learned by the GAD systems. Our research thus opens\nthe door to studying a new type of attack against security analytic tools that\nrely on graph data.",
          "link": "http://arxiv.org/abs/2106.09989",
          "publishedOn": "2021-07-06T01:58:11.811Z",
          "wordCount": 728,
          "title": "BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection. (arXiv:2106.09989v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziping Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuhang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jiechao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>",
          "description": "Policy gradient (PG) algorithms have been widely used in reinforcement\nlearning (RL). However, PG algorithms rely on exploiting the value function\nbeing learned with the first-order update locally, which results in limited\nsample efficiency. In this work, we propose an alternative method called\nZeroth-Order Supervised Policy Improvement (ZOSPI). ZOSPI exploits the\nestimated value function $Q$ globally while preserving the local exploitation\nof the PG methods based on zeroth-order policy optimization. This learning\nparadigm follows Q-learning but overcomes the difficulty of efficiently\noperating argmax in continuous action space. It finds max-valued action within\na small number of samples. The policy learning of ZOSPI has two steps: First,\nit samples actions and evaluates those actions with a learned value estimator,\nand then it learns to perform the action with the highest value through\nsupervised learning. We further demonstrate such a supervised learning\nframework can learn multi-modal policies. Experiments show that ZOSPI achieves\ncompetitive results on the continuous control benchmarks with a remarkable\nsample efficiency.",
          "link": "http://arxiv.org/abs/2006.06600",
          "publishedOn": "2021-07-06T01:58:11.804Z",
          "wordCount": 630,
          "title": "Zeroth-Order Supervised Policy Improvement. (arXiv:2006.06600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laidlaw_C/0/1/0/all/0/1\">Cassidy Laidlaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sahil Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>",
          "description": "A key challenge in adversarial robustness is the lack of a precise\nmathematical characterization of human perception, used in the very definition\nof adversarial attacks that are imperceptible to human eyes. Most current\nattacks and defenses try to avoid this issue by considering restrictive\nadversarial threat models such as those bounded by $L_2$ or $L_\\infty$\ndistance, spatial perturbations, etc. However, models that are robust against\nany of these restrictive threat models are still fragile against other threat\nmodels. To resolve this issue, we propose adversarial training against the set\nof all imperceptible adversarial examples, approximated using deep neural\nnetworks. We call this threat model the neural perceptual threat model (NPTM);\nit includes adversarial examples with a bounded neural perceptual distance (a\nneural network-based approximation of the true perceptual distance) to natural\nimages. Through an extensive perceptual study, we show that the neural\nperceptual distance correlates well with human judgements of perceptibility of\nadversarial examples, validating our threat model.\n\nUnder the NPTM, we develop novel perceptual adversarial attacks and defenses.\nBecause the NPTM is very broad, we find that Perceptual Adversarial Training\n(PAT) against a perceptual attack gives robustness against many other types of\nadversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five\ndiverse adversarial attacks. We find that PAT achieves state-of-the-art\nrobustness against the union of these five attacks, more than doubling the\naccuracy over the next best model, without training against any of them. That\nis, PAT generalizes well to unforeseen perturbation types. This is vital in\nsensitive applications where a particular threat model cannot be assumed, and\nto the best of our knowledge, PAT is the first adversarial training defense\nwith this property.",
          "link": "http://arxiv.org/abs/2006.12655",
          "publishedOn": "2021-07-06T01:58:11.797Z",
          "wordCount": 777,
          "title": "Perceptual Adversarial Robustness: Defense Against Unseen Threat Models. (arXiv:2006.12655v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1\">Felix Leeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanzillotta_G/0/1/0/all/0/1\">Guilia Lanzillotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annadani_Y/0/1/0/all/0/1\">Yashas Annadani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "We study the problem of self-supervised structured representation learning\nusing autoencoders for generative modeling. Unlike most methods which rely on\nmatching an arbitrary, relatively unstructured, prior distribution for\nsampling, we propose a sampling technique that relies solely on the\nindependence of latent variables, thereby avoiding the trade-off between\nreconstruction quality and generative performance inherent to VAEs. We design a\nnovel autoencoder architecture capable of learning a structured representation\nwithout the need for aggressive regularization. Our structural decoders learn a\nhierarchy of latent variables, akin to structural causal models, thereby\nordering the information without any additional regularization. We demonstrate\nhow these models learn a representation that improves results in a variety of\ndownstream tasks including generation, disentanglement, and extrapolation using\nseveral challenging and natural image datasets.",
          "link": "http://arxiv.org/abs/2006.07796",
          "publishedOn": "2021-07-06T01:58:11.519Z",
          "wordCount": 613,
          "title": "Structure by Architecture: Disentangled Representations without Regularization. (arXiv:2006.07796v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bouritsas_G/0/1/0/all/0/1\">Giorgos Bouritsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frasca_F/0/1/0/all/0/1\">Fabrizio Frasca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1\">Michael M. Bronstein</a>",
          "description": "While Graph Neural Networks (GNNs) have achieved remarkable results in a\nvariety of applications, recent studies exposed important shortcomings in their\nability to capture the structure of the underlying graph. It has been shown\nthat the expressive power of standard GNNs is bounded by the Weisfeiler-Leman\n(WL) graph isomorphism test, from which they inherit proven limitations such as\nthe inability to detect and count graph substructures. On the other hand, there\nis significant empirical evidence, e.g. in network science and bioinformatics,\nthat substructures are often intimately related to downstream tasks. To this\nend, we propose \"Graph Substructure Networks\" (GSN), a topologically-aware\nmessage passing scheme based on substructure encoding. We theoretically analyse\nthe expressive power of our architecture, showing that it is strictly more\nexpressive than the WL test, and provide sufficient conditions for\nuniversality. Importantly, we do not attempt to adhere to the WL hierarchy;\nthis allows us to retain multiple attractive properties of standard GNNs such\nas locality and linear network complexity, while being able to disambiguate\neven hard instances of graph isomorphism. We perform an extensive experimental\nevaluation on graph classification and regression tasks and obtain\nstate-of-the-art results in diverse real-world settings including molecular\ngraphs and social networks. The code is publicly available at\nhttps://github.com/gbouritsas/graph-substructure-networks.",
          "link": "http://arxiv.org/abs/2006.09252",
          "publishedOn": "2021-07-06T01:58:11.509Z",
          "wordCount": 692,
          "title": "Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting. (arXiv:2006.09252v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pinyan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chao Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaojin Zhang</a>",
          "description": "We study the problem of identifying the best arm in a stochastic multi-armed\nbandit game. Given a set of $n$ arms indexed from $1$ to $n$, each arm $i$ is\nassociated with an unknown reward distribution supported on $[0,1]$ with mean\n$\\theta_i$ and variance $\\sigma_i^2$. Assume $\\theta_1 > \\theta_2 \\geq \\cdots\n\\geq\\theta_n$. We propose an adaptive algorithm which explores the gaps and\nvariances of the rewards of the arms and makes future decisions based on the\ngathered information using a novel approach called \\textit{grouped median\nelimination}. The proposed algorithm guarantees to output the best arm with\nprobability $(1-\\delta)$ and uses at most $O \\left(\\sum_{i = 1}^n\n\\left(\\frac{\\sigma_i^2}{\\Delta_i^2} + \\frac{1}{\\Delta_i}\\right)(\\ln \\delta^{-1}\n+ \\ln \\ln \\Delta_i^{-1})\\right)$ samples, where $\\Delta_i$ ($i \\geq 2$) denotes\nthe reward gap between arm $i$ and the best arm and we define $\\Delta_1 =\n\\Delta_2$. This achieves a significant advantage over the variance-independent\nalgorithms in some favorable scenarios and is the first result that removes the\nextra $\\ln n$ factor on the best arm compared with the state-of-the-art. We\nfurther show that $\\Omega \\left( \\sum_{i = 1}^n \\left(\n\\frac{\\sigma_i^2}{\\Delta_i^2} + \\frac{1}{\\Delta_i} \\right) \\ln \\delta^{-1}\n\\right)$ samples are necessary for an algorithm to achieve the same goal,\nthereby illustrating that our algorithm is optimal up to doubly logarithmic\nterms.",
          "link": "http://arxiv.org/abs/2106.10417",
          "publishedOn": "2021-07-06T01:58:11.503Z",
          "wordCount": 662,
          "title": "Variance-Dependent Best Arm Identification. (arXiv:2106.10417v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a novel high-fidelity and low-latency universal neural\nvocoder framework based on multiband WaveRNN with data-driven linear prediction\nfor discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN\narchitecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit\nwith a relatively large size of hidden units is utilized, while the multiband\nmodeling is deployed to achieve real-time low-latency usage. A novel technique\nfor data-driven linear prediction (LP) with discrete waveform modeling is\nproposed, where the LP coefficients are estimated in a data-driven manner.\nMoreover, a novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed. The\nexperimental results demonstrate that the proposed MWDLP framework generates\nhigh-fidelity synthetic speech for seen and unseen speakers and/or language on\n300 speakers training data including clean and noisy/reverberant conditions,\nwhere the number of training utterances is limited to 60 per speaker, while\nallowing for real-time low-latency processing using a single core of $\\sim\\!$\n2.1--2.7 GHz CPU with $\\sim\\!$ 0.57--0.64 real-time factor including\ninput/output and feature extraction.",
          "link": "http://arxiv.org/abs/2105.09856",
          "publishedOn": "2021-07-06T01:58:11.496Z",
          "wordCount": 670,
          "title": "High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01994",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Riva_M/0/1/0/all/0/1\">Mateus Riva</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yger_F/0/1/0/all/0/1\">Florian Yger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cesar_R/0/1/0/all/0/1\">Roberto M. Cesar Jr.</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bloch_I/0/1/0/all/0/1\">Isabelle Bloch</a>",
          "description": "We propose a novel graph clustering method guided by additional information\non the underlying structure of the clusters (or communities). The problem is\nformulated as the matching of a graph to a template with smaller dimension,\nhence matching $n$ vertices of the observed graph (to be clustered) to the $k$\nvertices of a template graph, using its edges as support information, and\nrelaxed on the set of orthonormal matrices in order to find a $k$ dimensional\nembedding. With relevant priors that encode the density of the clusters and\ntheir relationships, our method outperforms classical methods, especially for\nchallenging cases.",
          "link": "http://arxiv.org/abs/2107.01994",
          "publishedOn": "2021-07-06T01:58:11.480Z",
          "wordCount": 555,
          "title": "Template-Based Graph Clustering. (arXiv:2107.01994v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Killian_J/0/1/0/all/0/1\">Jackson A. Killian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lily Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1\">Arpita Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1\">Milind Tambe</a>",
          "description": "We introduce Robust Restless Bandits, a challenging generalization of\nrestless multi-arm bandits (RMAB). RMABs have been widely studied for\nintervention planning with limited resources. However, most works make the\nunrealistic assumption that the transition dynamics are known perfectly,\nrestricting the applicability of existing methods to real-world scenarios. To\nmake RMABs more useful in settings with uncertain dynamics: (i) We introduce\nthe Robust RMAB problem and develop solutions for a minimax regret objective\nwhen transitions are given by interval uncertainties; (ii) We develop a double\noracle algorithm for solving Robust RMABs and demonstrate its effectiveness on\nthree experimental domains; (iii) To enable our double oracle approach, we\nintroduce RMABPPO, a novel deep reinforcement learning algorithm for solving\nRMABs. RMABPPO hinges on learning an auxiliary \"$\\lambda$-network\" that allows\neach arm's learning to decouple, greatly reducing sample complexity required\nfor training; (iv) Under minimax regret, the adversary in the double oracle\napproach is notoriously difficult to implement due to non-stationarity. To\naddress this, we formulate the adversary oracle as a multi-agent reinforcement\nlearning problem and solve it with a multi-agent extension of RMABPPO, which\nmay be of independent interest as the first known algorithm for this setting.\nCode is available at https://github.com/killian-34/RobustRMAB.",
          "link": "http://arxiv.org/abs/2107.01689",
          "publishedOn": "2021-07-06T01:58:11.474Z",
          "wordCount": 641,
          "title": "Robust Restless Bandits: Tackling Interval Uncertainty with Deep Reinforcement Learning. (arXiv:2107.01689v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jia-Jie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouridi_C/0/1/0/all/0/1\">Christina Kouridi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemmour_Y/0/1/0/all/0/1\">Yassine Nemmour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "We propose the adversarially robust kernel smoothing (ARKS) algorithm,\ncombining kernel smoothing, robust optimization, and adversarial training for\nrobust learning. Our methods are motivated by the convex analysis perspective\nof distributionally robust optimization based on probability metrics, such as\nthe Wasserstein distance and the maximum mean discrepancy. We adapt the\nintegral operator using supremal convolution in convex analysis to form a novel\nfunction majorant used for enforcing robustness. Our method is simple in form\nand applies to general loss functions and machine learning models. Furthermore,\nwe report experiments with general machine learning models, such as deep neural\nnetworks, to demonstrate that ARKS performs competitively with the\nstate-of-the-art methods based on the Wasserstein distance.",
          "link": "http://arxiv.org/abs/2102.08474",
          "publishedOn": "2021-07-06T01:58:11.467Z",
          "wordCount": 582,
          "title": "Adversarially Robust Kernel Smoothing. (arXiv:2102.08474v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tritrong_N/0/1/0/all/0/1\">Nontawat Tritrong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rewatbowornwong_P/0/1/0/all/0/1\">Pitchaporn Rewatbowornwong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suwajanakorn_S/0/1/0/all/0/1\">Supasorn Suwajanakorn</a>",
          "description": "While GANs have shown success in realistic image generation, the idea of\nusing GANs for other tasks unrelated to synthesis is underexplored. Do GANs\nlearn meaningful structural parts of objects during their attempt to reproduce\nthose objects? In this work, we test this hypothesis and propose a simple and\neffective approach based on GANs for semantic part segmentation that requires\nas few as one label example along with an unlabeled dataset. Our key idea is to\nleverage a trained GAN to extract pixel-wise representation from the input\nimage and use it as feature vectors for a segmentation network. Our experiments\ndemonstrate that GANs representation is \"readily discriminative\" and produces\nsurprisingly good results that are comparable to those from supervised\nbaselines trained with significantly more labels. We believe this novel\nrepurposing of GANs underlies a new class of unsupervised representation\nlearning that is applicable to many other tasks. More results are available at\nhttps://repurposegans.github.io/.",
          "link": "http://arxiv.org/abs/2103.04379",
          "publishedOn": "2021-07-06T01:58:11.457Z",
          "wordCount": 648,
          "title": "Repurposing GANs for One-shot Semantic Part Segmentation. (arXiv:2103.04379v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>",
          "description": "Sentiment analysis is a costly yet necessary task for enterprises to study\nthe opinions of their customers to improve their products and to determine\noptimal marketing strategies. Due to the existence of a wide range of domains\nacross different products and services, cross-domain sentiment analysis methods\nhave received significant attention. These methods mitigate the domain gap\nbetween different applications by training cross-domain generalizable\nclassifiers which help to relax the need for data annotation for each domain.\nMost existing methods focus on learning domain-agnostic representations that\nare invariant with respect to both the source and the target domains. As a\nresult, a classifier that is trained using the source domain annotated data\nwould generalize well in a related target domain. We introduce a new domain\nadaptation method which induces large margins between different classes in an\nembedding space. This embedding space is trained to be domain-agnostic by\nmatching the data distributions across the domains. Large intraclass margins in\nthe source domain help to reduce the effect of \"domain shift\" on the classifier\nperformance in the target domain. Theoretical and empirical analysis are\nprovided to demonstrate that the proposed method is effective.",
          "link": "http://arxiv.org/abs/2107.01598",
          "publishedOn": "2021-07-06T01:58:11.448Z",
          "wordCount": 624,
          "title": "Domain Adaptation for Sentiment Analysis Using Increased Intraclass Separation. (arXiv:2107.01598v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1\">Mana Ihori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriya_T/0/1/0/all/0/1\">Takafumi Moriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashihara_T/0/1/0/all/0/1\">Takanori Ashihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1\">Shota Orihashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>",
          "description": "We propose a cross-modal transformer-based neural correction models that\nrefines the output of an automatic speech recognition (ASR) system so as to\nexclude ASR errors. Generally, neural correction models are composed of\nencoder-decoder networks, which can directly model sequence-to-sequence mapping\nproblems. The most successful method is to use both input speech and its ASR\noutput text as the input contexts for the encoder-decoder networks. However,\nthe conventional method cannot take into account the relationships between\nthese two different modal inputs because the input contexts are separately\nencoded for each modal. To effectively leverage the correlated information\nbetween the two different modal inputs, our proposed models encode two\ndifferent contexts jointly on the basis of cross-modal self-attention using a\ntransformer. We expect that cross-modal self-attention can effectively capture\nthe relationships between two different modals for refining ASR hypotheses. We\nalso introduce a shallow fusion technique to efficiently integrate the\nfirst-pass ASR model and our proposed neural correction model. Experiments on\nJapanese natural language ASR tasks demonstrated that our proposed models\nachieve better ASR performance than conventional neural correction models.",
          "link": "http://arxiv.org/abs/2107.01569",
          "publishedOn": "2021-07-06T01:58:11.443Z",
          "wordCount": 629,
          "title": "Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition. (arXiv:2107.01569v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achddou_J/0/1/0/all/0/1\">Juliette Achddou</a> (PSL, DI-ENS, VALDA ), <a href=\"http://arxiv.org/find/cs/1/au:+Cappe_O/0/1/0/all/0/1\">Olivier Capp&#xe9;</a> (LTCI, VALDA ), <a href=\"http://arxiv.org/find/cs/1/au:+Garivier_A/0/1/0/all/0/1\">Aur&#xe9;lien Garivier</a> (UMPA-ENSL)",
          "description": "First-price auctions have largely replaced traditional bidding approaches\nbased on Vickrey auctions in programmatic advertising. As far as learning is\nconcerned, first-price auctions are more challenging because the optimal\nbidding strategy does not only depend on the value of the item but also\nrequires some knowledge of the other bids. They have already given rise to\nseveral works in sequential learning, many of which consider models for which\nthe value of the buyer or the opponents' maximal bid is chosen in an\nadversarial manner. Even in the simplest settings, this gives rise to\nalgorithms whose regret grows as $\\sqrt{T}$ with respect to the time horizon\n$T$. Focusing on the case where the buyer plays against a stationary stochastic\nenvironment, we show how to achieve significantly lower regret: when the\nopponents' maximal bid distribution is known we provide an algorithm whose\nregret can be as low as $\\log^2(T)$; in the case where the distribution must be\nlearnt sequentially, a generalization of this algorithm can achieve $T^{1/3+\n\\epsilon}$ regret, for any $\\epsilon>0$. To obtain these results, we introduce\ntwo novel ideas that can be of interest in their own right. First, by\ntransposing results obtained in the posted price setting, we provide conditions\nunder which the first-price biding utility is locally quadratic around its\noptimum. Second, we leverage the observation that, on small sub-intervals, the\nconcentration of the variations of the empirical distribution function may be\ncontrolled more accurately than by using the classical\nDvoretzky-Kiefer-Wolfowitz inequality. Numerical simulations confirm that our\nalgorithms converge much faster than alternatives proposed in the literature\nfor various bid distributions, including for bids collected on an actual\nprogrammatic advertising platform.",
          "link": "http://arxiv.org/abs/2107.01835",
          "publishedOn": "2021-07-06T01:58:11.426Z",
          "wordCount": 717,
          "title": "Fast Rate Learning in Stochastic First Price Bidding. (arXiv:2107.01835v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jerry Zikun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoran Wang</a>",
          "description": "Query reformulation aims to alter noisy or ambiguous text sequences into\ncoherent ones closer to natural language questions. This is to prevent errors\nfrom propagating in a client-facing pipeline and promote better communication\nwith users. Besides, it is crucial to maintain performance in downstream\nenvironments like question answering when rephrased queries are given as input.\nWe show that under the previous framework (AQA), attempts to alter RL\nalgorithms do not bring significant benefits to either reward acquisition or\nsequence fluency. Instead, we leverage a query-reformulating text-to-text\ntransformer (QRT5) and apply policy-based RL algorithms to further nudge this\nreformulator and obtain better answers downstream by generating\nreward-acquiring query trajectories. QRT5 shows better sample efficiency in RL\nto achieve the same level of QA performance as the previous approach. It can\ngenerate reformulations with more readability based on query well-formedness\nevaluations and can generalize to out-of-sample data. Our framework is\ndemonstrated to be flexible, allowing reward signals to be sourced from\ndifferent downstream environments such as intent classification.",
          "link": "http://arxiv.org/abs/2012.10033",
          "publishedOn": "2021-07-06T01:58:11.419Z",
          "wordCount": 649,
          "title": "Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning. (arXiv:2012.10033v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruofan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kai Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaofu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Leilei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_T/0/1/0/all/0/1\">Tao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuan Qi</a>",
          "description": "Deep learning provides a promising way to extract effective representations\nfrom raw data in an end-to-end fashion and has proven its effectiveness in\nvarious domains such as computer vision, natural language processing, etc.\nHowever, in domains such as content/product recommendation and risk management,\nwhere sequence of event data is the most used raw data form and experts derived\nfeatures are more commonly used, deep learning models struggle to dominate the\ngame. In this paper, we propose a symbolic testing framework that helps to\nanswer the question of what kinds of expert-derived features could be learned\nby a neural network. Inspired by this testing framework, we introduce an\nefficient architecture named SHORING, which contains two components:\n\\textit{event network} and \\textit{sequence network}. The \\textit{event}\nnetwork learns arbitrarily yet efficiently high-order \\textit{event-level}\nembeddings via a provable reparameterization trick, the \\textit{sequence}\nnetwork aggregates from sequence of \\textit{event-level} embeddings. We argue\nthat SHORING is capable of learning certain standard symbolic expressions which\nthe standard multi-head self-attention network fails to learn, and conduct\ncomprehensive experiments and ablation studies on four synthetic datasets and\nthree real-world datasets. The results show that SHORING empirically\noutperforms the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.01326",
          "publishedOn": "2021-07-06T01:58:11.409Z",
          "wordCount": 647,
          "title": "SHORING: Design Provable Conditional High-Order Interaction Network via Symbolic Testing. (arXiv:2107.01326v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1\">Max Simchowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tosh_C/0/1/0/all/0/1\">Christopher Tosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Akshay Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">Daniel Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1\">Thodoris Lykouris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1\">Miroslav Dud&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schapire_R/0/1/0/all/0/1\">Robert E. Schapire</a>",
          "description": "Thompson sampling and other Bayesian sequential decision-making algorithms\nare among the most popular approaches to tackle explore/exploit trade-offs in\n(contextual) bandits. The choice of prior in these algorithms offers\nflexibility to encode domain knowledge but can also lead to poor performance\nwhen misspecified. In this paper, we demonstrate that performance degrades\ngracefully with misspecification. We prove that the expected reward accrued by\nThompson sampling (TS) with a misspecified prior differs by at most\n$\\tilde{\\mathcal{O}}(H^2 \\epsilon)$ from TS with a well specified prior, where\n$\\epsilon$ is the total-variation distance between priors and $H$ is the\nlearning horizon. Our bound does not require the prior to have any parametric\nform. For priors with bounded support, our bound is independent of the\ncardinality or structure of the action space, and we show that it is tight up\nto universal constants in the worst case.\n\nBuilding on our sensitivity analysis, we establish generic PAC guarantees for\nalgorithms in the recently studied Bayesian meta-learning setting and derive\ncorollaries for various families of priors. Our results generalize along two\naxes: (1) they apply to a broader family of Bayesian decision-making\nalgorithms, including a Monte-Carlo implementation of the knowledge gradient\nalgorithm (KG), and (2) they apply to Bayesian POMDPs, the most general\nBayesian decision-making setting, encompassing contextual bandits as a special\ncase. Through numerical simulations, we illustrate how prior misspecification\nand the deployment of one-step look-ahead (as in KG) can impact the convergence\nof meta-learning in multi-armed and contextual bandits with structured and\ncorrelated priors.",
          "link": "http://arxiv.org/abs/2107.01509",
          "publishedOn": "2021-07-06T01:58:11.403Z",
          "wordCount": 698,
          "title": "Bayesian decision-making under misspecified priors with applications to meta-learning. (arXiv:2107.01509v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01333",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Shuyan Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spirtes_P/0/1/0/all/0/1\">Peter Spirtes</a>",
          "description": "Kalisch and B\\\"{u}hlmann (2007) showed that for linear Gaussian models, under\nthe Causal Markov Assumption, the Strong Causal Faithfulness Assumption, and\nthe assumption of causal sufficiency, the PC algorithm is a uniformly\nconsistent estimator of the Markov Equivalence Class of the true causal DAG for\nlinear Gaussian models; it follows from this that for the identifiable causal\neffects in the Markov Equivalence Class, there are uniformly consistent\nestimators of causal effects as well. The $k$-Triangle-Faithfulness Assumption\nis a strictly weaker assumption that avoids some implausible implications of\nthe Strong Causal Faithfulness Assumption and also allows for uniformly\nconsistent estimates of Markov Equivalence Classes (in a weakened sense), and\nof identifiable causal effects. However, both of these assumptions are\nrestricted to linear Gaussian models. We propose the Generalized $k$-Triangle\nFaithfulness, which can be applied to any smooth distribution. In addition,\nunder the Generalized $k$-Triangle Faithfulness Assumption, we describe the\nEdge Estimation Algorithm that provides uniformly consistent estimates of\ncausal effects in some cases (and otherwise outputs \"can't tell\"), and the\n\\textit{Very Conservative }$SGS$ Algorithm that (in a slightly weaker sense) is\na uniformly consistent estimator of the Markov equivalence class of the true\nDAG.",
          "link": "http://arxiv.org/abs/2107.01333",
          "publishedOn": "2021-07-06T01:58:11.386Z",
          "wordCount": 634,
          "title": "A Uniformly Consistent Estimator of non-Gaussian Causal Effects Under the k-Triangle-Faithfulness Assumption. (arXiv:2107.01333v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leporowski_B/0/1/0/all/0/1\">B&#x142;a&#x17c;ej Leporowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tola_D/0/1/0/all/0/1\">Daniella Tola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_C/0/1/0/all/0/1\">Casper Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>",
          "description": "Detecting faults in manufacturing applications can be difficult, especially\nif each fault model is to be engineered by hand. Data-driven approaches, using\nMachine Learning (ML) for detecting faults have recently gained increasing\ninterest, where a ML model can be trained on a set of data from a manufacturing\nprocess. In this paper, we present a use case of using ML models for detecting\nfaults during automated screwdriving operations, and introduce a new dataset\ncontaining fully monitored and registered data from a Universal Robot and\nOnRobot screwdriver during both normal and anomalous operations. We illustrate,\nwith the use of two time-series ML models, how to detect faults in an automated\nscrewdriving application.",
          "link": "http://arxiv.org/abs/2107.01955",
          "publishedOn": "2021-07-06T01:58:11.380Z",
          "wordCount": 561,
          "title": "Detecting Faults during Automatic Screwdriving: A Dataset and Use Case of Anomaly Detection for Automatic Screwdriving. (arXiv:2107.01955v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eungyeup Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Juyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Image classification models tend to make decisions based on peripheral\nattributes of data items that have strong correlation with a target variable\n(i.e., dataset bias). These biased models suffer from the poor generalization\ncapability when evaluated on unbiased datasets. Existing approaches for\ndebiasing often identify and emphasize those samples with no such correlation\n(i.e., bias-conflicting) without defining the bias type in advance. However,\nsuch bias-conflicting samples are significantly scarce in biased datasets,\nlimiting the debiasing capability of these approaches. This paper first\npresents an empirical analysis revealing that training with \"diverse\"\nbias-conflicting samples beyond a given training set is crucial for debiasing\nas well as the generalization capability. Based on this observation, we propose\na novel feature-level data augmentation technique in order to synthesize\ndiverse bias-conflicting samples. To this end, our method learns the\ndisentangled representation of (1) the intrinsic attributes (i.e., those\ninherently defining a certain class) and (2) bias attributes (i.e., peripheral\nattributes causing the bias), from a large number of bias-aligned samples, the\nbias attributes of which have strong correlation with the target variable.\nUsing the disentangled representation, we synthesize bias-conflicting samples\nthat contain the diverse intrinsic attributes of bias-aligned samples by\nswapping their latent features. By utilizing these diversified bias-conflicting\nfeatures during the training, our approach achieves superior classification\naccuracy and debiasing results against the existing baselines on both synthetic\nas well as real-world datasets.",
          "link": "http://arxiv.org/abs/2107.01372",
          "publishedOn": "2021-07-06T01:58:11.374Z",
          "wordCount": 658,
          "title": "Learning Debiased Representation via Disentangled Feature Augmentation. (arXiv:2107.01372v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+El_Hamamsy_L/0/1/0/all/0/1\">Laila El-Hamamsy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papaspyros_V/0/1/0/all/0/1\">Vaios Papaspyros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangur_T/0/1/0/all/0/1\">Taavet Kangur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathex_L/0/1/0/all/0/1\">Laura Mathex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giang_C/0/1/0/all/0/1\">Christian Giang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skweres_M/0/1/0/all/0/1\">Melissa Skweres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_B/0/1/0/all/0/1\">Barbara Bruno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondada_F/0/1/0/all/0/1\">Francesco Mondada</a>",
          "description": "Recently, introducing computer science and educational robots in compulsory\neducation has received increasing attention. However, the use of screens in\nclassrooms is often met with resistance, especially in primary school. To\naddress this issue, this study presents the development of a handwriting-based\nprogramming language for educational robots. Aiming to align better with\nexisting classroom practices, it allows students to program a robot by drawing\nsymbols with ordinary pens and paper. Regular smartphones are leveraged to\nprocess the hand-drawn instructions using computer vision and machine learning\nalgorithms, and send the commands to the robot for execution. To align with the\nlocal computer science curriculum, an appropriate playground and scaffolded\nlearning tasks were designed. The system was evaluated in a preliminary test\nwith eight teachers, developers and educational researchers. While the\nparticipants pointed out that some technical aspects could be improved, they\nalso acknowledged the potential of the approach to make computer science\neducation in primary school more accessible.",
          "link": "http://arxiv.org/abs/2105.04963",
          "publishedOn": "2021-07-06T01:58:11.367Z",
          "wordCount": 651,
          "title": "Exploring a Handwriting Programming Language for Educational Robots. (arXiv:2105.04963v2 [cs.PL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Youwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "Graph learning has emerged as a promising technique for multi-view clustering\nwith its ability to learn a unified and robust graph from multiple views.\nHowever, existing graph learning methods mostly focus on the multi-view\nconsistency issue, yet often neglect the inconsistency across multiple views,\nwhich makes them vulnerable to possibly low-quality or noisy datasets. To\novercome this limitation, we propose a new multi-view graph learning framework,\nwhich for the first time simultaneously and explicitly models multi-view\nconsistency and multi-view inconsistency in a unified objective function,\nthrough which the consistent and inconsistent parts of each single-view graph\nas well as the unified graph that fuses the consistent parts can be iteratively\nlearned. Though optimizing the objective function is NP-hard, we design a\nhighly efficient optimization algorithm which is able to obtain an approximate\nsolution with linear time complexity in the number of edges in the unified\ngraph. Furthermore, our multi-view graph learning approach can be applied to\nboth similarity graphs and dissimilarity graphs, which lead to two graph\nfusion-based variants in our framework. Experiments on twelve multi-view\ndatasets have demonstrated the robustness and efficiency of the proposed\napproach.",
          "link": "http://arxiv.org/abs/2008.10208",
          "publishedOn": "2021-07-06T01:58:11.361Z",
          "wordCount": 671,
          "title": "Multi-view Graph Learning by Joint Modeling of Consistency and Inconsistency. (arXiv:2008.10208v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Despite their overwhelming capacity to overfit, deep neural networks trained\nby specific optimization algorithms tend to generalize well to unseen data.\nRecently, researchers explained it by investigating the implicit regularization\neffect of optimization algorithms. A remarkable progress is the work (Lyu&Li,\n2019), which proves gradient descent (GD) maximizes the margin of homogeneous\ndeep neural networks. Except GD, adaptive algorithms such as AdaGrad, RMSProp\nand Adam are popular owing to their rapid training process. However,\ntheoretical guarantee for the generalization of adaptive optimization\nalgorithms is still lacking. In this paper, we study the implicit\nregularization of adaptive optimization algorithms when they are optimizing the\nlogistic loss on homogeneous deep neural networks. We prove that adaptive\nalgorithms that adopt exponential moving average strategy in conditioner (such\nas Adam and RMSProp) can maximize the margin of the neural network, while\nAdaGrad that directly sums historical squared gradients in conditioner can not.\nIt indicates superiority on generalization of exponential moving average\nstrategy in the design of the conditioner. Technically, we provide a unified\nframework to analyze convergent direction of adaptive optimization algorithms\nby constructing novel adaptive gradient flow and surrogate margin. Our\nexperiments can well support the theoretical findings on convergent direction\nof adaptive optimization algorithms.",
          "link": "http://arxiv.org/abs/2012.06244",
          "publishedOn": "2021-07-06T01:58:11.354Z",
          "wordCount": 676,
          "title": "The Implicit Bias for Adaptive Optimization Algorithms on Homogeneous Neural Networks. (arXiv:2012.06244v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1\">Lirong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>",
          "description": "Motivated by the recent discovery that the interpretation maps of CNNs could\neasily be manipulated by adversarial attacks against network interpretability,\nwe study the problem of interpretation robustness from a new perspective of\n\\Renyi differential privacy (RDP). The advantages of our Renyi-Robust-Smooth\n(RDP-based interpretation method) are three-folds. First, it can offer provable\nand certifiable top-$k$ robustness. That is, the top-$k$ important attributions\nof the interpretation map are provably robust under any input perturbation with\nbounded $\\ell_d$-norm (for any $d\\geq 1$, including $d = \\infty$). Second, our\nproposed method offers $\\sim10\\%$ better experimental robustness than existing\napproaches in terms of the top-$k$ attributions. Remarkably, the accuracy of\nRenyi-Robust-Smooth also outperforms existing approaches. Third, our method can\nprovide a smooth tradeoff between robustness and computational efficiency.\nExperimentally, its top-$k$ attributions are {\\em twice} more robust than\nexisting approaches when the computational resources are highly constrained.",
          "link": "http://arxiv.org/abs/2107.01561",
          "publishedOn": "2021-07-06T01:58:11.337Z",
          "wordCount": 585,
          "title": "Certifiably Robust Interpretation via Renyi Differential Privacy. (arXiv:2107.01561v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afchar_D/0/1/0/all/0/1\">Darius Afchar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1\">Romain Hennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guigue_V/0/1/0/all/0/1\">Vincent Guigue</a>",
          "description": "Feature attribution is often loosely presented as the process of selecting a\nsubset of relevant features as a rationale of a prediction. Task-dependent by\nnature, precise definitions of \"relevance\" encountered in the literature are\nhowever not always consistent. This lack of clarity stems from the fact that we\nusually do not have access to any notion of ground-truth attribution and from a\nmore general debate on what good interpretations are. In this paper we propose\nto formalise feature selection/attribution based on the concept of relaxed\nfunctional dependence. In particular, we extend our notions to the\ninstance-wise setting and derive necessary properties for candidate selection\nsolutions, while leaving room for task-dependence. By computing ground-truth\nattributions on synthetic datasets, we evaluate many state-of-the-art\nattribution methods and show that, even when optimised, some fail to verify the\nproposed properties and provide wrong solutions.",
          "link": "http://arxiv.org/abs/2104.12437",
          "publishedOn": "2021-07-06T01:58:11.330Z",
          "wordCount": 609,
          "title": "Towards Rigorous Interpretations: a Formalisation of Feature Attribution. (arXiv:2104.12437v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaitang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>",
          "description": "The goal of text generation is to make machines express in human language. It\nis one of the most important yet challenging tasks in natural language\nprocessing (NLP). Since 2014, various neural encoder-decoder models pioneered\nby Seq2Seq have been proposed to achieve the goal by learning to map input text\nto output text. However, the input text alone often provides limited knowledge\nto generate the desired output, so the performance of text generation is still\nfar from satisfaction in many real-world scenarios. To address this issue,\nresearchers have considered incorporating various forms of knowledge beyond the\ninput text into the generation models. This research direction is known as\nknowledge-enhanced text generation. In this survey, we present a comprehensive\nreview of the research on knowledge enhanced text generation over the past five\nyears. The main content includes two parts: (i) general methods and\narchitectures for integrating knowledge into text generation; (ii) specific\ntechniques and applications according to different forms of knowledge data.\nThis survey can have broad audiences, researchers and practitioners, in\nacademia and industry.",
          "link": "http://arxiv.org/abs/2010.04389",
          "publishedOn": "2021-07-06T01:58:11.323Z",
          "wordCount": 660,
          "title": "A Survey of Knowledge-Enhanced Text Generation. (arXiv:2010.04389v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.01489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_C/0/1/0/all/0/1\">Ciaran Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horgan_J/0/1/0/all/0/1\">Jonathan Horgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varley_P/0/1/0/all/0/1\">Padraig Varley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODea_D/0/1/0/all/0/1\">Derek O&#x27;Dea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uricar_M/0/1/0/all/0/1\">Michal Uricar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milz_S/0/1/0/all/0/1\">Stefan Milz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_M/0/1/0/all/0/1\">Martin Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amende_K/0/1/0/all/0/1\">Karl Amende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1\">Christian Witt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chennupati_S/0/1/0/all/0/1\">Sumanth Chennupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Sanjaya Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_S/0/1/0/all/0/1\">Saquib Mansoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perroton_X/0/1/0/all/0/1\">Xavier Perroton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick Perez</a>",
          "description": "Fisheye cameras are commonly employed for obtaining a large field of view in\nsurveillance, augmented reality and in particular automotive applications. In\nspite of their prevalence, there are few public datasets for detailed\nevaluation of computer vision algorithms on fisheye images. We release the\nfirst extensive fisheye automotive dataset, WoodScape, named after Robert Wood\nwho invented the fisheye camera in 1906. WoodScape comprises of four surround\nview cameras and nine tasks including segmentation, depth estimation, 3D\nbounding box detection and soiling detection. Semantic annotation of 40 classes\nat the instance level is provided for over 10,000 images and annotation for\nother tasks are provided for over 100,000 images. With WoodScape, we would like\nto encourage the community to adapt computer vision models for fisheye camera\ninstead of using naive rectification.",
          "link": "http://arxiv.org/abs/1905.01489",
          "publishedOn": "2021-07-06T01:58:11.313Z",
          "wordCount": 681,
          "title": "WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving. (arXiv:1905.01489v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1\">Mingliang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1\">Zhenhua Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinfu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Daren Yu</a>",
          "description": "Solar energy is a clean and renewable energy. Photovoltaic (PV) power is an\nimportant way to utilize solar energy. Accurate PV power forecast is crucial to\nthe large-scale application of PV power and the stability of electricity grid.\nThis paper proposes a novel method for short-term photovoltaic power forecast\nusing deep convolutional long short-term memory (ConvLSTM) network and kernel\ndensity estimation (KDE). In the proposed method, ConvLSTM is used to forecast\nthe future photovoltaic power and KDE is used for estimating the joint\nprobabilistic density function and giving the probabilistic confidence\ninterval. Experiments in an actual photovoltaic power station verify the\neffectiveness of the proposed method. Comparison experiments with convolutional\nneural network (CNN) and long short-term memory network (LSTM)shows that\nConvLSTM can combine the advantages of both CNN and LSTM and significantly\noutperform CNN and LSTM in terms of forecast accuracy. Through further\ncomparison with other five conventional methods including multilayer perceptron\n(MLP), support vector regression (SVR), extreme learning machine (ELM),\nclassification and regression tree (CART) and gradient boosting decision tree\n(GBDT), ConvLSTM can significantly improve the forecast accuracy by more than\n20% for most of the five methods and the superiorities of ConvLSTM are further\nverified.",
          "link": "http://arxiv.org/abs/2107.01343",
          "publishedOn": "2021-07-06T01:58:11.294Z",
          "wordCount": 651,
          "title": "Short-term probabilistic photovoltaic power forecast based on deep convolutional long short-term memory network and kernel density estimation. (arXiv:2107.01343v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01466",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhenyu Yuan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuxin Jiang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Huang_H/0/1/0/all/0/1\">Handong Huang</a>",
          "description": "Fractures are widely developed in hydrocarbon reservoirs and constitute the\naccumulation spaces and transport channels of oil and gas. Fracture detection\nis a fundamental task for reservoir characterization. From prestack seismic\ngathers, anisotropic analysis and inversion were commonly applied to\ncharacterize the dominant orientations and relative intensities of fractures.\nHowever, the existing methods were mostly based on the vertical aligned facture\nhypothesis, it is impossible for them to recognize fracture dip. Furthermore,\nit is difficult or impractical for existing methods to attain the real fracture\ndensities. Based on data-driven deep learning, this paper designed a\nconvolutional neural network to perform prestack fracture detection.\nCapitalizing on the connections between seismic responses and fracture\nparameters, a suitable azimuth dataset was firstly generated through fracture\neffective medium modeling and anisotropic plane wave analyzing. Then a\nmulti-input and multi-output convolutional neural network was constructed to\nsimultaneously detect fracture density, dip and strike azimuth. The application\non a practical survey validated the effectiveness of the proposed CNN model.",
          "link": "http://arxiv.org/abs/2107.01466",
          "publishedOn": "2021-07-06T01:58:11.288Z",
          "wordCount": 598,
          "title": "A convolutional neural network for prestack fracture detection. (arXiv:2107.01466v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bouritsas_G/0/1/0/all/0/1\">Giorgos Bouritsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1\">Andreas Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karalias_N/0/1/0/all/0/1\">Nikolaos Karalias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1\">Michael M. Bronstein</a>",
          "description": "Can we use machine learning to compress graph data? The absence of ordering\nin graphs poses a significant challenge to conventional compression algorithms,\nlimiting their attainable gains as well as their ability to discover relevant\npatterns. On the other hand, most graph compression approaches rely on\ndomain-dependent handcrafted representations and cannot adapt to different\nunderlying graph distributions. This work aims to establish the necessary\nprinciples a lossless graph compression method should follow to approach the\nentropy storage lower bound. Instead of making rigid assumptions about the\ngraph distribution, we formulate the compressor as a probabilistic model that\ncan be learned from data and generalise to unseen instances. Our \"Partition and\nCode\" framework entails three steps: first, a partitioning algorithm decomposes\nthe graph into elementary structures, then these are mapped to the elements of\na small dictionary on which we learn a probability distribution, and finally,\nan entropy encoder translates the representation into bits. All three steps are\nparametric and can be trained with gradient descent. We theoretically compare\nthe compression quality of several graph encodings and prove, under mild\nconditions, a total ordering of their expected description lengths. Moreover,\nwe show that, under the same conditions, PnC achieves compression gains w.r.t.\nthe baselines that grow either linearly or quadratically with the number of\nvertices. Our algorithms are quantitatively evaluated on diverse real-world\nnetworks obtaining significant performance improvements with respect to\ndifferent families of non-parametric and parametric graph compressors.",
          "link": "http://arxiv.org/abs/2107.01952",
          "publishedOn": "2021-07-06T01:58:11.280Z",
          "wordCount": 686,
          "title": "Partition and Code: learning how to compress graphs. (arXiv:2107.01952v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>",
          "description": "Recent studies assessing the efficacy of pruning neural networks methods\nuncovered a surprising finding: when conducting ablation studies on existing\npruning-at-initialization methods, namely SNIP, GraSP, SynFlow, and magnitude\npruning, performances of these methods remain unchanged and sometimes even\nimprove when randomly shuffling the mask positions within each layer (Layerwise\nShuffling) or sampling new initial weight values (Reinit), while keeping\npruning masks the same. We attempt to understand the reason behind such network\nimmunity towards weight/mask modifications, by studying layer-wise statistics\nbefore and after randomization operations. We found that under each of the\npruning-at-initialization methods, the distribution of unpruned weights changed\nminimally with randomization operations.",
          "link": "http://arxiv.org/abs/2107.01808",
          "publishedOn": "2021-07-06T01:58:11.274Z",
          "wordCount": 542,
          "title": "Why is Pruning at Initialization Immune to Reinitializing and Shuffling?. (arXiv:2107.01808v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qitong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amason_J/0/1/0/all/0/1\">Joshua D. Amason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1\">Ricardo Henao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadziahmetovic_M/0/1/0/all/0/1\">Majda Hadziahmetovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajic_M/0/1/0/all/0/1\">Miroslav Pajic</a>",
          "description": "Although recent works have developed methods that can generate estimations\n(or imputations) of the missing entries in a dataset to facilitate downstream\nanalysis, most depend on assumptions that may not align with real-world\napplications and could suffer from poor performance in subsequent tasks. This\nis particularly true if the data have large missingness rates or a small\npopulation. More importantly, the imputation error could be propagated into the\nprediction step that follows, causing the gradients used to train the\nprediction models to be biased. Consequently, in this work, we introduce the\nimportance guided stochastic gradient descent (IGSGD) method to train\nmultilayer perceptrons (MLPs) and long short-term memories (LSTMs) to directly\nperform inference from inputs containing missing values without imputation.\nSpecifically, we employ reinforcement learning (RL) to adjust the gradients\nused to train the models via back-propagation. This not only reduces bias but\nallows the model to exploit the underlying information behind missingness\npatterns. We test the proposed approach on real-world time-series (i.e.,\nMIMIC-III), tabular data obtained from an eye clinic, and a standard dataset\n(i.e., MNIST), where our imputation-free predictions outperform the traditional\ntwo-step imputation-based predictions using state-of-the-art imputation\nmethods.",
          "link": "http://arxiv.org/abs/2107.01983",
          "publishedOn": "2021-07-06T01:58:11.267Z",
          "wordCount": 628,
          "title": "Imputation-Free Learning from Incomplete Observations. (arXiv:2107.01983v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hadikhanloo_S/0/1/0/all/0/1\">Saeed Hadikhanloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laraki_R/0/1/0/all/0/1\">Rida Laraki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertikopoulos_P/0/1/0/all/0/1\">Panayotis Mertikopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorin_S/0/1/0/all/0/1\">Sylvain Sorin</a>",
          "description": "We examine the long-run behavior of a wide range of dynamics for learning in\nnonatomic games, in both discrete and continuous time. The class of dynamics\nunder consideration includes fictitious play and its regularized variants, the\nbest-reply dynamics (again, possibly regularized), as well as the dynamics of\ndual averaging / \"follow the regularized leader\" (which themselves include as\nspecial cases the replicator dynamics and Friedman's projection dynamics). Our\nanalysis concerns both the actual trajectory of play and its time-average, and\nwe cover potential and monotone games, as well as games with an evolutionarily\nstable state (global or otherwise). We focus exclusively on games with finite\naction spaces; nonatomic games with continuous action spaces are treated in\ndetail in Part II of this paper.",
          "link": "http://arxiv.org/abs/2107.01595",
          "publishedOn": "2021-07-06T01:58:11.260Z",
          "wordCount": 589,
          "title": "Learning in nonatomic games, Part I: Finite action spaces and population games. (arXiv:2107.01595v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Bo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lijffijt_J/0/1/0/all/0/1\">Jefrey Lijffijt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_T/0/1/0/all/0/1\">Tijl De Bie</a>",
          "description": "In today's networked society, many real-world problems can be formalized as\npredicting links in networks, such as Facebook friendship suggestions,\ne-commerce recommendations, and the prediction of scientific collaborations in\ncitation networks. Increasingly often, link prediction problem is tackled by\nmeans of network embedding methods, owing to their state-of-the-art\nperformance. However, these methods lack transparency when compared to simpler\nbaselines, and as a result their robustness against adversarial attacks is a\npossible point of concern: could one or a few small adversarial modifications\nto the network have a large impact on the link prediction performance when\nusing a network embedding model? Prior research has already investigated\nadversarial robustness for network embedding models, focused on classification\nat the node and graph level. Robustness with respect to the link prediction\ndownstream task, on the other hand, has been explored much less.\n\nThis paper contributes to filling this gap, by studying adversarial\nrobustness of Conditional Network Embedding (CNE), a state-of-the-art\nprobabilistic network embedding model, for link prediction. More specifically,\ngiven CNE and a network, we measure the sensitivity of the link predictions of\nthe model to small adversarial perturbations of the network, namely changes of\nthe link status of a node pair. Thus, our approach allows one to identify the\nlinks and non-links in the network that are most vulnerable to such\nperturbations, for further investigation by an analyst. We analyze the\ncharacteristics of the most and least sensitive perturbations, and empirically\nconfirm that our approach not only succeeds in identifying the most vulnerable\nlinks and non-links, but also that it does so in a time-efficient manner thanks\nto an effective approximation.",
          "link": "http://arxiv.org/abs/2107.01936",
          "publishedOn": "2021-07-06T01:58:11.252Z",
          "wordCount": 709,
          "title": "Adversarial Robustness of Probabilistic Network Embedding for Link Prediction. (arXiv:2107.01936v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Lanqing Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Duocai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Rap generation, which aims to produce lyrics and corresponding singing beats,\nneeds to model both rhymes and rhythms. Previous works for rap generation\nfocused on rhyming lyrics but ignored rhythmic beats, which are important for\nrap performance. In this paper, we develop DeepRapper, a Transformer-based rap\ngeneration system that can model both rhymes and rhythms. Since there is no\navailable rap dataset with rhythmic beats, we develop a data mining pipeline to\ncollect a large-scale rap dataset, which includes a large number of rap songs\nwith aligned lyrics and rhythmic beats. Second, we design a Transformer-based\nautoregressive language model which carefully models rhymes and rhythms.\nSpecifically, we generate lyrics in the reverse order with rhyme representation\nand constraint for rhyme enhancement and insert a beat symbol into lyrics for\nrhythm/beat modeling. To our knowledge, DeepRapper is the first system to\ngenerate rap with both rhymes and rhythms. Both objective and subjective\nevaluations demonstrate that DeepRapper generates creative and high-quality\nraps with rhymes and rhythms. Code will be released on GitHub.",
          "link": "http://arxiv.org/abs/2107.01875",
          "publishedOn": "2021-07-06T01:58:11.245Z",
          "wordCount": 636,
          "title": "DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling. (arXiv:2107.01875v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hakbin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Wan Choi</a>",
          "description": "In spite of the great success of deep learning technologies, training and\ndelivery of a practically serviceable model is still a highly time-consuming\nprocess. Furthermore, a resulting model is usually too generic and heavyweight,\nand hence essentially goes through another expensive model compression phase to\nfit in a resource-limited device like embedded systems. Inspired by the fact\nthat a machine learning task specifically requested by mobile users is often\nmuch simpler than it is supported by a massive generic model, this paper\nproposes a framework, called Pool of Experts (PoE), that instantly builds a\nlightweight and task-specific model without any training process. For a\nrealtime model querying service, PoE first extracts a pool of primitive\ncomponents, called experts, from a well-trained and sufficiently generic\nnetwork by exploiting a novel conditional knowledge distillation method, and\nthen performs our train-free knowledge consolidation to quickly combine\nnecessary experts into a lightweight network for a target task. Thanks to this\ntrain-free property, in our thorough empirical study, PoE can build a fairly\naccurate yet compact model in a realtime manner, whereas it takes a few minutes\nper query for the other training methods to achieve a similar level of the\naccuracy.",
          "link": "http://arxiv.org/abs/2107.01354",
          "publishedOn": "2021-07-06T01:58:11.205Z",
          "wordCount": 647,
          "title": "Pool of Experts: Realtime Querying Specialized Knowledge in Massive Neural Networks. (arXiv:2107.01354v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Ke Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qianqian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jinshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>",
          "description": "As pairwise ranking becomes broadly employed for elections, sports\ncompetitions, recommendations, and so on, attackers have strong motivation and\nincentives to manipulate the ranking list. They could inject malicious\ncomparisons into the training data to fool the victim. Such a technique is\ncalled poisoning attack in regression and classification tasks. In this paper,\nto the best of our knowledge, we initiate the first systematic investigation of\ndata poisoning attacks on pairwise ranking algorithms, which can be formalized\nas the dynamic and static games between the ranker and the attacker and can be\nmodeled as certain kinds of integer programming problems. To break the\ncomputational hurdle of the underlying integer programming problems, we\nreformulate them into the distributionally robust optimization (DRO) problems,\nwhich are computationally tractable. Based on such DRO formulations, we propose\ntwo efficient poisoning attack algorithms and establish the associated\ntheoretical guarantees. The effectiveness of the suggested poisoning attack\nstrategies is demonstrated by a series of toy simulations and several real data\nexperiments. These experimental results show that the proposed methods can\nsignificantly reduce the performance of the ranker in the sense that the\ncorrelation between the true ranking list and the aggregated results can be\ndecreased dramatically.",
          "link": "http://arxiv.org/abs/2107.01854",
          "publishedOn": "2021-07-06T01:58:11.192Z",
          "wordCount": 649,
          "title": "Poisoning Attack against Estimating from Pairwise Comparisons. (arXiv:2107.01854v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nouranizadeh_A/0/1/0/all/0/1\">Amirhossein Nouranizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matinkia_M/0/1/0/all/0/1\">Mohammadjavad Matinkia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_M/0/1/0/all/0/1\">Mohammad Rahmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safabakhsh_R/0/1/0/all/0/1\">Reza Safabakhsh</a>",
          "description": "In this paper, we propose a novel pooling layer for graph neural networks\nbased on maximizing the mutual information between the pooled graph and the\ninput graph. Since the maximum mutual information is difficult to compute, we\nemploy the Shannon capacity of a graph as an inductive bias to our pooling\nmethod. More precisely, we show that the input graph to the pooling layer can\nbe viewed as a representation of a noisy communication channel. For such a\nchannel, sending the symbols belonging to an independent set of the graph\nyields a reliable and error-free transmission of information. We show that\nreaching the maximum mutual information is equivalent to finding a maximum\nweight independent set of the graph where the weights convey entropy contents.\nThrough this communication theoretic standpoint, we provide a distinct\nperspective for posing the problem of graph pooling as maximizing the\ninformation transmission rate across a noisy communication channel, implemented\nby a graph neural network. We evaluate our method, referred to as Maximum\nEntropy Weighted Independent Set Pooling (MEWISPool), on graph classification\ntasks and the combinatorial optimization problem of the maximum independent\nset. Empirical results demonstrate that our method achieves the\nstate-of-the-art and competitive results on graph classification tasks and the\nmaximum independent set problem in several benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.01410",
          "publishedOn": "2021-07-06T01:58:11.181Z",
          "wordCount": 679,
          "title": "Maximum Entropy Weighted Independent Set Pooling for Graph Neural Networks. (arXiv:2107.01410v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rashtchian_C/0/1/0/all/0/1\">Cyrus Rashtchian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Peng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanlin Zhu</a>",
          "description": "We study statistical problems, such as planted clique, its variants, and\nsparse principal component analysis in the context of average-case\ncommunication complexity. Our motivation is to understand the\nstatistical-computational trade-offs in streaming, sketching, and query-based\nmodels. Communication complexity is the main tool for proving lower bounds in\nthese models, yet many prior results do not hold in an average-case setting. We\nprovide a general reduction method that preserves the input distribution for\nproblems involving a random graph or matrix with planted structure. Then, we\nderive two-party and multi-party communication lower bounds for detecting or\nfinding planted cliques, bipartite cliques, and related problems. As a\nconsequence, we obtain new bounds on the query complexity in the edge-probe,\nvector-matrix-vector, matrix-vector, linear sketching, and\n$\\mathbb{F}_2$-sketching models. Many of these results are nearly tight, and we\nuse our techniques to provide simple proofs of some known lower bounds for the\nedge-probe model.",
          "link": "http://arxiv.org/abs/2107.01335",
          "publishedOn": "2021-07-06T01:58:11.165Z",
          "wordCount": 594,
          "title": "Average-Case Communication Complexity of Statistical Problems. (arXiv:2107.01335v1 [cs.CC])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica I Aviles-Rivero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellars_P/0/1/0/all/0/1\">Philip Sellars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>",
          "description": "Can one learn to diagnose COVID-19 under extreme minimal supervision? Since\nthe outbreak of the novel COVID-19 there has been a rush for developing\nArtificial Intelligence techniques for expert-level disease identification on\nChest X-ray data. In particular, the use of deep supervised learning has become\nthe go-to paradigm. However, the performance of such models is heavily\ndependent on the availability of a large and representative labelled dataset.\nThe creation of which is a heavily expensive and time consuming task, and\nespecially imposes a great challenge for a novel disease. Semi-supervised\nlearning has shown the ability to match the incredible performance of\nsupervised models whilst requiring a small fraction of the labelled examples.\nThis makes the semi-supervised paradigm an attractive option for identifying\nCOVID-19. In this work, we introduce a graph based deep semi-supervised\nframework for classifying COVID-19 from chest X-rays. Our framework introduces\nan optimisation model for graph diffusion that reinforces the natural relation\namong the tiny labelled set and the vast unlabelled data. We then connect the\ndiffusion prediction output as pseudo-labels that are used in an iterative\nscheme in a deep net. We demonstrate, through our experiments, that our model\nis able to outperform the current leading supervised model with a tiny fraction\nof the labelled examples. Finally, we provide attention maps to accommodate the\nradiologist's mental model, better fitting their perceptual and cognitive\nabilities. These visualisation aims to assist the radiologist in judging\nwhether the diagnostic is correct or not, and in consequence to accelerate the\ndecision.",
          "link": "http://arxiv.org/abs/2010.00378",
          "publishedOn": "2021-07-06T01:58:11.145Z",
          "wordCount": 777,
          "title": "GraphXCOVID: Explainable Deep Graph Diffusion Pseudo-Labelling for Identifying COVID-19 on Chest X-rays. (arXiv:2010.00378v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01473",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Johansson_A/0/1/0/all/0/1\">Anton Johansson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Engsner_N/0/1/0/all/0/1\">Niklas Engsner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Strannegaard_C/0/1/0/all/0/1\">Claes Stranneg&#xe5;rd</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mostad_P/0/1/0/all/0/1\">Petter Mostad</a>",
          "description": "Neural networks are very successful tools in for example advanced\nclassification. From a statistical point of view, fitting a neural network may\nbe seen as a kind of regression, where we seek a function from the input space\nto a space of classification probabilities that follows the \"general\" shape of\nthe data, but avoids overfitting by avoiding memorization of individual data\npoints. In statistics, this can be done by controlling the geometric complexity\nof the regression function. We propose to do something similar when fitting\nneural networks by controlling the slope of the network.\n\nAfter defining the slope and discussing some of its theoretical properties,\nwe go on to show empirically in examples, using ReLU networks, that the\ndistribution of the slope of a well-trained neural network classifier is\ngenerally independent of the width of the layers in a fully connected network,\nand that the mean of the distribution only has a weak dependence on the model\narchitecture in general. The slope is of similar size throughout the relevant\nvolume, and varies smoothly. It also behaves as predicted in rescaling\nexamples. We discuss possible applications of the slope concept, such as using\nit as a part of the loss function or stopping criterion during network\ntraining, or ranking data sets in terms of their complexity.",
          "link": "http://arxiv.org/abs/2107.01473",
          "publishedOn": "2021-07-06T01:58:11.112Z",
          "wordCount": 646,
          "title": "Slope and generalization properties of neural networks. (arXiv:2107.01473v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S. M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-07-06T01:58:11.104Z",
          "wordCount": 641,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Weidong Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1\">Qi Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tijin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuanqing Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1\">Weipeng Cao</a>",
          "description": "Adaptive optimization methods have been widely used in deep learning. They\nscale the learning rates adaptively according to the past gradient, which has\nbeen shown to be effective to accelerate the convergence. However, they suffer\nfrom poor generalization performance compared with SGD. Recent studies point\nthat smoothing exponential gradient noise leads to generalization degeneration\nphenomenon. Inspired by this, we propose AdaL, with a transformation on the\noriginal gradient. AdaL accelerates the convergence by amplifying the gradient\nin the early stage, as well as dampens the oscillation and stabilizes the\noptimization by shrinking the gradient later. Such modification alleviates the\nsmoothness of gradient noise, which produces better generalization performance.\nWe have theoretically proved the convergence of AdaL and demonstrated its\neffectiveness on several benchmarks.",
          "link": "http://arxiv.org/abs/2107.01525",
          "publishedOn": "2021-07-06T01:58:11.068Z",
          "wordCount": 565,
          "title": "AdaL: Adaptive Gradient Transformation Contributes to Convergences and Generalizations. (arXiv:2107.01525v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Penco_L/0/1/0/all/0/1\">Luigi Penco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1\">Jean-Baptiste Mouret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivaldi_S/0/1/0/all/0/1\">Serena Ivaldi</a>",
          "description": "Humanoid robots could be versatile and intuitive human avatars that operate\nremotely in inaccessible places: the robot could reproduce in the remote\nlocation the movements of an operator equipped with a wearable motion capture\ndevice while sending visual feedback to the operator. While substantial\nprogress has been made on transferring (\"retargeting\") human motions to\nhumanoid robots, a major problem preventing the deployment of such systems in\nreal applications is the presence of communication delays between the human\ninput and the feedback from the robot: even a few hundred milliseconds of delay\ncan irreversibly disturb the operator, let alone a few seconds. To overcome\nthese delays, we introduce a system in which a humanoid robot executes commands\nbefore it actually receives them, so that the visual feedback appears to be\nsynchronized to the operator, whereas the robot executed the commands in the\npast. To do so, the robot continuously predicts future commands by querying a\nmachine learning model that is trained on past trajectories and conditioned on\nthe last received commands. In our experiments, an operator was able to\nsuccessfully control a humanoid robot (32 degrees of freedom) with stochastic\ndelays up to 2 seconds in several whole-body manipulation tasks, including\nreaching different targets, picking up, and placing a box at distinct\nlocations.",
          "link": "http://arxiv.org/abs/2107.01281",
          "publishedOn": "2021-07-06T01:58:11.040Z",
          "wordCount": 641,
          "title": "Prescient teleoperation of humanoid robots. (arXiv:2107.01281v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06188",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Danilova_M/0/1/0/all/0/1\">Marina Danilova</a>, <a href=\"http://arxiv.org/find/math/1/au:+Dvurechensky_P/0/1/0/all/0/1\">Pavel Dvurechensky</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1\">Alexander Gasnikov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gorbunov_E/0/1/0/all/0/1\">Eduard Gorbunov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Guminov_S/0/1/0/all/0/1\">Sergey Guminov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kamzolov_D/0/1/0/all/0/1\">Dmitry Kamzolov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Shibaev_I/0/1/0/all/0/1\">Innokentiy Shibaev</a>",
          "description": "Motivated by recent increased interest in optimization algorithms for\nnon-convex optimization in application to training deep neural networks and\nother optimization problems in data analysis, we give an overview of recent\ntheoretical results on global performance guarantees of optimization algorithms\nfor non-convex optimization. We start with classical arguments showing that\ngeneral non-convex problems could not be solved efficiently in a reasonable\ntime. Then we give a list of problems that can be solved efficiently to find\nthe global minimizer by exploiting the structure of the problem as much as it\nis possible. Another way to deal with non-convexity is to relax the goal from\nfinding the global minimum to finding a stationary point or a local minimum.\nFor this setting, we first present known results for the convergence rates of\ndeterministic first-order methods, which are then followed by a general\ntheoretical analysis of optimal stochastic and randomized gradient schemes, and\nan overview of the stochastic first-order methods. After that, we discuss quite\ngeneral classes of non-convex problems, such as minimization of\n$\\alpha$-weakly-quasi-convex functions and functions that satisfy\nPolyak--Lojasiewicz condition, which still allow obtaining theoretical\nconvergence guarantees of first-order methods. Then we consider higher-order\nand zeroth-order/derivative-free methods and their convergence rates for\nnon-convex optimization problems.",
          "link": "http://arxiv.org/abs/2012.06188",
          "publishedOn": "2021-07-06T01:58:11.018Z",
          "wordCount": 663,
          "title": "Recent Theoretical Advances in Non-Convex Optimization. (arXiv:2012.06188v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.09650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shaojun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haodong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaojing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1\">Hongyuan Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Haomin Zhou</a>",
          "description": "Inverse optimal transport (OT) refers to the problem of learning the cost\nfunction for OT from observed transport plan or its samples. In this paper, we\nderive an unconstrained convex optimization formulation of the inverse OT\nproblem, which can be further augmented by any customizable regularization. We\nprovide a comprehensive characterization of the properties of inverse OT,\nincluding uniqueness of solutions. We also develop two numerical algorithms,\none is a fast matrix scaling method based on the Sinkhorn-Knopp algorithm for\ndiscrete OT, and the other one is a learning based algorithm that parameterizes\nthe cost function as a deep neural network for continuous OT. The novel\nframework proposed in the work avoids repeatedly solving a forward OT in each\niteration which has been a thorny computational bottleneck for the bi-level\noptimization in existing inverse OT approaches. Numerical results demonstrate\npromising efficiency and accuracy advantages of the proposed algorithms over\nexisting state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2002.09650",
          "publishedOn": "2021-07-06T01:58:11.002Z",
          "wordCount": 613,
          "title": "Learning Cost Functions for Optimal Transport. (arXiv:2002.09650v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Suman Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1\">Menelaos Kanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We present an approach for encoding visual task relationships to improve\nmodel performance in an Unsupervised Domain Adaptation (UDA) setting. Semantic\nsegmentation and monocular depth estimation are shown to be complementary\ntasks; in a multi-task learning setting, a proper encoding of their\nrelationships can further improve performance on both tasks. Motivated by this\nobservation, we propose a novel Cross-Task Relation Layer (CTRL), which encodes\ntask dependencies between the semantic and depth predictions. To capture the\ncross-task relationships, we propose a neural network architecture that\ncontains task-specific and cross-task refinement heads. Furthermore, we propose\nan Iterative Self-Learning (ISL) training scheme, which exploits semantic\npseudo-labels to provide extra supervision on the target domain. We\nexperimentally observe improvements in both tasks' performance because the\ncomplementary information present in these tasks is better captured.\nSpecifically, we show that: (1) our approach improves performance on all tasks\nwhen they are complementary and mutually dependent; (2) the CTRL helps to\nimprove both semantic segmentation and depth estimation tasks performance in\nthe challenging UDA setting; (3) the proposed ISL training scheme further\nimproves the semantic segmentation performance. The implementation is available\nat https://github.com/susaha/ctrl-uda.",
          "link": "http://arxiv.org/abs/2105.07830",
          "publishedOn": "2021-07-06T01:58:10.994Z",
          "wordCount": 684,
          "title": "Learning to Relate Depth and Semantics for Unsupervised Domain Adaptation. (arXiv:2105.07830v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yangkun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiarui Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>",
          "description": "Over the past few years, graph neural networks (GNN) and label\npropagation-based methods have made significant progress in addressing node\nclassification tasks on graphs. However, in addition to their reliance on\nelaborate architectures and algorithms, there are several key technical details\nthat are frequently overlooked, and yet nonetheless can play a vital role in\nachieving satisfactory performance. In this paper, we first summarize a series\nof existing tricks-of-the-trade, and then propose several new ones related to\nlabel usage, loss function formulation, and model design that can significantly\nimprove various GNN architectures. We empirically evaluate their impact on\nfinal node classification accuracy by conducting ablation studies and\ndemonstrate consistently-improved performance, often to an extent that\noutweighs the gains from more dramatic changes in the underlying GNN\narchitecture. Notably, many of the top-ranked models on the Open Graph\nBenchmark (OGB) leaderboard and KDDCUP 2021 Large-Scale Challenge MAG240M-LSC\nbenefit from these techniques.",
          "link": "http://arxiv.org/abs/2103.13355",
          "publishedOn": "2021-07-06T01:58:10.985Z",
          "wordCount": null,
          "title": "Bag of Tricks for Node Classification with Graph Neural Networks. (arXiv:2103.13355v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_P/0/1/0/all/0/1\">Pengcheng An</a>",
          "description": "Current research on Explainable AI (XAI) heavily targets on expert users\n(data scientists or AI developers). However, increasing importance has been\nargued for making AI more understandable to nonexperts, who are expected to\nleverage AI techniques, but have limited knowledge about AI. We present a\nmobile application to support nonexperts to interactively make sense of\nConvolutional Neural Networks (CNN); it allows users to play with a pretrained\nCNN by taking pictures of their surrounding objects. We use an up-to-date XAI\ntechnique (Class Activation Map) to intuitively visualize the model's decision\n(the most important image regions that lead to a certain result). Deployed in a\nuniversity course, this playful learning tool was found to support design\nstudents to gain vivid understandings about the capabilities and limitations of\npretrained CNNs in real-world environments. Concrete examples of students'\nplayful explorations are reported to characterize their sensemaking processes\nreflecting different depths of thought.",
          "link": "http://arxiv.org/abs/2107.01996",
          "publishedOn": "2021-07-06T01:58:10.982Z",
          "wordCount": null,
          "title": "Explainability via Interactivity? Supporting Nonexperts' Sensemaking of Pretrained CNN by Interacting with Their Daily Surroundings. (arXiv:2107.01996v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1\">Isaac J. Sledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1\">Jose C. Principe</a>",
          "description": "A fundamental problem when aggregating Markov chains is the specification of\nthe number of state groups. Too few state groups may fail to sufficiently\ncapture the pertinent dynamics of the original, high-order Markov chain. Too\nmany state groups may lead to a non-parsimonious, reduced-order Markov chain\nwhose complexity rivals that of the original. In this paper, we show that an\naugmented value-of-information-based approach to aggregating Markov chains\nfacilitates the determination of the number of state groups. The optimal\nstate-group count coincides with the case where the complexity of the\nreduced-order chain is balanced against the mutual dependence between the\noriginal- and reduced-order chain dynamics.",
          "link": "http://arxiv.org/abs/2107.01799",
          "publishedOn": "2021-07-06T01:58:10.980Z",
          "wordCount": 564,
          "title": "An Information-Theoretic Approach for Automatically Determining the Number of States when Aggregating Markov Chains. (arXiv:2107.01799v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01734",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Passino_F/0/1/0/all/0/1\">Francesco Sanna Passino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heard_N/0/1/0/all/0/1\">Nicholas A. Heard</a>",
          "description": "Spectral embedding of network adjacency matrices often produces node\nrepresentations living approximately around low-dimensional submanifold\nstructures. In particular, hidden substructure is expected to arise when the\ngraph is generated from a latent position model. Furthermore, the presence of\ncommunities within the network might generate community-specific submanifold\nstructures in the embedding, but this is not explicitly accounted for in most\nstatistical models for networks. In this article, a class of models called\nlatent structure block models (LSBM) is proposed to address such scenarios,\nallowing for graph clustering when community-specific one dimensional manifold\nstructure is present. LSBMs focus on a specific class of latent space model,\nthe random dot product graph (RDPG), and assign a latent submanifold to the\nlatent positions of each community. A Bayesian model for the embeddings arising\nfrom LSBMs is discussed, and shown to have a good performance on simulated and\nreal world network data. The model is able to correctly recover the underlying\ncommunities living in a one-dimensional manifold, even when the parametric form\nof the underlying curves is unknown, achieving remarkable results on a variety\nof real data.",
          "link": "http://arxiv.org/abs/2107.01734",
          "publishedOn": "2021-07-06T01:58:10.972Z",
          "wordCount": 615,
          "title": "Latent structure blockmodels for Bayesian spectral graph clustering. (arXiv:2107.01734v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Struckmeier_O/0/1/0/all/0/1\">Oliver Struckmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1\">Kshitij Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1\">Ville Kyrki</a>",
          "description": "The slowness principle is a concept inspired by the visual cortex of the\nbrain. It postulates that the underlying generative factors of a quickly\nvarying sensory signal change on a slower time scale. Unsupervised learning of\nintermediate representations utilizing abundant unlabeled sensory data can be\nleveraged to perform data-efficient supervised downstream regression. In this\npaper, we propose a general formulation of slowness for unsupervised\nrepresentation learning adding a slowness regularization term to the estimate\nlower bound of the beta-VAE to encourage temporal similarity in observation and\nlatent space. Within this framework we compare existing slowness regularization\nterms such as the L1 and L2 loss used in existing end-to-end methods, the\nSlowVAE and propose a new term based on Brownian motion. We empirically\nevaluate these slowness regularization terms with respect to their downstream\ntask performance and data efficiency. We find that slow representations lead to\nequal or better downstream task performance and data efficiency in different\nexperiment domains when compared to representations without slowness\nregularization. Finally, we discuss how the Frechet Inception Distance (FID),\ntraditionally used to determine the generative capabilities of GANs, can serve\nas a measure to predict the performance of pre-trained Autoencoder model in a\nsupervised downstream task and accelerate hyperparameter search.",
          "link": "http://arxiv.org/abs/2012.06279",
          "publishedOn": "2021-07-06T01:58:10.947Z",
          "wordCount": null,
          "title": "Autoencoding Slow Representations for Semi-supervised Data Efficient Regression. (arXiv:2012.06279v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.00291",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1\">Niladri S. Chatterji</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter L. Bartlett</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Long_P/0/1/0/all/0/1\">Philip M. Long</a>",
          "description": "We consider the problem of sampling from a strongly log-concave density in\n$\\mathbb{R}^d$, and prove an information theoretic lower bound on the number of\nstochastic gradient queries of the log density needed. Several popular sampling\nalgorithms (including many Markov chain Monte Carlo methods) operate by using\nstochastic gradients of the log density to generate a sample; our results\nestablish an information theoretic limit for all these algorithms.\n\nWe show that for every algorithm, there exists a well-conditioned strongly\nlog-concave target density for which the distribution of points generated by\nthe algorithm would be at least $\\varepsilon$ away from the target in total\nvariation distance if the number of gradient queries is less than\n$\\Omega(\\sigma^2 d/\\varepsilon^2)$, where $\\sigma^2 d$ is the variance of the\nstochastic gradient. Our lower bound follows by combining the ideas of Le Cam\ndeficiency routinely used in the comparison of statistical experiments along\nwith standard information theoretic tools used in lower bounding Bayes risk\nfunctions. To the best of our knowledge our results provide the first\nnontrivial dimension-dependent lower bound for this problem.",
          "link": "http://arxiv.org/abs/2002.00291",
          "publishedOn": "2021-07-06T01:58:10.939Z",
          "wordCount": null,
          "title": "Oracle Lower Bounds for Stochastic Gradient Sampling Algorithms. (arXiv:2002.00291v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Collins_L/0/1/0/all/0/1\">Liam Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokhtari_A/0/1/0/all/0/1\">Aryan Mokhtari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>",
          "description": "Model-Agnostic Meta-Learning (MAML) has become increasingly popular for\ntraining models that can quickly adapt to new tasks via one or few stochastic\ngradient descent steps. However, the MAML objective is significantly more\ndifficult to optimize compared to standard Empirical Risk Minimization (ERM),\nand little is understood about how much MAML improves over ERM in terms of the\nfast adaptability of their solutions in various scenarios. We analytically\naddress this issue in a linear regression setting consisting of a mixture of\neasy and hard tasks, where hardness is related to the condition number of the\ntask's loss function. Specifically, we prove that in order for MAML to achieve\nsubstantial gain over ERM, (i) there must be some discrepancy in hardness among\nthe tasks, and (ii) the optimal solutions of the hard tasks must be closely\npacked with the center far from the center of the easy tasks optimal solutions.\nWe also give numerical and analytical results suggesting that these insights\nalso apply to two-layer neural networks. Finally, we provide few-shot image\nclassification experiments that support our insights for when MAML should be\nused and emphasize the importance of training MAML on hard tasks in practice.",
          "link": "http://arxiv.org/abs/2010.14672",
          "publishedOn": "2021-07-06T01:58:10.887Z",
          "wordCount": 669,
          "title": "How Does the Task Landscape Affect MAML Performance?. (arXiv:2010.14672v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiulou Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yuxia Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shouju Wang</a>",
          "description": "Intratumoral nanoparticles (NPs) distribution is critical for the success of\nnanomedicine in imaging and treatment, but computational models to describe the\nNPs distribution remain unavailable due to the complex tumor-nano interactions.\nHere, we develop a Generative Adversarial Network for Distribution Analysis\n(GANDA) to describe and conditionally generates the intratumoral quantum dots\n(QDs) distribution after i.v. injection. This deep generative model is trained\nautomatically by 27 775 patches of tumor vessels and cell nuclei decomposed\nfrom whole-slide images of 4T1 breast cancer sections. The GANDA model can\nconditionally generate images of intratumoral QDs distribution under the\nconstraint of given tumor vessels and cell nuclei channels with the same\nspatial resolution (pixels-to-pixels), minimal loss (mean squared error, MSE =\n1.871) and excellent reliability (intraclass correlation, ICC = 0.94).\nQuantitative analysis of QDs extravasation distance (ICC = 0.95) and subarea\ndistribution (ICC = 0.99) is allowed on the generated images without knowing\nthe real QDs distribution. We believe this deep generative model may provide\nopportunities to investigate how influencing factors affect NPs distribution in\nindividual tumors and guide nanomedicine optimization for molecular imaging and\npersonalized treatment.",
          "link": "http://arxiv.org/abs/2012.12561",
          "publishedOn": "2021-07-06T01:58:10.846Z",
          "wordCount": 668,
          "title": "GANDA: A deep generative adversarial network predicts the spatial distribution of nanoparticles in tumor pixelly. (arXiv:2012.12561v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khodabakhsh_A/0/1/0/all/0/1\">Ali Khodabakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zahid Akhtar</a>",
          "description": "Despite the impressive progress in the field of presentation attack detection\nand multimedia forensics over the last decade, these systems are still\nvulnerable to attacks in real-life settings. Some of the challenges for\nexisting solutions are the detection of unknown attacks, the ability to perform\nin adversarial settings, few-shot learning, and explainability. In this study,\nthese limitations are approached by reliance on a game-theoretic view for\nmodeling the interactions between the attacker and the detector. Consequently,\na new optimization criterion is proposed and a set of requirements are defined\nfor improving the performance of these systems in real-life settings.\nFurthermore, a novel detection technique is proposed using generator-based\nfeature sets that are not biased towards any specific attack species. To\nfurther optimize the performance on known attacks, a new loss function coined\ncategorical margin maximization loss (C-marmax) is proposed which gradually\nimproves the performance against the most powerful attack. The proposed\napproach provides a more balanced performance across known and unknown attacks\nand achieves state-of-the-art performance in known and unknown attack detection\ncases against rational attackers. Lastly, the few-shot learning potential of\nthe proposed approach is studied as well as its ability to provide pixel-level\nexplainability.",
          "link": "http://arxiv.org/abs/2010.01592",
          "publishedOn": "2021-07-06T01:58:10.792Z",
          "wordCount": 668,
          "title": "Unknown Presentation Attack Detection against Rational Attackers. (arXiv:2010.01592v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zizhou Su</a>",
          "description": "Many practical applications of reinforcement learning (RL) constrain the\nagent to learn from a fixed offline dataset of logged interactions, which has\nalready been gathered, without offering further possibility for data\ncollection. However, commonly used off-policy RL algorithms, such as the Deep Q\nNetwork and the Deep Deterministic Policy Gradient, are incapable of learning\nwithout data correlated to the distribution under the current policy, making\nthem ineffective for this offline setting. As the first step towards useful\noffline RL algorithms, we analysis the reason of instability in standard\noff-policy RL algorithms. It is due to the bootstrapping error. The key to\navoiding this error, is ensuring that the agent's action space does not go out\nof the fixed offline dataset. Based on our consideration, a creative offline RL\nframework, the Least Restriction (LR), is proposed in this paper. The LR\nregards selecting an action as taking a sample from the probability\ndistribution. It merely set a little limit for action selection, which not only\navoid the action being out of the offline dataset but also remove all the\nunreasonable restrictions in earlier approaches (e.g. Batch-Constrained Deep\nQ-Learning). In the further, we will demonstrate that the LR, is able to learn\nrobustly from different offline datasets, including random and suboptimal\ndemonstrations, on a range of practical control tasks.",
          "link": "http://arxiv.org/abs/2107.01757",
          "publishedOn": "2021-07-06T01:58:10.782Z",
          "wordCount": 642,
          "title": "The Least Restriction for Offline Reinforcement Learning. (arXiv:2107.01757v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tax_N/0/1/0/all/0/1\">Niek Tax</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_K/0/1/0/all/0/1\">Kees Jan de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Mathijs de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dosoula_N/0/1/0/all/0/1\">Nikoleta Dosoula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akker_B/0/1/0/all/0/1\">Bram van den Akker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Jon Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thuong_O/0/1/0/all/0/1\">Olivier Thuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardi_L/0/1/0/all/0/1\">Lucas Bernardi</a>",
          "description": "Fraud detection and prevention play an important part in ensuring the\nsustained operation of any e-commerce business. Machine learning (ML) often\nplays an important role in these anti-fraud operations, but the organizational\ncontext in which these ML models operate cannot be ignored. In this paper, we\ntake an organization-centric view on the topic of fraud detection by\nformulating an operational model of the anti-fraud departments in e-commerce\norganizations. We derive 6 research topics and 12 practical challenges for\nfraud detection from this operational model. We summarize the state of the\nliterature for each research topic, discuss potential solutions to the\npractical challenges, and identify 22 open research challenges.",
          "link": "http://arxiv.org/abs/2107.01979",
          "publishedOn": "2021-07-06T01:58:10.757Z",
          "wordCount": 590,
          "title": "Machine Learning for Fraud Detection in E-Commerce: A Research Agenda. (arXiv:2107.01979v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rex Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramli_A/0/1/0/all/0/1\">Albara Ah Ramli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanle Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_E/0/1/0/all/0/1\">Esha Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henricson_E/0/1/0/all/0/1\">Erik Henricson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>",
          "description": "With the rapid development of the internet of things (IoT) and artificial\nintelligence (AI) technologies, human activity recognition (HAR) has been\napplied in a variety of domains such as security and surveillance, human-robot\ninteraction, and entertainment. Even though a number of surveys and review\npapers have been published, there is a lack of HAR overview papers focusing on\nhealthcare applications that use wearable sensors. Therefore, we fill in the\ngap by presenting this overview paper. In particular, we present our projects\nto illustrate the system design of HAR applications for healthcare. Our\nprojects include early mobility identification of human activities for\nintensive care unit (ICU) patients and gait analysis of Duchenne muscular\ndystrophy (DMD) patients. We cover essential components of designing HAR\nsystems including sensor factors (e.g., type, number, and placement location),\nAI model selection (e.g., classical machine learning models versus deep\nlearning models), and feature engineering. In addition, we highlight the\nchallenges of such healthcare-oriented HAR systems and propose several research\nopportunities for both the medical and the computer science community.",
          "link": "http://arxiv.org/abs/2103.15990",
          "publishedOn": "2021-07-06T01:58:10.750Z",
          "wordCount": 657,
          "title": "An Overview of Human Activity Recognition Using Wearable Sensors: Healthcare and Artificial Intelligence. (arXiv:2103.15990v3 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi-Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Generalizable person Re-Identification (ReID) has attracted growing attention\nin recent computer vision community. In this work, we construct a structural\ncausal model among identity labels, identity-specific factors (clothes/shoes\ncolor etc), and domain-specific factors (background, viewpoints etc). According\nto the causal analysis, we propose a novel Domain Invariant Representation\nLearning for generalizable person Re-Identification (DIR-ReID) framework.\nSpecifically, we first propose to disentangle the identity-specific and\ndomain-specific feature spaces, based on which we propose an effective\nalgorithmic implementation for backdoor adjustment, essentially serving as a\ncausal intervention towards the SCM. Extensive experiments have been conducted,\nshowing that DIR-ReID outperforms state-of-the-art methods on large-scale\ndomain generalization ReID benchmarks.",
          "link": "http://arxiv.org/abs/2103.15890",
          "publishedOn": "2021-07-06T01:58:10.743Z",
          "wordCount": 581,
          "title": "Learning Domain Invariant Representations for Generalizable Person Re-Identification. (arXiv:2103.15890v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16161",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_D/0/1/0/all/0/1\">Ding Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kin_T/0/1/0/all/0/1\">Taichi Kin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Igarashi_T/0/1/0/all/0/1\">Takeo Igarashi</a>",
          "description": "The exact shape of intracranial aneurysms is critical in medical diagnosis\nand surgical planning. While voxel-based deep learning frameworks have been\nproposed for this segmentation task, their performance remains limited. In this\nstudy, we offer a two-step surface-based deep learning pipeline that achieves\nsignificantly higher performance. Our proposed model takes a surface model of\nentire principal brain arteries containing aneurysms as input and returns\naneurysms surfaces as output. A user first generates a surface model by\nmanually specifying multiple thresholds for time-of-flight magnetic resonance\nangiography images. The system then samples small surface fragments from the\nentire brain arteries and classifies the surface fragments according to whether\naneurysms are present using a point-based deep learning network (PointNet++).\nFinally, the system applies surface segmentation (SO-Net) to surface fragments\ncontaining aneurysms. We conduct a direct comparison of segmentation\nperformance by counting voxels between the proposed surface-based framework and\nthe existing voxel-based method, in which our framework achieves a much higher\ndice similarity coefficient score (72%) than the prior approach (46%).",
          "link": "http://arxiv.org/abs/2006.16161",
          "publishedOn": "2021-07-06T01:58:10.737Z",
          "wordCount": 640,
          "title": "A Two-step Surface-based 3D Deep Learning Pipeline for Segmentation of Intracranial Aneurysms. (arXiv:2006.16161v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fiky_A/0/1/0/all/0/1\">Ahmed Hashem El Fiky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenawy_A/0/1/0/all/0/1\">Ayman El Shenawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madkour_M/0/1/0/all/0/1\">Mohamed Ashraf Madkour</a>",
          "description": "Android malware is one of the most dangerous threats on the internet, and\nit's been on the rise for several years. Despite significant efforts in\ndetecting and classifying android malware from innocuous android applications,\nthere is still a long way to go. As a result, there is a need to provide a\nbasic understanding of the behavior displayed by the most common Android\nmalware categories and families. Each Android malware family and category has a\ndistinct objective. As a result, it has impacted every corporate area,\nincluding healthcare, banking, transportation, government, and e-commerce. In\nthis paper, we presented two machine-learning approaches for Dynamic Analysis\nof Android Malware: one for detecting and identifying Android Malware\nCategories and the other for detecting and identifying Android Malware\nFamilies, which was accomplished by analyzing a massive malware dataset with 14\nprominent malware categories and 180 prominent malware families of\nCCCS-CIC-AndMal2020 dataset on Dynamic Layers. Our approach achieves in Android\nMalware Category detection more than 96 % accurate and achieves in Android\nMalware Family detection more than 99% accurate. Our approach provides a method\nfor high-accuracy Dynamic Analysis of Android Malware while also shortening the\ntime required to analyze smartphone malware.",
          "link": "http://arxiv.org/abs/2107.01927",
          "publishedOn": "2021-07-06T01:58:10.731Z",
          "wordCount": 644,
          "title": "Android Malware Category and Family Detection and Identification using Machine Learning. (arXiv:2107.01927v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jun Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mohammad Al Hasan</a>",
          "description": "Supervised learning, while deployed in real-life scenarios, often encounters\ninstances of unknown classes. Conventional algorithms for training a supervised\nlearning model do not provide an option to detect such instances, so they\nmiss-classify such instances with 100% probability. Open Set Recognition (OSR)\nand Non-Exhaustive Learning (NEL) are potential solutions to overcome this\nproblem. Most existing methods of OSR first classify members of existing\nclasses and then identify instances of new classes. However, many of the\nexisting methods of OSR only makes a binary decision, i.e., they only identify\nthe existence of the unknown class. Hence, such methods cannot distinguish test\ninstances belonging to incremental unseen classes. On the other hand, the\nmajority of NEL methods often make a parametric assumption over the data\ndistribution, which either fail to return good results, due to the reason that\nreal-life complex datasets may not follow a well-known data distribution. In\nthis paper, we propose a new online non-exhaustive learning model, namely,\nNon-Exhaustive Gaussian Mixture Generative Adversarial Networks (NE-GM-GAN) to\naddress these issues. Our proposed model synthesizes Gaussian mixture based\nlatent representation over a deep generative model, such as GAN, for\nincremental detection of instances of emerging classes in the test data.\nExtensive experimental results on several benchmark datasets show that\nNE-GM-GAN significantly outperforms the state-of-the-art methods in detecting\ninstances of novel classes in streaming data.",
          "link": "http://arxiv.org/abs/2106.14344",
          "publishedOn": "2021-07-06T01:58:10.704Z",
          "wordCount": 680,
          "title": "Non-Exhaustive Learning Using Gaussian Mixture Generative Adversarial Networks. (arXiv:2106.14344v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlechtinger_M/0/1/0/all/0/1\">Michael Schlechtinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosack_D/0/1/0/all/0/1\">Damaris Kosack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetzer_T/0/1/0/all/0/1\">Thomas Fetzer</a>",
          "description": "Pricing decisions are increasingly made by AI. Thanks to their ability to\ntrain with live market data while making decisions on the fly, deep\nreinforcement learning algorithms are especially effective in taking such\npricing decisions. In e-commerce scenarios, multiple reinforcement learning\nagents can set prices based on their competitor's prices. Therefore, research\nstates that agents might end up in a state of collusion in the long run. To\nfurther analyze this issue, we build a scenario that is based on a modified\nversion of a prisoner's dilemma where three agents play the game of rock paper\nscissors. Our results indicate that the action selection can be dissected into\nspecific stages, establishing the possibility to develop collusion prevention\nsystems that are able to recognize situations which might lead to a collusion\nbetween competitors. We furthermore provide evidence for a situation where\nagents are capable of performing a tacit cooperation strategy without being\nexplicitly trained to do so.",
          "link": "http://arxiv.org/abs/2107.01856",
          "publishedOn": "2021-07-06T01:58:10.691Z",
          "wordCount": 618,
          "title": "Winning at Any Cost -- Infringing the Cartel Prohibition With Reinforcement Learning. (arXiv:2107.01856v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>",
          "description": "Intelligence necessitates memory. Without memory, humans fail to perform\nvarious nontrivial tasks such as reading novels, playing games or solving\nmaths. As the ultimate goal of machine learning is to derive intelligent\nsystems that learn and act automatically just like human, memory construction\nfor machine is inevitable. Artificial neural networks model neurons and\nsynapses in the brain by interconnecting computational units via weights, which\nis a typical class of machine learning algorithms that resembles memory\nstructure. Their descendants with more complicated modeling techniques (a.k.a\ndeep learning) have been successfully applied to many practical problems and\ndemonstrated the importance of memory in the learning process of machinery\nsystems. Recent progresses on modeling memory in deep learning have revolved\naround external memory constructions, which are highly inspired by\ncomputational Turing models and biological neuronal systems. Attention\nmechanisms are derived to support acquisition and retention operations on the\nexternal memory. Despite the lack of theoretical foundations, these approaches\nhave shown promises to help machinery systems reach a higher level of\nintelligence. The aim of this thesis is to advance the understanding on memory\nand attention in deep learning. Its contributions include: (i) presenting a\ncollection of taxonomies for memory, (ii) constructing new memory-augmented\nneural networks (MANNs) that support multiple control and memory units, (iii)\nintroducing variability via memory in sequential generative models, (iv)\nsearching for optimal writing operations to maximise the memorisation capacity\nin slot-based memory networks, and (v) simulating the Universal Turing Machine\nvia Neural Stored-program Memory-a new kind of external memory for neural\nnetworks.",
          "link": "http://arxiv.org/abs/2107.01390",
          "publishedOn": "2021-07-06T01:58:10.680Z",
          "wordCount": 681,
          "title": "Memory and attention in deep learning. (arXiv:2107.01390v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01275",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lohrenz_T/0/1/0/all/0/1\">Timo Lohrenz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwarz_P/0/1/0/all/0/1\">Patrick Schwarz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhengyang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>",
          "description": "Recently, attention-based encoder-decoder (AED) models have shown high\nperformance for end-to-end automatic speech recognition (ASR) across several\ntasks. Addressing overconfidence in such models, in this paper we introduce the\nconcept of relaxed attention, which is a simple gradual injection of a uniform\ndistribution to the encoder-decoder attention weights during training that is\neasily implemented with two lines of code. We investigate the effect of relaxed\nattention across different AED model architectures and two prominent ASR tasks,\nWall Street Journal (WSJ) and Librispeech. We found that transformers trained\nwith relaxed attention outperform the standard baseline models consistently\nduring decoding with external language models. On WSJ, we set a new benchmark\nfor transformer-based end-to-end speech recognition with a word error rate of\n3.65%, outperforming state of the art (4.20%) by 13.1% relative, while\nintroducing only a single hyperparameter. Upon acceptance, models will be\npublished on github.",
          "link": "http://arxiv.org/abs/2107.01275",
          "publishedOn": "2021-07-06T01:58:10.629Z",
          "wordCount": 610,
          "title": "Relaxed Attention: A Simple Method to Boost Performance of End-to-End Automatic Speech Recognition. (arXiv:2107.01275v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01392",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kumar_P/0/1/0/all/0/1\">Peeyush Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gangal_A/0/1/0/all/0/1\">Ayushe Gangal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumari_S/0/1/0/all/0/1\">Sunita Kumari</a>",
          "description": "Coronavirus is a large virus family consisting of diverse viruses, some of\nwhich disseminate among mammals and others cause sickness among humans.\nCOVID-19 is highly contagious and is rapidly spreading, rendering its early\ndiagnosis of preeminent status. Researchers, medical specialists and\norganizations all over the globe have been working tirelessly to combat this\nvirus and help in its containment. In this paper, a novel neural network called\nWisdomNet has been proposed, for the diagnosis of COVID-19 using chest X-rays.\nThe WisdomNet uses the concept of Wisdom of Crowds as its founding idea. It is\na two-layered convolutional Neural Network (CNN), which takes chest x-ray\nimages as input. Both layers of the proposed neural network consist of a number\nof neural networks each. The dataset used for this study consists of chest\nx-ray images of COVID-19 positive patients, compiled and shared by Dr. Cohen on\nGitHub, and the chest x-ray images of healthy lungs and lungs affected by viral\nand bacterial pneumonia were obtained from Kaggle. The network not only\npinpoints the presence of COVID-19, but also gives the probability of the\ndisease maturing into Acute Respiratory Distress Syndrome (ARDS). Thus,\npredicting the progression of the disease in the COVID-19 positive patients.\nThe network also slender the occurrences of false negative cases by employing a\nhigh threshold value, thus aids in curbing the spread of the disease and gives\nan accuracy of 100% for successfully predicting COVID-19 among the chest x-rays\nof patients affected with COVID-19, bacterial and viral pneumonia.",
          "link": "http://arxiv.org/abs/2107.01392",
          "publishedOn": "2021-07-06T01:58:10.620Z",
          "wordCount": 795,
          "title": "WisdomNet: Prognosis of COVID-19 with Slender Prospect of False Negative Cases and Vaticinating the Probability of Maturation to ARDS using Posteroanterior Chest X-Rays. (arXiv:2107.01392v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01988",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Louiset_R/0/1/0/all/0/1\">Robin Louiset</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dufumier_B/0/1/0/all/0/1\">Benoit Dufumier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Houenou_J/0/1/0/all/0/1\">Josselin Houenou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grigis_A/0/1/0/all/0/1\">Antoine Grigis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duchesnay_E/0/1/0/all/0/1\">Edouard Duchesnay</a>",
          "description": "Subtype Discovery consists in finding interpretable and consistent sub-parts\nof a dataset, which are also relevant to a certain supervised task. From a\nmathematical point of view, this can be defined as a clustering task driven by\nsupervised learning in order to uncover subgroups in line with the supervised\nprediction. In this paper, we propose a general Expectation-Maximization\nensemble framework entitled UCSL (Unsupervised Clustering driven by Supervised\nLearning). Our method is generic, it can integrate any clustering method and\ncan be driven by both binary classification and regression. We propose to\nconstruct a non-linear model by merging multiple linear estimators, one per\ncluster. Each hyperplane is estimated so that it correctly discriminates - or\npredict - only one cluster. We use SVC or Logistic Regression for\nclassification and SVR for regression. Furthermore, to perform cluster analysis\nwithin a more suitable space, we also propose a dimension-reduction algorithm\nthat projects the data onto an orthonormal space relevant to the supervised\ntask. We analyze the robustness and generalization capability of our algorithm\nusing synthetic and experimental datasets. In particular, we validate its\nability to identify suitable consistent sub-types by conducting a\npsychiatric-diseases cluster analysis with known ground-truth labels. The gain\nof the proposed method over previous state-of-the-art techniques is about +1.9\npoints in terms of balanced accuracy. Finally, we make codes and examples\navailable in a scikit-learn-compatible Python package at\nhttps://github.com/neurospin-projects/2021_rlouiset_ucsl",
          "link": "http://arxiv.org/abs/2107.01988",
          "publishedOn": "2021-07-06T01:58:10.591Z",
          "wordCount": 699,
          "title": "UCSL : A Machine Learning Expectation-Maximization framework for Unsupervised Clustering driven by Supervised Learning. (arXiv:2107.01988v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01658",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dallakyan_A/0/1/0/all/0/1\">Aramayis Dallakyan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pourahmadi_M/0/1/0/all/0/1\">Mohsen Pourahmadi</a>",
          "description": "We establish a novel framework for learning a directed acyclic graph (DAG)\nwhen data are generated from a Gaussian, linear structural equation model. It\nconsists of two parts: (1) introduce a permutation matrix as a new parameter\nwithin a regularized Gaussian log-likelihood to represent variable ordering;\nand (2) given the ordering, estimate the DAG structure through sparse Cholesky\nfactor of the inverse covariance matrix. For permutation matrix estimation, we\npropose a relaxation technique that avoids the NP-hard combinatorial problem of\norder estimation. Given an ordering, a sparse Cholesky factor is estimated\nusing a cyclic coordinatewise descent algorithm which decouples row-wise. Our\nframework recovers DAGs without the need for an expensive verification of the\nacyclicity constraint or enumeration of possible parent sets. We establish\nnumerical convergence of the algorithm, and consistency of the Cholesky factor\nestimator when the order of variables is known. Through several simulated and\nmacro-economic datasets, we study the scope and performance of the proposed\nmethodology.",
          "link": "http://arxiv.org/abs/2107.01658",
          "publishedOn": "2021-07-06T01:58:10.527Z",
          "wordCount": 589,
          "title": "Learning Bayesian Networks through Birkhoff Polytope: A Relaxation Method. (arXiv:2107.01658v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2009.03671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1\">Haocong Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bin Hu</a>",
          "description": "Person re-identification (Re-ID) via gait features within 3D skeleton\nsequences is a newly-emerging topic with several advantages. Existing solutions\neither rely on hand-crafted descriptors or supervised gait representation\nlearning. This paper proposes a self-supervised gait encoding approach that can\nleverage unlabeled skeleton data to learn gait representations for person\nRe-ID. Specifically, we first create self-supervision by learning to\nreconstruct unlabeled skeleton sequences reversely, which involves richer\nhigh-level semantics to obtain better gait representations. Other pretext tasks\nare also explored to further improve self-supervised learning. Second, inspired\nby the fact that motion's continuity endows adjacent skeletons in one skeleton\nsequence and temporally consecutive skeleton sequences with higher correlations\n(referred as locality in 3D skeleton data), we propose a locality-aware\nattention mechanism and a locality-aware contrastive learning scheme, which aim\nto preserve locality-awareness on intra-sequence level and inter-sequence level\nrespectively during self-supervised learning. Last, with context vectors\nlearned by our locality-aware attention mechanism and contrastive learning\nscheme, a novel feature named Constrastive Attention-based Gait Encodings\n(CAGEs) is designed to represent gait effectively. Empirical evaluations show\nthat our approach significantly outperforms skeleton-based counterparts by\n15-40% Rank-1 accuracy, and it even achieves superior performance to numerous\nmulti-modal methods with extra RGB or depth information. Our codes are\navailable at https://github.com/Kali-Hac/Locality-Awareness-SGE.",
          "link": "http://arxiv.org/abs/2009.03671",
          "publishedOn": "2021-07-06T01:58:10.516Z",
          "wordCount": 742,
          "title": "A Self-Supervised Gait Encoding Approach with Locality-Awareness for 3D Skeleton Based Person Re-Identification. (arXiv:2009.03671v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1\">Arnu Pretorius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1\">Kale-ab Tessera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smit_A/0/1/0/all/0/1\">Andries P. Smit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Formanek_C/0/1/0/all/0/1\">Claude Formanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimbly_S/0/1/0/all/0/1\">St John Grimbly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eloff_K/0/1/0/all/0/1\">Kevin Eloff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danisa_S/0/1/0/all/0/1\">Siphelele Danisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_L/0/1/0/all/0/1\">Lawrence Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shock_J/0/1/0/all/0/1\">Jonathan Shock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brink_W/0/1/0/all/0/1\">Willie Brink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelbrecht_H/0/1/0/all/0/1\">Herman Engelbrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laterre_A/0/1/0/all/0/1\">Alexandre Laterre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beguir_K/0/1/0/all/0/1\">Karim Beguir</a>",
          "description": "Breakthrough advances in reinforcement learning (RL) research have led to a\nsurge in the development and application of RL. To support the field and its\nrapid growth, several frameworks have emerged that aim to help the community\nmore easily build effective and scalable agents. However, very few of these\nframeworks exclusively support multi-agent RL (MARL), an increasingly active\nfield in itself, concerned with decentralised decision-making problems. In this\nwork, we attempt to fill this gap by presenting Mava: a research framework\nspecifically designed for building scalable MARL systems. Mava provides useful\ncomponents, abstractions, utilities and tools for MARL and allows for simple\nscaling for multi-process system training and execution, while providing a high\nlevel of flexibility and composability. Mava is built on top of DeepMind's Acme\n\\citep{hoffman2020acme}, and therefore integrates with, and greatly benefits\nfrom, a wide range of already existing single-agent RL components made\navailable in Acme. Several MARL baseline systems have already been implemented\nin Mava. These implementations serve as examples showcasing Mava's reusable\nfeatures, such as interchangeable system architectures, communication and\nmixing modules. Furthermore, these implementations allow existing MARL\nalgorithms to be easily reproduced and extended. We provide experimental\nresults for these implementations on a wide range of multi-agent environments\nand highlight the benefits of distributed system training.",
          "link": "http://arxiv.org/abs/2107.01460",
          "publishedOn": "2021-07-06T01:58:10.500Z",
          "wordCount": 673,
          "title": "Mava: a research framework for distributed multi-agent reinforcement learning. (arXiv:2107.01460v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wanyun Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Sen Yan</a>",
          "description": "Knowledge distillation uses both real hard labels and soft labels predicted\nby teacher models as supervision. Intuitively, we expect the soft labels and\nhard labels to be concordant w.r.t. their orders of probabilities. However, we\nfound {\\it critical order violations} between hard labels and soft labels in\naugmented samples. For example, for an augmented sample $x=0.7*panda+0.3*cat$,\nwe expect the order of meaningful soft labels to be\n$P_\\text{soft}(panda|x)>P_\\text{soft}(cat|x)>P_\\text{soft}(other|x)$. But real\nsoft labels usually violate the order, e.g.\n$P_\\text{soft}(tiger|x)>P_\\text{soft}(panda|x)>P_\\text{soft}(cat|x)$. We\nattribute this to the unsatisfactory generalization ability of the teacher,\nwhich leads to the prediction error of augmented samples. Empirically, we found\nthe violations are common and injure the knowledge transfer.In this paper, we\nintroduce order restrictions to data augmentation for knowledge distillation,\nwhich is denoted as isotonic data augmentation (IDA). We use isotonic\nregression (IR) -- a classic technique from statistics -- to eliminate the\norder violations. We show that IDA can be modeled as a tree-structured IR\nproblem. We thereby adapt the classical IRT-BIN algorithm for optimal solutions\nwith $O(c \\log c)$ time complexity, where $c$ is the number of labels. In order\nto further reduce the time complexity, we also \\cwy{propose} a GPU-friendly\napproximation with linear time complexity. We have verified on variant datasets\nand data augmentation techniques that our proposed IDA algorithms effectively\nincreases the accuracy of knowledge distillation by eliminating the rank\nviolations.",
          "link": "http://arxiv.org/abs/2107.01412",
          "publishedOn": "2021-07-06T01:58:10.440Z",
          "wordCount": 656,
          "title": "Isotonic Data Augmentation for Knowledge Distillation. (arXiv:2107.01412v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Arshad Shaikh</a>",
          "description": "Session-based recommendation systems suggest relevant items to users by\nmodeling user behavior and preferences using short-term anonymous sessions.\nExisting methods leverage Graph Neural Networks (GNNs) that propagate and\naggregate information from neighboring nodes i.e., local message passing. Such\ngraph-based architectures have representational limits, as a single sub-graph\nis susceptible to overfit the sequential dependencies instead of accounting for\ncomplex transitions between items in different sessions. We propose using a\nTransformer in combination with a target attentive GNN, which allows richer\nRepresentation Learning. Our experimental results and ablation show that our\nproposed method outperforms the existing methods on real-world benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2107.01516",
          "publishedOn": "2021-07-06T01:58:10.395Z",
          "wordCount": 540,
          "title": "Improved Representation Learning for Session-based Recommendation. (arXiv:2107.01516v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01863",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Mesuga_R/0/1/0/all/0/1\">Reymond Mesuga</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Bayanay_B/0/1/0/all/0/1\">Brian James Bayanay</a>",
          "description": "LIGO is considered the most sensitive and complicated gravitational\nexperiment ever built. Its main objective is to detect the gravitational wave\nfrom the strongest events in the universe by observing if the length of its\n4-kilometer arms change by a distance 10,000 times smaller than the diameter of\na proton. Due to its sensitivity, LIGO is prone to the disturbance of external\nnoises which affects the data being collected to detect the gravitational wave.\nThese noises are commonly called by the LIGO community as glitches. The\nobjective of this study is to evaluate the effeciency of various deep trasnfer\nlearning models namely VGG19, ResNet50V2, VGG16 and ResNet101 to detect glitch\nwaveform in gravitational wave data. The accuracy achieved by the said models\nare 98.98%, 98.35%, 97.56% and 94.73% respectively. Even though the models\nachieved fairly high accuracy, it is observed that all of the model suffered\nfrom the lack of data for certain classes which is the main concern found in\nthe experiment",
          "link": "http://arxiv.org/abs/2107.01863",
          "publishedOn": "2021-07-06T01:58:10.377Z",
          "wordCount": 633,
          "title": "On the Efficiency of Various Deep Transfer Learning Models in Glitch Waveform Detection in Gravitational-Wave Data. (arXiv:2107.01863v1 [gr-qc])"
        },
        {
          "id": "http://arxiv.org/abs/1602.03822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "We use random geometric graphs to describe clusters of higher dimensional\ndata points which are bijectively mapped to a (possibly) lower dimensional\nspace where an equivalent random cluster model is used to calculate the\nexpected number of modes to be found when separating the data of a multi-modal\ndata set into distinct clusters. Furthermore, as a function of the expected\nnumber of modes and the number of data points in the sample, an upper bound on\na given distance measure is found such that data points have the greatest\ncorrelation if their mutual distances from a common center is less than or\nequal to the calculated bound. Anomalies are exposed, which lie outside of the\nunion of all regularized clusters of data points. Finally, similarly to finding\na hyperplane which can be shifted along its normal to expose the maximal\ndistance between binary classes, it is shown that the union of regularized\nclusters can be used to define a hyperplane which can be shifted by a certain\namount to separate the data into binary classes.",
          "link": "http://arxiv.org/abs/1602.03822",
          "publishedOn": "2021-07-06T01:58:10.370Z",
          "wordCount": 725,
          "title": "A Critical Connectivity Radius for Randomly-Generated, High Dimensional Data Points. (arXiv:1602.03822v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01269",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Moritz_N/0/1/0/all/0/1\">Niko Moritz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hori_T/0/1/0/all/0/1\">Takaaki Hori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>",
          "description": "Attention-based end-to-end automatic speech recognition (ASR) systems have\nrecently demonstrated state-of-the-art results for numerous tasks. However, the\napplication of self-attention and attention-based encoder-decoder models\nremains challenging for streaming ASR, where each word must be recognized\nshortly after it was spoken. In this work, we present the dual\ncausal/non-causal self-attention (DCN) architecture, which in contrast to\nrestricted self-attention prevents the overall context to grow beyond the\nlook-ahead of a single layer when used in a deep architecture. DCN is compared\nto chunk-based and restricted self-attention using streaming transformer and\nconformer architectures, showing improved ASR performance over restricted\nself-attention and competitive ASR results compared to chunk-based\nself-attention, while providing the advantage of frame-synchronous processing.\nCombined with triggered attention, the proposed streaming end-to-end ASR\nsystems obtained state-of-the-art results on the LibriSpeech, HKUST, and\nSwitchboard ASR tasks.",
          "link": "http://arxiv.org/abs/2107.01269",
          "publishedOn": "2021-07-06T01:58:10.351Z",
          "wordCount": 582,
          "title": "Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech Recognition. (arXiv:2107.01269v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tupadha_L/0/1/0/all/0/1\">Lolitha Sresta Tupadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "Malware evolves over time and antivirus must adapt to such evolution. Hence,\nit is critical to detect those points in time where malware has evolved so that\nappropriate countermeasures can be undertaken. In this research, we perform a\nvariety of experiments on a significant number of malware families to determine\nwhen malware evolution is likely to have occurred. All of the evolution\ndetection techniques that we consider are based on machine learning and can be\nfully automated -- in particular, no reverse engineering or other\nlabor-intensive manual analysis is required. Specifically, we consider analysis\nbased on hidden Markov models (HMM) and the word embedding techniques HMM2Vec\nand Word2Vec.",
          "link": "http://arxiv.org/abs/2107.01627",
          "publishedOn": "2021-07-06T01:58:10.345Z",
          "wordCount": 536,
          "title": "Machine Learning for Malware Evolution Detection. (arXiv:2107.01627v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Rong Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>",
          "description": "While over-parameterization is widely believed to be crucial for the success\nof optimization for the neural networks, most existing theories on\nover-parameterization do not fully explain the reason -- they either work in\nthe Neural Tangent Kernel regime where neurons don't move much, or require an\nenormous number of neurons. In practice, when the data is generated using a\nteacher neural network, even mildly over-parameterized neural networks can\nachieve 0 loss and recover the directions of teacher neurons. In this paper we\ndevelop a local convergence theory for mildly over-parameterized two-layer\nneural net. We show that as long as the loss is already lower than a threshold\n(polynomial in relevant parameters), all student neurons in an\nover-parameterized two-layer neural network will converge to one of teacher\nneurons, and the loss will go to 0. Our result holds for any number of student\nneurons as long as it is at least as large as the number of teacher neurons,\nand our convergence rate is independent of the number of student neurons. A key\ncomponent of our analysis is the new characterization of local optimization\nlandscape -- we show the gradient satisfies a special case of Lojasiewicz\nproperty which is different from local strong convexity or PL conditions used\nin previous work.",
          "link": "http://arxiv.org/abs/2102.02410",
          "publishedOn": "2021-07-06T01:58:10.329Z",
          "wordCount": 684,
          "title": "A Local Convergence Theory for Mildly Over-Parameterized Two-Layer Neural Network. (arXiv:2102.02410v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingda Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chen Lv</a>",
          "description": "To further improve the learning efficiency and performance of reinforcement\nlearning (RL), in this paper we propose a novel uncertainty-aware model-based\nRL (UA-MBRL) framework, and then implement and validate it in autonomous\ndriving under various task scenarios. First, an action-conditioned ensemble\nmodel with the ability of uncertainty assessment is established as the virtual\nenvironment model. Then, a novel uncertainty-aware model-based RL framework is\ndeveloped based on the adaptive truncation approach, providing virtual\ninteractions between the agent and environment model, and improving RL's\ntraining efficiency and performance. The developed algorithms are then\nimplemented in end-to-end autonomous vehicle control tasks, validated and\ncompared with state-of-the-art methods under various driving scenarios. The\nvalidation results suggest that the proposed UA-MBRL method surpasses the\nexisting model-based and model-free RL approaches, in terms of learning\nefficiency and achieved performance. The results also demonstrate the good\nability of the proposed method with respect to the adaptiveness and robustness,\nunder various autonomous driving scenarios.",
          "link": "http://arxiv.org/abs/2106.12194",
          "publishedOn": "2021-07-06T01:58:10.234Z",
          "wordCount": null,
          "title": "Uncertainty-Aware Model-Based Reinforcement Learning with Application to Autonomous Driving. (arXiv:2106.12194v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1\">Lirong Xia</a>",
          "description": "Differential privacy (DP) is a widely-accepted and widely-applied notion of\nprivacy based on worst-case analysis. Often, DP classifies most mechanisms\nwithout external noise as non-private [Dwork et al., 2014], and external\nnoises, such as Gaussian noise or Laplacian noise [Dwork et al., 2006], are\nintroduced to improve privacy. In many real-world applications, however, adding\nexternal noise is undesirable and sometimes prohibited. For example,\npresidential elections often require a deterministic rule to be used [Liu et\nal., 2020], and small noises can lead to dramatic decreases in the prediction\naccuracy of deep neural networks, especially the underrepresented classes\n[Bagdasaryan et al., 2019].\n\nIn this paper, we propose a natural extension and relaxation of DP following\nthe worst average-case idea behind the celebrated smoothed analysis [Spielman\nand Teng, 2004]. Our notion, the smoothed DP, can effectively measure the\nprivacy leakage of mechanisms without external noises under realistic settings.\n\nWe prove several strong properties of the smoothed DP, including\ncomposability, robustness to post-processing and etc. We proved that any\ndiscrete mechanism with sampling procedures is more private than what DP\npredicts. In comparison, many continuous mechanisms with sampling procedures\nare still non-private under smoothed DP. Experimentally, we first verified that\nthe discrete sampling mechanisms are private in real-world elections. Then, we\napply the smoothed DP notion on quantized gradient descent, which indicates\nsome neural networks can be private without adding any extra noises. We believe\nthat these results contribute to the theoretical foundation of realistic\nprivacy measures beyond worst-case analysis.",
          "link": "http://arxiv.org/abs/2107.01559",
          "publishedOn": "2021-07-06T01:58:10.224Z",
          "wordCount": null,
          "title": "Smoothed Differential Privacy. (arXiv:2107.01559v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murayama_T/0/1/0/all/0/1\">Taichi Murayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wakamiya_S/0/1/0/all/0/1\">Shoko Wakamiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aramaki_E/0/1/0/all/0/1\">Eiji Aramaki</a>",
          "description": "The accurate forecasting of infectious epidemic diseases such as influenza is\na crucial task undertaken by medical institutions. Although numerous flu\nforecasting methods and models based mainly on historical flu activity data and\nonline user-generated contents have been proposed in previous studies, no flu\nforecasting model targeting multiple countries using two types of data exists\nat present. Our paper leverages multi-task learning to tackle the challenge of\nbuilding one flu forecasting model targeting multiple countries; each country\nas each task. Also, to develop the flu prediction model with higher\nperformance, we solved two issues; finding suitable search queries, which are\npart of the user-generated contents, and how to leverage search queries\nefficiently in the model creation. For the first issue, we propose the transfer\napproaches from English to other languages. For the second issue, we propose a\nnovel flu forecasting model that takes advantage of search queries using an\nattention mechanism and extend the model to a multi-task model for multiple\ncountries' flu forecasts. Experiments on forecasting flu epidemics in five\ncountries demonstrate that our model significantly improved the performance by\nleveraging the search queries and multi-task learning compared to the\nbaselines.",
          "link": "http://arxiv.org/abs/2107.01760",
          "publishedOn": "2021-07-06T01:58:10.224Z",
          "wordCount": null,
          "title": "Single Model for Influenza Forecasting of Multiple Countries by Multi-task Learning. (arXiv:2107.01760v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12807",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Withnell_E/0/1/0/all/0/1\">Eloise Withnell</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sun_K/0/1/0/all/0/1\">Kai Sun</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>",
          "description": "The lack of explainability is one of the most prominent disadvantages of deep\nlearning applications in omics. This \"black box\" problem can undermine the\ncredibility and limit the practical implementation of biomedical deep learning\nmodels. Here we present XOmiVAE, a variational autoencoder (VAE) based\ninterpretable deep learning model for cancer classification using\nhigh-dimensional omics data. XOmiVAE is capable of revealing the contribution\nof each gene and latent dimension for each classification prediction, and the\ncorrelation between each gene and each latent dimension. It is also\ndemonstrated that XOmiVAE can explain not only the supervised classification\nbut the unsupervised clustering results from the deep learning network. To the\nbest of our knowledge, XOmiVAE is one of the first activation level-based\ninterpretable deep learning models explaining novel clusters generated by VAE.\nThe explainable results generated by XOmiVAE were validated by both the\nperformance of downstream tasks and the biomedical knowledge. In our\nexperiments, XOmiVAE explanations of deep learning based cancer classification\nand clustering aligned with current domain knowledge including biological\nannotation and academic literature, which shows great potential for novel\nbiomedical knowledge discovery from deep learning models.",
          "link": "http://arxiv.org/abs/2105.12807",
          "publishedOn": "2021-07-06T01:58:10.208Z",
          "wordCount": 661,
          "title": "XOmiVAE: an interpretable deep learning model for cancer classification using high-dimensional omics data. (arXiv:2105.12807v2 [q-bio.GN] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Hoang Phan</a>",
          "description": "This paper proposed a new methodology for machine learning in 2-dimensional\nspace (2-D ML) in inline coordinates. It is a full machine learning approach\nthat does not require to deal with n-dimensional data in n-dimensional space.\nIt allows discovering n-D patterns in 2-D space without loss of n-D information\nusing graph representations of n-D data in 2-D. Specifically, it can be done\nwith the inline based coordinates in different modifications, including static\nand dynamic ones. The classification and regression algorithms based on these\ninline coordinates were introduced. A successful case study based on a\nbenchmark data demonstrated the feasibility of the approach. This approach\nhelps to consolidate further a whole new area of full 2-D machine learning as a\npromising ML methodology. It has advantages of abilities to involve actively\nthe end-users into the discovering of models and their justification. Another\nadvantage is providing interpretable ML models.",
          "link": "http://arxiv.org/abs/2106.07568",
          "publishedOn": "2021-07-06T01:58:10.197Z",
          "wordCount": 612,
          "title": "Full interpretable machine learning in 2D with inline coordinates. (arXiv:2106.07568v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lorbeer_B/0/1/0/all/0/1\">Boris Lorbeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botler_M/0/1/0/all/0/1\">Max Botler</a>",
          "description": "In this paper, we propose POTATOES (Partitioning OverfiTting AuTOencoder\nEnSemble), a new method for unsupervised outlier detection (UOD). More\nprecisely, given any autoencoder for UOD, this technique can be used to improve\nits accuracy while at the same time removing the burden of tuning its\nregularization. The idea is to not regularize at all, but to rather randomly\npartition the data into sufficiently many equally sized parts, overfit each\npart with its own autoencoder, and to use the maximum over all autoencoder\nreconstruction errors as the anomaly score. We apply our model to various\nrealistic datasets and show that if the set of inliers is dense enough, our\nmethod indeed improves the UOD performance of a given autoencoder\nsignificantly. For reproducibility, the code is made available on github so the\nreader can recreate the results in this paper as well as apply the method to\nother autoencoders and datasets.",
          "link": "http://arxiv.org/abs/2009.02755",
          "publishedOn": "2021-07-06T01:58:10.174Z",
          "wordCount": 647,
          "title": "Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles. (arXiv:2009.02755v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shulman_Y/0/1/0/all/0/1\">Yaniv Shulman</a>",
          "description": "Quantization based model compression serves as high performing and fast\napproach for inference that yields highly compressed models compared to their\nfull-precision floating point counterparts. The most extreme quantization is a\n1-bit representation of parameters such that they have only two possible\nvalues, typically -1(0) or +1. Models that constrain the weights to binary\nvalues enable efficient implementation of the ubiquitous dot product by\nadditions only without requiring floating point multiplications which is\nbeneficial for resources constrained inference. The main contribution of this\nwork is the introduction of a method to smooth the combinatorial problem of\ndetermining a binary vector of weights to minimize the expected loss for a\ngiven objective by means of empirical risk minimization with backpropagation.\nThis is achieved by approximating a multivariate binary state over the weights\nutilizing a deterministic and differentiable transformation of real-valued\ncontinuous parameters. The proposed method adds little overhead in training,\ncan be readily applied without any substantial modifications to the original\narchitecture, does not introduce additional saturating non-linearities or\nauxiliary losses, and does not prohibit applying other methods for binarizing\nthe activations. It is demonstrated that contrary to common assertions made in\nthe literature, binary weighted networks can train well with the same standard\noptimization techniques and similar hyperparameters settings as their\nfull-precision counterparts, namely momentum SGD with large learning rates and\n$L_2$ regularization. The source code is publicly available at\nhttps://bitbucket.org/YanivShu/binary_weighted_networks_public",
          "link": "http://arxiv.org/abs/2107.01400",
          "publishedOn": "2021-07-06T01:58:10.168Z",
          "wordCount": 666,
          "title": "Exact Backpropagation in Binary Weighted Networks with Group Weight Transformations. (arXiv:2107.01400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_A/0/1/0/all/0/1\">Adrien Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esling_P/0/1/0/all/0/1\">Philippe Esling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>",
          "description": "Granular sound synthesis is a popular audio generation technique based on\nrearranging sequences of small waveform windows. In order to control the\nsynthesis, all grains in a given corpus are analyzed through a set of acoustic\ndescriptors. This provides a representation reflecting some form of local\nsimilarities across the grains. However, the quality of this grain space is\nbound by that of the descriptors. Its traversal is not continuously invertible\nto signal and does not render any structured temporality.\n\nWe demonstrate that generative neural networks can implement granular\nsynthesis while alleviating most of its shortcomings. We efficiently replace\nits audio descriptor basis by a probabilistic latent space learned with a\nVariational Auto-Encoder. In this setting the learned grain space is\ninvertible, meaning that we can continuously synthesize sound when traversing\nits dimensions. It also implies that original grains are not stored for\nsynthesis. Another major advantage of our approach is to learn structured paths\ninside this latent space by training a higher-level temporal embedding over\narranged grain sequences.\n\nThe model can be applied to many types of libraries, including pitched notes\nor unpitched drums and environmental noises. We report experiments on the\ncommon granular synthesis processes as well as novel ones such as conditional\nsampling and morphing.",
          "link": "http://arxiv.org/abs/2008.01393",
          "publishedOn": "2021-07-06T01:58:10.161Z",
          "wordCount": 680,
          "title": "Neural Granular Sound Synthesis. (arXiv:2008.01393v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaabouni_R/0/1/0/all/0/1\">Rahma Chaabouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>",
          "description": "Despite their practical success, modern seq2seq architectures are unable to\ngeneralize systematically on several SCAN tasks. Hence, it is not clear if\nSCAN-style compositional generalization is useful in realistic NLP tasks. In\nthis work, we study the benefit that such compositionality brings about to\nseveral machine translation tasks. We present several focused modifications of\nTransformer that greatly improve generalization capabilities on SCAN and select\none that remains on par with a vanilla Transformer on a standard machine\ntranslation (MT) task. Next, we study its performance in low-resource settings\nand on a newly introduced distribution-shifted English-French translation task.\nOverall, we find that improvements of a SCAN-capable model do not directly\ntransfer to the resource-rich MT setup. In contrast, in the low-resource setup,\ngeneral modifications lead to an improvement of up to 13.1% BLEU score w.r.t. a\nvanilla Transformer. Similarly, an improvement of 14% in an accuracy-based\nmetric is achieved in the introduced compositional English-French translation\ntask. This provides experimental evidence that the compositional generalization\nassessed in SCAN is particularly useful in resource-starved and domain-shifted\nscenarios.",
          "link": "http://arxiv.org/abs/2107.01366",
          "publishedOn": "2021-07-06T01:58:10.154Z",
          "wordCount": 621,
          "title": "Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN. (arXiv:2107.01366v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kerroumi_M/0/1/0/all/0/1\">Mohamed Kerroumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayem_O/0/1/0/all/0/1\">Othmane Sayem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabou_A/0/1/0/all/0/1\">Aymen Shabou</a>",
          "description": "We introduce a novel approach for scanned document representation to perform\nfield extraction. It allows the simultaneous encoding of the textual, visual\nand layout information in a 3-axis tensor used as an input to a segmentation\nmodel. We improve the recent Chargrid and Wordgrid \\cite{chargrid} models in\nseveral ways, first by taking into account the visual modality, then by\nboosting its robustness in regards to small datasets while keeping the\ninference time low. Our approach is tested on public and private document-image\ndatasets, showing higher performances compared to the recent state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2010.02358",
          "publishedOn": "2021-07-06T01:58:10.147Z",
          "wordCount": 585,
          "title": "VisualWordGrid: Information Extraction From Scanned Documents Using A Multimodal Approach. (arXiv:2010.02358v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sandeep Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowdur_J/0/1/0/all/0/1\">Jaya Shradha Fowdur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gawlikowski_J/0/1/0/all/0/1\">Jakob Gawlikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medina_D/0/1/0/all/0/1\">Daniel Medina</a>",
          "description": "Understanding and representing traffic patterns are key to detecting\nanomalies in the maritime domain. To this end, we propose a novel graph-based\ntraffic representation and association scheme to cluster trajectories of\nvessels using automatic identification system (AIS) data. We utilize the\n(un)clustered data to train a recurrent neural network (RNN)-based evidential\nregression model, which can predict a vessel's trajectory at future timesteps\nwith its corresponding prediction uncertainty. This paper proposes the usage of\na deep learning (DL)-based uncertainty estimation in detecting maritime\nanomalies, such as unusual vessel maneuvering. Furthermore, we utilize the\nevidential deep learning classifiers to detect unusual turns of vessels and the\nloss of AIS signal using predicted class probabilities with associated\nuncertainties. Our experimental results suggest that using graph-based\nclustered data improves the ability of the DL models to learn the\ntemporal-spatial correlation of data and associated uncertainties. Using\ndifferent AIS datasets and experiments, we demonstrate that the estimated\nprediction uncertainty yields fundamental information for the detection of\ntraffic anomalies in the maritime and, possibly in other domains.",
          "link": "http://arxiv.org/abs/2107.01557",
          "publishedOn": "2021-07-06T01:58:10.129Z",
          "wordCount": 619,
          "title": "Leveraging Evidential Deep Learning Uncertainties with Graph-based Clustering to Detect Anomalies. (arXiv:2107.01557v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Luonan Chen</a>",
          "description": "Making predictions in a robust way is not easy for nonlinear systems. In this\nwork, a neural network computing framework, i.e., a spatiotemporal\nconvolutional network (STCN), was developed to efficiently and accurately\nrender a multistep-ahead prediction of a time series by employing a\nspatial-temporal information (STI) transformation. The STCN combines the\nadvantages of both the temporal convolutional network (TCN) and the STI\nequation, which maps the high-dimensional/spatial data to the future temporal\nvalues of a target variable, thus naturally providing the prediction of the\ntarget variable. From the observed variables, the STCN also infers the causal\nfactors of the target variable in the sense of Granger causality, which are in\nturn selected as effective spatial information to improve the prediction\nrobustness. The STCN was successfully applied to both benchmark systems and\nreal-world datasets, all of which show superior and robust performance in\nmultistep-ahead prediction, even when the data were perturbed by noise. From\nboth theoretical and computational viewpoints, the STCN has great potential in\npractical applications in artificial intelligence (AI) or machine learning\nfields as a model-free method based only on the observed data, and also opens a\nnew way to explore the observed high-dimensional data in a dynamical manner for\nmachine learning.",
          "link": "http://arxiv.org/abs/2107.01353",
          "publishedOn": "2021-07-06T01:58:10.123Z",
          "wordCount": 651,
          "title": "Spatiotemporal convolutional network for time-series prediction and causal inference. (arXiv:2107.01353v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.07006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasirigerdeh_R/0/1/0/all/0/1\">Reza Nasirigerdeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakhtiari_M/0/1/0/all/0/1\">Mohammad Bakhtiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torkzadehmahani_R/0/1/0/all/0/1\">Reihaneh Torkzadehmahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayat_A/0/1/0/all/0/1\">Amirhossein Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+List_M/0/1/0/all/0/1\">Markus List</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blumenthal_D/0/1/0/all/0/1\">David B. Blumenthal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumbach_J/0/1/0/all/0/1\">Jan Baumbach</a>",
          "description": "Federated learning has faced performance and network communication\nchallenges, especially in the environments where the data is not independent\nand identically distributed (IID) across the clients. To address the former\nchallenge, we introduce the federated-centralized concordance property and show\nthat the federated single-mini-batch training approach can achieve comparable\nperformance as the corresponding centralized training in the Non-IID\nenvironments. To deal with the latter, we present the federated\nmulti-mini-batch approach and illustrate that it can establish a trade-off\nbetween the performance and communication efficiency and outperforms federated\naveraging in the Non-IID settings.",
          "link": "http://arxiv.org/abs/2011.07006",
          "publishedOn": "2021-07-06T01:58:10.115Z",
          "wordCount": 568,
          "title": "Federated Multi-Mini-Batch: An Efficient Training Approach to Federated Learning in Non-IID Environments. (arXiv:2011.07006v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhicheng An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dijun Luo</a>",
          "description": "Model-based deep reinforcement learning has achieved success in various\ndomains that require high sample efficiencies, such as Go and robotics.\nHowever, there are some remaining issues, such as planning efficient\nexplorations to learn more accurate dynamic models, evaluating the uncertainty\nof the learned models, and more rational utilization of models. To mitigate\nthese issues, we present MEEE, a model-ensemble method that consists of\noptimistic exploration and weighted exploitation. During exploration, unlike\nprior methods directly selecting the optimal action that maximizes the expected\naccumulative return, our agent first generates a set of action candidates and\nthen seeks out the optimal action that takes both expected return and future\nobservation novelty into account. During exploitation, different discounted\nweights are assigned to imagined transition tuples according to their model\nuncertainty respectively, which will prevent model predictive error propagation\nin agent training. Experiments on several challenging continuous control\nbenchmark tasks demonstrated that our approach outperforms other model-free and\nmodel-based state-of-the-art methods, especially in sample complexity.",
          "link": "http://arxiv.org/abs/2107.01825",
          "publishedOn": "2021-07-06T01:58:10.106Z",
          "wordCount": 617,
          "title": "Sample Efficient Reinforcement Learning via Model-Ensemble Exploration and Exploitation. (arXiv:2107.01825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.08603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nitesh_K/0/1/0/all/0/1\">Kumar Nitesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondrej_K/0/1/0/all/0/1\">Kuzelka Ondrej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luc_D/0/1/0/all/0/1\">De Raedt Luc</a>",
          "description": "Relational autocompletion is the problem of automatically filling out some\nmissing values in multi-relational data. We tackle this problem within the\nprobabilistic logic programming framework of Distributional Clauses (DC), which\nsupports both discrete and continuous probability distributions. Within this\nframework, we introduce DiceML { an approach to learn both the structure and\nthe parameters of DC programs from relational data (with possibly missing\ndata). To realize this, DiceML integrates statistical modeling and\ndistributional clauses with rule learning. The distinguishing features of\nDiceML are that it 1) tackles autocompletion in relational data, 2) learns\ndistributional clauses extended with statistical models, 3) deals with both\ndiscrete and continuous distributions, 4) can exploit background knowledge, and\n5) uses an expectation-maximization based algorithm to cope with missing data.\nThe empirical results show the promise of the approach, even when there is\nmissing data.",
          "link": "http://arxiv.org/abs/2001.08603",
          "publishedOn": "2021-07-06T01:58:10.094Z",
          "wordCount": 628,
          "title": "Learning Distributional Programs for Relational Autocompletion. (arXiv:2001.08603v5 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ailon_N/0/1/0/all/0/1\">Nir Ailon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibovich_O/0/1/0/all/0/1\">Omer Leibovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1\">Vineet Nair</a>",
          "description": "A butterfly network consists of logarithmically many layers, each with a\nlinear number of non-zero weights (pre-specified). The fast\nJohnson-Lindenstrauss transform (FJLT) can be represented as a butterfly\nnetwork followed by a projection onto a random subset of the coordinates.\nMoreover, a random matrix based on FJLT with high probability approximates the\naction of any matrix on a vector. Motivated by these facts, we propose to\nreplace a dense linear layer in any neural network by an architecture based on\nthe butterfly network. The proposed architecture significantly improves upon\nthe quadratic number of weights required in a standard dense layer to nearly\nlinear with little compromise in expressibility of the resulting operator. In a\ncollection of wide variety of experiments, including supervised prediction on\nboth the NLP and vision data, we show that this not only produces results that\nmatch and at times outperform existing well-known architectures, but it also\noffers faster training and prediction in deployment. To understand the\noptimization problems posed by neural networks with a butterfly network, we\nalso study the optimization landscape of the encoder-decoder network, where the\nencoder is replaced by a butterfly network followed by a dense linear layer in\nsmaller dimension. Theoretical result presented in the paper explains why the\ntraining speed and outcome are not compromised by our proposed approach.",
          "link": "http://arxiv.org/abs/2007.08864",
          "publishedOn": "2021-07-06T01:58:10.081Z",
          "wordCount": 689,
          "title": "Sparse Linear Networks with a Fixed Butterfly Structure: Theory and Practice. (arXiv:2007.08864v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foorthuis_R/0/1/0/all/0/1\">Ralph Foorthuis</a>",
          "description": "Anomalies are cases that are in some way unusual and do not appear to fit the\ngeneral patterns present in the dataset. Several conceptualizations exist to\ndistinguish between different types of anomalies. However, these are either too\nspecific to be generally applicable or so abstract that they neither provide\nconcrete insight into the nature of anomaly types nor facilitate the functional\nevaluation of anomaly detection algorithms. With the recent criticism on 'black\nbox' algorithms and analytics it has become clear that this is an undesirable\nsituation. This paper therefore introduces a general typology of anomalies that\noffers a clear and tangible definition of the different types of anomalies in\ndatasets. The typology also facilitates the evaluation of the functional\ncapabilities of anomaly detection algorithms and as a framework assists in\nanalyzing the conceptual levels of data, patterns and anomalies. Finally, it\nserves as an analytical tool for studying anomaly types from other typologies.",
          "link": "http://arxiv.org/abs/2107.01615",
          "publishedOn": "2021-07-06T01:58:10.060Z",
          "wordCount": 645,
          "title": "A Typology of Data Anomalies. (arXiv:2107.01615v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1902.09434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caselles_Dupre_H/0/1/0/all/0/1\">Hugo Caselles-Dupr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Ortiz_M/0/1/0/all/0/1\">Michael Garcia-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1\">David Filliat</a>",
          "description": "We consider the problem of building a state representation model for control,\nin a continual learning setting. As the environment changes, the aim is to\nefficiently compress the sensory state's information without losing past\nknowledge, and then use Reinforcement Learning on the resulting features for\nefficient policy learning. To this end, we propose S-TRIGGER, a general method\nfor Continual State Representation Learning applicable to Variational\nAuto-Encoders and its many variants. The method is based on Generative Replay,\ni.e. the use of generated samples to maintain past knowledge. It comes along\nwith a statistically sound method for environment change detection, which\nself-triggers the Generative Replay. Our experiments on VAEs show that\nS-TRIGGER learns state representations that allows fast and high-performing\nReinforcement Learning, while avoiding catastrophic forgetting. The resulting\nsystem is capable of autonomously learning new information without using past\ndata and with a bounded system size. Code for our experiments is attached in\nAppendix.",
          "link": "http://arxiv.org/abs/1902.09434",
          "publishedOn": "2021-07-06T01:58:10.045Z",
          "wordCount": 629,
          "title": "S-TRIGGER: Continual State Representation Learning via Self-Triggered Generative Replay. (arXiv:1902.09434v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pricope_T/0/1/0/all/0/1\">Tidor-Vlad Pricope</a>",
          "description": "Classifying hand-written digits and letters has taken a big leap with the\nintroduction of ConvNets. However, on very constrained hardware the time\nnecessary to train such models would be high. Our main contribution is twofold.\nFirst, we extensively test an end-to-end vanilla neural network (MLP) approach\nin pure numpy without any pre-processing or feature extraction done beforehand.\nSecond, we show that basic data mining operations can significantly improve the\nperformance of the models in terms of computational time, without sacrificing\nmuch accuracy. We illustrate our claims on a simpler variant of the Extended\nMNIST dataset, called Balanced EMNIST dataset. Our experiments show that,\nwithout any data mining, we get increased generalization performance when using\nmore hidden layers and regularization techniques, the best model achieving\n84.83% accuracy on a test dataset. Using dimensionality reduction done by PCA\nwe were able to increase that figure to 85.08% with only 10% of the original\nfeature space, reducing the memory size needed by 64%. Finally, adding methods\nto remove possibly harmful training samples like deviation from the mean helped\nus to still achieve over 84% test accuracy but with only 32.8% of the original\nmemory size for the training set. This compares favorably to the majority of\nliterature results obtained through similar architectures. Although this\napproach gets outshined by state-of-the-art models, it does scale to some\n(AlexNet, VGGNet) trained on 50% of the same dataset.",
          "link": "http://arxiv.org/abs/2107.01782",
          "publishedOn": "2021-07-06T01:58:10.013Z",
          "wordCount": 675,
          "title": "A contextual analysis of multi-layer perceptron models in classifying hand-written digits and letters: limited resources. (arXiv:2107.01782v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_R/0/1/0/all/0/1\">Ron Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maman_N/0/1/0/all/0/1\">Nadav Maman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Inderjeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momiyama_S/0/1/0/all/0/1\">Satoru Momiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1\">Asaf Shabtai</a>",
          "description": "Although cyberattacks on machine learning (ML) production systems can be\ndestructive, many industry practitioners are ill equipped, lacking tactical and\nstrategic tools that would allow them to analyze, detect, protect against, and\nrespond to cyberattacks targeting their ML-based systems. In this paper, we\ntake a significant step toward securing ML production systems by integrating\nthese systems and their vulnerabilities into cybersecurity risk assessment\nframeworks. Specifically, we performed a comprehensive threat analysis of ML\nproduction systems and developed an extension to the MulVAL attack graph\ngeneration and analysis framework to incorporate cyberattacks on ML production\nsystems. Using the proposed extension, security practitioners can apply attack\ngraph analysis methods in environments that include ML components, thus\nproviding security experts with a practical tool for evaluating the impact and\nquantifying the risk of a cyberattack targeting an ML production system.",
          "link": "http://arxiv.org/abs/2107.01806",
          "publishedOn": "2021-07-06T01:58:09.984Z",
          "wordCount": 589,
          "title": "A Framework for Evaluating the Cybersecurity Risk of Real World, Machine Learning Production Systems. (arXiv:2107.01806v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.08707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qureshi_A/0/1/0/all/0/1\">Ahmed H. Qureshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiangeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baig_A/0/1/0/all/0/1\">Asfiya Baig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_M/0/1/0/all/0/1\">Michael C. Yip</a>",
          "description": "Constrained motion planning is a challenging field of research, aiming for\ncomputationally efficient methods that can find a collision-free path on the\nconstraint manifolds between a given start and goal configuration. These\nplanning problems come up surprisingly frequently, such as in robot\nmanipulation for performing daily life assistive tasks. However, few solutions\nto constrained motion planning are available, and those that exist struggle\nwith high computational time complexity in finding a path solution on the\nmanifolds. To address this challenge, we present Constrained Motion Planning\nNetworks X (CoMPNetX). It is a neural planning approach, comprising a\nconditional deep neural generator and discriminator with neural gradients-based\nfast projection operator. We also introduce neural task and scene\nrepresentations conditioned on which the CoMPNetX generates implicit manifold\nconfigurations to turbo-charge any underlying classical planner such as\nSampling-based Motion Planning methods for quickly solving complex constrained\nplanning tasks. We show that our method finds path solutions with high success\nrates and lower computation times than state-of-the-art traditional\npath-finding tools on various challenging scenarios.",
          "link": "http://arxiv.org/abs/2010.08707",
          "publishedOn": "2021-07-06T01:58:09.968Z",
          "wordCount": null,
          "title": "Constrained Motion Planning Networks X. (arXiv:2010.08707v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiayi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hai Li</a>",
          "description": "Learning with graphs has attracted significant attention recently. Existing\nrepresentation learning methods on graphs have achieved state-of-the-art\nperformance on various graph-related tasks such as node classification, link\nprediction, etc. However, we observe that these methods could leak serious\nprivate information. For instance, one can accurately infer the links (or node\nidentity) in a graph from a node classifier (or link predictor) trained on the\nlearnt node representations by existing methods. To address the issue, we\npropose a privacy-preserving representation learning framework on graphs from\nthe \\emph{mutual information} perspective. Specifically, our framework includes\na primary learning task and a privacy protection task, and we consider node\nclassification and link prediction as the two tasks of interest. Our goal is to\nlearn node representations such that they can be used to achieve high\nperformance for the primary learning task, while obtaining performance for the\nprivacy protection task close to random guessing. We formally formulate our\ngoal via mutual information objectives. However, it is intractable to compute\nmutual information in practice. Then, we derive tractable variational bounds\nfor the mutual information terms, where each bound can be parameterized via a\nneural network. Next, we train these parameterized neural networks to\napproximate the true mutual information and learn privacy-preserving node\nrepresentations. We finally evaluate our framework on various graph datasets.",
          "link": "http://arxiv.org/abs/2107.01475",
          "publishedOn": "2021-07-06T01:58:09.963Z",
          "wordCount": null,
          "title": "Privacy-Preserving Representation Learning on Graphs: A Mutual Information Perspective. (arXiv:2107.01475v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10558",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Webber_R/0/1/0/all/0/1\">Robert J. Webber</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lindsey_M/0/1/0/all/0/1\">Michael Lindsey</a>",
          "description": "Variational Monte Carlo (VMC) is an approach for computing ground-state\nwavefunctions that has recently become more powerful due to the introduction of\nneural network-based wavefunction parametrizations. However, efficiently\ntraining neural wavefunctions to converge to an energy minimum remains a\ndifficult problem. In this work, we analyze optimization and sampling methods\nused in VMC and introduce alterations to improve their performance. First,\nbased on theoretical convergence analysis in a noiseless setting, we motivate a\nnew optimizer that we call the Rayleigh-Gauss-Newton method, which can improve\nupon gradient descent and natural gradient descent to achieve superlinear\nconvergence with little added computational cost. Second, in order to realize\nthis favorable comparison in the presence of stochastic noise, we analyze the\neffect of sampling error on VMC parameter updates and experimentally\ndemonstrate that it can be reduced by the parallel tempering method. In\nparticular, we demonstrate that RGN can be made robust to energy spikes that\noccur when new regions of configuration space become available to the sampler\nover the course of optimization. Finally, putting theory into practice, we\napply our enhanced optimization and sampling methods to the transverse-field\nIsing and XXZ models on large lattices, yielding ground-state energy estimates\nwith remarkably high accuracy after just 200-500 parameter updates.",
          "link": "http://arxiv.org/abs/2106.10558",
          "publishedOn": "2021-07-06T01:58:09.961Z",
          "wordCount": 671,
          "title": "Rayleigh-Gauss-Newton optimization with enhanced sampling for variational Monte Carlo. (arXiv:2106.10558v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1\">Hidetaka Kamigaito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Katsuhiko Hayashi</a>",
          "description": "In knowledge graph embedding, the theoretical relationship between the\nsoftmax cross-entropy and negative sampling loss functions has not been\ninvestigated. This makes it difficult to fairly compare the results of the two\ndifferent loss functions. We attempted to solve this problem by using the\nBregman divergence to provide a unified interpretation of the softmax\ncross-entropy and negative sampling loss functions. Under this interpretation,\nwe can derive theoretical findings for fair comparison. Experimental results on\nthe FB15k-237 and WN18RR datasets show that the theoretical findings are valid\nin practical settings.",
          "link": "http://arxiv.org/abs/2106.07250",
          "publishedOn": "2021-07-06T01:58:09.954Z",
          "wordCount": 570,
          "title": "Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding. (arXiv:2106.07250v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Sunny Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_P/0/1/0/all/0/1\">Pranav Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakuwal_I/0/1/0/all/0/1\">Ishan Pakuwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kafle_P/0/1/0/all/0/1\">Prabhakar Kafle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynch_J/0/1/0/all/0/1\">Jayson Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "Can a machine learn Machine Learning? This work trains a machine learning\nmodel to solve machine learning problems from a University undergraduate level\ncourse. We generate a new training set of questions and answers consisting of\ncourse exercises, homework, and quiz questions from MIT's 6.036 Introduction to\nMachine Learning course and train a machine learning model to answer these\nquestions. Our system demonstrates an overall accuracy of 96% for open-response\nquestions and 97% for multiple-choice questions, compared with MIT students'\naverage of 93%, achieving grade A performance in the course, all in real-time.\nQuestions cover all 12 topics taught in the course, excluding coding questions\nor questions with images. Topics include: (i) basic machine learning\nprinciples; (ii) perceptrons; (iii) feature extraction and selection; (iv)\nlogistic regression; (v) regression; (vi) neural networks; (vii) advanced\nneural networks; (viii) convolutional neural networks; (ix) recurrent neural\nnetworks; (x) state machines and MDPs; (xi) reinforcement learning; and (xii)\ndecision trees. Our system uses Transformer models within an encoder-decoder\narchitecture with graph and tree representations. An important aspect of our\napproach is a data-augmentation scheme for generating new example problems. We\nalso train a machine learning model to generate problem hints. Thus, our system\nautomatically generates new questions across topics, answers both open-response\nquestions and multiple-choice questions, classifies problems, and generates\nproblem hints, pushing the envelope of AI for STEM education.",
          "link": "http://arxiv.org/abs/2107.01238",
          "publishedOn": "2021-07-06T01:58:09.945Z",
          "wordCount": null,
          "title": "Solving Machine Learning Problems. (arXiv:2107.01238v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gan_S/0/1/0/all/0/1\">Shaoduo Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1\">Xiangru Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianbin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hongmei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengzhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianghong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tengxu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiawei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Binhang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Recently years have witnessed a growing list of systems for distributed\ndata-parallel training. Existing systems largely fit into two paradigms, i.e.,\nparameter server and MPI-style collective operations. On the algorithmic side,\nresearchers have proposed a wide range of techniques to lower the communication\nvia system relaxations: quantization, decentralization, and communication\ndelay. However, most, if not all, existing systems only rely on standard\nsynchronous and asynchronous stochastic gradient (SG) based optimization,\ntherefore, cannot take advantage of all possible optimizations that the machine\nlearning community has been developing recently. Given this emerging gap\nbetween the current landscapes of systems and theory, we build BAGUA, a\ncommunication framework whose design goal is to provide a system abstraction\nthat is both flexible and modular to support state-of-the-art system relaxation\ntechniques of distributed training. Powered by the new system design, BAGUA has\na great ability to implement and extend various state-of-the-art distributed\nlearning algorithms. In a production cluster with up to 16 machines (128 GPUs),\nBAGUA can outperform PyTorch-DDP, Horovod and BytePS in the end-to-end training\ntime by a significant margin (up to 1.95 times) across a diverse range of\ntasks. Moreover, we conduct a rigorous tradeoff exploration showing that\ndifferent algorithms and system relaxations achieve the best performance over\ndifferent network conditions.",
          "link": "http://arxiv.org/abs/2107.01499",
          "publishedOn": "2021-07-06T01:58:09.942Z",
          "wordCount": 664,
          "title": "BAGUA: Scaling up Distributed Learning with System Relaxations. (arXiv:2107.01499v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01214",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Miller_B/0/1/0/all/0/1\">Benjamin Kurt Miller</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cole_A/0/1/0/all/0/1\">Alex Cole</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Forre_P/0/1/0/all/0/1\">Patrick Forr&#xe9;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Louppe_G/0/1/0/all/0/1\">Gilles Louppe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Weniger_C/0/1/0/all/0/1\">Christoph Weniger</a>",
          "description": "Parametric stochastic simulators are ubiquitous in science, often featuring\nhigh-dimensional input parameters and/or an intractable likelihood. Performing\nBayesian parameter inference in this context can be challenging. We present a\nneural simulator-based inference algorithm which simultaneously offers\nsimulation efficiency and fast empirical posterior testability, which is unique\namong modern algorithms. Our approach is simulation efficient by simultaneously\nestimating low-dimensional marginal posteriors instead of the joint posterior\nand by proposing simulations targeted to an observation of interest via a prior\nsuitably truncated by an indicator function. Furthermore, by estimating a\nlocally amortized posterior our algorithm enables efficient empirical tests of\nthe robustness of the inference results. Such tests are important for\nsanity-checking inference in real-world applications, which do not feature a\nknown ground truth. We perform experiments on a marginalized version of the\nsimulation-based inference benchmark and two complex and narrow posteriors,\nhighlighting the simulator efficiency of our algorithm as well as the quality\nof the estimated marginal posteriors. Implementation on GitHub.",
          "link": "http://arxiv.org/abs/2107.01214",
          "publishedOn": "2021-07-06T01:58:09.899Z",
          "wordCount": null,
          "title": "Truncated Marginal Neural Ratio Estimation. (arXiv:2107.01214v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07279",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Iraji_M/0/1/0/all/0/1\">Mohammad Saber Iraji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanha_J/0/1/0/all/0/1\">Jafar Tanha</a>",
          "description": "The new Coronavirus is spreading rapidly, and it has taken the lives of many\npeople so far. The virus has destructive effects on the human lung, and early\ndetection is very important. Deep Convolution neural networks are such powerful\ntools in classifying images. Therefore, in this paper, a hybrid approach based\non a deep network is presented. Feature vectors were extracted by applying a\ndeep convolution neural network on the images, and useful features were\nselected by the binary differential meta-heuristic algorithm. These optimized\nfeatures were given to the SVM classifier. A database consisting of three\ncategories of images such as COVID-19, pneumonia, and healthy included in 1092\nX-ray samples was considered. The proposed method achieved an accuracy of\n99.43%, a sensitivity of 99.16%, and a specificity of 99.57%. Our results\ndemonstrate that the suggested approach is better than recent studies on\nCOVID-19 detection with X-ray images.",
          "link": "http://arxiv.org/abs/2104.07279",
          "publishedOn": "2021-07-06T01:58:09.898Z",
          "wordCount": 679,
          "title": "COVID-19 detection using deep convolutional neural networks and binary-differential-algorithm-based feature selection on X-ray images. (arXiv:2104.07279v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>",
          "description": "Data augmentation has recently seen increased interest in NLP due to more\nwork in low-resource domains, new tasks, and the popularity of large-scale\nneural networks that require large amounts of training data. Despite this\nrecent upsurge, this area is still relatively underexplored, perhaps due to the\nchallenges posed by the discrete nature of language data. In this paper, we\npresent a comprehensive and unifying survey of data augmentation for NLP by\nsummarizing the literature in a structured manner. We first introduce and\nmotivate data augmentation for NLP, and then discuss major methodologically\nrepresentative approaches. Next, we highlight techniques that are used for\npopular NLP applications and tasks. We conclude by outlining current challenges\nand directions for future research. Overall, our paper aims to clarify the\nlandscape of existing literature in data augmentation for NLP and motivate\nadditional work in this area. We also present a GitHub repository with a paper\nlist that will be continuously updated at\nhttps://github.com/styfeng/DataAug4NLP",
          "link": "http://arxiv.org/abs/2105.03075",
          "publishedOn": "2021-07-06T01:58:09.890Z",
          "wordCount": 672,
          "title": "A Survey of Data Augmentation Approaches for NLP. (arXiv:2105.03075v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drechsler_J/0/1/0/all/0/1\">Joerg Drechsler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globus_Harris_I/0/1/0/all/0/1\">Ira Globus-Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_A/0/1/0/all/0/1\">Audra McMillan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarathy_J/0/1/0/all/0/1\">Jayshree Sarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Adam Smith</a>",
          "description": "Differential privacy is a restriction on data processing algorithms that\nprovides strong confidentiality guarantees for individual records in the data.\nHowever, research on proper statistical inference, that is, research on\nproperly quantifying the uncertainty of the (noisy) sample estimate regarding\nthe true value in the population, is currently still limited. This paper\nproposes and evaluates several strategies to compute valid differentially\nprivate confidence intervals for the median. Instead of computing a\ndifferentially private point estimate and deriving its uncertainty, we directly\nestimate the interval bounds and discuss why this approach is superior if\nensuring privacy is important. We also illustrate that addressing both sources\nof uncertainty--the error from sampling and the error from protecting the\noutput--simultaneously should be preferred over simpler approaches that\nincorporate the uncertainty in a sequential fashion. We evaluate the\nperformance of the different algorithms under various parameter settings in\nextensive simulation studies and demonstrate how the findings could be applied\nin practical settings using data from the 1940 Decennial Census.",
          "link": "http://arxiv.org/abs/2106.10333",
          "publishedOn": "2021-07-06T01:58:09.883Z",
          "wordCount": 644,
          "title": "Non-parametric Differentially Private Confidence Intervals for the Median. (arXiv:2106.10333v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morrison_K/0/1/0/all/0/1\">Katelyn Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilby_B/0/1/0/all/0/1\">Benjamin Gilby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipchak_C/0/1/0/all/0/1\">Colton Lipchak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattioli_A/0/1/0/all/0/1\">Adam Mattioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>",
          "description": "Recently, vision transformers and MLP-based models have been developed in\norder to address some of the prevalent weaknesses in convolutional neural\nnetworks. Due to the novelty of transformers being used in this domain along\nwith the self-attention mechanism, it remains unclear to what degree these\narchitectures are robust to corruptions. Despite some works proposing that data\naugmentation remains essential for a model to be robust against corruptions, we\npropose to explore the impact that the architecture has on corruption\nrobustness. We find that vision transformer architectures are inherently more\nrobust to corruptions than the ResNet-50 and MLP-Mixers. We also find that\nvision transformers with 5 times fewer parameters than a ResNet-50 have more\nshape bias. Our code is available to reproduce.",
          "link": "http://arxiv.org/abs/2106.13122",
          "publishedOn": "2021-07-06T01:58:09.807Z",
          "wordCount": 619,
          "title": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers. (arXiv:2106.13122v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zehao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiayi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "Domain generalization is challenging due to the domain shift and the\nuncertainty caused by the inaccessibility of target domain data. In this paper,\nwe address both challenges with a probabilistic framework based on variational\nBayesian inference, by incorporating uncertainty into neural network weights.\nWe couple domain invariance in a probabilistic formula with the variational\nBayesian inference. This enables us to explore domain-invariant learning in a\nprincipled way. Specifically, we derive domain-invariant representations and\nclassifiers, which are jointly established in a two-layer Bayesian neural\nnetwork. We empirically demonstrate the effectiveness of our proposal on four\nwidely used cross-domain visual recognition benchmarks. Ablation studies\nvalidate the synergistic benefits of our Bayesian treatment when jointly\nlearning domain-invariant representations and classifiers for domain\ngeneralization. Further, our method consistently delivers state-of-the-art mean\naccuracy on all benchmarks.",
          "link": "http://arxiv.org/abs/2105.04030",
          "publishedOn": "2021-07-06T01:58:09.785Z",
          "wordCount": 600,
          "title": "A Bit More Bayesian: Domain-Invariant Learning with Uncertainty. (arXiv:2105.04030v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1\">Isaac J. Sledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryner_D/0/1/0/all/0/1\">Darshan W. Bryner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1\">Jose C. Principe</a>",
          "description": "Reinforcement learning in large-scale environments is challenging due to the\nmany possible actions that can be taken in specific situations. We have\npreviously developed a means of constraining, and hence speeding up, the search\nprocess through the use of motion primitives; motion primitives are sequences\nof pre-specified actions taken across a state series. As a byproduct of this\nwork, we have found that if the motion primitives' motions and actions are\nlabeled, then the search can be sped up further. Since motion primitives may\ninitially lack such details, we propose a theoretically viewpoint-insensitive\nand speed-insensitive means of automatically annotating the underlying motions\nand actions. We do this through a differential-geometric, spatio-temporal\nkinematics descriptor, which analyzes how the poses of entities in two motion\nsequences change over time. We use this descriptor in conjunction with a\nweighted-nearest-neighbor classifier to label the primitives using a limited\nset of training examples. In our experiments, we achieve high motion and action\nannotation rates for human-action-derived primitives with as few as one\ntraining sample. We also demonstrate that reinforcement learning using\naccurately labeled trajectories leads to high-performing policies more quickly\nthan standard reinforcement learning techniques. This is partly because motion\nprimitives encode prior domain knowledge and preempt the need to re-discover\nthat knowledge during training. It is also because agents can leverage the\nlabels to systematically ignore action classes that do not facilitate task\nobjectives, thereby reducing the action space.",
          "link": "http://arxiv.org/abs/2102.12017",
          "publishedOn": "2021-07-06T01:58:09.779Z",
          "wordCount": 725,
          "title": "Annotating Motion Primitives for Simplifying Action Search in Reinforcement Learning. (arXiv:2102.12017v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.08081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>",
          "description": "In sequence-to-sequence learning, the decoder relies on the attention\nmechanism to efficiently extract information from the encoder. While it is\ncommon practice to draw information from only the last encoder layer, recent\nwork has proposed to use representations from different encoder layers for\ndiversified levels of information. Nonetheless, the decoder still obtains only\na single view of the source sequences, which might lead to insufficient\ntraining of the encoder layer stack due to the hierarchy bypassing problem. In\nthis work, we propose layer-wise cross-view decoding, where for each decoder\nlayer, together with the representations from the last encoder layer, which\nserve as a global view, those from other encoder layers are supplemented for a\nstereoscopic view of the source sequences. Systematic experiments show that we\nsuccessfully address the hierarchy bypassing problem and substantially improve\nthe performance of sequence-to-sequence learning with deep representations on\ndiverse tasks.",
          "link": "http://arxiv.org/abs/2005.08081",
          "publishedOn": "2021-07-06T01:58:09.770Z",
          "wordCount": 637,
          "title": "Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning. (arXiv:2005.08081v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maile_K/0/1/0/all/0/1\">Kaitlin Maile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecarpentier_E/0/1/0/all/0/1\">Erwan Lecarpentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luga_H/0/1/0/all/0/1\">Herv&#xe9; Luga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1\">Dennis G. Wilson</a>",
          "description": "Differentiable Architecture Search (DARTS) is a recently proposed neural\narchitecture search (NAS) method based on a differentiable relaxation. Due to\nits success, numerous variants analyzing and improving parts of the DARTS\nframework have recently been proposed. By considering the problem as a\nconstrained bilevel optimization, we propose and analyze three improvements to\narchitectural weight competition, update scheduling, and regularization towards\ndiscretization. First, we introduce a new approach to the activation of\narchitecture weights, which prevents confounding competition within an edge and\nallows for fair comparison across edges to aid in discretization. Next, we\npropose a dynamic schedule based on per-minibatch network information to make\narchitecture updates more informed. Finally, we consider two regularizations,\nbased on proximity to discretization and the Alternating Directions Method of\nMultipliers (ADMM) algorithm, to promote early discretization. Our results show\nthat this new activation scheme reduces final architecture size and the\nregularizations improve reliability in search results while maintaining\ncomparable performance to state-of-the-art in NAS, especially when used with\nour new dynamic informed schedule.",
          "link": "http://arxiv.org/abs/2106.11655",
          "publishedOn": "2021-07-06T01:58:09.752Z",
          "wordCount": 631,
          "title": "On Constrained Optimization in Differentiable Neural Architecture Search. (arXiv:2106.11655v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01408",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lee_H/0/1/0/all/0/1\">Hyungi Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yun_E/0/1/0/all/0/1\">Eunggu Yun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_H/0/1/0/all/0/1\">Hongseok Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1\">Juho Lee</a>",
          "description": "Recent works have revealed that infinitely-wide feed-forward or recurrent\nneural networks of any architecture correspond to Gaussian processes referred\nto as $\\mathrm{NNGP}$. While these works have extended the class of neural\nnetworks converging to Gaussian processes significantly, however, there has\nbeen little focus on broadening the class of stochastic processes that such\nneural networks converge to. In this work, inspired by the scale mixture of\nGaussian random variables, we propose the scale mixture of $\\mathrm{NNGP}$ for\nwhich we introduce a prior distribution on the scale of the last-layer\nparameters. We show that simply introducing a scale prior on the last-layer\nparameters can turn infinitely-wide neural networks of any architecture into a\nricher class of stochastic processes. Especially, with certain scale priors, we\nobtain heavy-tailed stochastic processes, and we recover Student's $t$\nprocesses in the case of inverse gamma priors. We further analyze the\ndistributions of the neural networks initialized with our prior setting and\ntrained with gradient descents and obtain similar results as for\n$\\mathrm{NNGP}$. We present a practical posterior-inference algorithm for the\nscale mixture of $\\mathrm{NNGP}$ and empirically demonstrate its usefulness on\nregression and classification tasks.",
          "link": "http://arxiv.org/abs/2107.01408",
          "publishedOn": "2021-07-06T01:58:09.745Z",
          "wordCount": 618,
          "title": "Scale Mixtures of Neural Network Gaussian Processes. (arXiv:2107.01408v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.00492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>",
          "description": "Sentiment analysis in conversations has gained increasing attention in recent\nyears for the growing amount of applications it can serve, e.g., sentiment\nanalysis, recommender systems, and human-robot interaction. The main difference\nbetween conversational sentiment analysis and single sentence sentiment\nanalysis is the existence of context information which may influence the\nsentiment of an utterance in a dialogue. How to effectively encode contextual\ninformation in dialogues, however, remains a challenge. Existing approaches\nemploy complicated deep learning structures to distinguish different parties in\na conversation and then model the context information. In this paper, we\npropose a fast, compact and parameter-efficient party-ignorant framework named\nbidirectional emotional recurrent unit for conversational sentiment analysis.\nIn our system, a generalized neural tensor block followed by a two-channel\nclassifier is designed to perform context compositionality and sentiment\nclassification, respectively. Extensive experiments on three standard datasets\ndemonstrate that our model outperforms the state of the art in most cases.",
          "link": "http://arxiv.org/abs/2006.00492",
          "publishedOn": "2021-07-06T01:58:09.738Z",
          "wordCount": 633,
          "title": "BiERU: Bidirectional Emotional Recurrent Unit for Conversational Sentiment Analysis. (arXiv:2006.00492v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dewanto_V/0/1/0/all/0/1\">Vektor Dewanto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1\">Marcus Gallagher</a>",
          "description": "In reinforcement learning (RL), the goal is to obtain an optimal policy, for\nwhich the optimality criterion is fundamentally important. Two major optimality\ncriteria are average and discounted rewards, where the later is typically\nconsidered as an approximation to the former. While the discounted reward is\nmore popular, it is problematic to apply in environments that have no natural\nnotion of discounting. This motivates us to revisit a) the progression of\noptimality criteria in dynamic programming, b) justification for and\ncomplication of an artificial discount factor, and c) benefits of directly\nmaximizing the average reward. Our contributions include a thorough examination\nof the relationship between average and discounted rewards, as well as a\ndiscussion of their pros and cons in RL. We emphasize that average-reward RL\nmethods possess the ingredient and mechanism for developing the general\ndiscounting-free optimality criterion (Veinott, 1969) in RL.",
          "link": "http://arxiv.org/abs/2107.01348",
          "publishedOn": "2021-07-06T01:58:09.731Z",
          "wordCount": 594,
          "title": "Examining average and discounted reward optimality criteria in reinforcement learning. (arXiv:2107.01348v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.10696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhenghao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>",
          "description": "In problem-solving, we humans can come up with multiple novel solutions to\nthe same problem. However, reinforcement learning algorithms can only produce a\nset of monotonous policies that maximize the cumulative reward but lack\ndiversity and novelty. In this work, we address the problem of generating novel\npolicies in reinforcement learning tasks. Instead of following the\nmulti-objective framework used in existing methods, we propose to rethink the\nproblem under a novel perspective of constrained optimization. We first\nintroduce a new metric to evaluate the difference between policies and then\ndesign two practical novel policy generation methods following the new\nperspective. The two proposed methods, namely the Constrained Task Novel\nBisector (CTNB) and the Interior Policy Differentiation (IPD), are derived from\nthe feasible direction method and the interior point method commonly known in\nthe constrained optimization literature. Experimental comparisons on the MuJoCo\ncontrol suite show our methods can achieve substantial improvement over\nprevious novelty-seeking methods in terms of both the novelty of policies and\ntheir performances in the primal task.",
          "link": "http://arxiv.org/abs/2005.10696",
          "publishedOn": "2021-07-06T01:58:09.724Z",
          "wordCount": 639,
          "title": "Novel Policy Seeking with Constrained Optimization. (arXiv:2005.10696v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Grzegorz Dudek</a>",
          "description": "Feedforward neural networks are widely used as universal predictive models to\nfit data distribution. Common gradient-based learning, however, suffers from\nmany drawbacks making the training process ineffective and time-consuming.\nAlternative randomized learning does not use gradients but selects hidden node\nparameters randomly. This makes the training process extremely fast. However,\nthe problem in randomized learning is how to determine the random parameters. A\nrecently proposed method uses autoencoders for unsupervised parameter learning.\nThis method showed superior performance on classification tasks. In this work,\nwe apply this method to regression problems, and, finding that it has some\ndrawbacks, we show how to improve it. We propose a learning method of\nautoencoders that controls the produced random weights. We also propose how to\ndetermine the biases of hidden nodes. We empirically compare autoencoder based\nlearning with other randomized learning methods proposed recently for\nregression and find that despite the proposed improvement of the autoencoder\nbased learning, it does not outperform its competitors in fitting accuracy.\nMoreover, the method is much more complex than its competitors.",
          "link": "http://arxiv.org/abs/2107.01711",
          "publishedOn": "2021-07-06T01:58:09.717Z",
          "wordCount": 616,
          "title": "Autoencoder based Randomized Learning of Feedforward Neural Networks for Regression. (arXiv:2107.01711v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dann_C/0/1/0/all/0/1\">Christoph Dann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinov_T/0/1/0/all/0/1\">Teodor V. Marinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1\">Mehryar Mohri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmert_J/0/1/0/all/0/1\">Julian Zimmert</a>",
          "description": "We provide improved gap-dependent regret bounds for reinforcement learning in\nfinite episodic Markov decision processes. Compared to prior work, our bounds\ndepend on alternative definitions of gaps. These definitions are based on the\ninsight that, in order to achieve a favorable regret, an algorithm does not\nneed to learn how to behave optimally in states that are not reached by an\noptimal policy. We prove tighter upper regret bounds for optimistic algorithms\nand accompany them with new information-theoretic lower bounds for a large\nclass of MDPs. Our results show that optimistic algorithms can not achieve the\ninformation-theoretic lower bounds even in deterministic MDPs unless there is a\nunique optimal policy.",
          "link": "http://arxiv.org/abs/2107.01264",
          "publishedOn": "2021-07-06T01:58:09.699Z",
          "wordCount": 548,
          "title": "Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Reinforcement Learning. (arXiv:2107.01264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_X/0/1/0/all/0/1\">Xin Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>",
          "description": "Academia and industry have developed several platforms to support the popular\nprivacy-preserving distributed learning method -- Federated Learning (FL).\nHowever, these platforms are complex to use and require a deep understanding of\nFL, which imposes high barriers to entry for beginners, limits the productivity\nof researchers, and compromises deployment efficiency. In this paper, we\npropose the first low-code FL platform, EasyFL, to enable users with various\nlevels of expertise to experiment and prototype FL applications with little\ncoding. We achieve this goal while ensuring great flexibility and extensibility\nfor customization by unifying simple API design, modular design, and granular\ntraining flow abstraction. With only a few lines of code, EasyFL empowers them\nwith many out-of-the-box functionalities to accelerate experimentation and\ndeployment. These practical functionalities are heterogeneity simulation,\ncomprehensive tracking, distributed training optimization, and seamless\ndeployment. They are proposed based on challenges identified in the proposed FL\nlife cycle. Compared with other platforms, EasyFL not only requires just three\nlines of code (at least 10x lesser) to build a vanilla FL application but also\nincurs lower training overhead. Besides, our evaluations demonstrate that\nEasyFL expedites distributed training by 1.5x. It also improves the efficiency\nof deployment. We believe that EasyFL will increase the productivity of\nresearchers and democratize FL to wider audiences.",
          "link": "http://arxiv.org/abs/2105.07603",
          "publishedOn": "2021-07-06T01:58:09.690Z",
          "wordCount": 682,
          "title": "EasyFL: A Low-code Federated Learning Platform For Dummies. (arXiv:2105.07603v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaofeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaiping Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadish_H/0/1/0/all/0/1\">H. V. Jagadish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ooi_B/0/1/0/all/0/1\">Beng Chin Ooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihui Zhang</a>",
          "description": "Relational databases are the de facto standard for storing and querying\nstructured data, and extracting insights from structured data requires advanced\nanalytics. Deep neural networks (DNNs) have achieved super-human prediction\nperformance in particular data types, e.g., images. However, existing DNNs may\nnot produce meaningful results when applied to structured data. The reason is\nthat there are correlations and dependencies across combinations of attribute\nvalues in a table, and these do not follow simple additive patterns that can be\neasily mimicked by a DNN. The number of possible such cross features is\ncombinatorial, making them computationally prohibitive to model. Furthermore,\nthe deployment of learning models in real-world applications has also\nhighlighted the need for interpretability, especially for high-stakes\napplications, which remains another issue of concern to DNNs.\n\nIn this paper, we present ARM-Net, an adaptive relation modeling network\ntailored for structured data, and a lightweight framework ARMOR based on\nARM-Net for relational data analytics. The key idea is to model feature\ninteractions with cross features selectively and dynamically, by first\ntransforming the input features into exponential space, and then determining\nthe interaction order and interaction weights adaptively for each cross\nfeature. We propose a novel sparse attention mechanism to dynamically generate\nthe interaction weights given the input tuple, so that we can explicitly model\ncross features of arbitrary orders with noisy features filtered selectively.\nThen during model inference, ARM-Net can specify the cross features being used\nfor each prediction for higher accuracy and better interpretability. Our\nextensive experiments on real-world datasets demonstrate that ARM-Net\nconsistently outperforms existing models and provides more interpretable\npredictions for data-driven decision making.",
          "link": "http://arxiv.org/abs/2107.01830",
          "publishedOn": "2021-07-06T01:58:09.684Z",
          "wordCount": 724,
          "title": "ARM-Net: Adaptive Relation Modeling Network for Structured Data. (arXiv:2107.01830v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02130",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_T/0/1/0/all/0/1\">Tianfang Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bokrantz_R/0/1/0/all/0/1\">Rasmus Bokrantz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Olsson_J/0/1/0/all/0/1\">Jimmy Olsson</a>",
          "description": "We present a new nonparametric mixture-of-experts model for multivariate\nregression problems, inspired by the probabilistic $k$-nearest neighbors\nalgorithm. Using a conditionally specified model, predictions for out-of-sample\ninputs are based on similarities to each observed data point, yielding\npredictive distributions represented by Gaussian mixtures. Posterior inference\nis performed on the parameters of the mixture components as well as the\ndistance metric using a mean-field variational Bayes algorithm accompanied with\na stochastic gradient-based optimization procedure. The proposed method is\nespecially advantageous in settings where inputs are of relatively high\ndimension in comparison to the data size, where input--output relationships are\ncomplex, and where predictive distributions may be skewed or multimodal.\nComputational studies on two synthetic datasets and one dataset comprising dose\nstatistics of radiation therapy treatment plans show that our\nmixture-of-experts method performs similarly or better than a conditional\nDirichlet process mixture model both in terms of validation metrics and visual\ninspection.",
          "link": "http://arxiv.org/abs/2012.02130",
          "publishedOn": "2021-07-06T01:58:09.677Z",
          "wordCount": 605,
          "title": "A similarity-based Bayesian mixture-of-experts model. (arXiv:2012.02130v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1\">Zewei Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_L/0/1/0/all/0/1\">Liwei Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Muchao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Houping Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>",
          "description": "Federated learning (FL) has emerged as an effective technique to co-training\nmachine learning models without actually sharing data and leaking privacy.\nHowever, most existing FL methods focus on the supervised setting and ignore\nthe utilization of unlabeled data. Although there are a few existing studies\ntrying to incorporate unlabeled data into FL, they all fail to maintain\nperformance guarantees or generalization ability in various real-world\nsettings. In this paper, we focus on designing a general framework FedSiam to\ntackle different scenarios of federated semi-supervised learning, including\nfour settings in the labels-at-client scenario and two setting in the\nlabels-at-server scenario. FedSiam is built upon a siamese network into FL with\na momentum update to handle the non-IID challenges introduced by unlabeled\ndata. We further propose a new metric to measure the divergence of local model\nlayers within the siamese network. Based on the divergence, FedSiam can\nautomatically select layer-level parameters to be uploaded to the server in an\nadaptive manner. Experimental results on three datasets under two scenarios\nwith different data distribution settings demonstrate that the proposed FedSiam\nframework outperforms state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2012.03292",
          "publishedOn": "2021-07-06T01:58:09.670Z",
          "wordCount": 644,
          "title": "FedSiam: Towards Adaptive Federated Semi-Supervised Learning. (arXiv:2012.03292v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01876",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zheng_X/0/1/0/all/0/1\">Xiangyu Zheng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_X/0/1/0/all/0/1\">Xinwei Sun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "This paper proposes an invariant causal predictor that is robust to\ndistribution shift across domains and maximally reserves the transferable\ninvariant information. Based on a disentangled causal factorization, we\nformulate the distribution shift as soft interventions in the system, which\ncovers a wide range of cases for distribution shift as we do not make prior\nspecifications on the causal structure or the intervened variables. Instead of\nimposing regularizations to constrain the invariance of the predictor, we\npropose to predict by the intervened conditional expectation based on the\ndo-operator and then prove that it is invariant across domains. More\nimportantly, we prove that the proposed predictor is the robust predictor that\nminimizes the worst-case quadratic loss among the distributions of all domains.\nFor empirical learning, we propose an intuitive and flexible estimating method\nbased on data regeneration and present a local causal discovery procedure to\nguide the regeneration step. The key idea is to regenerate data such that the\nregenerated distribution is compatible with the intervened graph, which allows\nus to incorporate standard supervised learning methods with the regenerated\ndata. Experimental results on both synthetic and real data demonstrate the\nefficacy of our predictor in improving the predictive accuracy and robustness\nacross domains.",
          "link": "http://arxiv.org/abs/2107.01876",
          "publishedOn": "2021-07-06T01:58:09.652Z",
          "wordCount": 630,
          "title": "Causally Invariant Predictor with Shift-Robustness. (arXiv:2107.01876v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Naftali Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_S/0/1/0/all/0/1\">Srijan Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balch_T/0/1/0/all/0/1\">Tucker Balch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1\">Manuela Veloso</a>",
          "description": "In this work, we address time-series forecasting as a computer vision task.\nWe capture input data as an image and train a model to produce the subsequent\nimage. This approach results in predicting distributions as opposed to\npointwise values. To assess the robustness and quality of our approach, we\nexamine various datasets and multiple evaluation metrics. Our experiments show\nthat our forecasting tool is effective for cyclic data but somewhat less for\nirregular data such as stock prices. Importantly, when using image-based\nevaluation metrics, we find our method to outperform various baselines,\nincluding ARIMA, and a numerical variation of our deep learning approach.",
          "link": "http://arxiv.org/abs/2107.01273",
          "publishedOn": "2021-07-06T01:58:09.646Z",
          "wordCount": 554,
          "title": "Visual Time Series Forecasting: An Image-driven Approach. (arXiv:2107.01273v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohammadi Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1\">Avinash Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalan_A/0/1/0/all/0/1\">Aditya Gopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>",
          "description": "We consider an improper reinforcement learning setting where a learner is\ngiven $M$ base controllers for an unknown Markov decision process, and wishes\nto combine them optimally to produce a potentially new controller that can\noutperform each of the base ones. This can be useful in tuning across\ncontrollers, learnt possibly in mismatched or simulated environments, to obtain\na good controller for a given target environment with relatively few trials.\n\n\\par We propose a gradient-based approach that operates over a class of\nimproper mixtures of the controllers. We derive convergence rate guarantees for\nthe approach assuming access to a gradient oracle. The value function of the\nmixture and its gradient may not be available in closed-form; however, we show\nthat we can employ rollouts and simultaneous perturbation stochastic\napproximation (SPSA) for explicit gradient descent optimization. Numerical\nresults on (i) the standard control theoretic benchmark of stabilizing an\ninverted pendulum and (ii) a constrained queueing task show that our improper\npolicy optimization algorithm can stabilize the system even when the base\npolicies at its disposal are unstable\\footnote{Under review. Please do not\ndistribute.}.",
          "link": "http://arxiv.org/abs/2102.08201",
          "publishedOn": "2021-07-06T01:58:09.637Z",
          "wordCount": 659,
          "title": "Improper Reinforcement Learning with Gradient-based Policy Optimization. (arXiv:2102.08201v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Titsias_M/0/1/0/all/0/1\">Michalis K. Titsias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_F/0/1/0/all/0/1\">Francisco J. R. Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoloutsopoulos_S/0/1/0/all/0/1\">Sotirios Nikoloutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galashov_A/0/1/0/all/0/1\">Alexandre Galashov</a>",
          "description": "We formulate meta learning using information theoretic concepts; namely,\nmutual information and the information bottleneck. The idea is to learn a\nstochastic representation or encoding of the task description, given by a\ntraining set, that is highly informative about predicting the validation set.\nBy making use of variational approximations to the mutual information, we\nderive a general and tractable framework for meta learning. This framework\nunifies existing gradient-based algorithms and also allows us to derive new\nalgorithms. In particular, we develop a memory-based algorithm that uses\nGaussian processes to obtain non-parametric encoding representations. We\ndemonstrate our method on a few-shot regression problem and on four few-shot\nclassification problems, obtaining competitive accuracy when compared to\nexisting baselines.",
          "link": "http://arxiv.org/abs/2009.03228",
          "publishedOn": "2021-07-06T01:58:09.629Z",
          "wordCount": 599,
          "title": "Information Theoretic Meta Learning with Gaussian Processes. (arXiv:2009.03228v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.11918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Adaptive gradient methods such as RMSProp and Adam use exponential moving\nestimate of the squared gradient to compute adaptive step sizes, achieving\nbetter convergence than SGD in face of noisy objectives. However, Adam can have\nundesirable convergence behaviors due to unstable or extreme adaptive learning\nrates. Methods such as AMSGrad and AdaBound have been proposed to stabilize the\nadaptive learning rates of Adam in the later stage of training, but they do not\noutperform Adam in some practical tasks such as training Transformers\n\\cite{transformer}. In this paper, we propose an adaptive learning rate\nprinciple, in which the running mean of squared gradient in Adam is replaced by\na weighted mean, with weights chosen to maximize the estimated variance of each\ncoordinate. This results in a faster adaptation to the local gradient variance,\nwhich leads to more desirable empirical convergence behaviors than Adam. We\nprove the proposed algorithm converges under mild assumptions for nonconvex\nstochastic optimization problems, and demonstrate the improved efficacy of our\nadaptive averaging approach on machine translation, natural language\nunderstanding and large-batch pretraining of BERT. The code is available at\nhttps://github.com/zhuchen03/MaxVA.",
          "link": "http://arxiv.org/abs/2006.11918",
          "publishedOn": "2021-07-06T01:58:09.623Z",
          "wordCount": 686,
          "title": "MaxVA: Fast Adaptation of Step Sizes by Maximizing Observed Variance of Gradients. (arXiv:2006.11918v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.02325",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1\">David D. Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_J/0/1/0/all/0/1\">Jennifer Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thakker_R/0/1/0/all/0/1\">Rohan Thakker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alatur_N/0/1/0/all/0/1\">Nikhilesh Alatur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agha_mohammadi_A/0/1/0/all/0/1\">Ali-akbar Agha-mohammadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Theodorou_E/0/1/0/all/0/1\">Evangelos A. Theodorou</a>",
          "description": "Deep learning has enjoyed much recent success, and applying state-of-the-art\nmodel learning methods to controls is an exciting prospect. However, there is a\nstrong reluctance to use these methods on safety-critical systems, which have\nconstraints on safety, stability, and real-time performance. We propose a\nframework which satisfies these constraints while allowing the use of deep\nneural networks for learning model uncertainties. Central to our method is the\nuse of Bayesian model learning, which provides an avenue for maintaining\nappropriate degrees of caution in the face of the unknown. In the proposed\napproach, we develop an adaptive control framework leveraging the theory of\nstochastic CLFs (Control Lyapunov Functions) and stochastic CBFs (Control\nBarrier Functions) along with tractable Bayesian model learning via Gaussian\nProcesses or Bayesian neural networks. Under reasonable assumptions, we\nguarantee stability and safety while adapting to unknown dynamics with\nprobability 1. We demonstrate this architecture for high-speed terrestrial\nmobility targeting potential applications in safety-critical high-speed Mars\nrover missions.",
          "link": "http://arxiv.org/abs/1910.02325",
          "publishedOn": "2021-07-06T01:58:09.605Z",
          "wordCount": 712,
          "title": "Bayesian Learning-Based Adaptive Control for Safety Critical Systems. (arXiv:1910.02325v3 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yujun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_M/0/1/0/all/0/1\">Milad Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1\">Kevin Swersky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaoqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1\">Danai Koutra</a>",
          "description": "Most graph convolutional neural networks (GCNs) perform poorly in graphs\nwhere neighbors typically have different features/classes (heterophily) and\nwhen stacking multiple layers (oversmoothing). These two seemingly unrelated\nproblems have been studied independently, but there is recent empirical\nevidence that solving one problem may benefit the other. In this work, going\nbeyond empirical observations, we aim to: (1) propose a new perspective to\nanalyze the heterophily and oversmoothing problems under a unified theoretical\nframework, (2) identify the common causes of the two problems based on the\nproposed framework, and (3) propose simple yet effective strategies that\naddress the common causes. Focusing on the node classification task, we use\nlinear separability of node representations as an indicator to reflect the\nperformance of GCNs and we propose to study the linear separability by\nanalyzing the statistical change of the node representations in the graph\nconvolution. We find that the relative degree of a node (compared to its\nneighbors) and the heterophily level of a node's neighborhood are the root\ncauses that influence the separability of node representations. Our analysis\nsuggests that: (1) Nodes with high heterophily always produce less separable\nrepresentations after graph convolution; (2) Even with low heterophily, degree\ndisparity between nodes can influence the network dynamics and result in a\npseudo-heterophily situation, which helps to explain oversmoothing. Based on\nour insights, we propose simple modifications to the GCN architecture -- i.e.,\ndegree corrections and signed messages -- which alleviate the root causes of\nthese issues, and also show this empirically on 9 real networks. Compared to\nother approaches, which tend to work well in one regime but fail in others, our\nmodified GCN model consistently performs well across all settings.",
          "link": "http://arxiv.org/abs/2102.06462",
          "publishedOn": "2021-07-06T01:58:09.598Z",
          "wordCount": 766,
          "title": "Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks. (arXiv:2102.06462v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a low-latency real-time (LLRT) non-parallel voice\nconversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\nand multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\nrobust non-parallel multispeaker spectral model, which utilizes a\nspeaker-independent latent space and a speaker-dependent code to generate\nreconstructed/converted spectral features given the spectral features of an\ninput speaker. On the other hand, MWDLP is an efficient and a high-quality\nneural vocoder that can handle multispeaker data and generate speech waveform\nfor LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\npropose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\nfeatures and is built with a sparse network architecture. Further, to improve\nthe modeling performance, we also propose a novel fine-tuning procedure that\nrefines the frame-rate CycleVAE network by utilizing the waveform loss from the\nMWDLP network. The experimental results demonstrate that the proposed framework\nachieves high-performance VC, while allowing for LLRT usage with a single-core\nof $2.1$--$2.7$ GHz CPU on a real-time factor of $0.87$--$0.95$, including\ninput/output, feature extraction, on a frame shift of $10$ ms, a window length\nof $27.5$ ms, and $2$ lookup frames.",
          "link": "http://arxiv.org/abs/2105.09858",
          "publishedOn": "2021-07-06T01:58:09.591Z",
          "wordCount": 681,
          "title": "Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagar_S/0/1/0/all/0/1\">Sandeep Nagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufraisse_M/0/1/0/all/0/1\">Marius Dufraisse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_G/0/1/0/all/0/1\">Girish Varma</a>",
          "description": "Normalizing flows are an essential alternative to GANs for generative\nmodelling, which can be optimized directly on the maximum likelihood of the\ndataset. They also allow computation of the exact latent vector corresponding\nto an image since they are composed of invertible transformations. However, the\nrequirement of invertibility of the transformation prevents standard and\nexpressive neural network models such as CNNs from being directly used.\nEmergent convolutions were proposed to construct an invertible 3$\\times$3 CNN\nlayer using a pair of masked CNN layers, making them inefficient. We study\nconditions such that 3$\\times$3 CNNs are invertible, allowing them to construct\nexpressive normalizing flows. We derive necessary and sufficient conditions on\na padded CNN for it to be invertible. Our conditions for invertibility are\nsimple, can easily be maintained during the training process. Since we require\nonly a single CNN layer for every effective invertible CNN layer, our approach\nis more efficient than emerging convolutions. We also proposed a coupling\nmethod, Quad-coupling. We benchmark our approach and show similar performance\nresults to emergent convolutions while improving the model's efficiency.",
          "link": "http://arxiv.org/abs/2107.01358",
          "publishedOn": "2021-07-06T01:58:09.584Z",
          "wordCount": 623,
          "title": "CInC Flow: Characterizable Invertible 3x3 Convolution. (arXiv:2107.01358v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minkyo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoonho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>",
          "description": "This paper studies probability distributions ofpenultimate activations of\nclassification networks.We show that, when a classification network istrained\nwith the cross-entropy loss, its final classi-fication layer forms\naGenerative-Discriminativepairwith a generative classifier based on a\nspecificdistribution of penultimate activations. More im-portantly, the\ndistribution is parameterized by theweights of the final fully-connected layer,\nand canbe considered as a generative model that synthe-sizes the penultimate\nactivations without feedinginput data. We empirically demonstrate that\nthisgenerative model enables stable knowledge dis-tillation in the presence of\ndomain shift, and cantransfer knowledge from a classifier to\nvariationalautoencoders and generative adversarial networksfor\nclass-conditional image generation.",
          "link": "http://arxiv.org/abs/2107.01900",
          "publishedOn": "2021-07-06T01:58:09.567Z",
          "wordCount": 538,
          "title": "On The Distribution of Penultimate Activations of Classification Networks. (arXiv:2107.01900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.15714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Aditya Ojha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1\">Daniel Neider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>",
          "description": "Despite the fact that deep reinforcement learning (RL) has surpassed\nhuman-level performances in various tasks, it still has several fundamental\nchallenges. First, most RL methods require intensive data from the exploration\nof the environment to achieve satisfactory performance. Second, the use of\nneural networks in RL renders it hard to interpret the internals of the system\nin a way that humans can understand. To address these two challenges, we\npropose a framework that enables an RL agent to reason over its exploration\nprocess and distill high-level knowledge for effectively guiding its future\nexplorations. Specifically, we propose a novel RL algorithm that learns\nhigh-level knowledge in the form of a finite reward automaton by using the L*\nlearning algorithm. We prove that in episodic RL, a finite reward automaton can\nexpress any non-Markovian bounded reward functions with finitely many reward\nvalues and approximate any non-Markovian bounded reward function (with\ninfinitely many reward values) with arbitrary precision. We also provide a\nlower bound for the episode length such that the proposed RL approach almost\nsurely converges to an optimal policy in the limit. We test this approach on\ntwo RL environments with non-Markovian reward functions, choosing a variety of\ntasks with increasing complexity for each environment. We compare our algorithm\nwith the state-of-the-art RL algorithms for non-Markovian reward functions,\nsuch as Joint Inference of Reward machines and Policies for RL (JIRP), Learning\nReward Machine (LRM), and Proximal Policy Optimization (PPO2). Our results show\nthat our algorithm converges to an optimal policy faster than other baseline\nmethods.",
          "link": "http://arxiv.org/abs/2006.15714",
          "publishedOn": "2021-07-06T01:58:09.560Z",
          "wordCount": 745,
          "title": "Active Finite Reward Automaton Inference and Reinforcement Learning Using Queries and Counterexamples. (arXiv:2006.15714v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.04675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Persand_K/0/1/0/all/0/1\">Kaveena Persand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Andrew Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1\">David Gregg</a>",
          "description": "Pruning unimportant parameters can allow deep neural networks (DNNs) to\nreduce their heavy computation and memory requirements. A saliency metric\nestimates which parameters can be safely pruned with little impact on the\nclassification performance of the DNN. Many saliency metrics have been\nproposed, each within the context of a wider pruning algorithm. The result is\nthat it is difficult to separate the effectiveness of the saliency metric from\nthe wider pruning algorithm that surrounds it. Similar-looking saliency metrics\ncan yield very different results because of apparently minor design choices. We\npropose a taxonomy of saliency metrics based on four mostly-orthogonal\nprincipal components. We show that a broad range of metrics from the pruning\nliterature can be grouped according to these components. Our taxonomy not only\nserves as a guide to prior work, but allows us to construct new saliency\nmetrics by exploring novel combinations of our taxonomic components. We perform\nan in-depth experimental investigation of more than 300 saliency metrics. Our\nresults provide decisive answers to open research questions, and demonstrate\nthe importance of reduction and scaling when pruning groups of weights. We find\nthat some of our constructed metrics can outperform the best existing\nstate-of-the-art metrics for convolutional neural network channel pruning.",
          "link": "http://arxiv.org/abs/1906.04675",
          "publishedOn": "2021-07-06T01:58:09.553Z",
          "wordCount": 662,
          "title": "Taxonomy of Saliency Metrics for Channel Pruning. (arXiv:1906.04675v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yue Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xudong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jian Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Off-policy evaluation (OPE) leverages data generated by other policies to\nevaluate a target policy. Previous OPE methods mainly focus on precisely\nestimating the true performance of a policy. We observe that in many\napplications, (1) the end goal of OPE is to compare two or multiple candidate\npolicies and choose a good one, which is actually a much simpler task than\nevaluating their true performance; and (2) there are usually multiple policies\nthat have been deployed in real-world systems and thus whose true performance\nis known through serving real users. Inspired by the two observations, in this\nwork, we define a new problem, supervised off-policy ranking (SOPR), which aims\nto rank a set of new/target policies based on supervised learning by leveraging\noff-policy data and policies with known performance. We further propose a\nmethod for supervised off-policy ranking that learns a policy scoring model by\ncorrectly ranking training policies with known performance rather than\nestimating their precise performance. Our method leverages logged states and\npolicies to learn a Transformer based model that maps offline interaction data\nincluding logged states and the actions taken by a target policy on these\nstates to a score. Experiments on different games, datasets, training policy\nsets, and test policy sets show that our method outperforms strong baseline OPE\nmethods in terms of both rank correlation and performance gap between the truly\nbest and the best of the ranked top three policies. Furthermore, our method is\nmore stable than baseline methods.",
          "link": "http://arxiv.org/abs/2107.01360",
          "publishedOn": "2021-07-06T01:58:09.545Z",
          "wordCount": 671,
          "title": "Supervised Off-Policy Ranking. (arXiv:2107.01360v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiarpoor_R/0/1/0/all/0/1\">Reza Esfandiarpoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_A/0/1/0/all/0/1\">Amy Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajabdollahi_M/0/1/0/all/0/1\">Mohsen Hajabdollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>",
          "description": "In many practical few-shot learning problems, even though labeled examples\nare scarce, there are abundant auxiliary datasets that potentially contain\nuseful information. We propose the problem of extended few-shot learning to\nstudy these scenarios. We then introduce a framework to address the challenges\nof efficiently selecting and effectively using auxiliary data in few-shot image\nclassification. Given a large auxiliary dataset and a notion of semantic\nsimilarity among classes, we automatically select pseudo shots, which are\nlabeled examples from other classes related to the target task. We show that\nnaive approaches, such as (1) modeling these additional examples the same as\nthe target task examples or (2) using them to learn features via transfer\nlearning, only increase accuracy by a modest amount. Instead, we propose a\nmasking module that adjusts the features of auxiliary data to be more similar\nto those of the target classes. We show that this masking module performs\nbetter than naively modeling the support examples and transfer learning by 4.68\nand 6.03 percentage points, respectively.",
          "link": "http://arxiv.org/abs/2012.07176",
          "publishedOn": "2021-07-06T01:58:09.538Z",
          "wordCount": 656,
          "title": "Extended Few-Shot Learning: Exploiting Existing Resources for Novel Tasks. (arXiv:2012.07176v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>",
          "description": "Modelling mix-and-match relationships among fashion items has become\nincreasingly demanding yet challenging for modern E-commerce recommender\nsystems. When performing clothes matching, most existing approaches leverage\nthe latent visual features extracted from fashion item images for compatibility\nmodelling, which lacks explainability of generated matching results and can\nhardly convince users of the recommendations. Though recent methods start to\nincorporate pre-defined attribute information (e.g., colour, style, length,\netc.) for learning item representations and improving the model\ninterpretability, their utilisation of attribute information is still mainly\nreserved for enhancing the learned item representations and generating\nexplanations via post-processing. As a result, this creates a severe bottleneck\nwhen we are trying to advance the recommendation accuracy and generating\nfine-grained explanations since the explicit attributes have only loose\nconnections to the actual recommendation process. This work aims to tackle the\nexplainability challenge in fashion recommendation tasks by proposing a novel\nAttribute-aware Fashion Recommender (AFRec). Specifically, AFRec recommender\nassesses the outfit compatibility by explicitly leveraging the extracted\nattribute-level representations from each item's visual feature. The attributes\nserve as the bridge between two fashion items, where we quantify the affinity\nof a pair of items through the learned compatibility between their attributes.\nExtensive experiments have demonstrated that, by making full use of the\nexplicit attributes in the recommendation process, AFRec is able to achieve\nstate-of-the-art recommendation accuracy and generate intuitive explanations at\nthe same time.",
          "link": "http://arxiv.org/abs/2107.01655",
          "publishedOn": "2021-07-06T01:58:09.518Z",
          "wordCount": 654,
          "title": "Attribute-aware Explainable Complementary Clothing Recommendation. (arXiv:2107.01655v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2007.08243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elesedy_B/0/1/0/all/0/1\">Bryn Elesedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanade_V/0/1/0/all/0/1\">Varun Kanade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>",
          "description": "We analyse the pruning procedure behind the lottery ticket hypothesis\narXiv:1803.03635v5, iterative magnitude pruning (IMP), when applied to linear\nmodels trained by gradient flow. We begin by presenting sufficient conditions\non the statistical structure of the features under which IMP prunes those\nfeatures that have smallest projection onto the data. Following this, we\nexplore IMP as a method for sparse estimation.",
          "link": "http://arxiv.org/abs/2007.08243",
          "publishedOn": "2021-07-06T01:58:09.511Z",
          "wordCount": 549,
          "title": "Lottery Tickets in Linear Models: An Analysis of Iterative Magnitude Pruning. (arXiv:2007.08243v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagstaff_E/0/1/0/all/0/1\">Edward Wagstaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_F/0/1/0/all/0/1\">Fabian B. Fuchs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelcke_M/0/1/0/all/0/1\">Martin Engelcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osborne_M/0/1/0/all/0/1\">Michael A. Osborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1\">Ingmar Posner</a>",
          "description": "Modelling functions of sets, or equivalently, permutation-invariant\nfunctions, is a long-standing challenge in machine learning. Deep Sets is a\npopular method which is known to be a universal approximator for continuous set\nfunctions. We provide a theoretical analysis of Deep Sets which shows that this\nuniversal approximation property is only guaranteed if the model's latent space\nis sufficiently high-dimensional. If the latent space is even one dimension\nlower than necessary, there exist piecewise-affine functions for which Deep\nSets performs no better than a na\\\"ive constant baseline, as judged by\nworst-case error. Deep Sets may be viewed as the most efficient incarnation of\nthe Janossy pooling paradigm. We identify this paradigm as encompassing most\ncurrently popular set-learning methods. Based on this connection, we discuss\nthe implications of our results for set learning more broadly, and identify\nsome open questions on the universality of Janossy pooling in general.",
          "link": "http://arxiv.org/abs/2107.01959",
          "publishedOn": "2021-07-06T01:58:09.504Z",
          "wordCount": 585,
          "title": "Universal Approximation of Functions on Sets. (arXiv:2107.01959v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vadillo_J/0/1/0/all/0/1\">Jon Vadillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santana_R/0/1/0/all/0/1\">Roberto Santana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_J/0/1/0/all/0/1\">Jose A. Lozano</a>",
          "description": "Reliable deployment of machine learning models such as neural networks\ncontinues to be challenging due to several limitations. Some of the main\nshortcomings are the lack of interpretability and the lack of robustness\nagainst adversarial examples or out-of-distribution inputs. In this paper, we\nexplore the possibilities and limits of adversarial attacks for explainable\nmachine learning models. First, we extend the notion of adversarial examples to\nfit in explainable machine learning scenarios, in which the inputs, the output\nclassifications and the explanations of the model's decisions are assessed by\nhumans. Next, we propose a comprehensive framework to study whether (and how)\nadversarial examples can be generated for explainable models under human\nassessment, introducing novel attack paradigms. In particular, our framework\nconsiders a wide range of relevant (yet often ignored) factors such as the type\nof problem, the user expertise or the objective of the explanations in order to\nidentify the attack strategies that should be adopted in each scenario to\nsuccessfully deceive the model (and the human). These contributions intend to\nserve as a basis for a more rigorous and realistic study of adversarial\nexamples in the field of explainable machine learning.",
          "link": "http://arxiv.org/abs/2107.01943",
          "publishedOn": "2021-07-06T01:58:09.498Z",
          "wordCount": 639,
          "title": "When and How to Fool Explainable Models (and Humans) with Adversarial Examples. (arXiv:2107.01943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gorbunov_E/0/1/0/all/0/1\">Eduard Gorbunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlachenko_K/0/1/0/all/0/1\">Konstantin Burlachenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhize Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>",
          "description": "We develop and analyze MARINA: a new communication efficient method for\nnon-convex distributed learning over heterogeneous datasets. MARINA employs a\nnovel communication compression strategy based on the compression of gradient\ndifferences that is reminiscent of but different from the strategy employed in\nthe DIANA method of Mishchenko et al. (2019). Unlike virtually all competing\ndistributed first-order methods, including DIANA, ours is based on a carefully\ndesigned biased gradient estimator, which is the key to its superior\ntheoretical and practical performance. The communication complexity bounds we\nprove for MARINA are evidently better than those of all previous first-order\nmethods. Further, we develop and analyze two variants of MARINA: VR-MARINA and\nPP-MARINA. The first method is designed for the case when the local loss\nfunctions owned by clients are either of a finite sum or of an expectation\nform, and the second method allows for a partial participation of clients -- a\nfeature important in federated learning. All our methods are superior to\nprevious state-of-the-art methods in terms of oracle/communication complexity.\nFinally, we provide a convergence analysis of all methods for problems\nsatisfying the Polyak-Lojasiewicz condition.",
          "link": "http://arxiv.org/abs/2102.07845",
          "publishedOn": "2021-07-06T01:58:09.487Z",
          "wordCount": 658,
          "title": "MARINA: Faster Non-Convex Distributed Learning with Compression. (arXiv:2102.07845v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.04463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kangle Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Aayush Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>",
          "description": "We present an unsupervised approach that converts the input speech of any\nindividual into audiovisual streams of potentially-infinitely many output\nspeakers. Our approach builds on simple autoencoders that project out-of-sample\ndata onto the distribution of the training set. We use Exemplar Autoencoders to\nlearn the voice, stylistic prosody, and visual appearance of a specific target\nexemplar speech. In contrast to existing methods, the proposed approach can be\neasily extended to an arbitrarily large number of speakers and styles using\nonly 3 minutes of target audio-video data, without requiring {\\em any} training\ndata for the input speaker. To do so, we learn audiovisual bottleneck\nrepresentations that capture the structured linguistic content of speech. We\noutperform prior approaches on both audio and video synthesis, and provide\nextensive qualitative analysis on our project page --\nhttps://www.cs.cmu.edu/~exemplar-ae/.",
          "link": "http://arxiv.org/abs/2001.04463",
          "publishedOn": "2021-07-06T01:58:09.480Z",
          "wordCount": 626,
          "title": "Unsupervised Audiovisual Synthesis via Exemplar Autoencoders. (arXiv:2001.04463v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacotte_J/0/1/0/all/0/1\">Jonathan Lacotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>",
          "description": "We prove that finding all globally optimal two-layer ReLU neural networks can\nbe performed by solving a convex optimization program with cone constraints.\nOur analysis is novel, characterizes all optimal solutions, and does not\nleverage duality-based analysis which was recently used to lift neural network\ntraining into convex spaces. Given the set of solutions of our convex\noptimization program, we show how to construct exactly the entire set of\noptimal neural networks. We provide a detailed characterization of this optimal\nset and its invariant transformations. As additional consequences of our convex\nperspective, (i) we establish that Clarke stationary points found by stochastic\ngradient descent correspond to the global optimum of a subsampled convex\nproblem (ii) we provide a polynomial-time algorithm for checking if a neural\nnetwork is a global minimum of the training loss (iii) we provide an explicit\nconstruction of a continuous path between any neural network and the global\nminimum of its sublevel set and (iv) characterize the minimal size of the\nhidden layer so that the neural network optimization landscape has no spurious\nvalleys. Overall, we provide a rich framework for studying the landscape of\nneural network training loss through convexity.",
          "link": "http://arxiv.org/abs/2006.05900",
          "publishedOn": "2021-07-06T01:58:09.463Z",
          "wordCount": 668,
          "title": "All Local Minima are Global for Two-Layer ReLU Neural Networks: The Hidden Convex Optimization Landscape. (arXiv:2006.05900v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01850",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaqi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Squires_C/0/1/0/all/0/1\">Chandler Squires</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uhler_C/0/1/0/all/0/1\">Caroline Uhler</a>",
          "description": "Transforming a causal system from a given initial state to a desired target\nstate is an important task permeating multiple fields including control theory,\nbiology, and materials science. In causal models, such transformations can be\nachieved by performing a set of interventions. In this paper, we consider the\nproblem of identifying a shift intervention that matches the desired mean of a\nsystem through active learning. We define the Markov equivalence class that is\nidentifiable from shift interventions and propose two active learning\nstrategies that are guaranteed to exactly match a desired mean. We then derive\na worst-case lower bound for the number of interventions required and show that\nthese strategies are optimal for certain classes of graphs. In particular, we\nshow that our strategies may require exponentially fewer interventions than the\npreviously considered approaches, which optimize for structure learning in the\nunderlying causal graph. In line with our theoretical results, we also\ndemonstrate experimentally that our proposed active learning strategies require\nfewer interventions compared to several baselines.",
          "link": "http://arxiv.org/abs/2107.01850",
          "publishedOn": "2021-07-06T01:58:09.456Z",
          "wordCount": 601,
          "title": "Matching a Desired Causal State via Shift Interventions. (arXiv:2107.01850v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2006.16785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blonde_L/0/1/0/all/0/1\">Lionel Blond&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strasser_P/0/1/0/all/0/1\">Pablo Strasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1\">Alexandros Kalousis</a>",
          "description": "Despite the recent success of reinforcement learning in various domains,\nthese approaches remain, for the most part, deterringly sensitive to\nhyper-parameters and are often riddled with essential engineering feats\nallowing their success. We consider the case of off-policy generative\nadversarial imitation learning, and perform an in-depth review, qualitative and\nquantitative, of the method. We show that forcing the learned reward function\nto be local Lipschitz-continuous is a sine qua non condition for the method to\nperform well. We then study the effects of this necessary condition and provide\nseveral theoretical results involving the local Lipschitzness of the\nstate-value function. We complement these guarantees with empirical evidence\nattesting to the strong positive effect that the consistent satisfaction of the\nLipschitzness constraint on the reward has on imitation performance. Finally,\nwe tackle a generic pessimistic reward preconditioning add-on spawning a large\nclass of reward shaping methods, which makes the base method it is plugged into\nprovably more robust, as shown in several additional theoretical guarantees. We\nthen discuss these through a fine-grained lens and share our insights.\nCrucially, the guarantees derived and reported in this work are valid for any\nreward satisfying the Lipschitzness condition, nothing is specific to\nimitation. As such, these may be of independent interest.",
          "link": "http://arxiv.org/abs/2006.16785",
          "publishedOn": "2021-07-06T01:58:09.449Z",
          "wordCount": 674,
          "title": "Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning. (arXiv:2006.16785v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhisong Pan</a>",
          "description": "Despite the empirical success of deep learning, it still lacks theoretical\nunderstandings to explain why randomly initialized neural network trained by\nfirst-order optimization methods is able to achieve zero training loss, even\nthough its landscape is non-convex and non-smooth. Recently, there are some\nworks to demystifies this phenomenon under over-parameterized regime. In this\nwork, we make further progress on this area by considering a commonly used\nmomentum optimization algorithm: Nesterov accelerated method (NAG). We analyze\nthe convergence of NAG for two-layer fully connected neural network with ReLU\nactivation. Specifically, we prove that the error of NAG converges to zero at a\nlinear convergence rate $1-\\Theta(1/\\sqrt{\\kappa})$, where $\\kappa > 1$ is\ndetermined by the initialization and the architecture of neural network.\nComparing to the rate $1-\\Theta(1/\\kappa)$ of gradient descent, NAG achieves an\nacceleration. Besides, it also validates NAG and Heavy-ball method can achieve\na similar convergence rate.",
          "link": "http://arxiv.org/abs/2107.01832",
          "publishedOn": "2021-07-06T01:58:09.443Z",
          "wordCount": 583,
          "title": "Provable Convergence of Nesterov Accelerated Method for Over-Parameterized Neural Networks. (arXiv:2107.01832v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Szelogowski_D/0/1/0/all/0/1\">Daniel Szelogowski</a>",
          "description": "Current computational-emotion research has focused on applying acoustic\nproperties to analyze how emotions are perceived mathematically or used in\nnatural language processing machine learning models. While recent interest has\nfocused on analyzing emotions from the spoken voice, little experimentation has\nbeen performed to discover how emotions are recognized in the singing voice --\nboth in noiseless and noisy data (i.e., data that is either inaccurate,\ndifficult to interpret, has corrupted/distorted/nonsense information like\nactual noise sounds in this case, or has a low ratio of usable/unusable\ninformation). Not only does this ignore the challenges of training machine\nlearning models on more subjective data and testing them with much noisier\ndata, but there is also a clear disconnect in progress between advancing the\ndevelopment of convolutional neural networks and the goal of emotionally\ncognizant artificial intelligence. By training a new model to include this type\nof information with a rich comprehension of psycho-acoustic properties, not\nonly can models be trained to recognize information within extremely noisy\ndata, but advancement can be made toward more complex biofeedback applications\n-- including creating a model which could recognize emotions given any human\ninformation (language, breath, voice, body, posture) and be used in any\nperformance medium (music, speech, acting) or psychological assistance for\npatients with disorders such as BPD, alexithymia, autism, among others. This\npaper seeks to reflect and expand upon the findings of related research and\npresent a stepping-stone toward this end goal.",
          "link": "http://arxiv.org/abs/2105.00173",
          "publishedOn": "2021-07-06T01:58:09.425Z",
          "wordCount": 733,
          "title": "Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis Tool for Singers. (arXiv:2105.00173v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asadi_R/0/1/0/all/0/1\">Reza Asadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regan_A/0/1/0/all/0/1\">Amelia Regan</a>",
          "description": "Time Series data are broadly studied in various domains of transportation\nsystems. Traffic data area challenging example of spatio-temporal data, as it\nis multi-variate time series with high correlations in spatial and temporal\nneighborhoods. Spatio-temporal clustering of traffic flow data find similar\npatterns in both spatial and temporal domain, where it provides better\ncapability for analyzing a transportation network, and improving related\nmachine learning models, such as traffic flow prediction and anomaly detection.\nIn this paper, we propose a spatio-temporal clustering model, where it clusters\ntime series data based on spatial and temporal contexts. We propose a variation\nof a Deep Embedded Clustering(DEC) model for finding spatio-temporal clusters.\nThe proposed model Spatial-DEC (S-DEC) use prior geographical information in\nbuilding latent feature representations. We also define evaluation metrics for\nspatio-temporal clusters. Not only do the obtained clusters have better\ntemporal similarity when evaluated using DTW distance, but also the clusters\nbetter represents spatial connectivity and dis-connectivity. We use traffic\nflow data obtained by PeMS in our analysis. The results show that the proposed\nSpatial-DEC can find more desired spatio-temporal clusters.",
          "link": "http://arxiv.org/abs/2107.01310",
          "publishedOn": "2021-07-06T01:58:09.419Z",
          "wordCount": 607,
          "title": "Clustering of Time Series Data with Prior Geographical Information. (arXiv:2107.01310v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.07248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ustimenko_A/0/1/0/all/0/1\">Aleksei Ustimenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1\">Liudmila Prokhorenkova</a>",
          "description": "This paper introduces Stochastic Gradient Langevin Boosting (SGLB) - a\npowerful and efficient machine learning framework that may deal with a wide\nrange of loss functions and has provable generalization guarantees. The method\nis based on a special form of the Langevin diffusion equation specifically\ndesigned for gradient boosting. This allows us to theoretically guarantee the\nglobal convergence even for multimodal loss functions, while standard gradient\nboosting algorithms can guarantee only local optimum. We also empirically show\nthat SGLB outperforms classic gradient boosting when applied to classification\ntasks with 0-1 loss function, which is known to be multimodal.",
          "link": "http://arxiv.org/abs/2001.07248",
          "publishedOn": "2021-07-06T01:58:09.408Z",
          "wordCount": 568,
          "title": "SGLB: Stochastic Gradient Langevin Boosting. (arXiv:2001.07248v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1807.04209",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Dwork_C/0/1/0/all/0/1\">Cynthia Dwork</a>, <a href=\"http://arxiv.org/find/math/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>",
          "description": "Differential privacy provides a rigorous framework for privacy-preserving\ndata analysis. This paper proposes the first differentially private procedure\nfor controlling the false discovery rate (FDR) in multiple hypothesis testing.\nInspired by the Benjamini-Hochberg procedure (BHq), our approach is to first\nrepeatedly add noise to the logarithms of the $p$-values to ensure differential\nprivacy and to select an approximately smallest $p$-value serving as a\npromising candidate at each iteration; the selected $p$-values are further\nsupplied to the BHq and our private procedure releases only the rejected ones.\nMoreover, we develop a new technique that is based on a backward submartingale\nfor proving FDR control of a broad class of multiple testing procedures,\nincluding our private procedure, and both the BHq step-up and step-down\nprocedures. As a novel aspect, the proof works for arbitrary dependence between\nthe true null and false null test statistics, while FDR control is maintained\nup to a small multiplicative factor.",
          "link": "http://arxiv.org/abs/1807.04209",
          "publishedOn": "2021-07-06T01:58:09.400Z",
          "wordCount": 612,
          "title": "Differentially Private False Discovery Rate Control. (arXiv:1807.04209v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leibfried_F/0/1/0/all/0/1\">Felix Leibfried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutordoir_V/0/1/0/all/0/1\">Vincent Dutordoir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+John_S/0/1/0/all/0/1\">ST John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrande_N/0/1/0/all/0/1\">Nicolas Durrande</a>",
          "description": "Gaussian processes (GPs) provide a framework for Bayesian inference that can\noffer principled uncertainty estimates for a large range of problems. For\nexample, if we consider regression problems with Gaussian likelihoods, a GP\nmodel enjoys a posterior in closed form. However, identifying the posterior GP\nscales cubically with the number of training examples and requires to store all\nexamples in memory. In order to overcome these obstacles, sparse GPs have been\nproposed that approximate the true posterior GP with pseudo-training examples.\nImportantly, the number of pseudo-training examples is user-defined and enables\ncontrol over computational and memory complexity. In the general case, sparse\nGPs do not enjoy closed-form solutions and one has to resort to approximate\ninference. In this context, a convenient choice for approximate inference is\nvariational inference (VI), where the problem of Bayesian inference is cast as\nan optimization problem -- namely, to maximize a lower bound of the log\nmarginal likelihood. This paves the way for a powerful and versatile framework,\nwhere pseudo-training examples are treated as optimization arguments of the\napproximate posterior that are jointly identified together with hyperparameters\nof the generative model (i.e. prior and likelihood). The framework can\nnaturally handle a wide scope of supervised learning problems, ranging from\nregression with heteroscedastic and non-Gaussian likelihoods to classification\nproblems with discrete labels, but also multilabel problems. The purpose of\nthis tutorial is to provide access to the basic matter for readers without\nprior knowledge in both GPs and VI. A proper exposition to the subject enables\nalso access to more recent advances (like importance-weighted VI as well as\ninterdomain, multioutput and deep GPs) that can serve as an inspiration for new\nresearch ideas.",
          "link": "http://arxiv.org/abs/2012.13962",
          "publishedOn": "2021-07-06T01:58:09.393Z",
          "wordCount": 825,
          "title": "A Tutorial on Sparse Gaussian Processes and Variational Inference. (arXiv:2012.13962v11 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vovk_V/0/1/0/all/0/1\">Vladimir Vovk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petej_I/0/1/0/all/0/1\">Ivan Petej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gammerman_A/0/1/0/all/0/1\">Alex Gammerman</a>",
          "description": "This note proposes a way of making probability forecasting rules less\nsensitive to changes in data distribution, concentrating on the simple case of\nbinary classification. This is important in applications of machine learning,\nwhere the quality of a trained predictor may drop significantly in the process\nof its exploitation. Our techniques are based on recent work on conformal test\nmartingales and older work on prediction with expert advice, namely tracking\nthe best expert.",
          "link": "http://arxiv.org/abs/2107.01726",
          "publishedOn": "2021-07-06T01:58:09.374Z",
          "wordCount": 507,
          "title": "Adaptive calibration for binary classification. (arXiv:2107.01726v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alguacil_A/0/1/0/all/0/1\">Antonio Alguacil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_W/0/1/0/all/0/1\">Wagner Gon&#xe7;alves Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauerheim_M/0/1/0/all/0/1\">Michael Bauerheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_M/0/1/0/all/0/1\">Marc C. Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_S/0/1/0/all/0/1\">St&#xe9;phane Moreau</a>",
          "description": "Accurate modeling of boundary conditions is crucial in computational physics.\nThe ever increasing use of neural networks as surrogates for physics-related\nproblems calls for an improved understanding of boundary condition treatment,\nand its influence on the network accuracy. In this paper, several strategies to\nimpose boundary conditions (namely padding, improved spatial context, and\nexplicit encoding of physical boundaries) are investigated in the context of\nfully convolutional networks applied to recurrent tasks. These strategies are\nevaluated on two spatio-temporal evolving problems modeled by partial\ndifferential equations: the 2D propagation of acoustic waves (hyperbolic PDE)\nand the heat equation (parabolic PDE). Results reveal a high sensitivity of\nboth accuracy and stability on the boundary implementation in such recurrent\ntasks. It is then demonstrated that the choice of the optimal padding strategy\nis directly linked to the data semantics. Furthermore, the inclusion of\nadditional input spatial context or explicit physics-based rules allows a\nbetter handling of boundaries in particular for large number of recurrences,\nresulting in more robust and stable neural networks, while facilitating the\ndesign and versatility of such networks.",
          "link": "http://arxiv.org/abs/2106.11160",
          "publishedOn": "2021-07-06T01:58:09.330Z",
          "wordCount": 668,
          "title": "Effects of boundary conditions in fully convolutional networks for learning spatio-temporal dynamics. (arXiv:2106.11160v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.10190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1\">Zeyuan Allen-Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>",
          "description": "Despite the empirical success of using Adversarial Training to defend deep\nlearning models against adversarial perturbations, so far, it still remains\nrather unclear what the principles are behind the existence of adversarial\nperturbations, and what adversarial training does to the neural network to\nremove them.\n\nIn this paper, we present a principle that we call Feature Purification,\nwhere we show one of the causes of the existence of adversarial examples is the\naccumulation of certain small dense mixtures in the hidden weights during the\ntraining process of a neural network; and more importantly, one of the goals of\nadversarial training is to remove such mixtures to purify hidden weights. We\npresent both experiments on the CIFAR-10 dataset to illustrate this principle,\nand a theoretical result proving that for certain natural classification tasks,\ntraining a two-layer neural network with ReLU activation using randomly\ninitialized gradient descent indeed satisfies this principle.\n\nTechnically, we give, to the best of our knowledge, the first result proving\nthat the following two can hold simultaneously for training a neural network\nwith ReLU activation. (1) Training over the original data is indeed non-robust\nto small adversarial perturbations of some radius. (2) Adversarial training,\neven with an empirical perturbation algorithm such as FGM, can in fact be\nprovably robust against ANY perturbations of the same radius. Finally, we also\nprove a complexity lower bound, showing that low complexity models such as\nlinear classifiers, low-degree polynomials, or even the neural tangent kernel\nfor this network, CANNOT defend against perturbations of this same radius, no\nmatter what algorithms are used to train them.",
          "link": "http://arxiv.org/abs/2005.10190",
          "publishedOn": "2021-07-06T01:58:09.317Z",
          "wordCount": 752,
          "title": "Feature Purification: How Adversarial Training Performs Robust Deep Learning. (arXiv:2005.10190v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Ye Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_V/0/1/0/all/0/1\">Vincent Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Songfu Cai</a>",
          "description": "Sparse coding is a class of unsupervised methods for learning a sparse\nrepresentation of the input data in the form of a linear combination of a\ndictionary and a sparse code. This learning framework has led to\nstate-of-the-art results in various image and video processing tasks. However,\nclassical methods learn the dictionary and the sparse code based on alternative\noptimizations, usually without theoretical guarantees for either optimality or\nconvergence due to non-convexity of the problem. Recent works on sparse coding\nwith a complete dictionary provide strong theoretical guarantees thanks to the\ndevelopment of the non-convex optimization. However, initial non-convex\napproaches learn the dictionary in the sparse coding problem sequentially in an\natom-by-atom manner, which leads to a long execution time. More recent works\nseek to directly learn the entire dictionary at once, which substantially\nreduces the execution time. However, the associated recovery performance is\ndegraded with a finite number of data samples. In this paper, we propose an\nefficient sparse coding scheme with a two-stage optimization. The proposed\nscheme leverages the global and local Riemannian geometry of the two-stage\noptimization problem and facilitates fast implementation for superb dictionary\nrecovery performance by a finite number of samples without atom-by-atom\ncalculation. We further prove that, with high probability, the proposed scheme\ncan exactly recover any atom in the target dictionary with a finite number of\nsamples if it is adopted to recover one atom of the dictionary. An application\non wireless sensor data compression is also proposed. Experiments on both\nsynthetic and real-world data verify the efficiency and effectiveness of the\nproposed scheme.",
          "link": "http://arxiv.org/abs/2104.10314",
          "publishedOn": "2021-07-06T01:58:09.306Z",
          "wordCount": 744,
          "title": "Efficient Sparse Coding using Hierarchical Riemannian Pursuit. (arXiv:2104.10314v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+el_Bouri_R/0/1/0/all/0/1\">Rasheed el-Bouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tingting Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David A. Clifton</a>",
          "description": "Given the abundance and ease of access of personal data today, individual\nprivacy has become of paramount importance, particularly in the healthcare\ndomain. In this work, we aim to utilise patient data extracted from multiple\nhospital data centres to train a machine learning model without sacrificing\npatient privacy. We develop a scheduling algorithm in conjunction with a\nstudent-teacher algorithm that is deployed in a federated manner. This allows a\ncentral model to learn from batches of data at each federal node. The teacher\nacts between data centres to update the main task (student) algorithm using the\ndata that is stored in the various data centres. We show that the scheduler,\ntrained using meta-gradients, can effectively organise training and as a result\ntrain a machine learning model on a diverse dataset without needing explicit\naccess to the patient data. We achieve state-of-the-art performance and show\nhow our method overcomes some of the problems faced in the federated learning\nsuch as node poisoning. We further show how the scheduler can be used as a\nmechanism for transfer learning, allowing different teachers to work together\nin training a student for state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2107.01707",
          "publishedOn": "2021-07-06T01:58:09.281Z",
          "wordCount": 638,
          "title": "Towards Scheduling Federated Deep Learning using Meta-Gradients for Inter-Hospital Learning. (arXiv:2107.01707v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01562",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hanin_B/0/1/0/all/0/1\">Boris Hanin</a>",
          "description": "This article gives a new proof that fully connected neural networks with\nrandom weights and biases converge to Gaussian processes in the regime where\nthe input dimension, output dimension, and depth are kept fixed, while the\nhidden layer widths tend to infinity. Unlike prior work, convergence is shown\nassuming only moment conditions for the distribution of weights and for quite\ngeneral non-linearities.",
          "link": "http://arxiv.org/abs/2107.01562",
          "publishedOn": "2021-07-06T01:58:09.273Z",
          "wordCount": 502,
          "title": "Random Neural Networks in the Infinite Width Limit as Gaussian Processes. (arXiv:2107.01562v1 [math.PR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mougan_C/0/1/0/all/0/1\">Carlos Mougan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masip_D/0/1/0/all/0/1\">David Masip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nin_J/0/1/0/all/0/1\">Jordi Nin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujol_O/0/1/0/all/0/1\">Oriol Pujol</a>",
          "description": "Regression problems have been widely studied in machinelearning literature\nresulting in a plethora of regression models and performance measures. However,\nthere are few techniques specially dedicated to solve the problem of how to\nincorporate categorical features to regression problems. Usually, categorical\nfeature encoders are general enough to cover both classification and regression\nproblems. This lack of specificity results in underperforming regression\nmodels. In this paper,we provide an in-depth analysis of how to tackle high\ncardinality categor-ical features with the quantile. Our proposal outperforms\nstate-of-the-encoders, including the traditional statistical mean target\nencoder, when considering the Mean Absolute Error, especially in the presence\nof long-tailed or skewed distributions. Besides, to deal with possible\noverfitting when there are categories with small support, our encoder benefits\nfrom additive smoothing. Finally, we describe how to expand the encoded values\nby creating a set of features with different quantiles. This expanded encoder\nprovides a more informative output about the categorical feature in question,\nfurther boosting the performance of the regression model.",
          "link": "http://arxiv.org/abs/2105.13783",
          "publishedOn": "2021-07-06T01:58:09.267Z",
          "wordCount": 647,
          "title": "Quantile Encoder: Tackling High Cardinality Categorical Features in Regression Problems. (arXiv:2105.13783v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saini_U/0/1/0/all/0/1\">Uday Singh Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devineni_P/0/1/0/all/0/1\">Pravallika Devineni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1\">Evangelos E. Papalexakis</a>",
          "description": "Tools to analyze the latent space of deep neural networks provide a step\ntowards better understanding them. In this work, we motivate sparse subspace\nclustering (SSC) with an aim to learn affinity graphs from the latent structure\nof a given neural network layer trained over a set of inputs. We then use tools\nfrom Community Detection to quantify structures present in the input. These\nexperiments reveal that as we go deeper in a network, inputs tend to have an\nincreasing affinity to other inputs of the same class. Subsequently, we utilise\nmatrix similarity measures to perform layer-wise comparisons between affinity\ngraphs. In doing so we first demonstrate that when comparing a given layer\ncurrently under training to its final state, the shallower the layer of the\nnetwork, the quicker it is to converge than the deeper layers. When performing\na pairwise analysis of the entire network architecture, we observe that, as the\nnetwork increases in size, it reorganises from a state where each layer is\nmoderately similar to its neighbours, to a state where layers within a block\nhave high similarity than to layers in other blocks. Finally, we analyze the\nlearned affinity graphs of the final convolutional layer of the network and\ndemonstrate how an input's local neighbourhood affects its classification by\nthe network.",
          "link": "http://arxiv.org/abs/2107.01296",
          "publishedOn": "2021-07-06T01:58:09.260Z",
          "wordCount": 643,
          "title": "Subspace Clustering Based Analysis of Neural Networks. (arXiv:2107.01296v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sedova_A/0/1/0/all/0/1\">Anastasiia Sedova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stephan_A/0/1/0/all/0/1\">Andreas Stephan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speranskaya_M/0/1/0/all/0/1\">Marina Speranskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>",
          "description": "Strategies for improving the training and prediction quality of weakly\nsupervised machine learning models vary in how much they are tailored to a\nspecific task or integrated with a specific model architecture. In this work,\nwe introduce Knodle, a software framework that treats weak data annotations,\ndeep learning models, and methods for improving weakly supervised training as\nseparate, modular components. This modularization gives the training process\naccess to fine-grained information such as data set characteristics, matches of\nheuristic rules, or elements of the deep learning model ultimately used for\nprediction. Hence, our framework can encompass a wide range of training methods\nfor improving weak supervision, ranging from methods that only look at\ncorrelations of rules and output classes (independently of the machine learning\nmodel trained with the resulting labels), to those that harness the interplay\nof neural networks and weakly labeled data. We illustrate the benchmarking\npotential of the framework with a performance comparison of several reference\nimplementations on a selection of datasets that are already available in\nKnodle.\n\nThe framework is published as an open-source Python package knodle and\navailable at https://github.com/knodle/knodle.",
          "link": "http://arxiv.org/abs/2104.11557",
          "publishedOn": "2021-07-06T01:58:09.244Z",
          "wordCount": 652,
          "title": "Knodle: Modular Weakly Supervised Learning with PyTorch. (arXiv:2104.11557v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xueying Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>",
          "description": "Active learning aims to achieve greater accuracy with less training data by\nselecting the most useful data samples from which it learns. Single-criterion\nbased methods (i.e., informativeness and representativeness based methods) are\nsimple and efficient; however, they lack adaptability to different real-world\nscenarios. In this paper, we introduce a multiple-criteria based active\nlearning algorithm, which incorporates three complementary criteria, i.e.,\ninformativeness, representativeness and diversity, to make appropriate\nselections in the active learning rounds under different data types. We\nconsider the selection process as a Determinantal Point Process, which good\nbalance among these criteria. We refine the query selection strategy by both\nselecting the hardest unlabeled data sample and biasing towards the classifiers\nthat are more suitable for the current data distribution. In addition, we also\nconsider the dependencies and relationships between these data points in data\nselection by means of centroidbased clustering approaches. Through evaluations\non synthetic and real-world datasets, we show that our method performs\nsignificantly better and is more stable than other multiple-criteria based AL\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.01622",
          "publishedOn": "2021-07-06T01:58:09.235Z",
          "wordCount": 599,
          "title": "Multiple-criteria Based Active Learning with Fixed-size Determinantal Point Processes. (arXiv:2107.01622v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Botteghi_N/0/1/0/all/0/1\">Nicol&#xf2; Botteghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poel_M/0/1/0/all/0/1\">Mannes Poel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirmacek_B/0/1/0/all/0/1\">Beril Sirmacek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1\">Christoph Brune</a>",
          "description": "Deep Reinforcement Learning has shown its ability in solving complicated\nproblems directly from high-dimensional observations. However, in end-to-end\nsettings, Reinforcement Learning algorithms are not sample-efficient and\nrequires long training times and quantities of data. In this work, we proposed\na framework for sample-efficient Reinforcement Learning that take advantage of\nstate and action representations to transform a high-dimensional problem into a\nlow-dimensional one. Moreover, we seek to find the optimal policy mapping\nlatent states to latent actions. Because now the policy is learned on abstract\nrepresentations, we enforce, using auxiliary loss functions, the lifting of\nsuch policy to the original problem domain. Results show that the novel\nframework can efficiently learn low-dimensional and interpretable state and\naction representations and the optimal latent policy.",
          "link": "http://arxiv.org/abs/2107.01677",
          "publishedOn": "2021-07-06T01:58:09.216Z",
          "wordCount": 560,
          "title": "Low-Dimensional State and Action Representation Learning with MDP Homomorphism Metrics. (arXiv:2107.01677v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ultsch_A/0/1/0/all/0/1\">Alfred Ultsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">J&#xf6;rg Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohnert_M/0/1/0/all/0/1\">Maximilian R&#xf6;hnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonin_M/0/1/0/all/0/1\">Malte Von Bonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oelschlagel_U/0/1/0/all/0/1\">Uta Oelschl&#xe4;gel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_C/0/1/0/all/0/1\">Cornelia Brendel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrun_M/0/1/0/all/0/1\">Michael C. Thrun</a>",
          "description": "Typical state of the art flow cytometry data samples consists of measures of\nmore than 100.000 cells in 10 or more features. AI systems are able to diagnose\nsuch data with almost the same accuracy as human experts. However, there is one\ncentral challenge in such systems: their decisions have far-reaching\nconsequences for the health and life of people, and therefore, the decisions of\nAI systems need to be understandable and justifiable by humans. In this work,\nwe present a novel explainable AI method, called ALPODS, which is able to\nclassify (diagnose) cases based on clusters, i.e., subpopulations, in the\nhigh-dimensional data. ALPODS is able to explain its decisions in a form that\nis understandable for human experts. For the identified subpopulations, fuzzy\nreasoning rules expressed in the typical language of domain experts are\ngenerated. A visualization method based on these rules allows human experts to\nunderstand the reasoning used by the AI system. A comparison to a selection of\nstate of the art explainable AI systems shows that ALPODS operates efficiently\non known benchmark data and also on everyday routine case data.",
          "link": "http://arxiv.org/abs/2107.01820",
          "publishedOn": "2021-07-06T01:58:09.209Z",
          "wordCount": 657,
          "title": "An Explainable AI System for the Diagnosis of High Dimensional Biomedical Data. (arXiv:2107.01820v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Putra_R/0/1/0/all/0/1\">Rachmad Vidya Wicaksana Putra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1\">Muhammad Shafique</a>",
          "description": "A prominent technique for reducing the memory footprint of Spiking Neural\nNetworks (SNNs) without decreasing the accuracy significantly is quantization.\nHowever, the state-of-the-art only focus on employing the weight quantization\ndirectly from a specific quantization scheme, i.e., either the post-training\nquantization (PTQ) or the in-training quantization (ITQ), and do not consider\n(1) quantizing other SNN parameters (e.g., neuron membrane potential), (2)\nexploring different combinations of quantization approaches (i.e., quantization\nschemes, precision levels, and rounding schemes), and (3) selecting the SNN\nmodel with a good memory-accuracy trade-off at the end. Therefore, the memory\nsaving offered by these state-of-the-art to meet the targeted accuracy is\nlimited, thereby hindering processing SNNs on the resource-constrained systems\n(e.g., the IoT-Edge devices). Towards this, we propose Q-SpiNN, a novel\nquantization framework for memory-efficient SNNs. The key mechanisms of the\nQ-SpiNN are: (1) employing quantization for different SNN parameters based on\ntheir significance to the accuracy, (2) exploring different combinations of\nquantization schemes, precision levels, and rounding schemes to find efficient\nSNN model candidates, and (3) developing an algorithm that quantifies the\nbenefit of the memory-accuracy trade-off obtained by the candidates, and\nselects the Pareto-optimal one. The experimental results show that, for the\nunsupervised network, the Q-SpiNN reduces the memory footprint by ca. 4x, while\nmaintaining the accuracy within 1% from the baseline on the MNIST dataset. For\nthe supervised network, the Q-SpiNN reduces the memory by ca. 2x, while keeping\nthe accuracy within 2% from the baseline on the DVS-Gesture dataset.",
          "link": "http://arxiv.org/abs/2107.01807",
          "publishedOn": "2021-07-06T01:58:09.202Z",
          "wordCount": 705,
          "title": "Q-SpiNN: A Framework for Quantizing Spiking Neural Networks. (arXiv:2107.01807v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shachaf_G/0/1/0/all/0/1\">Gal Shachaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brutzkus_A/0/1/0/all/0/1\">Alon Brutzkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>",
          "description": "Fine-tuning is a common practice in deep learning, achieving excellent\ngeneralization results on downstream tasks using relatively little training\ndata. Although widely used in practice, it is lacking strong theoretical\nunderstanding. We analyze the sample complexity of this scheme for regression\nwith linear teachers in several architectures. Intuitively, the success of\nfine-tuning depends on the similarity between the source tasks and the target\ntask, however measuring it is non trivial. We show that a relevant measure\nconsiders the relation between the source task, the target task and the\ncovariance structure of the target data. In the setting of linear regression,\nwe show that under realistic settings a substantial sample complexity reduction\nis plausible when the above measure is low. For deep linear regression, we\npresent a novel result regarding the inductive bias of gradient-based training\nwhen the network is initialized with pretrained weights. Using this result we\nshow that the similarity measure for this setting is also affected by the depth\nof the network. We further present results on shallow ReLU models, and analyze\nthe dependence of sample complexity there on source and target tasks. We\nempirically demonstrate our results for both synthetic and realistic data.",
          "link": "http://arxiv.org/abs/2107.01641",
          "publishedOn": "2021-07-06T01:58:09.176Z",
          "wordCount": 624,
          "title": "A Theoretical Analysis of Fine-tuning with Linear Teachers. (arXiv:2107.01641v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nilsen_G/0/1/0/all/0/1\">Geir K. Nilsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munthe_Kaas_A/0/1/0/all/0/1\">Antonella Z. Munthe-Kaas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skaug_H/0/1/0/all/0/1\">Hans J. Skaug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brun_M/0/1/0/all/0/1\">Morten Brun</a>",
          "description": "We validate the recently introduced deep learning classification adapted\nDelta method by a comparison with the classical Bootstrap. We show that there\nis a strong linear relationship between the quantified predictive epistemic\nuncertainty levels obtained from the two methods when applied on two\nLeNet-based neural network classifiers using the MNIST and CIFAR-10 datasets.\nFurthermore, we demonstrate that the Delta method offers a five times\ncomputation time reduction compared to the Bootstrap.",
          "link": "http://arxiv.org/abs/2107.01606",
          "publishedOn": "2021-07-06T01:58:09.169Z",
          "wordCount": 521,
          "title": "A Comparison of the Delta Method and the Bootstrap in Deep Learning Classification. (arXiv:2107.01606v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1\">Zeyuan Allen-Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>",
          "description": "We formally study how ensemble of deep learning models can improve test\naccuracy, and how the superior performance of ensemble can be distilled into a\nsingle model using knowledge distillation. We consider the challenging case\nwhere the ensemble is simply an average of the outputs of a few independently\ntrained neural networks with the SAME architecture, trained using the SAME\nalgorithm on the SAME data set, and they only differ by the random seeds used\nin the initialization.\n\nWe empirically show that ensemble/knowledge distillation in deep learning\nworks very differently from traditional learning theory, especially differently\nfrom ensemble of random feature mappings or the neural-tangent-kernel feature\nmappings, and is potentially out of the scope of existing theorems. Thus, to\nproperly understand ensemble and knowledge distillation in deep learning, we\ndevelop a theory showing that when data has a structure we refer to as\n\"multi-view\", then ensemble of independently trained neural networks can\nprovably improve test accuracy, and such superior test accuracy can also be\nprovably distilled into a single model by training a single model to match the\noutput of the ensemble instead of the true label. Our result sheds light on how\nensemble works in deep learning in a way that is completely different from\ntraditional theorems, and how the \"dark knowledge\" is hidden in the outputs of\nthe ensemble -- that can be used in knowledge distillation -- comparing to the\ntrue data labels. In the end, we prove that self-distillation can also be\nviewed as implicitly combining ensemble and knowledge distillation to improve\ntest accuracy.",
          "link": "http://arxiv.org/abs/2012.09816",
          "publishedOn": "2021-07-06T01:58:09.105Z",
          "wordCount": 736,
          "title": "Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning. (arXiv:2012.09816v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rakotomamonjy_A/0/1/0/all/0/1\">Alain Rakotomamonjy</a> (DocApp - LITIS), <a href=\"http://arxiv.org/find/cs/1/au:+Ralaivola_L/0/1/0/all/0/1\">Liva Ralaivola</a>",
          "description": "Developing machine learning methods that are privacy preserving is today a\ncentral topic of research, with huge practical impacts. Among the numerous ways\nto address privacy-preserving learning, we here take the perspective of\ncomputing the divergences between distributions under the Differential Privacy\n(DP) framework -- being able to compute divergences between distributions is\npivotal for many machine learning problems, such as learning generative models\nor domain adaptation problems. Instead of resorting to the popular\ngradient-based sanitization method for DP, we tackle the problem at its roots\nby focusing on the Sliced Wasserstein Distance and seamlessly making it\ndifferentially private. Our main contribution is as follows: we analyze the\nproperty of adding a Gaussian perturbation to the intrinsic randomized\nmechanism of the Sliced Wasserstein Distance, and we establish the\nsensitivityof the resulting differentially private mechanism. One of our\nimportant findings is that this DP mechanism transforms the Sliced Wasserstein\ndistance into another distance, that we call the Smoothed Sliced Wasserstein\nDistance. This new differentially private distribution distance can be plugged\ninto generative models and domain adaptation algorithms in a transparent way,\nand we empirically show that it yields highly competitive performance compared\nwith gradient-based DP approaches from the literature, with almost no loss in\naccuracy for the domain adaptation problems that we consider.",
          "link": "http://arxiv.org/abs/2107.01848",
          "publishedOn": "2021-07-06T01:58:09.091Z",
          "wordCount": 654,
          "title": "Differentially Private Sliced Wasserstein Distance. (arXiv:2107.01848v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heiss_J/0/1/0/all/0/1\">Jakob Heiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teichmann_J/0/1/0/all/0/1\">Josef Teichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutte_H/0/1/0/all/0/1\">Hanna Wutte</a>",
          "description": "Today, various forms of neural networks are trained to perform approximation\ntasks in many fields. However, the estimates obtained are not fully understood\non function space. Empirical results suggest that typical training algorithms\nfavor regularized solutions. These observations motivate us to analyze\nproperties of the neural networks found by gradient descent initialized close\nto zero, that is frequently employed to perform the training task. As a\nstarting point, we consider one dimensional (shallow) ReLU neural networks in\nwhich weights are chosen randomly and only the terminal layer is trained.\nFirst, we rigorously show that for such networks ridge regularized regression\ncorresponds in function space to regularizing the estimate's second derivative\nfor fairly general loss functionals. For least squares regression, we show that\nthe trained network converges to the smooth spline interpolation of the\ntraining data as the number of hidden nodes tends to infinity. Moreover, we\nderive a correspondence between the early stopped gradient descent and the\nsmoothing spline regression. Our analysis might give valuable insight on the\nproperties of the solutions obtained using gradient descent methods in general\nsettings.",
          "link": "http://arxiv.org/abs/1911.02903",
          "publishedOn": "2021-07-06T01:58:09.074Z",
          "wordCount": 704,
          "title": "How Implicit Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part I: the 1-D Case of Two Layers with Random First Layer. (arXiv:1911.02903v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Transfer-based adversarial attacks can effectively evaluate model robustness\nin the black-box setting. Though several methods have demonstrated impressive\ntransferability of untargeted adversarial examples, targeted adversarial\ntransferability is still challenging. The existing methods either have low\ntargeted transferability or sacrifice computational efficiency. In this paper,\nwe develop a simple yet practical framework to efficiently craft targeted\ntransfer-based adversarial examples. Specifically, we propose a conditional\ngenerative attacking model, which can generate the adversarial examples\ntargeted at different classes by simply altering the class embedding and share\na single backbone. Extensive experiments demonstrate that our method improves\nthe success rates of targeted black-box attacks by a significant margin over\nthe existing methods -- it reaches an average success rate of 29.6\\% against\nsix diverse models based only on one substitute white-box model in the standard\ntesting of NeurIPS 2017 competition, which outperforms the state-of-the-art\ngradient-based attack methods (with an average success rate of $<$2\\%) by a\nlarge margin. Moreover, the proposed method is also more efficient beyond an\norder of magnitude than gradient-based methods.",
          "link": "http://arxiv.org/abs/2107.01809",
          "publishedOn": "2021-07-06T01:58:09.061Z",
          "wordCount": 611,
          "title": "Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks. (arXiv:2107.01809v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lawless_C/0/1/0/all/0/1\">Connor Lawless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunluk_O/0/1/0/all/0/1\">Oktay Gunluk</a>",
          "description": "In recent years, machine learning has begun automating decision making in\nfields as varied as college admissions, credit lending, and criminal\nsentencing. The socially sensitive nature of some of these applications\ntogether with increasing regulatory constraints has necessitated the need for\nalgorithms that are both fair and interpretable. In this paper we consider the\nproblem of building Boolean rule sets in disjunctive normal form (DNF), an\ninterpretable model for binary classification, subject to fairness constraints.\nWe formulate the problem as an integer program that maximizes classification\naccuracy with explicit constraints on two different measures of classification\nparity: equality of opportunity and equalized odds. Column generation\nframework, with a novel formulation, is used to efficiently search over\nexponentially many possible rules. When combined with faster heuristics, our\nmethod can deal with large data-sets. Compared to other fair and interpretable\nclassifiers, our method is able to find rule sets that meet stricter notions of\nfairness with a modest trade-off in accuracy.",
          "link": "http://arxiv.org/abs/2107.01325",
          "publishedOn": "2021-07-06T01:58:08.884Z",
          "wordCount": 590,
          "title": "Fair Decision Rules for Binary Classification. (arXiv:2107.01325v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pauloski_J/0/1/0/all/0/1\">J. Gregory Pauloski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1\">Shivaram Venkataraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chard_K/0/1/0/all/0/1\">Kyle Chard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1\">Ian Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>",
          "description": "Kronecker-factored Approximate Curvature (K-FAC) has recently been shown to\nconverge faster in deep neural network (DNN) training than stochastic gradient\ndescent (SGD); however, K-FAC's larger memory footprint hinders its\napplicability to large models. We present KAISA, a K-FAC-enabled, Adaptable,\nImproved, and ScAlable second-order optimizer framework that adapts the memory\nfootprint, communication, and computation given specific models and hardware to\nachieve maximized performance and enhanced scalability. We quantify the\ntradeoffs between memory and communication cost and evaluate KAISA on large\nmodels, including ResNet-50, Mask R-CNN, U-Net, and BERT, on up to 128 NVIDIA\nA100 GPUs. Compared to the original optimizers, KAISA converges 18.1-36.3%\nfaster across applications with the same global batch size. Under a fixed\nmemory budget, KAISA converges 32.5% and 41.6% faster in ResNet-50 and\nBERT-Large, respectively. KAISA can balance memory and communication to achieve\nscaling efficiency equal to or better than the baseline optimizers.",
          "link": "http://arxiv.org/abs/2107.01739",
          "publishedOn": "2021-07-06T01:58:08.876Z",
          "wordCount": 613,
          "title": "KAISA: An Adaptive Second-order Optimizer Framework for Deep Neural Networks. (arXiv:2107.01739v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Yeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Wan Choi</a>",
          "description": "Continual learning has been a major problem in the deep learning community,\nwhere the main challenge is how to effectively learn a series of newly arriving\ntasks without forgetting the knowledge of previous tasks. Initiated by Learning\nwithout Forgetting (LwF), many of the existing works report that knowledge\ndistillation is effective to preserve the previous knowledge, and hence they\ncommonly use a soft label for the old task, namely a knowledge distillation\n(KD) loss, together with a class label for the new task, namely a cross entropy\n(CE) loss, to form a composite loss for a single neural network. However, this\napproach suffers from learning the knowledge by a CE loss as a KD loss often\nmore strongly influences the objective function when they are in a competitive\nsituation within a single network. This could be a critical problem\nparticularly in a class incremental scenario, where the knowledge across tasks\nas well as within the new task, both of which can only be acquired by a CE\nloss, is essentially learned due to the existence of a unified classifier. In\nthis paper, we propose a novel continual learning method, called\nSplit-and-Bridge, which can successfully address the above problem by partially\nsplitting a neural network into two partitions for training the new task\nseparated from the old task and re-connecting them for learning the knowledge\nacross tasks. In our thorough experimental analysis, our Split-and-Bridge\nmethod outperforms the state-of-the-art competitors in KD-based continual\nlearning.",
          "link": "http://arxiv.org/abs/2107.01349",
          "publishedOn": "2021-07-06T01:58:08.870Z",
          "wordCount": 704,
          "title": "Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network. (arXiv:2107.01349v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>",
          "description": "Modeling complex physical dynamics is a fundamental task in science and\nengineering. Traditional physics-based models are interpretable but rely on\nrigid assumptions. And the direct numerical approximation is usually\ncomputationally intensive, requiring significant computational resources and\nexpertise. While deep learning (DL) provides novel alternatives for efficiently\nrecognizing complex patterns and emulating nonlinear dynamics, it does not\nnecessarily obey the governing laws of physical systems, nor do they generalize\nwell across different systems. Thus, the study of physics-guided DL emerged and\nhas gained great progress. It aims to take the best from both physics-based\nmodeling and state-of-the-art DL models to better solve scientific problems. In\nthis paper, we provide a structured overview of existing methodologies of\nintegrating prior physical knowledge or physics-based modeling into DL and\ndiscuss the emerging opportunities.",
          "link": "http://arxiv.org/abs/2107.01272",
          "publishedOn": "2021-07-06T01:58:08.863Z",
          "wordCount": 553,
          "title": "Physics-Guided Deep Learning for Dynamical Systems: A survey. (arXiv:2107.01272v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Telukunta_M/0/1/0/all/0/1\">Mukund Telukunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadendla_V/0/1/0/all/0/1\">Venkata Sriram Siddhardh Nadendla</a>",
          "description": "Bias evaluation in machine-learning based services (MLS) based on traditional\nalgorithmic fairness notions that rely on comparative principles is practically\ndifficult, making it necessary to rely on human auditor feedback. However, in\nspite of taking rigorous training on various comparative fairness notions,\nhuman auditors are known to disagree on various aspects of fairness notions in\npractice, making it difficult to collect reliable feedback. This paper offers a\nparadigm shift to the domain of algorithmic fairness via proposing a new\nfairness notion based on the principle of non-comparative justice. In contrary\nto traditional fairness notions where the outcomes of two individuals/groups\nare compared, our proposed notion compares the MLS' outcome with a desired\noutcome for each input. This desired outcome naturally describes a human\nauditor's expectation, and can be easily used to evaluate MLS on crowd-auditing\nplatforms. We show that any MLS can be deemed fair from the perspective of\ncomparative fairness (be it in terms of individual fairness, statistical\nparity, equal opportunity or calibration) if it is non-comparatively fair with\nrespect to a fair auditor. We also show that the converse holds true in the\ncontext of individual fairness. Given that such an evaluation relies on the\ntrustworthiness of the auditor, we also present an approach to identify fair\nand reliable auditors by estimating their biases with respect to a given set of\nsensitive attributes, as well as quantify the uncertainty in the estimation of\nbiases within a given MLS. Furthermore, all of the above results are also\nvalidated on COMPAS, German credit and Adult Census Income datasets.",
          "link": "http://arxiv.org/abs/2107.01277",
          "publishedOn": "2021-07-06T01:58:08.833Z",
          "wordCount": 709,
          "title": "Non-Comparative Fairness for Human-Auditing and Its Relation to Traditional Fairness Notions. (arXiv:2107.01277v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hlavac_J/0/1/0/all/0/1\">Jaroslav Hlav&#xe1;&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_M/0/1/0/all/0/1\">Martin Kopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohout_J/0/1/0/all/0/1\">Jan Kohout</a>",
          "description": "The nearest prototype classification is a less computationally intensive\nreplacement for the $k$-NN method, especially when large datasets are\nconsidered. In metric spaces, centroids are often used as prototypes to\nrepresent whole clusters. The selection of cluster prototypes in non-metric\nspaces is more challenging as the idea of computing centroids is not directly\napplicable.\n\nIn this paper, we present CRS, a novel method for selecting a small yet\nrepresentative subset of objects as a cluster prototype. Memory and\ncomputationally efficient selection of representatives is enabled by leveraging\nthe similarity graph representation of each cluster created by the NN-Descent\nalgorithm. CRS can be used in an arbitrary metric or non-metric space because\nof the graph-based approach, which requires only a pairwise similarity measure.\nAs we demonstrate in the experimental evaluation, our method outperforms the\nstate of the art techniques on multiple datasets from different domains.",
          "link": "http://arxiv.org/abs/2107.01345",
          "publishedOn": "2021-07-06T01:58:08.827Z",
          "wordCount": 575,
          "title": "Cluster Representatives Selection in Non-Metric Spaces for Nearest Prototype Classification. (arXiv:2107.01345v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1\">M. A&#xdf;enmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corvonato_A/0/1/0/all/0/1\">A. Corvonato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heumann_C/0/1/0/all/0/1\">C. Heumann</a>",
          "description": "The lack of a commonly used benchmark data set (collection) such as\n(Super-)GLUE (Wang et al., 2018, 2019) for the evaluation of non-English\npre-trained language models is a severe shortcoming of current English-centric\nNLP-research. It concentrates a large part of the research on English,\nneglecting the uncertainty when transferring conclusions found for the English\nlanguage to other languages. We evaluate the performance of the German and\nmultilingual BERT-based models currently available via the huggingface\ntransformers library on the four tasks of the GermEval17 workshop. We compare\nthem to pre-BERT architectures (Wojatzki et al., 2017; Schmitt et al., 2018;\nAttia et al., 2018) as well as to an ELMo-based architecture (Biesialska et\nal., 2020) and a BERT-based approach (Guhr et al., 2020). The observed\nimprovements are put in relation to those for similar tasks and similar models\n(pre-BERT vs. BERT-based) for the English language in order to draw tentative\nconclusions about whether the observed improvements are transferable to German\nor potentially other related languages.",
          "link": "http://arxiv.org/abs/2102.12330",
          "publishedOn": "2021-07-06T01:58:08.819Z",
          "wordCount": 642,
          "title": "Re-Evaluating GermEval17 Using German Pre-Trained Language Models. (arXiv:2102.12330v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiexia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Furong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Juanjuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Kejiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>",
          "description": "Accurate traffic state prediction is the foundation of transportation control\nand guidance. It is very challenging due to the complex spatiotemporal\ndependencies in traffic data. Existing works cannot perform well for multi-step\ntraffic prediction that involves long future time period. The spatiotemporal\ninformation dilution becomes serve when the time gap between input step and\npredicted step is large, especially when traffic data is not sufficient or\nnoisy. To address this issue, we propose a multi-spatial graph convolution\nbased Seq2Seq model. Our main novelties are three aspects: (1) We enrich the\nspatiotemporal information of model inputs by fusing multi-view features (time,\nlocation and traffic states) (2) We build multiple kinds of spatial\ncorrelations based on both prior knowledge and data-driven knowledge to improve\nmodel performance especially in insufficient or noisy data cases. (3) A\nspatiotemporal attention mechanism based on reachability knowledge is novelly\ndesigned to produce high-level features fed into decoder of Seq2Seq directly to\nease information dilution. Our model is evaluated on two real world traffic\ndatasets and achieves better performance than other competitors.",
          "link": "http://arxiv.org/abs/2107.01528",
          "publishedOn": "2021-07-06T01:58:08.812Z",
          "wordCount": 626,
          "title": "Incorporating Reachability Knowledge into a Multi-Spatial Graph Convolution Based Seq2Seq Model for Traffic Forecasting. (arXiv:2107.01528v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palmes_P/0/1/0/all/0/1\">Paulito P. Palmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kishimoto_A/0/1/0/all/0/1\">Akihiro Kishimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinescu_R/0/1/0/all/0/1\">Radu Marinescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1\">Parikshit Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_E/0/1/0/all/0/1\">Elizabeth Daly</a>",
          "description": "The pipeline optimization problem in machine learning requires simultaneous\noptimization of pipeline structures and parameter adaptation of their elements.\nHaving an elegant way to express these structures can help lessen the\ncomplexity in the management and analysis of their performances together with\nthe different choices of optimization strategies. With these issues in mind, we\ncreated the AMLP toolkit which facilitates the creation and evaluation of\ncomplex machine learning pipeline structures using simple expressions. We use\nAMLP to find optimal pipeline signatures, datamine them, and use these\ndatamined features to speed-up learning and prediction. We formulated a\ntwo-stage pipeline optimization with surrogate modeling in AMLP which\noutperforms other AutoML approaches with a 4-hour time budget in less than 5\nminutes of AMLP computation time.",
          "link": "http://arxiv.org/abs/2107.01253",
          "publishedOn": "2021-07-06T01:58:08.806Z",
          "wordCount": 564,
          "title": "Designing Machine Learning Pipeline Toolkit for AutoML Surrogate Modeling Optimization. (arXiv:2107.01253v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01777",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Singh_S/0/1/0/all/0/1\">Shashank Singh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Khim_J/0/1/0/all/0/1\">Justin Khim</a>",
          "description": "Within the vast body of statistical theory developed for binary\nclassification, few meaningful results exist for imbalanced classification, in\nwhich data are dominated by samples from one of the two classes. Existing\ntheory faces at least two main challenges. First, meaningful results must\nconsider more complex performance measures than classification accuracy. To\naddress this, we characterize a novel generalization of the Bayes-optimal\nclassifier to any performance metric computed from the confusion matrix, and we\nuse this to show how relative performance guarantees can be obtained in terms\nof the error of estimating the class probability function under uniform\n($\\mathcal{L}_\\infty$) loss. Second, as we show, optimal classification\nperformance depends on certain properties of class imbalance that have not\npreviously been formalized. Specifically, we propose a novel sub-type of class\nimbalance, which we call Uniform Class Imbalance. We analyze how Uniform Class\nImbalance influences optimal classifier performance and show that it\nnecessitates different classifier behavior than other types of class imbalance.\nWe further illustrate these two contributions in the case of $k$-nearest\nneighbor classification, for which we develop novel guarantees. Together, these\nresults provide some of the first meaningful finite-sample statistical theory\nfor imbalanced binary classification.",
          "link": "http://arxiv.org/abs/2107.01777",
          "publishedOn": "2021-07-06T01:58:08.777Z",
          "wordCount": 637,
          "title": "Statistical Theory for Imbalanced Binary Classification. (arXiv:2107.01777v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shih-Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thilak_V/0/1/0/all/0/1\">Vimal Thilak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Littwin_E/0/1/0/all/0/1\">Etai Littwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saremi_O/0/1/0/all/0/1\">Omid Saremi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Joshua M. Susskind</a>",
          "description": "Deep linear networks trained with gradient descent yield low rank solutions,\nas is typically studied in matrix factorization. In this paper, we take a step\nfurther and analyze implicit rank regularization in autoencoders. We show\ngreedy learning of low-rank latent codes induced by a linear sub-network at the\nautoencoder bottleneck. We further propose orthogonal initialization and\nprincipled learning rate adjustment to mitigate sensitivity of training\ndynamics to spectral prior and linear depth. With linear autoencoders on\nsynthetic data, our method converges stably to ground-truth latent code rank.\nWith nonlinear autoencoders, our method converges to latent ranks optimal for\ndownstream classification and image sampling.",
          "link": "http://arxiv.org/abs/2107.01301",
          "publishedOn": "2021-07-06T01:58:08.770Z",
          "wordCount": 540,
          "title": "Implicit Greedy Rank Learning in Autoencoders via Overparameterized Linear Networks. (arXiv:2107.01301v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.02931",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Giampouras_P/0/1/0/all/0/1\">Paris V. Giampouras</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rontogiannis_A/0/1/0/all/0/1\">Athanasios A. Rontogiannis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kofidis_E/0/1/0/all/0/1\">Eleftherios Kofidis</a>",
          "description": "The so-called block-term decomposition (BTD) tensor model, especially in its\nrank-$(L_r,L_r,1)$ version, has been recently receiving increasing attention\ndue to its enhanced ability of representing systems and signals that are\ncomposed of \\emph{blocks} of rank higher than one, a scenario encountered in\nnumerous and diverse applications. Uniqueness conditions and fitting methods\nhave thus been thoroughly studied. Nevertheless, the challenging problem of\nestimating the BTD model structure, namely the number of block terms, $R$, and\ntheir individual ranks, $L_r$, has only recently started to attract significant\nattention, mainly through regularization-based approaches which entail the need\nto tune the regularization parameter(s). In this work, we build on ideas of\nsparse Bayesian learning (SBL) and put forward a fully automated Bayesian\napproach. Through a suitably crafted multi-level \\emph{hierarchical}\nprobabilistic model, which gives rise to heavy-tailed prior distributions for\nthe BTD factors, structured sparsity is \\emph{jointly} imposed. Ranks are then\nestimated from the numbers of blocks ($R$) and columns ($L_r$) of\nnon-negligible energy. Approximate posterior inference is implemented, within\nthe variational inference framework. The resulting iterative algorithm\ncompletely avoids hyperparameter tuning, which is a significant defect of\nregularization-based methods. Alternative probabilistic models are also\nexplored and the connections with their regularization-based counterparts are\nbrought to light with the aid of the associated maximum a-posteriori (MAP)\nestimators. We report simulation results with both synthetic and real-word\ndata, which demonstrate the merits of the proposed method in terms of both rank\nestimation and model fitting as compared to state-of-the-art relevant methods.",
          "link": "http://arxiv.org/abs/2101.02931",
          "publishedOn": "2021-07-06T01:58:08.764Z",
          "wordCount": 703,
          "title": "Block-Term Tensor Decomposition Model Selection and Computation: The Bayesian Way. (arXiv:2101.02931v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Grzegorz Dudek</a>",
          "description": "This work contributes to the development of a new data-driven method (D-DM)\nof feedforward neural networks (FNNs) learning. This method was proposed\nrecently as a way of improving randomized learning of FNNs by adjusting the\nnetwork parameters to the target function fluctuations. The method employs\nlogistic sigmoid activation functions for hidden nodes. In this study, we\nintroduce other activation functions, such as bipolar sigmoid, sine function,\nsaturating linear functions, reLU, and softplus. We derive formulas for their\nparameters, i.e. weights and biases. In the simulation study, we evaluate the\nperformance of FNN data-driven learning with different activation functions.\nThe results indicate that the sigmoid activation functions perform much better\nthan others in the approximation of complex, fluctuated target functions.",
          "link": "http://arxiv.org/abs/2107.01702",
          "publishedOn": "2021-07-06T01:58:08.753Z",
          "wordCount": 565,
          "title": "Data-Driven Learning of Feedforward Neural Networks with Different Activation Functions. (arXiv:2107.01702v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01502",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xinglong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_N/0/1/0/all/0/1\">Ning Huang</a>",
          "description": "Pulmonary vessel segmentation is important for clinical diagnosis of\npulmonary diseases, while is also challenging due to the complicated structure.\nIn this work, we present an effective framework and refinement process of\npulmonary vessel segmentation from chest computed tomographic (CT) images. The\nkey to our approach is a 2.5D segmentation network applied from three\northogonal axes, which presents a robust and fully automated pulmonary vessel\nsegmentation result with lower network complexity and memory usage compared to\n3D networks. The slice radius is introduced to convolve the adjacent\ninformation of the center slice and the multi-planar fusion optimizes the\npresentation of intra- and inter- slice features. Besides, the tree-like\nstructure of the pulmonary vessel is extracted in the post-processing process,\nwhich is used for segmentation refining and pruning. In the evaluation\nexperiments, three fusion methods are tested and the most promising one is\ncompared with the state-of-the-art 2D and 3D structures on 300 cases of lung\nimages randomly selected from LIDC dataset. Our method outperforms other\nnetwork structures by a large margin and achieves by far the highest average\nDICE score of 0.9272 and precision of 0.9310, as per our knowledge from the\npulmonary vessel segmentation models available in the literature.",
          "link": "http://arxiv.org/abs/2107.01502",
          "publishedOn": "2021-07-06T01:58:08.746Z",
          "wordCount": 677,
          "title": "Pulmonary Vessel Segmentation based on Orthogonal Fused U-Net++ of Chest CT Images. (arXiv:2107.01502v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhi_W/0/1/0/all/0/1\">Weiming Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_L/0/1/0/all/0/1\">Lionel Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonilla_E/0/1/0/all/0/1\">Edwin V. Bonilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1\">Fabio Ramos</a>",
          "description": "Advances in differentiable numerical integrators have enabled the use of\ngradient descent techniques to learn ordinary differential equations (ODEs). In\nthe context of machine learning, differentiable solvers are central for Neural\nODEs (NODEs), a class of deep learning models with continuous depth, rather\nthan discrete layers. However, these integrators can be unsatisfactorily slow\nand inaccurate when learning systems of ODEs from long sequences, or when\nsolutions of the system vary at widely different timescales in each dimension.\nIn this paper we propose an alternative approach to learning ODEs from data: we\nrepresent the underlying ODE as a vector field that is related to another base\nvector field by a differentiable bijection, modelled by an invertible neural\nnetwork. By restricting the base ODE to be amenable to integration, we can\ndrastically speed up and improve the robustness of integration. We demonstrate\nthe efficacy of our method in training and evaluating continuous neural\nnetworks models, as well as in learning benchmark ODE systems. We observe\nimprovements of up to two orders of magnitude when integrating learned ODEs\nwith GPUs computation.",
          "link": "http://arxiv.org/abs/2107.01650",
          "publishedOn": "2021-07-06T01:58:08.739Z",
          "wordCount": 612,
          "title": "Learning ODEs via Diffeomorphisms for Fast and Robust Integration. (arXiv:2107.01650v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baier_L/0/1/0/all/0/1\">Lucas Baier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlor_T/0/1/0/all/0/1\">Tim Schl&#xf6;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoffer_J/0/1/0/all/0/1\">Jakob Sch&#xf6;ffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1\">Niklas K&#xfc;hl</a>",
          "description": "Deployed machine learning models are confronted with the problem of changing\ndata over time, a phenomenon also called concept drift. While existing\napproaches of concept drift detection already show convincing results, they\nrequire true labels as a prerequisite for successful drift detection.\nEspecially in many real-world application scenarios-like the ones covered in\nthis work-true labels are scarce, and their acquisition is expensive.\nTherefore, we introduce a new algorithm for drift detection, Uncertainty Drift\nDetection (UDD), which is able to detect drifts without access to true labels.\nOur approach is based on the uncertainty estimates provided by a deep neural\nnetwork in combination with Monte Carlo Dropout. Structural changes over time\nare detected by applying the ADWIN technique on the uncertainty estimates, and\ndetected drifts trigger a retraining of the prediction model. In contrast to\ninput data-based drift detection, our approach considers the effects of the\ncurrent input data on the properties of the prediction model rather than\ndetecting change on the input data only (which can lead to unnecessary\nretrainings). We show that UDD outperforms other state-of-the-art strategies on\ntwo synthetic as well as ten real-world data sets for both regression and\nclassification tasks.",
          "link": "http://arxiv.org/abs/2107.01873",
          "publishedOn": "2021-07-06T01:58:08.714Z",
          "wordCount": 627,
          "title": "Detecting Concept Drift With Neural Network Model Uncertainty. (arXiv:2107.01873v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.14209",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ding_Z/0/1/0/all/0/1\">Zhiyan Ding</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>",
          "description": "Sampling from a log-concave distribution function on $\\mathbb{R}^d$ (with\n$d\\gg 1$) is a popular problem that has wide applications. In this paper we\nstudy the application of random coordinate descent method (RCD) on the Langevin\nMonte Carlo (LMC) sampling method, and we find two sides of the theory:\n\n1. The direct application of RCD on LMC does reduce the number of finite\ndifferencing approximations per iteration, but it induces a large variance\nerror term. More iterations are then needed, and ultimately the method gains no\ncomputational advantage;\n\n2. When variance reduction techniques (such as SAGA and SVRG) are\nincorporated in RCD-LMC, the variance error term is reduced. The new methods,\ncompared to the vanilla LMC, reduce the total computational cost by $d$ folds,\nand achieve the optimal cost rate.\n\nWe perform our investigations in both overdamped and underdamped settings.",
          "link": "http://arxiv.org/abs/2007.14209",
          "publishedOn": "2021-07-06T01:58:08.699Z",
          "wordCount": 623,
          "title": "Langevin Monte Carlo: random coordinate descent and variance reduction. (arXiv:2007.14209v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weiyue Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zeyang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Hui Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Siming Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yunsheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shikun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>",
          "description": "WikiKG90M in KDD Cup 2021 is a large encyclopedic knowledge graph, which\ncould benefit various downstream applications such as question answering and\nrecommender systems. Participants are invited to complete the knowledge graph\nby predicting missing triplets. Recent representation learning methods have\nachieved great success on standard datasets like FB15k-237. Thus, we train the\nadvanced algorithms in different domains to learn the triplets, including OTE,\nQuatE, RotatE and TransE. Significantly, we modified OTE into NOTE (short for\nNorm-OTE) for better performance. Besides, we use both the DeepWalk and the\npost-smoothing technique to capture the graph structure for supplementation. In\naddition to the representations, we also use various statistical probabilities\namong the head entities, the relations and the tail entities for the final\nprediction. Experimental results show that the ensemble of state-of-the-art\nrepresentation learning methods could draw on each others strengths. And we\ndevelop feature engineering from validation candidates for further\nimprovements. Please note that we apply the same strategy on the test set for\nfinal inference. And these features may not be practical in the real world when\nconsidering ranking against all the entities.",
          "link": "http://arxiv.org/abs/2107.01892",
          "publishedOn": "2021-07-06T01:58:08.690Z",
          "wordCount": 637,
          "title": "NOTE: Solution for KDD-CUP 2021 WikiKG90M-LSC. (arXiv:2107.01892v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jegorova_M/0/1/0/all/0/1\">Marija Jegorova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_C/0/1/0/all/0/1\">Chaitanya Kaul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayor_C/0/1/0/all/0/1\">Charlie Mayor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison Q. O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weir_A/0/1/0/all/0/1\">Alexander Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1\">Roderick Murray-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "Leakage of data from publicly available Machine Learning (ML) models is an\narea of growing significance as commercial and government applications of ML\ncan draw on multiple sources of data, potentially including users' and clients'\nsensitive data. We provide a comprehensive survey of contemporary advances on\nseveral fronts, covering involuntary data leakage which is natural to ML\nmodels, potential malevolent leakage which is caused by privacy attacks, and\ncurrently available defence mechanisms. We focus on inference-time leakage, as\nthe most likely scenario for publicly available models. We first discuss what\nleakage is in the context of different data, tasks, and model architectures. We\nthen propose a taxonomy across involuntary and malevolent leakage, available\ndefences, followed by the currently available assessment metrics and\napplications. We conclude with outstanding challenges and open questions,\noutlining some promising directions for future research.",
          "link": "http://arxiv.org/abs/2107.01614",
          "publishedOn": "2021-07-06T01:58:08.682Z",
          "wordCount": 574,
          "title": "Survey: Leakage and Privacy at Inference Time. (arXiv:2107.01614v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01323",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiong Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1\">Jiahua Chen</a>",
          "description": "When a population exhibits heterogeneity, we often model it via a finite\nmixture: decompose it into several different but homogeneous subpopulations.\nContemporary practice favors learning the mixtures by maximizing the likelihood\nfor statistical efficiency and the convenient EM-algorithm for numerical\ncomputation. Yet the maximum likelihood estimate (MLE) is not well defined for\nthe most widely used finite normal mixture in particular and for finite\nlocation-scale mixture in general. We hence investigate feasible alternatives\nto MLE such as minimum distance estimators. Recently, the Wasserstein distance\nhas drawn increased attention in the machine learning community. It has\nintuitive geometric interpretation and is successfully employed in many new\napplications. Do we gain anything by learning finite location-scale mixtures\nvia a minimum Wasserstein distance estimator (MWDE)? This paper investigates\nthis possibility in several respects. We find that the MWDE is consistent and\nderive a numerical solution under finite location-scale mixtures. We study its\nrobustness against outliers and mild model mis-specifications. Our moderate\nscaled simulation study shows the MWDE suffers some efficiency loss against a\npenalized version of MLE in general without noticeable gain in robustness. We\nreaffirm the general superiority of the likelihood based learning strategies\neven for the non-regular finite location-scale mixtures.",
          "link": "http://arxiv.org/abs/2107.01323",
          "publishedOn": "2021-07-06T01:58:08.676Z",
          "wordCount": 628,
          "title": "Minimum Wasserstein Distance Estimator under Finite Location-scale Mixtures. (arXiv:2107.01323v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01407",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blonde_L/0/1/0/all/0/1\">Lionel Blond&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1\">Alexandros Kalousis</a>",
          "description": "The performance of state-of-the-art baselines in the offline RL regime varies\nwidely over the spectrum of dataset qualities, ranging from \"far-from-optimal\"\nrandom data to \"close-to-optimal\" expert demonstrations. We re-implement these\nunder a fair, unified, and highly factorized framework, and show that when a\ngiven baseline outperforms its competing counterparts on one end of the\nspectrum, it never does on the other end. This consistent trend prevents us\nfrom naming a victor that outperforms the rest across the board. We attribute\nthe asymmetry in performance between the two ends of the quality spectrum to\nthe amount of inductive bias injected into the agent to entice it to posit that\nthe behavior underlying the offline dataset is optimal for the task. The more\nbias is injected, the higher the agent performs, provided the dataset is\nclose-to-optimal. Otherwise, its effect is brutally detrimental. Adopting an\nadvantage-weighted regression template as base, we conduct an investigation\nwhich corroborates that injections of such optimality inductive bias, when not\ndone parsimoniously, makes the agent subpar in the datasets it was dominant as\nsoon as the offline policy is sub-optimal. In an effort to design methods that\nperform well across the whole spectrum, we revisit the generalized policy\niteration scheme for the offline regime, and study the impact of nine distinct\nnewly-introduced proposal distributions over actions, involved in proposed\ngeneralization of the policy evaluation and policy improvement update rules. We\nshow that certain orchestrations strike the right balance and can improve the\nperformance on one end of the spectrum without harming it on the other end.",
          "link": "http://arxiv.org/abs/2107.01407",
          "publishedOn": "2021-07-06T01:58:08.658Z",
          "wordCount": 698,
          "title": "Where is the Grass Greener? Revisiting Generalized Policy Iteration for Offline Reinforcement Learning. (arXiv:2107.01407v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.15134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zixin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>",
          "description": "How can neural networks trained by contrastive learning extract features from\nthe unlabeled data? Why does contrastive learning usually need much stronger\ndata augmentations than supervised learning to ensure good representations?\nThese questions involve both the optimization and statistical aspects of deep\nlearning, but can hardly be answered by analyzing supervised learning, where\nthe target functions are the highest pursuit. Indeed, in self-supervised\nlearning, it is inevitable to relate to the optimization/generalization of\nneural networks to how they can encode the latent structures in the data, which\nwe refer to as the feature learning process.\n\nIn this work, we formally study how contrastive learning learns the feature\nrepresentations for neural networks by analyzing its feature learning process.\nWe consider the case where our data are comprised of two types of features: the\nmore semantically aligned sparse features which we want to learn from, and the\nother dense features we want to avoid. Theoretically, we prove that contrastive\nlearning using $\\mathbf{ReLU}$ networks provably learns the desired sparse\nfeatures if proper augmentations are adopted. We present an underlying\nprinciple called $\\textbf{feature decoupling}$ to explain the effects of\naugmentations, where we theoretically characterize how augmentations can reduce\nthe correlations of dense features between positive samples while keeping the\ncorrelations of sparse features intact, thereby forcing the neural networks to\nlearn from the self-supervision of sparse features. Empirically, we verified\nthat the feature decoupling principle matches the underlying mechanism of\ncontrastive learning in practice.",
          "link": "http://arxiv.org/abs/2105.15134",
          "publishedOn": "2021-07-06T01:58:08.652Z",
          "wordCount": 725,
          "title": "Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning. (arXiv:2105.15134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01590",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ming_D/0/1/0/all/0/1\">Deyu Ming</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Williamson_D/0/1/0/all/0/1\">Daniel Williamson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guillas_S/0/1/0/all/0/1\">Serge Guillas</a>",
          "description": "We propose a novel deep Gaussian process (DGP) inference method for computer\nmodel emulation using stochastic imputation. By stochastically imputing the\nlatent layers, the approach transforms the DGP into the linked GP, a\nstate-of-the-art surrogate model formed by linking a system of feed-forward\ncoupled GPs. This transformation renders a simple while efficient DGP training\nprocedure that only involves optimizations of conventional stationary GPs. In\naddition, the analytically tractable mean and variance of the linked GP allows\none to implement predictions from DGP emulators in a fast and accurate manner.\nWe demonstrate the method in a series of synthetic examples and real-world\napplications, and show that it is a competitive candidate for efficient DGP\nsurrogate modeling in comparison to the variational inference and the\nfully-Bayesian approach. A $\\texttt{Python}$ package $\\texttt{dgpsi}$\nimplementing the method is also produced and available at\nhttps://github.com/mingdeyu/DGP.",
          "link": "http://arxiv.org/abs/2107.01590",
          "publishedOn": "2021-07-06T01:58:08.644Z",
          "wordCount": 577,
          "title": "Deep Gaussian Process Emulation using Stochastic Imputation. (arXiv:2107.01590v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nozad_S/0/1/0/all/0/1\">Sayyed Ahmad Naghavi Nozad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haeri_M/0/1/0/all/0/1\">Maryam Amir Haeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folino_G/0/1/0/all/0/1\">Gianluigi Folino</a>",
          "description": "This paper presents a batch-wise density-based clustering approach for local\noutlier detection in massive-scale datasets. Unlike the well-known traditional\nalgorithms, which assume that all the data is memory-resident, our proposed\nmethod is scalable and processes the input data chunk-by-chunk within the\nconfines of a limited memory buffer. A temporary clustering model is built at\nthe first phase; then, it is gradually updated by analyzing consecutive memory\nloads of points. Subsequently, at the end of scalable clustering, the\napproximate structure of the original clusters is obtained. Finally, by another\nscan of the entire dataset and using a suitable criterion, an outlying score is\nassigned to each object called SDCOR (Scalable Density-based Clustering\nOutlierness Ratio). Evaluations on real-life and synthetic datasets demonstrate\nthat the proposed method has a low linear time complexity and is more effective\nand efficient compared to best-known conventional density-based methods, which\nneed to load all data into the memory; and also, to some fast distance-based\nmethods, which can perform on data resident in the disk.",
          "link": "http://arxiv.org/abs/2006.07616",
          "publishedOn": "2021-07-06T01:58:08.636Z",
          "wordCount": 755,
          "title": "SDCOR: Scalable Density-based Clustering for Local Outlier Detection in Massive-Scale Datasets. (arXiv:2006.07616v11 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maulana_M/0/1/0/all/0/1\">Muhammad Rizki Maulana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wee Sun Lee</a>",
          "description": "Ensemble and auxiliary tasks are both well known to improve the performance\nof machine learning models when data is limited. However, the interaction\nbetween these two methods is not well studied, particularly in the context of\ndeep reinforcement learning. In this paper, we study the effects of ensemble\nand auxiliary tasks when combined with the deep Q-learning algorithm. We\nperform a case study on ATARI games under limited data constraint. Moreover, we\nderive a refined bias-variance-covariance decomposition to analyze the\ndifferent ways of learning ensembles and using auxiliary tasks, and use the\nanalysis to help provide some understanding of the case study. Our code is open\nsource and available at https://github.com/NUS-LID/RENAULT.",
          "link": "http://arxiv.org/abs/2107.01904",
          "publishedOn": "2021-07-06T01:58:08.629Z",
          "wordCount": 556,
          "title": "Ensemble and Auxiliary Tasks for Data-Efficient Deep Reinforcement Learning. (arXiv:2107.01904v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Grzegorz Dudek</a>",
          "description": "This work contributes to the development of neural forecasting models with\nnovel randomization-based learning methods. These methods improve the fitting\nabilities of the neural model, in comparison to the standard method, by\ngenerating network parameters in accordance with the data and target function\nfeatures. A pattern-based representation of time series makes the proposed\napproach useful for forecasting time series with multiple seasonality. In the\nsimulation study, we evaluate the performance of the proposed models and find\nthat they can compete in terms of forecasting accuracy with fully-trained\nnetworks. Extremely fast and easy training, simple architecture, ease of\nimplementation, high accuracy as well as dealing with nonstationarity and\nmultiple seasonality in time series make the proposed model very attractive for\na wide range of complex time series forecasting problems.",
          "link": "http://arxiv.org/abs/2107.01705",
          "publishedOn": "2021-07-06T01:58:08.608Z",
          "wordCount": 572,
          "title": "Randomized Neural Networks for Forecasting Time Series with Multiple Seasonality. (arXiv:2107.01705v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weien Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "Temperature monitoring during the life time of heat source components in\nengineering systems becomes essential to guarantee the normal work and the\nworking life of these components. However, prior methods, which mainly use the\ninterpolate estimation to reconstruct the temperature field from limited\nmonitoring points, require large amounts of temperature tensors for an accurate\nestimation. This may decrease the availability and reliability of the system\nand sharply increase the monitoring cost. To solve this problem, this work\ndevelops a novel physics-informed deep reversible regression models for\ntemperature field reconstruction of heat-source systems (TFR-HSS), which can\nbetter reconstruct the temperature field with limited monitoring points\nunsupervisedly. First, we define the TFR-HSS task mathematically, and\nnumerically model the task, and hence transform the task as an image-to-image\nregression problem. Then this work develops the deep reversible regression\nmodel which can better learn the physical information, especially over the\nboundary. Finally, considering the physical characteristics of heat conduction\nas well as the boundary conditions, this work proposes the physics-informed\nreconstruction loss including four training losses and jointly learns the deep\nsurrogate model with these losses unsupervisedly. Experimental studies have\nconducted over typical two-dimensional heat-source systems to demonstrate the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2106.11929",
          "publishedOn": "2021-07-06T01:58:08.602Z",
          "wordCount": 686,
          "title": "Physics-Informed Deep Reversible Regression Model for Temperature Field Reconstruction of Heat-Source Systems. (arXiv:2106.11929v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zijie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Graph neural networks (GNNs) have been widely used in various graph-related\nproblems such as node classification and graph classification, where the\nsuperior performance is mainly established when natural node features are\navailable. However, it is not well understood how GNNs work without natural\nnode features, especially regarding the various ways to construct artificial\nones. In this paper, we point out the two types of artificial node\nfeatures,i.e., positional and structural node features, and provide insights on\nwhy each of them is more appropriate for certain tasks,i.e., positional node\nclassification, structural node classification, and graph classification.\nExtensive experimental results on 10 benchmark datasets validate our insights,\nthus leading to a practical guideline on the choices between different\nartificial node features for GNNs on non-attributed graphs. The code is\navailable at https://github.com/zjzijielu/gnn-exp/.",
          "link": "http://arxiv.org/abs/2107.01495",
          "publishedOn": "2021-07-06T01:58:08.594Z",
          "wordCount": 611,
          "title": "On Positional and Structural Node Features for Graph Neural Networks on Non-attributed Graphs. (arXiv:2107.01495v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.01099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1\">Ajay Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitlangia_S/0/1/0/all/0/1\">Sharad Chitlangia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baths_V/0/1/0/all/0/1\">Veeky Baths</a>",
          "description": "Reinforcement learning methods have recently been very successful at\nperforming complex sequential tasks like playing Atari games, Go and Poker.\nThese algorithms have outperformed humans in several tasks by learning from\nscratch, using only scalar rewards obtained through interaction with their\nenvironment. While there certainly has been considerable independent innovation\nto produce such results, many core ideas in reinforcement learning are inspired\nby phenomena in animal learning, psychology and neuroscience. In this paper, we\ncomprehensively review a large number of findings in both neuroscience and\npsychology that evidence reinforcement learning as a promising candidate for\nmodeling learning and decision making in the brain. In doing so, we construct a\nmapping between various classes of modern RL algorithms and specific findings\nin both neurophysiological and behavioral literature. We then discuss the\nimplications of this observed relationship between RL, neuroscience and\npsychology and its role in advancing research in both AI and brain science.",
          "link": "http://arxiv.org/abs/2007.01099",
          "publishedOn": "2021-07-06T01:58:08.581Z",
          "wordCount": 639,
          "title": "Reinforcement Learning and its Connections with Neuroscience and Psychology. (arXiv:2007.01099v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>",
          "description": "Single-pixel imaging is a novel imaging scheme that has gained popularity due\nto its huge computational gain and potential for a low-cost alternative to\nimaging beyond the visible spectrum. The traditional reconstruction methods\nstruggle to produce a clear recovery when one limits the number of illumination\npatterns from a spatial light modulator. As a remedy, several\ndeep-learning-based solutions have been proposed which lack good generalization\nability due to the architectural setup and loss functions. In this paper, we\npropose a generative adversarial network-based reconstruction framework for\nsingle-pixel imaging, referred to as SPI-GAN. Our method can reconstruct images\nwith 17.92 dB PSNR and 0.487 SSIM, even if the sampling ratio drops to 5%. This\nfacilitates much faster reconstruction making our method suitable for\nsingle-pixel video. Furthermore, our ResNet-like architecture for the generator\nleads to useful representation learning that allows us to reconstruct\ncompletely unseen objects. The experimental results demonstrate that SPI-GAN\nachieves significant performance gain, e.g. near 3dB PSNR gain, over the\ncurrent state-of-the-art method.",
          "link": "http://arxiv.org/abs/2107.01330",
          "publishedOn": "2021-07-06T01:58:08.574Z",
          "wordCount": 611,
          "title": "SPI-GAN: Towards Single-Pixel Imaging through Generative Adversarial Network. (arXiv:2107.01330v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01337",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Selim_M/0/1/0/all/0/1\">Md Selim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fei_B/0/1/0/all/0/1\">Baowei Fei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guo-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jin Chen</a>",
          "description": "While remarkable advances have been made in Computed Tomography (CT),\ncapturing CT images with non-standardized protocols causes low reproducibility\nregarding radiomic features, forming a barrier on CT image analysis in a large\nscale. RadiomicGAN is developed to effectively mitigate the discrepancy caused\nby using non-standard reconstruction kernels. RadiomicGAN consists of hybrid\nneural blocks including both pre-trained and trainable layers adopted to learn\nradiomic feature distributions efficiently. A novel training approach, called\nDynamic Window-based Training, has been developed to smoothly transform the\npre-trained model to the medical imaging domain. Model performance evaluated\nusing 1401 radiomic features show that RadiomicGAN clearly outperforms the\nstate-of-art image standardization models.",
          "link": "http://arxiv.org/abs/2107.01337",
          "publishedOn": "2021-07-06T01:58:08.555Z",
          "wordCount": 553,
          "title": "CT Image Harmonization for Enhancing Radiomics Studies. (arXiv:2107.01337v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1\">Sabato Marco Siniscalchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xianjun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuanjun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuzhong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>",
          "description": "We propose a novel neural model compression strategy combining data\naugmentation, knowledge transfer, pruning, and quantization for device-robust\nacoustic scene classification (ASC). Specifically, we tackle the ASC task in a\nlow-resource environment leveraging a recently proposed advanced neural network\npruning mechanism, namely Lottery Ticket Hypothesis (LTH), to find a\nsub-network neural model associated with a small amount non-zero model\nparameters. The effectiveness of LTH for low-complexity acoustic modeling is\nassessed by investigating various data augmentation and compression schemes,\nand we report an efficient joint framework for low-complexity multi-device ASC,\ncalled Acoustic Lottery. Acoustic Lottery could compress an ASC model over\n$1/10^{4}$ and attain a superior performance (validation accuracy of 74.01% and\nLog loss of 0.76) compared to its not compressed seed model. All results\nreported in this work are based on a joint effort of four groups, namely\nGT-USTC-UKE-Tencent, aiming to address the \"Low-Complexity Acoustic Scene\nClassification (ASC) with Multiple Devices\" in the DCASE 2021 Challenge Task\n1a.",
          "link": "http://arxiv.org/abs/2107.01461",
          "publishedOn": "2021-07-06T01:58:08.548Z",
          "wordCount": 647,
          "title": "A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification. (arXiv:2107.01461v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/1911.04872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hufei Zhu</a>",
          "description": "The original Broad Learning System (BLS) on new added nodes and its existing\nefficient implementation both assume the ridge parameter is near 0 in the ridge\ninverse to approximate the generalized inverse, and compute the generalized\ninverse solution for the output weights. In this paper, we propose two ridge\nsolutions for the output weights in the BLS on added nodes, where the ridge\nparameter can be any positive real number. One of the proposed ridge solutions\ncomputes the output weights from the inverse Cholesky factor, which is updated\nby extending the existing inverse Cholesky factorization. The other proposed\nridge solution computes the output weights from the ridge inverse, and updates\nthe ridge inverse by extending the Greville method that can only computes the\ngeneralized inverse of a partitioned matrix. The proposed BLS algorithm based\non the ridge inverse requires the same complexity as the original BLS\nalgorithm, while the proposed BLS algorithm based on the inverse Cholesky\nfactor requires less complexity and training time than the original BLS and the\nexisting efficient BLS. Both the proposed ridge solutions for BLS achieve the\nsame testing accuracy as the standard ridge solution in the numerical\nexperiments. The difference between the testing accuracy of the proposed ridge\nsolutions and that of the existing generalized inverse solutions is negligible\nwhen the ridge parameter is very small, and becomes too big to be ignored when\nthe ridge parameter is not very small. When the ridge parameter is not near 0,\nusually the proposed two ridge solutions for BLS achieve better testing\naccuracy than the existing generalized inverse solutions for BLS, and then the\nformer are more preferred than the latter.",
          "link": "http://arxiv.org/abs/1911.04872",
          "publishedOn": "2021-07-06T01:58:08.539Z",
          "wordCount": 737,
          "title": "Two Ridge Solutions for the Incremental Broad Learning System on Added Nodes. (arXiv:1911.04872v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01629",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Cong_Z/0/1/0/all/0/1\">Ziwei Cong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Manchanda_P/0/1/0/all/0/1\">Puneet Manchanda</a>",
          "description": "The common belief about the growing medium of livestreaming is that its value\nlies in its \"live\" component. In this paper, we leverage data from a large\nlivestreaming platform to examine this belief. We are able to do this as this\nplatform also allows viewers to purchase the recorded version of the\nlivestream. We summarize the value of livestreaming content by estimating how\ndemand responds to price before, on the day of, and after the livestream. We do\nthis by proposing a generalized Orthogonal Random Forest framework. This\nframework allows us to estimate heterogeneous treatment effects in the presence\nof high-dimensional confounders whose relationships with the treatment policy\n(i.e., price) are complex but partially known. We find significant dynamics in\nthe price elasticity of demand over the temporal distance to the scheduled\nlivestreaming day and after. Specifically, demand gradually becomes less price\nsensitive over time to the livestreaming day and is inelastic on the\nlivestreaming day. Over the post-livestream period, demand is still sensitive\nto price, but much less than the pre-livestream period. This indicates that the\nvlaue of livestreaming persists beyond the live component. Finally, we provide\nsuggestive evidence for the likely mechanisms driving our results. These are\nquality uncertainty reduction for the patterns pre- and post-livestream and the\npotential of real-time interaction with the creator on the day of the\nlivestream.",
          "link": "http://arxiv.org/abs/2107.01629",
          "publishedOn": "2021-07-06T01:58:08.532Z",
          "wordCount": 675,
          "title": "The Role of \"Live\" in Livestreaming Markets: Evidence Using Orthogonal Random Forest. (arXiv:2107.01629v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kage_P/0/1/0/all/0/1\">Patrick Kage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreadis_P/0/1/0/all/0/1\">Pavlos Andreadis</a>",
          "description": "Detecting latent structure within a dataset is a crucial step in performing\nanalysis of a dataset. However, existing state-of-the-art techniques for\nsubclass discovery are limited: either they are limited to detecting very small\nnumbers of outliers or they lack the statistical power to deal with complex\ndata such as image or audio. This paper proposes a solution to this subclass\ndiscovery problem: by leveraging instance explanation methods, an existing\nclassifier can be extended to detect latent classes via differences in the\nclassifier's internal decisions about each instance. This works not only with\nsimple classification techniques but also with deep neural networks, allowing\nfor a powerful and flexible approach to detecting latent structure within\ndatasets. Effectively, this represents a projection of the dataset into the\nclassifier's \"explanation space,\" and preliminary results show that this\ntechnique outperforms the baseline for the detection of latent classes even\nwith limited processing. This paper also contains a pipeline for analyzing\nclassifiers automatically, and a web application for interactively exploring\nthe results from this technique.",
          "link": "http://arxiv.org/abs/2107.01657",
          "publishedOn": "2021-07-06T01:58:08.517Z",
          "wordCount": 606,
          "title": "Class Introspection: A Novel Technique for Detecting Unlabeled Subclasses by Leveraging Classifier Explainability Methods. (arXiv:2107.01657v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuohang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Luyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>",
          "description": "Federated Learning (FL) enables multiple distributed clients (e.g., mobile\ndevices) to collaboratively train a centralized model while keeping the\ntraining data locally on the client. Compared to traditional centralized\nmachine learning, FL offers many favorable features such as offloading\noperations which would usually be performed by a central server and reducing\nrisks of serious privacy leakage. However, Byzantine clients that send\nincorrect or disruptive updates due to system failures or adversarial attacks\nmay disturb the joint learning process, consequently degrading the performance\nof the resulting model. In this paper, we propose to mitigate these failures\nand attacks from a spatial-temporal perspective. Specifically, we use a\nclustering-based method to detect and exclude incorrect updates by leveraging\ntheir geometric properties in the parameter space. Moreover, to further handle\nmalicious clients with time-varying behaviors, we propose to adaptively adjust\nthe learning rate according to momentum-based update speculation. Extensive\nexperiments on 4 public datasets demonstrate that our algorithm achieves\nenhanced robustness comparing to existing methods under both cross-silo and\ncross-device FL settings with faulty/malicious clients.",
          "link": "http://arxiv.org/abs/2107.01477",
          "publishedOn": "2021-07-06T01:58:08.500Z",
          "wordCount": 605,
          "title": "Byzantine-robust Federated Learning through Spatial-temporal Analysis of Local Model Updates. (arXiv:2107.01477v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fazzini_P/0/1/0/all/0/1\">Paolo Fazzini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wheeler_I/0/1/0/all/0/1\">Isaac Wheeler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petracchini_F/0/1/0/all/0/1\">Francesco Petracchini</a>",
          "description": "In this work we theoretically and experimentally analyze Multi-Agent\nAdvantage Actor-Critic (MA2C) and Independent Advantage Actor-Critic (IA2C),\ntwo recently proposed multi-agent reinforcement learning methods that can be\napplied to control traffic signals in urban areas. The two methods differ in\ntheir use of a reward calculated locally or globally and in the management of\nagents' communication. We analyze the methods theoretically with the framework\nprovided by non-Markov decision processes, which provides useful insights in\nthe analysis of the algorithms. Moreover, we analyze the efficacy and the\nrobustness of the methods experimentally by testing them in two traffic areas\nin the Bologna (Italy) area, simulated by SUMO, a software tool. The\nexperimental results indicate that MA2C achieves the best performance in the\nmajority of cases, outperforms the alternative method considered, and displays\nsufficient stability during the learning process.",
          "link": "http://arxiv.org/abs/2107.01347",
          "publishedOn": "2021-07-06T01:58:08.493Z",
          "wordCount": 585,
          "title": "Traffic Signal Control with Communicative Deep Reinforcement Learning Agents: a Case Study. (arXiv:2107.01347v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuezheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shui Yu</a>",
          "description": "Federated learning (FL) empowers distributed clients to collaboratively train\na shared machine learning model through exchanging parameter information.\nDespite the fact that FL can protect clients' raw data, malicious users can\nstill crack original data with disclosed parameters. To amend this flaw,\ndifferential privacy (DP) is incorporated into FL clients to disturb original\nparameters, which however can significantly impair the accuracy of the trained\nmodel. In this work, we study a crucial question which has been vastly\noverlooked by existing works: what are the optimal numbers of queries and\nreplies in FL with DP so that the final model accuracy is maximized. In FL, the\nparameter server (PS) needs to query participating clients for multiple global\niterations to complete training. Each client responds a query from the PS by\nconducting a local iteration. Our work investigates how many times the PS\nshould query clients and how many times each client should reply the PS. We\ninvestigate two most extensively used DP mechanisms (i.e., the Laplace\nmechanism and Gaussian mechanisms). Through conducting convergence rate\nanalysis, we can determine the optimal numbers of queries and replies in FL\nwith DP so that the final model accuracy can be maximized. Finally, extensive\nexperiments are conducted with publicly available datasets: MNIST and FEMNIST,\nto verify our analysis and the results demonstrate that properly setting the\nnumbers of queries and replies can significantly improve the final model\naccuracy in FL with DP.",
          "link": "http://arxiv.org/abs/2107.01895",
          "publishedOn": "2021-07-06T01:58:08.420Z",
          "wordCount": 694,
          "title": "Optimizing the Numbers of Queries and Replies in Federated Learning with Differential Privacy. (arXiv:2107.01895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yifan Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tianjun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1\">David Wipf Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a hierarchical graph neural network (GNN) model that learns how to\ncluster a set of images into an unknown number of identities using a training\nset of images annotated with labels belonging to a disjoint set of identities.\nOur hierarchical GNN uses a novel approach to merge connected components\npredicted at each level of the hierarchy to form a new graph at the next level.\nUnlike fully unsupervised hierarchical clustering, the choice of grouping and\ncomplexity criteria stems naturally from supervision in the training set. The\nresulting method, Hi-LANDER, achieves an average of 54% improvement in F-score\nand 8% increase in Normalized Mutual Information (NMI) relative to current\nGNN-based clustering algorithms. Additionally, state-of-the-art GNN-based\nmethods rely on separate models to predict linkage probabilities and node\ndensities as intermediate steps of the clustering process. In contrast, our\nunified framework achieves a seven-fold decrease in computational cost. We\nrelease our training and inference code at\nhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.",
          "link": "http://arxiv.org/abs/2107.01319",
          "publishedOn": "2021-07-06T01:58:08.413Z",
          "wordCount": 608,
          "title": "Learning Hierarchical Graph Neural Networks for Image Clustering. (arXiv:2107.01319v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shyam Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1\">Sandeep Silwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1\">Piotr Indyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_O/0/1/0/all/0/1\">Or Zamir</a>",
          "description": "Random dimensionality reduction is a versatile tool for speeding up\nalgorithms for high-dimensional problems. We study its application to two\nclustering problems: the facility location problem, and the single-linkage\nhierarchical clustering problem, which is equivalent to computing the minimum\nspanning tree. We show that if we project the input pointset $X$ onto a random\n$d = O(d_X)$-dimensional subspace (where $d_X$ is the doubling dimension of\n$X$), then the optimum facility location cost in the projected space\napproximates the original cost up to a constant factor. We show an analogous\nstatement for minimum spanning tree, but with the dimension $d$ having an extra\n$\\log \\log n$ term and the approximation factor being arbitrarily close to $1$.\nFurthermore, we extend these results to approximating solutions instead of just\ntheir costs. Lastly, we provide experimental results to validate the quality of\nsolutions and the speedup due to the dimensionality reduction. Unlike several\nprevious papers studying this approach in the context of $k$-means and\n$k$-medians, our dimension bound does not depend on the number of clusters but\nonly on the intrinsic dimensionality of $X$.",
          "link": "http://arxiv.org/abs/2107.01804",
          "publishedOn": "2021-07-06T01:58:08.393Z",
          "wordCount": 635,
          "title": "Randomized Dimensionality Reduction for Facility Location and Single-Linkage Clustering. (arXiv:2107.01804v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yakhchi_S/0/1/0/all/0/1\">Shahpar Yakhchi</a>",
          "description": "Recommender systems (RSs) have emerged as very useful tools to help customers\nwith their decision-making process, find items of their interest, and alleviate\nthe information overload problem. There are two different lines of approaches\nin RSs: (1) general recommenders with the main goal of discovering long-term\nusers' preferences, and (2) sequential recommenders with the main focus of\ncapturing short-term users' preferences in a session of user-item interaction\n(here, a session refers to a record of purchasing multiple items in one\nshopping event). While considering short-term users' preferences may satisfy\ntheir current needs and interests, long-term users' preferences provide users\nwith the items that they may interact with, eventually. In this thesis, we\nfirst focus on improving the performance of general RSs. Most of the existing\ngeneral RSs tend to exploit the users' rating patterns on common items to\ndetect similar users. The data sparsity problem (i.e. the lack of available\ninformation) is one of the major challenges for the current general RSs, and\nthey may fail to have any recommendations when there are no common items of\ninterest among users. We call this problem data sparsity with no feedback on\ncommon items (DSW-n-FCI). To overcome this problem, we propose a\npersonality-based RS in which similar users are identified based on the\nsimilarity of their personality traits.",
          "link": "http://arxiv.org/abs/2107.01529",
          "publishedOn": "2021-07-06T01:58:08.304Z",
          "wordCount": 655,
          "title": "Learning Complex Users' Preferences for Recommender Systems. (arXiv:2107.01529v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01303",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Dadashkarimi_J/0/1/0/all/0/1\">Javid Dadashkarimi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Karbasi_A/0/1/0/all/0/1\">Amin Karbasi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Scheinost_D/0/1/0/all/0/1\">Dustin Scheinost</a>",
          "description": "Functional connectomes derived from functional magnetic resonance imaging\nhave long been used to understand the functional organization of the brain.\nNevertheless, a connectome is intrinsically linked to the atlas used to create\nit. In other words, a connectome generated from one atlas is different in scale\nand resolution compared to a connectome generated from another atlas. Being\nable to map connectomes and derived results between different atlases without\nadditional pre-processing is a crucial step in improving interpretation and\ngeneralization between studies that use different atlases. Here, we use optimal\ntransport, a powerful mathematical technique, to find an optimum mapping\nbetween two atlases. This mapping is then used to transform time series from\none atlas to another in order to reconstruct a connectome. We validate our\napproach by comparing transformed connectomes against their \"gold-standard\"\ncounterparts (i.e., connectomes generated directly from an atlas) and\ndemonstrate the utility of transformed connectomes by applying these\nconnectomes to predictive models based on a different atlas. We show that these\ntransformed connectomes are significantly similar to their \"gold-standard\"\ncounterparts and maintain individual differences in brain-behavior\nassociations, demonstrating both the validity of our approach and its utility\nin downstream analyses. Overall, our approach is a promising avenue to increase\nthe generalization of connectome-based results across different atlases.",
          "link": "http://arxiv.org/abs/2107.01303",
          "publishedOn": "2021-07-06T01:58:08.269Z",
          "wordCount": 645,
          "title": "Data-driven mapping between functional connectomes using optimal transport. (arXiv:2107.01303v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01338",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Shankar_S/0/1/0/all/0/1\">Shiv Shankar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sheldon_D/0/1/0/all/0/1\">Daniel Sheldon</a>",
          "description": "Field observations form the basis of many scientific studies, especially in\necological and social sciences. Despite efforts to conduct such surveys in a\nstandardized way, observations can be prone to systematic measurement errors.\nThe removal of systematic variability introduced by the observation process, if\npossible, can greatly increase the value of this data. Existing non-parametric\ntechniques for correcting such errors assume linear additive noise models. This\nleads to biased estimates when applied to generalized linear models (GLM). We\npresent an approach based on residual functions to address this limitation. We\nthen demonstrate its effectiveness on synthetic data and show it reduces\nsystematic detection variability in moth surveys.",
          "link": "http://arxiv.org/abs/2107.01338",
          "publishedOn": "2021-07-06T01:58:08.180Z",
          "wordCount": 530,
          "title": "Sibling Regression for Generalized Linear Models. (arXiv:2107.01338v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sean Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>",
          "description": "Graph neural networks (GNNs) have been extensively studied for prediction\ntasks on graphs. As pointed out by recent studies, most GNNs assume local\nhomophily, i.e., strong similarities in local neighborhoods. This assumption\nhowever limits the generalizability power of GNNs. To address this limitation,\nwe propose a flexible GNN model, which is capable of handling any graphs\nwithout being restricted by their underlying homophily. At its core, this model\nadopts a node attention mechanism based on multiple learnable spectral filters;\ntherefore, the aggregation scheme is learned adaptively for each graph in the\nspectral domain. We evaluated the proposed model on node classification tasks\nover eight benchmark datasets. The proposed model is shown to generalize well\nto both homophilic and heterophilic graphs. Further, it outperforms all\nstate-of-the-art baselines on heterophilic graphs and performs comparably with\nthem on homophilic graphs.",
          "link": "http://arxiv.org/abs/2103.14187",
          "publishedOn": "2021-07-05T01:55:00.439Z",
          "wordCount": 614,
          "title": "Beyond Low-Pass Filters: Adaptive Feature Propagation on Graphs. (arXiv:2103.14187v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.03409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1\">Gabriel Henrique de Almeida Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusioka_A/0/1/0/all/0/1\">Andr&#xe9; Minoro Fusioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassu_B/0/1/0/all/0/1\">Bogdan Tomoyuki Nassu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1\">Rodrigo Minetto</a>",
          "description": "Active fire detection in satellite imagery is of critical importance to the\nmanagement of environmental conservation policies, supporting decision-making\nand law enforcement. This is a well established field, with many techniques\nbeing proposed over the years, usually based on pixel or region-level\ncomparisons involving sensor-specific thresholds and neighborhood statistics.\nIn this paper, we address the problem of active fire detection using deep\nlearning techniques. In recent years, deep learning techniques have been\nenjoying an enormous success in many fields, but their use for active fire\ndetection is relatively new, with open questions and demand for datasets and\narchitectures for evaluation. This paper addresses these issues by introducing\na new large-scale dataset for active fire detection, with over 150,000 image\npatches (more than 200 GB of data) extracted from Landsat-8 images captured\naround the world in August and September 2020, containing wildfires in several\nlocations. The dataset was split in two parts, and contains 10-band spectral\nimages with associated outputs, produced by three well known handcrafted\nalgorithms for active fire detection in the first part, and manually annotated\nmasks in the second part. We also present a study on how different\nconvolutional neural network architectures can be used to approximate these\nhandcrafted algorithms, and how models trained on automatically segmented\npatches can be combined to achieve better performance than the original\nalgorithms - with the best combination having 87.2% precision and 92.4% recall\non our manually annotated dataset. The proposed dataset, source codes and\ntrained models are available on Github\n(https://github.com/pereira-gha/activefire), creating opportunities for further\nadvances in the field",
          "link": "http://arxiv.org/abs/2101.03409",
          "publishedOn": "2021-07-05T01:55:00.423Z",
          "wordCount": 747,
          "title": "Active Fire Detection in Landsat-8 Imagery: a Large-Scale Dataset and a Deep-Learning Study. (arXiv:2101.03409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lavastida_T/0/1/0/all/0/1\">Thomas Lavastida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1\">Benjamin Moseley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_R/0/1/0/all/0/1\">R. Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenyang Xu</a>",
          "description": "We propose a new model for augmenting algorithms with predictions by\nrequiring that they are formally learnable and instance robust. Learnability\nensures that predictions can be efficiently constructed from a reasonable\namount of past data. Instance robustness ensures that the prediction is robust\nto modest changes in the problem input, where the measure of the change may be\nproblem specific. Instance robustness insists on a smooth degradation in\nperformance as a function of the change. Ideally, the performance is never\nworse than worst-case bounds. This also allows predictions to be objectively\ncompared.\n\nWe design online algorithms with predictions for a network flow allocation\nproblem and restricted assignment makespan minimization. For both problems, two\nkey properties are established: high quality predictions can be learned from a\nsmall sample of prior instances and these predictions are robust to errors that\nsmoothly degrade as the underlying problem instance changes.",
          "link": "http://arxiv.org/abs/2011.11743",
          "publishedOn": "2021-07-05T01:55:00.416Z",
          "wordCount": 622,
          "title": "Learnable and Instance-Robust Predictions for Online Matching, Flows and Load Balancing. (arXiv:2011.11743v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04579",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Chabaud_U/0/1/0/all/0/1\">Ulysse Chabaud</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Markham_D/0/1/0/all/0/1\">Damian Markham</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sohbi_A/0/1/0/all/0/1\">Adel Sohbi</a>",
          "description": "We study supervised learning algorithms in which a quantum device is used to\nperform a computational subroutine - either for prediction via probability\nestimation, or to compute a kernel via estimation of quantum states overlap. We\ndesign implementations of these quantum subroutines using Boson Sampling\narchitectures in linear optics, supplemented by adaptive measurements. We then\nchallenge these quantum algorithms by deriving classical simulation algorithms\nfor the tasks of output probability estimation and overlap estimation. We\nobtain different classical simulability regimes for these two computational\ntasks in terms of the number of adaptive measurements and input photons. In\nboth cases, our results set explicit limits to the range of parameters for\nwhich a quantum advantage can be envisaged with adaptive linear optics compared\nto classical machine learning algorithms: we show that the number of input\nphotons and the number of adaptive measurements cannot be simultaneously small\ncompared to the number of modes. Interestingly, our analysis leaves open the\npossibility of a near-term quantum advantage with a single adaptive\nmeasurement.",
          "link": "http://arxiv.org/abs/2102.04579",
          "publishedOn": "2021-07-05T01:55:00.393Z",
          "wordCount": 624,
          "title": "Quantum machine learning with adaptive linear optics. (arXiv:2102.04579v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fisman_D/0/1/0/all/0/1\">Dana Fisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frenkel_H/0/1/0/all/0/1\">Hadar Frenkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilles_S/0/1/0/all/0/1\">Sandra Zilles</a>",
          "description": "We revisit the complexity of procedures on SFAs (such as intersection,\nemptiness, etc.) and analyze them according to the measures we find suitable\nfor symbolic automata: the number of states, the maximal number of transitions\nexiting a state, and the size of the most complex transition predicate. We pay\nattention to the special forms of SFAs: {normalized SFAs} and {neat SFAs}, as\nwell as to SFAs over a {monotonic} effective Boolean algebra.",
          "link": "http://arxiv.org/abs/2011.05389",
          "publishedOn": "2021-07-05T01:55:00.384Z",
          "wordCount": 544,
          "title": "On the Complexity of Symbolic Finite-State Automata. (arXiv:2011.05389v3 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>",
          "description": "3D point cloud classification has many safety-critical applications such as\nautonomous driving and robotic grasping. However, several studies showed that\nit is vulnerable to adversarial attacks. In particular, an attacker can make a\nclassifier predict an incorrect label for a 3D point cloud via carefully\nmodifying, adding, and/or deleting a small number of its points. Randomized\nsmoothing is state-of-the-art technique to build certifiably robust 2D image\nclassifiers. However, when applied to 3D point cloud classification, randomized\nsmoothing can only certify robustness against adversarially modified points.\n\nIn this work, we propose PointGuard, the first defense that has provable\nrobustness guarantees against adversarially modified, added, and/or deleted\npoints. Specifically, given a 3D point cloud and an arbitrary point cloud\nclassifier, our PointGuard first creates multiple subsampled point clouds, each\nof which contains a random subset of the points in the original point cloud;\nthen our PointGuard predicts the label of the original point cloud as the\nmajority vote among the labels of the subsampled point clouds predicted by the\npoint cloud classifier. Our first major theoretical contribution is that we\nshow PointGuard provably predicts the same label for a 3D point cloud when the\nnumber of adversarially modified, added, and/or deleted points is bounded. Our\nsecond major theoretical contribution is that we prove the tightness of our\nderived bound when no assumptions on the point cloud classifier are made.\nMoreover, we design an efficient algorithm to compute our certified robustness\nguarantees. We also empirically evaluate PointGuard on ModelNet40 and ScanNet\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2103.03046",
          "publishedOn": "2021-07-05T01:55:00.377Z",
          "wordCount": 732,
          "title": "PointGuard: Provably Robust 3D Point Cloud Classification. (arXiv:2103.03046v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01683",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Azzimonti_L/0/1/0/all/0/1\">Laura Azzimonti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Corani_G/0/1/0/all/0/1\">Giorgio Corani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scutari_M/0/1/0/all/0/1\">Marco Scutari</a>",
          "description": "Score functions for learning the structure of Bayesian networks in the\nliterature assume that data are a homogeneous set of observations; whereas it\nis often the case that they comprise different related, but not homogeneous,\ndata sets collected in different ways. In this paper we propose a new Bayesian\nDirichlet score, which we call Bayesian Hierarchical Dirichlet (BHD). The\nproposed score is based on a hierarchical model that pools information across\ndata sets to learn a single encompassing network structure, while taking into\naccount the differences in their probabilistic structures. We derive a\nclosed-form expression for BHD using a variational approximation of the\nmarginal likelihood and we study its performance using simulated data. We find\nthat, when data comprise multiple related data sets, BHD outperforms the\nBayesian Dirichlet equivalent uniform (BDeu) score in terms of reconstruction\naccuracy as measured by the Structural Hamming distance, and that it is as\naccurate as BDeu when data are homogeneous. Moreover, the estimated networks\nare sparser and therefore more interpretable than those obtained with BDeu,\nthanks to a lower number of false positive arcs.",
          "link": "http://arxiv.org/abs/2008.01683",
          "publishedOn": "2021-07-05T01:55:00.359Z",
          "wordCount": 646,
          "title": "Structure Learning from Related Data Sets with a Hierarchical Bayesian Score. (arXiv:2008.01683v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05138",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Emmenegger_N/0/1/0/all/0/1\">Nicolas Emmenegger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kyng_R/0/1/0/all/0/1\">Rasmus Kyng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zehmakan_A/0/1/0/all/0/1\">Ahad N. Zehmakan</a>",
          "description": "We prove lower bounds for higher-order methods in smooth non-convex\nfinite-sum optimization. Our contribution is threefold: We first show that a\ndeterministic algorithm cannot profit from the finite-sum structure of the\nobjective, and that simulating a pth-order regularized method on the whole\nfunction by constructing exact gradient information is optimal up to constant\nfactors. We further show lower bounds for randomized algorithms and compare\nthem with the best known upper bounds. To address some gaps between the bounds,\nwe propose a new second-order smoothness assumption that can be seen as an\nanalogue of the first-order mean-squared smoothness assumption. We prove that\nit is sufficient to ensure state-of-the-art convergence guarantees, while\nallowing for a sharper lower bound.",
          "link": "http://arxiv.org/abs/2103.05138",
          "publishedOn": "2021-07-05T01:55:00.332Z",
          "wordCount": 596,
          "title": "On the Oracle Complexity of Higher-Order Smooth Non-Convex Finite-Sum Optimization. (arXiv:2103.05138v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nouri_M/0/1/0/all/0/1\">Mahbod Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_F/0/1/0/all/0/1\">Faraz Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaemi_H/0/1/0/all/0/1\">Hafez Ghaemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_A/0/1/0/all/0/1\">Ali Motie Nasrabadi</a>",
          "description": "A conventional subject-dependent (SD) brain-computer interface (BCI) requires\na complete data-gathering, training, and calibration phase for each user before\nit can be used. In recent years, a number of subject-independent (SI) BCIs have\nbeen developed. However, there are many problems preventing them from being\nused in real-world BCI applications. A weaker performance compared to the\nsubject-dependent (SD) approach, and a relatively large model requiring high\ncomputational power are the most important ones. Therefore, a potential\nreal-world BCI would greatly benefit from a compact low-power\nsubject-independent BCI framework, ready to be used immediately after the user\nputs it on. To move towards this goal, we propose a novel subject-independent\nBCI framework named CCSPNet (Convolutional Common Spatial Pattern Network)\ntrained on the motor imagery (MI) paradigm of a large-scale\nelectroencephalography (EEG) signals database consisting of 21600 trials for 54\nsubjects performing two-class hand-movement MI tasks. The proposed framework\napplies a wavelet kernel convolutional neural network (WKCNN) and a temporal\nconvolutional neural network (TCNN) in order to represent and extract the\ndiverse spectral features of EEG signals. The outputs of the convolutional\nlayers go through a common spatial pattern (CSP) algorithm for spatial feature\nextraction. The number of CSP features is reduced by a dense neural network,\nand the final class label is determined by a linear discriminative analysis\n(LDA) classifier. The CCSPNet framework evaluation results show that it is\npossible to have a low-power compact BCI that achieves both SD and SI\nperformance comparable to complex and computationally expensive.",
          "link": "http://arxiv.org/abs/2012.13567",
          "publishedOn": "2021-07-05T01:55:00.326Z",
          "wordCount": 750,
          "title": "Towards Real-World BCI: CCSPNet, A Compact Subject-Independent Motor Imagery Framework. (arXiv:2012.13567v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.12391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1\">David Bull</a>",
          "description": "This paper reviews the current state of the art in Artificial Intelligence\n(AI) technologies and applications in the context of the creative industries. A\nbrief background of AI, and specifically Machine Learning (ML) algorithms, is\nprovided including Convolutional Neural Network (CNNs), Generative Adversarial\nNetworks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement\nLearning (DRL). We categorise creative applications into five groups related to\nhow AI technologies are used: i) content creation, ii) information analysis,\niii) content enhancement and post production workflows, iv) information\nextraction and enhancement, and v) data compression. We critically examine the\nsuccesses and limitations of this rapidly advancing technology in each of these\nareas. We further differentiate between the use of AI as a creative tool and\nits potential as a creator in its own right. We foresee that, in the near\nfuture, machine learning-based AI will be adopted widely as a tool or\ncollaborative assistant for creativity. In contrast, we observe that the\nsuccesses of machine learning in domains with fewer constraints, where AI is\nthe `creator', remain modest. The potential of AI (or its developers) to win\nawards for its original creations in competition with human creatives is also\nlimited, based on contemporary technologies. We therefore conclude that, in the\ncontext of creative industries, maximum benefit from AI will be derived where\nits focus is human centric -- where it is designed to augment, rather than\nreplace, human creativity.",
          "link": "http://arxiv.org/abs/2007.12391",
          "publishedOn": "2021-07-05T01:55:00.319Z",
          "wordCount": 746,
          "title": "Artificial Intelligence in the Creative Industries: A Review. (arXiv:2007.12391v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1\">Fei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>",
          "description": "A crucial ability of human intelligence is to build up models of individual\n3D objects from partial scene observations. Recent works achieve object-centric\ngeneration but without the ability to infer the representation, or achieve 3D\nscene representation learning but without object-centric compositionality.\nTherefore, learning to represent and render 3D scenes with object-centric\ncompositionality remains elusive. In this paper, we propose a probabilistic\ngenerative model for learning to build modular and compositional 3D object\nmodels from partial observations of a multi-object scene. The proposed model\ncan (i) infer the 3D object representations by learning to search and group\nobject areas and also (ii) render from an arbitrary viewpoint not only\nindividual objects but also the full scene by compositing the objects. The\nentire learning process is unsupervised and end-to-end. In experiments, in\naddition to generation quality, we also demonstrate that the learned\nrepresentation permits object-wise manipulation and novel scene generation, and\ngeneralizes to various settings. Results can be found on our project website:\nhttps://sites.google.com/view/roots3d",
          "link": "http://arxiv.org/abs/2006.06130",
          "publishedOn": "2021-07-05T01:55:00.310Z",
          "wordCount": 644,
          "title": "ROOTS: Object-Centric Representation and Rendering of 3D Scenes. (arXiv:2006.06130v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fel_T/0/1/0/all/0/1\">Thomas Fel</a> (ANITI), <a href=\"http://arxiv.org/find/cs/1/au:+Vigouroux_D/0/1/0/all/0/1\">David Vigouroux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadene_R/0/1/0/all/0/1\">R&#xe9;mi Cad&#xe8;ne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a> (ANITI)",
          "description": "A plethora of methods have been proposed to explain howdeep neural networks\nreach a decision but comparativelylittle effort has been made to ensure that\nthe explanationsproduced by these methods are objectively relevant.\nWhiledesirable properties for a good explanation are easy to come,objective\nmeasures have been harder to derive. Here, we pro-pose two new measures to\nevaluate explanations borrowedfrom the field of algorithmic stability: relative\nconsistencyReCo and mean generalizability MeGe. We conduct severalexperiments\non multiple image datasets and network archi-tectures to demonstrate the\nbenefits of the proposed measuresover representative methods. We show that\npopular fidelitymeasures are not sufficient to guarantee good\nexplanations.Finally, we show empirically that 1-Lipschitz networks pro-vide\ngeneral and consistent explanations, regardless of theexplanation method used,\nmaking them a relevant directionfor explainability.",
          "link": "http://arxiv.org/abs/2009.04521",
          "publishedOn": "2021-07-05T01:55:00.293Z",
          "wordCount": 606,
          "title": "How good is your explanation? Algorithmic stability measures to assess the qualityof explanations for deep neural networks. (arXiv:2009.04521v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08028",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kapse_S/0/1/0/all/0/1\">Saarthak Kapse</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gattu_R/0/1/0/all/0/1\">Rishabh Gattu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Syed Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shah_N/0/1/0/all/0/1\">Neal Shah</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Marshall_C/0/1/0/all/0/1\">Colin Marshall</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pierce_J/0/1/0/all/0/1\">Jonathan Pierce</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Phatak_T/0/1/0/all/0/1\">Tej Phatak</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gupta_A/0/1/0/all/0/1\">Amit Gupta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Green_J/0/1/0/all/0/1\">Jeremy Green</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Madan_N/0/1/0/all/0/1\">Nikhil Madan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>",
          "description": "We predict mechanical ventilation requirement and mortality using\ncomputational modeling of chest radiographs (CXRs) for coronavirus disease 2019\n(COVID-19) patients. This two-center, retrospective study analyzed 530\ndeidentified CXRs from 515 COVID-19 patients treated at Stony Brook University\nHospital and Newark Beth Israel Medical Center between March and August 2020.\nDL and machine learning classifiers to predict mechanical ventilation\nrequirement and mortality were trained and evaluated using patient CXRs. A\nnovel radiomic embedding framework was also explored for outcome prediction.\nAll results are compared against radiologist grading of CXRs (zone-wise expert\nseverity scores). Radiomic and DL classification models had mAUCs of\n0.78+/-0.02 and 0.81+/-0.04, compared with expert scores mAUCs of 0.75+/-0.02\nand 0.79+/-0.05 for mechanical ventilation requirement and mortality\nprediction, respectively. Combined classifiers using both radiomics and expert\nseverity scores resulted in mAUCs of 0.79+/-0.04 and 0.83+/-0.04 for each\nprediction task, demonstrating improvement over either artificial intelligence\nor radiologist interpretation alone. Our results also suggest instances where\ninclusion of radiomic features in DL improves model predictions, something that\nmight be explored in other pathologies. The models proposed in this study and\nthe prognostic information they provide might aid physician decision making and\nresource allocation during the COVID-19 pandemic.",
          "link": "http://arxiv.org/abs/2007.08028",
          "publishedOn": "2021-07-05T01:55:00.265Z",
          "wordCount": 757,
          "title": "Predicting Clinical Outcomes in COVID-19 using Radiomics and Deep Learning on Chest Radiographs: A Multi-Institutional Study. (arXiv:2007.08028v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1\">Brian Kenji Iwana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "In recent times, deep artificial neural networks have achieved many successes\nin pattern recognition. Part of this success can be attributed to the reliance\non big data to increase generalization. However, in the field of time series\nrecognition, many datasets are often very small. One method of addressing this\nproblem is through the use of data augmentation. In this paper, we survey data\naugmentation techniques for time series and their application to time series\nclassification with neural networks. We propose a taxonomy and outline the four\nfamilies in time series data augmentation, including transformation-based\nmethods, pattern mixing, generative models, and decomposition methods.\nFurthermore, we empirically evaluate 12 time series data augmentation methods\non 128 time series classification datasets with six different types of neural\nnetworks. Through the results, we are able to analyze the characteristics,\nadvantages and disadvantages, and recommendations of each data augmentation\nmethod. This survey aims to help in the selection of time series data\naugmentation for neural network applications.",
          "link": "http://arxiv.org/abs/2007.15951",
          "publishedOn": "2021-07-05T01:55:00.255Z",
          "wordCount": 651,
          "title": "An Empirical Survey of Data Augmentation for Time Series Classification with Neural Networks. (arXiv:2007.15951v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02898",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghalamkari_K/0/1/0/all/0/1\">Kazu Ghalamkari</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1\">Mahito Sugiyama</a>",
          "description": "We present an efficient low-rank approximation algorithm for non-negative\ntensors. The algorithm is derived from our two findings: First, we show that\nrank-1 approximation for tensors can be viewed as a mean-field approximation by\ntreating each tensor as a probability distribution. Second, we theoretically\nprovide a sufficient condition for distribution parameters to reduce Tucker\nranks of tensors and, interestingly, this sufficient condition can be achieved\nby iterative application of the mean-field approximation. Since the mean-field\napproximation is always given as a closed formula, our findings lead to a fast\nlow-rank approximation algorithm without using a gradient method. We\nempirically demonstrate that our algorithm is faster than the existing\nnon-negative Tucker rank reduction methods with achieving competitive or better\napproximation of given tensors.",
          "link": "http://arxiv.org/abs/2103.02898",
          "publishedOn": "2021-07-05T01:55:00.248Z",
          "wordCount": 581,
          "title": "Fast Tucker Rank Reduction for Non-Negative Tensors Using Mean-Field Approximation. (arXiv:2103.02898v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.02725",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Sebbouh_O/0/1/0/all/0/1\">Othmane Sebbouh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gazagnadou_N/0/1/0/all/0/1\">Nidham Gazagnadou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jelassi_S/0/1/0/all/0/1\">Samy Jelassi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gower_R/0/1/0/all/0/1\">Robert M. Gower</a>",
          "description": "Among the very first variance reduced stochastic methods for solving the\nempirical risk minimization problem was the SVRG method (Johnson & Zhang 2013).\nSVRG is an inner-outer loop based method, where in the outer loop a reference\nfull gradient is evaluated, after which $m \\in \\mathbb{N}$ steps of an inner\nloop are executed where the reference gradient is used to build a variance\nreduced estimate of the current gradient. The simplicity of the SVRG method and\nits analysis have led to multiple extensions and variants for even non-convex\noptimization. We provide a more general analysis of SVRG than had been\npreviously done by using arbitrary sampling, which allows us to analyse\nvirtually all forms of mini-batching through a single theorem. Furthermore, our\nanalysis is focused on more practical variants of SVRG including a new variant\nof the loopless SVRG (Hofman et al 2015, Kovalev et al 2019, Kulunchakov and\nMairal 2019) and a variant of k-SVRG (Raj and Stich 2018) where $m=n$ and where\n$n$ is the number of data points. Since our setup and analysis reflect what is\ndone in practice, we are able to set the parameters such as the mini-batch size\nand step size using our theory in such a way that produces a more efficient\nalgorithm in practice, as we show in extensive numerical experiments.",
          "link": "http://arxiv.org/abs/1908.02725",
          "publishedOn": "2021-07-05T01:55:00.242Z",
          "wordCount": 692,
          "title": "Towards closing the gap between the theory and practice of SVRG. (arXiv:1908.02725v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Randomized smoothing is a recent technique that achieves state-of-art\nperformance in training certifiably robust deep neural networks. While the\nsmoothing family of distributions is often connected to the choice of the norm\nused for certification, the parameters of these distributions are always set as\nglobal hyper parameters independent of the input data on which a network is\ncertified. In this work, we revisit Gaussian randomized smoothing and show that\nthe variance of the Gaussian distribution can be optimized at each input so as\nto maximize the certification radius for the construction of the smoothed\nclassifier. This new approach is generic, parameter-free, and easy to\nimplement. In fact, we show that our data dependent framework can be seamlessly\nincorporated into 3 randomized smoothing approaches, leading to consistent\nimproved certified accuracy. When this framework is used in the training\nroutine of these approaches followed by a data dependent certification, we\nachieve 9\\% and 6\\% improvement over the certified accuracy of the strongest\nbaseline for a radius of 0.5 on CIFAR10 and ImageNet.",
          "link": "http://arxiv.org/abs/2012.04351",
          "publishedOn": "2021-07-05T01:55:00.224Z",
          "wordCount": 633,
          "title": "Data Dependent Randomized Smoothing. (arXiv:2012.04351v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01105",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bronskill_J/0/1/0/all/0/1\">John Bronskill</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Massiceti_D/0/1/0/all/0/1\">Daniela Massiceti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Patacchiola_M/0/1/0/all/0/1\">Massimiliano Patacchiola</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nowozin_S/0/1/0/all/0/1\">Sebastian Nowozin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1\">Richard E. Turner</a>",
          "description": "Meta learning approaches to few-shot classification are computationally\nefficient at test time requiring just a few optimization steps or single\nforward pass to learn a new task, but they remain highly memory-intensive to\ntrain. This limitation arises because a task's entire support set, which can\ncontain up to 1000 images, must be processed before an optimization step can be\ntaken. Harnessing the performance gains offered by large images thus requires\neither parallelizing the meta-learner across multiple GPUs, which may not be\navailable, or trade-offs between task and image size when memory constraints\napply. We improve on both options by proposing LITE, a general and memory\nefficient episodic training scheme that enables meta-training on large tasks\ncomposed of large images on a single GPU. We achieve this by observing that the\ngradients for a task can be decomposed into a sum of gradients over the task's\ntraining images. This enables us to perform a forward pass on a task's entire\ntraining set but realize significant memory savings by back-propagating only a\nrandom subset of these images which we show is an unbiased approximation of the\nfull gradient. We use LITE to train meta-learners and demonstrate new\nstate-of-the-art accuracy on the real-world ORBIT benchmark and 3 of the 4\nparts of the challenging VTAB+MD benchmark relative to leading meta-learners.\nLITE also enables meta-learners to be competitive with transfer learning\napproaches but at a fraction of the test-time computational cost, thus serving\nas a counterpoint to the recent narrative that transfer learning is all you\nneed for few-shot classification.",
          "link": "http://arxiv.org/abs/2107.01105",
          "publishedOn": "2021-07-05T01:55:00.209Z",
          "wordCount": 691,
          "title": "Memory Efficient Meta-Learning with Large Images. (arXiv:2107.01105v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.03774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tann_W/0/1/0/all/0/1\">Wesley Joon-Wie Tann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ee-Chien Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>",
          "description": "We introduce the controllable graph generation problem, formulated as\ncontrolling graph attributes during the generative process to produce desired\ngraphs with understandable structures. Using a transparent and straightforward\nMarkov model to guide this generative process, practitioners can shape and\nunderstand the generated graphs. We propose ${\\rm S{\\small HADOW}C{\\small\nAST}}$, a generative model capable of controlling graph generation while\nretaining the original graph's intrinsic properties. The proposed model is\nbased on a conditional generative adversarial network. Given an observed graph\nand some user-specified Markov model parameters, ${\\rm S{\\small HADOW}C{\\small\nAST}}$ controls the conditions to generate desired graphs. Comprehensive\nexperiments on three real-world network datasets demonstrate our model's\ncompetitive performance in the graph generation task. Furthermore, we show its\neffective controllability by directing ${\\rm S{\\small HADOW}C{\\small AST}}$ to\ngenerate hypothetical scenarios with different graph structures.",
          "link": "http://arxiv.org/abs/2006.03774",
          "publishedOn": "2021-07-05T01:55:00.202Z",
          "wordCount": 610,
          "title": "SHADOWCAST: Controllable Graph Generation. (arXiv:2006.03774v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.01025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunasekar_S/0/1/0/all/0/1\">Suriya Gunasekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1\">Blake Woodworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1\">Nathan Srebro</a>",
          "description": "We present a primal only derivation of Mirror Descent as a \"partial\"\ndiscretization of gradient flow on a Riemannian manifold where the metric\ntensor is the Hessian of the Mirror Descent potential. We contrast this\ndiscretization to Natural Gradient Descent, which is obtained by a \"full\"\nforward Euler discretization. This view helps shed light on the relationship\nbetween the methods and allows generalizing Mirror Descent to general\nRiemannian geometries, even when the metric tensor is {\\em not} a Hessian, and\nthus there is no \"dual.\"",
          "link": "http://arxiv.org/abs/2004.01025",
          "publishedOn": "2021-07-05T01:55:00.195Z",
          "wordCount": 567,
          "title": "Mirrorless Mirror Descent: A Natural Derivation of Mirror Descent. (arXiv:2004.01025v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.06952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zweig_A/0/1/0/all/0/1\">Aaron Zweig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>",
          "description": "Symmetric functions, which take as input an unordered, fixed-size set, are\nknown to be universally representable by neural networks that enforce\npermutation invariance. These architectures only give guarantees for fixed\ninput sizes, yet in many practical applications, including point clouds and\nparticle physics, a relevant notion of generalization should include varying\nthe input size. In this work we treat symmetric functions (of any size) as\nfunctions over probability measures, and study the learning and representation\nof neural networks defined on measures. By focusing on shallow architectures,\nwe establish approximation and generalization bounds under different choices of\nregularization (such as RKHS and variation norms), that capture a hierarchy of\nfunctional spaces with increasing degree of non-linear learning. The resulting\nmodels can be learned efficiently and enjoy generalization guarantees that\nextend across input sizes, as we verify empirically.",
          "link": "http://arxiv.org/abs/2008.06952",
          "publishedOn": "2021-07-05T01:55:00.161Z",
          "wordCount": 612,
          "title": "A Functional Perspective on Learning Symmetric Functions with Neural Networks. (arXiv:2008.06952v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.02892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peterson_J/0/1/0/all/0/1\">J. Luc Peterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bay_B/0/1/0/all/0/1\">Ben Bay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koning_J/0/1/0/all/0/1\">Joe Koning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_P/0/1/0/all/0/1\">Peter Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semler_J/0/1/0/all/0/1\">Jessica Semler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jeremy White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1\">Rushil Anirudh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athey_K/0/1/0/all/0/1\">Kevin Athey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremer_P/0/1/0/all/0/1\">Peer-Timo Bremer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natale_F/0/1/0/all/0/1\">Francesco Di Natale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">David Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaffney_J/0/1/0/all/0/1\">Jim A. Gaffney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_S/0/1/0/all/0/1\">Sam A. Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kustowski_B/0/1/0/all/0/1\">Bogdan Kustowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langer_S/0/1/0/all/0/1\">Steven Langer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spears_B/0/1/0/all/0/1\">Brian Spears</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman Thiagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essen_B/0/1/0/all/0/1\">Brian Van Essen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeom_J/0/1/0/all/0/1\">Jae-Seung Yeom</a>",
          "description": "With the growing complexity of computational and experimental facilities,\nmany scientific researchers are turning to machine learning (ML) techniques to\nanalyze large scale ensemble data. With complexities such as multi-component\nworkflows, heterogeneous machine architectures, parallel file systems, and\nbatch scheduling, care must be taken to facilitate this analysis in a high\nperformance computing (HPC) environment. In this paper, we present Merlin, a\nworkflow framework to enable large ML-friendly ensembles of scientific HPC\nsimulations. By augmenting traditional HPC with distributed compute\ntechnologies, Merlin aims to lower the barrier for scientific subject matter\nexperts to incorporate ML into their analysis. In addition to its design, we\ndescribe some example applications that Merlin has enabled on leadership-class\nHPC resources, such as the ML-augmented optimization of nuclear fusion\nexperiments and the calibration of infectious disease models to study the\nprogression of and possible mitigation strategies for COVID-19.",
          "link": "http://arxiv.org/abs/1912.02892",
          "publishedOn": "2021-07-05T01:55:00.147Z",
          "wordCount": 717,
          "title": "Enabling Machine Learning-Ready HPC Ensembles with Merlin. (arXiv:1912.02892v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bajic_M/0/1/0/all/0/1\">Milena Bajic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pour_S/0/1/0/all/0/1\">Shahrzad M. Pour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skar_A/0/1/0/all/0/1\">Asmus Skar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pettinari_M/0/1/0/all/0/1\">Matteo Pettinari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levenberg_E/0/1/0/all/0/1\">Eyal Levenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alstrom_T/0/1/0/all/0/1\">Tommy Sonne Alstr&#xf8;m</a>",
          "description": "Road roughness is a very important road condition for the infrastructure, as\nthe roughness affects both the safety and ride comfort of passengers. The roads\ndeteriorate over time which means the road roughness must be continuously\nmonitored in order to have an accurate understand of the condition of the road\ninfrastructure. In this paper, we propose a machine learning pipeline for road\nroughness prediction using the vertical acceleration of the car and the car\nspeed. We compared well-known supervised machine learning models such as linear\nregression, naive Bayes, k-nearest neighbor, random forest, support vector\nmachine, and the multi-layer perceptron neural network. The models are trained\non an optimally selected set of features computed in the temporal and\nstatistical domain. The results demonstrate that machine learning methods can\naccurately predict road roughness, using the recordings of the cost\napproachable in-vehicle sensors installed in conventional passenger cars. Our\nfindings demonstrate that the technology is well suited to meet future pavement\ncondition monitoring, by enabling continuous monitoring of a wide road network.",
          "link": "http://arxiv.org/abs/2107.01199",
          "publishedOn": "2021-07-05T01:55:00.140Z",
          "wordCount": 606,
          "title": "Road Roughness Estimation Using Machine Learning. (arXiv:2107.01199v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Mohd Zeeshan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1\">M M Sufyan Beg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Tanvir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohd Jazib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasim_G/0/1/0/all/0/1\">Ghazali Wasim</a>",
          "description": "Language identification of social media text has been an interesting problem\nof study in recent years. Social media messages are predominantly in code mixed\nin non-English speaking states. Prior knowledge by pre-training contextual\nembeddings have shown state of the art results for a range of downstream tasks.\nRecently, models such as BERT have shown that using a large amount of unlabeled\ndata, the pretrained language models are even more beneficial for learning\ncommon language representations. Extensive experiments exploiting transfer\nlearning and fine-tuning BERT models to identify language on Twitter are\npresented in this paper. The work utilizes a data collection of\nHindi-English-Urdu codemixed text for language pre-training and Hindi-English\ncodemixed for subsequent word-level language classification. The results show\nthat the representations pre-trained over codemixed data produce better results\nby their monolingual counterpart.",
          "link": "http://arxiv.org/abs/2107.01202",
          "publishedOn": "2021-07-05T01:55:00.126Z",
          "wordCount": 573,
          "title": "Language Identification of Hindi-English tweets using code-mixed BERT. (arXiv:2107.01202v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1912.02254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Huixin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei-Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yongcan Cao</a>",
          "description": "Besides accuracy, the model size of convolutional neural networks (CNN)\nmodels is another important factor considering limited hardware resources in\npractical applications. For example, employing deep neural networks on mobile\nsystems requires the design of accurate yet fast CNN for low latency in\nclassification and object detection. To fulfill the need, we aim at obtaining\nCNN models with both high testing accuracy and small size to address resource\nconstraints in many embedded devices. In particular, this paper focuses on\nproposing a generic reinforcement learning-based model compression approach in\na two-stage compression pipeline: pruning and quantization. The first stage of\ncompression, i.e., pruning, is achieved via exploiting deep reinforcement\nlearning (DRL) to co-learn the accuracy and the FLOPs updated after layer-wise\nchannel pruning and element-wise variational pruning via information dropout.\nThe second stage, i.e., quantization, is achieved via a similar DRL approach\nbut focuses on obtaining the optimal bits representation for individual layers.\nWe further conduct experimental results on CIFAR-10 and ImageNet datasets. For\nthe CIFAR-10 dataset, the proposed method can reduce the size of VGGNet by 9x\nfrom 20.04MB to 2.2MB with a slight accuracy increase. For the ImageNet\ndataset, the proposed method can reduce the size of VGG-16 by 33x from 138MB to\n4.14MB with no accuracy loss.",
          "link": "http://arxiv.org/abs/1912.02254",
          "publishedOn": "2021-07-05T01:55:00.113Z",
          "wordCount": 674,
          "title": "Deep Model Compression Via Two-Stage Deep Reinforcement Learning. (arXiv:1912.02254v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01126",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Melkas_L/0/1/0/all/0/1\">Laila Melkas</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Savvides_R/0/1/0/all/0/1\">Rafael Savvides</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chandramouli_S/0/1/0/all/0/1\">Suyog Chandramouli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Makela_J/0/1/0/all/0/1\">Jarmo M&#xe4;kel&#xe4;</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nieminen_T/0/1/0/all/0/1\">Tuomo Nieminen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mammarella_I/0/1/0/all/0/1\">Ivan Mammarella</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Puolamaki_K/0/1/0/all/0/1\">Kai Puolam&#xe4;ki</a>",
          "description": "Causal structure discovery (CSD) models are making inroads into several\ndomains, including Earth system sciences. Their widespread adaptation is\nhowever hampered by the fact that the resulting models often do not take into\naccount the domain knowledge of the experts and that it is often necessary to\nmodify the resulting models iteratively. We present a workflow that is required\nto take this knowledge into account and to apply CSD algorithms in Earth system\nsciences. At the same time, we describe open research questions that still need\nto be addressed. We present a way to interactively modify the outputs of the\nCSD algorithms and argue that the user interaction can be modelled as a greedy\nfinding of the local maximum-a-posteriori solution of the likelihood function,\nwhich is composed of the likelihood of the causal model and the prior\ndistribution representing the knowledge of the expert user. We use a real-world\ndata set for examples constructed in collaboration with our co-authors, who are\nthe domain area experts. We show that finding maximally usable causal models in\nthe Earth system sciences or other similar domains is a difficult task which\ncontains many interesting open research questions. We argue that taking the\ndomain knowledge into account has a substantial effect on the final causal\nmodels discovered.",
          "link": "http://arxiv.org/abs/2107.01126",
          "publishedOn": "2021-07-05T01:55:00.095Z",
          "wordCount": 681,
          "title": "Interactive Causal Structure Discovery in Earth System Sciences. (arXiv:2107.01126v1 [physics.data-an])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01131",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1\">Junya Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1\">Yuewei Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deng_X/0/1/0/all/0/1\">Xinwei Deng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>",
          "description": "Successful applications of InfoNCE and its variants have popularized the use\nof contrastive variational mutual information (MI) estimators in machine\nlearning. While featuring superior stability, these estimators crucially depend\non costly large-batch training, and they sacrifice bound tightness for variance\nreduction. To overcome these limitations, we revisit the mathematics of popular\nvariational MI bounds from the lens of unnormalized statistical modeling and\nconvex optimization. Our investigation not only yields a new unified\ntheoretical framework encompassing popular variational MI bounds but also leads\nto a novel, simple, and powerful contrastive MI estimator named as FLO.\nTheoretically, we show that the FLO estimator is tight, and it provably\nconverges under stochastic gradient descent. Empirically, our FLO estimator\novercomes the limitations of its predecessors and learns more efficiently. The\nutility of FLO is verified using an extensive set of benchmarks, which also\nreveals the trade-offs in practical MI estimation.",
          "link": "http://arxiv.org/abs/2107.01131",
          "publishedOn": "2021-07-05T01:55:00.088Z",
          "wordCount": 597,
          "title": "Tight Mutual Information Estimation With Contrastive Fenchel-Legendre Optimization. (arXiv:2107.01131v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Podder_P/0/1/0/all/0/1\">Prajoy Podder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharati_S/0/1/0/all/0/1\">Subrato Bharati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_M/0/1/0/all/0/1\">M. Rubaiyat Hossain Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_P/0/1/0/all/0/1\">Pinto Kumar Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kose_U/0/1/0/all/0/1\">Utku Kose</a>",
          "description": "Cybersecurity is a very emerging field that protects systems, networks, and\ndata from digital attacks. With the increase in the scale of the Internet and\nthe evolution of cyber attacks, developing novel cybersecurity tools has become\nimportant, particularly for Internet of things (IoT) networks. This paper\nprovides a systematic review of the application of deep learning (DL)\napproaches for cybersecurity. This paper provides a short description of DL\nmethods which is used in cybersecurity, including deep belief networks,\ngenerative adversarial networks, recurrent neural networks, and others. Next,\nwe illustrate the differences between shallow learning and DL. Moreover, a\ndiscussion is provided on the currently prevailing cyber-attacks in IoT and\nother networks, and the effectiveness of DL methods to manage these attacks.\nBesides, this paper describes studies that highlight the DL technique,\ncybersecurity applications, and the source of datasets. Next, a discussion is\nprovided on the feasibility of DL systems for malware detection and\nclassification, intrusion detection, and other frequent cyber-attacks,\nincluding identifying file type, spam, and network traffic. Our review\nindicates that high classification accuracy of 99.72% is obtained by restricted\nBoltzmann machine (RBM) when applied to a custom dataset, while long short-term\nmemory (LSTM) achieves an accuracy of 99.80% for KDD Cup 99 dataset. Finally,\nthis article discusses the importance of cybersecurity for reliable and\npracticable IoT-driven healthcare systems.",
          "link": "http://arxiv.org/abs/2107.01185",
          "publishedOn": "2021-07-05T01:55:00.082Z",
          "wordCount": 683,
          "title": "Artificial Neural Network for Cybersecurity: A Comprehensive Review. (arXiv:2107.01185v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiku_S/0/1/0/all/0/1\">Saideep Tiku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasricha_S/0/1/0/all/0/1\">Sudeep Pasricha</a>",
          "description": "GPS technology has revolutionized the way we localize and navigate outdoors.\nHowever, the poor reception of GPS signals in buildings makes it unsuitable for\nindoor localization. WiFi fingerprinting-based indoor localization is one of\nthe most promising ways to meet this demand. Unfortunately, most work in the\ndomain fails to resolve challenges associated with deployability on\nresource-limited embedded devices. In this work, we propose a compression-aware\nand high-accuracy deep learning framework called CHISEL that outperforms the\nbest-known works in the area while maintaining localization robustness on\nembedded devices.",
          "link": "http://arxiv.org/abs/2107.01192",
          "publishedOn": "2021-07-05T01:55:00.074Z",
          "wordCount": 519,
          "title": "CHISEL: Compression-Aware High-Accuracy Embedded Indoor Localization with Deep Learning. (arXiv:2107.01192v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01201",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rikhye_R/0/1/0/all/0/1\">Rajeev Rikhye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Q/0/1/0/all/0/1\">Qiao Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McGraw_I/0/1/0/all/0/1\">Ian McGraw</a>",
          "description": "In this paper, we propose a solution to allow speaker conditioned speech\nmodels, such as VoiceFilter-Lite, to support an arbitrary number of enrolled\nusers in a single pass. This is achieved by using an attention mechanism on\nmultiple speaker embeddings to compute a single attentive embedding, which is\nthen used as a side input to the model. We implemented multi-user\nVoiceFilter-Lite and evaluated it for three tasks: (1) a streaming automatic\nspeech recognition (ASR) task; (2) a text-independent speaker verification\ntask; and (3) a personalized keyphrase detection task, where ASR has to detect\nkeyphrases from multiple enrolled users in a noisy environment. Our experiments\nshow that, with up to four enrolled users, multi-user VoiceFilter-Lite is able\nto significantly reduce speech recognition and speaker verification errors when\nthere is overlapping speech, without affecting performance under other acoustic\nconditions. This attentive speaker embedding approach can also be easily\napplied to other speaker-conditioned models such as personal VAD and\npersonalized ASR.",
          "link": "http://arxiv.org/abs/2107.01201",
          "publishedOn": "2021-07-05T01:55:00.068Z",
          "wordCount": 601,
          "title": "Multi-user VoiceFilter-Lite via Attentive Speaker Embedding. (arXiv:2107.01201v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shanu Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod Kumar Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Praphul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P Namboodiri</a>",
          "description": "Understanding unsupervised domain adaptation has been an important task that\nhas been well explored. However, the wide variety of methods have not analyzed\nthe role of a classifier's performance in detail. In this paper, we thoroughly\nexamine the role of a classifier in terms of matching source and target\ndistributions. We specifically investigate the classifier ability by matching\na) the distribution of features, b) probabilistic uncertainty for samples and\nc) certainty activation mappings. Our analysis suggests that using these three\ndistributions does result in a consistently improved performance on all the\ndatasets. Our work thus extends present knowledge on the role of the various\ndistributions obtained from the classifier towards solving unsupervised domain\nadaptation.",
          "link": "http://arxiv.org/abs/2107.00727",
          "publishedOn": "2021-07-05T01:54:59.465Z",
          "wordCount": 552,
          "title": "Mitigating Uncertainty of Classifier for Unsupervised Domain Adaptation. (arXiv:2107.00727v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1809.04091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sepehry_B/0/1/0/all/0/1\">Behrooz Sepehry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iranmanesh_E/0/1/0/all/0/1\">Ehsan Iranmanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_M/0/1/0/all/0/1\">Michael P. Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronagh_P/0/1/0/all/0/1\">Pooya Ronagh</a>",
          "description": "We introduce two quantum algorithms for solving structured prediction\nproblems. We first show that a stochastic gradient descent that uses the\nquantum minimum finding algorithm and takes its probabilistic failure into\naccount solves the structured prediction problem with a runtime that scales\nwith the square root of the size of the label space, and in $\\widetilde\nO\\left(1/\\epsilon\\right)$ with respect to the precision, $\\epsilon$, of the\nsolution. Motivated by robust inference techniques in machine learning, we then\nintroduce another quantum algorithm that solves a smooth approximation of the\nstructured prediction problem with a similar quantum speedup in the size of the\nlabel space and a similar scaling in the precision parameter. In doing so, we\nanalyze a variant of stochastic gradient descent for convex optimization in the\npresence of an additive error in the calculation of the gradients, and show\nthat its convergence rate does not deteriorate if the additive errors are of\nthe order $O(\\sqrt\\epsilon)$. This algorithm uses quantum Gibbs sampling at\ntemperature $\\Omega (\\epsilon)$ as a subroutine. Based on these theoretical\nobservations, we propose a method for using quantum Gibbs samplers to combine\nfeedforward neural networks with probabilistic graphical models for quantum\nmachine learning. Our numerical results using Monte Carlo simulations on an\nimage tagging task demonstrate the benefit of the approach.",
          "link": "http://arxiv.org/abs/1809.04091",
          "publishedOn": "2021-07-05T01:54:59.459Z",
          "wordCount": 716,
          "title": "Quantum Algorithms for Structured Prediction. (arXiv:1809.04091v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naeemullah Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Deep neural networks are vulnerable to input deformations in the form of\nvector fields of pixel displacements and to other parameterized geometric\ndeformations e.g. translations, rotations, etc. Current input deformation\ncertification methods either (i) do not scale to deep networks on large input\ndatasets, or (ii) can only certify a specific class of deformations, e.g. only\nrotations. We reformulate certification in randomized smoothing setting for\nboth general vector field and parameterized deformations and propose\nDeformRS-VF and DeformRS-Par, respectively. Our new formulation scales to large\nnetworks on large input datasets. For instance, DeformRS-Par certifies rich\ndeformations, covering translations, rotations, scaling, affine deformations,\nand other visually aligned deformations such as ones parameterized by\nDiscrete-Cosine-Transform basis. Extensive experiments on MNIST, CIFAR10 and\nImageNet show that DeformRS-Par outperforms existing state-of-the-art in\ncertified accuracy, e.g. improved certified accuracy of 6% against perturbed\nrotations in the set [-10,10] degrees on ImageNet.",
          "link": "http://arxiv.org/abs/2107.00996",
          "publishedOn": "2021-07-05T01:54:59.453Z",
          "wordCount": 591,
          "title": "DeformRS: Certifying Input Deformations with Randomized Smoothing. (arXiv:2107.00996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1\">Chen Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Christopher M. Jermaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "We propose {\\rm \\texttt{ResIST}}, a novel distributed training protocol for\nResidual Networks (ResNets). {\\rm \\texttt{ResIST}} randomly decomposes a global\nResNet into several shallow sub-ResNets that are trained independently in a\ndistributed manner for several local iterations, before having their updates\nsynchronized and aggregated into the global model. In the next round, new\nsub-ResNets are randomly generated and the process repeats. By construction,\nper iteration, {\\rm \\texttt{ResIST}} communicates only a small portion of\nnetwork parameters to each machine and never uses the full model during\ntraining. Thus, {\\rm \\texttt{ResIST}} reduces the communication, memory, and\ntime requirements of ResNet training to only a fraction of the requirements of\nprevious methods. In comparison to common protocols like data-parallel training\nand data-parallel training with local SGD, {\\rm \\texttt{ResIST}} yields a\ndecrease in wall-clock training time, while being competitive with respect to\nmodel performance.",
          "link": "http://arxiv.org/abs/2107.00961",
          "publishedOn": "2021-07-05T01:54:59.446Z",
          "wordCount": 593,
          "title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training. (arXiv:2107.00961v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Mingzhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Metro origin-destination prediction is a crucial yet challenging task for\nintelligent transportation management, which aims to accurately forecast two\nspecific types of cross-station ridership, i.e., Origin-Destination (OD) one\nand Destination-Origin (DO) one. However, complete OD matrices of previous time\nintervals can not be obtained immediately in online metro systems, and\nconventional methods only used limited information to forecast the future OD\nand DO ridership separately.In this work, we proposed a novel neural network\nmodule termed Heterogeneous Information Aggregation Machine (HIAM), which fully\nexploits heterogeneous information of historical data (e.g., incomplete OD\nmatrices, unfinished order vectors, and DO matrices) to jointly learn the\nevolutionary patterns of OD and DO ridership. Specifically, an OD modeling\nbranch estimates the potential destinations of unfinished orders explicitly to\ncomplement the information of incomplete OD matrices, while a DO modeling\nbranch takes DO matrices as input to capture the spatial-temporal distribution\nof DO ridership. Moreover, a Dual Information Transformer is introduced to\npropagate the mutual information among OD features and DO features for modeling\nthe OD-DO causality and correlation. Based on the proposed HIAM, we develop a\nunified Seq2Seq network to forecast the future OD and DO ridership\nsimultaneously. Extensive experiments conducted on two large-scale benchmarks\ndemonstrate the effectiveness of our method for online metro origin-destination\nprediction.",
          "link": "http://arxiv.org/abs/2107.00946",
          "publishedOn": "2021-07-05T01:54:59.440Z",
          "wordCount": 651,
          "title": "Online Metro Origin-Destination Prediction via Heterogeneous Information Aggregation. (arXiv:2107.00946v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammernik_K/0/1/0/all/0/1\">Kerstin Hammernik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>",
          "description": "Deep learning-based segmentation methods are vulnerable to unforeseen data\ndistribution shifts during deployment, e.g. change of image appearances or\ncontrasts caused by different scanners, unexpected imaging artifacts etc. In\nthis paper, we present a cooperative framework for training image segmentation\nmodels and a latent space augmentation method for generating hard examples.\nBoth contributions improve model generalization and robustness with limited\ndata. The cooperative training framework consists of a fast-thinking network\n(FTN) and a slow-thinking network (STN). The FTN learns decoupled image\nfeatures and shape features for image reconstruction and segmentation tasks.\nThe STN learns shape priors for segmentation correction and refinement. The two\nnetworks are trained in a cooperative manner. The latent space augmentation\ngenerates challenging examples for training by masking the decoupled latent\nspace in both channel-wise and spatial-wise manners. We performed extensive\nexperiments on public cardiac imaging datasets. Using only 10 subjects from a\nsingle site for training, we demonstrated improved cross-site segmentation\nperformance and increased robustness against various unforeseen imaging\nartifacts compared to strong baseline methods. Particularly, cooperative\ntraining with latent space data augmentation yields 15% improvement in terms of\naverage Dice score when compared to a standard training method.",
          "link": "http://arxiv.org/abs/2107.01079",
          "publishedOn": "2021-07-05T01:54:59.424Z",
          "wordCount": 657,
          "title": "Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation. (arXiv:2107.01079v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleindessner_M/0/1/0/all/0/1\">Matth&#xe4;us Kleindessner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>",
          "description": "When machine learning systems meet real world applications, accuracy is only\none of several requirements. In this paper, we assay a complementary\nperspective originating from the increasing availability of pre-trained and\nregularly improving state-of-the-art models. While new improved models develop\nat a fast pace, downstream tasks vary more slowly or stay constant. Assume that\nwe have a large unlabelled data set for which we want to maintain accurate\npredictions. Whenever a new and presumably better ML models becomes available,\nwe encounter two problems: (i) given a limited budget, which data points should\nbe re-evaluated using the new model?; and (ii) if the new predictions differ\nfrom the current ones, should we update? Problem (i) is about compute cost,\nwhich matters for very large data sets and models. Problem (ii) is about\nmaintaining consistency of the predictions, which can be highly relevant for\ndownstream applications; our demand is to avoid negative flips, i.e., changing\ncorrect to incorrect predictions. In this paper, we formalize the Prediction\nUpdate Problem and present an efficient probabilistic approach as answer to the\nabove questions. In extensive experiments on standard classification benchmark\ndata sets, we show that our method outperforms alternative strategies along key\nmetrics for backward-compatible prediction updates.",
          "link": "http://arxiv.org/abs/2107.01057",
          "publishedOn": "2021-07-05T01:54:59.417Z",
          "wordCount": 637,
          "title": "Backward-Compatible Prediction Updates: A Probabilistic Approach. (arXiv:2107.01057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>",
          "description": "In this paper, we study stochastic optimization of areas under\nprecision-recall curves (AUPRC), which is widely used for combating imbalanced\nclassification tasks. Although a few methods have been proposed for maximizing\nAUPRC, stochastic optimization of AUPRC with convergence guarantee remains an\nundeveloped territory. A recent work [42] has proposed a promising approach\ntowards AUPRC based on maximizing a surrogate loss for the average precision,\nand proved an $O(1/\\epsilon^5)$ complexity for finding an $\\epsilon$-stationary\nsolution of the non-convex objective. In this paper, we further improve the\nstochastic optimization of AURPC by (i) developing novel stochastic momentum\nmethods with a better iteration complexity of $O(1/\\epsilon^4)$ for finding an\n$\\epsilon$-stationary solution; and (ii) designing a novel family of stochastic\nadaptive methods with the same iteration complexity of $O(1/\\epsilon^4)$, which\nenjoy faster convergence in practice. To this end, we propose two innovative\ntechniques that are critical for improving the convergence: (i) the biased\nestimators for tracking individual ranking scores are updated in a randomized\ncoordinate-wise manner; and (ii) a momentum update is used on top of the\nstochastic gradient estimator for tracking the gradient of the objective.\nExtensive experiments on various data sets demonstrate the effectiveness of the\nproposed algorithms. Of independent interest, the proposed stochastic momentum\nand adaptive algorithms are also applicable to a class of two-level stochastic\ndependent compositional optimization problems.",
          "link": "http://arxiv.org/abs/2107.01173",
          "publishedOn": "2021-07-05T01:54:59.410Z",
          "wordCount": 655,
          "title": "Momentum Accelerates the Convergence of Stochastic AUPRC Maximization. (arXiv:2107.01173v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pavlichenko_N/0/1/0/all/0/1\">Nikita Pavlichenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stelmakh_I/0/1/0/all/0/1\">Ivan Stelmakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ustalov_D/0/1/0/all/0/1\">Dmitry Ustalov</a>",
          "description": "Domain-specific data is the crux of the successful transfer of machine\nlearning systems from benchmarks to real life. Crowdsourcing has become one of\nthe standard tools for cheap and time-efficient data collection for simple\nproblems such as image classification: thanks in large part to advances in\nresearch on aggregation methods. However, the applicability of crowdsourcing to\nmore complex tasks (e.g., speech recognition) remains limited due to the lack\nof principled aggregation methods for these modalities. The main obstacle\ntowards designing advanced aggregation methods is the absence of training data,\nand in this work, we focus on bridging this gap in speech recognition. For\nthis, we collect and release CrowdSpeech -- the first publicly available\nlarge-scale dataset of crowdsourced audio transcriptions. Evaluation of\nexisting aggregation methods on our data shows room for improvement, suggesting\nthat our work may entail the design of better algorithms. At a higher level, we\nalso contribute to the more general challenge of collecting high-quality\ndatasets using crowdsourcing: we develop a principled pipeline for constructing\ndatasets of crowdsourced audio transcriptions in any novel domain. We show its\napplicability on an under-resourced language by constructing VoxDIY -- a\ncounterpart of CrowdSpeech for the Russian language. We also release the code\nthat allows a full replication of our data collection pipeline and share\nvarious insights on best practices of data collection via crowdsourcing.",
          "link": "http://arxiv.org/abs/2107.01091",
          "publishedOn": "2021-07-05T01:54:59.403Z",
          "wordCount": 668,
          "title": "Vox Populi, Vox DIY: Benchmark Dataset for Crowdsourced Audio Transcription. (arXiv:2107.01091v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1\">Alberto Badias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Ashis Banerjee</a>",
          "description": "We present a new framework to measure the intrinsic properties of (deep)\nneural networks. While we focus on convolutional networks, our framework can be\nextrapolated to any network architecture. In particular, we evaluate two\nnetwork properties, namely, capacity (related to expressivity) and compression,\nboth of which depend only on the network structure and are independent of the\ntraining and test data. To this end, we propose two metrics: the first one,\ncalled layer complexity, captures the architectural complexity of any network\nlayer; and, the second one, called layer intrinsic power, encodes how data is\ncompressed along the network. The metrics are based on the concept of layer\nalgebra, which is also introduced in this paper. This concept is based on the\nidea that the global properties depend on the network topology, and the leaf\nnodes of any neural network can be approximated using local transfer functions,\nthereby, allowing a simple computation of the global metrics. We also compare\nthe properties of the state-of-the art architectures using our metrics and use\nthe properties to analyze the classification accuracy on benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.01081",
          "publishedOn": "2021-07-05T01:54:59.372Z",
          "wordCount": 617,
          "title": "Neural Network Layer Algebra: A Framework to Measure Capacity and Compression in Deep Learning. (arXiv:2107.01081v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jerrick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkawhich_N/0/1/0/all/0/1\">Nathan Inkawhich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nina_O/0/1/0/all/0/1\">Oliver Nina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yuru Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songzheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mengru Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Huanqia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengxue Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_S/0/1/0/all/0/1\">Sol Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_C/0/1/0/all/0/1\">Casian Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasarica_A/0/1/0/all/0/1\">Alexandru Pasarica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hung-Min Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiarui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chia-Ying Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_M/0/1/0/all/0/1\">Michael Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Z/0/1/0/all/0/1\">Zhongkai Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yifei_X/0/1/0/all/0/1\">Xu Yifei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Min Feng</a>",
          "description": "In this paper, we introduce the first Challenge on Multi-modal Aerial View\nObject Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at\nCVPR. This challenge is composed of two different tracks using EO andSAR\nimagery. Both EO and SAR sensors possess different advantages and drawbacks.\nThe purpose of this competition is to analyze how to use both sets of sensory\ninformation in complementary ways. We discuss the top methods submitted for\nthis competition and evaluate their results on our blind test set. Our\nchallenge results show significant improvement of more than 15% accuracy from\nour current baselines for each track of the competition",
          "link": "http://arxiv.org/abs/2107.01189",
          "publishedOn": "2021-07-05T01:54:59.347Z",
          "wordCount": 632,
          "title": "NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.13020",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pena_J/0/1/0/all/0/1\">Jose M. Pe&#xf1;a</a>",
          "description": "We present a method for assessing the sensitivity of the true causal effect\nto unmeasured confounding. The method requires the analyst to specify two\nintuitive parameters. Otherwise, the method is assumption-free. The method\nreturns an interval that contains the true causal effect. Moreover, the bounds\nof the interval are sharp, i.e. attainable. We show experimentally that our\nbounds can be sharper than those obtained by the method of Ding and VanderWeele\n(2016). Finally, we extend our method to bound the natural direct and indirect\neffects when there are measured mediators and unmeasured exposure-outcome\nconfounding.",
          "link": "http://arxiv.org/abs/2104.13020",
          "publishedOn": "2021-07-05T01:54:59.325Z",
          "wordCount": 543,
          "title": "Simple yet Sharp Sensitivity Analysis for Unmeasured Confounding. (arXiv:2104.13020v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1\">Sushrut Karmalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jessica Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>",
          "description": "This work tackles the issue of fairness in the context of generative\nprocedures, such as image super-resolution, which entail different definitions\nfrom the standard classification setting. Moreover, while traditional group\nfairness definitions are typically defined with respect to specified protected\ngroups -- camouflaging the fact that these groupings are artificial and carry\nhistorical and political motivations -- we emphasize that there are no ground\ntruth identities. For instance, should South and East Asians be viewed as a\nsingle group or separate groups? Should we consider one race as a whole or\nfurther split by gender? Choosing which groups are valid and who belongs in\nthem is an impossible dilemma and being \"fair\" with respect to Asians may\nrequire being \"unfair\" with respect to South Asians. This motivates the\nintroduction of definitions that allow algorithms to be \\emph{oblivious} to the\nrelevant groupings.\n\nWe define several intuitive notions of group fairness and study their\nincompatibilities and trade-offs. We show that the natural extension of\ndemographic parity is strongly dependent on the grouping, and \\emph{impossible}\nto achieve obliviously. On the other hand, the conceptually new definition we\nintroduce, Conditional Proportional Representation, can be achieved obliviously\nthrough Posterior Sampling. Our experiments validate our theoretical results\nand achieve fair image reconstruction using state-of-the-art generative models.",
          "link": "http://arxiv.org/abs/2106.12182",
          "publishedOn": "2021-07-05T01:54:59.318Z",
          "wordCount": 685,
          "title": "Fairness for Image Generation with Uncertain Sensitive Attributes. (arXiv:2106.12182v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01152",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1\">Junya Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1\">Xuan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1\">Liqun Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_S/0/1/0/all/0/1\">Shuyang Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lu_W/0/1/0/all/0/1\">Wenlian Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>",
          "description": "InfoNCE-based contrastive representation learners, such as SimCLR, have been\ntremendously successful in recent years. However, these contrastive schemes are\nnotoriously resource demanding, as their effectiveness breaks down with\nsmall-batch training (i.e., the log-K curse, whereas K is the batch-size). In\nthis work, we reveal mathematically why contrastive learners fail in the\nsmall-batch-size regime, and present a novel simple, non-trivial contrastive\nobjective named FlatNCE, which fixes this issue. Unlike InfoNCE, our FlatNCE no\nlonger explicitly appeals to a discriminative classification goal for\ncontrastive learning. Theoretically, we show FlatNCE is the mathematical dual\nformulation of InfoNCE, thus bridging the classical literature on energy\nmodeling; and empirically, we demonstrate that, with minimal modification of\ncode, FlatNCE enables immediate performance boost independent of the\nsubject-matter engineering efforts. The significance of this work is furthered\nby the powerful generalization of contrastive learning techniques, and the\nintroduction of new tools to monitor and diagnose contrastive training. We\nsubstantiate our claims with empirical evidence on CIFAR10, ImageNet, and other\ndatasets, where FlatNCE consistently outperforms InfoNCE.",
          "link": "http://arxiv.org/abs/2107.01152",
          "publishedOn": "2021-07-05T01:54:59.311Z",
          "wordCount": 644,
          "title": "Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE. (arXiv:2107.01152v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuetz_M/0/1/0/all/0/1\">Martin J. A. Schuetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_J/0/1/0/all/0/1\">J. Kyle Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katzgraber_H/0/1/0/all/0/1\">Helmut G. Katzgraber</a>",
          "description": "We demonstrate how graph neural networks can be used to solve combinatorial\noptimization problems. Our approach is broadly applicable to canonical NP-hard\nproblems in the form of quadratic unconstrained binary optimization problems,\nsuch as maximum cut, minimum vertex cover, maximum independent set, as well as\nIsing spin glasses and higher-order generalizations thereof in the form of\npolynomial unconstrained binary optimization problems. We apply a relaxation\nstrategy to the problem Hamiltonian to generate a differentiable loss function\nwith which we train the graph neural network and apply a simple projection to\ninteger variables once the unsupervised training process has completed. We\nshowcase our approach with numerical results for the canonical maximum cut and\nmaximum independent set problems. We find that the graph neural network\noptimizer performs on par or outperforms existing solvers, with the ability to\nscale beyond the state of the art to problems with millions of variables.",
          "link": "http://arxiv.org/abs/2107.01188",
          "publishedOn": "2021-07-05T01:54:59.305Z",
          "wordCount": 616,
          "title": "Combinatorial Optimization with Physics-Inspired Graph Neural Networks. (arXiv:2107.01188v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cody_T/0/1/0/all/0/1\">Tyler Cody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1\">Stephen Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beling_P/0/1/0/all/0/1\">Peter A. Beling</a>",
          "description": "Classical machine learning approaches are sensitive to non-stationarity.\nTransfer learning can address non-stationarity by sharing knowledge from one\nsystem to another, however, in areas like machine prognostics and defense, data\nis fundamentally limited. Therefore, transfer learning algorithms have little,\nif any, examples from which to learn. Herein, we suggest that these constraints\non algorithmic learning can be addressed by systems engineering. We formally\ndefine transfer distance in general terms and demonstrate its use in\nempirically quantifying the transferability of models. We consider the use of\ntransfer distance in the design of machine rebuild procedures to allow for\ntransferable prognostic models. We also consider the use of transfer distance\nin predicting operational performance in computer vision. Practitioners can use\nthe presented methodology to design and operate systems with consideration for\nthe learning theoretic challenges faced by component learning systems.",
          "link": "http://arxiv.org/abs/2107.01184",
          "publishedOn": "2021-07-05T01:54:59.298Z",
          "wordCount": 576,
          "title": "Empirically Measuring Transfer Distance for System Design and Operation. (arXiv:2107.01184v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tengyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Ching-An Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1\">Paul Mineiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Alekh Agarwal</a>",
          "description": "The use of pessimism, when reasoning about datasets lacking exhaustive\nexploration has recently gained prominence in offline reinforcement learning.\nDespite the robustness it adds to the algorithm, overly pessimistic reasoning\ncan be equally damaging in precluding the discovery of good policies, which is\nan issue for the popular bonus-based pessimism. In this paper, we introduce the\nnotion of Bellman-consistent pessimism for general function approximation:\ninstead of calculating a point-wise lower bound for the value function, we\nimplement pessimism at the initial state over the set of functions consistent\nwith the Bellman equations. Our theoretical guarantees only require Bellman\nclosedness as standard in the exploratory setting, in which case bonus-based\npessimism fails to provide guarantees. Even in the special case of linear MDPs\nwhere stronger function-approximation assumptions hold, our result improves\nupon a recent bonus-based approach by $\\mathcal{O}(d)$ in its sample complexity\nwhen the action space is finite. Remarkably, our algorithms automatically adapt\nto the best bias-variance tradeoff in the hindsight, whereas most prior\napproaches require tuning extra hyperparameters a priori.",
          "link": "http://arxiv.org/abs/2106.06926",
          "publishedOn": "2021-07-05T01:54:59.261Z",
          "wordCount": 637,
          "title": "Bellman-consistent Pessimism for Offline Reinforcement Learning. (arXiv:2106.06926v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haike Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "This paper presents a new model-free algorithm for episodic finite-horizon\nMarkov Decision Processes (MDP), Adaptive Multi-step Bootstrap (AMB), which\nenjoys a stronger gap-dependent regret bound. The first innovation is to\nestimate the optimal $Q$-function by combining an optimistic bootstrap with an\nadaptive multi-step Monte Carlo rollout. The second innovation is to select the\naction with the largest confidence interval length among admissible actions\nthat are not dominated by any other actions. We show when each state has a\nunique optimal action, AMB achieves a gap-dependent regret bound that only\nscales with the sum of the inverse of the sub-optimality gaps. In contrast,\nSimchowitz and Jamieson (2019) showed all upper-confidence-bound (UCB)\nalgorithms suffer an additional $\\Omega\\left(\\frac{S}{\\Delta_{min}}\\right)$\nregret due to over-exploration where $\\Delta_{min}$ is the minimum\nsub-optimality gap and $S$ is the number of states. We further show that for\ngeneral MDPs, AMB suffers an additional $\\frac{|Z_{mul}|}{\\Delta_{min}}$\nregret, where $Z_{mul}$ is the set of state-action pairs $(s,a)$'s satisfying\n$a$ is a non-unique optimal action for $s$. We complement our upper bound with\na lower bound showing the dependency on $\\frac{|Z_{mul}|}{\\Delta_{min}}$ is\nunavoidable for any consistent algorithm. This lower bound also implies a\nseparation between reinforcement learning and contextual bandits.",
          "link": "http://arxiv.org/abs/2102.04692",
          "publishedOn": "2021-07-05T01:54:59.250Z",
          "wordCount": 670,
          "title": "Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap. (arXiv:2102.04692v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanoh_R/0/1/0/all/0/1\">Ryuichi Kanoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Mahito Sugiyama</a>",
          "description": "A multiplicative constant scaling factor is often applied to the model output\nto adjust the dynamics of neural network parameters. This has been used as one\nof the key interventions in an empirical study of lazy and active behavior.\nHowever, we show that the combination of such scaling and a commonly used\nadaptive learning rate optimizer strongly affects the training behavior of the\nneural network. This is problematic as it can cause \\emph{unintended behavior}\nof neural networks, resulting in the misinterpretation of experimental results.\nSpecifically, for some scaling settings, the effect of the adaptive learning\nrate disappears or is strongly influenced by the scaling factor. To avoid the\nunintended effect, we present a modification of an optimization algorithm and\ndemonstrate remarkable differences between adaptive learning rate optimization\nand simple gradient descent, especially with a small ($<1.0$) scaling factor.",
          "link": "http://arxiv.org/abs/2103.03466",
          "publishedOn": "2021-07-05T01:54:59.244Z",
          "wordCount": 607,
          "title": "Unintended Effects on Adaptive Learning Rate for Training Neural Network with Output Scale Change. (arXiv:2103.03466v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cunnington_D/0/1/0/all/0/1\">Daniel Cunnington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Mark Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1\">Alessandra Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1\">Jorge Lobo</a>",
          "description": "Inductive Logic Programming (ILP) aims to learn generalised, interpretable\nhypotheses in a data-efficient manner. However, current ILP systems require\ntraining examples to be specified in a structured logical form. To address this\nproblem, this paper proposes a neural-symbolic learning framework, called\nFeed-Forward Neural-Symbolic Learner (FF-NSL), that integrates state-of-the-art\nILP systems, based on the Answer Set semantics, with Neural Networks (NNs), in\norder to learn interpretable hypotheses from labelled unstructured data. To\ndemonstrate the generality and robustness of FF-NSL, we use two datasets\nsubject to distributional shifts, for which pre-trained NNs may give incorrect\npredictions with high confidence. Experimental results show that FF-NSL\noutperforms tree-based and neural-based approaches by learning more accurate\nand interpretable hypotheses with fewer examples.",
          "link": "http://arxiv.org/abs/2106.13103",
          "publishedOn": "2021-07-05T01:54:59.237Z",
          "wordCount": 572,
          "title": "FF-NSL: Feed-Forward Neural-Symbolic Learner. (arXiv:2106.13103v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1\">Mohammad Javad Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>",
          "description": "We study the problem of best-arm identification (BAI) in contextual bandits\nin the fixed-budget setting. We propose a general successive elimination\nalgorithm that proceeds in stages and eliminates a fixed fraction of suboptimal\narms in each stage. This design takes advantage of the strengths of static and\nadaptive allocations. We analyze the algorithm in linear models and obtain a\nbetter error bound than prior work. We also apply it to generalized linear\nmodels (GLMs) and bound its error. This is the first BAI algorithm for GLMs in\nthe fixed-budget setting. Our extensive numerical experiments show that our\nalgorithm outperforms the state of art.",
          "link": "http://arxiv.org/abs/2106.04763",
          "publishedOn": "2021-07-05T01:54:59.220Z",
          "wordCount": 582,
          "title": "Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03361",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Zotov_M/0/1/0/all/0/1\">Mikhail Zotov</a>",
          "description": "We employ neural networks for classification of data of the TUS fluorescence\ntelescope, the world's first orbital detector of ultra-high energy cosmic rays.\nWe focus on two particular types of signals in the TUS data: track-like flashes\nproduced by cosmic ray hits of the photodetector and flashes that originated\nfrom distant lightnings. We demonstrate that even simple neural networks\ncombined with certain conventional methods of data analysis can be highly\neffective in tasks of classification of data of fluorescence telescopes.",
          "link": "http://arxiv.org/abs/2106.03361",
          "publishedOn": "2021-07-05T01:54:59.214Z",
          "wordCount": 561,
          "title": "Application of neural networks to classification of data of the TUS orbital telescope. (arXiv:2106.03361v2 [astro-ph.IM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01034",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dumas_J/0/1/0/all/0/1\">Jonathan Dumas</a>",
          "description": "The Intergovernmental Panel on Climate Change proposes different mitigation\nstrategies to achieve the net emissions reductions that would be required to\nfollow a pathway that limits global warming to 1.5{\\deg}C with no or limited\novershoot. The transition towards a carbon-free society goes through an\ninevitable increase of the share of renewable generation in the energy mix and\na drastic decrease in terms of the total consumption of fossil fuels.\nTherefore, this thesis studies the integration of renewables in power systems\nby investigating forecasting and decision-making tools. Indeed, in contrast to\nconventional power plants, renewable energy is subject to uncertainty. Most of\nthe generation technologies based on renewable sources are non-dispatchable,\nand their production is stochastic and hard to predict in advance. A high share\nof renewables is a great challenge for power systems that have been designed\nand sized for dispatchable units. In this context, probabilistic forecasts,\nwhich aim at modeling the distribution of all possible future realizations,\nhave become an important tool to equip decision-makers, hopefully leading to\nbetter decisions in energy applications. This thesis focus on two main research\nquestions: (1) How to produce reliable probabilistic forecasts of renewable\ngeneration, consumption, and electricity prices? (2) How to make decisions with\nuncertainty using probabilistic forecasts? The thesis perimeter is the energy\nmanagement of \"small\" systems such as microgrids at a residential scale on a\nday-ahead basis. It is divided into two main parts to propose directions to\naddress both research questions (1) a forecasting part; (2) a planning and\ncontrol part.",
          "link": "http://arxiv.org/abs/2107.01034",
          "publishedOn": "2021-07-05T01:54:59.206Z",
          "wordCount": 724,
          "title": "Weather-based forecasting of energy generation, consumption and price for electrical microgrids management. (arXiv:2107.01034v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12828",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Heaton_H/0/1/0/all/0/1\">Howard Heaton</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_J/0/1/0/all/0/1\">Jialin Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1\">Wotao Yin</a>",
          "description": "Learning to optimize (L2O) is an emerging approach that leverages machine\nlearning to develop optimization methods, aiming at reducing the laborious\niterations of hand engineering. It automates the design of an optimization\nmethod based on its performance on a set of training problems. This data-driven\nprocedure generates methods that can efficiently solve problems similar to\nthose in the training. In sharp contrast, the typical and traditional designs\nof optimization methods are theory-driven, so they obtain performance\nguarantees over the classes of problems specified by the theory. The difference\nmakes L2O suitable for repeatedly solving a certain type of optimization\nproblems over a specific distribution of data, while it typically fails on\nout-of-distribution problems. The practicality of L2O depends on the type of\ntarget optimization, the chosen architecture of the method to learn, and the\ntraining procedure. This new paradigm has motivated a community of researchers\nto explore L2O and report their findings.\n\nThis article is poised to be the first comprehensive survey and benchmark of\nL2O for continuous optimization. We set up taxonomies, categorize existing\nworks and research directions, present insights, and identify open challenges.\nWe also benchmarked many existing L2O approaches on a few but representative\noptimization problems. For reproducible research and fair benchmarking\npurposes, we released our software implementation and data in the package\nOpen-L2O at https://github.com/VITA-Group/Open-L2O.",
          "link": "http://arxiv.org/abs/2103.12828",
          "publishedOn": "2021-07-05T01:54:59.199Z",
          "wordCount": 685,
          "title": "Learning to Optimize: A Primer and A Benchmark. (arXiv:2103.12828v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05842",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dwivedi_R/0/1/0/all/0/1\">Raaz Dwivedi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>",
          "description": "We introduce kernel thinning, a new procedure for compressing a distribution\n$\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given\na suitable reproducing kernel $\\mathbf{k}$ and $\\mathcal{O}(n^2)$ time, kernel\nthinning compresses an $n$-point approximation to $\\mathbb{P}$ into a\n$\\sqrt{n}$-point approximation with comparable worst-case integration error in\nthe associated reproducing kernel Hilbert space. With high probability, the\nmaximum discrepancy in integration error is\n$\\mathcal{O}_d(n^{-\\frac{1}{2}}\\sqrt{\\log n})$ for compactly supported\n$\\mathbb{P}$ and $\\mathcal{O}_d(n^{-\\frac{1}{2}} \\sqrt{(\\log n)^{d+1}\\log\\log\nn})$ for sub-exponential $\\mathbb{P}$ on $\\mathbb{R}^d$. In contrast, an\nequal-sized i.i.d. sample from $\\mathbb{P}$ suffers $\\Omega(n^{-\\frac14})$\nintegration error. Our sub-exponential guarantees resemble the classical\nquasi-Monte Carlo error rates for uniform $\\mathbb{P}$ on $[0,1]^d$ but apply\nto general distributions on $\\mathbb{R}^d$ and a wide range of common kernels.\nWe use our results to derive explicit non-asymptotic maximum mean discrepancy\nbounds for Gaussian, Mat\\'ern, and B-spline kernels and present two vignettes\nillustrating the practical benefits of kernel thinning over i.i.d. sampling and\nstandard Markov chain Monte Carlo thinning.",
          "link": "http://arxiv.org/abs/2105.05842",
          "publishedOn": "2021-07-05T01:54:59.191Z",
          "wordCount": 630,
          "title": "Kernel Thinning. (arXiv:2105.05842v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shingi_G/0/1/0/all/0/1\">Geet Shingi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saglani_H/0/1/0/all/0/1\">Harsh Saglani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Preeti Jain</a>",
          "description": "Cyberattacks are a major issues and it causes organizations great financial,\nand reputation harm. However, due to various factors, the current network\nintrusion detection systems (NIDS) seem to be insufficent. Predominant NIDS\nidentifies Cyberattacks through a handcrafted dataset of rules. Although the\nrecent applications of machine learning and deep learning have alleviated the\nenormous effort in NIDS, the security of network data has always been a prime\nconcern. However, to encounter the security problem and enable sharing among\norganizations, Federated Learning (FL) scheme is employed. Although the current\nFL systems have been successful, a network's data distribution does not always\nfit into a single global model as in FL. Thus, in such cases, having a single\nglobal model in FL is no feasible. In this paper, we propose a\nSegmented-Federated Learning (Segmented-FL) learning scheme for a more\nefficient NIDS. The Segmented-FL approach employs periodic local model\nevaluation based on which the segmentation occurs. We aim to bring similar\nnetwork environments to the same group. Further, the Segmented-FL system is\ncoupled with a weighted aggregation of local model parameters based on the\nnumber of data samples a worker possesses to further augment the performance.\nThe improved performance by our system as compared to the FL and centralized\nsystems on standard dataset further validates our system and makes a strong\ncase for extending our technique across various tasks. The solution finds its\napplication in organizations that want to collaboratively learn on diverse\nnetwork environments and protect the privacy of individual datasets.",
          "link": "http://arxiv.org/abs/2107.00881",
          "publishedOn": "2021-07-05T01:54:59.184Z",
          "wordCount": 712,
          "title": "Segmented Federated Learning for Adaptive Intrusion Detection System. (arXiv:2107.00881v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_T/0/1/0/all/0/1\">Thamme Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattmann_C/0/1/0/all/0/1\">Chris A Mattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>",
          "description": "While there are more than 7000 languages in the world, most translation\nresearch efforts have targeted a few high-resource languages. Commercial\ntranslation systems support only one hundred languages or fewer, and do not\nmake these models available for transfer to low resource languages. In this\nwork, we present useful tools for machine translation research: MTData,\nNLCodec, and RTG. We demonstrate their usefulness by creating a multilingual\nneural machine translation model capable of translating from 500 source\nlanguages to English. We make this multilingual model readily downloadable and\nusable as a service, or as a parent model for transfer-learning to even\nlower-resource languages.",
          "link": "http://arxiv.org/abs/2104.00290",
          "publishedOn": "2021-07-05T01:54:59.164Z",
          "wordCount": 578,
          "title": "Many-to-English Machine Translation Tools, Data, and Pretrained Models. (arXiv:2104.00290v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dovrat_S/0/1/0/all/0/1\">Shaked Dovrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1\">Eliya Nachmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "Single channel speech separation has experienced great progress in the last\nfew years. However, training neural speech separation for a large number of\nspeakers (e.g., more than 10 speakers) is out of reach for the current methods,\nwhich rely on the Permutation Invariant Loss (PIT). In this work, we present a\npermutation invariant training that employs the Hungarian algorithm in order to\ntrain with an $O(C^3)$ time complexity, where $C$ is the number of speakers, in\ncomparison to $O(C!)$ of PIT based methods. Furthermore, we present a modified\narchitecture that can handle the increased number of speakers. Our approach\nseparates up to $20$ speakers and improves the previous results for large $C$\nby a wide margin.",
          "link": "http://arxiv.org/abs/2104.08955",
          "publishedOn": "2021-07-05T01:54:59.156Z",
          "wordCount": 599,
          "title": "Many-Speakers Single Channel Speech Separation with Optimal Permutation Training. (arXiv:2104.08955v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xucheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fanxing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Ce Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>",
          "description": "Weakly-supervised anomaly detection aims at learning an anomaly detector from\na limited amount of labeled data and abundant unlabeled data. Recent works\nbuild deep neural networks for anomaly detection by discriminatively mapping\nthe normal samples and abnormal samples to different regions in the feature\nspace or fitting different distributions. However, due to the limited number of\nannotated anomaly samples, directly training networks with the discriminative\nloss may not be sufficient. To overcome this issue, this paper proposes a novel\nstrategy to transform the input data into a more meaningful representation that\ncould be used for anomaly detection. Specifically, we leverage an autoencoder\nto encode the input data and utilize three factors, hidden representation,\nreconstruction residual vector, and reconstruction error, as the new\nrepresentation for the input data. This representation amounts to encode a test\nsample with its projection on the training data manifold, its direction to its\nprojection and its distance to its projection. In addition to this encoding, we\nalso propose a novel network architecture to seamlessly incorporate those three\nfactors. From our extensive experiments, the benefits of the proposed strategy\nare clearly demonstrated by its superior performance over the competitive\nmethods.",
          "link": "http://arxiv.org/abs/2105.10500",
          "publishedOn": "2021-07-05T01:54:59.149Z",
          "wordCount": 701,
          "title": "Feature Encoding with AutoEncoders for Weakly-supervised Anomaly Detection. (arXiv:2105.10500v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramos_J/0/1/0/all/0/1\">Joao A. Candido Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blonde_L/0/1/0/all/0/1\">Lionel Blond&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armand_S/0/1/0/all/0/1\">St&#xe9;phane Armand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1\">Alexandros Kalousis</a>",
          "description": "In this work, we want to learn to model the dynamics of similar yet distinct\ngroups of interacting objects. These groups follow some common physical laws\nthat exhibit specificities that are captured through some vectorial\ndescription. We develop a model that allows us to do conditional generation\nfrom any such group given its vectorial description. Unlike previous work on\nlearning dynamical systems that can only do trajectory completion and require a\npart of the trajectory dynamics to be provided as input in generation time, we\ndo generation using only the conditioning vector with no access to generation\ntime's trajectories. We evaluate our model in the setting of modeling human\ngait and, in particular pathological human gait.",
          "link": "http://arxiv.org/abs/2106.11083",
          "publishedOn": "2021-07-05T01:54:59.142Z",
          "wordCount": 576,
          "title": "Conditional Neural Relational Inference for Interacting Systems. (arXiv:2106.11083v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11713",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lumini_A/0/1/0/all/0/1\">Alessandra Lumini</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Brahnam_S/0/1/0/all/0/1\">Sheryl Brahnam</a>",
          "description": "Motivation: Automatic Anatomical Therapeutic Chemical (ATC) classification is\na critical and highly competitive area of research in bioinformatics because of\nits potential for expediting drug develop-ment and research. Predicting an\nunknown compound's therapeutic and chemical characteristics ac-cording to how\nthese characteristics affect multiple organs/systems makes automatic ATC\nclassifica-tion a challenging multi-label problem. Results: In this work, we\npropose combining multiple multi-label classifiers trained on distinct sets of\nfeatures, including sets extracted from a Bidirectional Long Short-Term Memory\nNetwork (BiLSTM). Experiments demonstrate the power of this approach, which is\nshown to outperform the best methods reported in the literature, including the\nstate-of-the-art developed by the fast.ai research group. Availability: All\nsource code developed for this study is available at\nhttps://github.com/LorisNanni. Contact: loris.nanni@unipd.it",
          "link": "http://arxiv.org/abs/2101.11713",
          "publishedOn": "2021-07-05T01:54:59.135Z",
          "wordCount": 571,
          "title": "Neural networks for Anatomical Therapeutic Chemical (ATC). (arXiv:2101.11713v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellani_A/0/1/0/all/0/1\">Andrea Castellani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1\">Sebastian Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "In complex industrial settings, it is common practice to monitor the\noperation of machines in order to detect undesired states, adjust maintenance\nschedules, optimize system performance or collect usage statistics of\nindividual machines. In this work, we focus on estimating the power output of a\nCombined Heat and Power (CHP) machine of a medium-sized company facility by\nanalyzing the total facility power consumption. We formulate the problem as a\ntime-series classification problem where the class label represents the CHP\npower output. As the facility is fully instrumented and sensor measurements\nfrom the CHP are available, we generate the training labels in an automated\nfashion from the CHP sensor readings. However, sensor failures result in\nmislabeled training data samples which are hard to detect and remove from the\ndataset. Therefore, we propose a novel multi-task deep learning approach that\njointly trains a classifier and an autoencoder with a shared embedding\nrepresentation. The proposed approach targets to gradually correct the\nmislabelled data samples during training in a self-supervised fashion, without\nany prior assumption on the amount of label noise. We benchmark our approach on\nseveral time-series classification datasets and find it to be comparable and\nsometimes better than state-of-the-art methods. On the real-world use-case of\npredicting the CHP power output, we thoroughly evaluate the architectural\ndesign choices and show that the final architecture considerably increases the\nrobustness of the learning process and consistently beats other recent\nstate-of-the-art algorithms in the presence of unstructured as well as\nstructured label noise.",
          "link": "http://arxiv.org/abs/2105.00349",
          "publishedOn": "2021-07-05T01:54:59.116Z",
          "wordCount": 742,
          "title": "Estimating the electrical power output of industrial devices with end-to-end time-series classification in the presence of label noise. (arXiv:2105.00349v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.06006",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ding_S/0/1/0/all/0/1\">Shaojin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>",
          "description": "In this paper, we propose Textual Echo Cancellation (TEC) - a framework for\ncancelling the text-to-speech (TTS) playback echo from overlapping speech\nrecordings. Such a system can largely improve speech recognition performance\nand user experience for intelligent devices such as smart speakers, as the user\ncan talk to the device while the device is still playing the TTS signal\nresponding to the previous query. We implement this system by using a novel\nsequence-to-sequence model with multi-source attention that takes both the\nmicrophone mixture signal and source text of the TTS playback as inputs, and\npredicts the enhanced audio. Experiments show that the textual information of\nthe TTS playback is critical to enhancement performance. Besides, the text\nsequence is much smaller in size compared with the raw acoustic signal of the\nTTS playback, and can be immediately transmitted to the device or ASR server\neven before the playback is synthesized. Therefore, our proposed approach\neffectively reduces Internet communication and latency compared with\nalternative approaches such as acoustic echo cancellation (AEC).",
          "link": "http://arxiv.org/abs/2008.06006",
          "publishedOn": "2021-07-05T01:54:59.108Z",
          "wordCount": 632,
          "title": "Textual Echo Cancellation. (arXiv:2008.06006v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01106",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>",
          "description": "The conditional gradient method (CGM) is widely used in large-scale sparse\nconvex optimization, having a low per iteration computational cost for\nstructured sparse regularizers and a greedy approach to collecting nonzeros. We\nexplore the sparsity acquiring properties of a general penalized CGM (P-CGM)\nfor convex regularizers and a reweighted penalized CGM (RP-CGM) for nonconvex\nregularizers, replacing the usual convex constraints with gauge-inspired\npenalties. This generalization does not increase the per-iteration complexity\nnoticeably. Without assuming bounded iterates or using line search, we show\n$O(1/t)$ convergence of the gap of each subproblem, which measures distance to\na stationary point. We couple this with a screening rule which is safe in the\nconvex case, converging to the true support at a rate $O(1/(\\delta^2))$ where\n$\\delta \\geq 0$ measures how close the problem is to degeneracy. In the\nnonconvex case the screening rule converges to the true support in a finite\nnumber of iterations, but is not necessarily safe in the intermediate iterates.\nIn our experiments, we verify the consistency of the method and adjust the\naggressiveness of the screening rule by tuning the concavity of the\nregularizer.",
          "link": "http://arxiv.org/abs/2107.01106",
          "publishedOn": "2021-07-05T01:54:59.096Z",
          "wordCount": 617,
          "title": "Screening for a Reweighted Penalized Conditional Gradient Method. (arXiv:2107.01106v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yafang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>",
          "description": "Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2107.00967",
          "publishedOn": "2021-07-05T01:54:59.089Z",
          "wordCount": 582,
          "title": "R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling. (arXiv:2107.00967v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01103",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Majumdar_S/0/1/0/all/0/1\">Subhabrata Majumdar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chatterjee_S/0/1/0/all/0/1\">Snigdhansu Chatterjee</a>",
          "description": "High-dimensional data, where the dimension of the feature space is much\nlarger than sample size, arise in a number of statistical applications. In this\ncontext, we construct the generalized multivariate sign transformation, defined\nas a vector divided by its norm. For different choices of the norm function,\nthe resulting transformed vector adapts to certain geometrical features of the\ndata distribution. Building up on this idea, we obtain one-sample and\ntwo-sample testing procedures for mean vectors of high-dimensional data using\nthese generalized sign vectors. These tests are based on U-statistics using\nkernel inner products, do not require prohibitive assumptions, and are amenable\nto a fast randomization-based implementation. Through experiments in a number\nof data settings, we show that tests using generalized signs display higher\npower than existing tests, while maintaining nominal type-I error rates.\nFinally, we provide example applications on the MNIST and Minnesota Twin\nStudies genomic data.",
          "link": "http://arxiv.org/abs/2107.01103",
          "publishedOn": "2021-07-05T01:54:59.079Z",
          "wordCount": 583,
          "title": "Generalized Multivariate Signs for Nonparametric Hypothesis Testing in High Dimensions. (arXiv:2107.01103v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01163",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Baldassi_C/0/1/0/all/0/1\">Carlo Baldassi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lauditi_C/0/1/0/all/0/1\">Clarissa Lauditi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Malatesta_E/0/1/0/all/0/1\">Enrico M. Malatesta</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Perugini_G/0/1/0/all/0/1\">Gabriele Perugini</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zecchina_R/0/1/0/all/0/1\">Riccardo Zecchina</a>",
          "description": "The success of deep learning has revealed the application potential of neural\nnetworks across the sciences and opened up fundamental theoretical problems. In\nparticular, the fact that learning algorithms based on simple variants of\ngradient methods are able to find near-optimal minima of highly nonconvex loss\nfunctions is an unexpected feature of neural networks which needs to be\nunderstood in depth. Such algorithms are able to fit the data almost perfectly,\neven in the presence of noise, and yet they have excellent predictive\ncapabilities. Several empirical results have shown a reproducible correlation\nbetween the so-called flatness of the minima achieved by the algorithms and the\ngeneralization performance. At the same time, statistical physics results have\nshown that in nonconvex networks a multitude of narrow minima may coexist with\na much smaller number of wide flat minima, which generalize well. Here we show\nthat wide flat minima arise from the coalescence of minima that correspond to\nhigh-margin classifications. Despite being exponentially rare compared to\nzero-margin solutions, high-margin minima tend to concentrate in particular\nregions. These minima are in turn surrounded by other solutions of smaller and\nsmaller margin, leading to dense regions of solutions over long distances. Our\nanalysis also provides an alternative analytical method for estimating when\nflat minima appear and when algorithms begin to find solutions, as the number\nof model parameters varies.",
          "link": "http://arxiv.org/abs/2107.01163",
          "publishedOn": "2021-07-05T01:54:59.061Z",
          "wordCount": 687,
          "title": "Unveiling the structure of wide flat minima in neural networks. (arXiv:2107.01163v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marz_L/0/1/0/all/0/1\">Luisa M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweter_S/0/1/0/all/0/1\">Stefan Schweter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poerner_N/0/1/0/all/0/1\">Nina Poerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "We propose new methods for in-domain and cross-domain Named Entity\nRecognition (NER) on historical data for Dutch and French. For the cross-domain\ncase, we address domain shift by integrating unsupervised in-domain data via\ncontextualized string embeddings; and OCR errors by injecting synthetic OCR\nerrors into the source domain and address data centric domain adaptation. We\npropose a general approach to imitate OCR errors in arbitrary input data. Our\ncross-domain as well as our in-domain results outperform several strong\nbaselines and establish state-of-the-art results. We publish preprocessed\nversions of the French and Dutch Europeana NER corpora.",
          "link": "http://arxiv.org/abs/2107.00927",
          "publishedOn": "2021-07-05T01:54:59.047Z",
          "wordCount": 543,
          "title": "Data Centric Domain Adaptation for Historical Text with OCR Errors. (arXiv:2107.00927v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13922",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rosca_M/0/1/0/all/0/1\">Mihaela Rosca</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1\">Yan Wu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dherin_B/0/1/0/all/0/1\">Benoit Dherin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Barrett_D/0/1/0/all/0/1\">David G. T. Barrett</a>",
          "description": "Gradient-based methods for two-player games produce rich dynamics that can\nsolve challenging problems, yet can be difficult to stabilize and understand.\nPart of this complexity originates from the discrete update steps given by\nsimultaneous or alternating gradient descent, which causes each player to drift\naway from the continuous gradient flow -- a phenomenon we call discretization\ndrift. Using backward error analysis, we derive modified continuous dynamical\nsystems that closely follow the discrete dynamics. These modified dynamics\nprovide an insight into the notorious challenges associated with zero-sum\ngames, including Generative Adversarial Networks. In particular, we identify\ndistinct components of the discretization drift that can alter performance and\nin some cases destabilize the game. Finally, quantifying discretization drift\nallows us to identify regularizers that explicitly cancel harmful forms of\ndrift or strengthen beneficial forms of drift, and thus improve performance of\nGAN training.",
          "link": "http://arxiv.org/abs/2105.13922",
          "publishedOn": "2021-07-05T01:54:59.041Z",
          "wordCount": 587,
          "title": "Discretization Drift in Two-Player Games. (arXiv:2105.13922v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01017",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Menezes_A/0/1/0/all/0/1\">Angelo Garangau Menezes</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mastelini_S/0/1/0/all/0/1\">Saulo Martiello Mastelini</a>",
          "description": "Forecasting financial time series is considered to be a difficult task due to\nthe chaotic feature of the series. Statistical approaches have shown solid\nresults in some specific problems such as predicting market direction and\nsingle-price of stocks; however, with the recent advances in deep learning and\nbig data techniques, new promising options have arises to tackle financial time\nseries forecasting. Moreover, recent literature has shown that employing a\ncombination of statistics and machine learning may improve accuracy in the\nforecasts in comparison to single solutions. Taking into consideration the\nmentioned aspects, in this work, we proposed the MegazordNet, a framework that\nexplores statistical features within a financial series combined with a\nstructured deep learning model for time series forecasting. We evaluated our\napproach predicting the closing price of stocks in the S&P 500 using different\nmetrics, and we were able to beat single statistical and machine learning\nmethods.",
          "link": "http://arxiv.org/abs/2107.01017",
          "publishedOn": "2021-07-05T01:54:59.002Z",
          "wordCount": 598,
          "title": "MegazordNet: combining statistical and machine learning standpoints for time series forecasting. (arXiv:2107.01017v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suwannaphong_T/0/1/0/all/0/1\">Thanaphon Suwannaphong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavana_S/0/1/0/all/0/1\">Sawaphob Chavana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tongsom_S/0/1/0/all/0/1\">Sahapol Tongsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palasuwan_D/0/1/0/all/0/1\">Duangdao Palasuwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalidabhongse_T/0/1/0/all/0/1\">Thanarat H. Chalidabhongse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>",
          "description": "Intestinal parasitic infection leads to several morbidities to humans\nworldwide, especially in tropical countries. The traditional diagnosis usually\nrelies on manual analysis from microscopic images which is prone to human error\ndue to morphological similarity of different parasitic eggs and abundance of\nimpurities in a sample. Many studies have developed automatic systems for\nparasite egg detection to reduce human workload. However, they work with high\nquality microscopes, which unfortunately remain unaffordable in some rural\nareas. Our work thus exploits a benefit of a low-cost USB microscope. This\ninstrument however provides poor quality of images due to limitation of\nmagnification (10x), causing difficulty in parasite detection and species\nclassification. In this paper, we propose a CNN-based technique using transfer\nlearning strategy to enhance the efficiency of automatic parasite\nclassification in poor-quality microscopic images. The patch-based technique\nwith sliding window is employed to search for location of the eggs. Two\nnetworks, AlexNet and ResNet50, are examined with a trade-off between\narchitecture size and classification performance. The results show that our\nproposed framework outperforms the state-of-the-art object recognition methods.\nOur system combined with final decision from an expert may improve the real\nfaecal examination with low-cost microscopes.",
          "link": "http://arxiv.org/abs/2107.00968",
          "publishedOn": "2021-07-05T01:54:58.985Z",
          "wordCount": 654,
          "title": "Parasitic Egg Detection and Classification in Low-cost Microscopic Images using Transfer Learning. (arXiv:2107.00968v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sijia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhechba_M/0/1/0/all/0/1\">Mehdi Boukhechba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_L/0/1/0/all/0/1\">Laura E. Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daqing Zhang</a>",
          "description": "Mobile Sensing Apps have been widely used as a practical approach to collect\nbehavioral and health-related information from individuals and provide timely\nintervention to promote health and well-beings, such as mental health and\nchronic cares. As the objectives of mobile sensing could be either \\emph{(a)\npersonalized medicine for individuals} or \\emph{(b) public health for\npopulations}, in this work we review the design of these mobile sensing apps,\nand propose to categorize the design of these apps/systems in two paradigms --\n\\emph{(i) Personal Sensing} and \\emph{(ii) Crowd Sensing} paradigms. While both\nsensing paradigms might incorporate with common ubiquitous sensing\ntechnologies, such as wearable sensors, mobility monitoring, mobile data\noffloading, and/or cloud-based data analytics to collect and process sensing\ndata from individuals, we present a novel taxonomy system with two major\ncomponents that can specify and classify apps/systems from aspects of the\nlife-cycle of mHealth Sensing: \\emph{(1) Sensing Task Creation \\&\nParticipation}, \\emph{(2) Health Surveillance \\& Data Collection}, and\n\\emph{(3) Data Analysis \\& Knowledge Discovery}. With respect to different\ngoals of the two paradigms, this work systematically reviews this field, and\nsummarizes the design of typical apps/systems in the view of the configurations\nand interactions between these two components. In addition to summarization,\nthe proposed taxonomy system also helps figure out the potential directions of\nmobile sensing for health from both personalized medicines and population\nhealth perspectives.",
          "link": "http://arxiv.org/abs/2107.00948",
          "publishedOn": "2021-07-05T01:54:58.978Z",
          "wordCount": 685,
          "title": "From Personalized Medicine to Population Health: A Survey of mHealth Sensing Techniques. (arXiv:2107.00948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01032",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shezan_S/0/1/0/all/0/1\">SK. A. Shezan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rawdah_S/0/1/0/all/0/1\">S. Rawdah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Shafin Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_Z/0/1/0/all/0/1\">Ziaur Rahman</a>",
          "description": "The energy demand is growing daily at an accelerated pace due to the\ninternationalization and development of civilization. Yet proper economic\nutilization of additional energy generated by the Islanded Hybrid Microgrid\nSystem (IHMS) that was not consumed by the load is a major global challenge. To\nresolve the above-stated summons, this research focuses on a multi-optimal\ncombination of IHMS for the Penang Hill Resort located on Penang Island,\nMalaysia, with effective use of redundant energy. To avail this excess energy\nefficiently, an electrical heater along with a storage tank has been designed\nconcerning diversion load having proper energy management. Furthermore, the\nsystem design has adopted the HOMER Pro software for profitable and practical\nanalysis. Alongside, MATLAB Simulink had stabilized the whole system by\nrepresenting the values of 2068 and 19,072 kW that have been determined as the\napproximated peak and average load per day for the resort. Moreover, the\noptimized IHMS is comprehended of Photovoltaic (PV) cells, Diesel Generator,\nWind Turbine, Battery, and Converter. Adjacent to this, the optimized system\nensued in having a Net Present Cost (NPC) of $21.66 million, Renewable Fraction\n(RF) of 27.8%, Cost of Energy (COE) of $0.165/kWh, CO2 of 1,735,836 kg/year,\nand excess energy of 517.29MWh per annum. Since the diesel generator lead\nsystem was included in the scheme, a COE of $0.217/kWh, CO2 of 5,124,879\nkg/year, and NPC of $23.25 million were attained. The amount of excess energy\nis effectively utilized with an electrical heater as a diversion load.",
          "link": "http://arxiv.org/abs/2107.01032",
          "publishedOn": "2021-07-05T01:54:58.971Z",
          "wordCount": 736,
          "title": "Design and implementation of an islanded hybrid microgrid system for a large resort center for Penang Island with the proper application of excess energy. (arXiv:2107.01032v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafeiropoulos_C/0/1/0/all/0/1\">Charalampos Zafeiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzortzis_I/0/1/0/all/0/1\">Ioannis N. Tzortzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallis_I/0/1/0/all/0/1\">Ioannis Rallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_A/0/1/0/all/0/1\">Anastasios Doulamis</a>",
          "description": "In this paper, we scrutinize the effectiveness of various clustering\ntechniques, investigating their applicability in Cultural Heritage monitoring\napplications. In the context of this paper, we detect the level of\ndecomposition and corrosion on the walls of Saint Nicholas fort in Rhodes\nutilizing hyperspectral images. A total of 6 different clustering approaches\nhave been evaluated over a set of 14 different orthorectified hyperspectral\nimages. Experimental setup in this study involves K-means, Spectral, Meanshift,\nDBSCAN, Birch and Optics algorithms. For each of these techniques we evaluate\nits performance by the use of performance metrics such as Calinski-Harabasz,\nDavies-Bouldin indexes and Silhouette value. In this approach, we evaluate the\noutcomes of the clustering methods by comparing them with a set of annotated\nimages which denotes the ground truth regarding the decomposition and/or\ncorrosion area of the original images. The results depict that a few clustering\ntechniques applied on the given dataset succeeded decent accuracy, precision,\nrecall and f1 scores. Eventually, it was observed that the deterioration was\ndetected quite accurately.",
          "link": "http://arxiv.org/abs/2107.00964",
          "publishedOn": "2021-07-05T01:54:58.965Z",
          "wordCount": 616,
          "title": "Evaluating the Usefulness of Unsupervised monitoring in Cultural Heritage Monuments. (arXiv:2107.00964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_P/0/1/0/all/0/1\">Pierpaolo Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazzi_M/0/1/0/all/0/1\">Marya Bazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1\">Mihai Cucuringu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGillivray_B/0/1/0/all/0/1\">Barbara McGillivray</a>",
          "description": "Lexical semantic change (detecting shifts in the meaning and usage of words)\nis an important task for social and cultural studies as well as for Natural\nLanguage Processing applications. Diachronic word embeddings (time-sensitive\nvector representations of words that preserve their meaning) have become the\nstandard resource for this task. However, given the significant computational\nresources needed for their generation, very few resources exist that make\ndiachronic word embeddings available to the scientific community.\n\nIn this paper we present DUKweb, a set of large-scale resources designed for\nthe diachronic analysis of contemporary English. DUKweb was created from the\nJISC UK Web Domain Dataset (1996-2013), a very large archive which collects\nresources from the Internet Archive that were hosted on domains ending in\n`.uk'. DUKweb consists of a series word co-occurrence matrices and two types of\nword embeddings for each year in the JISC UK Web Domain dataset. We show the\nreuse potential of DUKweb and its quality standards via a case study on word\nmeaning change detection.",
          "link": "http://arxiv.org/abs/2107.01076",
          "publishedOn": "2021-07-05T01:54:58.957Z",
          "wordCount": 617,
          "title": "DUKweb: Diachronic word representations from the UK Web Archive corpus. (arXiv:2107.01076v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karmanov_I/0/1/0/all/0/1\">Ilia Karmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanjani_F/0/1/0/all/0/1\">Farhad G. Zanjani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlin_S/0/1/0/all/0/1\">Simone Merlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadampot_I/0/1/0/all/0/1\">Ishaque Kadampot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijkman_D/0/1/0/all/0/1\">Daniel Dijkman</a>",
          "description": "We introduce WiCluster, a new machine learning (ML) approach for passive\nindoor positioning using radio frequency (RF) channel state information (CSI).\nWiCluster can predict both a zone-level position and a precise 2D or 3D\nposition, without using any precise position labels during training. Prior\nCSI-based indoor positioning work has relied on non-parametric approaches using\ndigital signal-processing (DSP) and, more recently, parametric approaches\n(e.g., fully supervised ML methods). However these do not handle the complexity\nof real-world environments well and do not meet requirements for large-scale\ncommercial deployments: the accuracy of DSP-based method deteriorates\nsignificantly in non-line-of-sight conditions, while supervised ML methods need\nlarge amounts of hard-to-acquire centimeter accuracy position labels. In\ncontrast, WiCluster is both precise and requires weaker label-information that\ncan be easily collected. Our first contribution is a novel dimensionality\nreduction method for charting. It combines a triplet-loss with a multi-scale\nclustering-loss to map the high-dimensional CSI representation to a 2D/3D\nlatent space. Our second contribution is two weakly supervised losses that map\nthis latent space into a Cartesian map, resulting in meter-accuracy position\nresults. These losses only require simple to acquire priors: a sketch of the\nfloorplan, approximate location of access-point locations and a few CSI packets\nthat are labeled with the corresponding zone in the floorplan. Thirdly, we\nreport results and a robustness study for 2D positioning in a single-floor\noffice building and 3D positioning in a two-floor home to show the robustness\nof our method.",
          "link": "http://arxiv.org/abs/2107.01002",
          "publishedOn": "2021-07-05T01:54:58.940Z",
          "wordCount": 696,
          "title": "WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise Labels. (arXiv:2107.01002v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jitani_A/0/1/0/all/0/1\">Anirudha Jitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1\">Aditya Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhongwen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abou_zeid_H/0/1/0/all/0/1\">Hatem Abou-zeid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fapi_E/0/1/0/all/0/1\">Emmanuel T. Fapi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purmehdi_H/0/1/0/all/0/1\">Hakimeh Purmehdi</a>",
          "description": "Mobile Edge Computing (MEC) refers to the concept of placing computational\ncapability and applications at the edge of the network, providing benefits such\nas reduced latency in handling client requests, reduced network congestion, and\nimproved performance of applications. The performance and reliability of MEC\nare degraded significantly when one or several edge servers in the cluster are\noverloaded. Especially when a server crashes due to the overload, it causes\nservice failures in MEC. In this work, an adaptive admission control policy to\nprevent edge node from getting overloaded is presented. This approach is based\non a recently-proposed low complexity RL (Reinforcement Learning) algorithm\ncalled SALMUT (Structure-Aware Learning for Multiple Thresholds), which\nexploits the structure of the optimal admission control policy in multi-class\nqueues for an average-cost setting. We extend the framework to work for node\noverload-protection problem in a discounted-cost setting. The proposed solution\nis validated using several scenarios mimicking real-world deployments in two\ndifferent settings - computer simulations and a docker testbed. Our empirical\nevaluations show that the total discounted cost incurred by SALMUT is similar\nto state-of-the-art deep RL algorithms such as PPO (Proximal Policy\nOptimization) and A2C (Advantage Actor Critic) but requires an order of\nmagnitude less time to train, outputs easily interpretable policy, and can be\ndeployed in an online manner.",
          "link": "http://arxiv.org/abs/2107.01025",
          "publishedOn": "2021-07-05T01:54:58.932Z",
          "wordCount": 671,
          "title": "Structure-aware reinforcement learning for node-overload protection in mobile edge computing. (arXiv:2107.01025v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1\">Tony Q. S. Quek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chaoqun You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xianbin Cao</a>",
          "description": "This paper investigates the problem of providing ultra-reliable and\nenergy-efficient virtual reality (VR) experiences for wireless mobile users. To\nensure reliable ultra-high-definition (UHD) video frame delivery to mobile\nusers and enhance their immersive visual experiences, a coordinated multipoint\n(CoMP) transmission technique and millimeter wave (mmWave) communications are\nexploited. Owing to user movement and time-varying wireless channels, the\nwireless VR experience enhancement problem is formulated as a\nsequence-dependent and mixed-integer problem with a goal of maximizing users'\nfeeling of presence (FoP) in the virtual world, subject to power consumption\nconstraints on access points (APs) and users' head-mounted displays (HMDs). The\nproblem, however, is hard to be directly solved due to the lack of users'\naccurate tracking information and the sequence-dependent and mixed-integer\ncharacteristics. To overcome this challenge, we develop a parallel echo state\nnetwork (ESN) learning method to predict users' tracking information by\ntraining fresh and historical tracking samples separately collected by APs.\nWith the learnt results, we propose a deep reinforcement learning (DRL) based\noptimization algorithm to solve the formulated problem. In this algorithm, we\nimplement deep neural networks (DNNs) as a scalable solution to produce integer\ndecision variables and solving a continuous power control problem to criticize\nthe integer decision variables. Finally, the performance of the proposed\nalgorithm is compared with various benchmark algorithms, and the impact of\ndifferent design parameters is also discussed. Simulation results demonstrate\nthat the proposed algorithm is more 4.14% energy-efficient than the benchmark\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.01001",
          "publishedOn": "2021-07-05T01:54:58.924Z",
          "wordCount": 696,
          "title": "Feeling of Presence Maximization: mmWave-Enabled Virtual Reality Meets Deep Reinforcement Learning. (arXiv:2107.01001v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Albert Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chun Yin Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hains_G/0/1/0/all/0/1\">Ga&#xe9;tan Hains</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humphrey_J/0/1/0/all/0/1\">Jack Humphrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuhrmann_H/0/1/0/all/0/1\">Hans Fuhrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khmelevsky_Y/0/1/0/all/0/1\">Youry Khmelevsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazur_C/0/1/0/all/0/1\">Chris Mazur</a>",
          "description": "Gamers Private Network (GPN) is a client/server technology that guarantees a\nconnection for online video games that is more reliable and lower latency than\na standard internet connection. Users of the GPN technology benefit from a\nstable and high-quality gaming experience for online games, which are hosted\nand played across the world. After transforming a massive volume of raw\nnetworking data collected by WTFast, we have structured the cleaned data into a\nspecial-purpose data warehouse and completed the extensive analysis using\nmachine learning and neural nets technologies, and business intelligence tools.\nThese analyses demonstrate the ability to predict and quantify changes in the\nnetwork and demonstrate the benefits gained from the use of a GPN for users\nwhen connected to an online game session.",
          "link": "http://arxiv.org/abs/2107.00998",
          "publishedOn": "2021-07-05T01:54:58.916Z",
          "wordCount": 595,
          "title": "Gamers Private Network Performance Forecasting. From Raw Data to the Data Warehouse with Machine Learning and Neural Nets. (arXiv:2107.00998v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monperrus_M/0/1/0/all/0/1\">Martin Monperrus</a>",
          "description": "Semantic code search is about finding semantically relevant code snippets for\na given natural language query. In the state-of-the-art approaches, the\nsemantic similarity between code and query is quantified as the distance of\ntheir representation in the shared vector space. In this paper, to improve the\nvector space, we introduce tree-serialization methods on a simplified form of\nAST and build the multimodal representation for the code data. We conduct\nextensive experiments using a single corpus that is large-scale and\nmulti-language: CodeSearchNet. Our results show that both our tree-serialized\nrepresentations and multimodal learning model improve the performance of neural\ncode search. Last, we define two intuitive quantification metrics oriented to\nthe completeness of semantic and syntactic information of the code data.",
          "link": "http://arxiv.org/abs/2107.00992",
          "publishedOn": "2021-07-05T01:54:58.901Z",
          "wordCount": 556,
          "title": "Multimodal Representation for Neural Code Search. (arXiv:2107.00992v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2104.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaicheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zihuiwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "With the rapid development of NLP research, leaderboards have emerged as one\ntool to track the performance of various systems on various NLP tasks. They are\neffective in this goal to some extent, but generally present a rather\nsimplistic one-dimensional view of the submitted systems, communicated only\nthrough holistic accuracy numbers. In this paper, we present a new\nconceptualization and implementation of NLP evaluation: the ExplainaBoard,\nwhich in addition to inheriting the functionality of the standard leaderboard,\nalso allows researchers to (i) diagnose strengths and weaknesses of a single\nsystem (e.g.~what is the best-performing system bad at?) (ii) interpret\nrelationships between multiple systems. (e.g.~where does system A outperform\nsystem B? What if we combine systems A, B, and C?) and (iii) examine prediction\nresults closely (e.g.~what are common errors made by multiple systems, or in\nwhat contexts do particular errors occur?). So far, ExplainaBoard covers more\nthan 400 systems, 50 datasets, 40 languages, and 12 tasks. ExplainaBoard keeps\nupdated and is recently upgraded by supporting (1) multilingual multi-task\nbenchmark, (2) meta-evaluation, and (3) more complicated task: machine\ntranslation, which reviewers also suggested.} We not only released an online\nplatform on the website \\url{this http URL} but also make\nour evaluation tool an API with MIT Licence at Github\n\\url{https://github.com/neulab/explainaBoard} and PyPi\n\\url{https://pypi.org/project/interpret-eval/} that allows users to\nconveniently assess their models offline. We additionally release all output\nfiles from systems that we have run or collected to motivate \"output-driven\"\nresearch in the future.",
          "link": "http://arxiv.org/abs/2104.06387",
          "publishedOn": "2021-07-05T01:54:58.894Z",
          "wordCount": 724,
          "title": "ExplainaBoard: An Explainable Leaderboard for NLP. (arXiv:2104.06387v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.09379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gepperth_A/0/1/0/all/0/1\">Alexander Gepperth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfulb_B/0/1/0/all/0/1\">Benedikt Pf&#xfc;lb</a>",
          "description": "We present an approach for efficiently training Gaussian Mixture Model (GMM)\nby Stochastic Gradient Descent (SGD) with non-stationary, high-dimensional\nstreaming data. Our training scheme does not require data-driven parameter\ninitialization (e.g., k-means) and can thus be trained based on a random\ninitialization. Furthermore, the approach allows mini-batch sizes as low as 1,\nwhich are typical for streaming-data settings. Major problems in such settings\nare undesirable local optima during early training phases and numerical\ninstabilities due to high data dimensionalities. We introduce an adaptive\nannealing procedure to address the first problem, whereas numerical\ninstabilities are eliminated by using an exponential-free approximation to the\nstandard GMM log-likelihood. Experiments on a variety of visual and non-visual\nbenchmarks show that our SGD approach can be trained completely without, for\ninstance, k-means based centroid initialization. It also compares favorably to\nan online variant of Expectation-Maximization (EM) - stochastic EM (sEM), which\nit outperforms by a large margin for very high-dimensional data.",
          "link": "http://arxiv.org/abs/1912.09379",
          "publishedOn": "2021-07-05T01:54:58.854Z",
          "wordCount": 635,
          "title": "Gradient-based training of Gaussian Mixture Models for High-Dimensional Streaming Data. (arXiv:1912.09379v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+dEon_G/0/1/0/all/0/1\">Greg d&#x27;Eon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dEon_J/0/1/0/all/0/1\">Jason d&#x27;Eon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1\">James R. Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1\">Kevin Leyton-Brown</a>",
          "description": "Supervised learning models often make systematic errors on rare subsets of\nthe data. However, such systematic errors can be difficult to identify, as\nmodel performance can only be broken down across sensitive groups when these\ngroups are known and explicitly labelled. This paper introduces a method for\ndiscovering systematic errors, which we call the spotlight. The key idea is\nthat similar inputs tend to have similar representations in the final hidden\nlayer of a neural network. We leverage this structure by \"shining a spotlight\"\non this representation space to find contiguous regions where the model\nperforms poorly. We show that the spotlight surfaces semantically meaningful\nareas of weakness in a wide variety of model architectures, including image\nclassifiers, language models, and recommender systems.",
          "link": "http://arxiv.org/abs/2107.00758",
          "publishedOn": "2021-07-05T01:54:58.836Z",
          "wordCount": 569,
          "title": "The Spotlight: A General Method for Discovering Systematic Errors in Deep Learning Models. (arXiv:2107.00758v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Younsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_D/0/1/0/all/0/1\">Dongjin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huh_S/0/1/0/all/0/1\">Soonsang Huh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongjoon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1\">Sunbeom Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Junyoung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1\">Hanyoung Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jongkeun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyung_W/0/1/0/all/0/1\">Wonshik Kyung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_B/0/1/0/all/0/1\">Byungmin Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Suyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyun_J/0/1/0/all/0/1\">Jounghoon Hyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeonghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeongkwan Kimand Changyoung Kim</a>",
          "description": "In spectroscopic experiments, data acquisition in multi-dimensional phase\nspace may require long acquisition time, owing to the large phase space volume\nto be covered. In such case, the limited time available for data acquisition\ncan be a serious constraint for experiments in which multidimensional spectral\ndata are acquired. Here, taking angle-resolved photoemission spectroscopy\n(ARPES) as an example, we demonstrate a denoising method that utilizes deep\nlearning as an intelligent way to overcome the constraint. With readily\navailable ARPES data and random generation of training data set, we\nsuccessfully trained the denoising neural network without overfitting. The\ndenoising neural network can remove the noise in the data while preserving its\nintrinsic information. We show that the denoising neural network allows us to\nperform similar level of second-derivative and line shape analysis on data\ntaken with two orders of magnitude less acquisition time. The importance of our\nmethod lies in its applicability to any multidimensional spectral data that are\nsusceptible to statistical noise.",
          "link": "http://arxiv.org/abs/2107.00844",
          "publishedOn": "2021-07-05T01:54:58.830Z",
          "wordCount": 645,
          "title": "Deep learning-based statistical noise reduction for multidimensional spectral data. (arXiv:2107.00844v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1\">Davood Zabihzadeh</a>",
          "description": "Deep Metric Learning (DML) learns a non-linear semantic embedding from input\ndata that brings similar pairs together while keeps dissimilar data away from\neach other. To this end, many different methods are proposed in the last decade\nwith promising results in various applications. The success of a DML algorithm\ngreatly depends on its loss function. However, no loss function is perfect, and\nit deals only with some aspects of an optimal similarity embedding. Besides,\nthe generalizability of the DML on unseen categories during the test stage is\nan important matter that is not considered by existing loss functions. To\naddress these challenges, we propose novel approaches to combine different\nlosses built on top of a shared deep feature extractor. The proposed ensemble\nof losses enforces the deep model to extract features that are consistent with\nall losses. Since the selected losses are diverse and each emphasizes different\naspects of an optimal semantic embedding, our effective combining methods yield\na considerable improvement over any individual loss and generalize well on\nunseen categories. Here, there is no limitation in choosing loss functions, and\nour methods can work with any set of existing ones. Besides, they can optimize\neach loss function as well as its weight in an end-to-end paradigm with no need\nto adjust any hyper-parameter. We evaluate our methods on some popular datasets\nfrom the machine vision domain in conventional Zero-Shot-Learning (ZSL)\nsettings. The results are very encouraging and show that our methods outperform\nall baseline losses by a large margin in all datasets.",
          "link": "http://arxiv.org/abs/2107.01130",
          "publishedOn": "2021-07-05T01:54:58.815Z",
          "wordCount": 713,
          "title": "Ensemble of Loss Functions to Improve Generalizability of Deep Metric Learning methods. (arXiv:2107.01130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "We introduce Neural Marching Cubes (NMC), a data-driven approach for\nextracting a triangle mesh from a discretized implicit field. Classical MC is\ndefined by coarse tessellation templates isolated to individual cubes. While\nmore refined tessellations have been proposed, they all make heuristic\nassumptions, such as trilinearity, when determining the vertex positions and\nlocal mesh topologies in each cube. In principle, none of these approaches can\nreconstruct geometric features that reveal coherence or dependencies between\nnearby cubes (e.g., a sharp edge), as such information is unaccounted for,\nresulting in poor estimates of the true underlying implicit field. To tackle\nthese challenges, we re-cast MC from a deep learning perspective, by designing\ntessellation templates more apt at preserving geometric features, and learning\nthe vertex positions and mesh topologies from training meshes, to account for\ncontextual information from nearby cubes. We develop a compact per-cube\nparameterization to represent the output triangle mesh, while being compatible\nwith neural processing, so that a simple 3D convolutional network can be\nemployed for the training. We show that all topological cases in each cube that\nare applicable to our design can be easily derived using our representation,\nand the resulting tessellations can also be obtained naturally and efficiently\nby following a few design guidelines. In addition, our network learns local\nfeatures with limited receptive fields, hence it generalizes well to new shapes\nand new datasets. We evaluate our neural MC approach by quantitative and\nqualitative comparisons to all well-known MC variants. In particular, we\ndemonstrate the ability of our network to recover sharp features such as edges\nand corners, a long-standing issue of MC and its variants. Our network also\nreconstructs local mesh topologies more accurately than previous approaches.",
          "link": "http://arxiv.org/abs/2106.11272",
          "publishedOn": "2021-07-05T01:54:58.780Z",
          "wordCount": 738,
          "title": "Neural Marching Cubes. (arXiv:2106.11272v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiulong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>",
          "description": "Joint Energy-based Model (JEM) of Grathwohl et al. shows that a standard\nsoftmax classifier can be reinterpreted as an energy-based model (EBM) for the\njoint distribution p(x,y); the resulting model can be optimized to improve\ncalibration, robustness, and out-of-distribution detection, while generating\nsamples rivaling the quality of recent GAN-based approaches. However, the\nsoftmax classifier that JEM exploits is inherently discriminative and its\nlatent feature space is not well formulated as probabilistic distributions,\nwhich may hinder its potential for image generation and incur training\ninstability. We hypothesize that generative classifiers, such as Linear\nDiscriminant Analysis (LDA), might be more suitable for image generation since\ngenerative classifiers model the data generation process explicitly. This paper\ntherefore investigates an LDA classifier for image classification and\ngeneration. In particular, the Max-Mahalanobis Classifier (MMC), a special case\nof LDA, fits our goal very well. We show that our Generative MMC (GMMC) can be\ntrained discriminatively, generatively, or jointly for image classification and\ngeneration. Extensive experiments on multiple datasets show that GMMC achieves\nstate-of-the-art discriminative and generative performances, while\noutperforming JEM in calibration, adversarial robustness, and\nout-of-distribution detection by a significant margin. Our source code is\navailable at https://github.com/sndnyang/GMMC.",
          "link": "http://arxiv.org/abs/2101.00122",
          "publishedOn": "2021-07-05T01:54:58.774Z",
          "wordCount": 692,
          "title": "Generative Max-Mahalanobis Classifiers for Image Classification, Generation and More. (arXiv:2101.00122v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1\">Simon Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>",
          "description": "State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.",
          "link": "http://arxiv.org/abs/2106.12672",
          "publishedOn": "2021-07-05T01:54:58.767Z",
          "wordCount": 665,
          "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1\">Grgur Kova&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1\">R&#xe9;my Portelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Building embodied autonomous agents capable of participating in social\ninteractions with humans is one of the main challenges in AI. Within the Deep\nReinforcement Learning (DRL) field, this objective motivated multiple works on\nembodied language use. However, current approaches focus on language as a\ncommunication tool in very simplified and non-diverse social situations: the\n\"naturalness\" of language is reduced to the concept of high vocabulary size and\nvariability. In this paper, we argue that aiming towards human-level AI\nrequires a broader set of key social skills: 1) language use in complex and\nvariable social contexts; 2) beyond language, complex embodied communication in\nmultimodal settings within constantly evolving social worlds. We explain how\nconcepts from cognitive sciences could help AI to draw a roadmap towards\nhuman-like intelligence, with a focus on its social dimensions. As a first\nstep, we propose to expand current research to a broader set of core social\nskills. To do this, we present SocialAI, a benchmark to assess the acquisition\nof social skills of DRL agents using multiple grid-world environments featuring\nother (scripted) social agents. We then study the limits of a recent SOTA DRL\napproach when tested on SocialAI and discuss important next steps towards\nproficient social agents. Videos and code are available at\nhttps://sites.google.com/view/socialai.",
          "link": "http://arxiv.org/abs/2107.00956",
          "publishedOn": "2021-07-05T01:54:58.752Z",
          "wordCount": 663,
          "title": "SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement Learning Agents. (arXiv:2107.00956v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yunhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Quanyan Zhu</a>",
          "description": "The rapid growth in the number of devices and their connectivity has enlarged\nthe attack surface and weakened cyber systems. As attackers become increasingly\nsophisticated and resourceful, mere reliance on traditional cyber protection,\nsuch as intrusion detection, firewalls, and encryption, is insufficient to\nsecure cyber systems. Cyber resilience provides a new security paradigm that\ncomplements inadequate protection with resilience mechanisms. A Cyber-Resilient\nMechanism (CRM) adapts to the known or zero-day threats and uncertainties in\nreal-time and strategically responds to them to maintain the critical functions\nof the cyber systems. Feedback architectures play a pivotal role in enabling\nthe online sensing, reasoning, and actuation of the CRM. Reinforcement Learning\n(RL) is an important class of algorithms that epitomize the feedback\narchitectures for cyber resiliency, allowing the CRM to provide dynamic and\nsequential responses to attacks with limited prior knowledge of the attacker.\nIn this work, we review the literature on RL for cyber resiliency and discuss\nthe cyber-resilient defenses against three major types of vulnerabilities,\ni.e., posture-related, information-related, and human-related vulnerabilities.\nWe introduce moving target defense, defensive cyber deception, and assistive\nhuman security technologies as three application domains of CRMs to elaborate\non their designs. The RL technique also has vulnerabilities itself. We explain\nthe major vulnerabilities of RL and present several attack models in which the\nattacks target the rewards, the measurements, and the actuators. We show that\nthe attacker can trick the RL agent into learning a nefarious policy with\nminimum attacking effort, which shows serious security concerns for RL-enabled\nsystems. Finally, we discuss the future challenges of RL for cyber security and\nresiliency and emerging applications of RL-based CRMs.",
          "link": "http://arxiv.org/abs/2107.00783",
          "publishedOn": "2021-07-05T01:54:58.746Z",
          "wordCount": 708,
          "title": "Reinforcement Learning for Feedback-Enabled Cyber Resilience. (arXiv:2107.00783v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salem_T/0/1/0/all/0/1\">Tareq Si Salem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neglia_G/0/1/0/all/0/1\">Giovanni Neglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carra_D/0/1/0/all/0/1\">Damiano Carra</a>",
          "description": "Similarity search is a key operation in multimedia retrieval systems and\nrecommender systems, and it will play an important role also for future machine\nlearning and augmented reality applications. When these systems need to serve\nlarge objects with tight delay constraints, edge servers close to the end-user\ncan operate as similarity caches to speed up the retrieval. In this paper we\npresent A\\c{C}AI, a new similarity caching policy which improves on the state\nof the art by using (i) an (approximate) index for the whole catalog to decide\nwhich objects to serve locally and which to retrieve from the remote server,\nand (ii) a mirror ascent algorithm to update the set of local objects with\nstrong guarantees even when the request process does not exhibit any\nstatistical regularity.",
          "link": "http://arxiv.org/abs/2107.00957",
          "publishedOn": "2021-07-05T01:54:58.739Z",
          "wordCount": 566,
          "title": "A\\c{C}AI: Ascent Similarity Caching with Approximate Indexes. (arXiv:2107.00957v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naumann_P/0/1/0/all/0/1\">Philip Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>",
          "description": "Counterfactuals have become a popular technique nowadays for interacting with\nblack-box machine learning models and understanding how to change a particular\ninstance to obtain a desired outcome from the model. However, most existing\napproaches assume instant materialization of these changes, ignoring that they\nmay require effort and a specific order of application. Recently, methods have\nbeen proposed that also consider the order in which actions are applied,\nleading to the so-called sequential counterfactual generation problem.\n\nIn this work, we propose a model-agnostic method for sequential\ncounterfactual generation. We formulate the task as a multi-objective\noptimization problem and present a genetic algorithm approach to find optimal\nsequences of actions leading to the counterfactuals. Our cost model considers\nnot only the direct effect of an action, but also its consequences.\nExperimental results show that compared to state-of-the-art, our approach\ngenerates less costly solutions, is more efficient and provides the user with a\ndiverse set of solutions to choose from.",
          "link": "http://arxiv.org/abs/2104.05592",
          "publishedOn": "2021-07-05T01:54:58.734Z",
          "wordCount": 620,
          "title": "Consequence-aware Sequential Counterfactual Generation. (arXiv:2104.05592v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.05686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Daniel T. Chang</a>",
          "description": "Deep learning models are full of hyperparameters, which are set manually\nbefore the learning process can start. To find the best configuration for these\nhyperparameters in such a high dimensional space, with time-consuming and\nexpensive model training / validation, is not a trivial challenge. Bayesian\noptimization is a powerful tool for the joint optimization of hyperparameters,\nefficiently trading off exploration and exploitation of the hyperparameter\nspace. In this paper, we discuss Bayesian hyperparameter optimization,\nincluding hyperparameter optimization, Bayesian optimization, and Gaussian\nprocesses. We also review BoTorch, GPyTorch and Ax, the new open-source\nframeworks that we use for Bayesian optimization, Gaussian process inference\nand adaptive experimentation, respectively. For experimentation, we apply\nBayesian hyperparameter optimization, for optimizing group weights, to weighted\ngroup pooling, which couples unsupervised tiered graph autoencoders learning\nand supervised graph prediction learning for molecular graphs. We find that Ax,\nBoTorch and GPyTorch together provide a simple-to-use but powerful framework\nfor Bayesian hyperparameter optimization, using Ax's high-level API that\nconstructs and runs a full optimization loop and returns the best\nhyperparameter configuration.",
          "link": "http://arxiv.org/abs/1912.05686",
          "publishedOn": "2021-07-05T01:54:58.726Z",
          "wordCount": 629,
          "title": "Bayesian Hyperparameter Optimization with BoTorch, GPyTorch and Ax. (arXiv:1912.05686v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziheng Wang</a>",
          "description": "The last few years have seen gigantic leaps in algorithms and systems to\nsupport efficient deep learning inference. Pruning and quantization algorithms\ncan now consistently compress neural networks by an order of magnitude. For a\ncompressed neural network, a multitude of inference frameworks have been\ndesigned to maximize the performance of the target hardware. While we find\nmature support for quantized neural networks in production frameworks such as\nOpenVINO and MNN, support for pruned sparse neural networks is still lacking.\nTo tackle this challenge, we present SparseDNN, a sparse deep learning\ninference engine targeting CPUs. We present both kernel-level optimizations\nwith a sparse code generator to accelerate sparse operators and novel\nnetwork-level optimizations catering to sparse networks. We show that our\nsparse code generator can achieve significant speedups over state-of-the-art\nsparse and dense libraries. On end-to-end benchmarks such as Huggingface\npruneBERT, SparseDNN achieves up to 5x throughput improvement over dense\ninference with state-of-the-art OpenVINO.",
          "link": "http://arxiv.org/abs/2101.07948",
          "publishedOn": "2021-07-05T01:54:58.721Z",
          "wordCount": 612,
          "title": "SparseDNN: Fast Sparse Deep Learning Inference on CPUs. (arXiv:2101.07948v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00848",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ke_N/0/1/0/all/0/1\">Nan Rosemary Ke</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Didolkar_A/0/1/0/all/0/1\">Aniket Didolkar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mittal_S/0/1/0/all/0/1\">Sarthak Mittal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goyal_A/0/1/0/all/0/1\">Anirudh Goyal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lajoie_G/0/1/0/all/0/1\">Guillaume Lajoie</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rezende_D/0/1/0/all/0/1\">Danilo Rezende</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mozer_M/0/1/0/all/0/1\">Michael Mozer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>",
          "description": "Inducing causal relationships from observations is a classic problem in\nmachine learning. Most work in causality starts from the premise that the\ncausal variables themselves are observed. However, for AI agents such as robots\ntrying to make sense of their environment, the only observables are low-level\nvariables like pixels in images. To generalize well, an agent must induce\nhigh-level variables, particularly those which are causal or are affected by\ncausal variables. A central goal for AI and causality is thus the joint\ndiscovery of abstract representations and causal structure. However, we note\nthat existing environments for studying causal induction are poorly suited for\nthis objective because they have complicated task-specific causal graphs which\nare impossible to manipulate parametrically (e.g., number of nodes, sparsity,\ncausal chain length, etc.). In this work, our goal is to facilitate research in\nlearning representations of high-level variables as well as causal structures\namong them. In order to systematically probe the ability of methods to identify\nthese variables and structures, we design a suite of benchmarking RL\nenvironments. We evaluate various representation learning algorithms from the\nliterature and find that explicitly incorporating structure and modularity in\nmodels can help causal induction in model-based reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.00848",
          "publishedOn": "2021-07-05T01:54:58.704Z",
          "wordCount": 653,
          "title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning. (arXiv:2107.00848v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2101.03164",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Batzner_S/0/1/0/all/0/1\">Simon Batzner</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Musaelian_A/0/1/0/all/0/1\">Albert Musaelian</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sun_L/0/1/0/all/0/1\">Lixin Sun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Geiger_M/0/1/0/all/0/1\">Mario Geiger</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mailoa_J/0/1/0/all/0/1\">Jonathan P. Mailoa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kornbluth_M/0/1/0/all/0/1\">Mordechai Kornbluth</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Molinari_N/0/1/0/all/0/1\">Nicola Molinari</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Smidt_T/0/1/0/all/0/1\">Tess E. Smidt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kozinsky_B/0/1/0/all/0/1\">Boris Kozinsky</a>",
          "description": "This work presents Neural Equivariant Interatomic Potentials (NequIP), a\nSE(3)-equivariant neural network approach for learning interatomic potentials\nfrom ab-initio calculations for molecular dynamics simulations. While most\ncontemporary symmetry-aware models use invariant convolutions and only act on\nscalars, NequIP employs SE(3)-equivariant convolutions for interactions of\ngeometric tensors, resulting in a more information-rich and faithful\nrepresentation of atomic environments. The method achieves state-of-the-art\naccuracy on a challenging set of diverse molecules and materials while\nexhibiting remarkable data efficiency. NequIP outperforms existing models with\nup to three orders of magnitude fewer training data, challenging the widely\nheld belief that deep neural networks require massive training sets. The high\ndata efficiency of the method allows for the construction of accurate\npotentials using high-order quantum chemical level of theory as reference and\nenables high-fidelity molecular dynamics simulations over long time scales.",
          "link": "http://arxiv.org/abs/2101.03164",
          "publishedOn": "2021-07-05T01:54:58.698Z",
          "wordCount": 614,
          "title": "SE(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials. (arXiv:2101.03164v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cody_T/0/1/0/all/0/1\">Tyler Cody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beling_P/0/1/0/all/0/1\">Peter A. Beling</a>",
          "description": "Existing frameworks for transfer learning are incomplete from a systems\ntheoretic perspective. They place emphasis on notions of domain and task, and\nneglect notions of structure and behavior. In doing so, they limit the extent\nto which formalism can be carried through into the elaboration of their\nframeworks. Herein, we use Mesarovician systems theory to define transfer\nlearning as a relation on sets and subsequently characterize the general nature\nof transfer learning as a mathematical construct. We interpret existing\nframeworks in terms of ours and go beyond existing frameworks to define notions\nof transferability, transfer roughness, and transfer distance. Importantly,\ndespite its formalism, our framework avoids the detailed mathematics of\nlearning theory or machine learning solution methods without excluding their\nconsideration. As such, we provide a formal, general systems framework for\nmodeling transfer learning that offers a rigorous foundation for system design\nand analysis.",
          "link": "http://arxiv.org/abs/2107.01196",
          "publishedOn": "2021-07-05T01:54:58.691Z",
          "wordCount": 580,
          "title": "A Systems Theory of Transfer Learning. (arXiv:2107.01196v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00877",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Amakasu_T/0/1/0/all/0/1\">Takashi Amakasu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chauvet_N/0/1/0/all/0/1\">Nicolas Chauvet</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bachelier_G/0/1/0/all/0/1\">Guillaume Bachelier</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Huant_S/0/1/0/all/0/1\">Serge Huant</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Horisaki_R/0/1/0/all/0/1\">Ryoichi Horisaki</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Naruse_M/0/1/0/all/0/1\">Makoto Naruse</a>",
          "description": "In recent cross-disciplinary studies involving both optics and computing,\nsingle-photon-based decision-making has been demonstrated by utilizing the\nwave-particle duality of light to solve multi-armed bandit problems.\nFurthermore, entangled-photon-based decision-making has managed to solve a\ncompetitive multi-armed bandit problem in such a way that conflicts of\ndecisions among players are avoided while ensuring equality. However, as these\nstudies are based on the polarization of light, the number of available choices\nis limited to two, corresponding to two orthogonal polarization states. Here we\npropose a scalable principle to solve competitive decision-making situations by\nusing the orbital angular momentum as the tunable degree of freedom of photons,\nwhich theoretically allows an unlimited number of arms. Moreover, by extending\nthe Hong-Ou-Mandel effect to more than two states, we theoretically establish\nan experimental configuration able to generate entangled photon states with\norbital angular momentum and conditions that provide conflict-free selections\nat every turn. We numerically examine total rewards regarding three-armed\nbandit problems, for which the proposed strategy accomplishes almost the\ntheoretical maximum, which is greater than a conventional mixed strategy\nintending to realize Nash equilibrium. This is thanks to the entanglement\nproperty that achieves no-conflict selections, even in the exploring phase to\nfind the best arms.",
          "link": "http://arxiv.org/abs/2107.00877",
          "publishedOn": "2021-07-05T01:54:58.675Z",
          "wordCount": 651,
          "title": "Conflict-free collective stochastic decision making by orbital angular momentum entangled photons. (arXiv:2107.00877v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takabatake_K/0/1/0/all/0/1\">Kazuya Takabatake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akaho_S/0/1/0/all/0/1\">Shotaro Akaho</a>",
          "description": "Dependency networks (Heckerman et al., 2000) are potential probabilistic\ngraphical models for systems comprising a large number of variables. Like\nBayesian networks, the structure of a dependency network is represented by a\ndirected graph, and each node has a conditional probability table. Learning and\ninference are realized locally on individual nodes; therefore, computation\nremains tractable even with a large number of variables. However, the\ndependency network's learned distribution is the stationary distribution of a\nMarkov chain called pseudo-Gibbs sampling and has no closed-form expressions.\nThis technical disadvantage has impeded the development of dependency networks.\nIn this paper, we consider a certain manifold for each node. Then, we can\ninterpret pseudo-Gibbs sampling as iterative m-projections onto these\nmanifolds. This interpretation provides a theoretical bound for the location\nwhere the stationary distribution of pseudo-Gibbs sampling exists in\ndistribution space. Furthermore, this interpretation involves structure and\nparameter learning algorithms as optimization problems. In addition, we compare\ndependency and Bayesian networks experimentally. The results demonstrate that\nthe dependency network and the Bayesian network have roughly the same\nperformance in terms of the accuracy of their learned distributions. The\nresults also show that the dependency network can learn much faster than the\nBayesian network.",
          "link": "http://arxiv.org/abs/2107.00871",
          "publishedOn": "2021-07-05T01:54:58.667Z",
          "wordCount": 632,
          "title": "Reconsidering Dependency Networks from an Information Geometry Perspective. (arXiv:2107.00871v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wenqi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Gong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyengar_A/0/1/0/all/0/1\">Arun Iyengar</a>",
          "description": "Federated learning(FL) is an emerging distributed learning paradigm with\ndefault client privacy because clients can keep sensitive data on their devices\nand only share local training parameter updates with the federated server.\nHowever, recent studies reveal that gradient leakages in FL may compromise the\nprivacy of client training data. This paper presents a gradient leakage\nresilient approach to privacy-preserving federated learning with per training\nexample-based client differential privacy, coined as Fed-CDP. It makes three\noriginal contributions. First, we identify three types of client gradient\nleakage threats in federated learning even with encrypted client-server\ncommunications. We articulate when and why the conventional server coordinated\ndifferential privacy approach, coined as Fed-SDP, is insufficient to protect\nthe privacy of the training data. Second, we introduce Fed-CDP, the per\nexample-based client differential privacy algorithm, and provide a formal\nanalysis of Fed-CDP with the $(\\epsilon, \\delta)$ differential privacy\nguarantee, and a formal comparison between Fed-CDP and Fed-SDP in terms of\nprivacy accounting. Third, we formally analyze the privacy-utility trade-off\nfor providing differential privacy guarantee by Fed-CDP and present a dynamic\ndecay noise-injection policy to further improve the accuracy and resiliency of\nFed-CDP. We evaluate and compare Fed-CDP and Fed-CDP(decay) with Fed-SDP in\nterms of differential privacy guarantee and gradient leakage resilience over\nfive benchmark datasets. The results show that the Fed-CDP approach outperforms\nconventional Fed-SDP in terms of resilience to client gradient leakages while\noffering competitive accuracy performance in federated learning.",
          "link": "http://arxiv.org/abs/2107.01154",
          "publishedOn": "2021-07-05T01:54:58.660Z",
          "wordCount": 664,
          "title": "Gradient-Leakage Resilient Federated Learning. (arXiv:2107.01154v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1\">Tong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongjie Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Ding-Xuan Zhou</a>",
          "description": "We consider a family of deep neural networks consisting of two groups of\nconvolutional layers, a downsampling operator, and a fully connected layer. The\nnetwork structure depends on two structural parameters which determine the\nnumbers of convolutional layers and the width of the fully connected layer. We\nestablish an approximation theory with explicit approximation rates when the\napproximated function takes a composite form $f\\circ Q$ with a feature\npolynomial $Q$ and a univariate function $f$. In particular, we prove that such\na network can outperform fully connected shallow networks in approximating\nradial functions with $Q(x) =|x|^2$, when the dimension $d$ of data from\n$\\mathbb{R}^d$ is large. This gives the first rigorous proof for the\nsuperiority of deep convolutional neural networks in approximating functions\nwith special structures. Then we carry out generalization analysis for\nempirical risk minimization with such a deep network in a regression framework\nwith the regression function of the form $f\\circ Q$. Our network structure\nwhich does not use any composite information or the functions $Q$ and $f$ can\nautomatically extract features and make use of the composite nature of the\nregression function via tuning the structural parameters. Our analysis provides\nan error bound which decreases with the network depth to a minimum and then\nincreases, verifying theoretically a trade-off phenomenon observed for network\ndepths in many practical applications.",
          "link": "http://arxiv.org/abs/2107.00896",
          "publishedOn": "2021-07-05T01:54:58.639Z",
          "wordCount": 653,
          "title": "Theory of Deep Convolutional Neural Networks III: Approximating Radial Functions. (arXiv:2107.00896v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">John Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "The double descent curve is one of the most intriguing properties of deep\nneural networks. It contrasts the classical bias-variance curve with the\nbehavior of modern neural networks, occurring where the number of samples nears\nthe number of parameters. In this work, we explore the connection between the\ndouble descent phenomena and the number of samples in the deep neural network\nsetting. In particular, we propose a construction which augments the existing\ndataset by artificially increasing the number of samples. This construction\nempirically mitigates the double descent curve in this setting. We reproduce\nexisting work on deep double descent, and observe a smooth descent into the\noverparameterized region for our construction. This occurs both with respect to\nthe model size, and with respect to the number epochs.",
          "link": "http://arxiv.org/abs/2107.00797",
          "publishedOn": "2021-07-05T01:54:58.620Z",
          "wordCount": 553,
          "title": "Mitigating deep double descent by concatenating inputs. (arXiv:2107.00797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makarychev_K/0/1/0/all/0/1\">Konstantin Makarychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_L/0/1/0/all/0/1\">Liren Shan</a>",
          "description": "We consider the problem of explainable $k$-medians and $k$-means introduced\nby Dasgupta, Frost, Moshkovitz, and Rashtchian~(ICML 2020). In this problem,\nour goal is to find a \\emph{threshold decision tree} that partitions data into\n$k$ clusters and minimizes the $k$-medians or $k$-means objective. The obtained\nclustering is easy to interpret because every decision node of a threshold tree\nsplits data based on a single feature into two groups. We propose a new\nalgorithm for this problem which is $\\tilde O(\\log k)$ competitive with\n$k$-medians with $\\ell_1$ norm and $\\tilde O(k)$ competitive with $k$-means.\nThis is an improvement over the previous guarantees of $O(k)$ and $O(k^2)$ by\nDasgupta et al (2020). We also provide a new algorithm which is $O(\\log^{3/2}\nk)$ competitive for $k$-medians with $\\ell_2$ norm. Our first algorithm is\nnear-optimal: Dasgupta et al (2020) showed a lower bound of $\\Omega(\\log k)$\nfor $k$-medians; in this work, we prove a lower bound of $\\tilde\\Omega(k)$ for\n$k$-means. We also provide a lower bound of $\\Omega(\\log k)$ for $k$-medians\nwith $\\ell_2$ norm.",
          "link": "http://arxiv.org/abs/2107.00798",
          "publishedOn": "2021-07-05T01:54:58.600Z",
          "wordCount": 608,
          "title": "Near-optimal Algorithms for Explainable k-Medians and k-Means. (arXiv:2107.00798v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benato_L/0/1/0/all/0/1\">Lisa Benato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhmann_E/0/1/0/all/0/1\">Erik Buhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdmann_M/0/1/0/all/0/1\">Martin Erdmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fackeldey_P/0/1/0/all/0/1\">Peter Fackeldey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glombitza_J/0/1/0/all/0/1\">Jonas Glombitza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_N/0/1/0/all/0/1\">Nikolai Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasieczka_G/0/1/0/all/0/1\">Gregor Kasieczka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korcari_W/0/1/0/all/0/1\">William Korcari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhr_T/0/1/0/all/0/1\">Thomas Kuhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinheimer_J/0/1/0/all/0/1\">Jan Steinheimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stocker_H/0/1/0/all/0/1\">Horst St&#xf6;cker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plehn_T/0/1/0/all/0/1\">Tilman Plehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kai Zhou</a>",
          "description": "We introduce a collection of datasets from fundamental physics research --\nincluding particle physics, astroparticle physics, and hadron- and nuclear\nphysics -- for supervised machine learning studies. These datasets, containing\nhadronic top quarks, cosmic-ray induced air showers, phase transitions in\nhadronic matter, and generator-level histories, are made public to simplify\nfuture work on cross-disciplinary machine learning and transfer learning in\nfundamental physics. Based on these data, we present a simple yet flexible\ngraph-based neural network architecture that can easily be applied to a wide\nrange of supervised learning tasks in these domains. We show that our approach\nreaches performance close to state-of-the-art dedicated methods on all\ndatasets. To simplify adaptation for various problems, we provide\neasy-to-follow instructions on how graph-based representations of data\nstructures, relevant for fundamental physics, can be constructed and provide\ncode implementations for several of them. Implementations are also provided for\nour proposed method and all reference algorithms.",
          "link": "http://arxiv.org/abs/2107.00656",
          "publishedOn": "2021-07-05T01:54:58.587Z",
          "wordCount": 645,
          "title": "Shared Data and Algorithms for Deep Learning in Fundamental Physics. (arXiv:2107.00656v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00813",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Qiu_C/0/1/0/all/0/1\">Changxin Qiu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yan_J/0/1/0/all/0/1\">Jue Yan</a>",
          "description": "Motivated by finite volume scheme, a cell-average based neural network method\nis proposed. The method is based on the integral or weak formulation of partial\ndifferential equations. A simple feed forward network is forced to learn the\nsolution average evolution between two neighboring time steps. Offline\nsupervised training is carried out to obtain the optimal network parameter set,\nwhich uniquely identifies one finite volume like neural network method. Once\nwell trained, the network method is implemented as a finite volume scheme, thus\nis mesh dependent. Different to traditional numerical methods, our method can\nbe relieved from the explicit scheme CFL restriction and can adapt to any time\nstep size for solution evolution. For Heat equation, first order of convergence\nis observed and the errors are related to the spatial mesh size but are\nobserved independent of the mesh size in time. The cell-average based neural\nnetwork method can sharply evolve contact discontinuity with almost zero\nnumerical diffusion introduced. Shock and rarefaction waves are well captured\nfor nonlinear hyperbolic conservation laws.",
          "link": "http://arxiv.org/abs/2107.00813",
          "publishedOn": "2021-07-05T01:54:58.580Z",
          "wordCount": 608,
          "title": "Cell-average based neural network method for hyperbolic and parabolic partial differential equations. (arXiv:2107.00813v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kothawade_S/0/1/0/all/0/1\">Suraj Kothawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1\">Nathan Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1\">Krishnateja Killamsetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "Active learning has proven to be useful for minimizing labeling costs by\nselecting the most informative samples. However, existing active learning\nmethods do not work well in realistic scenarios such as imbalance or rare\nclasses, out-of-distribution data in the unlabeled set, and redundancy. In this\nwork, we propose SIMILAR (Submodular Information Measures based actIve\nLeARning), a unified active learning framework using recently proposed\nsubmodular information measures (SIM) as acquisition functions. We argue that\nSIMILAR not only works in standard active learning, but also easily extends to\nthe realistic settings considered above and acts as a one-stop solution for\nactive learning that is scalable to large real-world datasets. Empirically, we\nshow that SIMILAR significantly outperforms existing active learning algorithms\nby as much as ~5% - 18% in the case of rare classes and ~5% - 10% in the case\nof out-of-distribution data on several image classification tasks like\nCIFAR-10, MNIST, and ImageNet.",
          "link": "http://arxiv.org/abs/2107.00717",
          "publishedOn": "2021-07-05T01:54:58.559Z",
          "wordCount": 592,
          "title": "SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios. (arXiv:2107.00717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banna_V/0/1/0/all/0/1\">Vishnu Banna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinnakotla_A/0/1/0/all/0/1\">Akhil Chinnakotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhengxin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vegesana_A/0/1/0/all/0/1\">Ani Vegesana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivek_N/0/1/0/all/0/1\">Naveen Vivek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnappa_K/0/1/0/all/0/1\">Kruthi Krishnappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yung-Hsiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1\">George K. Thiruvathukal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">James C. Davis</a>",
          "description": "Machine learning techniques are becoming a fundamental tool for scientific\nand engineering progress. These techniques are applied in contexts as diverse\nas astronomy and spam filtering. However, correctly applying these techniques\nrequires careful engineering. Much attention has been paid to the technical\npotential; relatively little attention has been paid to the software\nengineering process required to bring research-based machine learning\ntechniques into practical utility. Technology companies have supported the\nengineering community through machine learning frameworks such as TensorFLow\nand PyTorch, but the details of how to engineer complex machine learning models\nin these frameworks have remained hidden.\n\nTo promote best practices within the engineering community, academic\ninstitutions and Google have partnered to launch a Special Interest Group on\nMachine Learning Models (SIGMODELS) whose goal is to develop exemplary\nimplementations of prominent machine learning models in community locations\nsuch as the TensorFlow Model Garden (TFMG). The purpose of this report is to\ndefine a process for reproducing a state-of-the-art machine learning model at a\nlevel of quality suitable for inclusion in the TFMG. We define the engineering\nprocess and elaborate on each step, from paper analysis to model release. We\nreport on our experiences implementing the YOLO model family with a team of 26\nstudent researchers, share the tools we developed, and describe the lessons we\nlearned along the way.",
          "link": "http://arxiv.org/abs/2107.00821",
          "publishedOn": "2021-07-05T01:54:58.546Z",
          "wordCount": 688,
          "title": "An Experience Report on Machine Learning Reproducibility: Guidance for Practitioners and TensorFlow Model Garden Contributors. (arXiv:2107.00821v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zehao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S.Du</a>",
          "description": "As one of the most popular methods in the field of reinforcement learning,\nQ-learning has received increasing attention. Recently, there have been more\ntheoretical works on the regret bound of algorithms that belong to the\nQ-learning class in different settings. In this paper, we analyze the\ncumulative regret when conducting Nash Q-learning algorithm on 2-player\nturn-based stochastic Markov games (2-TBSG), and propose the very first gap\ndependent logarithmic upper bounds in the episodic tabular setting. This bound\nmatches the theoretical lower bound only up to a logarithmic term. Furthermore,\nwe extend the conclusion to the discounted game setting with infinite horizon\nand propose a similar gap dependent logarithmic regret bound. Also, under the\nlinear MDP assumption, we obtain another logarithmic regret for 2-TBSG, in both\ncentralized and independent settings.",
          "link": "http://arxiv.org/abs/2107.00685",
          "publishedOn": "2021-07-05T01:54:58.513Z",
          "wordCount": 564,
          "title": "Gap-Dependent Bounds for Two-Player Markov Games. (arXiv:2107.00685v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lihong Li</a>",
          "description": "The rich body of Bandit literature not only offers a diverse toolbox of\nalgorithms, but also makes it hard for a practitioner to find the right\nsolution to solve the problem at hand. Typical textbooks on Bandits focus on\ndesigning and analyzing algorithms, and surveys on applications often present a\nlist of individual applications. While these are valuable resources, there\nexists a gap in mapping applications to appropriate Bandit algorithms. In this\npaper, we aim to reduce this gap with a structured map of Bandits to help\npractitioners navigate to find relevant and practical Bandit algorithms.\nInstead of providing a comprehensive overview, we focus on a small number of\nkey decision points related to reward, action, and features, which often affect\nhow Bandit algorithms are chosen in practice.",
          "link": "http://arxiv.org/abs/2107.00680",
          "publishedOn": "2021-07-05T01:54:58.507Z",
          "wordCount": 560,
          "title": "A Map of Bandits for E-commerce. (arXiv:2107.00680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyung_E/0/1/0/all/0/1\">Eunyoung Hyung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Despite the success of recent Neural Architecture Search (NAS) methods on\nvarious tasks which have shown to output networks that largely outperform\nhuman-designed networks, conventional NAS methods have mostly tackled the\noptimization of searching for the network architecture for a single task\n(dataset), which does not generalize well across multiple tasks (datasets).\nMoreover, since such task-specific methods search for a neural architecture\nfrom scratch for every given task, they incur a large computational cost, which\nis problematic when the time and monetary budget are limited. In this paper, we\npropose an efficient NAS framework that is trained once on a database\nconsisting of datasets and pretrained networks and can rapidly search for a\nneural architecture for a novel dataset. The proposed MetaD2A (Meta\nDataset-to-Architecture) model can stochastically generate graphs\n(architectures) from a given set (dataset) via a cross-modal latent space\nlearned with amortized meta-learning. Moreover, we also propose a\nmeta-performance predictor to estimate and select the best architecture without\ndirect training on target datasets. The experimental results demonstrate that\nour model meta-learned on subsets of ImageNet-1K and architectures from\nNAS-Bench 201 search space successfully generalizes to multiple unseen datasets\nincluding CIFAR-10 and CIFAR-100, with an average search time of 33 GPU\nseconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than\nNSGANetV2, a transferable NAS method, with comparable performance. We believe\nthat the MetaD2A proposes a new research direction for rapid NAS as well as\nways to utilize the knowledge from rich databases of datasets and architectures\naccumulated over the past years. Code is available at\nhttps://github.com/HayeonLee/MetaD2A.",
          "link": "http://arxiv.org/abs/2107.00860",
          "publishedOn": "2021-07-05T01:54:58.500Z",
          "wordCount": 705,
          "title": "Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets. (arXiv:2107.00860v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00839",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Delarue_F/0/1/0/all/0/1\">Fran&#xe7;ois Delarue</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vasileiadis_A/0/1/0/all/0/1\">Athanasios Vasileiadis</a>",
          "description": "The goal of this paper is to demonstrate that common noise may serve as an\nexploration noise for learning the solution of a mean field game. This concept\nis here exemplified through a toy linear-quadratic model, for which a suitable\nform of common noise has already been proven to restore existence and\nuniqueness. We here go one step further and prove that the same form of common\nnoise may force the convergence of the learning algorithm called `fictitious\nplay', and this without any further potential or monotone structure. Several\nnumerical examples are provided in order to support our theoretical analysis.",
          "link": "http://arxiv.org/abs/2107.00839",
          "publishedOn": "2021-07-05T01:54:58.491Z",
          "wordCount": 539,
          "title": "Exploration noise for learning linear-quadratic mean field games. (arXiv:2107.00839v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00719",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_P/0/1/0/all/0/1\">Po-Yu Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_S/0/1/0/all/0/1\">Shu-Min Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_N/0/1/0/all/0/1\">Nan-Lan Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chu Lin</a>",
          "description": "Drug-target interaction (DTI) prediction plays a crucial role in drug\ndiscovery, and deep learning approaches have achieved state-of-the-art\nperformance in this field. We introduce an ensemble of deep learning models\n(EnsembleDLM) for robust DTI prediction. EnsembleDLM only uses the sequence\ninformation of chemical compounds and proteins, and it aggregates the\npredictions from multiple deep neural networks. This approach reduces the\nchance of overfitting, yields an unbiased prediction, and achieves\nstate-of-the-art performance in Davis and KIBA datasets. EnsembleDLM also\nreaches state-of-the-art performance in cross-domain applications and decent\ncross-domain performance (Pearson correlation coefficient and concordance index\n> 0.8) with transfer learning using approximately twice the amount of test data\nin the new domain.",
          "link": "http://arxiv.org/abs/2107.00719",
          "publishedOn": "2021-07-05T01:54:58.483Z",
          "wordCount": 560,
          "title": "Toward Robust Drug-Target Interaction Prediction via Ensemble Modeling and Transfer Learning. (arXiv:2107.00719v1 [q-bio.BM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maddu_S/0/1/0/all/0/1\">Suryanarayana Maddu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturm_D/0/1/0/all/0/1\">Dominik Sturm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M%7Fuller_C/0/1/0/all/0/1\">Christian L. M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sbalzarini_I/0/1/0/all/0/1\">Ivo F. Sbalzarini</a>",
          "description": "We characterize and remedy a failure mode that may arise from multi-scale\ndynamics with scale imbalances during training of deep neural networks, such as\nPhysics Informed Neural Networks (PINNs). PINNs are popular machine-learning\ntemplates that allow for seamless integration of physical equation models with\ndata. Their training amounts to solving an optimization problem over a weighted\nsum of data-fidelity and equation-fidelity objectives. Conflicts between\nobjectives can arise from scale imbalances, heteroscedasticity in the data,\nstiffness of the physical equation, or from catastrophic interference during\nsequential training. We explain the training pathology arising from this and\npropose a simple yet effective inverse-Dirichlet weighting strategy to\nalleviate the issue. We compare with Sobolev training of neural networks,\nproviding the baseline of analytically $\\boldsymbol{\\epsilon}$-optimal\ntraining. We demonstrate the effectiveness of inverse-Dirichlet weighting in\nvarious applications, including a multi-scale model of active turbulence, where\nwe show orders of magnitude improvement in accuracy and convergence over\nconventional PINN training. For inverse modeling using sequential training, we\nfind that inverse-Dirichlet weighting protects a PINN against catastrophic\nforgetting.",
          "link": "http://arxiv.org/abs/2107.00940",
          "publishedOn": "2021-07-05T01:54:58.464Z",
          "wordCount": 623,
          "title": "Inverse-Dirichlet Weighting Enables Reliable Training of Physics Informed Neural Networks. (arXiv:2107.00940v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagtap_R/0/1/0/all/0/1\">Raj Jagtap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shakshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_C/0/1/0/all/0/1\">Clint P. George</a>",
          "description": "Millions of people use platforms such as YouTube, Facebook, Twitter, and\nother mass media. Due to the accessibility of these platforms, they are often\nused to establish a narrative, conduct propaganda, and disseminate\nmisinformation. This work proposes an approach that uses state-of-the-art NLP\ntechniques to extract features from video captions (subtitles). To evaluate our\napproach, we utilize a publicly accessible and labeled dataset for classifying\nvideos as misinformation or not. The motivation behind exploring video captions\nstems from our analysis of videos metadata. Attributes such as the number of\nviews, likes, dislikes, and comments are ineffective as videos are hard to\ndifferentiate using this information. Using caption dataset, the proposed\nmodels can classify videos among three classes (Misinformation, Debunking\nMisinformation, and Neutral) with 0.85 to 0.90 F1-score. To emphasize the\nrelevance of the misinformation class, we re-formulate our classification\nproblem as a two-class classification - Misinformation vs. others (Debunking\nMisinformation and Neutral). In our experiments, the proposed models can\nclassify videos with 0.92 to 0.95 F1-score and 0.78 to 0.90 AUC ROC.",
          "link": "http://arxiv.org/abs/2107.00941",
          "publishedOn": "2021-07-05T01:54:58.452Z",
          "wordCount": 610,
          "title": "Misinformation Detection on YouTube Using Video Captions. (arXiv:2107.00941v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>",
          "description": "The Transformer model is widely used in natural language processing for\nsentence representation. However, the previous Transformer-based models focus\non function words that have limited meaning in most cases and could merely\nextract high-level semantic abstraction features. In this paper, two approaches\nare introduced to improve the performance of Transformers. We calculated the\nattention score by multiplying the part-of-speech weight vector with the\ncorrelation coefficient, which helps extract the words with more practical\nmeaning. The weight vector is obtained by the input text sequence based on the\nimportance of the part-of-speech. Furthermore, we fuse the features of each\nlayer to make the sentence representation results more comprehensive and\naccurate. In experiments, we demonstrate the effectiveness of our model\nTransformer-F on three standard text classification datasets. Experimental\nresults show that our proposed model significantly boosts the performance of\ntext classification as compared to the baseline model. Specifically, we obtain\na 5.28% relative improvement over the vanilla Transformer on the simple tasks.",
          "link": "http://arxiv.org/abs/2107.00653",
          "publishedOn": "2021-07-05T01:54:58.445Z",
          "wordCount": 597,
          "title": "Transformer-F: A Transformer network with effective methods for learning universal sentence representation. (arXiv:2107.00653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blanc_G/0/1/0/all/0/1\">Guy Blanc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_J/0/1/0/all/0/1\">Jane Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1\">Mingda Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Li-Yang Tan</a>",
          "description": "Greedy decision tree learning heuristics are mainstays of machine learning\npractice, but theoretical justification for their empirical success remains\nelusive. In fact, it has long been known that there are simple target functions\nfor which they fail badly (Kearns and Mansour, STOC 1996).\n\nRecent work of Brutzkus, Daniely, and Malach (COLT 2020) considered the\nsmoothed analysis model as a possible avenue towards resolving this disconnect.\nWithin the smoothed setting and for targets $f$ that are $k$-juntas, they\nshowed that these heuristics successfully learn $f$ with depth-$k$ decision\ntree hypotheses. They conjectured that the same guarantee holds more generally\nfor targets that are depth-$k$ decision trees.\n\nWe provide a counterexample to this conjecture: we construct targets that are\ndepth-$k$ decision trees and show that even in the smoothed setting, these\nheuristics build trees of depth $2^{\\Omega(k)}$ before achieving high accuracy.\nWe also show that the guarantees of Brutzkus et al. cannot extend to the\nagnostic setting: there are targets that are very close to $k$-juntas, for\nwhich these heuristics build trees of depth $2^{\\Omega(k)}$ before achieving\nhigh accuracy.",
          "link": "http://arxiv.org/abs/2107.00819",
          "publishedOn": "2021-07-05T01:54:58.439Z",
          "wordCount": 628,
          "title": "Decision tree heuristics can fail, even in the smoothed setting. (arXiv:2107.00819v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1\">Kevin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kai-Zhan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1\">Elias Bareinboim</a>",
          "description": "One of the central elements of any causal inference is an object called\nstructural causal model (SCM), which represents a collection of mechanisms and\nexogenous sources of random variation of the system under investigation (Pearl,\n2000). An important property of many kinds of neural networks is universal\napproximability: the ability to approximate any function to arbitrary\nprecision. Given this property, one may be tempted to surmise that a collection\nof neural nets is capable of learning any SCM by training on data generated by\nthat SCM. In this paper, we show this is not the case by disentangling the\nnotions of expressivity and learnability. Specifically, we show that the causal\nhierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits\nof what can be learned from data, still holds for neural models. For instance,\nan arbitrarily complex and expressive neural net is unable to predict the\neffects of interventions given observational data alone. Given this result, we\nintroduce a special type of SCM called a neural causal model (NCM), and\nformalize a new type of inductive bias to encode structural constraints\nnecessary for performing causal inferences. Building on this new class of\nmodels, we focus on solving two canonical tasks found in the literature known\nas causal identification and estimation. Leveraging the neural toolbox, we\ndevelop an algorithm that is both sufficient and necessary to determine whether\na causal effect can be learned from data (i.e., causal identifiability); it\nthen estimates the effect whenever identifiability holds (causal estimation).\nSimulations corroborate the proposed approach.",
          "link": "http://arxiv.org/abs/2107.00793",
          "publishedOn": "2021-07-05T01:54:58.431Z",
          "wordCount": 710,
          "title": "The Causal Neural Connection: Expressiveness, Learnability, and Inference. (arXiv:2107.00793v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong-You Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Federated learning is promising for its ability to collaboratively train\nmodels with multiple clients without accessing their data, but vulnerable when\nclients' data distributions diverge from each other. This divergence further\nleads to a dilemma: \"Should we prioritize the learned model's generic\nperformance (for future use at the server) or its personalized performance (for\neach client)?\" These two, seemingly competing goals have divided the community\nto focus on one or the other, yet in this paper we show that it is possible to\napproach both at the same time. Concretely, we propose a novel federated\nlearning framework that explicitly decouples a model's dual duties with two\nprediction tasks. On the one hand, we introduce a family of losses that are\nrobust to non-identical class distributions, enabling clients to train a\ngeneric predictor with a consistent objective across them. On the other hand,\nwe formulate the personalized predictor as a lightweight adaptive module that\nis learned to minimize each client's empirical risk on top of the generic\npredictor. With this two-loss, two-predictor framework which we name Federated\nRobust Decoupling Fed-RoD, the learned model can simultaneously achieve\nstate-of-the-art generic and personalized performance, essentially bridging the\ntwo tasks.",
          "link": "http://arxiv.org/abs/2107.00778",
          "publishedOn": "2021-07-05T01:54:58.415Z",
          "wordCount": 622,
          "title": "On Bridging Generic and Personalized Federated Learning. (arXiv:2107.00778v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nitish Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>",
          "description": "While pretrained language models achieve excellent performance on natural\nlanguage understanding benchmarks, they tend to rely on spurious correlations\nand generalize poorly to out-of-distribution (OOD) data. Recent work has\nexplored using counterfactually-augmented data (CAD) -- data generated by\nminimally perturbing examples to flip the ground-truth label -- to identify\nrobust features that are invariant under distribution shift. However, empirical\nresults using CAD for OOD generalization have been mixed. To explain this\ndiscrepancy, we draw insights from a linear Gaussian model and demonstrate the\npitfalls of CAD. Specifically, we show that (a) while CAD is effective at\nidentifying robust features, it may prevent the model from learning unperturbed\nrobust features, and (b) CAD may exacerbate existing spurious correlations in\nthe data. Our results show that the lack of perturbation diversity in current\nCAD datasets limits its effectiveness on OOD generalization, calling for\ninnovative crowdsourcing procedures to elicit diverse perturbation of examples.",
          "link": "http://arxiv.org/abs/2107.00753",
          "publishedOn": "2021-07-05T01:54:58.409Z",
          "wordCount": 584,
          "title": "An Investigation of the (In)effectiveness of Counterfactually Augmented Data. (arXiv:2107.00753v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunzhuang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eberhard_A/0/1/0/all/0/1\">Andrew Eberhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>",
          "description": "This paper proposes a novel primal heuristic for Mixed Integer Programs, by\nemploying machine learning techniques. Mixed Integer Programming is a general\ntechnique for formulating combinatorial optimization problems. Inside a solver,\nprimal heuristics play a critical role in finding good feasible solutions that\nenable one to tighten the duality gap from the outset of the Branch-and-Bound\nalgorithm (B&B), greatly improving its performance by pruning the B&B tree\naggressively. In this paper, we investigate whether effective primal heuristics\ncan be automatically learned via machine learning. We propose a new method to\nrepresent an optimization problem as a graph, and train a Graph Convolutional\nNetwork on solved problem instances with known optimal solutions. This in turn\ncan predict the values of decision variables in the optimal solution for an\nunseen problem instance of a similar type. The prediction of variable solutions\nis then leveraged by a novel configuration of the B&B method, Probabilistic\nBranching with guided Depth-first Search (PB-DFS) approach, aiming to find\n(near-)optimal solutions quickly. The experimental results show that this new\nheuristic can find better primal solutions at a much earlier stage of the\nsolving process, compared to other state-of-the-art primal heuristics.",
          "link": "http://arxiv.org/abs/2107.00866",
          "publishedOn": "2021-07-05T01:54:58.402Z",
          "wordCount": 633,
          "title": "Learning Primal Heuristics for Mixed Integer Programs. (arXiv:2107.00866v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hantao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>",
          "description": "Neural network based speech recognition systems suffer from performance\ndegradation due to accented speech, especially unfamiliar accents. In this\npaper, we study the supervised contrastive learning framework for accented\nspeech recognition. To build different views (similar \"positive\" data samples)\nfor contrastive learning, three data augmentation techniques including noise\ninjection, spectrogram augmentation and TTS-same-sentence generation are\nfurther investigated. From the experiments on the Common Voice dataset, we have\nshown that contrastive learning helps to build data-augmentation invariant and\npronunciation invariant representations, which significantly outperforms\ntraditional joint training methods in both zero-shot and full-shot settings.\nExperiments show that contrastive learning can improve accuracy by 3.66%\n(zero-shot) and 3.78% (full-shot) on average, comparing to the joint training\nmethod.",
          "link": "http://arxiv.org/abs/2107.00921",
          "publishedOn": "2021-07-05T01:54:58.391Z",
          "wordCount": 564,
          "title": "Supervised Contrastive Learning for Accented Speech Recognition. (arXiv:2107.00921v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaeemzadeh_A/0/1/0/all/0/1\">Alireza Zaeemzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>",
          "description": "A reinforcement-learning-based non-uniform compressed sensing (NCS) framework\nfor time-varying signals is introduced. The proposed scheme, referred to as\nRL-NCS, aims to boost the performance of signal recovery through an optimal and\nadaptive distribution of sensing energy among two groups of coefficients of the\nsignal, referred to as the region of interest (ROI) coefficients and non-ROI\ncoefficients. The coefficients in ROI usually have greater importance and need\nto be reconstructed with higher accuracy compared to non-ROI coefficients. In\norder to accomplish this task, the ROI is predicted at each time step using two\nspecific approaches. One of these approaches incorporates a long short-term\nmemory (LSTM) network for the prediction. The other approach employs the\nprevious ROI information for predicting the next step ROI. Using the\nexploration-exploitation technique, a Q-network learns to choose the best\napproach for designing the measurement matrix. Furthermore, a joint loss\nfunction is introduced for the efficient training of the Q-network as well as\nthe LSTM network. The result indicates a significant performance gain for our\nproposed method, even for rapidly varying signals and a reduced number of\nmeasurements.",
          "link": "http://arxiv.org/abs/2107.00838",
          "publishedOn": "2021-07-05T01:54:58.385Z",
          "wordCount": 625,
          "title": "RL-NCS: Reinforcement learning based data-driven approach for nonuniform compressed sensing. (arXiv:2107.00838v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cote_Allard_U/0/1/0/all/0/1\">Ulysse C&#xf4;t&#xe9;-Allard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakobsen_P/0/1/0/all/0/1\">Petter Jakobsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stautland_A/0/1/0/all/0/1\">Andrea Stautland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nordgreen_T/0/1/0/all/0/1\">Tine Nordgreen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasmer_O/0/1/0/all/0/1\">Ole Bernt Fasmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oedegaard_K/0/1/0/all/0/1\">Ketil Joachim Oedegaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresen_J/0/1/0/all/0/1\">Jim Torresen</a>",
          "description": "Manic episodes of bipolar disorder can lead to uncritical behaviour and\ndelusional psychosis, often with destructive consequences for those affected\nand their surroundings. Early detection and intervention of a manic episode are\ncrucial to prevent escalation, hospital admission and premature death. However,\npeople with bipolar disorder may not recognize that they are experiencing a\nmanic episode and symptoms such as euphoria and increased productivity can also\ndeter affected individuals from seeking help. This work proposes to perform\nuser-independent, automatic mood-state detection based on actigraphy and\nelectrodermal activity acquired from a wrist-worn device during mania and after\nrecovery (euthymia). This paper proposes a new deep learning-based ensemble\nmethod leveraging long (20h) and short (5 minutes) time-intervals to\ndiscriminate between the mood-states. When tested on 47 bipolar patients, the\nproposed classification scheme achieves an average accuracy of 91.59% in\neuthymic/manic mood-state recognition.",
          "link": "http://arxiv.org/abs/2107.00710",
          "publishedOn": "2021-07-05T01:54:58.367Z",
          "wordCount": 609,
          "title": "Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00801",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kumagai_A/0/1/0/all/0/1\">Atsutoshi Kumagai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Iwata_T/0/1/0/all/0/1\">Tomoharu Iwata</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fujiwara_Y/0/1/0/all/0/1\">Yasuhiro Fujiwara</a>",
          "description": "The ratio of two probability densities, called a density-ratio, is a vital\nquantity in machine learning. In particular, a relative density-ratio, which is\na bounded extension of the density-ratio, has received much attention due to\nits stability and has been used in various applications such as outlier\ndetection and dataset comparison. Existing methods for (relative) density-ratio\nestimation (DRE) require many instances from both densities. However,\nsufficient instances are often unavailable in practice. In this paper, we\npropose a meta-learning method for relative DRE, which estimates the relative\ndensity-ratio from a few instances by using knowledge in related datasets.\nSpecifically, given two datasets that consist of a few instances, our model\nextracts the datasets' information by using neural networks and uses it to\nobtain instance embeddings appropriate for the relative DRE. We model the\nrelative density-ratio by a linear model on the embedded space, whose global\noptimum solution can be obtained as a closed-form solution. The closed-form\nsolution enables fast and effective adaptation to a few instances, and its\ndifferentiability enables us to train our model such that the expected test\nerror for relative DRE can be explicitly minimized after adapting to a few\ninstances. We empirically demonstrate the effectiveness of the proposed method\nby using three problems: relative DRE, dataset comparison, and outlier\ndetection.",
          "link": "http://arxiv.org/abs/2107.00801",
          "publishedOn": "2021-07-05T01:54:58.361Z",
          "wordCount": 643,
          "title": "Meta-Learning for Relative Density-Ratio Estimation. (arXiv:2107.00801v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00745",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Masrani_V/0/1/0/all/0/1\">Vaden Masrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brekelmans_R/0/1/0/all/0/1\">Rob Brekelmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Thang Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1\">Greg Ver Steeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>",
          "description": "Many common machine learning methods involve the geometric annealing path, a\nsequence of intermediate densities between two distributions of interest\nconstructed using the geometric average. While alternatives such as the\nmoment-averaging path have demonstrated performance gains in some settings,\ntheir practical applicability remains limited by exponential family endpoint\nassumptions and a lack of closed form energy function. In this work, we\nintroduce $q$-paths, a family of paths which is derived from a generalized\nnotion of the mean, includes the geometric and arithmetic mixtures as special\ncases, and admits a simple closed form involving the deformed logarithm\nfunction from nonextensive thermodynamics. Following previous analysis of the\ngeometric path, we interpret our $q$-paths as corresponding to a\n$q$-exponential family of distributions, and provide a variational\nrepresentation of intermediate densities as minimizing a mixture of\n$\\alpha$-divergences to the endpoints. We show that small deviations away from\nthe geometric path yield empirical gains for Bayesian inference using\nSequential Monte Carlo and generative model evaluation using Annealed\nImportance Sampling.",
          "link": "http://arxiv.org/abs/2107.00745",
          "publishedOn": "2021-07-05T01:54:58.355Z",
          "wordCount": 619,
          "title": "q-Paths: Generalizing the Geometric Annealing Path using Power Means. (arXiv:2107.00745v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumagai_A/0/1/0/all/0/1\">Atsutoshi Kumagai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwata_T/0/1/0/all/0/1\">Tomoharu Iwata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujiwara_Y/0/1/0/all/0/1\">Yasuhiro Fujiwara</a>",
          "description": "We propose a few-shot learning method for unsupervised feature selection,\nwhich is a task to select a subset of relevant features in unlabeled data.\nExisting methods usually require many instances for feature selection. However,\nsufficient instances are often unavailable in practice. The proposed method can\nselect a subset of relevant features in a target task given a few unlabeled\ntarget instances by training with unlabeled instances in multiple source tasks.\nOur model consists of a feature selector and decoder. The feature selector\noutputs a subset of relevant features taking a few unlabeled instances as input\nsuch that the decoder can reconstruct the original features of unseen instances\nfrom the selected ones. The feature selector uses the Concrete random variables\nto select features via gradient descent. To encode task-specific properties\nfrom a few unlabeled instances to the model, the Concrete random variables and\ndecoder are modeled using permutation-invariant neural networks that take a few\nunlabeled instances as input. Our model is trained by minimizing the expected\ntest reconstruction error given a few unlabeled instances that is calculated\nwith datasets in source tasks. We experimentally demonstrate that the proposed\nmethod outperforms existing feature selection methods.",
          "link": "http://arxiv.org/abs/2107.00816",
          "publishedOn": "2021-07-05T01:54:58.349Z",
          "wordCount": 625,
          "title": "Few-shot Learning for Unsupervised Feature Selection. (arXiv:2107.00816v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00693",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Arefeen_A/0/1/0/all/0/1\">Asiful Arefeen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akbari_A/0/1/0/all/0/1\">Ali Akbari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mirzadeh_S/0/1/0/all/0/1\">Seyed Iman Mirzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jafari_R/0/1/0/all/0/1\">Roozbeh Jafari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirazi_B/0/1/0/all/0/1\">Behrooz A. Shirazi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghasemzadeh_H/0/1/0/all/0/1\">Hassan Ghasemzadeh</a>",
          "description": "Inter-beat interval (IBI) measurement enables estimation of heart-rate\nvariability (HRV) which, in turns, can provide early indication of potential\ncardiovascular diseases. However, extracting IBIs from noisy signals is\nchallenging since the morphology of the signal is distorted in the presence of\nthe noise. Electrocardiogram (ECG) of a person in heavy motion is highly\ncorrupted with noise, known as motion-artifact, and IBI extracted from it is\ninaccurate. As a part of remote health monitoring and wearable system\ndevelopment, denoising ECG signals and estimating IBIs correctly from them have\nbecome an emerging topic among signal-processing researchers. Apart from\nconventional methods, deep-learning techniques have been successfully used in\nsignal denoising recently, and diagnosis process has become easier, leading to\naccuracy levels that were previously unachievable. We propose a deep-learning\napproach leveraging tiramisu autoencoder model to suppress motion-artifact\nnoise and make the R-peaks of the ECG signal prominent even in the presence of\nhigh-intensity motion. After denoising, IBIs are estimated more accurately\nexpediting diagnosis tasks. Results illustrate that our method enables IBI\nestimation from noisy ECG signals with SNR up to -30dB with average root mean\nsquare error (RMSE) of 13 milliseconds for estimated IBIs. At this noise level,\nour error percentage remains below 8% and outperforms other state of the art\ntechniques.",
          "link": "http://arxiv.org/abs/2107.00693",
          "publishedOn": "2021-07-05T01:54:58.342Z",
          "wordCount": 665,
          "title": "Inter-Beat Interval Estimation with Tiramisu Model: A Novel Approach with Reduced Error. (arXiv:2107.00693v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curmei_M/0/1/0/all/0/1\">Mihaela Curmei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_S/0/1/0/all/0/1\">Sarah Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1\">Benjamin Recht</a>",
          "description": "In this work, we consider how preference models in interactive recommendation\nsystems determine the availability of content and users' opportunities for\ndiscovery. We propose an evaluation procedure based on stochastic reachability\nto quantify the maximum probability of recommending a target piece of content\nto an user for a set of allowable strategic modifications. This framework\nallows us to compute an upper bound on the likelihood of recommendation with\nminimal assumptions about user behavior. Stochastic reachability can be used to\ndetect biases in the availability of content and diagnose limitations in the\nopportunities for discovery granted to users. We show that this metric can be\ncomputed efficiently as a convex program for a variety of practical settings,\nand further argue that reachability is not inherently at odds with accuracy. We\ndemonstrate evaluations of recommendation algorithms trained on large datasets\nof explicit and implicit ratings. Our results illustrate how preference models,\nselection rules, and user interventions impact reachability and how these\neffects can be distributed unevenly.",
          "link": "http://arxiv.org/abs/2107.00833",
          "publishedOn": "2021-07-05T01:54:58.325Z",
          "wordCount": 610,
          "title": "Quantifying Availability and Discovery in Recommender Systems via Stochastic Reachability. (arXiv:2107.00833v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohtasib_A/0/1/0/all/0/1\">Abdalkarim Mohtasib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E%2E_A/0/1/0/all/0/1\">Amir Ghalamzan E.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellotto_N/0/1/0/all/0/1\">Nicola Bellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuayahuitl_H/0/1/0/all/0/1\">Heriberto Cuay&#xe1;huitl</a>",
          "description": "Robots learning a new manipulation task from a small amount of demonstrations\nare increasingly demanded in different workspaces. A classifier model assessing\nthe quality of actions can predict the successful completion of a task, which\ncan be used by intelligent agents for action-selection. This paper presents a\nnovel classifier that learns to classify task completion only from a few\ndemonstrations. We carry out a comprehensive comparison of different neural\nclassifiers, e.g. fully connected-based, fully convolutional-based,\nsequence2sequence-based, and domain adaptation-based classification. We also\npresent a new dataset including five robot manipulation tasks, which is\npublicly available. We compared the performances of our novel classifier and\nthe existing models using our dataset and the MIME dataset. The results suggest\ndomain adaptation and timing-based features improve success prediction. Our\nnovel model, i.e. fully convolutional neural network with domain adaptation and\ntiming features, achieves an average classification accuracy of 97.3\\% and\n95.5\\% across tasks in both datasets whereas state-of-the-art classifiers\nwithout domain adaptation and timing-features only achieve 82.4\\% and 90.3\\%,\nrespectively.",
          "link": "http://arxiv.org/abs/2107.00722",
          "publishedOn": "2021-07-05T01:54:58.319Z",
          "wordCount": 613,
          "title": "Neural Task Success Classifiers for Robotic Manipulation from Few Real Demonstrations. (arXiv:2107.00722v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanervisto_A/0/1/0/all/0/1\">Anssi Kanervisto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheller_C/0/1/0/all/0/1\">Christian Scheller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schraner_Y/0/1/0/all/0/1\">Yanick Schraner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hautamaki_V/0/1/0/all/0/1\">Ville Hautam&#xe4;ki</a>",
          "description": "Reinforcement learning (RL) research focuses on general solutions that can be\napplied across different domains. This results in methods that RL practitioners\ncan use in almost any domain. However, recent studies often lack the\nengineering steps (\"tricks\") which may be needed to effectively use RL, such as\nreward shaping, curriculum learning, and splitting a large task into smaller\nchunks. Such tricks are common, if not necessary, to achieve state-of-the-art\nresults and win RL competitions. To ease the engineering efforts, we distill\ndescriptions of tricks from state-of-the-art results and study how well these\ntricks can improve a standard deep Q-learning agent. The long-term goal of this\nwork is to enable combining proven RL methods with domain-specific tricks by\nproviding a unified software framework and accompanying insights in multiple\ndomains.",
          "link": "http://arxiv.org/abs/2107.00703",
          "publishedOn": "2021-07-05T01:54:58.308Z",
          "wordCount": 577,
          "title": "Distilling Reinforcement Learning Tricks for Video Games. (arXiv:2107.00703v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_E/0/1/0/all/0/1\">Elia Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Francesco Silvestri</a>",
          "description": "A free-floating bike-sharing system (FFBSS) is a dockless rental system where\nan individual can borrow a bike and returns it everywhere, within the service\narea. To improve the rental service, available bikes should be distributed over\nthe entire service area: a customer leaving from any position is then more\nlikely to find a near bike and then to use the service. Moreover, spreading\nbikes among the entire service area increases urban spatial equity since the\nbenefits of FFBSS are not a prerogative of just a few zones. For guaranteeing\nsuch distribution, the FFBSS operator can use vans to manually relocate bikes,\nbut it incurs high economic and environmental costs. We propose a novel\napproach that exploits the existing bike flows generated by customers to\ndistribute bikes. More specifically, by envisioning the problem as an Influence\nMaximization problem, we show that it is possible to position batches of bikes\non a small number of zones, and then the daily use of FFBSS will efficiently\nspread these bikes on a large area. We show that detecting these areas is\nNP-complete, but there exists a simple and efficient $1-1/e$ approximation\nalgorithm; our approach is then evaluated on a dataset of rides from the\nfree-floating bike-sharing system of the city of Padova.",
          "link": "http://arxiv.org/abs/2107.00761",
          "publishedOn": "2021-07-05T01:54:58.297Z",
          "wordCount": 654,
          "title": "On the Bike Spreading Problem. (arXiv:2107.00761v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiari_H/0/1/0/all/0/1\">Hossein Esfandiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1\">Vahab Mirrokni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shyam Narayanan</a>",
          "description": "Recently, due to an increasing interest for transparency in artificial\nintelligence, several methods of explainable machine learning have been\ndeveloped with the simultaneous goal of accuracy and interpretability by\nhumans. In this paper, we study a recent framework of explainable clustering\nfirst suggested by Dasgupta et al.~\\cite{dasgupta2020explainable}.\nSpecifically, we focus on the $k$-means and $k$-medians problems and provide\nnearly tight upper and lower bounds.\n\nFirst, we provide an $O(\\log k \\log \\log k)$-approximation algorithm for\nexplainable $k$-medians, improving on the best known algorithm of\n$O(k)$~\\cite{dasgupta2020explainable} and nearly matching the known\n$\\Omega(\\log k)$ lower bound~\\cite{dasgupta2020explainable}. In addition, in\nlow-dimensional spaces $d \\ll \\log k$, we show that our algorithm also provides\nan $O(d \\log^2 d)$-approximate solution for explainable $k$-medians. This\nimproves over the best known bound of $O(d \\log k)$ for low\ndimensions~\\cite{laber2021explainable}, and is a constant for constant\ndimensional spaces. To complement this, we show a nearly matching $\\Omega(d)$\nlower bound. Next, we study the $k$-means problem in this context and provide\nan $O(k \\log k)$-approximation algorithm for explainable $k$-means, improving\nover the $O(k^2)$ bound of Dasgupta et al. and the $O(d k \\log k)$ bound of\n\\cite{laber2021explainable}. To complement this we provide an almost tight\n$\\Omega(k)$ lower bound, improving over the $\\Omega(\\log k)$ lower bound of\nDasgupta et al. All our algorithms run in near linear time in the number of\npoints and the dimension.",
          "link": "http://arxiv.org/abs/2107.00774",
          "publishedOn": "2021-07-05T01:54:58.290Z",
          "wordCount": 662,
          "title": "Almost Tight Approximation Algorithms for Explainable Clustering. (arXiv:2107.00774v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Altuner_A/0/1/0/all/0/1\">Anil Berk Altuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilimci_Z/0/1/0/all/0/1\">Zeynep Hilal Kilimci</a>",
          "description": "Stock market prediction has been an important topic for investors,\nresearchers, and analysts. Because it is affected by too many factors, stock\nmarket prediction is a difficult task to handle. In this study, we propose a\nnovel method that is based on deep reinforcement learning methodologies for the\ndirection prediction of stocks using sentiments of community and knowledge\ngraph. For this purpose, we firstly construct a social knowledge graph of users\nby analyzing relations between connections. After that, time series analysis of\nrelated stock and sentiment analysis is blended with deep reinforcement\nmethodology. Turkish version of Bidirectional Encoder Representations from\nTransformers (BerTurk) is employed to analyze the sentiments of the users while\ndeep Q-learning methodology is used for the deep reinforcement learning side of\nthe proposed model to construct the deep Q network. In order to demonstrate the\neffectiveness of the proposed model, Garanti Bank (GARAN), Akbank (AKBNK),\nT\\\"urkiye \\.I\\c{s} Bankas{\\i} (ISCTR) stocks in Istanbul Stock Exchange are\nused as a case study. Experiment results show that the proposed novel model\nachieves remarkable results for stock market prediction task.",
          "link": "http://arxiv.org/abs/2107.00931",
          "publishedOn": "2021-07-05T01:54:58.273Z",
          "wordCount": 642,
          "title": "A Novel Deep Reinforcement Learning Based Stock Direction Prediction using Knowledge Graph and Community Aware Sentiments. (arXiv:2107.00931v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Anubhab Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honore_A/0/1/0/all/0/1\">Antoine Honor&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Saikat Chatterjee</a>",
          "description": "In pursuit of explainability, we develop generative models for sequential\ndata. The proposed models provide state-of-the-art classification results and\nrobust performance for speech phone classification. We combine modern neural\nnetworks (normalizing flows) and traditional generative models (hidden Markov\nmodels - HMMs). Normalizing flow-based mixture models (NMMs) are used to model\nthe conditional probability distribution given the hidden state in the HMMs.\nModel parameters are learned through judicious combinations of time-tested\nBayesian learning methods and contemporary neural network learning methods. We\nmainly combine expectation-maximization (EM) and mini-batch gradient descent.\nThe proposed generative models can compute likelihood of a data and hence\ndirectly suitable for maximum-likelihood (ML) classification approach. Due to\nstructural flexibility of HMMs, we can use different normalizing flow models.\nThis leads to different types of HMMs providing diversity in data modeling\ncapacity. The diversity provides an opportunity for easy decision fusion from\ndifferent models. For a standard speech phone classification setup involving 39\nphones (classes) and the TIMIT dataset, we show that the use of standard\nfeatures called mel-frequency-cepstral-coeffcients (MFCCs), the proposed\ngenerative models, and the decision fusion together can achieve $86.6\\%$\naccuracy by generative training only. This result is close to state-of-the-art\nresults, for examples, $86.2\\%$ accuracy of PyTorch-Kaldi toolkit [1], and\n$85.1\\%$ accuracy using light gated recurrent units [2]. We do not use any\ndiscriminative learning approach and related sophisticated features in this\narticle.",
          "link": "http://arxiv.org/abs/2107.00730",
          "publishedOn": "2021-07-05T01:54:58.262Z",
          "wordCount": 690,
          "title": "Normalizing Flow based Hidden Markov Models for Classification of Speech Phones with Explainability. (arXiv:2107.00730v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00734",
          "author": "<a href=\"http://arxiv.org/find/hep-lat/1/au:+Hackett_D/0/1/0/all/0/1\">Daniel C. Hackett</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Hsieh_C/0/1/0/all/0/1\">Chung-Chun Hsieh</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Albergo_M/0/1/0/all/0/1\">Michael S. Albergo</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Boyda_D/0/1/0/all/0/1\">Denis Boyda</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Chen_J/0/1/0/all/0/1\">Jiunn-Wei Chen</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Chen_K/0/1/0/all/0/1\">Kai-Feng Chen</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Cranmer_K/0/1/0/all/0/1\">Kyle Cranmer</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Kanwar_G/0/1/0/all/0/1\">Gurtej Kanwar</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Shanahan_P/0/1/0/all/0/1\">Phiala E. Shanahan</a>",
          "description": "Recent results have demonstrated that samplers constructed with flow-based\ngenerative models are a promising new approach for configuration generation in\nlattice field theory. In this paper, we present a set of methods to construct\nflow models for targets with multiple separated modes (i.e. theories with\nmultiple vacua). We demonstrate the application of these methods to modeling\ntwo-dimensional real scalar field theory in its symmetry-broken phase. In this\ncontext we investigate the performance of different flow-based sampling\nalgorithms, including a composite sampling algorithm where flow-based proposals\nare occasionally augmented by applying updates using traditional algorithms\nlike HMC.",
          "link": "http://arxiv.org/abs/2107.00734",
          "publishedOn": "2021-07-05T01:54:58.256Z",
          "wordCount": 563,
          "title": "Flow-based sampling for multimodal distributions in lattice field theory. (arXiv:2107.00734v1 [hep-lat])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}