{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.14154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1\">Chester Palen-Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holley_N/0/1/0/all/0/1\">Nolan Holley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>",
          "description": "To address what we believe is a looming crisis of unreproducible evaluation\nfor named entity recognition tasks, we present guidelines for reproducible\nevaluation. The guidelines we propose are extremely simple, focusing on\ntransparency regarding how chunks are encoded and scored, but very few papers\ncurrently being published fully comply with them. We demonstrate that despite\nthe apparent simplicity of NER evaluation, unreported differences in the\nscoring procedure can result in changes to scores that are both of noticeable\nmagnitude and are statistically significant. We provide SeqScore, an open\nsource toolkit that addresses many of the issues that cause replication\nfailures and makes following our guidelines easy.",
          "link": "http://arxiv.org/abs/2107.14154",
          "publishedOn": "2021-07-30T02:13:27.900Z",
          "wordCount": 539,
          "title": "Addressing Barriers to Reproducible Named Entity Recognition Evaluation. (arXiv:2107.14154v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jingzhen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Duan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaoxi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zeng Zhao</a>",
          "description": "Recently, large-scale transformer-based models have been proven to be\neffective over a variety of tasks across many domains. Nevertheless, putting\nthem into production is very expensive, requiring comprehensive optimization\ntechniques to reduce inference costs. This paper introduces a series of\ntransformer inference optimization techniques that are both in algorithm level\nand hardware level. These techniques include a pre-padding decoding mechanism\nthat improves token parallelism for text generation, and highly optimized\nkernels designed for very long input length and large hidden size. On this\nbasis, we propose a transformer inference acceleration library -- Easy and\nEfficient Transformer (EET), which has a significant performance improvement\nover existing libraries. Compared to Faster Transformer v4.0's implementation\nfor GPT-2 layer on A100, EET achieves a 1.5-4.5x state-of-art speedup varying\nwith different context lengths. EET is available at\nhttps://github.com/NetEase-FuXi/EET. A demo video is available at\nhttps://youtu.be/22UPcNGcErg.",
          "link": "http://arxiv.org/abs/2104.12470",
          "publishedOn": "2021-07-30T02:13:27.864Z",
          "wordCount": 627,
          "title": "Easy and Efficient Transformer : Scalable Inference Solution For large NLP model. (arXiv:2104.12470v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1\">Tomer Wolfson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>",
          "description": "Recent efforts to create challenge benchmarks that test the abilities of\nnatural language understanding models have largely depended on human\nannotations. In this work, we introduce the \"Break, Perturb, Build\" (BPB)\nframework for automatic reasoning-oriented perturbation of question-answer\npairs. BPB represents a question by decomposing it into the reasoning steps\nthat are required to answer it, symbolically perturbs the decomposition, and\nthen generates new question-answer pairs. We demonstrate the effectiveness of\nBPB by creating evaluation sets for three reading comprehension (RC)\nbenchmarks, generating thousands of high-quality examples without human\nintervention. We evaluate a range of RC models on our evaluation sets, which\nreveals large performance gaps on generated examples compared to the original\ndata. Moreover, symbolic perturbations enable fine-grained analysis of the\nstrengths and limitations of models. Last, augmenting the training data with\nexamples generated by BPB helps close performance gaps, without any drop on the\noriginal data distribution.",
          "link": "http://arxiv.org/abs/2107.13935",
          "publishedOn": "2021-07-30T02:13:27.852Z",
          "wordCount": 585,
          "title": "Break, Perturb, Build: Automatic Perturbation of Reasoning Paths through Question Decomposition. (arXiv:2107.13935v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oka_Y/0/1/0/all/0/1\">Yui Oka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>",
          "description": "Non-autoregressive neural machine translation (NAT) usually employs\nsequence-level knowledge distillation using autoregressive neural machine\ntranslation (AT) as its teacher model. However, a NAT model often outputs\nshorter sentences than an AT model. In this work, we propose sequence-level\nknowledge distillation (SKD) using perturbed length-aware positional encoding\nand apply it to a student model, the Levenshtein Transformer. Our method\noutperformed a standard Levenshtein Transformer by 2.5 points in bilingual\nevaluation understudy (BLEU) at maximum in a WMT14 German to English\ntranslation. The NAT model output longer sentences than the baseline NAT\nmodels.",
          "link": "http://arxiv.org/abs/2107.13689",
          "publishedOn": "2021-07-30T02:13:27.825Z",
          "wordCount": 536,
          "title": "Using Perturbed Length-aware Positional Encoding for Non-autoregressive Neural Machine Translation. (arXiv:2107.13689v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Rishabh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1\">Samson Yu Bai Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Romila Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Abhinaba Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhaya_N/0/1/0/all/0/1\">Niyati Chhaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>",
          "description": "We address the problem of recognizing emotion cause in conversations, define\ntwo novel sub-tasks of this problem, and provide a corresponding dialogue-level\ndataset, along with strong Transformer-based baselines. The dataset is\navailable at https://github.com/declare-lab/RECCON.\n\nIntroduction: Recognizing the cause behind emotions in text is a fundamental\nyet under-explored area of research in NLP. Advances in this area hold the\npotential to improve interpretability and performance in affect-based models.\nIdentifying emotion causes at the utterance level in conversations is\nparticularly challenging due to the intermingling dynamics among the\ninterlocutors.\n\nMethod: We introduce the task of Recognizing Emotion Cause in CONversations\nwith an accompanying dataset named RECCON, containing over 1,000 dialogues and\n10,000 utterance cause-effect pairs. Furthermore, we define different cause\ntypes based on the source of the causes, and establish strong Transformer-based\nbaselines to address two different sub-tasks on this dataset: causal span\nextraction and causal emotion entailment.\n\nResult: Our Transformer-based baselines, which leverage contextual\npre-trained embeddings, such as RoBERTa, outperform the state-of-the-art\nemotion cause extraction approaches\n\nConclusion: We introduce a new task highly relevant for (explainable)\nemotion-aware artificial intelligence: recognizing emotion cause in\nconversations, provide a new highly challenging publicly available\ndialogue-level dataset for this task, and give strong baseline results on this\ndataset.",
          "link": "http://arxiv.org/abs/2012.11820",
          "publishedOn": "2021-07-30T02:13:27.819Z",
          "wordCount": 705,
          "title": "Recognizing Emotion Cause in Conversations. (arXiv:2012.11820v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grunewald_S/0/1/0/all/0/1\">Stefan Gr&#xfc;newald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1\">Annemarie Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhn_J/0/1/0/all/0/1\">Jonas Kuhn</a>",
          "description": "The introduction of pre-trained transformer-based contextualized word\nembeddings has led to considerable improvements in the accuracy of graph-based\nparsers for frameworks such as Universal Dependencies (UD). However, previous\nworks differ in various dimensions, including their choice of pre-trained\nlanguage models and whether they use LSTM layers. With the aims of\ndisentangling the effects of these choices and identifying a simple yet widely\napplicable architecture, we introduce STEPS, a new modular graph-based\ndependency parser. Using STEPS, we perform a series of analyses on the UD\ncorpora of a diverse set of languages. We find that the choice of pre-trained\nembeddings has by far the greatest impact on parser performance and identify\nXLM-R as a robust choice across the languages in our study. Adding LSTM layers\nprovides no benefits when using transformer-based embeddings. A multi-task\ntraining setup outputting additional UD features may contort results. Taking\nthese insights together, we propose a simple but widely applicable parser\narchitecture and configuration, achieving new state-of-the-art results (in\nterms of LAS) for 10 out of 12 diverse languages.",
          "link": "http://arxiv.org/abs/2010.12699",
          "publishedOn": "2021-07-30T02:13:27.771Z",
          "wordCount": 665,
          "title": "Applying Occam's Razor to Transformer-Based Dependency Parsing: What Works, What Doesn't, and What is Really Necessary. (arXiv:2010.12699v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huayu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitfield_E/0/1/0/all/0/1\">Emma Whitfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>",
          "description": "The identification of rare diseases from clinical notes with Natural Language\nProcessing (NLP) is challenging due to the few cases available for machine\nlearning and the need of data annotation from clinical experts. We propose a\nmethod using ontologies and weak supervision. The approach includes two steps:\n(i) Text-to-UMLS, linking text mentions to concepts in Unified Medical Language\nSystem (UMLS), with a named entity linking tool (e.g. SemEHR) and weak\nsupervision based on customised rules and Bidirectional Encoder Representations\nfrom Transformers (BERT) based contextual representations, and (ii)\nUMLS-to-ORDO, matching UMLS concepts to rare diseases in Orphanet Rare Disease\nOntology (ORDO). Using MIMIC-III US intensive care discharge summaries as a\ncase study, we show that the Text-to-UMLS process can be greatly improved with\nweak supervision, without any annotated data from domain experts. Our analysis\nshows that the overall pipeline processing discharge summaries can surface rare\ndisease cases, which are mostly uncaptured in manual ICD codes of the hospital\nadmissions.",
          "link": "http://arxiv.org/abs/2105.01995",
          "publishedOn": "2021-07-30T02:13:27.757Z",
          "wordCount": 656,
          "title": "Rare Disease Identification from Clinical Notes with Ontologies and Weak Supervision. (arXiv:2105.01995v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>",
          "description": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.",
          "link": "http://arxiv.org/abs/2107.08661",
          "publishedOn": "2021-07-30T02:13:27.734Z",
          "wordCount": 631,
          "title": "Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Clouatre_L/0/1/0/all/0/1\">Louis Clouatre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1\">Prasanna Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zouaq_A/0/1/0/all/0/1\">Amal Zouaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>",
          "description": "Recent research analyzing the sensitivity of natural language understanding\nmodels to word-order perturbations have shown that the state-of-the-art models\nin several language tasks may have a unique way to understand the text that\ncould seldom be explained with conventional syntax and semantics. In this\npaper, we investigate the insensitivity of natural language models to\nword-order by quantifying perturbations and analysing their effect on neural\nmodels' performance on language understanding tasks in GLUE benchmark. Towards\nthat end, we propose two metrics - the Direct Neighbour Displacement (DND) and\nthe Index Displacement Count (IDC) - that score the local and global ordering\nof tokens in the perturbed texts and observe that perturbation functions found\nin prior literature affect only the global ordering while the local ordering\nremains relatively unperturbed. We propose perturbations at the granularity of\nsub-words and characters to study the correlation between DND, IDC and the\nperformance of neural language models on natural language tasks. We find that\nneural language models - pretrained and non-pretrained Transformers, LSTMs, and\nConvolutional architectures - require local ordering more so than the global\nordering of tokens. The proposed metrics and the suite of perturbations allow a\nsystematic way to study the (in)sensitivity of neural language understanding\nmodels to varying degree of perturbations.",
          "link": "http://arxiv.org/abs/2107.13955",
          "publishedOn": "2021-07-30T02:13:27.560Z",
          "wordCount": 648,
          "title": "Demystifying Neural Language Models' Insensitivity to Word-Order. (arXiv:2107.13955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ankush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sohom Ghosh</a>",
          "description": "Hypernym and synonym matching are one of the mainstream Natural Language\nProcessing (NLP) tasks. In this paper, we present systems that attempt to solve\nthis problem. We designed these systems to participate in the FinSim-3, a\nshared task of FinNLP workshop at IJCAI-2021. The shared task is focused on\nsolving this problem for the financial domain. We experimented with various\ntransformer based pre-trained embeddings by fine-tuning these for either\nclassification or phrase similarity tasks. We also augmented the provided\ndataset with abbreviations derived from prospectus provided by the organizers\nand definitions of the financial terms from DBpedia [Auer et al., 2007],\nInvestopedia, and the Financial Industry Business Ontology (FIBO). Our best\nperforming system uses both FinBERT [Araci, 2019] and data augmentation from\nthe afore-mentioned sources. We observed that term expansion using data\naugmentation in conjunction with semantic similarity is beneficial for this\ntask and could be useful for the other tasks that deal with short phrases. Our\nbest performing model (Accuracy: 0.917, Rank: 1.156) was developed by\nfine-tuning SentenceBERT [Reimers et al., 2019] (with FinBERT at the backend)\nover an extended labelled set created using the hierarchy of labels present in\nFIBO.",
          "link": "http://arxiv.org/abs/2107.13764",
          "publishedOn": "2021-07-30T02:13:27.536Z",
          "wordCount": 654,
          "title": "Term Expansion and FinBERT fine-tuning for Hypernym and Synonym Ranking of Financial Terms. (arXiv:2107.13764v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.05680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nortje_L/0/1/0/all/0/1\">Leanne Nortje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>",
          "description": "We propose direct multimodal few-shot models that learn a shared embedding\nspace of spoken words and images from only a few paired examples. Imagine an\nagent is shown an image along with a spoken word describing the object in the\npicture, e.g. pen, book and eraser. After observing a few paired examples of\neach class, the model is asked to identify the \"book\" in a set of unseen\npictures. Previous work used a two-step indirect approach relying on learned\nunimodal representations: speech-speech and image-image comparisons are\nperformed across the support set of given speech-image pairs. We propose two\ndirect models which instead learn a single multimodal space where inputs from\ndifferent modalities are directly comparable: a multimodal triplet network\n(MTriplet) and a multimodal correspondence autoencoder (MCAE). To train these\ndirect models, we mine speech-image pairs: the support set is used to pair up\nunlabelled in-domain speech and images. In a speech-to-image digit matching\ntask, direct models outperform indirect models, with the MTriplet achieving the\nbest multimodal five-shot accuracy. We show that the improvements are due to\nthe combination of unsupervised and transfer learning in the direct models, and\nthe absence of two-step compounding errors.",
          "link": "http://arxiv.org/abs/2012.05680",
          "publishedOn": "2021-07-30T02:13:27.511Z",
          "wordCount": 664,
          "title": "Direct multimodal few-shot learning of speech and images. (arXiv:2012.05680v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.11485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zehong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Tran</a>",
          "description": "Word embeddings can reflect the semantic representations, and the embedding\nqualities can be comprehensively evaluated with human natural reading-related\ncognitive data sources. In this paper, we proposed the CogniFNN framework,\nwhich is the first attempt at using fuzzy neural networks to extract non-linear\nand non-stationary characteristics for evaluations of English word embeddings\nagainst the corresponding cognitive datasets. In our experiment, we used 15\nhuman cognitive datasets across three modalities: EEG, fMRI, and eye-tracking,\nand selected the mean square error and multiple hypotheses testing as metrics\nto evaluate our proposed CogniFNN framework. Compared to the recent pioneer\nframework, our proposed CogniFNN showed smaller prediction errors of both\ncontext-independent (GloVe) and context-sensitive (BERT) word embeddings, and\nachieved higher significant ratios with randomly generated word embeddings. Our\nfindings suggested that the CogniFNN framework could provide a more accurate\nand comprehensive evaluation of cognitive word embeddings. It will potentially\nbe beneficial to the further word embeddings evaluation on extrinsic natural\nlanguage processing tasks.",
          "link": "http://arxiv.org/abs/2009.11485",
          "publishedOn": "2021-07-30T02:13:27.470Z",
          "wordCount": 641,
          "title": "CogniFNN: A Fuzzy Neural Network Framework for Cognitive Word Embedding Evaluation. (arXiv:2009.11485v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "This paper surveys and organizes research works in a new paradigm in natural\nlanguage processing, which we dub \"prompt-based learning\". Unlike traditional\nsupervised learning, which trains a model to take in an input x and predict an\noutput y as P(y|x), prompt-based learning is based on language models that\nmodel the probability of text directly. To use these models to perform\nprediction tasks, the original input x is modified using a template into a\ntextual string prompt x' that has some unfilled slots, and then the language\nmodel is used to probabilistically fill the unfilled information to obtain a\nfinal string x, from which the final output y can be derived. This framework is\npowerful and attractive for a number of reasons: it allows the language model\nto be pre-trained on massive amounts of raw text, and by defining a new\nprompting function the model is able to perform few-shot or even zero-shot\nlearning, adapting to new scenarios with few or no labeled data. In this paper\nwe introduce the basics of this promising paradigm, describe a unified set of\nmathematical notations that can cover a wide variety of existing work, and\norganize existing work along several dimensions, e.g.the choice of pre-trained\nmodels, prompts, and tuning strategies. To make the field more accessible to\ninterested beginners, we not only make a systematic review of existing works\nand a highly structured typology of prompt-based concepts, but also release\nother resources, e.g., a website this http URL including\nconstantly-updated survey, and paperlist.",
          "link": "http://arxiv.org/abs/2107.13586",
          "publishedOn": "2021-07-30T02:13:27.208Z",
          "wordCount": 710,
          "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (arXiv:2107.13586v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "Pre-trained language models (PLMs) have achieved great success in natural\nlanguage processing. Most of PLMs follow the default setting of architecture\nhyper-parameters (e.g., the hidden dimension is a quarter of the intermediate\ndimension in feed-forward sub-networks) in BERT (Devlin et al., 2019). Few\nstudies have been conducted to explore the design of architecture\nhyper-parameters in BERT, especially for the more efficient PLMs with tiny\nsizes, which are essential for practical deployment on resource-constrained\ndevices. In this paper, we adopt the one-shot Neural Architecture Search (NAS)\nto automatically search architecture hyper-parameters. Specifically, we\ncarefully design the techniques of one-shot learning and the search space to\nprovide an adaptive and efficient development way of tiny PLMs for various\nlatency constraints. We name our method AutoTinyBERT and evaluate its\neffectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show\nthat our method outperforms both the SOTA search-based baseline (NAS-BERT) and\nthe SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM and\nMobileBERT). In addition, based on the obtained architectures, we propose a\nmore efficient development method that is even faster than the development of a\nsingle PLM.",
          "link": "http://arxiv.org/abs/2107.13686",
          "publishedOn": "2021-07-30T02:13:27.181Z",
          "wordCount": 639,
          "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models. (arXiv:2107.13686v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>",
          "description": "Pre-training on larger datasets with ever increasing model size is now a\nproven recipe for increased performance across almost all NLP tasks. A notable\nexception is information retrieval, where additional pre-training has so far\nfailed to produce convincing results. We show that, with the right pre-training\nsetup, this barrier can be overcome. We demonstrate this by pre-training large\nbi-encoder models on 1) a recently released set of 65 million synthetically\ngenerated questions, and 2) 200 million post-comment pairs from a preexisting\ndataset of Reddit conversations made available by pushshift.io. We evaluate on\na set of information retrieval and dialogue retrieval benchmarks, showing\nsubstantial improvements over supervised baselines.",
          "link": "http://arxiv.org/abs/2107.13602",
          "publishedOn": "2021-07-30T02:13:27.152Z",
          "wordCount": 554,
          "title": "Domain-matched Pre-training Tasks for Dense Retrieval. (arXiv:2107.13602v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasquez_Rodriguez_L/0/1/0/all/0/1\">Laura V&#xe1;squez-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1\">Matthew Shardlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1\">Piotr Przyby&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>",
          "description": "Modern text simplification (TS) heavily relies on the availability of gold\nstandard data to build machine learning models. However, existing studies show\nthat parallel TS corpora contain inaccurate simplifications and incorrect\nalignments. Additionally, evaluation is usually performed by using metrics such\nas BLEU or SARI to compare system output to the gold standard. A major\nlimitation is that these metrics do not match human judgements and the\nperformance on different datasets and linguistic phenomena vary greatly.\nFurthermore, our research shows that the test and training subsets of parallel\ndatasets differ significantly. In this work, we investigate existing TS\ncorpora, providing new insights that will motivate the improvement of existing\nstate-of-the-art TS evaluation methods. Our contributions include the analysis\nof TS corpora based on existing modifications used for simplification and an\nempirical study on TS models performance by using better-distributed datasets.\nWe demonstrate that by improving the distribution of TS datasets, we can build\nmore robust TS models.",
          "link": "http://arxiv.org/abs/2107.13662",
          "publishedOn": "2021-07-30T02:13:27.101Z",
          "wordCount": 601,
          "title": "Investigating Text Simplification Evaluation. (arXiv:2107.13662v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nurce_E/0/1/0/all/0/1\">Erida Nurce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keci_J/0/1/0/all/0/1\">Jorgel Keci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>",
          "description": "The ever growing usage of social media in the recent years has had a direct\nimpact on the increased presence of hate speech and offensive speech in online\nplatforms. Research on effective detection of such content has mainly focused\non English and a few other widespread languages, while the leftover majority\nfail to have the same work put into them and thus cannot benefit from the\nsteady advancements made in the field. In this paper we present \\textsc{Shaj},\nan annotated Albanian dataset for hate speech and offensive speech that has\nbeen constructed from user-generated content on various social media platforms.\nIts annotation follows the hierarchical schema introduced in OffensEval. The\ndataset is tested using three different classification models, the best of\nwhich achieves an F1 score of 0.77 for the identification of offensive\nlanguage, 0.64 F1 score for the automatic categorization of offensive types and\nlastly, 0.52 F1 score for the offensive language target identification.",
          "link": "http://arxiv.org/abs/2107.13592",
          "publishedOn": "2021-07-30T02:13:27.042Z",
          "wordCount": 573,
          "title": "Detecting Abusive Albanian. (arXiv:2107.13592v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:07.674Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1\">Keld T. Lundgaard</a>",
          "description": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.",
          "link": "http://arxiv.org/abs/2107.13054",
          "publishedOn": "2021-07-29T02:00:07.655Z",
          "wordCount": 623,
          "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2104.04039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>",
          "description": "Large pre-trained neural language models (LM) have very powerful text\ngeneration capabilities. However, in practice, they are hard to control for\ncreative purposes. We describe a Plug-and-Play controllable language generation\nframework, Plug-and-Blend, that allows a human user to input multiple control\ncodes (topics). In the context of automated story generation, this allows a\nhuman user loose or fine-grained control of the topics and transitions between\nthem that will appear in the generated story, and can even allow for\noverlapping, blended topics. Automated evaluations show our framework, working\nwith different generative LMs, controls the generation towards given\ncontinuous-weighted control codes while keeping the generated sentences fluent,\ndemonstrating strong blending capability. A human participant evaluation shows\nthat the generated stories are observably transitioning between two topics.",
          "link": "http://arxiv.org/abs/2104.04039",
          "publishedOn": "2021-07-29T02:00:07.636Z",
          "wordCount": 601,
          "title": "Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes. (arXiv:2104.04039v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "This paper argues that training GANs on local and non-local dependencies in\nspeech data offers insights into how deep neural networks discretize continuous\ndata and how symbolic-like rule-based morphophonological processes emerge in a\ndeep convolutional architecture. Acquisition of speech has recently been\nmodeled as a dependency between latent space and data generated by GANs in\nBegu\\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local\nallophonic distribution. We extend this approach to test learning of local and\nnon-local phonological processes that include approximations of morphological\nprocesses. We further parallel outputs of the model to results of a behavioral\nexperiment where human subjects are trained on the data used for training the\nGAN network. Four main conclusions emerge: (i) the networks provide useful\ninformation for computational models of speech acquisition even if trained on a\ncomparatively small dataset of an artificial grammar learning experiment; (ii)\nlocal processes are easier to learn than non-local processes, which matches\nboth behavioral data in human subjects and typology in the world's languages.\nThis paper also proposes (iii) how we can actively observe the network's\nprogress in learning and explore the effect of training steps on learning\nrepresentations by keeping latent space constant across different training\nsteps. Finally, this paper shows that (iv) the network learns to encode the\npresence of a prefix with a single latent variable; by interpolating this\nvariable, we can actively observe the operation of a non-local phonological\nprocess. The proposed technique for retrieving learning representations has\ngeneral implications for our understanding of how GANs discretize continuous\nspeech data and suggests that rule-like generalizations in the training data\nare represented as an interaction between variables in the network's latent\nspace.",
          "link": "http://arxiv.org/abs/2009.12711",
          "publishedOn": "2021-07-29T02:00:07.628Z",
          "wordCount": 761,
          "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks. (arXiv:2009.12711v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>",
          "description": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pre-trained vision models such as VGG16 and ResNet\n\nIndex Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image\nclassification, Natural Language Processing, t-SNE, text classification,\nTransfer Learning",
          "link": "http://arxiv.org/abs/2106.12479",
          "publishedOn": "2021-07-29T02:00:07.616Z",
          "wordCount": 706,
          "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heeringa_W/0/1/0/all/0/1\">Wilbert Heeringa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouma_G/0/1/0/all/0/1\">Gosse Bouma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofman_M/0/1/0/all/0/1\">Martha Hofman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drenth_E/0/1/0/all/0/1\">Eduard Drenth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijffels_J/0/1/0/all/0/1\">Jan Wijffels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velde_H/0/1/0/all/0/1\">Hans Van de Velde</a>",
          "description": "We present a lemmatizer/POS-tagger/dependency parser for West Frisian using a\ncorpus of 44,714 words in 3,126 sentences that were annotated according to the\nguidelines of Universal Dependency version 2. POS tags were assigned to words\nby using a Dutch POS tagger that was applied to a literal word-by-word\ntranslation, or to sentences of a Dutch parallel text. Best results were\nobtained when using literal translations that were created by using the Frisian\ntranslation program Oersetter. Morphologic and syntactic annotations were\ngenerated on the basis of a literal Dutch translation as well. The performance\nof the lemmatizer/tagger/annotator when it was trained using default parameters\nwas compared to the performance that was obtained when using the parameter\nvalues that were used for training the LassySmall UD 2.5 corpus. A significant\nimprovement was found for `lemma'. The Frisian lemmatizer/PoS tagger/dependency\nparser is released as a web app and as a web service.",
          "link": "http://arxiv.org/abs/2107.07974",
          "publishedOn": "2021-07-29T02:00:07.608Z",
          "wordCount": 623,
          "title": "POS tagging, lemmatization and dependency parsing of West Frisian. (arXiv:2107.07974v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Keunwoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>",
          "description": "We propose a multimodal singing language classification model that uses both\naudio content and textual metadata. LRID-Net, the proposed model, takes an\naudio signal and a language probability vector estimated from the metadata and\noutputs the probabilities of the target languages. Optionally, LRID-Net is\nfacilitated with modality dropouts to handle a missing modality. In the\nexperiment, we trained several LRID-Nets with varying modality dropout\nconfiguration and tested them with various combinations of input modalities.\nThe experiment results demonstrate that using multimodal input improves\nperformance. The results also suggest that adopting modality dropout does not\ndegrade the performance of the model when there are full modality inputs while\nenabling the model to handle missing modality cases to some extent.",
          "link": "http://arxiv.org/abs/2103.01893",
          "publishedOn": "2021-07-29T02:00:07.589Z",
          "wordCount": 607,
          "title": "Listen, Read, and Identify: Multimodal Singing Language Identification of Music. (arXiv:2103.01893v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelgwad_M/0/1/0/all/0/1\">Mohammed M.Abdelgwad</a>",
          "description": "Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that\ndefines the polarity of opinions on certain aspects related to specific\ntargets. The majority of research on ABSA is in English, with a small amount of\nwork available in Arabic. Most previous Arabic research has relied on deep\nlearning models that depend primarily on context-independent word embeddings\n(e.g.word2vec), where each word has a fixed representation independent of its\ncontext. This article explores the modeling capabilities of contextual\nembeddings from pre-trained language models, such as BERT, and making use of\nsentence pair input on Arabic ABSA tasks. In particular, we are building a\nsimple but effective BERT-based neural baseline to handle this task. Our BERT\narchitecture with a simple linear classification layer surpassed the\nstate-of-the-art works, according to the experimental results on the\nbenchmarked Arabic hotel reviews dataset.",
          "link": "http://arxiv.org/abs/2107.13290",
          "publishedOn": "2021-07-29T02:00:07.582Z",
          "wordCount": 561,
          "title": "Arabic aspect based sentiment analysis using BERT. (arXiv:2107.13290v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chawla_K/0/1/0/all/0/1\">Kushal Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clever_R/0/1/0/all/0/1\">Rene Clever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_J/0/1/0/all/0/1\">Jaysa Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_G/0/1/0/all/0/1\">Gale Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>",
          "description": "Negotiation is a complex social interaction that encapsulates emotional\nencounters in human decision-making. Virtual agents that can negotiate with\nhumans are useful in pedagogy and conversational AI. To advance the development\nof such agents, we explore the prediction of two important subjective goals in\na negotiation - outcome satisfaction and partner perception. Specifically, we\nanalyze the extent to which emotion attributes extracted from the negotiation\nhelp in the prediction, above and beyond the individual difference variables.\nWe focus on a recent dataset in chat-based negotiations, grounded in a\nrealistic camping scenario. We study three degrees of emotion dimensions -\nemoticons, lexical, and contextual by leveraging affective lexicons and a\nstate-of-the-art deep learning architecture. Our insights will be helpful in\ndesigning adaptive negotiation agents that interact through realistic\ncommunication interfaces.",
          "link": "http://arxiv.org/abs/2107.13165",
          "publishedOn": "2021-07-29T02:00:07.574Z",
          "wordCount": 579,
          "title": "Towards Emotion-Aware Agents For Negotiation Dialogues. (arXiv:2107.13165v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1\">Alex Rogozhnikov</a>",
          "description": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2106.03143",
          "publishedOn": "2021-07-29T02:00:07.459Z",
          "wordCount": 607,
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinshuai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>",
          "description": "Robustness against word substitutions has a well-defined and widely\nacceptable form, i.e., using semantically similar words as substitutions, and\nthus it is considered as a fundamental stepping-stone towards broader\nrobustness in natural language processing. Previous defense methods capture\nword substitutions in vector space by using either $l_2$-ball or\nhyper-rectangle, which results in perturbation sets that are not inclusive\nenough or unnecessarily large, and thus impedes mimicry of worst cases for\nrobust training. In this paper, we introduce a novel \\textit{Adversarial Sparse\nConvex Combination} (ASCC) method. We model the word substitution attack space\nas a convex hull and leverages a regularization term to enforce perturbation\ntowards an actual substitution, thus aligning our modeling better with the\ndiscrete textual space. Based on the ASCC method, we further propose\nASCC-defense, which leverages ASCC to generate worst-case perturbations and\nincorporates adversarial training towards robustness. Experiments show that\nASCC-defense outperforms the current state-of-the-arts in terms of robustness\non two prevailing NLP tasks, \\emph{i.e.}, sentiment analysis and natural\nlanguage inference, concerning several attacks across multiple model\narchitectures. Besides, we also envision a new class of defense towards\nrobustness in NLP, where our robustly trained word vectors can be plugged into\na normally trained model and enforce its robustness without applying any other\ndefense techniques.",
          "link": "http://arxiv.org/abs/2107.13541",
          "publishedOn": "2021-07-29T02:00:07.429Z",
          "wordCount": 644,
          "title": "Towards Robustness Against Natural Language Word Substitutions. (arXiv:2107.13541v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>",
          "description": "Recent years have witnessed the prosperity of legal artificial intelligence\nwith the development of technologies. In this paper, we propose a novel legal\napplication of legal provision prediction (LPP), which aims to predict the\nrelated legal provisions of affairs. We formulate this task as a challenging\nknowledge graph completion problem, which requires not only text understanding\nbut also graph reasoning. To this end, we propose a novel text-guided graph\nreasoning approach. We collect amounts of real-world legal provision data from\nthe Guangdong government service website and construct a legal dataset called\nLegalLPP. Extensive experimental results on the dataset show that our approach\nachieves better performance compared with baselines. The code and dataset are\navailable in \\url{https://github.com/zjunlp/LegalPP} for reproducibility.",
          "link": "http://arxiv.org/abs/2104.02284",
          "publishedOn": "2021-07-29T02:00:07.417Z",
          "wordCount": 591,
          "title": "Text-guided Legal Knowledge Graph Reasoning. (arXiv:2104.02284v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13530",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kessler_S/0/1/0/all/0/1\">Samuel Kessler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_B/0/1/0/all/0/1\">Bethan Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karout_S/0/1/0/all/0/1\">Salah Karout</a>",
          "description": "We present a method for continual learning of speech representations for\nmultiple languages using self-supervised learning (SSL) and applying these for\nautomatic speech recognition. There is an abundance of unannotated speech, so\ncreating self-supervised representations from raw audio and finetuning on a\nsmall annotated datasets is a promising direction to build speech recognition\nsystems. Wav2vec models perform SSL on raw audio in a pretraining phase and\nthen finetune on a small fraction of annotated data. SSL models have produced\nstate of the art results for ASR. However, these models are very expensive to\npretrain with self-supervision. We tackle the problem of learning new language\nrepresentations continually from audio without forgetting a previous language\nrepresentation. We use ideas from continual learning to transfer knowledge from\na previous task to speed up pretraining a new language task. Our\ncontinual-wav2vec2 model can decrease pretraining times by 32% when learning a\nnew language task, and learn this new audio-language representation without\nforgetting previous language representation.",
          "link": "http://arxiv.org/abs/2107.13530",
          "publishedOn": "2021-07-29T02:00:07.406Z",
          "wordCount": 634,
          "title": "Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition. (arXiv:2107.13530v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1\">Stephen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1\">Mark Dras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Mark Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>",
          "description": "Sequence-to-Sequence (S2S) neural text generation models, especially the\npre-trained ones (e.g., BART and T5), have exhibited compelling performance on\nvarious natural language generation tasks. However, the black-box nature of\nthese models limits their application in tasks where specific rules (e.g.,\ncontrollable constraints, prior knowledge) need to be executed. Previous works\neither design specific model structure (e.g., Copy Mechanism corresponding to\nthe rule \"the generated output should include certain words in the source\ninput\") or implement specialized inference algorithm (e.g., Constrained Beam\nSearch) to execute particular rules through the text generation. These methods\nrequire careful design case-by-case and are difficult to support multiple rules\nconcurrently. In this paper, we propose a novel module named Neural\nRule-Execution Tracking Machine that can be equipped into various\ntransformer-based generators to leverage multiple rules simultaneously to guide\nthe neural generation model for superior generation performance in a unified\nand scalable way. Extensive experimental results on several benchmarks verify\nthe effectiveness of our proposed model in both controllable and general text\ngeneration.",
          "link": "http://arxiv.org/abs/2107.13077",
          "publishedOn": "2021-07-29T02:00:07.398Z",
          "wordCount": 605,
          "title": "Neural Rule-Execution Tracking Machine For Transformer-Based Text Generation. (arXiv:2107.13077v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsividis_P/0/1/0/all/0/1\">Pedro A. Tsividis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madeano_J/0/1/0/all/0/1\">Jason Madeano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harper_B/0/1/0/all/0/1\">Brin Harper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>",
          "description": "Knowledge built culturally across generations allows humans to learn far more\nthan an individual could glean from their own experience in a lifetime.\nCultural knowledge in turn rests on language: language is the richest record of\nwhat previous generations believed, valued, and practiced. The power and\nmechanisms of language as a means of cultural learning, however, are not well\nunderstood. We take a first step towards reverse-engineering cultural learning\nthrough language. We developed a suite of complex high-stakes tasks in the form\nof minimalist-style video games, which we deployed in an iterated learning\nparadigm. Game participants were limited to only two attempts (two lives) to\nbeat each game and were allowed to write a message to a future participant who\nread the message before playing. Knowledge accumulated gradually across\ngenerations, allowing later generations to advance further in the games and\nperform more efficient actions. Multigenerational learning followed a\nstrikingly similar trajectory to individuals learning alone with an unlimited\nnumber of lives. These results suggest that language provides a sufficient\nmedium to express and accumulate the knowledge people acquire in these diverse\ntasks: the dynamics of the environment, valuable goals, dangerous risks, and\nstrategies for success. The video game paradigm we pioneer here is thus a rich\ntest bed for theories of cultural transmission and learning from language.",
          "link": "http://arxiv.org/abs/2107.13377",
          "publishedOn": "2021-07-29T02:00:07.388Z",
          "wordCount": 675,
          "title": "Growing knowledge culturally across generations to solve novel, complex tasks. (arXiv:2107.13377v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>",
          "description": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
          "link": "http://arxiv.org/abs/2010.06467",
          "publishedOn": "2021-07-29T02:00:07.368Z",
          "wordCount": 725,
          "title": "Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>",
          "description": "In task-oriented conversation systems, natural language generation systems\nthat generate sentences with specific information related to conversation flow\nare useful. Our study focuses on language generation by considering various\ninformation representing the meaning of utterances as multiple conditions of\ngeneration. NLG from meaning representations, the conditions for sentence\nmeaning, generally goes through two steps: sentence planning and surface\nrealization. However, we propose a simple one-stage framework to generate\nutterances directly from MR (Meaning Representation). Our model is based on\nGPT2 and generates utterances with flat conditions on slot and value pairs,\nwhich does not need to determine the structure of the sentence. We evaluate\nseveral systems in the E2E dataset with 6 automatic metrics. Our system is a\nsimple method, but it demonstrates comparable performance to previous systems\nin automated metrics. In addition, using only 10\\% of the data set without any\nother techniques, our model achieves comparable performance, and shows the\npossibility of performing zero-shot generation and expanding to other datasets.",
          "link": "http://arxiv.org/abs/2101.04257",
          "publishedOn": "2021-07-29T02:00:07.354Z",
          "wordCount": 616,
          "title": "Transforming Multi-Conditioned Generation from Meaning Representation. (arXiv:2101.04257v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>",
          "description": "The knowledge of scripts, common chains of events in stereotypical scenarios,\nis a valuable asset for task-oriented natural language understanding systems.\nWe propose the Goal-Oriented Script Construction task, where a model produces a\nsequence of steps to accomplish a given goal. We pilot our task on the first\nmultilingual script learning dataset supporting 18 languages collected from\nwikiHow, a website containing half a million how-to articles. For baselines, we\nconsider both a generation-based approach using a language model and a\nretrieval-based approach by first retrieving the relevant steps from a large\ncandidate pool and then ordering them. We show that our task is practical,\nfeasible but challenging for state-of-the-art Transformer models, and that our\nmethods can be readily deployed for various other datasets and domains with\ndecent zero-shot performance.",
          "link": "http://arxiv.org/abs/2107.13189",
          "publishedOn": "2021-07-29T02:00:07.330Z",
          "wordCount": 562,
          "title": "Goal-Oriented Script Construction. (arXiv:2107.13189v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tieyun Qian</a>",
          "description": "Existing methods in relation extraction have leveraged the lexical features\nin the word sequence and the syntactic features in the parse tree. Though\neffective, the lexical features extracted from the successive word sequence may\nintroduce some noise that has little or no meaningful content. Meanwhile, the\nsyntactic features are usually encoded via graph convolutional networks which\nhave restricted receptive field. To address the above limitations, we propose a\nmulti-scale feature and metric learning framework for relation extraction.\nSpecifically, we first develop a multi-scale convolutional neural network to\naggregate the non-successive mainstays in the lexical sequence. We also design\na multi-scale graph convolutional network which can increase the receptive\nfield towards specific syntactic roles. Moreover, we present a multi-scale\nmetric learning paradigm to exploit both the feature-level relation between\nlexical and syntactic features and the sample-level relation between instances\nwith the same or different classes. We conduct extensive experiments on three\nreal world datasets for various types of relation extraction tasks. The results\ndemonstrate that our model significantly outperforms the state-of-the-art\napproaches.",
          "link": "http://arxiv.org/abs/2107.13425",
          "publishedOn": "2021-07-29T02:00:07.322Z",
          "wordCount": 600,
          "title": "Multi-Scale Feature and Metric Learning for Relation Extraction. (arXiv:2107.13425v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_V/0/1/0/all/0/1\">Vivek Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witteveen_S/0/1/0/all/0/1\">Sam Witteveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_M/0/1/0/all/0/1\">Martin Andrews</a>",
          "description": "Creating explanations for answers to science questions is a challenging task\nthat requires multi-hop inference over a large set of fact sentences. This\nyear, to refocus the Textgraphs Shared Task on the problem of gathering\nrelevant statements (rather than solely finding a single 'correct path'), the\nWorldTree dataset was augmented with expert ratings of 'relevance' of\nstatements to each overall explanation. Our system, which achieved second place\non the Shared Task leaderboard, combines initial statement retrieval; language\nmodels trained to predict the relevance scores; and ensembling of a number of\nthe resulting rankings. Our code implementation is made available at\nhttps://github.com/mdda/worldtree_corpus/tree/textgraphs_2021",
          "link": "http://arxiv.org/abs/2107.13031",
          "publishedOn": "2021-07-29T02:00:07.305Z",
          "wordCount": 570,
          "title": "Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings. (arXiv:2107.13031v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.02951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "How can deep neural networks encode information that corresponds to words in\nhuman speech into raw acoustic data? This paper proposes two neural network\narchitectures for modeling unsupervised lexical learning from raw acoustic\ninputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN),\nthat combine a Deep Convolutional GAN architecture for audio data (WaveGAN;\narXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN\n(arXiv:1606.03657), and propose a new latent space structure that can model\nfeatural learning simultaneously with a higher level classification and allows\nfor a very low-dimension vector representation of lexical items. Lexical\nlearning is modeled as emergent from an architecture that forces a deep neural\nnetwork to output data such that unique information is retrievable from its\nacoustic outputs. The networks trained on lexical items from TIMIT learn to\nencode unique information corresponding to lexical items in the form of\ncategorical variables in their latent space. By manipulating these variables,\nthe network outputs specific lexical items. The network occasionally outputs\ninnovative lexical items that violate training data, but are linguistically\ninterpretable and highly informative for cognitive modeling and neural network\ninterpretability. Innovative outputs suggest that phonetic and phonological\nrepresentations learned by the network can be productively recombined and\ndirectly paralleled to productivity in human speech: a fiwGAN network trained\non `suit' and `dark' outputs innovative `start', even though it never saw\n`start' or even a [st] sequence in the training data. We also argue that\nsetting latent featural codes to values well beyond training range results in\nalmost categorical generation of prototypical lexical items and reveals\nunderlying values of each latent code.",
          "link": "http://arxiv.org/abs/2006.02951",
          "publishedOn": "2021-07-29T02:00:07.279Z",
          "wordCount": 756,
          "title": "CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks. (arXiv:2006.02951v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arrabales_R/0/1/0/all/0/1\">Ra&#xfa;l Arrabales</a>",
          "description": "Most depression assessment tools are based on self-report questionnaires,\nsuch as the Patient Health Questionnaire (PHQ-9). These psychometric\ninstruments can be easily adapted to an online setting by means of electronic\nforms. However, this approach lacks the interacting and engaging features of\nmodern digital environments. With the aim of making depression screening more\navailable, attractive and effective, we developed Perla, a conversational agent\nable to perform an interview based on the PHQ-9. We also conducted a validation\nstudy in which we compared the results obtained by the traditional self-report\nquestionnaire with Perla's automated interview. Analyzing the results from this\nstudy we draw two significant conclusions: firstly, Perla is much preferred by\nInternet users, achieving more than 2.5 times more reach than a traditional\nform-based questionnaire; secondly, her psychometric properties (Cronbach's\nalpha of 0.81, sensitivity of 96% and specificity of 90%) are excellent and\ncomparable to the traditional well-established depression screening\nquestionnaires.",
          "link": "http://arxiv.org/abs/2008.12875",
          "publishedOn": "2021-07-29T02:00:07.256Z",
          "wordCount": 634,
          "title": "Perla: A Conversational Agent for Depression Screening in Digital Ecosystems. Design, Implementation and Validation. (arXiv:2008.12875v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rocholl_J/0/1/0/all/0/1\">Johann C. Rocholl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zayats_V/0/1/0/all/0/1\">Vicky Zayats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_D/0/1/0/all/0/1\">Daniel D. Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murad_N/0/1/0/all/0/1\">Noah B. Murad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_A/0/1/0/all/0/1\">Aaron Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebling_D/0/1/0/all/0/1\">Daniel J. Liebling</a>",
          "description": "Disfluency detection models now approach high accuracy on English text.\nHowever, little exploration has been done in improving the size and inference\ntime of the model. At the same time, automatic speech recognition (ASR) models\nare moving from server-side inference to local, on-device inference. Supporting\nmodels in the transcription pipeline (like disfluency detection) must follow\nsuit. In this work we concentrate on the disfluency detection task, focusing on\nsmall, fast, on-device models based on the BERT architecture. We demonstrate it\nis possible to train disfluency detection models as small as 1.3 MiB, while\nretaining high performance. We build on previous work that showed the benefit\nof data augmentation approaches such as self-training. Then, we evaluate the\neffect of domain mismatch between conversational and written text on model\nperformance. We find that domain adaptation and data augmentation strategies\nhave a more pronounced effect on these smaller models, as compared to\nconventional BERT models.",
          "link": "http://arxiv.org/abs/2104.10769",
          "publishedOn": "2021-07-28T02:02:34.291Z",
          "wordCount": 624,
          "title": "Disfluency Detection with Unlabeled Data and Small BERT Models. (arXiv:2104.10769v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1\">Rotem Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>",
          "description": "The quality of a summarization evaluation metric is quantified by calculating\nthe correlation between its scores and human annotations across a large number\nof summaries. Currently, it is unclear how precise these correlation estimates\nare, nor whether differences between two metrics' correlations reflect a true\ndifference or if it is due to mere chance. In this work, we address these two\nproblems by proposing methods for calculating confidence intervals and running\nhypothesis tests for correlations using two resampling methods, bootstrapping\nand permutation. After evaluating which of the proposed methods is most\nappropriate for summarization through two simulation experiments, we analyze\nthe results of applying these methods to several different automatic evaluation\nmetrics across three sets of human annotations. We find that the confidence\nintervals are rather wide, demonstrating high uncertainty in the reliability of\nautomatic metrics. Further, although many metrics fail to show statistical\nimprovements over ROUGE, two recent works, QAEval and BERTScore, do in some\nevaluation settings.",
          "link": "http://arxiv.org/abs/2104.00054",
          "publishedOn": "2021-07-28T02:02:33.975Z",
          "wordCount": 630,
          "title": "A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods. (arXiv:2104.00054v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-07-28T02:02:30.851Z",
          "wordCount": 654,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>",
          "description": "NLP is deeply intertwined with the formal study of language, both\nconceptually and historically. Arguably, this connection goes all the way back\nto Chomsky's Syntactic Structures in 1957. It also still holds true today, with\na strand of recent works building formal analysis of modern neural networks\nmethods in terms of formal languages. In this document, I aim to explain\nbackground about formal languages as they relate to this recent work. I will by\nnecessity ignore large parts of the rich history of this field, instead\nfocusing on concepts connecting to modern deep learning-based NLP.",
          "link": "http://arxiv.org/abs/2102.10094",
          "publishedOn": "2021-07-28T02:02:30.830Z",
          "wordCount": 561,
          "title": "Formal Language Theory Meets Modern NLP. (arXiv:2102.10094v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekarchi_Z/0/1/0/all/0/1\">Zahra Shekarchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_S/0/1/0/all/0/1\">Suzanne Stevenson</a>",
          "description": "Children learn word meanings by tapping into the commonalities across\ndifferent situations in which words are used and overcome the high level of\nuncertainty involved in early word learning experiences. We propose a modeling\nframework to investigate the role of mutual exclusivity bias - asserting\none-to-one mappings between words and their meanings - in reducing uncertainty\nin word learning. In a set of computational studies, we show that to\nsuccessfully learn word meanings in the face of uncertainty, a learner needs to\nuse two types of competition: words competing for association to a referent\nwhen learning from an observation and referents competing for a word when the\nword is used. Our work highlights the importance of an algorithmic-level\nanalysis to shed light on the utility of different mechanisms that can\nimplement the same computational-level theory.",
          "link": "http://arxiv.org/abs/2012.03370",
          "publishedOn": "2021-07-28T02:02:30.787Z",
          "wordCount": 619,
          "title": "Competition in Cross-situational Word Learning: A Computational Study. (arXiv:2012.03370v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedrax_Weiss_T/0/1/0/all/0/1\">Tania Bedrax-Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>",
          "description": "A desirable property of a reference-based evaluation metric that measures the\ncontent quality of a summary is that it should estimate how much information\nthat summary has in common with a reference. Traditional text overlap based\nmetrics such as ROUGE fail to achieve this because they are limited to matching\ntokens, either lexically or via embeddings. In this work, we propose a metric\nto evaluate the content quality of a summary using question-answering (QA).\nQA-based methods directly measure a summary's information overlap with a\nreference, making them fundamentally different than text overlap metrics. We\ndemonstrate the experimental benefits of QA-based metrics through an analysis\nof our proposed metric, QAEval. QAEval out-performs current state-of-the-art\nmetrics on most evaluations using benchmark datasets, while being competitive\non others due to limitations of state-of-the-art models. Through a careful\nanalysis of each component of QAEval, we identify its performance bottlenecks\nand estimate that its potential upper-bound performance surpasses all other\nautomatic metrics, approaching that of the gold-standard Pyramid Method.",
          "link": "http://arxiv.org/abs/2010.00490",
          "publishedOn": "2021-07-28T02:02:30.780Z",
          "wordCount": 653,
          "title": "Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary. (arXiv:2010.00490v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>",
          "description": "In this paper we apply self-knowledge distillation to text summarization\nwhich we argue can alleviate problems with maximum-likelihood training on\nsingle reference and noisy datasets. Instead of relying on one-hot annotation\nlabels, our student summarization model is trained with guidance from a teacher\nwhich generates smoothed labels to help regularize training. Furthermore, to\nbetter model uncertainty during training, we introduce multiple noise signals\nfor both teacher and student models. We demonstrate experimentally on three\nbenchmarks that our framework boosts the performance of both pretrained and\nnon-pretrained summarizers achieving state-of-the-art results.",
          "link": "http://arxiv.org/abs/2009.07032",
          "publishedOn": "2021-07-28T02:02:30.742Z",
          "wordCount": 548,
          "title": "Noisy Self-Knowledge Distillation for Text Summarization. (arXiv:2009.07032v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang%7D_%7B/0/1/0/all/0/1\">{Bao Minh} {Doan Dang}</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberlander_L/0/1/0/all/0/1\">Laura Oberl&#xe4;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>",
          "description": "Emotion stimulus extraction is a fine-grained subtask of emotion analysis\nthat focuses on identifying the description of the cause behind an emotion\nexpression from a text passage (e.g., in the sentence \"I am happy that I passed\nmy exam\" the phrase \"passed my exam\" corresponds to the stimulus.). Previous\nwork mainly focused on Mandarin and English, with no resources or models for\nGerman. We fill this research gap by developing a corpus of 2006 German news\nheadlines annotated with emotions and 811 instances with annotations of\nstimulus phrases. Given that such corpus creation efforts are time-consuming\nand expensive, we additionally work on an approach for projecting the existing\nEnglish GoodNewsEveryone (GNE) corpus to a machine-translated German version.\nWe compare the performance of a conditional random field (CRF) model (trained\nmonolingually on German and cross-lingually via projection) with a multilingual\nXLM-RoBERTa (XLM-R) model. Our results show that training with the German\ncorpus achieves higher F1 scores than projection. Experiments with XLM-R\noutperform their respective CRF counterparts.",
          "link": "http://arxiv.org/abs/2107.12920",
          "publishedOn": "2021-07-28T02:02:30.731Z",
          "wordCount": 599,
          "title": "Emotion Stimulus Detection in German News Headlines. (arXiv:2107.12920v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Anne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talnikar_C/0/1/0/all/0/1\">Chaitanya Talnikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haziza_D/0/1/0/all/0/1\">Daniel Haziza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_M/0/1/0/all/0/1\">Mary Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>",
          "description": "We introduce VoxPopuli, a large-scale multilingual corpus providing 100K\nhours of unlabelled speech data in 23 languages. It is the largest open data to\ndate for unsupervised representation learning as well as semi-supervised\nlearning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16\nlanguages and their aligned oral interpretations into 5 other languages\ntotaling 5.1K hours. We provide speech recognition baselines and validate the\nversatility of VoxPopuli unlabelled data in semi-supervised learning under\nchallenging out-of-domain settings. We will release the corpus at\nhttps://github.com/facebookresearch/voxpopuli under an open license.",
          "link": "http://arxiv.org/abs/2101.00390",
          "publishedOn": "2021-07-28T02:02:30.716Z",
          "wordCount": 587,
          "title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation. (arXiv:2101.00390v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)",
          "description": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.",
          "link": "http://arxiv.org/abs/1907.02704",
          "publishedOn": "2021-07-28T02:02:30.707Z",
          "wordCount": 714,
          "title": "Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.07473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silveira_B/0/1/0/all/0/1\">B&#xe1;rbara Silveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1\">Henrique S. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1\">Fabricio Murai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Ana Paula Couto da Silva</a>",
          "description": "In recent years, Online Social Networks have become an important medium for\npeople who suffer from mental disorders to share moments of hardship, and\nreceive emotional and informational support. In this work, we analyze how\ndiscussions in Reddit communities related to mental disorders can help improve\nthe health conditions of their users. Using the emotional tone of users'\nwriting as a proxy for emotional state, we uncover relationships between user\ninteractions and state changes. First, we observe that authors of negative\nposts often write rosier comments after engaging in discussions, indicating\nthat users' emotional state can improve due to social support. Second, we build\nmodels based on SOTA text embedding techniques and RNNs to predict shifts in\nemotional tone. This differs from most of related work, which focuses primarily\non detecting mental disorders from user activity. We demonstrate the\nfeasibility of accurately predicting the users' reactions to the interactions\nexperienced in these platforms, and present some examples which illustrate that\nthe models are correctly capturing the effects of comments on the author's\nemotional tone. Our models hold promising implications for interventions to\nprovide support for people struggling with mental illnesses.",
          "link": "http://arxiv.org/abs/2005.07473",
          "publishedOn": "2021-07-28T02:02:30.649Z",
          "wordCount": 698,
          "title": "Predicting User Emotional Tone in Mental Disorder Online Communities. (arXiv:2005.07473v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barry_J/0/1/0/all/0/1\">James Barry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Joachim Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassidy_L/0/1/0/all/0/1\">Lauren Cassidy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowap_A/0/1/0/all/0/1\">Alan Cowap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynn_T/0/1/0/all/0/1\">Teresa Lynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walsh_A/0/1/0/all/0/1\">Abigail Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meachair_M/0/1/0/all/0/1\">M&#xed;che&#xe1;l J. &#xd3; Meachair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>",
          "description": "The BERT family of neural language models have become highly popular due to\ntheir ability to provide sequences of text with rich context-sensitive token\nencodings which are able to generalise well to many Natural Language Processing\ntasks. Over 120 monolingual BERT models covering over 50 languages have been\nreleased, as well as a multilingual model trained on 104 languages. We\nintroduce, gaBERT, a monolingual BERT model for the Irish language. We compare\nour gaBERT model to multilingual BERT and show that gaBERT provides better\nrepresentations for a downstream parsing task. We also show how different\nfiltering criteria, vocabulary size and the choice of subword tokenisation\nmodel affect downstream performance. We release gaBERT and related code to the\ncommunity.",
          "link": "http://arxiv.org/abs/2107.12930",
          "publishedOn": "2021-07-28T02:02:30.630Z",
          "wordCount": 555,
          "title": "gaBERT -- an Irish Language Model. (arXiv:2107.12930v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casel_F/0/1/0/all/0/1\">Felix Casel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heindl_A/0/1/0/all/0/1\">Amelie Heindl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>",
          "description": "Emotion classification in text is typically performed with neural network\nmodels which learn to associate linguistic units with emotions. While this\noften leads to good predictive performance, it does only help to a limited\ndegree to understand how emotions are communicated in various domains. The\nemotion component process model (CPM) by Scherer (2005) is an interesting\napproach to explain emotion communication. It states that emotions are a\ncoordinated process of various subcomponents, in reaction to an event, namely\nthe subjective feeling, the cognitive appraisal, the expression, a\nphysiological bodily reaction, and a motivational action tendency. We\nhypothesize that these components are associated with linguistic realizations:\nan emotion can be expressed by describing a physiological bodily reaction (\"he\nwas trembling\"), or the expression (\"she smiled\"), etc. We annotate existing\nliterature and Twitter emotion corpora with emotion component classes and find\nthat emotions on Twitter are predominantly expressed by event descriptions or\nsubjective reports of the feeling, while in literature, authors prefer to\ndescribe what characters do, and leave the interpretation to the reader. We\nfurther include the CPM in a multitask learning model and find that this\nsupports the emotion categorization. The annotated corpora are available at\nhttps://www.ims.uni-stuttgart.de/data/emotion.",
          "link": "http://arxiv.org/abs/2107.12895",
          "publishedOn": "2021-07-28T02:02:30.591Z",
          "wordCount": 645,
          "title": "Emotion Recognition under Consideration of the Emotion Component Process Model. (arXiv:2107.12895v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05299",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Correia_A/0/1/0/all/0/1\">A. D. Correia</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Moortgat_M/0/1/0/all/0/1\">M. Moortgat</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Stoof_H/0/1/0/all/0/1\">H. T. C. Stoof</a>",
          "description": "Grover's algorithm, a well-know quantum search algorithm, allows one to find\nthe correct item in a database, with quadratic speedup. In this paper we adapt\nGrover's algorithm to the problem of finding a correct answer to a natural\nlanguage question in English, thus contributing to the growing field of Quantum\nNatural Language Processing. Using a grammar that can be interpreted as tensor\ncontractions, each word is represented as a quantum state that serves as input\nto the quantum circuit. We here introduce a quantum measurement to contract the\nrepresentations of words, resulting in the representation of larger text\nfragments. Using this framework, a representation for the question is found\nthat contains all the possible answers in equal quantum superposition, and\nallows for the building of an oracle that can detect a correct answer, being\nagnostic to the specific question. Furthermore, we show that our construction\ncan deal with certain types of ambiguous phrases by keeping the various\ndifferent meanings in quantum superposition.",
          "link": "http://arxiv.org/abs/2106.05299",
          "publishedOn": "2021-07-28T02:02:30.531Z",
          "wordCount": 615,
          "title": "Grover's Algorithm for Question Answering. (arXiv:2106.05299v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eder_T/0/1/0/all/0/1\">Tobias Eder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hangya_V/0/1/0/all/0/1\">Viktor Hangya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>",
          "description": "Good quality monolingual word embeddings (MWEs) can be built for languages\nwhich have large amounts of unlabeled text. MWEs can be aligned to bilingual\nspaces using only a few thousand word translation pairs. For low resource\nlanguages training MWEs monolingually results in MWEs of poor quality, and thus\npoor bilingual word embeddings (BWEs) as well. This paper proposes a new\napproach for building BWEs in which the vector space of the high resource\nsource language is used as a starting point for training an embedding space for\nthe low resource target language. By using the source vectors as anchors the\nvector spaces are automatically aligned during training. We experiment on\nEnglish-German, English-Hiligaynon and English-Macedonian. We show that our\napproach results not only in improved BWEs and bilingual lexicon induction\nperformance, but also in improved target language MWE quality as measured using\nmonolingual word similarity.",
          "link": "http://arxiv.org/abs/2010.12627",
          "publishedOn": "2021-07-28T02:02:30.522Z",
          "wordCount": 624,
          "title": "Anchor-based Bilingual Word Embeddings for Low-Resource Languages. (arXiv:2010.12627v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1\">Stella Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Longxiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>",
          "description": "Federated Learning aims to learn machine learning models from multiple\ndecentralized edge devices (e.g. mobiles) or servers without sacrificing local\ndata privacy. Recent Natural Language Processing techniques rely on deep\nlearning and large pre-trained language models. However, both big deep neural\nand language models are trained with huge amounts of data which often lies on\nthe server side. Since text data is widely originated from end users, in this\nwork, we look into recent NLP models and techniques which use federated\nlearning as the learning framework. Our survey discusses major challenges in\nfederated natural language processing, including the algorithm challenges,\nsystem challenges as well as the privacy issues. We also provide a critical\nreview of the existing Federated NLP evaluation methods and tools. Finally, we\nhighlight the current research gaps and future directions.",
          "link": "http://arxiv.org/abs/2107.12603",
          "publishedOn": "2021-07-28T02:02:30.479Z",
          "wordCount": 584,
          "title": "Federated Learning Meets Natural Language Processing: A Survey. (arXiv:2107.12603v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jinyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuang_K/0/1/0/all/0/1\">Kai Shuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>",
          "description": "The goal of dialogue state tracking (DST) is to predict the current dialogue\nstate given all previous dialogue contexts. Existing approaches generally\npredict the dialogue state at every turn from scratch. However, the\noverwhelming majority of the slots in each turn should simply inherit the slot\nvalues from the previous turn. Therefore, the mechanism of treating slots\nequally in each turn not only is inefficient but also may lead to additional\nerrors because of the redundant slot value generation. To address this problem,\nwe devise the two-stage DSS-DST which consists of the Dual Slot Selector based\non the current turn dialogue, and the Slot Value Generator based on the\ndialogue history. The Dual Slot Selector determines each slot whether to update\nslot value or to inherit the slot value from the previous turn from two\naspects: (1) if there is a strong relationship between it and the current turn\ndialogue utterances; (2) if a slot value with high reliability can be obtained\nfor it through the current turn dialogue. The slots selected to be updated are\npermitted to enter the Slot Value Generator to update values by a hybrid\nmethod, while the other slots directly inherit the values from the previous\nturn. Empirical results show that our method achieves 56.93%, 60.73%, and\n58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets\nrespectively and achieves a new state-of-the-art performance with significant\nimprovements.",
          "link": "http://arxiv.org/abs/2107.12578",
          "publishedOn": "2021-07-28T02:02:30.433Z",
          "wordCount": 685,
          "title": "Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking. (arXiv:2107.12578v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yuchen Chai</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Palacios_J/0/1/0/all/0/1\">Juan Palacios</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianghao Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yichun Fan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a> (1) ((1) Massachusetts Institute of Technology, (2) Chinese Academy of Science)",
          "description": "COVID-19, as a global health crisis, has triggered the fear emotion with\nunprecedented intensity. Besides the fear of getting infected, the outbreak of\nCOVID-19 also created significant disruptions in people's daily life and thus\nevoked intensive psychological responses indirect to COVID-19 infections. Here,\nwe construct an expressed fear database using 16 million social media posts\ngenerated by 536 thousand users between January 1st, 2019 and August 31st, 2020\nin China. We employ deep learning techniques to detect the fear emotion within\neach post and apply topic models to extract the central fear topics. Based on\nthis database, we find that sleep disorders (\"nightmare\" and \"insomnia\") take\nup the largest share of fear-labeled posts in the pre-pandemic period (January\n2019-December 2019), and significantly increase during the COVID-19. We\nidentify health and work-related concerns are the two major sources of fear\ninduced by the COVID-19. We also detect gender differences, with females\ngenerating more posts containing the daily-life fear sources during the\nCOVID-19 period. This research adopts a data-driven approach to trace back\npublic emotion, which can be used to complement traditional surveys to achieve\nreal-time emotion monitoring to discern societal concerns and support policy\ndecision-making.",
          "link": "http://arxiv.org/abs/2107.12606",
          "publishedOn": "2021-07-28T02:02:30.394Z",
          "wordCount": 701,
          "title": "Measuring daily-life fear perception change: a computational study in the context of COVID-19. (arXiv:2107.12606v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" When viewing\nmice on the shelf, the number of buttons or presence of a wire may not be\nvisible from certain angles or positions. Flat images of candidate mice may not\nprovide the discriminative information needed for \"wireless\". The world, and\nobjects in it, are not flat images but complex 3D shapes. If a human requests\nan object based on any of its basic properties, such as color, shape, or\ntexture, robots should perform the necessary exploration to accomplish the\ntask. In particular, while substantial effort and progress has been made on\nunderstanding explicitly visual attributes like color and category,\ncomparatively little progress has been made on understanding language about\nshapes and contours. In this work, we introduce a novel reasoning task that\ntargets both visual and non-visual language about 3D objects. Our new\nbenchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a\nmodel to choose which of two objects is being referenced by a natural language\ndescription. We introduce several CLIP-based models for distinguishing objects\nand demonstrate that while recent advances in jointly modeling vision and\nlanguage are useful for robotic language understanding, it is still the case\nthat these models are weaker at understanding the 3D nature of objects --\nproperties which play a key role in manipulation. In particular, we find that\nadding view estimation to language grounding models improves accuracy on both\nSNARE and when identifying objects referred to in language on a robot platform.",
          "link": "http://arxiv.org/abs/2107.12514",
          "publishedOn": "2021-07-28T02:02:30.381Z",
          "wordCount": 709,
          "title": "Language Grounding with 3D Objects. (arXiv:2107.12514v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>",
          "description": "Alongside huge volumes of research on deep learning models in NLP in the\nrecent years, there has been also much work on benchmark datasets needed to\ntrack modeling progress. Question answering and reading comprehension have been\nparticularly prolific in this regard, with over 80 new datasets appearing in\nthe past two years. This study is the largest survey of the field to date. We\nprovide an overview of the various formats and domains of the current\nresources, highlighting the current lacunae for future work. We further discuss\nthe current classifications of ``reasoning types\" in question answering and\npropose a new taxonomy. We also discuss the implications of over-focusing on\nEnglish, and survey the current monolingual resources for other languages and\nmultilingual resources. The study is aimed at both practitioners looking for\npointers to the wealth of existing data, and at researchers working on new\nresources.",
          "link": "http://arxiv.org/abs/2107.12708",
          "publishedOn": "2021-07-28T02:02:30.344Z",
          "wordCount": 594,
          "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension. (arXiv:2107.12708v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yawen Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiasheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xinyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>",
          "description": "Unknown intent detection aims to identify the out-of-distribution (OOD)\nutterance whose intent has never appeared in the training set. In this paper,\nwe propose using energy scores for this task as the energy score is\ntheoretically aligned with the density of the input and can be derived from any\nclassifier. However, high-quality OOD utterances are required during the\ntraining stage in order to shape the energy gap between OOD and in-distribution\n(IND), and these utterances are difficult to collect in practice. To tackle\nthis problem, we propose a data manipulation framework to Generate high-quality\nOOD utterances with importance weighTs (GOT). Experimental results show that\nthe energy-based detector fine-tuned by GOT can achieve state-of-the-art\nresults on two benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.12542",
          "publishedOn": "2021-07-28T02:02:30.332Z",
          "wordCount": 572,
          "title": "Energy-based Unknown Intent Detection with Data Manipulation. (arXiv:2107.12542v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parnow_K/0/1/0/all/0/1\">Kevin Parnow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1\">Masao Utiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>",
          "description": "Though the pre-trained contextualized language model (PrLM) has made a\nsignificant impact on NLP, training PrLMs in languages other than English can\nbe impractical for two reasons: other languages often lack corpora sufficient\nfor training powerful PrLMs, and because of the commonalities among human\nlanguages, computationally expensive PrLM training for different languages is\nsomewhat redundant. In this work, building upon the recent works connecting\ncross-lingual model transferring and neural machine translation, we thus\npropose a novel cross-lingual model transferring framework for PrLMs: TreLM. To\nhandle the symbol order and sequence length differences between languages, we\npropose an intermediate ``TRILayer\" structure that learns from these\ndifferences and creates a better transfer in our primary translation direction,\nas well as a new cross-lingual language modeling objective for transfer\ntraining. Additionally, we showcase an embedding aligning that adversarially\nadapts a PrLM's non-contextualized embedding space and the TRILayer structure\nto learn a text transformation network across languages, which addresses the\nvocabulary difference between languages. Experiments on both language\nunderstanding and structure parsing tasks show the proposed framework\nsignificantly outperforms language models trained from scratch with limited\ndata in both performance and efficiency. Moreover, despite an insignificant\nperformance loss compared to pre-training from scratch in resource-rich\nscenarios, our cross-lingual model transferring framework is significantly more\neconomical.",
          "link": "http://arxiv.org/abs/2107.12627",
          "publishedOn": "2021-07-28T02:02:30.304Z",
          "wordCount": 647,
          "title": "Cross-lingual Transferring of Pre-trained Contextualized Language Models. (arXiv:2107.12627v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.",
          "link": "http://arxiv.org/abs/2107.12651",
          "publishedOn": "2021-07-28T02:02:30.284Z",
          "wordCount": 604,
          "title": "Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murdock_V/0/1/0/all/0/1\">Vanessa Murdock</a>",
          "description": "Online harassment in the form of hate speech has been on the rise in recent\nyears. Addressing the issue requires a combination of content moderation by\npeople, aided by automatic detection methods. As content moderation is itself\nharmful to the people doing it, we desire to reduce the burden by improving the\nautomatic detection of hate speech. Hate speech presents a challenge as it is\ndirected at different target groups using a completely different vocabulary.\nFurther the authors of the hate speech are incentivized to disguise their\nbehavior to avoid being removed from a platform. This makes it difficult to\ndevelop a comprehensive data set for training and evaluating hate speech\ndetection models because the examples that represent one hate speech domain do\nnot typically represent others, even within the same language or culture. We\npropose an unsupervised domain adaptation approach to augment labeled data for\nhate speech detection. We evaluate the approach with three different models\n(character CNNs, BiLSTMs and BERT) on three different collections. We show our\napproach improves Area under the Precision/Recall curve by as much as 42% and\nrecall by as much as 278%, with no loss (and in some cases a significant gain)\nin precision.",
          "link": "http://arxiv.org/abs/2107.12866",
          "publishedOn": "2021-07-28T02:02:30.266Z",
          "wordCount": 639,
          "title": "Unsupervised Domain Adaptation for Hate Speech Detection Using a Data Augmentation Approach. (arXiv:2107.12866v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehra_S/0/1/0/all/0/1\">Sunakshi Mehra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susan_S/0/1/0/all/0/1\">Seba Susan</a>",
          "description": "We introduce an unsupervised approach for correcting highly imperfect speech\ntranscriptions based on a decision-level fusion of stemming and two-way phoneme\npruning. Transcripts are acquired from videos by extracting audio using Ffmpeg\nframework and further converting audio to text transcript using Google API. In\nthe benchmark LRW dataset, there are 500 word categories, and 50 videos per\nclass in mp4 format. All videos consist of 29 frames (each 1.16 s long) and the\nword appears in the middle of the video. In our approach we tried to improve\nthe baseline accuracy from 9.34% by using stemming, phoneme extraction,\nfiltering and pruning. After applying the stemming algorithm to the text\ntranscript and evaluating the results, we achieved 23.34% accuracy in word\nrecognition. To convert words to phonemes we used the Carnegie Mellon\nUniversity (CMU) pronouncing dictionary that provides a phonetic mapping of\nEnglish words to their pronunciations. A two-way phoneme pruning is proposed\nthat comprises of the two non-sequential steps: 1) filtering and pruning the\nphonemes containing vowels and plosives 2) filtering and pruning the phonemes\ncontaining vowels and fricatives. After obtaining results of stemming and\ntwo-way phoneme pruning, we applied decision-level fusion and that led to an\nimprovement of word recognition rate upto 32.96%.",
          "link": "http://arxiv.org/abs/2107.12428",
          "publishedOn": "2021-07-28T02:02:30.231Z",
          "wordCount": 664,
          "title": "Improving Word Recognition in Speech Transcriptions by Decision-level Fusion of Stemming and Two-way Phoneme Pruning. (arXiv:2107.12428v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We introduce ParaBLEU, a paraphrase representation learning model and\nevaluation metric for text generation. Unlike previous approaches, ParaBLEU\nlearns to understand paraphrasis using generative conditioning as a pretraining\nobjective. ParaBLEU correlates more strongly with human judgements than\nexisting metrics, obtaining new state-of-the-art results on the 2017 WMT\nMetrics Shared Task. We show that our model is robust to data scarcity,\nexceeding previous state-of-the-art performance using only $50\\%$ of the\navailable training data and surpassing BLEU, ROUGE and METEOR with only $40$\nlabelled examples. Finally, we demonstrate that ParaBLEU can be used to\nconditionally generate novel paraphrases from a single demonstration, which we\nuse to confirm our hypothesis that it learns abstract, generalized paraphrase\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.08251",
          "publishedOn": "2021-07-27T02:03:35.510Z",
          "wordCount": 563,
          "title": "Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiehang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianhan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liping Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>",
          "description": "Recently, few certified defense methods have been developed to provably\nguarantee the robustness of a text classifier to adversarial synonym\nsubstitutions. However, all existing certified defense methods assume that the\ndefenders are informed of how the adversaries generate synonyms, which is not a\nrealistic scenario. In this paper, we propose a certifiably robust defense\nmethod by randomly masking a certain proportion of the words in an input text,\nin which the above unrealistic assumption is no longer necessary. The proposed\nmethod can defend against not only word substitution-based attacks, but also\ncharacter-level perturbations. We can certify the classifications of over 50%\ntexts to be robust to any perturbation of 5 words on AGNEWS, and 2 words on\nSST2 dataset. The experimental results show that our randomized smoothing\nmethod significantly outperforms recently proposed defense methods across\nmultiple datasets.",
          "link": "http://arxiv.org/abs/2105.03743",
          "publishedOn": "2021-07-27T02:03:32.948Z",
          "wordCount": 616,
          "title": "Certified Robustness to Text Adversarial Attacks by Randomized [MASK]. (arXiv:2105.03743v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.",
          "link": "http://arxiv.org/abs/2106.11342",
          "publishedOn": "2021-07-27T02:03:32.927Z",
          "wordCount": 596,
          "title": "Dive into Deep Learning. (arXiv:2106.11342v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shuang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Songge Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansur_M/0/1/0/all/0/1\">Mairgup Mansur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>",
          "description": "Recent years have seen significant advancement in text generation tasks with\nthe help of neural language models. However, there exists a challenging task:\ngenerating math problem text based on mathematical equations, which has made\nlittle progress so far. In this paper, we present a novel equation-to-problem\ntext generation model. In our model, 1) we propose a flexible scheme to\neffectively encode math equations, we then enhance the equation encoder by a\nVaritional Autoen-coder (VAE) 2) given a math equation, we perform topic\nselection, followed by which a dynamic topic memory mechanism is introduced to\nrestrict the topic distribution of the generator 3) to avoid commonsense\nviolation in traditional generation model, we pretrain word embedding with\nbackground knowledge graph (KG), and we link decoded words to related words in\nKG, targeted at injecting background knowledge into our model. We evaluate our\nmodel through both automatic metrices and human evaluation, experiments\ndemonstrate our model outperforms baseline and previous models in both accuracy\nand richness of generated problem text.",
          "link": "http://arxiv.org/abs/2012.07379",
          "publishedOn": "2021-07-27T02:03:32.630Z",
          "wordCount": 641,
          "title": "Generating Math Word Problems from Equations with Topic Controlling and Commonsense Enforcement. (arXiv:2012.07379v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1\">Andros Tjandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_D/0/1/0/all/0/1\">Diptanu Gon Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Frank Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kritika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1\">Assaf Sela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1\">Yatharth Saraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>",
          "description": "Language identification greatly impacts the success of downstream tasks such\nas automatic speech recognition. Recently, self-supervised speech\nrepresentations learned by wav2vec 2.0 have been shown to be very effective for\na range of speech tasks. We extend previous self-supervised work on language\nidentification by experimenting with pre-trained models which were learned on\nreal-world unconstrained speech in multiple languages and not just on English.\nWe show that models pre-trained on many languages perform better and enable\nlanguage identification systems that require very little labeled data to\nperform well. Results on a 25 languages setup show that with only 10 minutes of\nlabeled data per language, a cross-lingually pre-trained model can achieve over\n93% accuracy.",
          "link": "http://arxiv.org/abs/2107.04082",
          "publishedOn": "2021-07-27T02:03:32.622Z",
          "wordCount": 584,
          "title": "Improved Language Identification Through Cross-Lingual Self-Supervised Learning. (arXiv:2107.04082v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>",
          "description": "The Minimum Linear Arrangement problem (MLA) consists of finding a mapping\n$\\pi$ from vertices of a graph to distinct integers that minimizes\n$\\sum_{\\{u,v\\}\\in E}|\\pi(u) - \\pi(v)|$. In that setting, vertices are often\nassumed to lie on a horizontal line and edges are drawn as semicircles above\nsaid line. For trees, various algorithms are available to solve the problem in\npolynomial time in $n=|V|$. There exist variants of the MLA in which the\narrangements are constrained. Iordanskii, and later Hochberg and Stallmann\n(HS), put forward $O(n)$-time algorithms that solve the problem when\narrangements are constrained to be planar (also known as one-page book\nembeddings). We also consider linear arrangements of rooted trees that are\nconstrained to be projective (planar embeddings where the root is not covered\nby any edge). Gildea and Temperley (GT) sketched an algorithm for projective\narrangements which they claimed runs in $O(n)$ but did not provide any\njustification of its cost. In contrast, Park and Levy claimed that GT's\nalgorithm runs in $O(n \\log d_{max})$ where $d_{max}$ is the maximum degree but\ndid not provide sufficient detail. Here we correct an error in HS's algorithm\nfor the planar case, show its relationship with the projective case, and derive\nsimple algorithms for the projective and planar cases that run undoubtlessly in\n$O(n)$-time.",
          "link": "http://arxiv.org/abs/2102.03277",
          "publishedOn": "2021-07-27T02:03:32.594Z",
          "wordCount": 699,
          "title": "Minimum projective linearizations of trees in linear time. (arXiv:2102.03277v3 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1\">Jesse Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_K/0/1/0/all/0/1\">Karan Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>",
          "description": "Novel neural architectures, training strategies, and the availability of\nlarge-scale corpora haven been the driving force behind recent progress in\nabstractive text summarization. However, due to the black-box nature of neural\nmodels, uninformative evaluation metrics, and scarce tooling for model and data\nanalysis, the true performance and failure modes of summarization models remain\nlargely unknown. To address this limitation, we introduce SummVis, an\nopen-source tool for visualizing abstractive summaries that enables\nfine-grained analysis of the models, data, and evaluation metrics associated\nwith text summarization. Through its lexical and semantic visualizations, the\ntools offers an easy entry point for in-depth model prediction exploration\nacross important dimensions such as factual consistency or abstractiveness. The\ntool together with several pre-computed model outputs is available at\nhttps://github.com/robustness-gym/summvis.",
          "link": "http://arxiv.org/abs/2104.07605",
          "publishedOn": "2021-07-27T02:03:32.450Z",
          "wordCount": 603,
          "title": "SummVis: Interactive Visual Analysis of Models, Data, and Evaluation for Text Summarization. (arXiv:2104.07605v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamm_A/0/1/0/all/0/1\">Andreas Hamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odrowski_S/0/1/0/all/0/1\">Simon Odrowski</a> (German Aerospace Center DLR)",
          "description": "Network-based procedures for topic detection in huge text collections offer\nan intuitive alternative to probabilistic topic models. We present in detail a\nmethod that is especially designed with the requirements of domain experts in\nmind. Like similar methods, it employs community detection in term\nco-occurrence graphs, but it is enhanced by including a resolution parameter\nthat can be used for changing the targeted topic granularity. We also establish\na term ranking and use semantic word-embedding for presenting term communities\nin a way that facilitates their interpretation. We demonstrate the application\nof our method with a widely used corpus of general news articles and show the\nresults of detailed social-sciences expert evaluations of detected topics at\nvarious resolutions. A comparison with topics detected by Latent Dirichlet\nAllocation is also included. Finally, we discuss factors that influence topic\ninterpretation.",
          "link": "http://arxiv.org/abs/2103.13550",
          "publishedOn": "2021-07-27T02:03:32.422Z",
          "wordCount": 604,
          "title": "Term-community-based topic detection with variable resolution. (arXiv:2103.13550v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Igamberdiev_T/0/1/0/all/0/1\">Timour Igamberdiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>",
          "description": "Graph convolutional networks (GCNs) are a powerful architecture for\nrepresentation learning on documents that naturally occur as graphs, e.g.,\ncitation or social networks. However, sensitive personal information, such as\ndocuments with people's profiles or relationships as edges, are prone to\nprivacy leaks, as the trained model might reveal the original input. Although\ndifferential privacy (DP) offers a well-founded privacy-preserving framework,\nGCNs pose theoretical and practical challenges due to their training specifics.\nWe address these challenges by adapting differentially-private gradient-based\ntraining to GCNs and conduct experiments using two optimizers on five NLP\ndatasets in two languages. We propose a simple yet efficient method based on\nrandom graph splits that not only improves the baseline privacy bounds by a\nfactor of 2.7 while retaining competitive F1 scores, but also provides strong\nprivacy guarantees of epsilon = 1.0. We show that, under certain modeling\nchoices, privacy-preserving GCNs perform up to 90% of their non-private\nvariants, while formally guaranteeing strong privacy measures.",
          "link": "http://arxiv.org/abs/2102.09604",
          "publishedOn": "2021-07-27T02:03:32.404Z",
          "wordCount": 625,
          "title": "Privacy-Preserving Graph Convolutional Networks for Text Classification. (arXiv:2102.09604v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianlong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuiqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fang Chen</a>",
          "description": "The outbreak of the novel Coronavirus Disease 2019 (COVID-19) has caused\nunprecedented impacts to people's daily life around the world. Various measures\nand policies such as lockdown and social-distancing are implemented by\ngovernments to combat the disease during the pandemic period. These measures\nand policies as well as virus itself may cause different mental health issues\nto people such as depression, anxiety, sadness, etc. In this paper, we exploit\nthe massive text data posted by Twitter users to analyse the sentiment dynamics\nof people living in the state of New South Wales (NSW) in Australia during the\npandemic period. Different from the existing work that mostly focuses the\ncountry-level and static sentiment analysis, we analyse the sentiment dynamics\nat the fine-grained local government areas (LGAs). Based on the analysis of\naround 94 million tweets that posted by around 183 thousand users located at\ndifferent LGAs in NSW in five months, we found that people in NSW showed an\noverall positive sentimental polarity and the COVID-19 pandemic decreased the\noverall positive sentimental polarity during the pandemic period. The\nfine-grained analysis of sentiment in LGAs found that despite the dominant\npositive sentiment most of days during the study period, some LGAs experienced\nsignificant sentiment changes from positive to negative. This study also\nanalysed the sentimental dynamics delivered by the hot topics in Twitter such\nas government policies (e.g. the Australia's JobKeeper program, lockdown,\nsocial-distancing) as well as the focused social events (e.g. the Ruby Princess\nCruise). The results showed that the policies and events did affect people's\noverall sentiment, and they affected people's overall sentiment differently at\ndifferent stages.",
          "link": "http://arxiv.org/abs/2006.12185",
          "publishedOn": "2021-07-27T02:03:32.386Z",
          "wordCount": 815,
          "title": "Examination of Community Sentiment Dynamics due to COVID-19 Pandemic: A Case Study from A State in Australia. (arXiv:2006.12185v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Holla_N/0/1/0/all/0/1\">Nithin Holla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Pushkar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1\">Helen Yannakoudakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>",
          "description": "Lifelong learning requires models that can continuously learn from sequential\nstreams of data without suffering catastrophic forgetting due to shifts in data\ndistributions. Deep learning models have thrived in the non-sequential learning\nparadigm; however, when used to learn a sequence of tasks, they fail to retain\npast knowledge and learn incrementally. We propose a novel approach to lifelong\nlearning of language tasks based on meta-learning with sparse experience replay\nthat directly optimizes to prevent forgetting. We show that under the realistic\nsetting of performing a single pass on a stream of tasks and without any task\nidentifiers, our method obtains state-of-the-art results on lifelong text\nclassification and relation extraction. We analyze the effectiveness of our\napproach and further demonstrate its low computational and space complexity.",
          "link": "http://arxiv.org/abs/2009.04891",
          "publishedOn": "2021-07-27T02:03:32.377Z",
          "wordCount": 591,
          "title": "Meta-Learning with Sparse Experience Replay for Lifelong Language Learning. (arXiv:2009.04891v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.03855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1\">Salvatore Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynn_V/0/1/0/all/0/1\">Veronica Lynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Keshav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Farhan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matz_S/0/1/0/all/0/1\">Sandra Matz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>",
          "description": "Social media is increasingly used for large-scale population predictions,\nsuch as estimating community health statistics. However, social media users are\nnot typically a representative sample of the intended population -- a\n\"selection bias\". Within the social sciences, such a bias is typically\naddressed with restratification techniques, where observations are reweighted\naccording to how under- or over-sampled their socio-demographic groups are.\nYet, restratifaction is rarely evaluated for improving prediction. Across four\ntasks of predicting U.S. county population health statistics from Twitter, we\nfind standard restratification techniques provide no improvement and often\ndegrade prediction accuracies. The core reasons for this seems to be both\nshrunken estimates (reduced variance of model predicted values) and sparse\nestimates of each population's socio-demographics. We thus develop and evaluate\nthree methods to address these problems: estimator redistribution to account\nfor shrinking, and adaptive binning and informed smoothing to handle sparse\nsocio-demographic estimates. We show that each of these methods significantly\noutperforms the standard restratification approaches. Combining approaches, we\nfind substantial improvements over non-restratified models, yielding a 53.0%\nincrease in predictive accuracy (R^2) in the case of surveyed life\nsatisfaction, and a 17.8% average increase across all tasks.",
          "link": "http://arxiv.org/abs/1911.03855",
          "publishedOn": "2021-07-27T02:03:32.356Z",
          "wordCount": 686,
          "title": "Correcting Sociodemographic Selection Biases for Population Prediction from Social Media. (arXiv:1911.03855v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1\">Janntatul Tajrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naira Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>",
          "description": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.",
          "link": "http://arxiv.org/abs/2107.03844",
          "publishedOn": "2021-07-27T02:03:32.347Z",
          "wordCount": 717,
          "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md. Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Sumon Kanti Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tanhim Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1\">Sagor Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1\">Mehadi Hasan Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1\">Kabir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Azam Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1\">Stefan Decker</a>",
          "description": "The exponential growths of social media and micro-blogging sites not only\nprovide platforms for empowering freedom of expressions and individual voices,\nbut also enables people to express anti-social behavior like online harassment,\ncyberbullying, and hate speech. Numerous works have been proposed to utilize\ntextual data for social and anti-social behavior analysis, by predicting the\ncontexts mostly for highly-resourced languages like English. However, some\nlanguages are under-resourced, e.g., South Asian languages like Bengali, that\nlack computational resources for accurate natural language processing (NLP). In\nthis paper, we propose an explainable approach for hate speech detection from\nthe under-resourced Bengali language, which we called DeepHateExplainer.\nBengali texts are first comprehensively preprocessed, before classifying them\ninto political, personal, geopolitical, and religious hates using a neural\nensemble method of transformer-based neural architectures (i.e., monolingual\nBangla BERT-base, multilingual BERT-cased/uncased, and XLM-RoBERTa).\nImportant~(most and least) terms are then identified using sensitivity analysis\nand layer-wise relevance propagation~(LRP), before providing\nhuman-interpretable explanations. Finally, we compute comprehensiveness and\nsufficiency scores to measure the quality of explanations w.r.t faithfulness.\nEvaluations against machine learning~(linear and tree-based models) and neural\nnetworks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word embeddings) baselines\nyield F1-scores of 78%, 91%, 89%, and 84%, for political, personal,\ngeopolitical, and religious hates, respectively, outperforming both ML and DNN\nbaselines.",
          "link": "http://arxiv.org/abs/2012.14353",
          "publishedOn": "2021-07-27T02:03:32.339Z",
          "wordCount": 699,
          "title": "DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language. (arXiv:2012.14353v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gargi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_D/0/1/0/all/0/1\">Dhanajit Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1\">Ashutosh Modi</a>",
          "description": "In this paper, we propose a new framework for fine-grained emotion prediction\nin the text through emotion definition modeling. Our approach involves a\nmulti-task learning framework that models definitions of emotions as an\nauxiliary task while being trained on the primary task of emotion prediction.\nWe model definitions using masked language modeling and class definition\nprediction tasks. Our models outperform existing state-of-the-art for\nfine-grained emotion dataset GoEmotions. We further show that this trained\nmodel can be used for transfer learning on other benchmark datasets in emotion\nprediction with varying emotion label sets, domains, and sizes. The proposed\nmodels outperform the baselines on transfer learning experiments demonstrating\nthe generalization capability of the models.",
          "link": "http://arxiv.org/abs/2107.12135",
          "publishedOn": "2021-07-27T02:03:32.322Z",
          "wordCount": 554,
          "title": "Fine-Grained Emotion Prediction by Modeling Emotion Definitions. (arXiv:2107.12135v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yue Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>",
          "description": "Health literacy has emerged as a crucial factor in making appropriate health\ndecisions and ensuring treatment outcomes. However, medical jargon and the\ncomplex structure of professional language in this domain make health\ninformation especially hard to interpret. Thus, there is an urgent unmet need\nfor automated methods to enhance the accessibility of the biomedical literature\nto the general population. This problem can be framed as a type of translation\nproblem between the language of healthcare professionals, and that of the\ngeneral public. In this paper, we introduce the novel task of automated\ngeneration of lay language summaries of biomedical scientific reviews, and\nconstruct a dataset to support the development and evaluation of automated\nmethods through which to enhance the accessibility of the biomedical\nliterature. We conduct analyses of the various challenges in solving this task,\nincluding not only summarization of the key points but also explanation of\nbackground knowledge and simplification of professional language. We experiment\nwith state-of-the-art summarization models as well as several data augmentation\ntechniques, and evaluate their performance using both automated metrics and\nhuman assessment. Results indicate that automatically generated summaries\nproduced using contemporary neural architectures can achieve promising quality\nand readability as compared with reference summaries developed for the lay\npublic by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score\nof 13.30). We also discuss the limitations of the current attempt, providing\ninsights and directions for future work.",
          "link": "http://arxiv.org/abs/2012.12573",
          "publishedOn": "2021-07-27T02:03:32.315Z",
          "wordCount": 716,
          "title": "Automated Lay Language Summarization of Biomedical Scientific Reviews. (arXiv:2012.12573v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1\">Geeticka Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1\">Brian Tse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>",
          "description": "Recent years have seen many breakthroughs in natural language processing\n(NLP), transitioning it from a mostly theoretical field to one with many\nreal-world applications. Noting the rising number of applications of other\nmachine learning and AI techniques with pervasive societal impact, we\nanticipate the rising importance of developing NLP technologies for social\ngood. Inspired by theories in moral philosophy and global priorities research,\nwe aim to promote a guideline for social good in the context of NLP. We lay the\nfoundations via the moral philosophy definition of social good, propose a\nframework to evaluate the direct and indirect real-world impact of NLP tasks,\nand adopt the methodology of global priorities research to identify priority\ncauses for NLP research. Finally, we use our theoretical framework to provide\nsome practical guidelines for future NLP research for social good. Our data and\ncode are available at this http URL In\naddition, we curate a list of papers and resources on NLP for social good at\nhttps://github.com/zhijing-jin/NLP4SocialGood_Papers.",
          "link": "http://arxiv.org/abs/2106.02359",
          "publishedOn": "2021-07-27T02:03:32.306Z",
          "wordCount": 671,
          "title": "How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pagnoni_A/0/1/0/all/0/1\">Artidoro Pagnoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>",
          "description": "Modern summarization models generate highly fluent but often factually\nunreliable outputs. This motivated a surge of metrics attempting to measure the\nfactuality of automatically generated summaries. Due to the lack of common\nbenchmarks, these metrics cannot be compared. Moreover, all these methods treat\nfactuality as a binary concept and fail to provide deeper insights into the\nkinds of inconsistencies made by different systems. To address these\nlimitations, we devise a typology of factual errors and use it to collect human\nannotations of generated summaries from state-of-the-art summarization systems\nfor the CNN/DM and XSum datasets. Through these annotations, we identify the\nproportion of different categories of factual errors in various summarization\nmodels and benchmark factuality metrics, showing their correlation with human\njudgment as well as their specific strengths and weaknesses.",
          "link": "http://arxiv.org/abs/2104.13346",
          "publishedOn": "2021-07-27T02:03:32.298Z",
          "wordCount": 605,
          "title": "Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics. (arXiv:2104.13346v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thang M. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1\">Long Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Do state-of-the-art natural language understanding models care about word\norder - one of the most important characteristics of a sequence? Not always! We\nfound 75% to 90% of the correct predictions of BERT-based classifiers, trained\non many GLUE tasks, remain constant after input words are randomly shuffled.\nDespite BERT embeddings are famously contextual, the contribution of each\nindividual word to downstream tasks is almost unchanged even after the word's\ncontext is shuffled. BERT-based models are able to exploit superficial cues\n(e.g. the sentiment of keywords in sentiment analysis; or the word-wise\nsimilarity between sequence-pair inputs in natural language inference) to make\ncorrect decisions when tokens are arranged in random orders. Encouraging\nclassifiers to capture word order information improves the performance on most\nGLUE tasks, SQuAD 2.0 and out-of-samples. Our work suggests that many GLUE\ntasks are not challenging machines to understand the meaning of a sentence.",
          "link": "http://arxiv.org/abs/2012.15180",
          "publishedOn": "2021-07-27T02:03:32.291Z",
          "wordCount": 649,
          "title": "Out of Order: How Important Is The Sequential Order of Words in a Sentence in Natural Language Understanding Tasks?. (arXiv:2012.15180v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malysheva_A/0/1/0/all/0/1\">Anastasia Malysheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>",
          "description": "Narrative generation and analysis are still on the fringe of modern natural\nlanguage processing yet are crucial in a variety of applications. This paper\nproposes a feature extraction method for plot dynamics. We present a dataset\nthat consists of the plot descriptions for thirteen thousand TV shows alongside\nmeta-information on their genres and dynamic plots extracted from them. We\nvalidate the proposed tool for plot dynamics extraction and discuss possible\napplications of this method to the tasks of narrative analysis and generation.",
          "link": "http://arxiv.org/abs/2107.12226",
          "publishedOn": "2021-07-27T02:03:32.283Z",
          "wordCount": 518,
          "title": "DYPLODOC: Dynamic Plots for Document Classification. (arXiv:2107.12226v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2001.05297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cathcart_C/0/1/0/all/0/1\">Chundra Aroor Cathcart</a>",
          "description": "This paper addresses a series of complex and unresolved issues in the\nhistorical phonology of West Iranian languages. The West Iranian languages\n(Persian, Kurdish, Balochi, and other languages) display a high degree of\nnon-Lautgesetzlich behavior. Most of this irregularity is undoubtedly due to\nlanguage contact; we argue, however, that an oversimplified view of the\nprocesses at work has prevailed in the literature on West Iranian dialectology,\nwith specialists assuming that deviations from an expected outcome in a given\nnon-Persian language are due to lexical borrowing from some chronological stage\nof Persian. It is demonstrated that this qualitative approach yields at times\nproblematic conclusions stemming from the lack of explicit probabilistic\ninferences regarding the distribution of the data: Persian may not be the sole\ndonor language; additionally, borrowing at the lexical level is not always the\nmechanism that introduces irregularity. In many cases, the possibility that\nWest Iranian languages show different reflexes in different conditioning\nenvironments remains under-explored. We employ a novel Bayesian approach\ndesigned to overcome these problems and tease apart the different determinants\nof irregularity in patterns of West Iranian sound change. Our methodology\nallows us to provisionally resolve a number of outstanding questions in the\nliterature on West Iranian dialectology concerning the dialectal affiliation of\ncertain sound changes. We outline future directions for work of this sort.",
          "link": "http://arxiv.org/abs/2001.05297",
          "publishedOn": "2021-07-27T02:03:32.276Z",
          "wordCount": 692,
          "title": "Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process Approach to Linguistic Relationships. (arXiv:2001.05297v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1\">Yew Ken Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>",
          "description": "Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA\nwhich outputs triplets of an aspect target, its associated sentiment, and the\ncorresponding opinion term. Recent models perform the triplet extraction in an\nend-to-end manner but heavily rely on the interactions between each target word\nand opinion word. Thereby, they cannot perform well on targets and opinions\nwhich contain multiple words. Our proposed span-level approach explicitly\nconsiders the interaction between the whole spans of targets and opinions when\npredicting their sentiment relation. Thus, it can make predictions with the\nsemantics of whole spans, ensuring better sentiment consistency. To ease the\nhigh computational cost caused by span enumeration, we propose a dual-channel\nspan pruning strategy by incorporating supervision from the Aspect Term\nExtraction (ATE) and Opinion Term Extraction (OTE) tasks. This strategy not\nonly improves computational efficiency but also distinguishes the opinion and\ntarget spans more properly. Our framework simultaneously achieves strong\nperformance for the ASTE as well as ATE and OTE tasks. In particular, our\nanalysis shows that our span-level approach achieves more significant\nimprovements over the baselines on triplets with multi-word targets or\nopinions.",
          "link": "http://arxiv.org/abs/2107.12214",
          "publishedOn": "2021-07-27T02:03:32.266Z",
          "wordCount": 624,
          "title": "Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction. (arXiv:2107.12214v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1\">Gongbo Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronchen_P/0/1/0/all/0/1\">Philipp R&#xf6;nchen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nivre_J/0/1/0/all/0/1\">Joakim Nivre</a>",
          "description": "In this paper, we evaluate the translation of negation both automatically and\nmanually, in English--German (EN--DE) and English--Chinese (EN--ZH). We show\nthat the ability of neural machine translation (NMT) models to translate\nnegation has improved with deeper and more advanced networks, although the\nperformance varies between language pairs and translation directions. The\naccuracy of manual evaluation in EN-DE, DE-EN, EN-ZH, and ZH-EN is 95.7%,\n94.8%, 93.4%, and 91.7%, respectively. In addition, we show that\nunder-translation is the most significant error type in NMT, which contrasts\nwith the more diverse error profile previously observed for statistical machine\ntranslation. To better understand the root of the under-translation of\nnegation, we study the model's information flow and training data. While our\ninformation flow analysis does not reveal any deficiencies that could be used\nto detect or fix the under-translation of negation, we find that negation is\noften rephrased during training, which could make it more difficult for the\nmodel to learn a reliable link between source and target negation. We finally\nconduct intrinsic analysis and extrinsic probing tasks on negation, showing\nthat NMT models can distinguish negation and non-negation tokens very well and\nencode a lot of information about negation in hidden states but nevertheless\nleave room for improvement.",
          "link": "http://arxiv.org/abs/2107.12203",
          "publishedOn": "2021-07-27T02:03:32.239Z",
          "wordCount": 648,
          "title": "Revisiting Negation in Neural Machine Translation. (arXiv:2107.12203v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1\">Biao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Guorui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>",
          "description": "Recent advances in linguistic steganalysis have successively applied CNNs,\nRNNs, GNNs and other deep learning models for detecting secret information in\ngenerative texts. These methods tend to seek stronger feature extractors to\nachieve higher steganalysis effects. However, we have found through experiments\nthat there actually exists significant difference between automatically\ngenerated steganographic texts and carrier texts in terms of the conditional\nprobability distribution of individual words. Such kind of statistical\ndifference can be naturally captured by the language model used for generating\nsteganographic texts, which drives us to give the classifier a priori knowledge\nof the language model to enhance the steganalysis ability. To this end, we\npresent two methods to efficient linguistic steganalysis in this paper. One is\nto pre-train a language model based on RNN, and the other is to pre-train a\nsequence autoencoder. Experimental results show that the two methods have\ndifferent degrees of performance improvement when compared to the randomly\ninitialized RNN classifier, and the convergence speed is significantly\naccelerated. Moreover, our methods have achieved the best detection results.",
          "link": "http://arxiv.org/abs/2107.12168",
          "publishedOn": "2021-07-27T02:03:32.217Z",
          "wordCount": 615,
          "title": "Exploiting Language Model for Efficient Linguistic Steganalysis: An Empirical Study. (arXiv:2107.12168v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.10058",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Slavnov_S/0/1/0/all/0/1\">Sergey Slavnov</a>",
          "description": "We consider tensor grammars, which are an example of \\commutative\" grammars,\nbased on the classical (rather than intuitionistic) linear logic. They can be\nseen as a surface representation of abstract categorial grammars ACG in the\nsense that derivations of ACG translate to derivations of tensor grammars and\nthis translation is isomorphic on the level of string languages. The basic\ningredient are tensor terms, which can be seen as encoding and generalizing\nproof-nets. Using tensor terms makes the syntax extremely simple and a direct\ngeometric meaning becomes transparent. Then we address the problem of encoding\nnoncommutative operations in our setting. This turns out possible after\nenriching the system with new unary operators. The resulting system allows\nrepresenting both ACG and Lambek grammars as conservative fragments, while the\nformalism remains, as it seems to us, rather simple and intuitive.",
          "link": "http://arxiv.org/abs/2005.10058",
          "publishedOn": "2021-07-27T02:03:32.187Z",
          "wordCount": 621,
          "title": "On embedding Lambek calculus into commutative categorial grammars. (arXiv:2005.10058v3 [math.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>",
          "description": "In this paper, we present coreference resolution experiments with a newly\ncreated multilingual corpus CorefUD. We focus on the following languages:\nCzech, Russian, Polish, German, Spanish, and Catalan. In addition to\nmonolingual experiments, we combine the training data in multilingual\nexperiments and train two joined models -- for Slavic languages and for all the\nlanguages together. We rely on an end-to-end deep learning model that we\nslightly adapted for the CorefUD corpus. Our results show that we can profit\nfrom harmonized annotations, and using joined models helps significantly for\nthe languages with smaller training data.",
          "link": "http://arxiv.org/abs/2107.12088",
          "publishedOn": "2021-07-27T02:03:32.180Z",
          "wordCount": 520,
          "title": "Multilingual Coreference Resolution with Harmonized Annotations. (arXiv:2107.12088v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12487",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mardaoui_D/0/1/0/all/0/1\">Dina Mardaoui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>",
          "description": "Text data are increasingly handled in an automated fashion by machine\nlearning algorithms. But the models handling these data are not always\nwell-understood due to their complexity and are more and more often referred to\nas \"black-boxes.\" Interpretability methods aim to explain how these models\noperate. Among them, LIME has become one of the most popular in recent years.\nHowever, it comes without theoretical guarantees: even for simple models, we\nare not sure that LIME behaves accurately. In this paper, we provide a first\ntheoretical analysis of LIME for text data. As a consequence of our theoretical\nfindings, we show that LIME indeed provides meaningful explanations for simple\nmodels, namely decision trees and linear models.",
          "link": "http://arxiv.org/abs/2010.12487",
          "publishedOn": "2021-07-27T02:03:32.173Z",
          "wordCount": 574,
          "title": "An Analysis of LIME for Text Data. (arXiv:2010.12487v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Menglong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Lei Zhang</a>",
          "description": "Recently, several studies reported that dot-product selfattention (SA) may\nnot be indispensable to the state-of-theart Transformer models. Motivated by\nthe fact that dense synthesizer attention (DSA), which dispenses with dot\nproducts and pairwise interactions, achieved competitive results in many\nlanguage processing tasks, in this paper, we first propose a DSA-based speech\nrecognition, as an alternative to SA. To reduce the computational complexity\nand improve the performance, we further propose local DSA (LDSA) to restrict\nthe attention scope of DSA to a local range around the current central frame\nfor speech recognition. Finally, we combine LDSA with SA to extract the local\nand global information simultaneously. Experimental results on the Ai-shell1\nMandarine speech recognition corpus show that the proposed LDSA-Transformer\nachieves a character error rate (CER) of 6.49%, which is slightly better than\nthat of the SA-Transformer. Meanwhile, the LDSA-Transformer requires less\ncomputation than the SATransformer. The proposed combination method not only\nachieves a CER of 6.18%, which significantly outperforms the SA-Transformer,\nbut also has roughly the same number of parameters and computational complexity\nas the latter. The implementation of the multi-head LDSA is available at\nhttps://github.com/mlxu995/multihead-LDSA.",
          "link": "http://arxiv.org/abs/2010.12155",
          "publishedOn": "2021-07-27T02:03:32.165Z",
          "wordCount": 673,
          "title": "Transformer-based End-to-End Speech Recognition with Local Dense Synthesizer Attention. (arXiv:2010.12155v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>",
          "description": "When humans solve complex problems, they rarely come up with a decision\nright-away. Instead, they start with an intuitive decision, reflect upon it,\nspot mistakes, resolve contradictions and jump between different hypotheses.\nThus, they create a sequence of ideas and follow a train of thought that\nultimately reaches a conclusive decision. Contrary to this, today's neural\nclassification models are mostly trained to map an input to one single and\nfixed output. In this paper, we investigate how we can give models the\nopportunity of a second, third and $k$-th thought. We take inspiration from\nHegel's dialectics and propose a method that turns an existing classifier's\nclass prediction (such as the image class forest) into a sequence of\npredictions (such as forest $\\rightarrow$ tree $\\rightarrow$ mushroom).\nConcretely, we propose a correction module that is trained to estimate the\nmodel's correctness as well as an iterative prediction update based on the\nprediction's gradient. Our approach results in a dynamic system over class\nprobability distributions $\\unicode{x2014}$ the thought flow. We evaluate our\nmethod on diverse datasets and tasks from computer vision and natural language\nprocessing. We observe surprisingly complex but intuitive behavior and\ndemonstrate that our method (i) can correct misclassifications, (ii)\nstrengthens model performance, (iii) is robust to high levels of adversarial\nattacks, (iv) can increase accuracy up to 4% in a label-distribution-shift\nsetting and (iv) provides a tool for model interpretability that uncovers model\nknowledge which otherwise remains invisible in a single distribution\nprediction.",
          "link": "http://arxiv.org/abs/2107.12220",
          "publishedOn": "2021-07-27T02:03:32.146Z",
          "wordCount": 694,
          "title": "Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.02275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1\">Collin Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Critch_A/0/1/0/all/0/1\">Andrew Critch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "We show how to assess a language model's knowledge of basic concepts of\nmorality. We introduce the ETHICS dataset, a new benchmark that spans concepts\nin justice, well-being, duties, virtues, and commonsense morality. Models\npredict widespread moral judgments about diverse text scenarios. This requires\nconnecting physical and social world knowledge to value judgements, a\ncapability that may enable us to steer chatbot outputs or eventually regularize\nopen-ended reinforcement learning agents. With the ETHICS dataset, we find that\ncurrent language models have a promising but incomplete ability to predict\nbasic human ethical judgements. Our work shows that progress can be made on\nmachine ethics today, and it provides a steppingstone toward AI that is aligned\nwith human values.",
          "link": "http://arxiv.org/abs/2008.02275",
          "publishedOn": "2021-07-27T02:03:32.139Z",
          "wordCount": 634,
          "title": "Aligning AI With Shared Human Values. (arXiv:2008.02275v5 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>",
          "description": "We present CORA, a Cross-lingual Open-Retrieval Answer Generation model that\ncan answer questions across many languages even when language-specific\nannotated data or knowledge sources are unavailable. We introduce a new dense\npassage retrieval algorithm that is trained to retrieve documents across\nlanguages for a question. Combined with a multilingual autoregressive\ngeneration model, CORA answers directly in the target language without any\ntranslation or in-language retrieval modules as used in prior work. We propose\nan iterative training method that automatically extends annotated data\navailable only in high-resource languages to low-resource ones. Our results\nshow that CORA substantially outperforms the previous state of the art on\nmultilingual open question answering benchmarks across 26 languages, 9 of which\nare unseen during training. Our analyses show the significance of cross-lingual\nretrieval and generation in many languages, particularly under low-resource\nsettings.",
          "link": "http://arxiv.org/abs/2107.11976",
          "publishedOn": "2021-07-27T02:03:32.011Z",
          "wordCount": 588,
          "title": "One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval. (arXiv:2107.11976v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zikun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "Knowledge Graph (KG) and attention mechanism have been demonstrated effective\nin introducing and selecting useful information for weakly supervised methods.\nHowever, only qualitative analysis and ablation study are provided as evidence.\nIn this paper, we contribute a dataset and propose a paradigm to quantitatively\nevaluate the effect of attention and KG on bag-level relation extraction (RE).\nWe find that (1) higher attention accuracy may lead to worse performance as it\nmay harm the model's ability to extract entity mention features; (2) the\nperformance of attention is largely influenced by various noise distribution\npatterns, which is closely related to real-world datasets; (3) KG-enhanced\nattention indeed improves RE performance, while not through enhanced attention\nbut by incorporating entity prior; and (4) attention mechanism may exacerbate\nthe issue of insufficient training data. Based on these findings, we show that\na straightforward variant of RE model can achieve significant improvements (6%\nAUC on average) on two real-world datasets as compared with three\nstate-of-the-art baselines. Our codes and datasets are available at\nhttps://github.com/zig-kwin-hu/how-KG-ATT-help.",
          "link": "http://arxiv.org/abs/2107.12064",
          "publishedOn": "2021-07-27T02:03:32.003Z",
          "wordCount": 615,
          "title": "How Knowledge Graph and Attention Help? A Quantitative Analysis into Bag-level Relation Extraction. (arXiv:2107.12064v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1\">Bo-Hsiang Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yinpei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreyssig_F/0/1/0/all/0/1\">Florian Kreyssig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>",
          "description": "One of the difficulties in training dialogue systems is the lack of training\ndata. We explore the possibility of creating dialogue data through the\ninteraction between a dialogue system and a user simulator. Our goal is to\ndevelop a modelling framework that can incorporate new dialogue scenarios\nthrough self-play between the two agents. In this framework, we first pre-train\nthe two agents on a collection of source domain dialogues, which equips the\nagents to converse with each other via natural language. With further\nfine-tuning on a small amount of target domain data, the agents continue to\ninteract with the aim of improving their behaviors using reinforcement learning\nwith structured reward functions. In experiments on the MultiWOZ dataset, two\npractical transfer learning problems are investigated: 1) domain adaptation and\n2) single-to-multiple domain transfer. We demonstrate that the proposed\nframework is highly effective in bootstrapping the performance of the two\nagents in transfer learning. We also show that our method leads to improvements\nin dialogue system performance on complete datasets.",
          "link": "http://arxiv.org/abs/2107.11904",
          "publishedOn": "2021-07-27T02:03:31.996Z",
          "wordCount": 606,
          "title": "Transferable Dialogue Systems and User Simulators. (arXiv:2107.11904v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>",
          "description": "Although Graph Convolutional Networks (GCNs) have demonstrated their power in\nvarious applications, the graph convolutional layers, as the most important\ncomponent of GCN, are still using linear transformations and a simple pooling\nstep. In this paper, we propose a novel generalization of Factorized Bilinear\n(FB) layer to model the feature interactions in GCNs. FB performs two\nmatrix-vector multiplications, that is, the weight matrix is multiplied with\nthe outer product of the vector of hidden features from both sides. However,\nthe FB layer suffers from the quadratic number of coefficients, overfitting and\nthe spurious correlations due to correlations between channels of hidden\nrepresentations that violate the i.i.d. assumption. Thus, we propose a compact\nFB layer by defining a family of summarizing operators applied over the\nquadratic term. We analyze proposed pooling operators and motivate their use.\nOur experimental results on multiple datasets demonstrate that the GFB-GCN is\ncompetitive with other methods for text classification.",
          "link": "http://arxiv.org/abs/2107.11666",
          "publishedOn": "2021-07-27T02:03:31.965Z",
          "wordCount": 588,
          "title": "Graph Convolutional Network with Generalized Factorized Bilinear Aggregation. (arXiv:2107.11666v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayaparan_M/0/1/0/all/0/1\">Mokanarangan Thayaparan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>",
          "description": "Regenerating natural language explanations for science questions is a\nchallenging task for evaluating complex multi-hop and abductive inference\ncapabilities. In this setting, Transformers trained on human-annotated\nexplanations achieve state-of-the-art performance when adopted as cross-encoder\narchitectures. However, while much attention has been devoted to the quality of\nthe constructed explanations, the problem of performing abductive inference at\nscale is still under-studied. As intrinsically not scalable, the cross-encoder\narchitectural paradigm is not suitable for efficient multi-hop inference on\nmassive facts banks. To maximise both accuracy and inference time, we propose a\nhybrid abductive solver that autoregressively combines a dense bi-encoder with\na sparse model of explanatory power, computed leveraging explicit patterns in\nthe explanations. Our experiments demonstrate that the proposed framework can\nachieve performance comparable with the state-of-the-art cross-encoder while\nbeing $\\approx 50$ times faster and scalable to corpora of millions of facts.\nMoreover, we study the impact of the hybridisation on semantic drift and\nscience question answering without additional training, showing that it boosts\nthe quality of the explanations and contributes to improved downstream\ninference performance.",
          "link": "http://arxiv.org/abs/2107.11879",
          "publishedOn": "2021-07-27T02:03:31.958Z",
          "wordCount": 617,
          "title": "Hybrid Autoregressive Solver for Scalable Abductive Natural Language Inference. (arXiv:2107.11879v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bohong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>",
          "description": "Multi-hop reading comprehension (MHRC) requires not only to predict the\ncorrect answer span in the given passage, but also to provide a chain of\nsupporting evidences for reasoning interpretability. It is natural to model\nsuch a process into graph structure by understanding multi-hop reasoning as\njumping over entity nodes, which has made graph modelling dominant on this\ntask. Recently, there have been dissenting voices about whether graph modelling\nis indispensable due to the inconvenience of the graph building, however\nexisting state-of-the-art graph-free attempts suffer from huge performance gap\ncompared to graph-based ones. This work presents a novel graph-free alternative\nwhich firstly outperform all graph models on MHRC. In detail, we exploit a\nselect-to-guide (S2G) strategy to accurately retrieve evidence paragraphs in a\ncoarse-to-fine manner, incorporated with two novel attention mechanisms, which\nsurprisingly shows conforming to the nature of multi-hop reasoning. Our\ngraph-free model achieves significant and consistent performance gain over\nstrong baselines and the current new state-of-the-art on the MHRC benchmark,\nHotpotQA, among all the published works.",
          "link": "http://arxiv.org/abs/2107.11823",
          "publishedOn": "2021-07-27T02:03:31.951Z",
          "wordCount": 595,
          "title": "Graph-free Multi-hop Reading Comprehension: A Select-to-Guide Strategy. (arXiv:2107.11823v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>",
          "description": "Much research in recent years has focused on automatic article commenting.\nHowever, few of previous studies focus on the controllable generation of\ncomments. Besides, they tend to generate dull and commonplace comments, which\nfurther limits their practical application. In this paper, we make the first\nstep towards controllable generation of comments, by building a system that can\nexplicitly control the emotion of the generated comments. To achieve this, we\nassociate each kind of emotion category with an embedding and adopt a dynamic\nfusion mechanism to fuse this embedding into the decoder. A sentence-level\nemotion classifier is further employed to better guide the model to generate\ncomments expressing the desired emotion. To increase the diversity of the\ngenerated comments, we propose a hierarchical copy mechanism that allows our\nmodel to directly copy words from the input articles. We also propose a\nrestricted beam search (RBS) algorithm to increase intra-sentence diversity.\nExperimental results show that our model can generate informative and diverse\ncomments that express the desired emotions with high accuracy.",
          "link": "http://arxiv.org/abs/2107.11781",
          "publishedOn": "2021-07-27T02:03:31.944Z",
          "wordCount": 596,
          "title": "Towards Controlled and Diverse Generation of Article Comments. (arXiv:2107.11781v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1\">Bettina Fazzinga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>",
          "description": "Dialogue systems are widely used in AI to support timely and interactive\ncommunication with users. We propose a general-purpose dialogue system\narchitecture that leverages computational argumentation and state-of-the-art\nlanguage technologies. We illustrate and evaluate the system using a COVID-19\nvaccine information case study.",
          "link": "http://arxiv.org/abs/2107.12079",
          "publishedOn": "2021-07-27T02:03:31.937Z",
          "wordCount": 541,
          "title": "An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>",
          "description": "In this work, we examine the ability of NER models to use contextual\ninformation when predicting the type of an ambiguous entity. We introduce NRB,\na new testbed carefully designed to diagnose Name Regularity Bias of NER\nmodels. Our results indicate that all state-of-the-art models we tested show\nsuch a bias; BERT fine-tuned models significantly outperforming feature-based\n(LSTM-CRF) ones on NRB, despite having comparable (sometimes lower) performance\non standard benchmarks.\n\nTo mitigate this bias, we propose a novel model-agnostic training method that\nadds learnable adversarial noise to some entity mentions, thus enforcing models\nto focus more strongly on the contextual signal, leading to significant gains\non NRB. Combining it with two other training strategies, data augmentation and\nparameter freezing, leads to further gains.",
          "link": "http://arxiv.org/abs/2107.11610",
          "publishedOn": "2021-07-27T02:03:31.916Z",
          "wordCount": 599,
          "title": "Context-aware Adversarial Training for Name Regularity Bias in Named Entity Recognition. (arXiv:2107.11610v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>",
          "description": "Recently, researchers have explored using the encoder-decoder framework to\ntackle dialogue state tracking (DST), which is a key component of task-oriented\ndialogue systems. However, they regard a multi-turn dialogue as a flat\nsequence, failing to focus on useful information when the sequence is long. In\nthis paper, we propose a Hierarchical Dynamic Copy Network (HDCN) to facilitate\nfocusing on the most informative turn, making it easier to extract slot values\nfrom the dialogue context. Based on the encoder-decoder framework, we adopt a\nhierarchical copy approach that calculates two levels of attention at the word-\nand turn-level, which are then renormalized to obtain the final copy\ndistribution. A focus loss term is employed to encourage the model to assign\nthe highest turn-level attention weight to the most informative turn.\nExperimental results show that our model achieves 46.76% joint accuracy on the\nMultiWOZ 2.1 dataset.",
          "link": "http://arxiv.org/abs/2107.11778",
          "publishedOn": "2021-07-27T02:03:31.909Z",
          "wordCount": 580,
          "title": "Learn to Focus: Hierarchical Dynamic Copy Network for Dialogue State Tracking. (arXiv:2107.11778v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin-Chun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shaoming Song</a>",
          "description": "Automatically mining sentiment tendency contained in natural language is a\nfundamental research to some artificial intelligent applications, where\nsolutions alternate with challenges. Transfer learning and multi-task learning\ntechniques have been leveraged to mitigate the supervision sparsity and\ncollaborate multiple heterogeneous domains correspondingly. Recent years, the\nsensitive nature of users' private data raises another challenge for sentiment\nclassification, i.e., data privacy protection. In this paper, we resort to\nfederated learning for multiple domain sentiment classification under the\nconstraint that the corpora must be stored on decentralized devices. In view of\nthe heterogeneous semantics across multiple parties and the peculiarities of\nword embedding, we pertinently provide corresponding solutions. First, we\npropose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework for\nbetter model aggregation and personalization in federated sentiment\nclassification. Second, we propose KTEPS$^\\star$ with the consideration of the\nrich semantic and huge embedding size properties of word vectors, utilizing\nProjection-based Dimension Reduction (PDR) methods for privacy protection and\nefficient transmission simultaneously. We propose two federated sentiment\nclassification scenes based on public benchmarks, and verify the superiorities\nof our proposed methods with abundant experimental investigations.",
          "link": "http://arxiv.org/abs/2107.11956",
          "publishedOn": "2021-07-27T02:03:31.902Z",
          "wordCount": 615,
          "title": "Preliminary Steps Towards Federated Sentiment Classification. (arXiv:2107.11956v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Vladimir Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1\">Andr&#xe9;s Carvallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aspillaga_C/0/1/0/all/0/1\">Carlos Aspillaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_C/0/1/0/all/0/1\">Camilo Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1\">Denis Parra</a>",
          "description": "The success of pretrained word embeddings has motivated their use in the\nbiomedical domain, with contextualized embeddings yielding remarkable results\nin several biomedical NLP tasks. However, there is a lack of research on\nquantifying their behavior under severe \"stress\" scenarios. In this work, we\nsystematically evaluate three language models with adversarial examples --\nautomatically constructed tests that allow us to examine how robust the models\nare. We propose two types of stress scenarios focused on the biomedical named\nentity recognition (NER) task, one inspired by spelling errors and another\nbased on the use of synonyms for medical terms. Our experiments with three\nbenchmarks show that the performance of the original models decreases\nconsiderably, in addition to revealing their weaknesses and strengths. Finally,\nwe show that adversarial training causes the models to improve their robustness\nand even to exceed the original performance in some cases.",
          "link": "http://arxiv.org/abs/2107.11652",
          "publishedOn": "2021-07-27T02:03:31.894Z",
          "wordCount": 586,
          "title": "Stress Test Evaluation of Biomedical Word Embeddings. (arXiv:2107.11652v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenhai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>",
          "description": "We describe an efficient hierarchical method to compute attention in the\nTransformer architecture. The proposed attention mechanism exploits a matrix\nstructure similar to the Hierarchical Matrix (H-Matrix) developed by the\nnumerical analysis community, and has linear run time and memory complexity. We\nperform extensive experiments to show that the inductive bias embodied by our\nhierarchical attention is effective in capturing the hierarchical structure in\nthe sequences typical for natural language and vision tasks. Our method is\nsuperior to alternative sub-quadratic proposals by over +6 points on average on\nthe Long Range Arena benchmark. It also sets a new SOTA test perplexity on\nOne-Billion Word dataset with 5x fewer model parameters than that of the\nprevious-best Transformer-based models.",
          "link": "http://arxiv.org/abs/2107.11906",
          "publishedOn": "2021-07-27T02:03:31.887Z",
          "wordCount": 552,
          "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences. (arXiv:2107.11906v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Ayush Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kagi_S/0/1/0/all/0/1\">Sammed S Kagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1\">Vivek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "Code-mixing is a phenomenon of mixing words and phrases from two or more\nlanguages in a single utterance of speech and text. Due to the high linguistic\ndiversity, code-mixing presents several challenges in evaluating standard\nnatural language generation (NLG) tasks. Various widely popular metrics perform\npoorly with the code-mixed NLG tasks. To address this challenge, we present a\nmetric independent evaluation pipeline MIPE that significantly improves the\ncorrelation between evaluation metrics and human judgments on the generated\ncode-mixed text. As a use case, we demonstrate the performance of MIPE on the\nmachine-generated Hinglish (code-mixing of Hindi and English languages)\nsentences from the HinGE corpus. We can extend the proposed evaluation strategy\nto other code-mixed language pairs, NLG tasks, and evaluation metrics with\nminimal to no effort.",
          "link": "http://arxiv.org/abs/2107.11534",
          "publishedOn": "2021-07-27T02:03:31.838Z",
          "wordCount": 562,
          "title": "MIPE: A Metric Independent Pipeline for Effective Code-Mixed NLG Evaluation. (arXiv:2107.11534v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gris_L/0/1/0/all/0/1\">Lucas Rafael Stefanel Gris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1\">Frederico Santos de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1\">Anderson da Silva Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>",
          "description": "Deep learning techniques have been shown to be efficient in various tasks,\nespecially in the development of speech recognition systems, that is, systems\nthat aim to transcribe a sentence in audio in a sequence of words. Despite the\nprogress in the area, speech recognition can still be considered difficult,\nespecially for languages lacking available data, as Brazilian Portuguese. In\nthis sense, this work presents the development of an public Automatic Speech\nRecognition system using only open available audio data, from the fine-tuning\nof the Wav2vec 2.0 XLSR-53 model pre-trained in many languages over Brazilian\nPortuguese data. The final model presents a Word Error Rate of 11.95% (Common\nVoice Dataset). This corresponds to 13% less than the best open Automatic\nSpeech Recognition model for Brazilian Portuguese available according to our\nbest knowledge, which is a promising result for the language. In general, this\nwork validates the use of self-supervising learning techniques, in special, the\nuse of the Wav2vec 2.0 architecture in the development of robust systems, even\nfor languages having few available data.",
          "link": "http://arxiv.org/abs/2107.11414",
          "publishedOn": "2021-07-27T02:03:31.830Z",
          "wordCount": 611,
          "title": "Brazilian Portuguese Speech Recognition Using Wav2vec 2.0. (arXiv:2107.11414v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sougata Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Souvik Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_R/0/1/0/all/0/1\">Rohini Srihari</a>",
          "description": "Generative neural conversational systems are generally trained with the\nobjective of minimizing the entropy loss between the training \"hard\" targets\nand the predicted logits. Often, performance gains and improved generalization\ncan be achieved by using regularization techniques like label smoothing, which\nconverts the training \"hard\" targets to \"soft\" targets. However, label\nsmoothing enforces a data independent uniform distribution on the incorrect\ntraining targets, which leads to an incorrect assumption of equi-probable\nincorrect targets for each correct target. In this paper we propose and\nexperiment with incorporating data dependent word similarity based weighing\nmethods to transforms the uniform distribution of the incorrect target\nprobabilities in label smoothing, to a more natural distribution based on\nsemantics. We introduce hyperparameters to control the incorrect target\ndistribution, and report significant performance gains over networks trained\nusing standard label smoothing based loss, on two standard open domain dialogue\ncorpora.",
          "link": "http://arxiv.org/abs/2107.11481",
          "publishedOn": "2021-07-27T02:03:31.811Z",
          "wordCount": 575,
          "title": "Similarity Based Label Smoothing For Dialogue Generation. (arXiv:2107.11481v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Troles_J/0/1/0/all/0/1\">Jonas-Dario Troles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1\">Ute Schmid</a>",
          "description": "Human gender bias is reflected in language and text production. Because\nstate-of-the-art machine translation (MT) systems are trained on large corpora\nof text, mostly generated by humans, gender bias can also be found in MT. For\ninstance when occupations are translated from a language like English, which\nmostly uses gender neutral words, to a language like German, which mostly uses\na feminine and a masculine version for an occupation, a decision must be made\nby the MT System. Recent research showed that MT systems are biased towards\nstereotypical translation of occupations. In 2019 the first, and so far only,\nchallenge set, explicitly designed to measure the extent of gender bias in MT\nsystems has been published. In this set measurement of gender bias is solely\nbased on the translation of occupations. In this paper we present an extension\nof this challenge set, called WiBeMT, with gender-biased adjectives and adds\nsentences with gender-biased verbs. The resulting challenge set consists of\nover 70, 000 sentences and has been translated with three commercial MT\nsystems: DeepL Translator, Microsoft Translator, and Google Translate. Results\nshow a gender bias for all three MT systems. This gender bias is to a great\nextent significantly influenced by adjectives and to a lesser extent by verbs.",
          "link": "http://arxiv.org/abs/2107.11584",
          "publishedOn": "2021-07-27T02:03:31.780Z",
          "wordCount": 656,
          "title": "Extending Challenge Sets to Uncover Gender Bias in Machine Translation: Impact of Stereotypical Verbs and Adjectives. (arXiv:2107.11584v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1\">Lea Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sertolli_B/0/1/0/all/0/1\">Benjamin Sertolli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weigel_B/0/1/0/all/0/1\">Benjamin Weigel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>",
          "description": "We introduce the MuSe-Toolbox - a Python-based open-source toolkit for\ncreating a variety of continuous and discrete emotion gold standards. In a\nsingle framework, we unify a wide range of fusion methods and propose the novel\nRater Aligned Annotation Weighting (RAAW), which aligns the annotations in a\ntranslation-invariant way before weighting and fusing them based on the\ninter-rater agreements between the annotations. Furthermore, discrete\ncategories tend to be easier for humans to interpret than continuous signals.\nWith this in mind, the MuSe-Toolbox provides the functionality to run\nexhaustive searches for meaningful class clusters in the continuous gold\nstandards. To our knowledge, this is the first toolkit that provides a wide\nselection of state-of-the-art emotional gold standard methods and their\ntransformation to discrete classes. Experimental results indicate that\nMuSe-Toolbox can provide promising and novel class formations which can be\nbetter predicted than hard-coded classes boundaries with minimal human\nintervention. The implementation (1) is out-of-the-box available with all\ndependencies using a Docker container (2).",
          "link": "http://arxiv.org/abs/2107.11757",
          "publishedOn": "2021-07-27T02:03:31.704Z",
          "wordCount": 620,
          "title": "MuSe-Toolbox: The Multimodal Sentiment Analysis Continuous Annotation Fusion and Discrete Class Transformation Toolbox. (arXiv:2107.11757v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>",
          "description": "Spoken Language Understanding (SLU) is composed of two subtasks: intent\ndetection (ID) and slot filling (SF). There are two lines of research on SLU.\nOne jointly tackles these two subtasks to improve their prediction accuracy,\nand the other focuses on the domain-adaptation ability of one of the subtasks.\nIn this paper, we attempt to bridge these two lines of research and propose a\njoint and domain adaptive approach to SLU. We formulate SLU as a constrained\ngeneration task and utilize a dynamic vocabulary based on domain-specific\nontology. We conduct experiments on the ASMixed and MTOD datasets and achieve\ncompetitive performance with previous state-of-the-art joint models. Besides,\nresults show that our joint model can be effectively adapted to a new domain.",
          "link": "http://arxiv.org/abs/2107.11768",
          "publishedOn": "2021-07-27T02:03:31.666Z",
          "wordCount": 561,
          "title": "A Joint and Domain-Adaptive Approach to Spoken Language Understanding. (arXiv:2107.11768v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David R. Mortensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>",
          "description": "Building language-universal speech recognition systems entails producing\nphonological units of spoken sound that can be shared across languages. While\nspeech annotations at the language-specific phoneme or surface levels are\nreadily available, annotations at a universal phone level are relatively rare\nand difficult to produce. In this work, we present a general framework to\nderive phone-level supervision from only phonemic transcriptions and\nphone-to-phoneme mappings with learnable weights represented using weighted\nfinite-state transducers, which we call differentiable allophone graphs. By\ntraining multilingually, we build a universal phone-based speech recognition\nmodel with interpretable probabilistic phone-to-phoneme mappings for each\nlanguage. These phone-based systems with learned allophone graphs can be used\nby linguists to document new languages, build phone-based lexicons that capture\nrich pronunciation variations, and re-evaluate the allophone mappings of seen\nlanguage. We demonstrate the aforementioned benefits of our proposed framework\nwith a system trained on 7 diverse languages.",
          "link": "http://arxiv.org/abs/2107.11628",
          "publishedOn": "2021-07-27T02:03:31.588Z",
          "wordCount": 600,
          "title": "Differentiable Allophone Graphs for Language-Universal Speech Recognition. (arXiv:2107.11628v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolanos_L/0/1/0/all/0/1\">Luis Bolanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanwar_A/0/1/0/all/0/1\">Ashwani Tanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokol_A/0/1/0/all/0/1\">Albert Sokol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vibhor Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>",
          "description": "Clinical notes contain information not present elsewhere, including drug\nresponse and symptoms, all of which are highly important when predicting key\noutcomes in acute care patients. We propose the automatic annotation of\nphenotypes from clinical notes as a method to capture essential information to\npredict outcomes in the Intensive Care Unit (ICU). This information is\ncomplementary to typically used vital signs and laboratory test results. We\ndemonstrate and validate our approach conducting experiments on the prediction\nof in-hospital mortality, physiological decompensation and length of stay in\nthe ICU setting for over 24,000 patients. The prediction models incorporating\nphenotypic information consistently outperform the baseline models leveraging\nonly vital signs and laboratory test results. Moreover, we conduct a thorough\ninterpretability study, showing that phenotypes provide valuable insights at\nthe patient and cohort levels. Our approach illustrates the viability of using\nphenotypes to determine outcomes in the ICU.",
          "link": "http://arxiv.org/abs/2107.11665",
          "publishedOn": "2021-07-27T02:03:31.498Z",
          "wordCount": 598,
          "title": "Clinical Utility of the Automatic Phenotype Annotation in Unstructured Clinical Notes: ICU Use Cases. (arXiv:2107.11665v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "This paper describes the University of Sydney& JD's joint submission of the\nIWSLT 2021 low resource speech translation task. We participated in the\nSwahili-English direction and got the best scareBLEU (25.3) score among all the\nparticipants. Our constrained system is based on a pipeline framework, i.e. ASR\nand NMT. We trained our models with the officially provided ASR and MT\ndatasets. The ASR system is based on the open-sourced tool Kaldi and this work\nmainly explores how to make the most of the NMT models. To reduce the\npunctuation errors generated by the ASR model, we employ our previous work\nSlotRefine to train a punctuation correction model. To achieve better\ntranslation performance, we explored the most recent effective strategies,\nincluding back translation, knowledge distillation, multi-feature reranking and\ntransductive finetuning. For model structure, we tried auto-regressive and\nnon-autoregressive models, respectively. In addition, we proposed two novel\npre-train approaches, i.e. \\textit{de-noising training} and\n\\textit{bidirectional training} to fully exploit the data. Extensive\nexperiments show that adding the above techniques consistently improves the\nBLEU scores, and the final submission system outperforms the baseline\n(Transformer ensemble model trained with the original parallel data) by\napproximately 10.8 BLEU score, achieving the SOTA performance.",
          "link": "http://arxiv.org/abs/2107.11572",
          "publishedOn": "2021-07-27T02:03:31.422Z",
          "wordCount": 644,
          "title": "The USYD-JD Speech Translation System for IWSLT 2021. (arXiv:2107.11572v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Harbi_O/0/1/0/all/0/1\">Omar Al-Harbi</a>",
          "description": "One crucial aspect of sentiment analysis is negation handling, where the\noccurrence of negation can flip the sentiment of a sentence and negatively\naffects the machine learning-based sentiment classification. The role of\nnegation in Arabic sentiment analysis has been explored only to a limited\nextent, especially for colloquial Arabic. In this paper, the author addresses\nthe negation problem of machine learning-based sentiment classification for a\ncolloquial Arabic language. To this end, we propose a simple rule-based\nalgorithm for handling the problem; the rules were crafted based on observing\nmany cases of negation. Additionally, simple linguistic knowledge and sentiment\nlexicon are used for this purpose. The author also examines the impact of the\nproposed algorithm on the performance of different machine learning algorithms.\nThe results given by the proposed algorithm are compared with three baseline\nmodels. The experimental results show that there is a positive impact on the\nclassifiers accuracy, precision and recall when the proposed algorithm is used\ncompared to the baselines.",
          "link": "http://arxiv.org/abs/2107.11597",
          "publishedOn": "2021-07-27T02:03:31.284Z",
          "wordCount": 608,
          "title": "Negation Handling in Machine Learning-Based Sentiment Classification for Colloquial Arabic. (arXiv:2107.11597v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.13617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lordelo_C/0/1/0/all/0/1\">Carlos Lordelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1\">Emmanouil Benetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1\">Simon Dixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1\">Sven Ahlb&#xe4;ck</a>",
          "description": "This paper proposes a deep convolutional neural network for performing\nnote-level instrument assignment. Given a polyphonic multi-instrumental music\nsignal along with its ground truth or predicted notes, the objective is to\nassign an instrumental source for each note. This problem is addressed as a\npitch-informed classification task where each note is analysed individually. We\nalso propose to utilise several kernel shapes in the convolutional layers in\norder to facilitate learning of efficient timbre-discriminative feature maps.\nExperiments on the MusicNet dataset using 7 instrument classes show that our\napproach is able to achieve an average F-score of 0.904 when the original\nmulti-pitch annotations are used as the pitch information for the system, and\nthat it also excels if the note information is provided using third-party\nmulti-pitch estimation algorithms. We also include ablation studies\ninvestigating the effects of the use of multiple kernel shapes and comparing\ndifferent input representations for the audio and the note-related information.",
          "link": "http://arxiv.org/abs/2107.13617",
          "publishedOn": "2021-07-30T02:13:27.230Z",
          "wordCount": 625,
          "title": "Pitch-Informed Instrument Assignment Using a Deep Convolutional Network with Multiple Kernel Shapes. (arXiv:2107.13617v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elwood_A/0/1/0/all/0/1\">Adam Elwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasparin_A/0/1/0/all/0/1\">Alberto Gasparin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozza_A/0/1/0/all/0/1\">Alessandro Rozza</a>",
          "description": "With the rise in use of social media to promote branded products, the demand\nfor effective influencer marketing has increased. Brands are looking for\nimproved ways to identify valuable influencers among a vast catalogue; this is\neven more challenging with \"micro-influencers\", which are more affordable than\nmainstream ones but difficult to discover. In this paper, we propose a novel\nmulti-task learning framework to improve the state of the art in\nmicro-influencer ranking based on multimedia content. Moreover, since the\nvisual congruence between a brand and influencer has been shown to be good\nmeasure of compatibility, we provide an effective visual method for\ninterpreting our models' decisions, which can also be used to inform brands'\nmedia strategies. We compare with the current state-of-the-art on a recently\nconstructed public dataset and we show significant improvement both in terms of\naccuracy and model complexity. The techniques for ranking and interpretation\npresented in this work can be generalised to arbitrary multimedia ranking tasks\nthat have datasets with a similar structure.",
          "link": "http://arxiv.org/abs/2107.13943",
          "publishedOn": "2021-07-30T02:13:27.216Z",
          "wordCount": 599,
          "title": "Ranking Micro-Influencers: a Novel Multi-Task Learning and Interpretable Framework. (arXiv:2107.13943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1\">Yashar Deldjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merra_F/0/1/0/all/0/1\">Felice Antonio Merra</a>",
          "description": "Recommender systems (RSs) employ user-item feedback, e.g., ratings, to match\ncustomers to personalized lists of products. Approaches to top-k recommendation\nmainly rely on Learning-To-Rank algorithms and, among them, the most widely\nadopted is Bayesian Personalized Ranking (BPR), which bases on a pair-wise\noptimization approach. Recently, BPR has been found vulnerable against\nadversarial perturbations of its model parameters. Adversarial Personalized\nRanking (APR) mitigates this issue by robustifying BPR via an adversarial\ntraining procedure. The empirical improvements of APR's accuracy performance on\nBPR have led to its wide use in several recommender models. However, a key\noverlooked aspect has been the beyond-accuracy performance of APR, i.e.,\nnovelty, coverage, and amplification of popularity bias, considering that\nrecent results suggest that BPR, the building block of APR, is sensitive to the\nintensification of biases and reduction of recommendation novelty. In this\nwork, we model the learning characteristics of the BPR and APR optimization\nframeworks to give mathematical evidence that, when the feedback data have a\ntailed distribution, APR amplifies the popularity bias more than BPR due to an\nunbalanced number of received positive updates from short-head items. Using\nmatrix factorization (MF), we empirically validate the theoretical results by\nperforming preliminary experiments on two public datasets to compare BPR-MF and\nAPR-MF performance on accuracy and beyond-accuracy metrics. The experimental\nresults consistently show the degradation of novelty and coverage measures and\na worrying amplification of bias.",
          "link": "http://arxiv.org/abs/2107.13876",
          "publishedOn": "2021-07-30T02:13:27.172Z",
          "wordCount": 687,
          "title": "Understanding the Effects of Adversarial Personalized Ranking Optimization Method on Recommendation Quality. (arXiv:2107.13876v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Depasquale_E/0/1/0/all/0/1\">Etienne-Victor Depasquale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salam_H/0/1/0/all/0/1\">Humaira Abdul Salam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davoli_F/0/1/0/all/0/1\">Franco Davoli</a>",
          "description": "We suggest an enhancement to structural coding through the use of (a)\ncausally bound codes, (b) basic constructs of graph theory and (c) statistics.\nAs is the norm with structural coding, the codes are collected into categories.\nThe categories are represented by nodes (graph theory). The causality is\nillustrated through links (graph theory) between the nodes and the entire set\nof linked nodes is collected into a single directed acyclic graph. The number\nof occurrences of the nodes and the links provide the input required to analyze\nrelative frequency of occurrence, as well as opening a scope for further\nstatistical analysis. While our raw data was a corpus of literature from a\nspecific discipline, this enhancement is accessible to any qualitative analysis\nthat recognizes causality in its structural codes.",
          "link": "http://arxiv.org/abs/2107.13983",
          "publishedOn": "2021-07-30T02:13:27.143Z",
          "wordCount": 579,
          "title": "PAD: a graphical and numerical enhancement of structural coding to facilitate thematic analysis of a literature corpus. (arXiv:2107.13983v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>",
          "description": "Despite advances in neural machine translation, cross-lingual retrieval tasks\nin which queries and documents live in different natural language spaces remain\nchallenging. Although neural translation models may provide an intuitive\napproach to tackle the cross-lingual problem, their resource-consuming training\nand advanced model structures may complicate the overall retrieval pipeline and\nreduce users engagement. In this paper, we build our end-to-end Cross-Lingual\nArabic Information REtrieval (CLAIRE) system based on the cross-lingual word\nembedding where searchers are assumed to have a passable passive understanding\nof Arabic and various supporting information in English is provided to aid\nretrieval experience. The proposed system has three major advantages: (1) The\nusage of English-Arabic word embedding simplifies the overall pipeline and\navoids the potential mistakes caused by machine translation. (2) Our CLAIRE\nsystem can incorporate arbitrary word embedding-based neural retrieval models\nwithout structural modification. (3) Early empirical results on an Arabic news\ncollection show promising performance.",
          "link": "http://arxiv.org/abs/2107.13751",
          "publishedOn": "2021-07-30T02:13:27.126Z",
          "wordCount": 572,
          "title": "The Cross-Lingual Arabic Information REtrieval (CLAIRE) System. (arXiv:2107.13751v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/1908.00814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peng-Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>",
          "description": "k-nearest neighbor graph is a fundamental data structure in many disciplines\nsuch as information retrieval, data-mining, pattern recognition, and machine\nlearning, etc. In the literature, considerable research has been focusing on\nhow to efficiently build an approximate k-nearest neighbor graph (k-NN graph)\nfor a fixed dataset. Unfortunately, a closely related issue of how to merge two\nexisting k-NN graphs has been overlooked. In this paper, we address the issue\nof k-NN graph merging in two different scenarios. In the first scenario, a\nsymmetric merge algorithm is proposed to combine two approximate k-NN graphs.\nThe algorithm facilitates large-scale processing by the efficient merging of\nk-NN graphs that are produced in parallel. In the second scenario, a joint\nmerge algorithm is proposed to expand an existing k-NN graph with a raw\ndataset. The algorithm enables the incremental construction of a hierarchical\napproximate k-NN graph. Superior performance is attained when leveraging the\nhierarchy for NN search of various data types, dimensionality, and distance\nmeasures.",
          "link": "http://arxiv.org/abs/1908.00814",
          "publishedOn": "2021-07-30T02:13:27.117Z",
          "wordCount": 660,
          "title": "On the Merge of k-NN Graph. (arXiv:1908.00814v6 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>",
          "description": "Pre-training on larger datasets with ever increasing model size is now a\nproven recipe for increased performance across almost all NLP tasks. A notable\nexception is information retrieval, where additional pre-training has so far\nfailed to produce convincing results. We show that, with the right pre-training\nsetup, this barrier can be overcome. We demonstrate this by pre-training large\nbi-encoder models on 1) a recently released set of 65 million synthetically\ngenerated questions, and 2) 200 million post-comment pairs from a preexisting\ndataset of Reddit conversations made available by pushshift.io. We evaluate on\na set of information retrieval and dialogue retrieval benchmarks, showing\nsubstantial improvements over supervised baselines.",
          "link": "http://arxiv.org/abs/2107.13602",
          "publishedOn": "2021-07-30T02:13:27.087Z",
          "wordCount": 554,
          "title": "Domain-matched Pre-training Tasks for Dense Retrieval. (arXiv:2107.13602v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.02590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_A/0/1/0/all/0/1\">Antonio Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malitesta_D/0/1/0/all/0/1\">Daniele Malitesta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merra_F/0/1/0/all/0/1\">Felice Antonio Merra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1\">Claudio Pomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donini_F/0/1/0/all/0/1\">Francesco Maria Donini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>",
          "description": "Recommender Systems have shown to be an effective way to alleviate the\nover-choice problem and provide accurate and tailored recommendations. However,\nthe impressive number of proposed recommendation algorithms, splitting\nstrategies, evaluation protocols, metrics, and tasks, has made rigorous\nexperimental evaluation particularly challenging. Puzzled and frustrated by the\ncontinuous recreation of appropriate evaluation benchmarks, experimental\npipelines, hyperparameter optimization, and evaluation procedures, we have\ndeveloped an exhaustive framework to address such needs. Elliot is a\ncomprehensive recommendation framework that aims to run and reproduce an entire\nexperimental pipeline by processing a simple configuration file. The framework\nloads, filters, and splits the data considering a vast set of strategies (13\nsplitting methods and 8 filtering approaches, from temporal training-test\nsplitting to nested K-folds Cross-Validation). Elliot optimizes hyperparameters\n(51 strategies) for several recommendation algorithms (50), selects the best\nmodels, compares them with the baselines providing intra-model statistics,\ncomputes metrics (36) spanning from accuracy to beyond-accuracy, bias, and\nfairness, and conducts statistical analysis (Wilcoxon and Paired t-test). The\naim is to provide the researchers with a tool to ease (and make them\nreproducible) all the experimental evaluation phases, from data reading to\nresults collection. Elliot is available on GitHub\n(https://github.com/sisinflab/elliot).",
          "link": "http://arxiv.org/abs/2103.02590",
          "publishedOn": "2021-07-30T02:13:27.069Z",
          "wordCount": 708,
          "title": "Elliot: a Comprehensive and Rigorous Framework for Reproducible Recommender Systems Evaluation. (arXiv:2103.02590v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daghaghi_S/0/1/0/all/0/1\">Shabnam Daghaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medini_T/0/1/0/all/0/1\">Tharun Medini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meisburger_N/0/1/0/all/0/1\">Nicholas Meisburger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beidi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengnan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>",
          "description": "Softmax classifiers with a very large number of classes naturally occur in\nmany applications such as natural language processing and information\nretrieval. The calculation of full softmax is costly from the computational and\nenergy perspective. There have been various sampling approaches to overcome\nthis challenge, popularly known as negative sampling (NS). Ideally, NS should\nsample negative classes from a distribution that is dependent on the input\ndata, the current parameters, and the correct positive class. Unfortunately,\ndue to the dynamically updated parameters and data samples, there is no\nsampling scheme that is provably adaptive and samples the negative classes\nefficiently. Therefore, alternative heuristics like random sampling, static\nfrequency-based sampling, or learning-based biased sampling, which primarily\ntrade either the sampling cost or the adaptivity of samples per iteration are\nadopted. In this paper, we show two classes of distributions where the sampling\nscheme is truly adaptive and provably generates negative samples in\nnear-constant time. Our implementation in C++ on CPU is significantly superior,\nboth in terms of wall-clock time and accuracy, compared to the most optimized\nTensorFlow implementations of other popular negative sampling approaches on\npowerful NVIDIA V100 GPU.",
          "link": "http://arxiv.org/abs/2012.15843",
          "publishedOn": "2021-07-30T02:13:27.055Z",
          "wordCount": 674,
          "title": "A Tale of Two Efficient and Informative Negative Sampling Distributions. (arXiv:2012.15843v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lartigou_F/0/1/0/all/0/1\">Fabrice Lartigou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govorov_M/0/1/0/all/0/1\">Michael Govorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aisake_T/0/1/0/all/0/1\">Tofiga Aisake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pankajeshwara N. Sharma</a>",
          "description": "This article deals with the development of an interactive up-to-date Pacific\nIslands Web GIS Atlas. It focuses on the compilation of spatial data from the\ntwelve member countries of the University of the South Pacific (Cook Islands,\nFiji Islands, Kiribati Islands, Marshall Islands, Nauru, Niue, Tonga, Tuvalu,\nTokelau, Solomon Islands, Vanuatu, and Western Samoa). A previous bitmap web\nAtlas was created in 1996, and was a pilot activity investigating the potential\nfor using Geographical Information Systems (GIS) in the South Pacific. The\nobjective of the new atlas is to provide sets of spatial and attributive data\nand maps for use of educators, students, researchers, policy makers and other\nrelevant user groups and the public. GIS is a highly flexible and dynamic\ntechnology that allows the construction and analysis of maps and data sets from\na variety of sources and formats. Nowadays, GIS application has moved from\nlocal and client-server applications to a three-tier architecture: Client (Web\nBrowser) -- Application Web Map Server -- Spatial Data Warehouses. The\nobjective of this project is to produce an Atlas that will include interactive\nmaps and data on an Application Web Map Server. Intergraph products such as\nGeoMedia Professional, Web Map and Web Publisher have been selected for the web\natlas production and design. In an interactive environment, an atlas will be\ncomposed from a series of maps and data profiles, which will be based on legend\nentries, queries, hot spots and cartographic tools. Only the first stage of\ndevelopment of the atlas and related technological solutions are outlined in\nthis article.",
          "link": "http://arxiv.org/abs/2107.14041",
          "publishedOn": "2021-07-30T02:13:27.025Z",
          "wordCount": 706,
          "title": "Interactive GIS Web-Atlas for Twelve Pacific Islands Countries. (arXiv:2107.14041v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>",
          "description": "The goal of information retrieval is to recommend a list of document\ncandidates that are most relevant to a given query. Listwise learning trains\nneural retrieval models by comparing various candidates simultaneously on a\nlarge scale, offering much more competitive performance than pairwise and\npointwise schemes. Existing listwise ranking losses treat the candidate\ndocument list as a whole unit without further inspection. Some candidates with\nmoderate semantic prominence may be ignored by the noisy similarity signals or\novershadowed by a few especially pronounced candidates. As a result, existing\nranking losses fail to exploit the full potential of neural retrieval models.\nTo address these concerns, we apply the classic pooling technique to conduct\nmulti-level coarse graining and propose ExpertRank, a novel expert-based\nlistwise ranking loss. The proposed scheme has three major advantages: (1)\nExpertRank introduces the profound physics concept of coarse graining to\ninformation retrieval by selecting prominent candidates at various local levels\nbased on model prediction and inter-document comparison. (2) ExpertRank applies\nthe mixture of experts (MoE) technique to combine different experts effectively\nby extending the traditional ListNet. (3) Compared to other existing listwise\nlearning approaches, ExpertRank produces much more reliable and competitive\nperformance for various neural retrieval models with different complexities,\nfrom traditional models, such as KNRM, ConvKNRM, MatchPyramid, to sophisticated\nBERT/ALBERT-based retrieval models.",
          "link": "http://arxiv.org/abs/2107.13752",
          "publishedOn": "2021-07-30T02:13:27.009Z",
          "wordCount": 639,
          "title": "ExpertRank: A Multi-level Coarse-grained Expert-based Listwise Ranking Loss. (arXiv:2107.13752v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadakis_M/0/1/0/all/0/1\">Manolis Fragkiadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>",
          "description": "Sign language lexica are a useful resource for researchers and people\nlearning sign languages. Current implementations allow a user to search a sign\neither by its gloss or by selecting its primary features such as handshape and\nlocation. This study focuses on exploring a reverse search functionality where\na user can sign a query sign in front of a webcam and retrieve a set of\nmatching signs. By extracting different body joints combinations (upper body,\ndominant hand's arm and wrist) using the pose estimation framework OpenPose, we\ncompare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance\nmetrics between 20 query signs, each performed by eight participants on a 1200\nsign lexicon. The results show that UMAP and DTW can predict a matching sign\nwith an 80\\% and 71\\% accuracy respectively at the top-20 retrieved signs using\nthe movement of the dominant hand arm. Using DTW and adding more sign instances\nfrom other participants in the lexicon, the accuracy can be raised to 90\\% at\nthe top-10 ranking. Our results suggest that our methodology can be used with\nno training in any sign language lexicon regardless of its size.",
          "link": "http://arxiv.org/abs/2107.13637",
          "publishedOn": "2021-07-30T02:13:26.946Z",
          "wordCount": 660,
          "title": "Sign and Search: Sign Search Functionality for Sign Language Lexica. (arXiv:2107.13637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mayor_O/0/1/0/all/0/1\">Oriol Barbany Mayor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellini_V/0/1/0/all/0/1\">Vito Bellini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchholz_A/0/1/0/all/0/1\">Alexander Buchholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benedetto_G/0/1/0/all/0/1\">Giuseppe Di Benedetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granziol_D/0/1/0/all/0/1\">Diego Marco Granziol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruffini_M/0/1/0/all/0/1\">Matteo Ruffini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_Y/0/1/0/all/0/1\">Yannik Stein</a>",
          "description": "Learning-to-rank (LTR) algorithms are ubiquitous and necessary to explore the\nextensive catalogs of media providers. To avoid the user examining all the\nresults, its preferences are used to provide a subset of relatively small size.\nThe user preferences can be inferred from the interactions with the presented\ncontent if explicit ratings are unavailable. However, directly using implicit\nfeedback can lead to learning wrong relevance models and is known as biased\nLTR. The mismatch between implicit feedback and true relevances is due to\nvarious nuisances, with position bias one of the most relevant. Position bias\nmodels consider that the lack of interaction with a presented item is not only\nattributed to the item being irrelevant but because the item was not examined.\nThis paper introduces a method for modeling the probability of an item being\nseen in different contexts, e.g., for different users, with a single estimator.\nOur suggested method, denoted as contextual (EM)-based regression, is\nranker-agnostic and able to correctly learn the latent examination\nprobabilities while only using implicit feedback. Our empirical results\nindicate that the method introduced in this paper outperforms other existing\nposition bias estimators in terms of relative error when the examination\nprobability varies across queries. Moreover, the estimated values provide a\nranking performance boost when used to debias the implicit ranking data even if\nthere is no context dependency on the examination probabilities.",
          "link": "http://arxiv.org/abs/2107.13327",
          "publishedOn": "2021-07-29T02:00:06.739Z",
          "wordCount": 661,
          "title": "Ranker-agnostic Contextual Position Bias Estimation. (arXiv:2107.13327v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Farwa K. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanagan_A/0/1/0/all/0/1\">Adrian Flanagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kuan E. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamgir_Z/0/1/0/all/0/1\">Zareen Alamgir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammad_Ud_Din_M/0/1/0/all/0/1\">Muhammad Ammad-Ud-Din</a>",
          "description": "We introduce the payload optimization method for federated recommender\nsystems (FRS). In federated learning (FL), the global model payload that is\nmoved between the server and users depends on the number of items to recommend.\nThe model payload grows when there is an increasing number of items. This\nbecomes challenging for an FRS if it is running in production mode. To tackle\nthe payload challenge, we formulated a multi-arm bandit solution that selected\npart of the global model and transmitted it to all users. The selection process\nwas guided by a novel reward function suitable for FL systems. So far as we are\naware, this is the first optimization method that seeks to address item\ndependent payloads. The method was evaluated using three benchmark\nrecommendation datasets. The empirical validation confirmed that the proposed\nmethod outperforms the simpler methods that do not benefit from the bandits for\nthe purpose of item selection. In addition, we have demonstrated the usefulness\nof our proposed method by rigorously evaluating the effects of a payload\nreduction on the recommendation performance degradation. Our method achieved up\nto a 90\\% reduction in model payload, yielding only a $\\sim$4\\% - 8\\% loss in\nthe recommendation performance for highly sparse datasets",
          "link": "http://arxiv.org/abs/2107.13078",
          "publishedOn": "2021-07-29T02:00:06.672Z",
          "wordCount": 674,
          "title": "A Payload Optimization Method for Federated Recommender Systems. (arXiv:2107.13078v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>",
          "description": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
          "link": "http://arxiv.org/abs/2010.06467",
          "publishedOn": "2021-07-29T02:00:06.660Z",
          "wordCount": 725,
          "title": "Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1\">Claudio Pomo</a>",
          "description": "Collaborative filtering models based on matrix factorization and learned\nsimilarities using Artificial Neural Networks (ANNs) have gained significant\nattention in recent years. This is, in part, because ANNs have demonstrated\ngood results in a wide variety of recommendation tasks. The introduction of\nANNs within the recommendation ecosystem has been recently questioned, raising\nseveral comparisons in terms of efficiency and effectiveness. One aspect most\nof these comparisons have in common is their focus on accuracy, neglecting\nother evaluation dimensions important for the recommendation, such as novelty,\ndiversity, or accounting for biases. We replicate experiments from three papers\nthat compare Neural Collaborative Filtering (NCF) and Matrix Factorization\n(MF), to extend the analysis to other evaluation dimensions. Our contribution\nshows that the experiments are entirely reproducible, and we extend the study\nincluding other accuracy metrics and two statistical hypothesis tests. We\ninvestigated the Diversity and Novelty of the recommendations, showing that MF\nprovides a better accuracy also on the long tail, although NCF provides a\nbetter item coverage and more diversified recommendations. We discuss the bias\neffect generated by the tested methods. They show a relatively small bias, but\nother recommendation baselines, with competitive accuracy performance,\nconsistently show to be less affected by this issue. This is the first work, to\nthe best of our knowledge, where several evaluation dimensions have been\nexplored for an array of SOTA algorithms covering recent adaptations of ANNs\nand MF. Hence, we show the potential these techniques may have on\nbeyond-accuracy evaluation while analyzing the effect on reproducibility these\ncomplementary dimensions may spark. Available at\ngithub.com/sisinflab/Reenvisioning-the-comparison-between-Neural-Collaborative-Filtering-and-Matrix-Factorization",
          "link": "http://arxiv.org/abs/2107.13472",
          "publishedOn": "2021-07-29T02:00:06.641Z",
          "wordCount": 710,
          "title": "Reenvisioning Collaborative Filtering vs Matrix Factorization. (arXiv:2107.13472v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dantong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>",
          "description": "Graph-based algorithms have shown great empirical potential for the\napproximate nearest neighbor (ANN) search problem. Currently, graph-based ANN\nsearch algorithms are designed mainly using heuristics, whereas theoretical\nanalysis of such algorithms is quite lacking. In this paper, we study a\nfundamental model of proximity graphs used in graph-based ANN search, called\nMonotonic Relative Neighborhood Graph (MRNG), from a theoretical perspective.\nWe use mathematical proofs to explain why proximity graphs that are built based\non MRNG tend to have good searching performance. We also run experiments on\nMRNG and graphs generalizing MRNG to obtain a deeper understanding of the\nmodel. Our experiments give guidance on how to approximate and generalize MRNG\nto build proximity graphs on a large scale. In addition, we discover and study\na hidden structure of MRNG called conflicting nodes, and we give theoretical\nevidence how conflicting nodes could be used to improve ANN search methods that\nare based on MRNG.",
          "link": "http://arxiv.org/abs/2107.13052",
          "publishedOn": "2021-07-29T02:00:06.609Z",
          "wordCount": 584,
          "title": "Understanding and Generalizing Monotonic Proximity Graphs for Approximate Nearest Neighbor Search. (arXiv:2107.13052v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_V/0/1/0/all/0/1\">Vivek Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witteveen_S/0/1/0/all/0/1\">Sam Witteveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_M/0/1/0/all/0/1\">Martin Andrews</a>",
          "description": "Creating explanations for answers to science questions is a challenging task\nthat requires multi-hop inference over a large set of fact sentences. This\nyear, to refocus the Textgraphs Shared Task on the problem of gathering\nrelevant statements (rather than solely finding a single 'correct path'), the\nWorldTree dataset was augmented with expert ratings of 'relevance' of\nstatements to each overall explanation. Our system, which achieved second place\non the Shared Task leaderboard, combines initial statement retrieval; language\nmodels trained to predict the relevance scores; and ensembling of a number of\nthe resulting rankings. Our code implementation is made available at\nhttps://github.com/mdda/worldtree_corpus/tree/textgraphs_2021",
          "link": "http://arxiv.org/abs/2107.13031",
          "publishedOn": "2021-07-29T02:00:06.591Z",
          "wordCount": 570,
          "title": "Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings. (arXiv:2107.13031v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dallmann_A/0/1/0/all/0/1\">Alexander Dallmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoller_D/0/1/0/all/0/1\">Daniel Zoller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1\">Andreas Hotho</a>",
          "description": "At the present time, sequential item recommendation models are compared by\ncalculating metrics on a small item subset (target set) to speed up\ncomputation. The target set contains the relevant item and a set of negative\nitems that are sampled from the full item set. Two well-known strategies to\nsample negative items are uniform random sampling and sampling by popularity to\nbetter approximate the item frequency distribution in the dataset. Most\nrecently published papers on sequential item recommendation rely on sampling by\npopularity to compare the evaluated models. However, recent work has already\nshown that an evaluation with uniform random sampling may not be consistent\nwith the full ranking, that is, the model ranking obtained by evaluating a\nmetric using the full item set as target set, which raises the question whether\nthe ranking obtained by sampling by popularity is equal to the full ranking. In\nthis work, we re-evaluate current state-of-the-art sequential recommender\nmodels from the point of view, whether these sampling strategies have an impact\non the final ranking of the models. We therefore train four recently proposed\nsequential recommendation models on five widely known datasets. For each\ndataset and model, we employ three evaluation strategies. First, we compute the\nfull model ranking. Then we evaluate all models on a target set sampled by the\ntwo different sampling strategies, uniform random sampling and sampling by\npopularity with the commonly used target set size of 100, compute the model\nranking for each strategy and compare them with each other. Additionally, we\nvary the size of the sampled target set. Overall, we find that both sampling\nstrategies can produce inconsistent rankings compared with the full ranking of\nthe models. Furthermore, both sampling by popularity and uniform random\nsampling do not consistently produce the same ranking ...",
          "link": "http://arxiv.org/abs/2107.13045",
          "publishedOn": "2021-07-29T02:00:06.544Z",
          "wordCount": 740,
          "title": "A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models. (arXiv:2107.13045v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/1907.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)",
          "description": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.",
          "link": "http://arxiv.org/abs/1907.02704",
          "publishedOn": "2021-07-28T02:02:30.601Z",
          "wordCount": 714,
          "title": "Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_L/0/1/0/all/0/1\">Luis Alberto Robles Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callahan_T/0/1/0/all/0/1\">Tiffany J. Callahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banda_J/0/1/0/all/0/1\">Juan M. Banda</a>",
          "description": "The use of social media data, like Twitter, for biomedical research has been\ngradually increasing over the years. With the COVID-19 pandemic, researchers\nhave turned to more nontraditional sources of clinical data to characterize the\ndisease in near real-time, study the societal implications of interventions, as\nwell as the sequelae that recovered COVID-19 cases present (Long-COVID).\nHowever, manually curated social media datasets are difficult to come by due to\nthe expensive costs of manual annotation and the efforts needed to identify the\ncorrect texts. When datasets are available, they are usually very small and\ntheir annotations do not generalize well over time or to larger sets of\ndocuments. As part of the 2021 Biomedical Linked Annotation Hackathon, we\nrelease our dataset of over 120 million automatically annotated tweets for\nbiomedical research purposes. Incorporating best practices, we identify tweets\nwith potentially high clinical relevance. We evaluated our work by comparing\nseveral SpaCy-based annotation frameworks against a manually annotated\ngold-standard dataset. Selecting the best method to use for automatic\nannotation, we then annotated 120 million tweets and released them publicly for\nfuture downstream usage within the biomedical domain.",
          "link": "http://arxiv.org/abs/2107.12565",
          "publishedOn": "2021-07-28T02:02:30.579Z",
          "wordCount": 672,
          "title": "A Biomedically oriented automatically annotated Twitter COVID-19 Dataset. (arXiv:2107.12565v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xueli Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Modeling user preference from his historical sequences is one of the core\nproblems of sequential recommendation. Existing methods in this field are\nwidely distributed from conventional methods to deep learning methods. However,\nmost of them only model users' interests within their own sequences and ignore\nthe dynamic collaborative signals among different user sequences, making it\ninsufficient to explore users' preferences. We take inspiration from dynamic\ngraph neural networks to cope with this challenge, modeling the user sequence\nand dynamic collaborative signals into one framework. We propose a new method\nnamed Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which\nconnects different user sequences through a dynamic graph structure, exploring\nthe interactive behavior of users and items with time and order information.\nFurthermore, we design a Dynamic Graph Recommendation Network to extract user's\npreferences from the dynamic graph. Consequently, the next-item prediction task\nin sequential recommendation is converted into a link prediction between the\nuser node and the item node in a dynamic graph. Extensive experiments on three\npublic benchmarks show that DGSR outperforms several state-of-the-art methods.\nFurther studies demonstrate the rationality and effectiveness of modeling user\nsequences through a dynamic graph.",
          "link": "http://arxiv.org/abs/2104.07368",
          "publishedOn": "2021-07-28T02:02:30.556Z",
          "wordCount": 653,
          "title": "Dynamic Graph Neural Networks for Sequential Recommendation. (arXiv:2104.07368v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bobadilla_J/0/1/0/all/0/1\">Jes&#xfa;s Bobadilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_F/0/1/0/all/0/1\">Fernando Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_A/0/1/0/all/0/1\">Abraham Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1\">&#xc1;ngel Gonz&#xe1;lez-Prieto</a>",
          "description": "Deep learning provides accurate collaborative filtering models to improve\nrecommender system results. Deep matrix factorization and their related\ncollaborative neural networks are the state-of-art in the field; nevertheless,\nboth models lack the necessary stochasticity to create the robust, continuous,\nand structured latent spaces that variational autoencoders exhibit. On the\nother hand, data augmentation through variational autoencoder does not provide\naccurate results in the collaborative filtering field due to the high sparsity\nof recommender systems. Our proposed models apply the variational concept to\ninject stochasticity in the latent space of the deep architecture, introducing\nthe variational technique in the neural collaborative filtering field. This\nmethod does not depend on the particular model used to generate the latent\nrepresentation. In this way, this approach can be applied as a plugin to any\ncurrent and future specific models. The proposed models have been tested using\nfour representative open datasets, three different quality measures, and\nstate-of-art baselines. The results show the superiority of the proposed\napproach in scenarios where the variational enrichment exceeds the injected\nnoise effect. Additionally, a framework is provided to enable the\nreproducibility of the conducted experiments.",
          "link": "http://arxiv.org/abs/2107.12677",
          "publishedOn": "2021-07-28T02:02:30.538Z",
          "wordCount": 637,
          "title": "Deep Variational Models for Collaborative Filtering-based Recommender Systems. (arXiv:2107.12677v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1\">Anastasios Nentidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1\">Anastasia Krithara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>",
          "description": "The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary\nwidely used in biomedical knowledge systems, particularly for semantic indexing\nof scientific literature. As the MeSH hierarchy evolves through annual version\nupdates, some new descriptors are introduced that were not previously\navailable. This paper explores the conceptual provenance of these new\ndescriptors. In particular, we investigate whether such new descriptors have\nbeen previously covered by older descriptors and what is their current relation\nto them. To this end, we propose a framework to categorize new descriptors\nbased on their current relation to older descriptors. Based on the proposed\nclassification scheme, we quantify, analyse and present the different types of\nnew descriptors introduced in MeSH during the last fifteen years. The results\nshow that only about 25% of new MeSH descriptors correspond to new emerging\nconcepts, whereas the rest were previously covered by one or more existing\ndescriptors, either implicitly or explicitly. Most of them were covered by a\nsingle existing descriptor and they usually end up as descendants of it in the\ncurrent hierarchy, gradually leading towards a more fine-grained MeSH\nvocabulary. These insights about the dynamics of the thesaurus are useful for\nthe retrospective study of scientific articles annotated with MeSH, but could\nalso be used to inform the policy of updating the thesaurus in the future.",
          "link": "http://arxiv.org/abs/2101.08293",
          "publishedOn": "2021-07-28T02:02:30.319Z",
          "wordCount": 718,
          "title": "What is all this new MeSH about? Exploring the semantic provenance of new descriptors in the MeSH thesaurus. (arXiv:2101.08293v3 [cs.DL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qingyun She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">PengTao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junlin Zhang</a>",
          "description": "Click-through rate (CTR) estimation is a fundamental task in personalized\nadvertising and recommender systems and it's important for ranking models to\neffectively capture complex high-order features.Inspired by the success of ELMO\nand Bert in NLP field, which dynamically refine word embedding according to the\ncontext sentence information where the word appears, we think it's also\nimportant to dynamically refine each feature's embedding layer by layer\naccording to the context information contained in input instance in CTR\nestimation tasks. We can effectively capture the useful feature interactions\nfor each feature in this way. In this paper, We propose a novel CTR Framework\nnamed ContextNet that implicitly models high-order feature interactions by\ndynamically refining each feature's embedding according to the input context.\nSpecifically, ContextNet consists of two key components: contextual embedding\nmodule and ContextNet block. Contextual embedding module aggregates contextual\ninformation for each feature from input instance and ContextNet block maintains\neach feature's embedding layer by layer and dynamically refines its\nrepresentation by merging contextual high-order interaction information into\nfeature embedding. To make the framework specific, we also propose two\nmodels(ContextNet-PFFN and ContextNet-SFFN) under this framework by introducing\nlinear contextual embedding network and two non-linear mapping sub-network in\nContextNet block. We conduct extensive experiments on four real-world datasets\nand the experiment results demonstrate that our proposed ContextNet-PFFN and\nContextNet-SFFN model outperform state-of-the-art models such as DeepFM and\nxDeepFM significantly.",
          "link": "http://arxiv.org/abs/2107.12025",
          "publishedOn": "2021-07-27T02:03:31.561Z",
          "wordCount": 678,
          "title": "ContextNet: A Click-Through Rate Prediction Framework Using Contextual information to Refine Feature Embedding. (arXiv:2107.12025v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanci Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1\">Tianming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yujie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donohue_L/0/1/0/all/0/1\">Lawrence Donohue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Rui Dai</a>",
          "description": "The quarterly financial statement, or Form 10-Q, is one of the most\nfrequently required filings for US public companies to disclose financial and\nother important business information. Due to the massive volume of 10-Q filings\nand the enormous variations in the reporting format, it has been a\nlong-standing challenge to retrieve item-specific information from 10-Q filings\nthat lack machine-readable hierarchy. This paper presents a solution for\nitemizing 10-Q files by complementing a rule-based algorithm with a\nConvolutional Neural Network (CNN) image classifier. This solution demonstrates\na pipeline that can be generalized to a rapid data retrieval solution among a\nlarge volume of textual data using only typographic items. The extracted\ntextual data can be used as unlabeled content-specific data to train\ntransformer models (e.g., BERT) or fit into various field-focus natural\nlanguage processing (NLP) applications.",
          "link": "http://arxiv.org/abs/2104.11783",
          "publishedOn": "2021-07-27T02:03:31.526Z",
          "wordCount": 613,
          "title": "Form 10-Q Itemization. (arXiv:2104.11783v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qingyun She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junlin Zhang</a>",
          "description": "Click-through rate (CTR) prediction plays important role in personalized\nadvertising and recommender systems. Though many models have been proposed such\nas FM, FFM and DeepFM in recent years, feature engineering is still a very\nimportant way to improve the model performance in many applications because\nusing raw features can rarely lead to optimal results. For example, the\ncontinuous features are usually transformed to the power forms by adding a new\nfeature to allow it to easily form non-linear functions of the feature.\nHowever, this kind of feature engineering heavily relies on peoples experience\nand it is both time consuming and labor consuming. On the other side, concise\nCTR model with both fast online serving speed and good model performance is\ncritical for many real life applications. In this paper, we propose LeafFM\nmodel based on FM to generate new features from the original feature embedding\nby learning the transformation functions automatically. We also design three\nconcrete Leaf-FM models according to the different strategies of combing the\noriginal and the generated features. Extensive experiments are conducted on\nthree real-world datasets and the results show Leaf-FM model outperforms\nstandard FMs by a large margin. Compared with FFMs, Leaf-FM can achieve\nsignificantly better performance with much less parameters. In Avazu and\nMalware dataset, add version Leaf-FM achieves comparable performance with some\ndeep learning based models such as DNN and AutoInt. As an improved FM model,\nLeaf-FM has the same computation complexity with FM in online serving phase and\nit means Leaf-FM is applicable in many industry applications because of its\nbetter performance and high computation efficiency.",
          "link": "http://arxiv.org/abs/2107.12024",
          "publishedOn": "2021-07-27T02:03:31.517Z",
          "wordCount": 700,
          "title": "Leaf-FM: A Learnable Feature Generation Factorization Machine for Click-Through Rate Prediction. (arXiv:2107.12024v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qingyun She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junlin Zhang</a>",
          "description": "Click-Through Rate(CTR) estimation has become one of the most fundamental\ntasks in many real-world applications and it's important for ranking models to\neffectively capture complex high-order features. Shallow feed-forward network\nis widely used in many state-of-the-art DNN models such as FNN, DeepFM and\nxDeepFM to implicitly capture high-order feature interactions. However, some\nresearch has proved that addictive feature interaction, particular feed-forward\nneural networks, is inefficient in capturing common feature interaction. To\nresolve this problem, we introduce specific multiplicative operation into DNN\nranking system by proposing instance-guided mask which performs element-wise\nproduct both on the feature embedding and feed-forward layers guided by input\ninstance. We also turn the feed-forward layer in DNN model into a mixture of\naddictive and multiplicative feature interactions by proposing MaskBlock in\nthis paper. MaskBlock combines the layer normalization, instance-guided mask,\nand feed-forward layer and it is a basic building block to be used to design\nnew ranking model under various configurations. The model consisting of\nMaskBlock is called MaskNet in this paper and two new MaskNet models are\nproposed to show the effectiveness of MaskBlock as basic building block for\ncomposing high performance ranking systems. The experiment results on three\nreal-world datasets demonstrate that our proposed MaskNet models outperform\nstate-of-the-art models such as DeepFM and xDeepFM significantly, which implies\nMaskBlock is an effective basic building unit for composing new high\nperformance ranking systems.",
          "link": "http://arxiv.org/abs/2102.07619",
          "publishedOn": "2021-07-27T02:03:31.467Z",
          "wordCount": 701,
          "title": "MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask. (arXiv:2102.07619v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1\">Bettina Fazzinga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>",
          "description": "Dialogue systems are widely used in AI to support timely and interactive\ncommunication with users. We propose a general-purpose dialogue system\narchitecture that leverages computational argumentation and state-of-the-art\nlanguage technologies. We illustrate and evaluate the system using a COVID-19\nvaccine information case study.",
          "link": "http://arxiv.org/abs/2107.12079",
          "publishedOn": "2021-07-27T02:03:31.455Z",
          "wordCount": 541,
          "title": "An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayaparan_M/0/1/0/all/0/1\">Mokanarangan Thayaparan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>",
          "description": "Regenerating natural language explanations for science questions is a\nchallenging task for evaluating complex multi-hop and abductive inference\ncapabilities. In this setting, Transformers trained on human-annotated\nexplanations achieve state-of-the-art performance when adopted as cross-encoder\narchitectures. However, while much attention has been devoted to the quality of\nthe constructed explanations, the problem of performing abductive inference at\nscale is still under-studied. As intrinsically not scalable, the cross-encoder\narchitectural paradigm is not suitable for efficient multi-hop inference on\nmassive facts banks. To maximise both accuracy and inference time, we propose a\nhybrid abductive solver that autoregressively combines a dense bi-encoder with\na sparse model of explanatory power, computed leveraging explicit patterns in\nthe explanations. Our experiments demonstrate that the proposed framework can\nachieve performance comparable with the state-of-the-art cross-encoder while\nbeing $\\approx 50$ times faster and scalable to corpora of millions of facts.\nMoreover, we study the impact of the hybridisation on semantic drift and\nscience question answering without additional training, showing that it boosts\nthe quality of the explanations and contributes to improved downstream\ninference performance.",
          "link": "http://arxiv.org/abs/2107.11879",
          "publishedOn": "2021-07-27T02:03:31.436Z",
          "wordCount": 617,
          "title": "Hybrid Autoregressive Solver for Scalable Abductive Natural Language Inference. (arXiv:2107.11879v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+AlGhamdi_K/0/1/0/all/0/1\">Kholoud AlGhamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1\">Elena Simperl</a>",
          "description": "Wikidata is an open knowledge graph built by a global community of\nvolunteers. As it advances in scale, it faces substantial challenges around\neditor engagement. These challenges are in terms of both attracting new editors\nto keep up with the sheer amount of work and retaining existing editors.\nExperience from other online communities and peer-production systems, including\nWikipedia, suggests that personalised recommendations could help, especially\nnewcomers, who are sometimes unsure about how to contribute best to an ongoing\neffort. For this reason, we propose a recommender system WikidataRec for\nWikidata items. The system uses a hybrid of content-based and collaborative\nfiltering techniques to rank items for editors relying on both item features\nand item-editor previous interaction. A neural network, named a neural mixture\nof representations, is designed to learn fine weights for the combination of\nitem-based representations and optimize them with editor-based representation\nby item-editor interaction. To facilitate further research in this space, we\nalso create two benchmark datasets, a general-purpose one with 220,000 editors\nresponsible for 14 million interactions with 4 million items and a second one\nfocusing on the contributions of more than 8,000 more active editors. We\nperform an offline evaluation of the system on both datasets with promising\nresults. Our code and datasets are available at\nhttps://github.com/WikidataRec-developer/Wikidata_Recommender.",
          "link": "http://arxiv.org/abs/2107.06423",
          "publishedOn": "2021-07-27T02:03:31.411Z",
          "wordCount": 665,
          "title": "Learning to Recommend Items to Wikidata Editors. (arXiv:2107.06423v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1\">Janntatul Tajrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naira Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>",
          "description": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.",
          "link": "http://arxiv.org/abs/2107.03844",
          "publishedOn": "2021-07-27T02:03:31.358Z",
          "wordCount": 717,
          "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1\">Yashar Deldjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knees_P/0/1/0/all/0/1\">Peter Knees</a>",
          "description": "The music domain is among the most important ones for adopting recommender\nsystems technology. In contrast to most other recommendation domains, which\npredominantly rely on collaborative filtering (CF) techniques, music\nrecommenders have traditionally embraced content-based (CB) approaches. In the\npast years, music recommendation models that leverage collaborative and content\ndata -- which we refer to as content-driven models -- have been replacing pure\nCF or CB models.\n\nIn this survey, we review 47 articles on content-driven music recommendation.\nBased on a thorough literature analysis, we first propose an onion model\ncomprising five layers, each of which corresponds to a category of music\ncontent we identified: signal, embedded metadata, expert-generated content,\nuser-generated content, and derivative content. We provide a detailed\ncharacterization of each category along several dimensions. Second, we identify\nsix overarching challenges, according to which we organize our main discussion:\nincreasing recommendation diversity and novelty, providing transparency and\nexplanations, accomplishing context-awareness, recommending sequences of music,\nimproving scalability and efficiency, and alleviating cold start. Each article\naddressing one or more of these challenges is categorized according to the\ncontent layers of our onion model, the article's goal(s), and main\nmethodological choices. Furthermore, articles are discussed in temporal order\nto shed light on the evolution of content-driven music recommendation\nstrategies. Finally, we provide our personal selection of the persisting grand\nchallenges, which are still waiting to be solved in future research endeavors.",
          "link": "http://arxiv.org/abs/2107.11803",
          "publishedOn": "2021-07-27T02:03:30.906Z",
          "wordCount": 669,
          "title": "Content-based Music Recommendation: Evolution, State of the Art, and Challenges. (arXiv:2107.11803v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roitero_K/0/1/0/all/0/1\">Kevin Roitero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soprano_M/0/1/0/all/0/1\">Michael Soprano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelli_B/0/1/0/all/0/1\">Beatrice Portelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luise_M/0/1/0/all/0/1\">Massimiliano De Luise</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spina_D/0/1/0/all/0/1\">Damiano Spina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mea_V/0/1/0/all/0/1\">Vincenzo Della Mea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1\">Giuseppe Serra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizzaro_S/0/1/0/all/0/1\">Stefano Mizzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demartini_G/0/1/0/all/0/1\">Gianluca Demartini</a>",
          "description": "Recently, the misinformation problem has been addressed with a\ncrowdsourcing-based approach: to assess the truthfulness of a statement,\ninstead of relying on a few experts, a crowd of non-expert is exploited. We\nstudy whether crowdsourcing is an effective and reliable method to assess\ntruthfulness during a pandemic, targeting statements related to COVID-19, thus\naddressing (mis)information that is both related to a sensitive and personal\nissue and very recent as compared to when the judgment is done. In our\nexperiments, crowd workers are asked to assess the truthfulness of statements,\nand to provide evidence for the assessments. Besides showing that the crowd is\nable to accurately judge the truthfulness of the statements, we report results\non workers behavior, agreement among workers, effect of aggregation functions,\nof scales transformations, and of workers background and bias. We perform a\nlongitudinal study by re-launching the task multiple times with both novice and\nexperienced workers, deriving important insights on how the behavior and\nquality change over time. Our results show that: workers are able to detect and\nobjectively categorize online (mis)information related to COVID-19; both\ncrowdsourced and expert judgments can be transformed and aggregated to improve\nquality; worker background and other signals (e.g., source of information,\nbehavior) impact the quality of the data. The longitudinal study demonstrates\nthat the time-span has a major effect on the quality of the judgments, for both\nnovice and experienced workers. Finally, we provide an extensive failure\nanalysis of the statements misjudged by the crowd-workers.",
          "link": "http://arxiv.org/abs/2107.11755",
          "publishedOn": "2021-07-27T02:03:30.714Z",
          "wordCount": 784,
          "title": "Can the Crowd Judge Truthfulness? A Longitudinal Study on Recent Misinformation about COVID-19. (arXiv:2107.11755v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2103.03496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mimnaugh_K/0/1/0/all/0/1\">Katherine J. Mimnaugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suomalainen_M/0/1/0/all/0/1\">Markku Suomalainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becerra_I/0/1/0/all/0/1\">Israel Becerra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_E/0/1/0/all/0/1\">Eliezer Lozano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murrieta_Cid_R/0/1/0/all/0/1\">Rafael Murrieta-Cid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LaValle_S/0/1/0/all/0/1\">Steven M. LaValle</a>",
          "description": "This paper considers how the motions of a telepresence robot moving\nautonomously affect a person immersed in the robot through a head-mounted\ndisplay. In particular, we explore the preference, comfort, and naturalness of\nelements of piecewise linear paths compared to the same elements on a smooth\npath. In a user study, thirty-six subjects watched panoramic videos of three\ndifferent paths through a simulated museum in virtual reality and responded to\nquestionnaires regarding each path. Preference for a particular path was\ninfluenced the most by comfort, forward speed, and characteristics of the\nturns. Preference was also strongly associated with the users' perceived\nnaturalness, which was primarily determined by the ability to see salient\nobjects, the distance to the walls and objects, as well as the turns.\nParticipants favored the paths that had a one meter per second forward speed\nand rated the path with the least amount of turns as the most comfortable",
          "link": "http://arxiv.org/abs/2103.03496",
          "publishedOn": "2021-07-30T02:13:27.261Z",
          "wordCount": 628,
          "title": "Analysis of User Preferences for Robot Motions in Immersive Telepresence. (arXiv:2103.03496v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xinyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongjin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuncong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingchao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>",
          "description": "Detecting abnormal activities in real-world surveillance videos is an\nimportant yet challenging task as the prior knowledge about video anomalies is\nusually limited or unavailable. Despite that many approaches have been\ndeveloped to resolve this problem, few of them can capture the normal\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\nseldom explicitly consider the local consistency at frame level and global\ncoherence of temporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative Adversarial\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\nSpecifically, we first present a convolutional transformer to perform future\nframe prediction. It contains three key components, i.e., a convolutional\nencoder to capture the spatial information of the input video clips, a temporal\nself-attention module to encode the temporal dynamics, and a convolutional\ndecoder to integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure, which jointly\nconsiders an image discriminator that can maintain the local consistency at\nframe-level and a video discriminator that can enforce the global coherence of\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\nthe prediction error is used to identify abnormal video frames. Thoroughly\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\nthe proposed adversarial spatio-temporal modeling framework.",
          "link": "http://arxiv.org/abs/2107.13720",
          "publishedOn": "2021-07-30T02:13:27.224Z",
          "wordCount": 708,
          "title": "Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. (arXiv:2107.13720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1\">Kin Wai Cheuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li Su</a>",
          "description": "Most of the current supervised automatic music transcription (AMT) models\nlack the ability to generalize. This means that they have trouble transcribing\nreal-world music recordings from diverse musical genres that are not presented\nin the labelled training data. In this paper, we propose a semi-supervised\nframework, ReconVAT, which solves this issue by leveraging the huge amount of\navailable unlabelled music recordings. The proposed ReconVAT uses\nreconstruction loss and virtual adversarial training. When combined with\nexisting U-net models for AMT, ReconVAT achieves competitive results on common\nbenchmark datasets such as MAPS and MusicNet. For example, in the few-shot\nsetting for the string part version of MusicNet, ReconVAT achieves F1-scores of\n61.0% and 41.6% for the note-wise and note-with-offset-wise metrics\nrespectively, which translates into an improvement of 22.2% and 62.5% compared\nto the supervised baseline model. Our proposed framework also demonstrates the\npotential of continual learning on new data, which could be useful in\nreal-world applications whereby new data is constantly available.",
          "link": "http://arxiv.org/abs/2107.04954",
          "publishedOn": "2021-07-30T02:13:27.197Z",
          "wordCount": 632,
          "title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_A/0/1/0/all/0/1\">Anique Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>",
          "description": "Photo-realistic point cloud capture and transmission are the fundamental\nenablers for immersive visual communication. The coding process of dynamic\npoint clouds, especially video-based point cloud compression (V-PCC) developed\nby the MPEG standardization group, is now delivering state-of-the-art\nperformance in compression efficiency. V-PCC is based on the projection of the\npoint cloud patches to 2D planes and encoding the sequence as 2D texture and\ngeometry patch sequences. However, the resulting quantization errors from\ncoding can introduce compression artifacts, which can be very unpleasant for\nthe quality of experience (QoE). In this work, we developed a novel\nout-of-the-loop point cloud geometry artifact removal solution that can\nsignificantly improve reconstruction quality without additional bandwidth cost.\nOur novel framework consists of a point cloud sampling scheme, an artifact\nremoval network, and an aggregation scheme. The point cloud sampling scheme\nemploys a cube-based neighborhood patch extraction to divide the point cloud\ninto patches. The geometry artifact removal network then processes these\npatches to obtain artifact-removed patches. The artifact-removed patches are\nthen merged together using an aggregation scheme to obtain the final\nartifact-removed point cloud. We employ 3D deep convolutional feature learning\nfor geometry artifact removal that jointly recovers both the quantization\ndirection and the quantization noise level by exploiting projection and\nquantization prior. The simulation results demonstrate that the proposed method\nis highly effective and can considerably improve the quality of the\nreconstructed point cloud.",
          "link": "http://arxiv.org/abs/2107.14179",
          "publishedOn": "2021-07-30T02:13:27.160Z",
          "wordCount": 673,
          "title": "Video-based Point Cloud Compression Artifact Removal. (arXiv:2107.14179v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadakis_M/0/1/0/all/0/1\">Manolis Fragkiadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>",
          "description": "Sign language lexica are a useful resource for researchers and people\nlearning sign languages. Current implementations allow a user to search a sign\neither by its gloss or by selecting its primary features such as handshape and\nlocation. This study focuses on exploring a reverse search functionality where\na user can sign a query sign in front of a webcam and retrieve a set of\nmatching signs. By extracting different body joints combinations (upper body,\ndominant hand's arm and wrist) using the pose estimation framework OpenPose, we\ncompare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance\nmetrics between 20 query signs, each performed by eight participants on a 1200\nsign lexicon. The results show that UMAP and DTW can predict a matching sign\nwith an 80\\% and 71\\% accuracy respectively at the top-20 retrieved signs using\nthe movement of the dominant hand arm. Using DTW and adding more sign instances\nfrom other participants in the lexicon, the accuracy can be raised to 90\\% at\nthe top-10 ranking. Our results suggest that our methodology can be used with\nno training in any sign language lexicon regardless of its size.",
          "link": "http://arxiv.org/abs/2107.13637",
          "publishedOn": "2021-07-30T02:13:26.993Z",
          "wordCount": 660,
          "title": "Sign and Search: Sign Search Functionality for Sign Language Lexica. (arXiv:2107.13637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:07.070Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Garcia_A/0/1/0/all/0/1\">Aaron Lopez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Francesc J. Ferri</a>",
          "description": "The use of multiple and semantically correlated sources can provide\ncomplementary information to each other that may not be evident when working\nwith individual modalities on their own. In this context, multi-modal models\ncan help producing more accurate and robust predictions in machine learning\ntasks where audio-visual data is available. This paper presents a multi-modal\nmodel for automatic scene classification that exploits simultaneously auditory\nand visual information. The proposed approach makes use of two separate\nnetworks which are respectively trained in isolation on audio and visual data,\nso that each network specializes in a given modality. The visual subnetwork is\na pre-trained VGG16 model followed by a bidiretional recurrent layer, while the\nresidual audio subnetwork is based on stacked squeeze-excitation convolutional\nblocks trained from scratch. After training each subnetwork, the fusion of\ninformation from the audio and visual streams is performed at two different\nstages. The early fusion stage combines features resulting from the last\nconvolutional block of the respective subnetworks at different time steps to\nfeed a bidirectional recurrent structure. The late fusion stage combines the\noutput of the early fusion stage with the independent predictions provided by\nthe two subnetworks, resulting in the final prediction. We evaluate the method\nusing the recently published TAU Audio-Visual Urban Scenes 2021, which contains\nsynchronized audio and video recordings from 12 European cities in 10 different\nscene classes. The proposed model has been shown to provide an excellent\ntrade-off between prediction performance (86.5%) and system complexity (15M\nparameters) in the evaluation results of the DCASE 2021 Challenge.",
          "link": "http://arxiv.org/abs/2107.13180",
          "publishedOn": "2021-07-29T02:00:06.906Z",
          "wordCount": 712,
          "title": "Squeeze-Excitation Convolutional Recurrent Neural Networks for Audio-Visual Scene Classification. (arXiv:2107.13180v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.",
          "link": "http://arxiv.org/abs/2107.13469",
          "publishedOn": "2021-07-29T02:00:06.786Z",
          "wordCount": 605,
          "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13385",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wieckowski_A/0/1/0/all/0/1\">Adam Wieckowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lehmann_C/0/1/0/all/0/1\">Christian Lehmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bross_B/0/1/0/all/0/1\">Benjamin Bross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marpe_D/0/1/0/all/0/1\">Detlev Marpe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biatek_T/0/1/0/all/0/1\">Thibaud Biatek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raulet_M/0/1/0/all/0/1\">Mickael Raulet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feuvre_J/0/1/0/all/0/1\">Jean Le Feuvre</a>",
          "description": "Versatile Video Coding (VVC) is the most recent international video coding\nstandard jointly developed by ITU-T and ISO/IEC, which has been finalized in\nJuly 2020. VVC allows for significant bit-rate reductions around 50% for the\nsame subjective video quality compared to its predecessor, High Efficiency\nVideo Coding (HEVC). One year after finalization, VVC support in devices and\nchipsets is still under development, which is aligned with the typical\ndevelopment cycles of new video coding standards. This paper presents\nopen-source software packages that allow building a complete VVC end-to-end\ntoolchain already one year after its finalization. This includes the Fraunhofer\nHHI VVenC library for fast and efficient VVC encoding as well as HHI's VVdeC\nlibrary for live decoding. An experimental integration of VVC in the GPAC\nsoftware tools and FFmpeg media framework allows packaging VVC bitstreams, e.g.\nencoded with VVenC, in MP4 file format and using DASH for content creation and\nstreaming. The integration of VVdeC allows playback on the receiver. Given\nthese packages, step-by-step tutorials are provided for two possible\napplication scenarios: VVC file encoding plus playback and adaptive streaming\nwith DASH.",
          "link": "http://arxiv.org/abs/2107.13385",
          "publishedOn": "2021-07-29T02:00:06.776Z",
          "wordCount": 652,
          "title": "A Complete End-To-End Open Source Toolchain for the Versatile Video Coding (VVC) Standard. (arXiv:2107.13385v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianhua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xiangui Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yun-Qing Shi</a>",
          "description": "A great challenge to steganography has arisen with the wide application of\nsteganalysis methods based on convolutional neural networks (CNNs). To this\nend, embedding cost learning frameworks based on generative adversarial\nnetworks (GANs) have been proposed and achieved success for spatial\nsteganography. However, the application of GAN to JPEG steganography is still\nin the prototype stage; its anti-detectability and training efficiency should\nbe improved. In conventional steganography, research has shown that the\nside-information calculated from the precover can be used to enhance security.\nHowever, it is hard to calculate the side-information without the spatial\ndomain image. In this work, an embedding cost learning framework for JPEG\nSteganography via a Generative Adversarial Network (JS-GAN) has been proposed,\nthe learned embedding cost can be further adjusted asymmetrically according to\nthe estimated side-information. Experimental results have demonstrated that the\nproposed method can automatically learn a content-adaptive embedding cost\nfunction, and use the estimated side-information properly can effectively\nimprove the security performance. For example, under the attack of a classic\nsteganalyzer GFR with quality factor 75 and 0.4 bpnzAC, the proposed JS-GAN can\nincrease the detection error 2.58% over J-UNIWARD, and the estimated\nside-information aided version JS-GAN(ESI) can further increase the security\nperformance by 11.25% over JS-GAN.",
          "link": "http://arxiv.org/abs/2107.13151",
          "publishedOn": "2021-07-29T02:00:06.723Z",
          "wordCount": 638,
          "title": "JPEG Steganography with Embedding Cost Learning and Side-Information Estimation. (arXiv:2107.13151v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1\">Alessio Xompero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donaher_S/0/1/0/all/0/1\">Santiago Donaher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iashin_V/0/1/0/all/0/1\">Vladimir Iashin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palermo_F/0/1/0/all/0/1\">Francesca Palermo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solak_G/0/1/0/all/0/1\">G&#xf6;khan Solak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppola_C/0/1/0/all/0/1\">Claudio Coppola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_R/0/1/0/all/0/1\">Reina Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagao_Y/0/1/0/all/0/1\">Yuichi Nagao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1\">Ryo Hachiuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chuanlin Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Rosa H. M. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christmann_G/0/1/0/all/0/1\">Guilherme Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jyun-Ting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neeharika_G/0/1/0/all/0/1\">Gonuguntla Neeharika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chinnakotla Krishna Teja Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Dinesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehman_B/0/1/0/all/0/1\">Bakhtawar Ur Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>",
          "description": "Acoustic and visual sensing can support the contactless estimation of the\nweight of a container and the amount of its content when the container is\nmanipulated by a person. However, transparencies (both of the container and of\nthe content) and the variability of materials, shapes and sizes make this\nproblem challenging. In this paper, we present an open benchmarking framework\nand an in-depth comparative analysis of recent methods that estimate the\ncapacity of a container, as well as the type, mass, and amount of its content.\nThese methods use learned and handcrafted features, such as mel-frequency\ncepstrum coefficients, zero-crossing rate, spectrograms, with different types\nof classifiers to estimate the type and amount of the content with acoustic\ndata, and geometric approaches with visual data to determine the capacity of\nthe container. Results on a newly distributed dataset show that audio alone is\na strong modality and methods achieves a weighted average F1-score up to 81%\nand 97% for content type and level classification, respectively. Estimating the\ncontainer capacity with vision-only approaches and filling mass with\nmulti-modal, multi-stage algorithms reaches up to 65% weighted average capacity\nand mass scores.",
          "link": "http://arxiv.org/abs/2107.12719",
          "publishedOn": "2021-07-28T02:02:30.621Z",
          "wordCount": 692,
          "title": "Multi-modal estimation of the properties of containers and their content: survey and evaluation. (arXiv:2107.12719v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simonetta_F/0/1/0/all/0/1\">Federico Simonetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntalampiras_S/0/1/0/all/0/1\">Stavros Ntalampiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avanzini_F/0/1/0/all/0/1\">Federico Avanzini</a>",
          "description": "Audio-to-score alignment (A2SA) is a multimodal task consisting in the\nalignment of audio signals to music scores. Recent literature confirms the\nbenefits of Automatic Music Transcription (AMT) for A2SA at the frame-level. In\nthis work, we aim to elaborate on the exploitation of AMT Deep Learning (DL)\nmodels for achieving alignment at the note-level. We propose a method which\nbenefits from HMM-based score-to-score alignment and AMT, showing a remarkable\nadvancement beyond the state-of-the-art. We design a systematic procedure to\ntake advantage of large datasets which do not offer an aligned score. Finally,\nwe perform a thorough comparison and extensive tests on multiple datasets.",
          "link": "http://arxiv.org/abs/2107.12854",
          "publishedOn": "2021-07-28T02:02:30.546Z",
          "wordCount": 540,
          "title": "Audio-to-Score Alignment Using Deep Automatic Music Transcription. (arXiv:2107.12854v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/1907.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)",
          "description": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.",
          "link": "http://arxiv.org/abs/1907.02704",
          "publishedOn": "2021-07-28T02:02:30.498Z",
          "wordCount": 714,
          "title": "Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Menghan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Simon X. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>",
          "description": "For people who ardently love painting but unfortunately have visual\nimpairments, holding a paintbrush to create a work is a very difficult task.\nPeople in this special group are eager to pick up the paintbrush, like Leonardo\nda Vinci, to create and make full use of their own talents. Therefore, to\nmaximally bridge this gap, we propose a painting navigation system to assist\nblind people in painting and artistic creation. The proposed system is composed\nof cognitive system and guidance system. The system adopts drawing board\npositioning based on QR code, brush navigation based on target detection and\nbush real-time positioning. Meanwhile, this paper uses human-computer\ninteraction on the basis of voice and a simple but efficient position\ninformation coding rule. In addition, we design a criterion to efficiently\njudge whether the brush reaches the target or not. According to the\nexperimental results, the thermal curves extracted from the faces of testers\nshow that it is relatively well accepted by blindfolded and even blind testers.\nWith the prompt frequency of 1s, the painting navigation system performs best\nwith the completion degree of 89% with SD of 8.37% and overflow degree of 347%\nwith SD of 162.14%. Meanwhile, the excellent and good types of brush tip\ntrajectory account for 74%, and the relative movement distance is 4.21 with SD\nof 2.51. This work demonstrates that it is practicable for the blind people to\nfeel the world through the brush in their hands. In the future, we plan to\ndeploy Angle's Eyes on the phone to make it more portable. The demo video of\nthe proposed painting navigation system is available at:\nhttps://doi.org/10.6084/m9.figshare.9760004.v1.",
          "link": "http://arxiv.org/abs/2107.12921",
          "publishedOn": "2021-07-28T02:02:30.489Z",
          "wordCount": 739,
          "title": "Angel's Girl for Blind Painters: an Efficient Painting Navigation System Validated by Multimodal Evaluation Approach. (arXiv:2107.12921v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Altinisik_E/0/1/0/all/0/1\">Enes Altinisik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sencar_H/0/1/0/all/0/1\">H&#xfc;srev Taha Sencar</a>",
          "description": "We address the problem of decoding video file fragments when the necessary\nencoding parameters are missing. With this objective, we propose a method that\nautomatically generates H.264 video headers containing these parameters and\nextracts coded pictures in the partially available compressed video data. To\naccomplish this, we examined a very large corpus of videos to learn patterns of\nencoding settings commonly used by encoders and created a parameter dictionary.\nFurther, to facilitate a more efficient search our method identifies\ncharacteristics of a coded bitstream to discriminate the entropy coding mode.\nIt also utilizes the application logs created by the decoder to identify\ncorrect parameter values. Evaluation of the effectiveness of the proposed\nmethod on more than 55K videos with diverse provenance shows that it can\ngenerate valid headers on average in 11.3 decoding trials per video. This\nresult represents an improvement by more than a factor of 10 over the\nconventional approach of video header stitching to recover video file\nfragments.",
          "link": "http://arxiv.org/abs/2104.14522",
          "publishedOn": "2021-07-28T02:02:30.469Z",
          "wordCount": 616,
          "title": "Automatic Generation of H.264 Parameter Sets to Recover Video File Fragments. (arXiv:2104.14522v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1711.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_M/0/1/0/all/0/1\">Michael Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi-Dong Shen</a>",
          "description": "Matching images and sentences demands a fine understanding of both\nmodalities. In this paper, we propose a new system to discriminatively embed\nthe image and text to a shared visual-textual space. In this field, most\nexisting works apply the ranking loss to pull the positive image / text pairs\nclose and push the negative pairs apart from each other. However, directly\ndeploying the ranking loss is hard for network learning, since it starts from\nthe two heterogeneous features to build inter-modal relationship. To address\nthis problem, we propose the instance loss which explicitly considers the\nintra-modal data distribution. It is based on an unsupervised assumption that\neach image / text group can be viewed as a class. So the network can learn the\nfine granularity from every image/text group. The experiment shows that the\ninstance loss offers better weight initialization for the ranking loss, so that\nmore discriminative embeddings can be learned. Besides, existing works usually\napply the off-the-shelf features, i.e., word2vec and fixed visual feature. So\nin a minor contribution, this paper constructs an end-to-end dual-path\nconvolutional network to learn the image and text representations. End-to-end\nlearning allows the system to directly learn from the data and fully utilize\nthe supervision. On two generic retrieval datasets (Flickr30k and MSCOCO),\nexperiments demonstrate that our method yields competitive accuracy compared to\nstate-of-the-art methods. Moreover, in language based person retrieval, we\nimprove the state of the art by a large margin. The code has been made publicly\navailable.",
          "link": "http://arxiv.org/abs/1711.05535",
          "publishedOn": "2021-07-28T02:02:30.455Z",
          "wordCount": 743,
          "title": "Dual-Path Convolutional Image-Text Embeddings with Instance Loss. (arXiv:1711.05535v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1\">Hannaneh Barahouei Pasandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1\">Tamer Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1\">Hadi Amirpour</a>",
          "description": "Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency\nhave promised to increase data throughput by allowing concurrent communication\nbetween one Access Point and multiple users. However, we are still a long way\nfrom enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry\napplications such as video streaming in practical WiFi network settings due to\nheterogeneous channel conditions and devices, unreliable transmissions, and\nlack of useful feedback exchange among the lower and upper layers'\nrequirements. This paper introduces MuViS, a novel dual-phase optimization\nframework that proposes a Quality of Experience (QoE) aware MU-MIMO\noptimization for multi-user video streaming over IEEE 802.11ac. MuViS first\nemploys reinforcement learning to optimize the MU-MIMO user group and mode\nselection for users based on their PHY/MAC layer characteristics. The video\nbitrate is then optimized based on the user's mode (Multi-User (MU) or\nSingle-User (SU)). We present our design and its evaluation on smartphones and\nlaptops using 802.11ac WiFi. Our experimental results in various indoor\nenvironments and configurations show a scalable framework that can support a\nlarge number of users with streaming at high video rates and satisfying QoE\nrequirements.",
          "link": "http://arxiv.org/abs/2106.15262",
          "publishedOn": "2021-07-28T02:02:30.421Z",
          "wordCount": 637,
          "title": "MU-MIMO Grouping For Real-time Applications. (arXiv:2106.15262v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuangjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>",
          "description": "Recent deep networks have convincingly demonstrated high capability in crowd\ncounting, which is a critical task attracting widespread attention due to its\nvarious industrial applications. Despite such progress, trained data-dependent\nmodels usually can not generalize well to unseen scenarios because of the\ninherent domain shift. To facilitate this issue, this paper proposes a novel\nadversarial scoring network (ASNet) to gradually bridge the gap across domains\nfrom coarse to fine granularity. In specific, at the coarse-grained stage, we\ndesign a dual-discriminator strategy to adapt source domain to be close to the\ntargets from the perspectives of both global and local feature space via\nadversarial learning. The distributions between two domains can thus be aligned\nroughly. At the fine-grained stage, we explore the transferability of source\ncharacteristics by scoring how similar the source samples are to target ones\nfrom multiple levels based on generative probability derived from coarse stage.\nGuided by these hierarchical scores, the transferable source features are\nproperly selected to enhance the knowledge transfer during the adaptation\nprocess. With the coarse-to-fine design, the generalization bottleneck induced\nfrom the domain discrepancy can be effectively alleviated. Three sets of\nmigration experiments show that the proposed methods achieve state-of-the-art\ncounting performance compared with major unsupervised methods.",
          "link": "http://arxiv.org/abs/2107.12858",
          "publishedOn": "2021-07-28T02:02:30.408Z",
          "wordCount": 657,
          "title": "Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring Network. (arXiv:2107.12858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1\">Yashar Deldjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knees_P/0/1/0/all/0/1\">Peter Knees</a>",
          "description": "The music domain is among the most important ones for adopting recommender\nsystems technology. In contrast to most other recommendation domains, which\npredominantly rely on collaborative filtering (CF) techniques, music\nrecommenders have traditionally embraced content-based (CB) approaches. In the\npast years, music recommendation models that leverage collaborative and content\ndata -- which we refer to as content-driven models -- have been replacing pure\nCF or CB models.\n\nIn this survey, we review 47 articles on content-driven music recommendation.\nBased on a thorough literature analysis, we first propose an onion model\ncomprising five layers, each of which corresponds to a category of music\ncontent we identified: signal, embedded metadata, expert-generated content,\nuser-generated content, and derivative content. We provide a detailed\ncharacterization of each category along several dimensions. Second, we identify\nsix overarching challenges, according to which we organize our main discussion:\nincreasing recommendation diversity and novelty, providing transparency and\nexplanations, accomplishing context-awareness, recommending sequences of music,\nimproving scalability and efficiency, and alleviating cold start. Each article\naddressing one or more of these challenges is categorized according to the\ncontent layers of our onion model, the article's goal(s), and main\nmethodological choices. Furthermore, articles are discussed in temporal order\nto shed light on the evolution of content-driven music recommendation\nstrategies. Finally, we provide our personal selection of the persisting grand\nchallenges, which are still waiting to be solved in future research endeavors.",
          "link": "http://arxiv.org/abs/2107.11803",
          "publishedOn": "2021-07-27T02:03:31.853Z",
          "wordCount": 669,
          "title": "Content-based Music Recommendation: Evolution, State of the Art, and Challenges. (arXiv:2107.11803v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuqian Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>",
          "description": "Given a video demonstration, can we imitate the action contained in this\nvideo? In this paper, we introduce a novel task, dubbed mesh-based action\nimitation. The goal of this task is to enable an arbitrary target human mesh to\nperform the same action shown on the video demonstration. To achieve this, a\nnovel Mesh-based Video Action Imitation (M-VAI) method is proposed by us. M-VAI\nfirst learns to reconstruct the meshes from the given source image frames, then\nthe initial recovered mesh sequence is fed into mesh2mesh, a mesh sequence\nsmooth module proposed by us, to improve the temporal consistency. Finally, we\nimitate the actions by transferring the pose from the constructed human body to\nour target identity mesh. High-quality and detailed human body meshes can be\ngenerated by using our M-VAI. Extensive experiments demonstrate the feasibility\nof our task and the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2107.11756",
          "publishedOn": "2021-07-27T02:03:31.845Z",
          "wordCount": 599,
          "title": "Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics from Videos. (arXiv:2107.11756v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shih-Lun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "Transformers and variational autoencoders (VAE) have been extensively\nemployed for symbolic (e.g., MIDI) domain music generation. While the former\nboast an impressive capability in modeling long sequences, the latter allow\nusers to willingly exert control over different parts (e.g., bars) of the music\nto be generated. In this paper, we are interested in bringing the two together\nto construct a single model that exhibits both strengths. The task is split\ninto two steps. First, we equip Transformer decoders with the ability to accept\nsegment-level, time-varying conditions during sequence generation.\nSubsequently, we combine the developed and tested in-attention decoder with a\nTransformer encoder, and train the resulting MuseMorphose model with the VAE\nobjective to achieve style transfer of long musical pieces, in which users can\nspecify musical attributes including rhythmic intensity and polyphony (i.e.,\nharmonic fullness) they desire, down to the bar level. Experiments show that\nMuseMorphose outperforms recurrent neural network (RNN) based baselines on\nnumerous widely-used metrics for style transfer tasks.",
          "link": "http://arxiv.org/abs/2105.04090",
          "publishedOn": "2021-07-27T02:03:31.771Z",
          "wordCount": 639,
          "title": "MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with One Transformer VAE. (arXiv:2105.04090v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_L/0/1/0/all/0/1\">Lovish Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Sarthak Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Abhijit Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Sandip Chakraborty</a>",
          "description": "With increasing advancements in technologies for capturing 360{\\deg} videos,\nadvances in streaming such videos have become a popular research topic.\nHowever, streaming 360{\\deg} videos require high bandwidth, thus escalating the\nneed for developing optimized streaming algorithms. Researchers have proposed\nvarious methods to tackle the problem, considering the network bandwidth or\nattempt to predict future viewports in advance. However, most of the existing\nworks either (1) do not consider video contents to predict user viewport, or\n(2) do not adapt to user preferences dynamically, or (3) require a lot of\ntraining data for new videos, thus making them potentially unfit for video\nstreaming purposes. We develop PARIMA, a fast and efficient online viewport\nprediction model that uses past viewports of users along with the trajectories\nof prime objects as a representative of video content to predict future\nviewports. We claim that the head movement of a user majorly depends upon the\ntrajectories of the prime objects in the video. We employ a pyramid-based\nbitrate allocation scheme and perform a comprehensive evaluation of the\nperformance of PARIMA. In our evaluation, we show that PARIMA outperforms\nstate-of-the-art approaches, improving the Quality of Experience by over 30\\%\nwhile maintaining a short response time.",
          "link": "http://arxiv.org/abs/2103.00981",
          "publishedOn": "2021-07-27T02:03:31.508Z",
          "wordCount": 664,
          "title": "PARIMA: Viewport Adaptive 360-Degree Video Streaming. (arXiv:2103.00981v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jingjing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1\">Zhixiong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>",
          "description": "Encouraging progress has been made towards Visual Question Answering (VQA) in\nrecent years, but it is still challenging to enable VQA models to adaptively\ngeneralize to out-of-distribution (OOD) samples. Intuitively, recompositions of\nexisting visual concepts (i.e., attributes and objects) can generate unseen\ncompositions in the training set, which will promote VQA models to generalize\nto OOD samples. In this paper, we formulate OOD generalization in VQA as a\ncompositional generalization problem and propose a graph generative\nmodeling-based training scheme (X-GGM) to handle the problem implicitly. X-GGM\nleverages graph generative modeling to iteratively generate a relation matrix\nand node representations for the predefined graph that utilizes\nattribute-object pairs as nodes. Furthermore, to alleviate the unstable\ntraining issue in graph generative modeling, we propose a gradient distribution\nconsistency loss to constrain the data distribution with adversarial\nperturbations and the generated distribution. The baseline VQA model (LXMERT)\ntrained with the X-GGM scheme achieves state-of-the-art OOD performance on two\nstandard VQA OOD benchmarks, i.e., VQA-CP v2 and GQA-OOD. Extensive ablation\nstudies demonstrate the effectiveness of X-GGM components.",
          "link": "http://arxiv.org/abs/2107.11576",
          "publishedOn": "2021-07-27T02:03:31.343Z",
          "wordCount": 624,
          "title": "X-GGM: Graph Generative Modeling for Out-of-Distribution Generalization in Visual Question Answering. (arXiv:2107.11576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1\">Biao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Guorui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>",
          "description": "Recent advances in linguistic steganalysis have successively applied CNNs,\nRNNs, GNNs and other deep learning models for detecting secret information in\ngenerative texts. These methods tend to seek stronger feature extractors to\nachieve higher steganalysis effects. However, we have found through experiments\nthat there actually exists significant difference between automatically\ngenerated steganographic texts and carrier texts in terms of the conditional\nprobability distribution of individual words. Such kind of statistical\ndifference can be naturally captured by the language model used for generating\nsteganographic texts, which drives us to give the classifier a priori knowledge\nof the language model to enhance the steganalysis ability. To this end, we\npresent two methods to efficient linguistic steganalysis in this paper. One is\nto pre-train a language model based on RNN, and the other is to pre-train a\nsequence autoencoder. Experimental results show that the two methods have\ndifferent degrees of performance improvement when compared to the randomly\ninitialized RNN classifier, and the convergence speed is significantly\naccelerated. Moreover, our methods have achieved the best detection results.",
          "link": "http://arxiv.org/abs/2107.12168",
          "publishedOn": "2021-07-27T02:03:31.329Z",
          "wordCount": 615,
          "title": "Exploiting Language Model for Efficient Linguistic Steganalysis: An Empirical Study. (arXiv:2107.12168v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Arun Kumar Singh</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Priyanka Singh</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Nathwani_K/0/1/0/all/0/1\">Karan Nathwani</a> (1) ((1) Indian Institute of Technology Jammu, (2) Dhirubhai Ambani Institute of Information and Communication Technology)",
          "description": "The recent developments in technology have re-warded us with amazing audio\nsynthesis models like TACOTRON and WAVENETS. On the other side, it poses\ngreater threats such as speech clones and deep fakes, that may go undetected.\nTo tackle these alarming situations, there is an urgent need to propose models\nthat can help discriminate a synthesized speech from an actual human speech and\nalso identify the source of such a synthesis. Here, we propose a model based on\nConvolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network\n(BiRNN) that helps to achieve both the aforementioned objectives. The temporal\ndependencies present in AI synthesized speech are exploited using Bidirectional\nRNN and CNN. The model outperforms the state-of-the-art approaches by\nclassifying the AI synthesized audio from real human speech with an error rate\nof 1.9% and detecting the underlying architecture with an accuracy of 97%.",
          "link": "http://arxiv.org/abs/2107.11412",
          "publishedOn": "2021-07-27T02:03:31.313Z",
          "wordCount": 629,
          "title": "Using Deep Learning Techniques and Inferential Speech Statistics for AI Synthesised Speech Recognition. (arXiv:2107.11412v1 [cs.LG])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2101.00591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>",
          "description": "Correspondence selection aims to correctly select the consistent matches\n(inliers) from an initial set of putative correspondences. The selection is\nchallenging since putative matches are typically extremely unbalanced, largely\ndominated by outliers, and the random distribution of such outliers further\ncomplicates the learning process for learning-based methods. To address this\nissue, we propose to progressively prune the correspondences via a\nlocal-to-global consensus learning procedure. We introduce a ``pruning'' block\nthat lets us identify reliable candidates among the initial matches according\nto consensus scores estimated using local-to-global dynamic graphs. We then\nachieve progressive pruning by stacking multiple pruning blocks sequentially.\nOur method outperforms state-of-the-arts on robust line fitting, camera pose\nestimation and retrieval-based image localization benchmarks by significant\nmargins and shows promising generalization ability to different datasets and\ndetector/descriptor combinations.",
          "link": "http://arxiv.org/abs/2101.00591",
          "publishedOn": "2021-07-30T02:13:30.129Z",
          "wordCount": 607,
          "title": "Progressive Correspondence Pruning by Consensus Learning. (arXiv:2101.00591v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1\">Eugene Valassakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1\">Norman Di Palo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>",
          "description": "In this paper, we study the problem of zero-shot sim-to-real when the task\nrequires both highly precise control with sub-millimetre error tolerance, and\nwide task space generalisation. Our framework involves a coarse-to-fine\ncontroller, where trajectories begin with classical motion planning using\nICP-based pose estimation, and transition to a learned end-to-end controller\nwhich maps images to actions and is trained in simulation with domain\nrandomisation. In this way, we achieve precise control whilst also generalising\nthe controller across wide task spaces, and keeping the robustness of\nvision-based, end-to-end control. Real-world experiments on a range of\ndifferent tasks show that, by exploiting the best of both worlds, our framework\nsignificantly outperforms purely motion planning methods, and purely\nlearning-based methods. Furthermore, we answer a range of questions on best\npractices for precise sim-to-real transfer, such as how different image sensor\nmodalities and image feature representations perform.",
          "link": "http://arxiv.org/abs/2105.11283",
          "publishedOn": "2021-07-30T02:13:29.558Z",
          "wordCount": 626,
          "title": "Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide Task Spaces. (arXiv:2105.11283v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shuquan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Songfang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>",
          "description": "Point cloud segmentation is a fundamental task in 3D. Despite recent progress\non point cloud segmentation with the power of deep networks, current deep\nlearning methods based on the clean label assumptions may fail with noisy\nlabels. Yet, object class labels are often mislabeled in real-world point cloud\ndatasets. In this work, we take the lead in solving this issue by proposing a\nnovel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing\nnoise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with\nthe spatially variant noise rate problem specific to point clouds.\nSpecifically, we propose a novel point-wise confidence selection to obtain\nreliable labels based on the historical predictions of each point. A novel\ncluster-wise label correction is proposed with a voting strategy to generate\nthe best possible label taking the neighbor point correlations into\nconsideration. We conduct extensive experiments to demonstrate the\neffectiveness of PNAL on both synthetic and real-world noisy datasets. In\nparticular, even with $60\\%$ symmetric noisy labels, our proposed method\nproduces much better results than its baseline counterpart without PNAL and is\ncomparable to the ideal upper bound trained on a completely clean dataset.\nMoreover, we fully re-labeled the test set of a popular but noisy real-world\nscene dataset ScanNetV2 to make it clean, for rigorous experiment and future\nresearch. Our code and data will be available at\n\\url{https://shuquanye.com/PNAL_website/}.",
          "link": "http://arxiv.org/abs/2107.14230",
          "publishedOn": "2021-07-30T02:13:29.197Z",
          "wordCount": 683,
          "title": "Learning with Noisy Labels for Robust Point Cloud Segmentation. (arXiv:2107.14230v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luczynski_T/0/1/0/all/0/1\">Tomasz Luczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willners_J/0/1/0/all/0/1\">Jonatan Scharff Willners</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vargas_E/0/1/0/all/0/1\">Elizabeth Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roe_J/0/1/0/all/0/1\">Joshua Roe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shida Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petillot_Y/0/1/0/all/0/1\">Yvan Petillot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>",
          "description": "This paper presents a novel dataset for the development of visual navigation\nand simultaneous localisation and mapping (SLAM) algorithms as well as for\nunderwater intervention tasks. It differs from existing datasets as it contains\nground truth for the vehicle's position captured by an underwater motion\ntracking system. The dataset contains distortion-free and rectified stereo\nimages along with the calibration parameters of the stereo camera setup.\nFurthermore, the experiments were performed and recorded in a controlled\nenvironment, where current and waves could be generated allowing the dataset to\ncover a wide range of conditions - from calm water to waves and currents of\nsignificant strength.",
          "link": "http://arxiv.org/abs/2107.13628",
          "publishedOn": "2021-07-30T02:13:29.140Z",
          "wordCount": 542,
          "title": "Underwater inspection and intervention dataset. (arXiv:2107.13628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shkodrani_S/0/1/0/all/0/1\">Sindi Shkodrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manfredi_M/0/1/0/all/0/1\">Marco Manfredi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baka_N/0/1/0/all/0/1\">N&#xf3;ra Baka</a>",
          "description": "Attempts of learning from hierarchical taxonomies in computer vision have\nbeen mostly focusing on image classification. Though ways of best harvesting\nlearning improvements from hierarchies in classification are far from being\nsolved, there is a need to target these problems in other vision tasks such as\nobject detection. As progress on the classification side is often dependent on\nhierarchical cross-entropy losses, novel detection architectures using sigmoid\nas an output function instead of softmax cannot easily apply these advances,\nrequiring novel methods in detection. In this work we establish a theoretical\nframework based on probability and set theory for extracting parent predictions\nand a hierarchical loss that can be used across tasks, showing results across\nclassification and detection benchmarks and opening up the possibility of\nhierarchical learning for sigmoid-based detection architectures.",
          "link": "http://arxiv.org/abs/2107.13627",
          "publishedOn": "2021-07-30T02:13:29.104Z",
          "wordCount": 574,
          "title": "United We Learn Better: Harvesting Learning Improvements From Class Hierarchies Across Tasks. (arXiv:2107.13627v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "3D object detection is an important capability needed in various practical\napplications such as driver assistance systems. Monocular 3D detection, as an\neconomical solution compared to conventional settings relying on binocular\nvision or LiDAR, has drawn increasing attention recently but still yields\nunsatisfactory results. This paper first presents a systematic study on this\nproblem and observes that the current monocular 3D detection problem can be\nsimplified as an instance depth estimation problem: The inaccurate instance\ndepth blocks all the other 3D attribute predictions from improving the overall\ndetection performance. However, recent methods directly estimate the depth\nbased on isolated instances or pixels while ignoring the geometric relations\nacross different objects, which can be valuable constraints as the key\ninformation about depth is not directly manifest in the monocular image.\nTherefore, we construct geometric relation graphs across predicted objects and\nuse the graph to facilitate depth estimation. As the preliminary depth\nestimation of each instance is usually inaccurate in this ill-posed setting, we\nincorporate a probabilistic representation to capture the uncertainty. It\nprovides an important indicator to identify confident predictions and further\nguide the depth propagation. Despite the simplicity of the basic idea, our\nmethod obtains significant improvements on KITTI and nuScenes benchmarks,\nachieving the 1st place out of all monocular vision-only methods while still\nmaintaining real-time efficiency. Code and models will be released at\nhttps://github.com/open-mmlab/mmdetection3d.",
          "link": "http://arxiv.org/abs/2107.14160",
          "publishedOn": "2021-07-30T02:13:29.036Z",
          "wordCount": 662,
          "title": "Probabilistic and Geometric Depth: Detecting Objects in Perspective. (arXiv:2107.14160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1\">Laura Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_Victor_J/0/1/0/all/0/1\">Jose Santos-Victor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardino_A/0/1/0/all/0/1\">Alexandre Bernardino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>",
          "description": "One-shot action recognition aims to recognize new action categories from a\nsingle reference example, typically referred to as the anchor example. This\nwork presents a novel approach for one-shot action recognition in the wild that\ncomputes motion representations robust to variable kinematic conditions.\nOne-shot action recognition is then performed by evaluating anchor and target\nmotion representations. We also develop a set of complementary steps that boost\nthe action recognition performance in the most challenging scenarios. Our\napproach is evaluated on the public NTU-120 one-shot action recognition\nbenchmark, outperforming previous action recognition models. Besides, we\nevaluate our framework on a real use-case of therapy with autistic people.\nThese recordings are particularly challenging due to high-level artifacts from\nthe patient motion. Our results provide not only quantitative but also online\nqualitative measures, essential for the patient evaluation and monitoring\nduring the actual therapy.",
          "link": "http://arxiv.org/abs/2102.08997",
          "publishedOn": "2021-07-30T02:13:28.989Z",
          "wordCount": 627,
          "title": "One-shot action recognition in challenging therapy scenarios. (arXiv:2102.08997v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baobei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Depth completion deals with the problem of recovering dense depth maps from\nsparse ones, where color images are often used to facilitate this completion.\nRecent approaches mainly focus on image guided learning to predict dense\nresults. However, blurry image guidance and object structures in depth still\nimpede the performance of image guided frameworks. To tackle these problems, we\nexplore a repetitive design in our image guided network to sufficiently and\ngradually recover depth values. Specifically, the repetition is embodied in a\ncolor image guidance branch and a depth generation branch. In the former\nbranch, we design a repetitive hourglass network to extract higher-level image\nfeatures of complex environments, which can provide powerful context guidance\nfor depth prediction. In the latter branch, we design a repetitive guidance\nmodule based on dynamic convolution where the convolution factorization is\napplied to simultaneously reduce its complexity and progressively model\nhigh-frequency structures, e.g., boundaries. Further, in this module, we\npropose an adaptive fusion mechanism to effectively aggregate multi-step depth\nfeatures. Extensive experiments show that our method achieves state-of-the-art\nresult on the NYUv2 dataset and ranks 1st on the KITTI benchmark at the time of\nsubmission.",
          "link": "http://arxiv.org/abs/2107.13802",
          "publishedOn": "2021-07-30T02:13:28.982Z",
          "wordCount": 639,
          "title": "RigNet: Repetitive Image Guided Network for Depth Completion. (arXiv:2107.13802v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zeyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jiaxiang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Runze Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiayu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1\">Chiew-Lan Tai</a>",
          "description": "In recent years, sparse voxel-based methods have become the state-of-the-arts\nfor 3D semantic segmentation of indoor scenes, thanks to the powerful 3D CNNs.\nNevertheless, being oblivious to the underlying geometry, voxel-based methods\nsuffer from ambiguous features on spatially close objects and struggle with\nhandling complex and irregular geometries due to the lack of geodesic\ninformation. In view of this, we present Voxel-Mesh Network (VMNet), a novel 3D\ndeep architecture that operates on the voxel and mesh representations\nleveraging both the Euclidean and geodesic information. Intuitively, the\nEuclidean information extracted from voxels can offer contextual cues\nrepresenting interactions between nearby objects, while the geodesic\ninformation extracted from meshes can help separate objects that are spatially\nclose but have disconnected surfaces. To incorporate such information from the\ntwo domains, we design an intra-domain attentive module for effective feature\naggregation and an inter-domain attentive module for adaptive feature fusion.\nExperimental results validate the effectiveness of VMNet: specifically, on the\nchallenging ScanNet dataset for large-scale segmentation of indoor scenes, it\noutperforms the state-of-the-art SparseConvNet and MinkowskiNet (74.6% vs 72.5%\nand 73.6% in mIoU) with a simpler network structure (17M vs 30M and 38M\nparameters). Code release: https://github.com/hzykent/VMNet",
          "link": "http://arxiv.org/abs/2107.13824",
          "publishedOn": "2021-07-30T02:13:28.957Z",
          "wordCount": 646,
          "title": "VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation. (arXiv:2107.13824v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.05101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Ternary Neural Networks (TNNs) have received much attention due to being\npotentially orders of magnitude faster in inference, as well as more power\nefficient, than full-precision counterparts. However, 2 bits are required to\nencode the ternary representation with only 3 quantization levels leveraged. As\na result, conventional TNNs have similar memory consumption and speed compared\nwith the standard 2-bit models, but have worse representational capability.\nMoreover, there is still a significant gap in accuracy between TNNs and\nfull-precision networks, hampering their deployment to real applications. To\ntackle these two challenges, in this work, we first show that, under some mild\nconstraints, computational complexity of the ternary inner product can be\nreduced by a factor of 2. Second, to mitigate the performance gap, we\nelaborately design an implementation-dependent ternary quantization algorithm.\nThe proposed framework is termed Fast and Accurate Ternary Neural Networks\n(FATNN). Experiments on image classification demonstrate that our FATNN\nsurpasses the state-of-the-arts by a significant margin in accuracy. More\nimportantly, speedup evaluation compared with various precisions is analyzed on\nseveral platforms, which serves as a strong benchmark for further research.",
          "link": "http://arxiv.org/abs/2008.05101",
          "publishedOn": "2021-07-30T02:13:28.948Z",
          "wordCount": 669,
          "title": "FATNN: Fast and Accurate Ternary Neural Networks. (arXiv:2008.05101v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samani_E/0/1/0/all/0/1\">Ekta U. Samani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingjian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Ashis G. Banerjee</a>",
          "description": "Object recognition in unseen indoor environments remains a challenging\nproblem for visual perception of mobile robots. In this letter, we propose the\nuse of topologically persistent features, which rely on the objects' shape\ninformation, to address this challenge. In particular, we extract two kinds of\nfeatures, namely, sparse persistence image (PI) and amplitude, by applying\npersistent homology to multi-directional height function-based filtrations of\nthe cubical complexes representing the object segmentation maps. The features\nare then used to train a fully connected network for recognition. For\nperformance evaluation, in addition to a widely used shape dataset and a\nbenchmark indoor scenes dataset, we collect a new dataset, comprising scene\nimages from two different environments, namely, a living room and a mock\nwarehouse. The scenes are captured using varying camera poses under different\nillumination conditions and include up to five different objects from a given\nset of fourteen objects. On the benchmark indoor scenes dataset, sparse PI\nfeatures show better recognition performance in unseen environments than the\nfeatures learned using the widely used ResNetV2-56 and EfficientNet-B4 models.\nFurther, they provide slightly higher recall and accuracy values than Faster\nR-CNN, an end-to-end object detection method, and its state-of-the-art variant,\nDomain Adaptive Faster R-CNN. The performance of our methods also remains\nrelatively unchanged from the training environment (living room) to the unseen\nenvironment (mock warehouse) in the new dataset. In contrast, the performance\nof the object detection methods drops substantially. We also implement the\nproposed method on a real-world robot to demonstrate its usefulness.",
          "link": "http://arxiv.org/abs/2010.03196",
          "publishedOn": "2021-07-30T02:13:28.943Z",
          "wordCount": 766,
          "title": "Visual Object Recognition in Indoor Environments Using Topologically Persistent Features. (arXiv:2010.03196v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengdi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peijun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>",
          "description": "Accurately describing and detecting 2D and 3D keypoints is crucial to\nestablishing correspondences across images and point clouds. Despite a plethora\nof learning-based 2D or 3D local feature descriptors and detectors having been\nproposed, the derivation of a shared descriptor and joint keypoint detector\nthat directly matches pixels and points remains under-explored by the\ncommunity. This work takes the initiative to establish fine-grained\ncorrespondences between 2D images and 3D point clouds. In order to directly\nmatch pixels and points, a dual fully convolutional framework is presented that\nmaps 2D and 3D inputs into a shared latent representation space to\nsimultaneously describe and detect keypoints. Furthermore, an ultra-wide\nreception mechanism in combination with a novel loss function are designed to\nmitigate the intrinsic information variations between pixel and point local\nregions. Extensive experimental results demonstrate that our framework shows\ncompetitive performance in fine-grained matching between images and point\nclouds and achieves state-of-the-art results for the task of indoor visual\nlocalization. Our source code will be available at [no-name-for-blind-review].",
          "link": "http://arxiv.org/abs/2103.01055",
          "publishedOn": "2021-07-30T02:13:28.934Z",
          "wordCount": 663,
          "title": "P2-Net: Joint Description and Detection of Local Features for Pixel and Point Matching. (arXiv:2103.01055v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tiange Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianhui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>",
          "description": "Discrete point cloud objects lack sufficient shape descriptors of 3D\ngeometries. In this paper, we present a novel method for aggregating\nhypothetical curves in point clouds. Sequences of connected points (curves) are\ninitially grouped by taking guided walks in the point clouds, and then\nsubsequently aggregated back to augment their point-wise features. We provide\nan effective implementation of the proposed aggregation strategy including a\nnovel curve grouping operator followed by a curve aggregation operator. Our\nmethod was benchmarked on several point cloud analysis tasks where we achieved\nthe state-of-the-art classification accuracy of 94.2% on the ModelNet40\nclassification task, instance IoU of 86.8 on the ShapeNetPart segmentation\ntask, and cosine error of 0.11 on the ModelNet40 normal estimation task.",
          "link": "http://arxiv.org/abs/2105.01288",
          "publishedOn": "2021-07-30T02:13:28.914Z",
          "wordCount": 592,
          "title": "Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis. (arXiv:2105.01288v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wehrbein_T/0/1/0/all/0/1\">Tom Wehrbein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1\">Marco Rudolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>",
          "description": "3D human pose estimation from monocular images is a highly ill-posed problem\ndue to depth ambiguities and occlusions. Nonetheless, most existing works\nignore these ambiguities and only estimate a single solution. In contrast, we\ngenerate a diverse set of hypotheses that represents the full posterior\ndistribution of feasible 3D poses. To this end, we propose a normalizing flow\nbased method that exploits the deterministic 3D-to-2D mapping to solve the\nambiguous inverse 2D-to-3D problem. Additionally, uncertain detections and\nocclusions are effectively modeled by incorporating uncertainty information of\nthe 2D detector as condition. Further keys to success are a learned 3D pose\nprior and a generalization of the best-of-M loss. We evaluate our approach on\nthe two benchmark datasets Human3.6M and MPI-INF-3DHP, outperforming all\ncomparable methods in most metrics. The implementation is available on GitHub.",
          "link": "http://arxiv.org/abs/2107.13788",
          "publishedOn": "2021-07-30T02:13:28.908Z",
          "wordCount": 575,
          "title": "Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows. (arXiv:2107.13788v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianxiao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongbin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shane Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinmei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiuda Yu</a>",
          "description": "Monocular depth estimation and semantic segmentation are two fundamental\ngoals of scene understanding. Due to the advantages of task interaction, many\nworks study the joint task learning algorithm. However, most existing methods\nfail to fully leverage the semantic labels, ignoring the provided context\nstructures and only using them to supervise the prediction of segmentation\nsplit. In this paper, we propose a network injected with contextual information\n(CI-Net) to solve the problem. Specifically, we introduce self-attention block\nin the encoder to generate attention map. With supervision from the ground\ntruth created by semantic labels, the network is embedded with contextual\ninformation so that it could understand the scene better, utilizing dependent\nfeatures to make accurate prediction. Besides, a feature sharing module is\nconstructed to make the task-specific features deeply fused and a consistency\nloss is devised to make the features mutually guided. We evaluate the proposed\nCI-Net on the NYU-Depth-v2 and SUN-RGBD datasets. The experimental results\nvalidate that our proposed CI-Net is competitive with the state-of-the-arts.",
          "link": "http://arxiv.org/abs/2107.13800",
          "publishedOn": "2021-07-30T02:13:28.902Z",
          "wordCount": 615,
          "title": "CI-Net: Contextual Information for Joint Semantic Segmentation and Depth Estimation. (arXiv:2107.13800v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>",
          "description": "With the rapid development of measurement technology, LiDAR and depth cameras\nare widely used in the perception of the 3D environment. Recent learning based\nmethods for robot perception most focus on the image or video, but deep\nlearning methods for dynamic 3D point cloud sequences are underexplored.\nTherefore, developing efficient and accurate perception method compatible with\nthese advanced instruments is pivotal to autonomous driving and service robots.\nAn Anchor-based Spatio-Temporal Attention 3D Convolution operation (ASTA3DConv)\nis proposed in this paper to process dynamic 3D point cloud sequences. The\nproposed convolution operation builds a regular receptive field around each\npoint by setting several virtual anchors around each point. The features of\nneighborhood points are firstly aggregated to each anchor based on the\nspatio-temporal attention mechanism. Then, anchor-based 3D convolution is\nadopted to aggregate these anchors' features to the core points. The proposed\nmethod makes better use of the structured information within the local region\nand learns spatio-temporal embedding features from dynamic 3D point cloud\nsequences. Anchor-based Spatio-Temporal Attention 3D Convolutional Neural\nNetworks (ASTA3DCNNs) are built for classification and segmentation tasks based\non the proposed ASTA3DConv and evaluated on action recognition and semantic\nsegmentation tasks. The experiments and ablation studies on MSRAction3D and\nSynthia datasets demonstrate the superior performance and effectiveness of our\nmethod for dynamic 3D point cloud sequences. Our method achieves the\nstate-of-the-art performance among the methods with dynamic 3D point cloud\nsequences as input on MSRAction3D and Synthia datasets.",
          "link": "http://arxiv.org/abs/2012.10860",
          "publishedOn": "2021-07-30T02:13:28.895Z",
          "wordCount": 724,
          "title": "Anchor-Based Spatio-Temporal Attention 3D Convolutional Networks for Dynamic 3D Point Cloud Sequences. (arXiv:2012.10860v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Compared with the visual grounding on 2D images, the natural-language-guided\n3D object localization on point clouds is more challenging. In this paper, we\npropose a new model, named InstanceRefer, to achieve a superior 3D visual\ngrounding through the grounding-by-matching strategy. In practice, our model\nfirst predicts the target category from the language descriptions using a\nsimple language classification model. Then, based on the category, our model\nsifts out a small number of instance candidates (usually less than 20) from the\npanoptic segmentation of point clouds. Thus, the non-trivial 3D visual\ngrounding task has been effectively re-formulated as a simplified\ninstance-matching problem, considering that instance-level candidates are more\nrational than the redundant 3D object proposals. Subsequently, for each\ncandidate, we perform the multi-level contextual inference, i.e., referring\nfrom instance attribute perception, instance-to-instance relation perception,\nand instance-to-background global localization perception, respectively.\nEventually, the most relevant candidate is selected and localized by ranking\nconfidence scores, which are obtained by the cooperative holistic\nvisual-language feature matching. Experiments confirm that our method\noutperforms previous state-of-the-arts on ScanRefer online benchmark and\nNr3D/Sr3D datasets.",
          "link": "http://arxiv.org/abs/2103.01128",
          "publishedOn": "2021-07-30T02:13:28.879Z",
          "wordCount": 675,
          "title": "InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring. (arXiv:2103.01128v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiongchao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatemi_A/0/1/0/all/0/1\">Arezou Fatemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lira_W/0/1/0/all/0/1\">Wallace Lira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fenggen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_B/0/1/0/all/0/1\">Biao Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "We introduce RaidaR, a rich annotated image dataset of rainy street scenes,\nto support autonomous driving research. The new dataset contains the largest\nnumber of rainy images (58,542) to date, 5,000 of which provide semantic\nsegmentations and 3,658 provide object instance segmentations. The RaidaR\nimages cover a wide range of realistic rain-induced artifacts, including fog,\ndroplets, and road reflections, which can effectively augment existing street\nscene datasets to improve data-driven machine perception during rainy weather.\nTo facilitate efficient annotation of a large volume of images, we develop a\nsemi-automatic scheme combining manual segmentation and an automated processing\nakin to cross validation, resulting in 10-20 fold reduction on annotation time.\nWe demonstrate the utility of our new dataset by showing how data augmentation\nwith RaidaR can elevate the accuracy of existing segmentation algorithms. We\nalso present a novel unpaired image-to-image translation algorithm for\nadding/removing rain artifacts, which directly benefits from RaidaR.",
          "link": "http://arxiv.org/abs/2104.04606",
          "publishedOn": "2021-07-30T02:13:28.873Z",
          "wordCount": 635,
          "title": "RaidaR: A Rich Annotated Image Dataset of Rainy Street Scenes. (arXiv:2104.04606v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Willes_J/0/1/0/all/0/1\">John Willes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_J/0/1/0/all/0/1\">James Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harakeh_A/0/1/0/all/0/1\">Ali Harakeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1\">Steven Waslander</a>",
          "description": "As autonomous decision-making agents move from narrow operating environments\nto unstructured worlds, learning systems must move from a closed-world\nformulation to an open-world and few-shot setting in which agents continuously\nlearn new classes from small amounts of information. This stands in stark\ncontrast to modern machine learning systems that are typically designed with a\nknown set of classes and a large number of examples for each class. In this\nwork we extend embedding-based few-shot learning algorithms to the open-world\nrecognition setting. We combine Bayesian non-parametric class priors with an\nembedding-based pre-training scheme to yield a highly flexible framework which\nwe refer to as few-shot learning for open world recognition (FLOWR). We\nbenchmark our framework on open-world extensions of the common MiniImageNet and\nTieredImageNet few-shot learning datasets. Our results show, compared to prior\nmethods, strong classification accuracy performance and up to a 12% improvement\nin H-measure (a measure of novel class detection) from our non-parametric\nopen-world few-shot learning scheme.",
          "link": "http://arxiv.org/abs/2107.13682",
          "publishedOn": "2021-07-30T02:13:28.867Z",
          "wordCount": 596,
          "title": "Bayesian Embeddings for Few-Shot Open World Recognition. (arXiv:2107.13682v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenpeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuemiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>",
          "description": "Existing GAN inversion methods are stuck in a paradox that the inverted codes\ncan either achieve high-fidelity reconstruction, or retain the editing\ncapability. Having only one of them clearly cannot realize real image editing.\nIn this paper, we resolve this paradox by introducing consecutive images (\\eg,\nvideo frames or the same person with different poses) into the inversion\nprocess. The rationale behind our solution is that the continuity of\nconsecutive images leads to inherent editable directions. This inborn property\nis used for two unique purposes: 1) regularizing the joint inversion process,\nsuch that each of the inverted code is semantically accessible from one of the\nother and fastened in a editable domain; 2) enforcing inter-image coherence,\nsuch that the fidelity of each inverted code can be maximized with the\ncomplement of other images. Extensive experiments demonstrate that our\nalternative significantly outperforms state-of-the-art methods in terms of\nreconstruction fidelity and editability on both the real image dataset and\nsynthesis dataset. Furthermore, our method provides the first support of\nvideo-based GAN inversion, and an interesting application of unsupervised\nsemantic transfer from consecutive images. Source code can be found at:\n\\url{https://github.com/Qingyang-Xu/InvertingGANs_with_ConsecutiveImgs}.",
          "link": "http://arxiv.org/abs/2107.13812",
          "publishedOn": "2021-07-30T02:13:28.852Z",
          "wordCount": 636,
          "title": "From Continuity to Editability: Inverting GANs with Consecutive Images. (arXiv:2107.13812v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raman_C/0/1/0/all/0/1\">Chirag Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1\">Hayley Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1\">Marco Loog</a>",
          "description": "The default paradigm for the forecasting of human behavior in social\nconversations is characterized by top-down approaches. These involve\nidentifying predictive relationships between low level nonverbal cues and\nfuture semantic events of interest (e.g. turn changes, group leaving). A common\nhurdle however, is the limited availability of labeled data for supervised\nlearning. In this work, we take the first step in the direction of a bottom-up\nself-supervised approach in the domain. We formulate the task of Social Cue\nForecasting to leverage the larger amount of unlabeled low-level behavior cues,\nand characterize the modeling challenges involved. To address these, we take a\nmeta-learning approach and propose the Social Process (SP) models--socially\naware sequence-to-sequence (Seq2Seq) models within the Neural Process (NP)\nfamily. SP models learn extractable representations of non-semantic future cues\nfor each participant, while capturing global uncertainty by jointly reasoning\nabout the future for all members of the group. Evaluation on synthesized and\nreal-world behavior data shows that our SP models achieve higher log-likelihood\nthan the NP baselines, and also highlights important considerations for\napplying such techniques within the domain of social human interactions.",
          "link": "http://arxiv.org/abs/2107.13576",
          "publishedOn": "2021-07-30T02:13:28.836Z",
          "wordCount": 628,
          "title": "Social Processes: Self-Supervised Forecasting of Nonverbal Cues in Social Conversations. (arXiv:2107.13576v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangyin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qingpei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yong Tan</a>",
          "description": "Reconstructing a high-precision and high-fidelity 3D human hand from a color\nimage plays a central role in replicating a realistic virtual hand in\nhuman-computer interaction and virtual reality applications. The results of\ncurrent methods are lacking in accuracy and fidelity due to various hand poses\nand severe occlusions. In this study, we propose an I2UV-HandNet model for\naccurate hand pose and shape estimation as well as 3D hand super-resolution\nreconstruction. Specifically, we present the first UV-based 3D hand shape\nrepresentation. To recover a 3D hand mesh from an RGB image, we design an\nAffineNet to predict a UV position map from the input in an image-to-image\ntranslation fashion. To obtain a higher fidelity shape, we exploit an\nadditional SRNet to transform the low-resolution UV map outputted by AffineNet\ninto a high-resolution one. For the first time, we demonstrate the\ncharacterization capability of the UV-based hand shape representation. Our\nexperiments show that the proposed method achieves state-of-the-art performance\non several challenging benchmarks.",
          "link": "http://arxiv.org/abs/2102.03725",
          "publishedOn": "2021-07-30T02:13:28.830Z",
          "wordCount": 648,
          "title": "I2UV-HandNet: Image-to-UV Prediction Network for Accurate and High-fidelity 3D Hand Mesh Modeling. (arXiv:2102.03725v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xianyu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Ziwei Liao</a>",
          "description": "Plane feature is a kind of stable landmark to reduce drift error in SLAM\nsystem. It is easy and fast to extract planes from dense point cloud, which is\ncommonly acquired from RGB-D camera or lidar. But for stereo camera, it is hard\nto compute dense point cloud accurately and efficiently. In this paper, we\npropose a novel method to compute plane parameters using intersecting lines\nwhich are extracted from the stereo image. The plane features commonly exist on\nthe surface of man-made objects and structure, which have regular shape and\nstraight edge lines. In 3D space, two intersecting lines can determine such a\nplane. Thus we extract line segments from both stereo left and right image. By\nstereo matching, we compute the endpoints and line directions in 3D space, and\nthen the planes from two intersecting lines. We discard those inaccurate plane\nfeatures in the frame tracking. Adding such plane features in stereo SLAM\nsystem reduces the drift error and refines the performance. We test our\nproposed system on public datasets and demonstrate its robust and accurate\nestimation results, compared with state-of-the-art SLAM systems. To benefit the\nresearch of plane-based SLAM, we release our codes at\nhttps://github.com/fishmarch/Stereo-Plane-SLAM.",
          "link": "http://arxiv.org/abs/2008.08218",
          "publishedOn": "2021-07-30T02:13:28.808Z",
          "wordCount": 675,
          "title": "Stereo Plane SLAM Based on Intersecting Lines. (arXiv:2008.08218v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingtian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1\">Chen Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zizhuang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yisong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoping Wang</a>",
          "description": "3D reconstruction has lately attracted increasing attention due to its wide\napplication in many areas, such as autonomous driving, robotics and virtual\nreality. As a dominant technique in artificial intelligence, deep learning has\nbeen successfully adopted to solve various computer vision problems. However,\ndeep learning for 3D reconstruction is still at its infancy due to its unique\nchallenges and varying pipelines. To stimulate future research, this paper\npresents a review of recent progress in deep learning methods for Multi-view\nStereo (MVS), which is considered as a crucial task of image-based 3D\nreconstruction. It also presents comparative results on several publicly\navailable datasets, with insightful observations and inspiring future research\ndirections.",
          "link": "http://arxiv.org/abs/2106.15328",
          "publishedOn": "2021-07-30T02:13:28.801Z",
          "wordCount": 568,
          "title": "Deep Learning for Multi-View Stereo via Plane Sweep: A Survey. (arXiv:2106.15328v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "We introduce a new image segmentation task, termed Entity Segmentation (ES)\nwith the aim to segment all visual entities in an image without considering\nsemantic category labels. It has many practical applications in image\nmanipulation/editing where the segmentation mask quality is typically crucial\nbut category labels are less important. In this setting, all\nsemantically-meaningful segments are equally treated as categoryless entities\nand there is no thing-stuff distinction. Based on our unified entity\nrepresentation, we propose a center-based entity segmentation framework with\ntwo novel modules to improve mask quality. Experimentally, both our new task\nand framework demonstrate superior advantages as against existing work. In\nparticular, ES enables the following: (1) merging multiple datasets to form a\nlarge training set without the need to resolve label conflicts; (2) any model\ntrained on one dataset can generalize exceptionally well to other datasets with\nunseen domains. Our code is made publicly available at\nhttps://github.com/dvlab-research/Entity.",
          "link": "http://arxiv.org/abs/2107.14228",
          "publishedOn": "2021-07-30T02:13:28.778Z",
          "wordCount": 595,
          "title": "Open-World Entity Segmentation. (arXiv:2107.14228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.14119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_N/0/1/0/all/0/1\">Nadav Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_I/0/1/0/all/0/1\">Itamar Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1\">Matan Protter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>",
          "description": "In a typical multi-label setting, a picture contains on average few positive\nlabels, and many negative ones. This positive-negative imbalance dominates the\noptimization process, and can lead to under-emphasizing gradients from positive\nlabels during training, resulting in poor accuracy. In this paper, we introduce\na novel asymmetric loss (\"ASL\"), which operates differently on positive and\nnegative samples. The loss enables to dynamically down-weights and\nhard-thresholds easy negative samples, while also discarding possibly\nmislabeled samples. We demonstrate how ASL can balance the probabilities of\ndifferent samples, and how this balancing is translated to better mAP scores.\nWith ASL, we reach state-of-the-art results on multiple popular multi-label\ndatasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate\nASL applicability for other tasks, such as single-label classification and\nobject detection. ASL is effective, easy to implement, and does not increase\nthe training time or complexity.\n\nImplementation is available at: https://github.com/Alibaba-MIIL/ASL.",
          "link": "http://arxiv.org/abs/2009.14119",
          "publishedOn": "2021-07-30T02:13:28.754Z",
          "wordCount": 650,
          "title": "Asymmetric Loss For Multi-Label Classification. (arXiv:2009.14119v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerri_P/0/1/0/all/0/1\">Pietro Cerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>",
          "description": "Image-to-image translation (i2i) networks suffer from entanglement effects in\npresence of physics-related phenomena in target domain (such as occlusions,\nfog, etc), thus lowering the translation quality and variability. In this\npaper, we present a comprehensive method for disentangling physics-based traits\nin the translation, guiding the learning process with neural or physical\nmodels. For the latter, we integrate adversarial estimation and genetic\nalgorithms to correctly achieve disentanglement. The results show our approach\ndramatically increase performances in many challenging scenarios for image\ntranslation.",
          "link": "http://arxiv.org/abs/2107.14229",
          "publishedOn": "2021-07-30T02:13:28.747Z",
          "wordCount": 524,
          "title": "Guided Disentanglement in Generative Networks. (arXiv:2107.14229v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1\">Nuoxing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liangliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>",
          "description": "Trajectory prediction is confronted with the dilemma to capture the\nmulti-modal nature of future dynamics with both diversity and accuracy. In this\npaper, we present a distribution discrimination (DisDis) method to predict\npersonalized motion patterns by distinguishing the potential distributions.\nMotivated by that the motion pattern of each person is personalized due to\nhis/her habit, our DisDis learns the latent distribution to represent different\nmotion patterns and optimize it by the contrastive discrimination. This\ndistribution discrimination encourages latent distributions to be more\ndiscriminative. Our method can be integrated with existing multi-modal\nstochastic predictive models as a plug-and-play module to learn the more\ndiscriminative latent distribution. To evaluate the latent distribution, we\nfurther propose a new metric, probability cumulative minimum distance (PCMD)\ncurve, which cumulatively calculates the minimum distance on the sorted\nprobabilities. Experimental results on the ETH and UCY datasets show the\neffectiveness of our method.",
          "link": "http://arxiv.org/abs/2107.14204",
          "publishedOn": "2021-07-30T02:13:28.729Z",
          "wordCount": 589,
          "title": "Personalized Trajectory Prediction via Distribution Discrimination. (arXiv:2107.14204v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1\">Eddie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>",
          "description": "Data augmentations are important ingredients in the recipe for training\nrobust neural networks, especially in computer vision. A fundamental question\nis whether neural network features encode data augmentation transformations. To\nanswer this question, we introduce a systematic approach to investigate which\nlayers of neural networks are the most predictive of augmentation\ntransformations. Our approach uses features in pre-trained vision models with\nminimal additional processing to predict common properties transformed by\naugmentation (scale, aspect ratio, hue, saturation, contrast, and brightness).\nSurprisingly, neural network features not only predict data augmentation\ntransformations, but they predict many transformations with high accuracy.\nAfter validating that neural networks encode features corresponding to\naugmentation transformations, we show that these features are encoded in the\nearly layers of modern CNNs, though the augmentation signal fades in deeper\nlayers.",
          "link": "http://arxiv.org/abs/2003.08773",
          "publishedOn": "2021-07-30T02:13:28.721Z",
          "wordCount": 601,
          "title": "Do CNNs Encode Data Augmentations?. (arXiv:2003.08773v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thoduka_S/0/1/0/all/0/1\">Santosh Thoduka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ploger_P/0/1/0/all/0/1\">Paul G. Pl&#xf6;ger</a>",
          "description": "Execution monitoring is essential for robots to detect and respond to\nfailures. Since it is impossible to enumerate all failures for a given task, we\nlearn from successful executions of the task to detect visual anomalies during\nruntime. Our method learns to predict the motions that occur during the nominal\nexecution of a task, including camera and robot body motion. A probabilistic\nU-Net architecture is used to learn to predict optical flow, and the robot's\nkinematics and 3D model are used to model camera and body motion. The errors\nbetween the observed and predicted motion are used to calculate an anomaly\nscore. We evaluate our method on a dataset of a robot placing a book on a\nshelf, which includes anomalies such as falling books, camera occlusions, and\nrobot disturbances. We find that modeling camera and body motion, in addition\nto the learning-based optical flow prediction, results in an improvement of the\narea under the receiver operating characteristic curve from 0.752 to 0.804, and\nthe area under the precision-recall curve from 0.467 to 0.549.",
          "link": "http://arxiv.org/abs/2107.14206",
          "publishedOn": "2021-07-30T02:13:28.698Z",
          "wordCount": 624,
          "title": "Using Visual Anomaly Detection for Task Execution Monitoring. (arXiv:2107.14206v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>",
          "description": "Most action recognition solutions rely on dense sampling to precisely cover\nthe informative temporal clip. Extensively searching temporal region is\nexpensive for a real-world application. In this work, we focus on improving the\ninference efficiency of current action recognition backbones on trimmed videos,\nand illustrate that one action model can also cover then informative region by\ndropping non-informative features. We present Selective Feature Compression\n(SFC), an action recognition inference strategy that greatly increase model\ninference efficiency without any accuracy compromise. Differently from previous\nworks that compress kernel sizes and decrease the channel dimension, we propose\nto compress feature flow at spatio-temporal dimension without changing any\nbackbone parameters. Our experiments on Kinetics-400, UCF101 and ActivityNet\nshow that SFC is able to reduce inference speed by 6-7x and memory usage by\n5-6x compared with the commonly used 30 crops dense sampling procedure, while\nalso slightly improving Top1 Accuracy. We thoroughly quantitatively and\nqualitatively evaluate SFC and all its components and show how does SFC learn\nto attend to important video regions and to drop temporal features that are\nuninformative for the task of action recognition.",
          "link": "http://arxiv.org/abs/2104.00179",
          "publishedOn": "2021-07-30T02:13:28.681Z",
          "wordCount": 654,
          "title": "Selective Feature Compression for Efficient Activity Recognition Inference. (arXiv:2104.00179v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1\">I&#xf1;igo Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>",
          "description": "Hand action recognition is a special case of action recognition with\napplications in human-robot interaction, virtual reality or life-logging\nsystems. Building action classifiers able to work for such heterogeneous action\ndomains is very challenging. There are very subtle changes across different\nactions from a given application but also large variations across domains (e.g.\nvirtual reality vs life-logging). This work introduces a novel skeleton-based\nhand motion representation model that tackles this problem. The framework we\npropose is agnostic to the application domain or camera recording view-point.\nWhen working on a single domain (intra-domain action classification) our\napproach performs better or similar to current state-of-the-art methods on\nwell-known hand action recognition benchmarks. And, more importantly, when\nperforming hand action recognition for action domains and camera perspectives\nwhich our approach has not been trained for (cross-domain action\nclassification), our proposed framework achieves comparable performance to\nintra-domain state-of-the-art methods. These experiments show the robustness\nand generalization capabilities of our framework.",
          "link": "http://arxiv.org/abs/2103.02303",
          "publishedOn": "2021-07-30T02:13:28.658Z",
          "wordCount": 619,
          "title": "Domain and View-point Agnostic Hand Action Recognition. (arXiv:2103.02303v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Smaranjit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Kanishka Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nethaji_N/0/1/0/all/0/1\">Niketha Nethaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Shivam Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_A/0/1/0/all/0/1\">Arnab Dutta Purkayastha</a>",
          "description": "Tourism in India plays a quintessential role in the country's economy with an\nestimated 9.2% GDP share for the year 2018. With a yearly growth rate of 6.2%,\nthe industry holds a huge potential for being the primary driver of the economy\nas observed in the nations of the Middle East like the United Arab Emirates.\nThe historical and cultural diversity exhibited throughout the geography of the\nnation is a unique spectacle for people around the world and therefore serves\nto attract tourists in tens of millions in number every year. Traditionally,\ntour guides or academic professionals who study these heritage monuments were\nresponsible for providing information to the visitors regarding their\narchitectural and historical significance. However, unfortunately this system\nhas several caveats when considered on a large scale such as unavailability of\nsufficient trained people, lack of accurate information, failure to convey the\nrichness of details in an attractive format etc. Recently, machine learning\napproaches revolving around the usage of monument pictures have been shown to\nbe useful for rudimentary analysis of heritage sights. This paper serves as a\nsurvey of the research endeavors undertaken in this direction which would\neventually provide insights for building an automated decision system that\ncould be utilized to make the experience of tourism in India more modernized\nfor visitors.",
          "link": "http://arxiv.org/abs/2107.14070",
          "publishedOn": "2021-07-30T02:13:28.638Z",
          "wordCount": 690,
          "title": "Machine Learning Advances aiding Recognition and Classification of Indian Monuments and Landmarks. (arXiv:2107.14070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuewen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingru Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>",
          "description": "Image captioning is shown to be able to achieve a better performance by using\nscene graphs to represent the relations of objects in the image. The current\ncaptioning encoders generally use a Graph Convolutional Net (GCN) to represent\nthe relation information and merge it with the object region features via\nconcatenation or convolution to get the final input for sentence decoding.\nHowever, the GCN-based encoders in the existing methods are less effective for\ncaptioning due to two reasons. First, using the image captioning as the\nobjective (i.e., Maximum Likelihood Estimation) rather than a relation-centric\nloss cannot fully explore the potential of the encoder. Second, using a\npre-trained model instead of the encoder itself to extract the relationships is\nnot flexible and cannot contribute to the explainability of the model. To\nimprove the quality of image captioning, we propose a novel architecture\nReFormer -- a RElational transFORMER to generate features with relation\ninformation embedded and to explicitly express the pair-wise relationships\nbetween objects in the image. ReFormer incorporates the objective of scene\ngraph generation with that of image captioning using one modified Transformer\nmodel. This design allows ReFormer to generate not only better image captions\nwith the bene-fit of extracting strong relational image features, but also\nscene graphs to explicitly describe the pair-wise relation-ships. Experiments\non publicly available datasets show that our model significantly outperforms\nstate-of-the-art methods on image captioning and scene graph generation",
          "link": "http://arxiv.org/abs/2107.14178",
          "publishedOn": "2021-07-30T02:13:28.600Z",
          "wordCount": 664,
          "title": "ReFormer: The Relational Transformer for Image Captioning. (arXiv:2107.14178v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eugene Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Cheng-Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yi Lee</a>",
          "description": "Deep neural networks (DNNs) are known to perform well when deployed to test\ndistributions that shares high similarity with the training distribution.\nFeeding DNNs with new data sequentially that were unseen in the training\ndistribution has two major challenges -- fast adaptation to new tasks and\ncatastrophic forgetting of old tasks. Such difficulties paved way for the\non-going research on few-shot learning and continual learning. To tackle these\nproblems, we introduce Attentive Independent Mechanisms (AIM). We incorporate\nthe idea of learning using fast and slow weights in conjunction with the\ndecoupling of the feature extraction and higher-order conceptual learning of a\nDNN. AIM is designed for higher-order conceptual learning, modeled by a mixture\nof experts that compete to learn independent concepts to solve a new task. AIM\nis a modular component that can be inserted into existing deep learning\nframeworks. We demonstrate its capability for few-shot learning by adding it to\nSIB and trained on MiniImageNet and CIFAR-FS, showing significant improvement.\nAIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and\nMiniImageNet to demonstrate its capability in continual learning. Code made\npublicly available at https://github.com/huang50213/AIM-Fewshot-Continual.",
          "link": "http://arxiv.org/abs/2107.14053",
          "publishedOn": "2021-07-30T02:13:28.594Z",
          "wordCount": 643,
          "title": "Few-Shot and Continual Learning with Attentive Independent Mechanisms. (arXiv:2107.14053v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1\">Nergis Tomen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>",
          "description": "Convolutional layers in CNNs implement linear filters which decompose the\ninput into different frequency bands. However, most modern architectures\nneglect standard principles of filter design when optimizing their model\nchoices regarding the size and shape of the convolutional kernel. In this work,\nwe consider the well-known problem of spectral leakage caused by windowing\nartifacts in filtering operations in the context of CNNs. We show that the\nsmall size of CNN kernels make them susceptible to spectral leakage, which may\ninduce performance-degrading artifacts. To address this issue, we propose the\nuse of larger kernel sizes along with the Hamming window function to alleviate\nleakage in CNN architectures. We demonstrate improved classification accuracy\non multiple benchmark datasets including Fashion-MNIST, CIFAR-10, CIFAR-100 and\nImageNet with the simple use of a standard window function in convolutional\nlayers. Finally, we show that CNNs employing the Hamming window display\nincreased robustness against various adversarial attacks.",
          "link": "http://arxiv.org/abs/2101.10143",
          "publishedOn": "2021-07-30T02:13:28.535Z",
          "wordCount": 615,
          "title": "Spectral Leakage and Rethinking the Kernel Size in CNNs. (arXiv:2101.10143v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan C. P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeanneret_G/0/1/0/all/0/1\">Guillaume Jeanneret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueda_L/0/1/0/all/0/1\">Laura Rueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbel&#xe1;ez</a>",
          "description": "Deep learning models are prone to being fooled by imperceptible perturbations\nknown as adversarial attacks. In this work, we study how equipping models with\nTest-time Transformation Ensembling (TTE) can work as a reliable defense\nagainst such attacks. While transforming the input data, both at train and test\ntimes, is known to enhance model performance, its effects on adversarial\nrobustness have not been studied. Here, we present a comprehensive empirical\nstudy of the impact of TTE, in the form of widely-used image transforms, on\nadversarial robustness. We show that TTE consistently improves model robustness\nagainst a variety of powerful attacks without any need for re-training, and\nthat this improvement comes at virtually no trade-off with accuracy on clean\nsamples. Finally, we show that the benefits of TTE transfer even to the\ncertified robustness domain, in which TTE provides sizable and consistent\nimprovements.",
          "link": "http://arxiv.org/abs/2107.14110",
          "publishedOn": "2021-07-30T02:13:28.493Z",
          "wordCount": 588,
          "title": "Enhancing Adversarial Robustness via Test-time Transformation Ensembling. (arXiv:2107.14110v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hongyang Chao</a>",
          "description": "Relative position encoding (RPE) is important for transformer to capture\nsequence ordering of input tokens. General efficacy has been proven in natural\nlanguage processing. However, in computer vision, its efficacy is not well\nstudied and even remains controversial, e.g., whether relative position\nencoding can work equally well as absolute position? In order to clarify this,\nwe first review existing relative position encoding methods and analyze their\npros and cons when applied in vision transformers. We then propose new relative\nposition encoding methods dedicated to 2D images, called image RPE (iRPE). Our\nmethods consider directional relative distance modeling as well as the\ninteractions between queries and relative position embeddings in self-attention\nmechanism. The proposed iRPE methods are simple and lightweight. They can be\neasily plugged into transformer blocks. Experiments demonstrate that solely due\nto the proposed encoding methods, DeiT and DETR obtain up to 1.5% (top-1 Acc)\nand 1.3% (mAP) stable improvements over their original versions on ImageNet and\nCOCO respectively, without tuning any extra hyperparameters such as learning\nrate and weight decay. Our ablation and analysis also yield interesting\nfindings, some of which run counter to previous understanding. Code and models\nare open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE.",
          "link": "http://arxiv.org/abs/2107.14222",
          "publishedOn": "2021-07-30T02:13:28.486Z",
          "wordCount": 641,
          "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer. (arXiv:2107.14222v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_N/0/1/0/all/0/1\">Nikhil Woodruff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enshaei_A/0/1/0/all/0/1\">Amir Enshaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_B/0/1/0/all/0/1\">Bashar Awwad Shiekh Hasan</a>",
          "description": "Signatures present on corporate documents are often used in investigations of\nrelationships between persons of interest, and prior research into the task of\noffline signature verification has evaluated a wide range of methods on\nstandard signature datasets. However, such tasks often benefit from prior human\nsupervision in the collection, adjustment and labelling of isolated signature\nimages from which all real-world context has been removed. Signatures found in\nonline document repositories such as the United Kingdom Companies House\nregularly contain high variation in location, size, quality and degrees of\nobfuscation under stamps. We propose an integrated pipeline of signature\nextraction and curation, with no human assistance from the obtaining of company\ndocuments to the clustering of individual signatures. We use a sequence of\nheuristic methods, convolutional neural networks, generative adversarial\nnetworks and convolutional Siamese networks for signature extraction,\nfiltering, cleaning and embedding respectively. We evaluate both the\neffectiveness of the pipeline at matching obscured same-author signature pairs\nand the effectiveness of the entire pipeline against a human baseline for\ndocument signature analysis, as well as presenting uses for such a pipeline in\nthe field of real-world anti-money laundering investigation.",
          "link": "http://arxiv.org/abs/2107.14091",
          "publishedOn": "2021-07-30T02:13:28.479Z",
          "wordCount": 629,
          "title": "Fully-Automatic Pipeline for Document Signature Analysis to Detect Money Laundering Activities. (arXiv:2107.14091v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangrui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chongruo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>",
          "description": "Semantic segmentation is a challenging problem due to difficulties in\nmodeling context in complex scenes and class confusions along boundaries. Most\nliterature either focuses on context modeling or boundary refinement, which is\nless generalizable in open-world scenarios. In this work, we advocate a unified\nframework(UN-EPT) to segment objects by considering both context information\nand boundary artifacts. We first adapt a sparse sampling strategy to\nincorporate the transformer-based attention mechanism for efficient context\nmodeling. In addition, a separate spatial branch is introduced to capture image\ndetails for boundary refinement. The whole model can be trained in an\nend-to-end manner. We demonstrate promising performance on three popular\nbenchmarks for semantic segmentation with low memory footprint. Code will be\nreleased soon.",
          "link": "http://arxiv.org/abs/2107.14209",
          "publishedOn": "2021-07-30T02:13:28.472Z",
          "wordCount": 558,
          "title": "A Unified Efficient Pyramid Transformer for Semantic Segmentation. (arXiv:2107.14209v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.03972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_H/0/1/0/all/0/1\">Haizhou Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zijie Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuang Liu</a>",
          "description": "Estimating 3D poses of multiple humans in real-time is a classic but still\nchallenging task in computer vision. Its major difficulty lies in the ambiguity\nin cross-view association of 2D poses and the huge state space when there are\nmultiple people in multiple views. In this paper, we present a novel solution\nfor multi-human 3D pose estimation from multiple calibrated camera views. It\ntakes 2D poses in different camera coordinates as inputs and aims for the\naccurate 3D poses in the global coordinate. Unlike previous methods that\nassociate 2D poses among all pairs of views from scratch at every frame, we\nexploit the temporal consistency in videos to match the 2D inputs with 3D poses\ndirectly in 3-space. More specifically, we propose to retain the 3D pose for\neach person and update them iteratively via the cross-view multi-human\ntracking. This novel formulation improves both accuracy and efficiency, as we\ndemonstrated on widely-used public datasets. To further verify the scalability\nof our method, we propose a new large-scale multi-human dataset with 12 to 28\ncamera views. Without bells and whistles, our solution achieves 154 FPS on 12\ncameras and 34 FPS on 28 cameras, indicating its ability to handle large-scale\nreal-world applications. The proposed dataset is released at\nhttps://github.com/longcw/crossview_3d_pose_tracking.",
          "link": "http://arxiv.org/abs/2003.03972",
          "publishedOn": "2021-07-30T02:13:28.456Z",
          "wordCount": 703,
          "title": "Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS. (arXiv:2003.03972v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14175",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Basty_N/0/1/0/all/0/1\">Nicolas Basty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thanaj_M/0/1/0/all/0/1\">Marjola Thanaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cule_M/0/1/0/all/0/1\">Madeleine Cule</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sorokin_E/0/1/0/all/0/1\">Elena P. Sorokin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bell_J/0/1/0/all/0/1\">Jimmy D. Bell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_E/0/1/0/all/0/1\">E. Louise Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whitcher_B/0/1/0/all/0/1\">Brandon Whitcher</a>",
          "description": "Dixon MRI is widely used for body composition studies. Current processing\nmethods associated with large whole-body volumes are time intensive and prone\nto artifacts during fat-water separation performed on the scanner, making the\ndata difficult to analyse. The most common artifact are fat-water swaps, where\nthe labels are inverted at the voxel level. It is common for researchers to\ndiscard swapped data (generally around 10%), which can be wasteful and lead to\nunintended biases. The UK Biobank is acquiring Dixon MRI for over 100,000\nparticipants, and thousands of swaps will occur. If those go undetected, errors\nwill propagate into processes such as abdominal organ segmentation and dilute\nthe results in population-based analyses. There is a clear need for a fast and\nrobust method to accurately separate fat and water channels. In this work we\npropose such a method based on style transfer using a conditional generative\nadversarial network. We also introduce a new Dixon loss function for the\ngenerator model. Using data from the UK Biobank Dixon MRI, our model is able to\npredict highly accurate fat and water channels that are free from artifacts. We\nshow that the model separates fat and water channels using either single input\n(in-phase) or dual input (in-phase and opposed-phase), with the latter\nproducing improved results. Our proposed method enables faster and more\naccurate downstream analysis of body composition from Dixon MRI in population\nstudies by eliminating the need for visual inspection or discarding data due to\nfat-water swaps.",
          "link": "http://arxiv.org/abs/2107.14175",
          "publishedOn": "2021-07-30T02:13:28.449Z",
          "wordCount": 707,
          "title": "Swap-Free Fat-Water Separation in Dixon MRI using Conditional Generative Adversarial Networks. (arXiv:2107.14175v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Chun-Han Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei-Chih Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>",
          "description": "Reasoning 3D shapes from 2D images is an essential yet challenging task,\nespecially when only single-view images are at our disposal. While an object\ncan have a complicated shape, individual parts are usually close to geometric\nprimitives and thus are easier to model. Furthermore, parts provide a mid-level\nrepresentation that is robust to appearance variations across objects in a\nparticular category. In this work, we tackle the problem of 3D part discovery\nfrom only 2D image collections. Instead of relying on manually annotated parts\nfor supervision, we propose a self-supervised approach, latent part discovery\n(LPD). Our key insight is to learn a novel part shape prior that allows each\npart to fit an object shape faithfully while constrained to have simple\ngeometry. Extensive experiments on the synthetic ShapeNet, PartNet, and\nreal-world Pascal 3D+ datasets show that our method discovers consistent object\nparts and achieves favorable reconstruction accuracy compared to the existing\nmethods with the same level of supervision.",
          "link": "http://arxiv.org/abs/2107.13629",
          "publishedOn": "2021-07-30T02:13:28.442Z",
          "wordCount": 599,
          "title": "Discovering 3D Parts from Image Collections. (arXiv:2107.13629v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Generalized zero-shot learning (GZSL) has achieved significant progress, with\nmany efforts dedicated to overcoming the problems of visual-semantic domain gap\nand seen-unseen bias. However, most existing methods directly use feature\nextraction models trained on ImageNet alone, ignoring the cross-dataset bias\nbetween ImageNet and GZSL benchmarks. Such a bias inevitably results in\npoor-quality visual features for GZSL tasks, which potentially limits the\nrecognition performance on both seen and unseen classes. In this paper, we\npropose a simple yet effective GZSL method, termed feature refinement for\ngeneralized zero-shot learning (FREE), to tackle the above problem. FREE\nemploys a feature refinement (FR) module that incorporates\n\\textit{semantic$\\rightarrow$visual} mapping into a unified generative model to\nrefine the visual features of seen and unseen class samples. Furthermore, we\npropose a self-adaptive margin center loss (SAMC-loss) that cooperates with a\nsemantic cycle-consistency loss to guide FR to learn class- and\nsemantically-relevant representations, and concatenate the features in FR to\nextract the fully refined features. Extensive experiments on five benchmark\ndatasets demonstrate the significant performance gain of FREE over its baseline\nand current state-of-the-art methods. Our codes are available at\nhttps://github.com/shiming-chen/FREE .",
          "link": "http://arxiv.org/abs/2107.13807",
          "publishedOn": "2021-07-30T02:13:28.436Z",
          "wordCount": 633,
          "title": "FREE: Feature Refinement for Generalized Zero-Shot Learning. (arXiv:2107.13807v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+LeBauer_D/0/1/0/all/0/1\">David LeBauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnette_M/0/1/0/all/0/1\">Max Burnette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahlgren_N/0/1/0/all/0/1\">Noah Fahlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooper_R/0/1/0/all/0/1\">Rob Kooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1\">Kenton McHenry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1\">Abby Stylianou</a>",
          "description": "A core objective of the TERRA-REF project was to generate an open-access\nreference dataset for the study of evaluation of sensing technology to study\nplants under field conditions. The TERRA-REF program deployed a suite of high\nresolution, cutting edge technology sensors on a gantry system with the aim of\nscanning 1 hectare (~$10^4$ m) at around $1 mm^2$ spatial resolution multiple\ntimes per week. The system contains co-located sensors including a stereo-pair\nRGB camera, a thermal imager, a laser scanner to capture 3D structure, and two\nhyperspectral cameras covering wavelengths of 300-2500nm. This sensor data is\nprovided alongside over sixty types of traditional plant measurements that can\nbe used to train new machine learning models. Associated weather and\nenvironmental measurements, information about agronomic management and\nexperimental design, and the genomic sequences of hundreds of plant varieties\nhave been collected and are available alongside the sensor and plant trait\n(phenotype) data.\n\nOver the course of four years and ten growing seasons, the TERRA-REF system\ngenerated over 1 PB of sensor data and almost 45 million files. The subset that\nhas been released to the public domain accounts for two seasons and about half\nof the total data volume. This provides an unprecedented opportunity for\ninvestigations far beyond the core biological scope of the project.\n\nThis focus of this paper is to provide the Computer Vision and Machine\nLearning communities an overview of the available data and some potential\napplications of this one of a kind data.",
          "link": "http://arxiv.org/abs/2107.14072",
          "publishedOn": "2021-07-30T02:13:28.427Z",
          "wordCount": 713,
          "title": "What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?. (arXiv:2107.14072v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Sihan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Recently, there has been an increasing concern about the privacy issue raised\nby using personally identifiable information in machine learning. However,\nprevious portrait matting methods were all based on identifiable portrait\nimages. To fill the gap, we present P3M-10k in this paper, which is the first\nlarge-scale anonymized benchmark for Privacy-Preserving Portrait Matting.\nP3M-10k consists of 10,000 high-resolution face-blurred portrait images along\nwith high-quality alpha mattes. We systematically evaluate both trimap-free and\ntrimap-based matting methods on P3M-10k and find that existing matting methods\nshow different generalization capabilities when following the\nPrivacy-Preserving Training (PPT) setting, i.e., training on face-blurred\nimages and testing on arbitrary images. To devise a better trimap-free portrait\nmatting model, we propose P3M-Net, which leverages the power of a unified\nframework for both semantic perception and detail matting, and specifically\nemphasizes the interaction between them and the encoder to facilitate the\nmatting process. Extensive experiments on P3M-10k demonstrate that P3M-Net\noutperforms the state-of-the-art methods in terms of both objective metrics and\nsubjective visual quality. Besides, it shows good generalization capacity under\nthe PPT setting, confirming the value of P3M-10k for facilitating future\nresearch and enabling potential real-world applications. The source code and\ndataset are available at https://github.com/JizhiziLi/P3M",
          "link": "http://arxiv.org/abs/2104.14222",
          "publishedOn": "2021-07-30T02:13:28.420Z",
          "wordCount": 672,
          "title": "Privacy-Preserving Portrait Matting. (arXiv:2104.14222v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">TianYang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">XiaoJun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>",
          "description": "The Transformer architecture has achieved rapiddevelopment in recent years,\noutperforming the CNN archi-tectures in many computer vision tasks, such as the\nVisionTransformers (ViT) for image classification. However, existingvisual\ntransformer models aim to extract semantic informationfor high-level tasks such\nas classification and detection, distortingthe spatial resolution of the input\nimage, thus sacrificing thecapacity in reconstructing the input or generating\nhigh-resolutionimages. In this paper, therefore, we propose a Patch\nPyramidTransformer(PPT) to effectively address the above issues. Specif-ically,\nwe first design a Patch Transformer to transform theimage into a sequence of\npatches, where transformer encodingis performed for each patch to extract local\nrepresentations.In addition, we construct a Pyramid Transformer to\neffectivelyextract the non-local information from the entire image.\nAfterobtaining a set of multi-scale, multi-dimensional, and multi-anglefeatures\nof the original image, we design the image reconstructionnetwork to ensure that\nthe features can be reconstructed intothe original input. To validate the\neffectiveness, we apply theproposed Patch Pyramid Transformer to the image\nfusion task.The experimental results demonstrate its superior\nperformanceagainst the state-of-the-art fusion approaches, achieving the\nbestresults on several evaluation indicators. The underlying capacityof the PPT\nnetwork is reflected by its universal power in featureextraction and image\nreconstruction, which can be directlyapplied to different image fusion tasks\nwithout redesigning orretraining the network.",
          "link": "http://arxiv.org/abs/2107.13967",
          "publishedOn": "2021-07-30T02:13:28.404Z",
          "wordCount": 671,
          "title": "PPT Fusion: Pyramid Patch Transformerfor a Case Study in Image Fusion. (arXiv:2107.13967v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Luchuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "Crowd counting is a challenging task due to the issues such as scale\nvariation and perspective variation in real crowd scenes. In this paper, we\npropose a novel Cascaded Residual Density Network (CRDNet) in a coarse-to-fine\napproach to generate the high-quality density map for crowd counting more\naccurately. (1) We estimate the residual density maps by multi-scale pyramidal\nfeatures through cascaded residual density modules. It can improve the quality\nof density map layer by layer effectively. (2) A novel additional local count\nloss is presented to refine the accuracy of crowd counting, which reduces the\nerrors of pixel-wise Euclidean loss by restricting the number of people in the\nlocal crowd areas. Experiments on two public benchmark datasets show that the\nproposed method achieves effective improvement compared with the\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.13718",
          "publishedOn": "2021-07-30T02:13:28.394Z",
          "wordCount": 566,
          "title": "Cascaded Residual Density Network for Crowd Counting. (arXiv:2107.13718v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chang-Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng-Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_F/0/1/0/all/0/1\">Feng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>",
          "description": "Semantic segmentation models trained on public datasets have achieved great\nsuccess in recent years. However, these models didn't consider the\npersonalization issue of segmentation though it is important in practice. In\nthis paper, we address the problem of personalized image segmentation. The\nobjective is to generate more accurate segmentation results on unlabeled\npersonalized images by investigating the data's personalized traits. To open up\nfuture research in this area, we collect a large dataset containing various\nusers' personalized images called PIS (Personalized Image Semantic\nSegmentation). We also survey some recent researches related to this problem\nand report their performance on our dataset. Furthermore, by observing the\ncorrelation among a user's personalized images, we propose a baseline method\nthat incorporates the inter-image context when segmenting certain images.\nExtensive experiments show that our method outperforms the existing methods on\nthe proposed dataset. The code and the PIS dataset will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2107.13978",
          "publishedOn": "2021-07-30T02:13:28.375Z",
          "wordCount": 582,
          "title": "Personalized Image Semantic Segmentation. (arXiv:2107.13978v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scabini_L/0/1/0/all/0/1\">Leonardo F. S. Scabini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_O/0/1/0/all/0/1\">Odemir M. Bruno</a>",
          "description": "Understanding the behavior of Artificial Neural Networks is one of the main\ntopics in the field recently, as black-box approaches have become usual since\nthe widespread of deep learning. Such high-dimensional models may manifest\ninstabilities and weird properties that resemble complex systems. Therefore, we\npropose Complex Network (CN) techniques to analyze the structure and\nperformance of fully connected neural networks. For that, we build a dataset\nwith 4 thousand models and their respective CN properties. They are employed in\na supervised classification setup considering four vision benchmarks. Each\nneural network is approached as a weighted and undirected graph of neurons and\nsynapses, and centrality measures are computed after training. Results show\nthat these measures are highly related to the network classification\nperformance. We also propose the concept of Bag-Of-Neurons (BoN), a CN-based\napproach for finding topological signatures linking similar neurons. Results\nsuggest that six neuronal types emerge in such networks, independently of the\ntarget domain, and are distributed differently according to classification\naccuracy. We also tackle specific CN properties related to performance, such as\nhigher subgraph centrality on lower-performing models. Our findings suggest\nthat CN properties play a critical role in the performance of fully connected\nneural networks, with topological patterns emerging independently on a wide\nrange of models.",
          "link": "http://arxiv.org/abs/2107.14062",
          "publishedOn": "2021-07-30T02:13:28.368Z",
          "wordCount": 686,
          "title": "Structure and Performance of Fully Connected Neural Networks: Emerging Complex Network Properties. (arXiv:2107.14062v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hengchang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kui Ren</a>",
          "description": "Transferability of adversarial examples is of central importance for\nattacking an unknown model, which facilitates adversarial attacks in more\npractical scenarios, e.g., blackbox attacks. Existing transferable attacks tend\nto craft adversarial examples by indiscriminately distorting features to\ndegrade prediction accuracy in a source model without aware of intrinsic\nfeatures of objects in the images. We argue that such brute-force degradation\nwould introduce model-specific local optimum into adversarial examples, thus\nlimiting the transferability. By contrast, we propose the Feature\nImportance-aware Attack (FIA), which disrupts important object-aware features\nthat dominate model decisions consistently. More specifically, we obtain\nfeature importance by introducing the aggregate gradient, which averages the\ngradients with respect to feature maps of the source model, computed on a batch\nof random transforms of the original clean image. The gradients will be highly\ncorrelated to objects of interest, and such correlation presents invariance\nacross different models. Besides, the random transforms will preserve intrinsic\nfeatures of objects and suppress model-specific information. Finally, the\nfeature importance guides to search for adversarial examples towards disrupting\ncritical features, achieving stronger transferability. Extensive experimental\nevaluation demonstrates the effectiveness and superior performance of the\nproposed FIA, i.e., improving the success rate by 8.4% against normally trained\nmodels and 11.7% against defense models as compared to the state-of-the-art\ntransferable attacks. Code is available at: https://github.com/hcguoO0/FIA",
          "link": "http://arxiv.org/abs/2107.14185",
          "publishedOn": "2021-07-30T02:13:28.353Z",
          "wordCount": 658,
          "title": "Feature Importance-aware Transferable Adversarial Attacks. (arXiv:2107.14185v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazaheri_A/0/1/0/all/0/1\">Amir Mazaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Video generation is one of the most challenging tasks in Machine Learning and\nComputer Vision fields of study. In this paper, we tackle the text to video\ngeneration problem, which is a conditional form of video generation. Humans can\nlisten/read natural language sentences, and can imagine or visualize what is\nbeing described; therefore, we believe that video generation from natural\nlanguage sentences will have an important impact on Artificial Intelligence.\nVideo generation is relatively a new field of study in Computer Vision, which\nis far from being solved. The majority of recent works deal with synthetic\ndatasets or real datasets with very limited types of objects, scenes, and\nemotions. To the best of our knowledge, this is the very first work on the text\n(free-form sentences) to video generation on more realistic video datasets like\nActor and Action Dataset (A2D) or UCF101. We tackle the complicated problem of\nvideo generation by regressing the latent representations of the first and last\nframes and employing a context-aware interpolation method to build the latent\nrepresentations of in-between frames. We propose a stacking ``upPooling'' block\nto sequentially generate RGB frames out of each latent representation and\nprogressively increase the resolution. Moreover, our proposed Discriminator\nencodes videos based on single and multiple frames. We provide quantitative and\nqualitative results to support our arguments and show the superiority of our\nmethod over well-known baselines like Recurrent Neural Network (RNN) and\nDeconvolution (as known as Convolutional Transpose) based video generation\nmethods.",
          "link": "http://arxiv.org/abs/2107.13766",
          "publishedOn": "2021-07-30T02:13:28.342Z",
          "wordCount": 682,
          "title": "Video Generation from Text Employing Latent Path Construction for Temporal Modeling. (arXiv:2107.13766v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kellenberger_B/0/1/0/all/0/1\">Benjamin Kellenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vargas_Munoz_J/0/1/0/all/0/1\">John E. Vargas-Mu&#xf1;oz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1\">Devis Tuia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daudt_R/0/1/0/all/0/1\">Rodrigo C. Daudt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whelan_T/0/1/0/all/0/1\">Thao T-T Whelan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayo_B/0/1/0/all/0/1\">Brenda Ayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>",
          "description": "Humanitarian actions require accurate information to efficiently delegate\nsupport operations. Such information can be maps of building footprints,\nbuilding functions, and population densities. While the access to this\ninformation is comparably easy in industrialized countries thanks to reliable\ncensus data and national geo-data infrastructures, this is not the case for\ndeveloping countries, where that data is often incomplete or outdated. Building\nmaps derived from remote sensing images may partially remedy this challenge in\nsuch countries, but are not always accurate due to different landscape\nconfigurations and lack of validation data. Even when they exist, building\nfootprint layers usually do not reveal more fine-grained building properties,\nsuch as the number of stories or the building's function (e.g., office,\nresidential, school, etc.). In this project we aim to automate building\nfootprint and function mapping using heterogeneous data sources. In a first\nstep, we intend to delineate buildings from satellite data, using deep learning\nmodels for semantic image segmentation. Building functions shall be retrieved\nby parsing social media data like for instance tweets, as well as ground-based\nimagery, to automatically identify different buildings functions and retrieve\nfurther information such as the number of building stories. Building maps\naugmented with those additional attributes make it possible to derive more\naccurate population density maps, needed to support the targeted provision of\nhumanitarian aid.",
          "link": "http://arxiv.org/abs/2107.14123",
          "publishedOn": "2021-07-30T02:13:28.335Z",
          "wordCount": 667,
          "title": "Mapping Vulnerable Populations with AI. (arXiv:2107.14123v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wenhang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chunyan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ancong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hongwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>",
          "description": "Person re-identification (Re-ID) aims to match person images across\nnon-overlapping camera views. The majority of Re-ID methods focus on\nsmall-scale surveillance systems in which each pedestrian is captured in\ndifferent camera views of adjacent scenes. However, in large-scale surveillance\nsystems that cover larger areas, it is required to track a pedestrian of\ninterest across distant scenes (e.g., a criminal suspect escapes from one city\nto another). Since most pedestrians appear in limited local areas, it is\ndifficult to collect training data with cross-camera pairs of the same person.\nIn this work, we study intra-camera supervised person re-identification across\ndistant scenes (ICS-DS Re-ID), which uses cross-camera unpaired data with\nintra-camera identity labels for training. It is challenging as cross-camera\npaired data plays a crucial role for learning camera-invariant features in most\nexisting Re-ID methods. To learn camera-invariant representation from\ncross-camera unpaired training data, we propose a cross-camera feature\nprediction method to mine cross-camera self supervision information from\ncamera-specific feature distribution by transforming fake cross-camera positive\nfeature pairs and minimize the distances of the fake pairs. Furthermore, we\nautomatically localize and extract local-level feature by a transformer. Joint\nlearning of global-level and local-level features forms a global-local\ncross-camera feature prediction scheme for mining fine-grained cross-camera\nself supervision information. Finally, cross-camera self supervision and\nintra-camera supervision are aggregated in a framework. The experiments are\nconducted in the ICS-DS setting on Market-SCT, Duke-SCT and MSMT17-SCT\ndatasets. The evaluation results demonstrate the superiority of our method,\nwhich gains significant improvements of 15.4 Rank-1 and 22.3 mAP on Market-SCT\nas compared to the second best method.",
          "link": "http://arxiv.org/abs/2107.13904",
          "publishedOn": "2021-07-30T02:13:28.325Z",
          "wordCount": 722,
          "title": "Cross-Camera Feature Prediction for Intra-Camera Supervised Person Re-identification across Distant Scenes. (arXiv:2107.13904v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13833",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Noort_F/0/1/0/all/0/1\">Frieda van den Noort</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sirmacek_B/0/1/0/all/0/1\">Beril Sirmacek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slump_C/0/1/0/all/0/1\">Cornelis H. Slump</a>",
          "description": "The prevalance of pelvic floor problems is high within the female population.\nTransperineal ultrasound (TPUS) is the main imaging modality used to\ninvestigate these problems. Automating the analysis of TPUS data will help in\ngrowing our understanding of pelvic floor related problems. In this study we\npresent a U-net like neural network with some convolutional long short term\nmemory (CLSTM) layers to automate the 3D segmentation of the levator ani muscle\n(LAM) in TPUS volumes. The CLSTM layers are added to preserve the inter-slice\n3D information. We reach human level performance on this segmentation task.\nTherefore, we conclude that we successfully automated the segmentation of the\nLAM on 3D TPUS data. This paves the way towards automatic in-vivo analysis of\nthe LAM mechanics in the context of large study populations.",
          "link": "http://arxiv.org/abs/2107.13833",
          "publishedOn": "2021-07-30T02:13:28.318Z",
          "wordCount": 587,
          "title": "Recurrent U-net for automatic pelvic floor muscle segmentation on 3D ultrasound. (arXiv:2107.13833v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Luchuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Huihui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "Abnormal behavior detection in surveillance video is a pivotal part of the\nintelligent city. Most existing methods only consider how to detect anomalies,\nwith less considering to explain the reason of the anomalies. We investigate an\northogonal perspective based on the reason of these abnormal behaviors. To this\nend, we propose a multivariate fusion method that analyzes each target through\nthree branches: object, action and motion. The object branch focuses on the\nappearance information, the motion branch focuses on the distribution of the\nmotion features, and the action branch focuses on the action category of the\ntarget. The information that these branches focus on is different, and they can\ncomplement each other and jointly detect abnormal behavior. The final abnormal\nscore can then be obtained by combining the abnormal scores of the three\nbranches.",
          "link": "http://arxiv.org/abs/2107.13706",
          "publishedOn": "2021-07-30T02:13:28.312Z",
          "wordCount": 569,
          "title": "Abnormal Behavior Detection Based on Target Analysis. (arXiv:2107.13706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyons_M/0/1/0/all/0/1\">Michael J. Lyons</a>",
          "description": "Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I\ndesigned and photographed JAFFE, a set of facial expression images intended for\nuse in a study of face perception. In 2019, without seeking permission or\ninforming us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely\npublicized art shows. In addition, they published a nonfactual account of the\nimages in the essay \"Excavating AI: The Politics of Images in Machine Learning\nTraining Sets.\" The present article recounts the creation of the JAFFE dataset\nand unravels each of Crawford and Paglen's fallacious statements. I also\ndiscuss JAFFE more broadly in connection with research on facial expression,\naffective computing, and human-computer interaction.",
          "link": "http://arxiv.org/abs/2107.13998",
          "publishedOn": "2021-07-30T02:13:28.297Z",
          "wordCount": 572,
          "title": "\"Excavating AI\" Re-excavated: Debunking a Fallacious Account of the JAFFE Dataset. (arXiv:2107.13998v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loginov_V/0/1/0/all/0/1\">Vladimir Loginov</a>",
          "description": "Recent works in the text recognition area have pushed forward the recognition\nresults to the new horizons. But for a long time a lack of large human-labeled\nnatural text recognition datasets has been forcing researchers to use synthetic\ndata for training text recognition models. Even though synthetic datasets are\nvery large (MJSynth and SynthTest, two most famous synthetic datasets, have\nseveral million images each), their diversity could be insufficient, compared\nto natural datasets like ICDAR and others. Fortunately, the recently released\ntext-recognition annotation for OpenImages V5 dataset has comparable with\nsynthetic dataset number of instances and more diverse examples. We have used\nthis annotation with a Text Recognition head architecture from the Yet Another\nMask Text Spotter and got comparable to the SOTA results. On some datasets we\nhave even outperformed previous SOTA models. In this paper we also introduce a\ntext recognition model. The model's code is available.",
          "link": "http://arxiv.org/abs/2107.13938",
          "publishedOn": "2021-07-30T02:13:28.291Z",
          "wordCount": 586,
          "title": "Why You Should Try the Real Data for the Scene Text Recognition. (arXiv:2107.13938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>",
          "description": "For over hundreds of millions of years, sea turtles and their ancestors have\nswum in the vast expanses of the ocean. They have undergone a number of\nevolutionary changes, leading to speciation and sub-speciation. However, in the\npast few decades, some of the most notable forces driving the genetic variance\nand population decline have been global warming and anthropogenic impact\nranging from large-scale poaching, collecting turtle eggs for food, besides\ndumping trash including plastic waste into the ocean. This leads to severe\ndetrimental effects in the sea turtle population, driving them to extinction.\nThis research focusses on the forces causing the decline in sea turtle\npopulation, the necessity for the global conservation efforts along with its\nsuccesses and failures, followed by an in-depth analysis of the modern advances\nin detection and recognition of sea turtles, involving Machine Learning and\nComputer Vision systems, aiding the conservation efforts.",
          "link": "http://arxiv.org/abs/2107.14061",
          "publishedOn": "2021-07-30T02:13:28.233Z",
          "wordCount": 610,
          "title": "The Need and Status of Sea Turtle Conservation and Survey of Associated Computer Vision Advances. (arXiv:2107.14061v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huan_J/0/1/0/all/0/1\">Jun Huan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "While deep learning succeeds in a wide range of tasks, it highly depends on\nthe massive collection of annotated data which is expensive and time-consuming.\nTo lower the cost of data annotation, active learning has been proposed to\ninteractively query an oracle to annotate a small proportion of informative\nsamples in an unlabeled dataset. Inspired by the fact that the samples with\nhigher loss are usually more informative to the model than the samples with\nlower loss, in this paper we present a novel deep active learning approach that\nqueries the oracle for data annotation when the unlabeled sample is believed to\nincorporate high loss. The core of our approach is a measurement Temporal\nOutput Discrepancy (TOD) that estimates the sample loss by evaluating the\ndiscrepancy of outputs given by models at different optimization steps. Our\ntheoretical investigation shows that TOD lower-bounds the accumulated sample\nloss thus it can be used to select informative unlabeled samples. On basis of\nTOD, we further develop an effective unlabeled data sampling strategy as well\nas an unsupervised learning criterion that enhances model performance by\nincorporating the unlabeled data. Due to the simplicity of TOD, our active\nlearning approach is efficient, flexible, and task-agnostic. Extensive\nexperimental results demonstrate that our approach achieves superior\nperformances than the state-of-the-art active learning methods on image\nclassification and semantic segmentation tasks.",
          "link": "http://arxiv.org/abs/2107.14153",
          "publishedOn": "2021-07-30T02:13:28.218Z",
          "wordCount": 673,
          "title": "Semi-Supervised Active Learning with Temporal Output Discrepancy. (arXiv:2107.14153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jizong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Chrisitian Desrosiers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>",
          "description": "Pre-training a recognition model with contrastive learning on a large dataset\nof unlabeled data has shown great potential to boost the performance of a\ndownstream task, e.g., image classification. However, in domains such as\nmedical imaging, collecting unlabeled data can be challenging and expensive. In\nthis work, we propose to adapt contrastive learning to work with meta-label\nannotations, for improving the model's performance in medical image\nsegmentation even when no additional unlabeled data is available. Meta-labels\nsuch as the location of a 2D slice in a 3D MRI scan or the type of device used,\noften come for free during the acquisition process. We use the meta-labels for\npre-training the image encoder as well as to regularize a semi-supervised\ntraining, in which a reduced set of annotated data is used for training.\nFinally, to fully exploit the weak annotations, a self-paced learning approach\nis used to help the learning and discriminate useful labels from noise. Results\non three different medical image segmentation datasets show that our approach:\ni) highly boosts the performance of a model trained on a few scans, ii)\noutperforms previous contrastive and semi-supervised approaches, and iii)\nreaches close to the performance of a model trained on the full data.",
          "link": "http://arxiv.org/abs/2107.13741",
          "publishedOn": "2021-07-30T02:13:28.205Z",
          "wordCount": 639,
          "title": "Self-Paced Contrastive Learning for Semi-supervisedMedical Image Segmentation with Meta-labels. (arXiv:2107.13741v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Forecasting human trajectories in complex dynamic environments plays a\ncritical role in autonomous vehicles and intelligent robots. Most existing\nmethods learn to predict future trajectories by behavior clues from history\ntrajectories and interaction clues from environments. However, the inherent\nbias between training and deployment environments is ignored. Hence, we propose\na counterfactual analysis method for human trajectory prediction to investigate\nthe causality between the predicted trajectories and input clues and alleviate\nthe negative effects brought by environment bias. We first build a causal graph\nfor trajectory forecasting with history trajectory, future trajectory, and the\nenvironment interactions. Then, we cut off the inference from environment to\ntrajectory by constructing the counterfactual intervention on the trajectory\nitself. Finally, we compare the factual and counterfactual trajectory clues to\nalleviate the effects of environment bias and highlight the trajectory clues.\nOur counterfactual analysis is a plug-and-play module that can be applied to\nany baseline prediction methods including RNN- and CNN-based ones. We show that\nour method achieves consistent improvement for different baselines and obtains\nthe state-of-the-art results on public pedestrian trajectory forecasting\nbenchmarks.",
          "link": "http://arxiv.org/abs/2107.14202",
          "publishedOn": "2021-07-30T02:13:28.151Z",
          "wordCount": 620,
          "title": "Human Trajectory Prediction via Counterfactual Analysis. (arXiv:2107.14202v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13820",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ching/0/1/0/all/0/1\">Ching</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_K/0/1/0/all/0/1\">Kai Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao/0/1/0/all/0/1\">Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_J/0/1/0/all/0/1\">Jerry Chang</a>, Yun, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1\">Chien Cheng</a>",
          "description": "The purpose of this study is to differentiate malignant and benign\nmediastinal lesions by using the three-dimensional convolutional neural network\nthrough the endobronchial ultrasound (EBUS) image. Compared with previous\nstudy, our proposed model is robust to noise and able to fuse various imaging\nfeatures and spatiotemporal features of EBUS videos. Endobronchial\nultrasound-guided transbronchial needle aspiration (EBUS-TBNA) is a diagnostic\ntool for intrathoracic lymph nodes. Physician can observe the characteristics\nof the lesion using grayscale mode, doppler mode, and elastography during the\nprocedure. To process the EBUS data in the form of a video and appropriately\nintegrate the features of multiple imaging modes, we used a time-series\nthree-dimensional convolutional neural network (3D CNN) to learn the\nspatiotemporal features and design a variety of architectures to fuse each\nimaging mode. Our model (Res3D_UDE) took grayscale mode, Doppler mode, and\nelastography as training data and achieved an accuracy of 82.00% and area under\nthe curve (AUC) of 0.83 on the validation set. Compared with previous study, we\ndirectly used videos recorded during procedure as training and validation data,\nwithout additional manual selection, which might be easier for clinical\napplication. In addition, model designed with 3D CNN can also effectively learn\nspatiotemporal features and improve accuracy. In the future, our model may be\nused to guide physicians to quickly and correctly find the target lesions for\nslice sampling during the inspection process, reduce the number of slices of\nbenign lesions, and shorten the inspection time.",
          "link": "http://arxiv.org/abs/2107.13820",
          "publishedOn": "2021-07-30T02:13:28.133Z",
          "wordCount": 707,
          "title": "The interpretation of endobronchial ultrasound image using 3D convolutional neural network for differentiating malignant and benign mediastinal lesions. (arXiv:2107.13820v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yanqing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_B/0/1/0/all/0/1\">Bangning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Miaogen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanlong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chunliu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Juan Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Changyu Shen</a>",
          "description": "Multiple optical scattering occurs when light propagates in a non-uniform\nmedium. During the multiple scattering, images were distorted and the spatial\ninformation they carried became scrambled. However, the image information is\nnot lost but presents in the form of speckle patterns (SPs). In this study, we\nbuilt up an optical random scattering system based on an LCD and an RGB laser\nsource. We found that the image classification can be improved by the help of\nrandom scattering which is considered as a feedforward neural network to\nextracts features from image. Along with the ridge classification deployed on\ncomputer, we achieved excellent classification accuracy higher than 94%, for a\nvariety of data sets covering medical, agricultural, environmental protection\nand other fields. In addition, the proposed optical scattering system has the\nadvantages of high speed, low power consumption, and miniaturization, which is\nsuitable for deploying in edge computing applications.",
          "link": "http://arxiv.org/abs/2107.14051",
          "publishedOn": "2021-07-30T02:13:27.993Z",
          "wordCount": 600,
          "title": "Improvement of image classification by multiple optical scattering. (arXiv:2107.14051v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinzhu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianzhu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yating Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "Geometry Projection is a powerful depth estimation method in monocular 3D\nobject detection. It estimates depth dependent on heights, which introduces\nmathematical priors into the deep model. But projection process also introduces\nthe error amplification problem, in which the error of the estimated height\nwill be amplified and reflected greatly at the output depth. This property\nleads to uncontrollable depth inferences and also damages the training\nefficiency. In this paper, we propose a Geometry Uncertainty Projection Network\n(GUP Net) to tackle the error amplification problem at both inference and\ntraining stages. Specifically, a GUP module is proposed to obtains the\ngeometry-guided uncertainty of the inferred depth, which not only provides high\nreliable confidence for each depth but also benefits depth learning.\nFurthermore, at the training stage, we propose a Hierarchical Task Learning\nstrategy to reduce the instability caused by error amplification. This learning\nalgorithm monitors the learning situation of each task by a proposed indicator\nand adaptively assigns the proper loss weights for different tasks according to\ntheir pre-tasks situation. Based on that, each task starts learning only when\nits pre-tasks are learned well, which can significantly improve the stability\nand efficiency of the training process. Extensive experiments demonstrate the\neffectiveness of the proposed method. The overall model can infer more reliable\nobject depth than existing methods and outperforms the state-of-the-art\nimage-based monocular 3D detectors by 3.74% and 4.7% AP40 of the car and\npedestrian categories on the KITTI benchmark.",
          "link": "http://arxiv.org/abs/2107.13774",
          "publishedOn": "2021-07-30T02:13:27.987Z",
          "wordCount": 691,
          "title": "Geometry Uncertainty Projection Network for Monocular 3D Object Detection. (arXiv:2107.13774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gla_R/0/1/0/all/0/1\">Rawan Gla</a>",
          "description": "Sign language is a set of gestures that deaf people use to communicate.\nUnfortunately, normal people don't understand it, which creates a communication\ngap that needs to be filled. Because of the variations in (Egyptian Sign\nLanguage) ESL from one region to another, ESL provides a challenging research\nproblem. In this work, we are providing applied research with its video-based\nEgyptian sign language recognition system that serves the local community of\ndeaf people in Egypt, with a moderate and reasonable accuracy. We present a\ncomputer vision system with two different neural networks architectures. The\nfirst is a Convolutional Neural Network (CNN) for extracting spatial features.\nThe CNN model was retrained on the inception mod. The second architecture is a\nCNN followed by a Long Short-Term Memory (LSTM) for extracting both spatial and\ntemporal features. The two models achieved an accuracy of 90% and 72%,\nrespectively. We examined the power of these two architectures to distinguish\nbetween 9 common words (with similar signs) among some deaf people community in\nEgypt.",
          "link": "http://arxiv.org/abs/2107.13647",
          "publishedOn": "2021-07-30T02:13:27.981Z",
          "wordCount": 599,
          "title": "Egyptian Sign Language Recognition Using CNN and LSTM. (arXiv:2107.13647v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13703",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Afshari_M/0/1/0/all/0/1\">Mehdi Afshari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R. Tizhoosh</a>",
          "description": "Histopathology digital scans are large-size images that contain valuable\ninformation at the pixel level. Content-based comparison of these images is a\nchallenging task. This study proposes a content-based similarity measure for\nhigh-resolution gigapixel histopathology images. The proposed similarity\nmeasure is an expansion of cosine vector similarity to a matrix. Each image is\ndivided into same-size patches with a meaningful amount of information (i.e.,\ncontained enough tissue). The similarity is measured by the extraction of\npatch-level deep embeddings of the last pooling layer of a pre-trained deep\nmodel at four different magnification levels, namely, 1x, 2.5x, 5x, and 10x\nmagnifications. In addition, for faster measurement, embedding reduction is\ninvestigated. Finally, to assess the proposed method, an image search method is\nimplemented. Results show that the similarity measure represents the slide\nlabels with a maximum accuracy of 93.18\\% for top-5 search at 5x magnification.",
          "link": "http://arxiv.org/abs/2107.13703",
          "publishedOn": "2021-07-30T02:13:27.974Z",
          "wordCount": 589,
          "title": "A Similarity Measure of Histopathology Images by Deep Embeddings. (arXiv:2107.13703v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shan_W/0/1/0/all/0/1\">Wenkang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>",
          "description": "Most of the existing 3D human pose estimation approaches mainly focus on\npredicting 3D positional relationships between the root joint and other human\njoints (local motion) instead of the overall trajectory of the human body\n(global motion). Despite the great progress achieved by these approaches, they\nare not robust to global motion, and lack the ability to accurately predict\nlocal motion with a small movement range. To alleviate these two problems, we\npropose a relative information encoding method that yields positional and\ntemporal enhanced representations. Firstly, we encode positional information by\nutilizing relative coordinates of 2D poses to enhance the consistency between\nthe input and output distribution. The same posture with different absolute 2D\npositions can be mapped to a common representation. It is beneficial to resist\nthe interference of global motion on the prediction results. Second, we encode\ntemporal information by establishing the connection between the current pose\nand other poses of the same person within a period of time. More attention will\nbe paid to the movement changes before and after the current pose, resulting in\nbetter prediction performance on local motion with a small movement range. The\nablation studies validate the effectiveness of the proposed relative\ninformation encoding method. Besides, we introduce a multi-stage optimization\nmethod to the whole framework to further exploit the positional and temporal\nenhanced representations. Our method outperforms state-of-the-art methods on\ntwo public datasets. Code is available at\nhttps://github.com/paTRICK-swk/Pose3D-RIE.",
          "link": "http://arxiv.org/abs/2107.13994",
          "publishedOn": "2021-07-30T02:13:27.957Z",
          "wordCount": 705,
          "title": "Improving Robustness and Accuracy via Relative Information Encoding in 3D Human Pose Estimation. (arXiv:2107.13994v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruicong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>",
          "description": "Deep neural networks have significantly improved appearance-based gaze\nestimation accuracy. However, it still suffers from unsatisfactory performance\nwhen generalizing the trained model to new domains, e.g., unseen environments\nor persons. In this paper, we propose a plug-and-play gaze adaptation framework\n(PnP-GA), which is an ensemble of networks that learn collaboratively with the\nguidance of outliers. Since our proposed framework does not require\nground-truth labels in the target domain, the existing gaze estimation networks\ncan be directly plugged into PnP-GA and generalize the algorithms to new\ndomains. We test PnP-GA on four gaze domain adaptation tasks, ETH-to-MPII,\nETH-to-EyeDiap, Gaze360-to-MPII, and Gaze360-to-EyeDiap. The experimental\nresults demonstrate that the PnP-GA framework achieves considerable performance\nimprovements of 36.9%, 31.6%, 19.4%, and 11.8% over the baseline system. The\nproposed framework also outperforms the state-of-the-art domain adaptation\napproaches on gaze domain adaptation tasks. Code has been released at\nhttps://github.com/DreamtaleCore/PnP-GA.",
          "link": "http://arxiv.org/abs/2107.13780",
          "publishedOn": "2021-07-30T02:13:27.942Z",
          "wordCount": 586,
          "title": "Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation. (arXiv:2107.13780v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuanguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhulin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Linhang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>",
          "description": "Knowledge distillation often involves how to define and transfer knowledge\nfrom teacher to student effectively. Although recent self-supervised\ncontrastive knowledge achieves the best performance, forcing the network to\nlearn such knowledge may damage the representation learning of the original\nclass recognition task. We therefore adopt an alternative self-supervised\naugmented task to guide the network to learn the joint distribution of the\noriginal recognition task and self-supervised auxiliary task. It is\ndemonstrated as a richer knowledge to improve the representation power without\nlosing the normal classification capability. Moreover, it is incomplete that\nprevious methods only transfer the probabilistic knowledge between the final\nlayers. We propose to append several auxiliary classifiers to hierarchical\nintermediate feature maps to generate diverse self-supervised knowledge and\nperform the one-to-one transfer to teach the student network thoroughly. Our\nmethod significantly surpasses the previous SOTA SSKD with an average\nimprovement of 2.56\\% on CIFAR-100 and an improvement of 0.77\\% on ImageNet\nacross widely used network pairs. Codes are available at\nhttps://github.com/winycg/HSAKD.",
          "link": "http://arxiv.org/abs/2107.13715",
          "publishedOn": "2021-07-30T02:13:27.915Z",
          "wordCount": 599,
          "title": "Hierarchical Self-supervised Augmented Knowledge Distillation. (arXiv:2107.13715v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iwanowski_M/0/1/0/all/0/1\">Marcin Iwanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzabka_M/0/1/0/all/0/1\">Marcin Grzabka</a>",
          "description": "The paper describes a method for measuring the similarity and symmetry of an\nimage annotated with bounding boxes indicating image objects. The latter\nrepresentation became popular recently due to the rapid development of fast and\nefficient deep-learning-based object-detection methods. The proposed approach\nallows for comparing sets of bounding boxes to estimate the degree of\nsimilarity of their underlying images. It is based on the fuzzy approach that\nuses the fuzzy mutual position (FMP) matrix to describe spatial composition and\nrelations between bounding boxes within an image. A method of computing the\nsimilarity of two images described by their FMP matrices is proposed and the\nalgorithm of its computation. It outputs the single scalar value describing the\ndegree of content-based image similarity. By modifying the method`s parameters,\ninstead of similarity, the reflectional symmetry of object composition may also\nbe measured. The proposed approach allows for measuring differences in objects`\ncomposition of various intensities. It is also invariant to translation and\nscaling and - in case of symmetry detection - position and orientation of the\nsymmetry axis. A couple of examples illustrate the method.",
          "link": "http://arxiv.org/abs/2107.13651",
          "publishedOn": "2021-07-30T02:13:27.909Z",
          "wordCount": 649,
          "title": "Similarity and symmetry measures based on fuzzy descriptors of image objects` composition. (arXiv:2107.13651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Chongyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xiaoxue Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_S/0/1/0/all/0/1\">Srinivas Sunkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jindong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1\">Blaise Aguera y Arcas</a>",
          "description": "To improve the accessibility of smart devices and to simplify their usage,\nbuilding models which understand user interfaces (UIs) and assist users to\ncomplete their tasks is critical. However, unique challenges are proposed by\nUI-specific characteristics, such as how to effectively leverage multimodal UI\nfeatures that involve image, text, and structural metadata and how to achieve\ngood performance when high-quality labeled data is unavailable. To address such\nchallenges we introduce UIBert, a transformer-based joint image-text model\ntrained through novel pre-training tasks on large-scale unlabeled UI data to\nlearn generic feature representations for a UI and its components. Our key\nintuition is that the heterogeneous features in a UI are self-aligned, i.e.,\nthe image and text features of UI components, are predictive of each other. We\npropose five pretraining tasks utilizing this self-alignment among different\nfeatures of a UI component and across various components in the same UI. We\nevaluate our method on nine real-world downstream UI tasks where UIBert\noutperforms strong multimodal baselines by up to 9.26% accuracy.",
          "link": "http://arxiv.org/abs/2107.13731",
          "publishedOn": "2021-07-30T02:13:27.894Z",
          "wordCount": 620,
          "title": "UIBert: Learning Generic Multimodal Representations for UI Understanding. (arXiv:2107.13731v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1\">Fariborz Taherkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talreja_V/0/1/0/all/0/1\">Veeru Talreja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenti_M/0/1/0/all/0/1\">Matthew C. Valenti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "In recent years, with the advent of deep-learning, face recognition has\nachieved exceptional success. However, many of these deep face recognition\nmodels perform much better in handling frontal faces compared to profile faces.\nThe major reason for poor performance in handling of profile faces is that it\nis inherently difficult to learn pose-invariant deep representations that are\nuseful for profile face recognition. In this paper, we hypothesize that the\nprofile face domain possesses a latent connection with the frontal face domain\nin a latent feature subspace. We look to exploit this latent connection by\nprojecting the profile faces and frontal faces into a common latent subspace\nand perform verification or retrieval in the latent domain. We leverage a\ncoupled conditional generative adversarial network (cpGAN) structure to find\nthe hidden relationship between the profile and frontal images in a latent\ncommon embedding subspace. Specifically, the cpGAN framework consists of two\nconditional GAN-based sub-networks, one dedicated to the frontal domain and the\nother dedicated to the profile domain. Each sub-network tends to find a\nprojection that maximizes the pair-wise correlation between the two feature\ndomains in a common embedding feature subspace. The efficacy of our approach\ncompared with the state-of-the-art is demonstrated using the CFP, CMU\nMulti-PIE, IJB-A, and IJB-C datasets. Additionally, we have also implemented a\ncoupled convolutional neural network (cpCNN) and an adversarial discriminative\ndomain adaptation network (ADDA) for profile to frontal face recognition. We\nhave evaluated the performance of cpCNN and ADDA and compared it with the\nproposed cpGAN. Finally, we have also evaluated our cpGAN for reconstruction of\nfrontal faces from input profile faces contained in the VGGFace2 dataset.",
          "link": "http://arxiv.org/abs/2107.13742",
          "publishedOn": "2021-07-30T02:13:27.888Z",
          "wordCount": 731,
          "title": "Profile to Frontal Face Recognition in the Wild Using Coupled Conditional GAN. (arXiv:2107.13742v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kloeker_L/0/1/0/all/0/1\">Laurent Kloeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloeker_A/0/1/0/all/0/1\">Amarin Kloeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomsen_F/0/1/0/all/0/1\">Fabian Thomsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erraji_A/0/1/0/all/0/1\">Armin Erraji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_L/0/1/0/all/0/1\">Lutz Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamberty_S/0/1/0/all/0/1\">Serge Lamberty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_A/0/1/0/all/0/1\">Adrian Fazekas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kallo_E/0/1/0/all/0/1\">Eszter Kall&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oeser_M/0/1/0/all/0/1\">Markus Oeser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flechon_C/0/1/0/all/0/1\">Charlotte Fl&#xe9;chon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lohmiller_J/0/1/0/all/0/1\">Jochen Lohmiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1\">Pascal Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommer_M/0/1/0/all/0/1\">Martin Sommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winter_H/0/1/0/all/0/1\">Helen Winter</a>",
          "description": "With the Corridor for New Mobility Aachen - D\\\"usseldorf, an integrated\ndevelopment environment is created, incorporating existing test capabilities,\nto systematically test and validate automated vehicles in interaction with\nconnected Intelligent Transport Systems Stations (ITS-Ss). This is achieved\nthrough a time- and cost-efficient toolchain and methodology, in which\nsimulation, closed test sites as well as test fields in public transport are\nlinked in the best possible way. By implementing a digital twin, the recorded\ntraffic events can be visualized in real-time and driving functions can be\ntested in the simulation based on real data. In order to represent diverse\ntraffic scenarios, the corridor contains a highway section, a rural area, and\nurban areas. First, this paper outlines the project goals before describing the\nindividual project contents in more detail. These include the concepts of\ntraffic detection, driving function development, digital twin development, and\npublic involvement.",
          "link": "http://arxiv.org/abs/2107.14048",
          "publishedOn": "2021-07-30T02:13:27.880Z",
          "wordCount": 613,
          "title": "Corridor for new mobility Aachen-D\\\"usseldorf: Methods and concepts of the research project ACCorD. (arXiv:2107.14048v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhiyuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yaohai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ruisong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yayu Gao</a>",
          "description": "In this paper, we propose an efficient human pose estimation network -- SFM\n(slender fusion model) by fusing multi-level features and adding lightweight\nattention blocks -- HSA (High-Level Spatial Attention). Many existing methods\non efficient network have already taken feature fusion into consideration,\nwhich largely boosts the performance. However, its performance is far inferior\nto large network such as ResNet and HRNet due to its limited fusion operation\nin the network. Specifically, we expand the number of fusion operation by\nbuilding bridges between two pyramid frameworks without adding layers.\nMeanwhile, to capture long-range dependency, we propose a lightweight attention\nblock -- HSA, which computes second-order attention map. In summary, SFM\nmaximizes the number of feature fusion in a limited number of layers. HSA\nlearns high precise spatial information by computing the attention of spatial\nattention map. With the help of SFM and HSA, our network is able to generate\nmulti-level feature and extract precise global spatial information with little\ncomputing resource. Thus, our method achieve comparable or even better accuracy\nwith less parameters and computational cost. Our SFM achieve 89.0 in PCKh@0.5,\n42.0 in PCKh@0.1 on MPII validation set and 71.7 in AP, 90.7 in AP@0.5 on COCO\nvalidation with only 1.7G FLOPs and 1.5M parameters. The source code will be\npublic soon.",
          "link": "http://arxiv.org/abs/2107.13693",
          "publishedOn": "2021-07-30T02:13:27.858Z",
          "wordCount": 657,
          "title": "Efficient Human Pose Estimation by Maximizing Fusion and High-Level Spatial Attention. (arXiv:2107.13693v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yu Cheng Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsougenis_E/0/1/0/all/0/1\">Efstratios Tsougenis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok-Leung Tsui</a>",
          "description": "Counting the repetition of human exercise and physical rehabilitation is a\ncommon task in rehabilitation and exercise training. The existing vision-based\nrepetition counting methods less emphasize the concurrent motions in the same\nvideo. This work presents a vision-based human motion repetition counting\napplicable to counting concurrent motions through the skeleton location\nextracted from various pose estimation methods. The presented method was\nvalidated on the University of Idaho Physical Rehabilitation Movements Data Set\n(UI-PRMD), and MM-fit dataset. The overall mean absolute error (MAE) for mm-fit\nwas 0.06 with off-by-one Accuracy (OBOA) 0.94. Overall MAE for UI-PRMD dataset\nwas 0.06 with OBOA 0.95. We have also tested the performance in a variety of\ncamera locations and concurrent motions with conveniently collected video with\noverall MAE 0.06 and OBOA 0.88. The proposed method provides a view-angle and\nmotion agnostic concurrent motion counting. This method can potentially use in\nlarge-scale remote rehabilitation and exercise training with only one camera.",
          "link": "http://arxiv.org/abs/2107.13760",
          "publishedOn": "2021-07-30T02:13:27.846Z",
          "wordCount": 589,
          "title": "Viewpoint-Invariant Exercise Repetition Counting. (arXiv:2107.13760v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeed_M/0/1/0/all/0/1\">Mohamed Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araia_M/0/1/0/all/0/1\">Musie Araia</a>",
          "description": "Human pose estimation (HPE) is one of the most challenging tasks in computer\nvision as humans are deformable by nature and thus their pose has so much\nvariance. HPE aims to correctly identify the main joint locations of a single\nperson or multiple people in a given image or video. Locating joints of a\nperson in images or videos is an important task that can be applied in action\nrecognition and object tracking. As have many computer vision tasks, HPE has\nadvanced massively with the introduction of deep learning to the field. In this\npaper, we focus on one of the deep learning-based approaches of HPE proposed by\nNewell et al., which they named the stacked hourglass network. Their approach\nis widely used in many applications and is regarded as one of the best works in\nthis area. The main focus of their approach is to capture as much information\nas it can at all possible scales so that a coherent understanding of the local\nfeatures and full-body location is achieved. Their findings demonstrate that\nimportant cues such as orientation of a person, arrangement of limbs, and\nadjacent joints' relative location can be identified from multiple scales at\ndifferent resolutions. To do so, they makes use of a single pipeline to process\nimages in multiple resolutions, which comprises a skip layer to not lose\nspatial information at each resolution. The resolution of the images stretches\nas lower as 4x4 to make sure that a smaller spatial feature is included. In\nthis study, we study the effect of architectural modifications on the\ncomputational speed and accuracy of the network.",
          "link": "http://arxiv.org/abs/2107.13643",
          "publishedOn": "2021-07-30T02:13:27.840Z",
          "wordCount": 703,
          "title": "Lighter Stacked Hourglass Human Pose Estimation. (arXiv:2107.13643v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xinyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongjin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuncong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingchao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>",
          "description": "Detecting abnormal activities in real-world surveillance videos is an\nimportant yet challenging task as the prior knowledge about video anomalies is\nusually limited or unavailable. Despite that many approaches have been\ndeveloped to resolve this problem, few of them can capture the normal\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\nseldom explicitly consider the local consistency at frame level and global\ncoherence of temporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative Adversarial\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\nSpecifically, we first present a convolutional transformer to perform future\nframe prediction. It contains three key components, i.e., a convolutional\nencoder to capture the spatial information of the input video clips, a temporal\nself-attention module to encode the temporal dynamics, and a convolutional\ndecoder to integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure, which jointly\nconsiders an image discriminator that can maintain the local consistency at\nframe-level and a video discriminator that can enforce the global coherence of\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\nthe prediction error is used to identify abnormal video frames. Thoroughly\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\nthe proposed adversarial spatio-temporal modeling framework.",
          "link": "http://arxiv.org/abs/2107.13720",
          "publishedOn": "2021-07-30T02:13:27.812Z",
          "wordCount": 708,
          "title": "Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. (arXiv:2107.13720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chengkuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaumberg_A/0/1/0/all/0/1\">Andrew J. Schaumberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>",
          "description": "The expanding adoption of digital pathology has enabled the curation of large\nrepositories of histology whole slide images (WSIs), which contain a wealth of\ninformation. Similar pathology image search offers the opportunity to comb\nthrough large historical repositories of gigapixel WSIs to identify cases with\nsimilar morphological features and can be particularly useful for diagnosing\nrare diseases, identifying similar cases for predicting prognosis, treatment\noutcomes, and potential clinical trial success. A critical challenge in\ndeveloping a WSI search and retrieval system is scalability, which is uniquely\nchallenging given the need to search a growing number of slides that each can\nconsist of billions of pixels and are several gigabytes in size. Such systems\nare typically slow and retrieval speed often scales with the size of the\nrepository they search through, making their clinical adoption tedious and are\nnot feasible for repositories that are constantly growing. Here we present Fast\nImage Search for Histopathology (FISH), a histology image search pipeline that\nis infinitely scalable and achieves constant search speed that is independent\nof the image database size while being interpretable and without requiring\ndetailed annotations. FISH uses self-supervised deep learning to encode\nmeaningful representations from WSIs and a Van Emde Boas tree for fast search,\nfollowed by an uncertainty-based ranking algorithm to retrieve similar WSIs. We\nevaluated FISH on multiple tasks and datasets with over 22,000 patient cases\nspanning 56 disease subtypes. We additionally demonstrate that FISH can be used\nto assist with the diagnosis of rare cancer types where sufficient cases may\nnot be available to train traditional supervised deep models. FISH is available\nas an easy-to-use, open-source software package\n(https://github.com/mahmoodlab/FISH).",
          "link": "http://arxiv.org/abs/2107.13587",
          "publishedOn": "2021-07-30T02:13:27.802Z",
          "wordCount": 725,
          "title": "Fast and Scalable Image Search For Histology. (arXiv:2107.13587v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadakis_M/0/1/0/all/0/1\">Manolis Fragkiadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>",
          "description": "Sign language lexica are a useful resource for researchers and people\nlearning sign languages. Current implementations allow a user to search a sign\neither by its gloss or by selecting its primary features such as handshape and\nlocation. This study focuses on exploring a reverse search functionality where\na user can sign a query sign in front of a webcam and retrieve a set of\nmatching signs. By extracting different body joints combinations (upper body,\ndominant hand's arm and wrist) using the pose estimation framework OpenPose, we\ncompare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance\nmetrics between 20 query signs, each performed by eight participants on a 1200\nsign lexicon. The results show that UMAP and DTW can predict a matching sign\nwith an 80\\% and 71\\% accuracy respectively at the top-20 retrieved signs using\nthe movement of the dominant hand arm. Using DTW and adding more sign instances\nfrom other participants in the lexicon, the accuracy can be raised to 90\\% at\nthe top-10 ranking. Our results suggest that our methodology can be used with\nno training in any sign language lexicon regardless of its size.",
          "link": "http://arxiv.org/abs/2107.13637",
          "publishedOn": "2021-07-30T02:13:27.788Z",
          "wordCount": 660,
          "title": "Sign and Search: Sign Search Functionality for Sign Language Lexica. (arXiv:2107.13637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiali Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>",
          "description": "The fact that there exists a gap between low-level features and semantic\nmeanings of images, called the semantic gap, is known for decades. Resolution\nof the semantic gap is a long standing problem. The semantic gap problem is\nreviewed and a survey on recent efforts in bridging the gap is made in this\nwork. Most importantly, we claim that the semantic gap is primarily bridged\nthrough supervised learning today. Experiences are drawn from two application\ndomains to illustrate this point: 1) object detection and 2) metric learning\nfor content-based image retrieval (CBIR). To begin with, this paper offers a\nhistorical retrospective on supervision, makes a gradual transition to the\nmodern data-driven methodology and introduces commonly used datasets. Then, it\nsummarizes various supervision methods to bridge the semantic gap in the\ncontext of object detection and metric learning.",
          "link": "http://arxiv.org/abs/2107.13757",
          "publishedOn": "2021-07-30T02:13:27.763Z",
          "wordCount": 576,
          "title": "Bridging Gap between Image Pixels and Semantics via Supervision: A Survey. (arXiv:2107.13757v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Breiki_F/0/1/0/all/0/1\">Farha Al Breiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridzuan_M/0/1/0/all/0/1\">Muhammad Ridzuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandhe_R/0/1/0/all/0/1\">Rushali Grandhe</a>",
          "description": "Fine-grained image classification involves identifying different\nsubcategories of a class which possess very subtle discriminatory features.\nFine-grained datasets usually provide bounding box annotations along with class\nlabels to aid the process of classification. However, building large scale\ndatasets with such annotations is a mammoth task. Moreover, this extensive\nannotation is time-consuming and often requires expertise, which is a huge\nbottleneck in building large datasets. On the other hand, self-supervised\nlearning (SSL) exploits the freely available data to generate supervisory\nsignals which act as labels. The features learnt by performing some pretext\ntasks on huge unlabelled data proves to be very helpful for multiple downstream\ntasks.\n\nOur idea is to leverage self-supervision such that the model learns useful\nrepresentations of fine-grained image classes. We experimented with 3 kinds of\nmodels: Jigsaw solving as pretext task, adversarial learning (SRGAN) and\ncontrastive learning based (SimCLR) model. The learned features are used for\ndownstream tasks such as fine-grained image classification. Our code is\navailable at\nthis http URL",
          "link": "http://arxiv.org/abs/2107.13973",
          "publishedOn": "2021-07-30T02:13:27.750Z",
          "wordCount": 608,
          "title": "Self-Supervised Learning for Fine-Grained Image Classification. (arXiv:2107.13973v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Philippe Burlina</a>",
          "description": "When deploying artificial intelligence (AI) in the real world, being able to\ntrust the operation of the AI by characterizing how it performs is an\never-present and important topic. An important and still largely unexplored\ntask in this characterization is determining major factors within the real\nworld that affect the AI's behavior, such as weather conditions or lighting,\nand either a) being able to give justification for why it may have failed or b)\neliminating the influence the factor has. Determining these sensitive factors\nheavily relies on collected data that is diverse enough to cover numerous\ncombinations of these factors, which becomes more onerous when having many\npotential sensitive factors or operating in complex environments. This paper\ninvestigates methods that discover and separate out individual semantic\nsensitive factors from a given dataset to conduct this characterization as well\nas addressing mitigation of these factors' sensitivity. We also broaden\nremediation of fairness, which normally only addresses socially relevant\nfactors, and widen it to deal with the desensitization of AI with regard to all\npossible aspects of variation in the domain. The proposed methods which\ndiscover these major factors reduce the potentially onerous demands of\ncollecting a sufficiently diverse dataset. In experiments using the road sign\n(GTSRB) and facial imagery (CelebA) datasets, we show the promise of using this\nscheme to perform this characterization and remediation and demonstrate that\nour approach outperforms state of the art approaches.",
          "link": "http://arxiv.org/abs/2107.13625",
          "publishedOn": "2021-07-30T02:13:27.728Z",
          "wordCount": 674,
          "title": "Generalizing Fairness: Discovery and Mitigation of Unknown Sensitive Attributes. (arXiv:2107.13625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinmin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinzhu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jun Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>",
          "description": "As a crucial task of autonomous driving, 3D object detection has made great\nprogress in recent years. However, monocular 3D object detection remains a\nchallenging problem due to the unsatisfactory performance in depth estimation.\nMost existing monocular methods typically directly regress the scene depth\nwhile ignoring important relationships between the depth and various geometric\nelements (e.g. bounding box sizes, 3D object dimensions, and object poses). In\nthis paper, we propose to learn geometry-guided depth estimation with\nprojective modeling to advance monocular 3D object detection. Specifically, a\nprincipled geometry formula with projective modeling of 2D and 3D depth\npredictions in the monocular 3D object detection network is devised. We further\nimplement and embed the proposed formula to enable geometry-aware deep\nrepresentation learning, allowing effective 2D and 3D interactions for boosting\nthe depth estimation. Moreover, we provide a strong baseline through addressing\nsubstantial misalignment between 2D annotation and projected boxes to ensure\nrobust learning with the proposed geometric formula. Experiments on the KITTI\ndataset show that our method remarkably improves the detection performance of\nthe state-of-the-art monocular-based method without extra data by 2.80% on the\nmoderate test setting. The model and code will be released at\nhttps://github.com/YinminZhang/MonoGeo.",
          "link": "http://arxiv.org/abs/2107.13931",
          "publishedOn": "2021-07-30T02:13:27.720Z",
          "wordCount": 650,
          "title": "Learning Geometry-Guided Depth via Projective Modeling for Monocular 3D Object Detection. (arXiv:2107.13931v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korschens_M/0/1/0/all/0/1\">Matthias K&#xf6;rschens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodesheim_P/0/1/0/all/0/1\">Paul Bodesheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romermann_C/0/1/0/all/0/1\">Christine R&#xf6;mermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_S/0/1/0/all/0/1\">Solveig Franziska Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migliavacca_M/0/1/0/all/0/1\">Mirco Migliavacca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulrich_J/0/1/0/all/0/1\">Josephine Ulrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "Monitoring the responses of plants to environmental changes is essential for\nplant biodiversity research. This, however, is currently still being done\nmanually by botanists in the field. This work is very laborious, and the data\nobtained is, though following a standardized method to estimate plant coverage,\nusually subjective and has a coarse temporal resolution. To remedy these\ncaveats, we investigate approaches using convolutional neural networks (CNNs)\nto automatically extract the relevant data from images, focusing on plant\ncommunity composition and species coverages of 9 herbaceous plant species. To\nthis end, we investigate several standard CNN architectures and different\npretraining methods. We find that we outperform our previous approach at higher\nimage resolutions using a custom CNN with a mean absolute error of 5.16%. In\naddition to these investigations, we also conduct an error analysis based on\nthe temporal aspect of the plant cover images. This analysis gives insight into\nwhere problems for automatic approaches lie, like occlusion and likely\nmisclassifications caused by temporal changes.",
          "link": "http://arxiv.org/abs/2106.11154",
          "publishedOn": "2021-07-29T02:00:11.252Z",
          "wordCount": 644,
          "title": "Automatic Plant Cover Estimation with Convolutional Neural Networks. (arXiv:2106.11154v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.03860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pellikka_M/0/1/0/all/0/1\">Matti Pellikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahtinen_V/0/1/0/all/0/1\">Valtteri Lahtinen</a>",
          "description": "We propose a novel method for large-scale image stitching that is robust\nagainst repetitive patterns and featureless regions in the imagery. In such\ncases, state-of-the-art image stitching methods easily produce image alignment\nartifacts, since they may produce false pairwise image registrations that are\nin conflict within the global connectivity graph. Our method augments the\ncurrent methods by collecting all the plausible pairwise image registration\ncandidates, among which globally consistent candidates are chosen. This enables\nthe stitching process to determine the correct pairwise registrations by\nutilizing all the available information from the whole imagery, such as\nunambiguous registrations outside the repeating pattern and featureless\nregions. We formalize the method as a weighted multigraph whose nodes represent\nthe individual image transformations from the composite image, and whose sets\nof multiple edges between two nodes represent all the plausible transformations\nbetween the pixel coordinates of the two images. The edge weights represent the\nplausibility of the transformations. The image transformations and the edge\nweights are solved from a non-linear minimization problem with linear\nconstraints, for which a projection method is used. As an example, we apply the\nmethod in a large-scale scanning application where the transformations are\nprimarily translations with only slight rotation and scaling component. Despite\nthese simplifications, the state-of-the-art methods do not produce adequate\nresults in such applications, since the image overlap is small, which can be\nfeatureless or repetitive, and misalignment artifacts and their concealment are\nunacceptable.",
          "link": "http://arxiv.org/abs/2004.03860",
          "publishedOn": "2021-07-29T02:00:11.244Z",
          "wordCount": 717,
          "title": "A Robust Method for Image Stitching. (arXiv:2004.03860v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shepley_A/0/1/0/all/0/1\">Andrew Shepley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falzon_G/0/1/0/all/0/1\">Greg Falzon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwan_P/0/1/0/all/0/1\">Paul Kwan</a>",
          "description": "Confluence is a novel non-Intersection over Union (IoU) alternative to\nNon-Maxima Suppression (NMS) in bounding box post-processing in object\ndetection. It overcomes the inherent limitations of IoU-based NMS variants to\nprovide a more stable, consistent predictor of bounding box clustering by using\na normalized Manhattan Distance inspired proximity metric to represent bounding\nbox clustering. Unlike Greedy and Soft NMS, it does not rely solely on\nclassification confidence scores to select optimal bounding boxes, instead\nselecting the box which is closest to every other box within a given cluster\nand removing highly confluent neighboring boxes. Confluence is experimentally\nvalidated on the MS COCO and CrowdHuman benchmarks, improving Average Precision\nby up to 2.3-3.8% and Average Recall by up to 5.3-7.2% when compared against\nde-facto standard and state of the art NMS variants. Quantitative results are\nsupported by extensive qualitative analysis and threshold sensitivity analysis\nexperiments support the conclusion that Confluence is more robust than NMS\nvariants. Confluence represents a paradigm shift in bounding box processing,\nwith potential to replace IoU in bounding box regression processes.",
          "link": "http://arxiv.org/abs/2012.00257",
          "publishedOn": "2021-07-29T02:00:11.228Z",
          "wordCount": 645,
          "title": "Confluence: A Robust Non-IoU Alternative to Non-Maxima Suppression in Object Detection. (arXiv:2012.00257v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagemann_A/0/1/0/all/0/1\">Annika Hagemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knorr_M/0/1/0/all/0/1\">Moritz Knorr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janssen_H/0/1/0/all/0/1\">Holger Janssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>",
          "description": "Accurate camera calibration is a precondition for many computer vision\napplications. Calibration errors, such as wrong model assumptions or imprecise\nparameter estimation, can deteriorate a system's overall performance, making\nthe reliable detection and quantification of these errors critical. In this\nwork, we introduce an evaluation scheme to capture the fundamental error\nsources in camera calibration: systematic errors (biases) and uncertainty\n(variance). The proposed bias detection method uncovers smallest systematic\nerrors and thereby reveals imperfections of the calibration setup and provides\nthe basis for camera model selection. A novel resampling-based uncertainty\nestimator enables uncertainty estimation under non-ideal conditions and thereby\nextends the classical covariance estimator. Furthermore, we derive a simple\nuncertainty metric that is independent of the camera model. In combination, the\nproposed methods can be used to assess the accuracy of individual calibrations,\nbut also to benchmark new calibration algorithms, camera models, or calibration\nsetups. We evaluate the proposed methods with simulations and real cameras.",
          "link": "http://arxiv.org/abs/2107.13484",
          "publishedOn": "2021-07-29T02:00:11.192Z",
          "wordCount": 589,
          "title": "Inferring bias and uncertainty in camera calibration. (arXiv:2107.13484v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.",
          "link": "http://arxiv.org/abs/2107.13469",
          "publishedOn": "2021-07-29T02:00:11.185Z",
          "wordCount": 605,
          "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03577",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Franco_Barranco_D/0/1/0/all/0/1\">Daniel Franco-Barranco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munoz_Barrutia_A/0/1/0/all/0/1\">Arrate Mu&#xf1;oz-Barrutia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arganda_Carreras_I/0/1/0/all/0/1\">Ignacio Arganda-Carreras</a>",
          "description": "Electron microscopy (EM) allows the identification of intracellular\norganelles such as mitochondria, providing insights for clinical and scientific\nstudies. In recent years, a number of novel deep learning architectures have\nbeen published reporting superior performance, or even human-level accuracy,\ncompared to previous approaches on public mitochondria segmentation datasets.\nUnfortunately, many of these publications do not make neither the code nor the\nfull training details public to support the results obtained, leading to\nreproducibility issues and dubious model comparisons. For that reason, and\nfollowing a recent code of best practices for reporting experimental results,\nwe present an extensive study of the state-of-the-art deep learning\narchitectures for the segmentation of mitochondria on EM volumes, and evaluate\nthe impact in performance of different variations of 2D and 3D U-Net-like\nmodels for this task. To better understand the contribution of each component,\na common set of pre- and post-processing operations has been implemented and\ntested with each approach. Moreover, an exhaustive sweep of hyperparameters\nvalues for all architectures have been performed and each configuration has\nbeen run multiple times to report the mean and standard deviation values of the\nevaluation metrics. Using this methodology, we found very stable architectures\nand hyperparameter configurations that consistently obtain state-of-the-art\nresults in the well-known EPFL Hippocampus mitochondria segmentation dataset.\nFurthermore, we have benchmarked our proposed models on two other available\ndatasets, Lucchi++ and Kasthuri++, where they outperform all previous works.\nThe code derived from this research and its documentation are publicly\navailable.",
          "link": "http://arxiv.org/abs/2104.03577",
          "publishedOn": "2021-07-29T02:00:11.149Z",
          "wordCount": 711,
          "title": "Stable deep neural network architectures for mitochondria segmentation on electron microscopy volumes. (arXiv:2104.03577v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "In recent years, the growth of Machine Learning (ML) algorithms has raised\nthe number of studies including their applicability in a variety of different\nscenarios. Among all, one of the hardest ones is the aerospace, due to its\npeculiar physical requirements. In this context, a feasibility study and a\nfirst prototype for an Artificial Intelligence (AI) model to be deployed on\nboard satellites are presented in this work. As a case study, the detection of\nvolcanic eruptions has been investigated as a method to swiftly produce alerts\nand allow immediate interventions. Two Convolutional Neural Networks (CNNs)\nhave been proposed and designed, showing how to efficiently implement them for\nidentifying the eruptions and at the same time adapting their complexity in\norder to fit on board requirements.",
          "link": "http://arxiv.org/abs/2106.15281",
          "publishedOn": "2021-07-29T02:00:10.339Z",
          "wordCount": 602,
          "title": "On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Liangxiu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleerekoper_A/0/1/0/all/0/1\">Anthony Kleerekoper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tongle Hu</a>",
          "description": "Late blight disease is one of the most destructive diseases in potato crop,\nleading to serious yield losses globally. Accurate diagnosis of the disease at\nearly stage is critical for precision disease control and management. Current\nfarm practices in crop disease diagnosis are based on manual visual inspection,\nwhich is costly, time consuming, subject to individual bias. Recent advances in\nimaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote\nsensing and machine learning offer the opportunity to address this challenge.\nParticularly, hyperspectral imagery (HSI) combining with machine learning/deep\nlearning approaches is preferable for accurately identifying specific plant\ndiseases because the HSI consists of a wide range of high-quality reflectance\ninformation beyond human vision, capable of capturing both spectral-spatial\ninformation. The proposed method considers the potential disease specific\nreflectance radiation variance caused by the canopy structural diversity,\nintroduces the multiple capsule layers to model the hierarchical structure of\nthe spectral-spatial disease attributes with the encapsulated features to\nrepresent the various classes and the rotation invariance of the disease\nattributes in the feature space. We have evaluated the proposed method with the\nreal UAV-based HSI data under the controlled field conditions. The\neffectiveness of the hierarchical features has been quantitatively assessed and\ncompared with the existing representative machine learning/deep learning\nmethods. The experiment results show that the proposed model significantly\nimproves the accuracy performance when considering hierarchical-structure of\nspectral-spatial features, comparing to the existing methods only using\nspectral, or spatial or spectral-spatial features without consider\nhierarchical-structure of spectral-spatial features.",
          "link": "http://arxiv.org/abs/2107.13277",
          "publishedOn": "2021-07-29T02:00:10.166Z",
          "wordCount": 707,
          "title": "A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery. (arXiv:2107.13277v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feeney_P/0/1/0/all/0/1\">Patrick Feeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "The pixelwise reconstruction error of deep autoencoders is often utilized for\nimage novelty detection and localization under the assumption that pixels with\nhigh error indicate which parts of the input image are unfamiliar and therefore\nlikely to be novel. This assumed correlation between pixels with high\nreconstruction error and novel regions of input images has not been verified\nand may limit the accuracy of these methods. In this paper we utilize saliency\nmaps to evaluate whether this correlation exists. Saliency maps reveal directly\nhow much a change in each input pixel would affect reconstruction loss, while\neach pixel's reconstruction error may be attributed to many input pixels when\nlayers are fully connected. We compare saliency maps to reconstruction error\nmaps via qualitative visualizations as well as quantitative correspondence\nbetween the top K elements of the maps for both novel and normal images. Our\nresults indicate that reconstruction error maps do not closely correlate with\nthe importance of pixels in the input images, making them insufficient for\nnovelty localization.",
          "link": "http://arxiv.org/abs/2107.13379",
          "publishedOn": "2021-07-29T02:00:10.151Z",
          "wordCount": 606,
          "title": "Evaluating the Use of Reconstruction Error for Novelty Localization. (arXiv:2107.13379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiufu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zhihui Lai</a>",
          "description": "Though widely used in image classification, convolutional neural networks\n(CNNs) are prone to noise interruptions, i.e. the CNN output can be drastically\nchanged by small image noise. To improve the noise robustness, we try to\nintegrate CNNs with wavelet by replacing the common down-sampling (max-pooling,\nstrided-convolution, and average pooling) with discrete wavelet transform\n(DWT). We firstly propose general DWT and inverse DWT (IDWT) layers applicable\nto various orthogonal and biorthogonal discrete wavelets like Haar, Daubechies,\nand Cohen, etc., and then design wavelet integrated CNNs (WaveCNets) by\nintegrating DWT into the commonly used CNNs (VGG, ResNets, and DenseNet).\nDuring the down-sampling, WaveCNets apply DWT to decompose the feature maps\ninto the low-frequency and high-frequency components. Containing the main\ninformation including the basic object structures, the low-frequency component\nis transmitted into the following layers to generate robust high-level\nfeatures. The high-frequency components are dropped to remove most of the data\nnoises. The experimental results show that %wavelet accelerates the CNN\ntraining, and WaveCNets achieve higher accuracy on ImageNet than various\nvanilla CNNs. We have also tested the performance of WaveCNets on the noisy\nversion of ImageNet, ImageNet-C and six adversarial attacks, the results\nsuggest that the proposed DWT/IDWT layers could provide better noise-robustness\nand adversarial robustness. When applying WaveCNets as backbones, the\nperformance of object detectors (i.e., faster R-CNN and RetinaNet) on COCO\ndetection dataset are consistently improved. We believe that suppression of\naliasing effect, i.e. separation of low frequency and high frequency\ninformation, is the main advantages of our approach. The code of our DWT/IDWT\nlayer and different WaveCNets are available at\nhttps://github.com/CVI-SZU/WaveCNet.",
          "link": "http://arxiv.org/abs/2107.13335",
          "publishedOn": "2021-07-29T02:00:10.141Z",
          "wordCount": 720,
          "title": "WaveCNet: Wavelet Integrated CNNs to Suppress Aliasing Effect for Noise-Robust Image Classification. (arXiv:2107.13335v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhigao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yin Zhao</a>",
          "description": "We conduct a subjective experiment to compare the performance of traditional\nimage coding methods and learning-based image coding methods. HEVC and VVC, the\nstate-of-the-art traditional coding methods, are used as the representative\ntraditional methods. The learning-based methods used contain not only CNN-based\nmethods, but also a GAN-based method, all of which are advanced or typical.\nSingle Stimuli (SS), which is also called Absolute Category Rating (ACR), is\nadopted as the methodology of the experiment to obtain perceptual quality of\nimages. Additionally, we utilize some typical and frequently used objective\nquality metrics to evaluate the coding methods in the experiment as comparison.\nThe experiment shows that CNN-based and GAN-based methods can perform better\nthan traditional methods in low bit-rates. In high bit-rates, however, it is\nhard to verify whether CNN-based methods are superior to traditional methods.\nBecause the GAN method does not provide models with high target bit-rates, we\ncannot exactly tell the performance of the GAN method in high bit-rates.\nFurthermore, some popular objective quality metrics have not shown the ability\nwell to measure quality of images generated by learning-based coding methods,\nespecially the GAN-based one.",
          "link": "http://arxiv.org/abs/2107.13122",
          "publishedOn": "2021-07-29T02:00:10.118Z",
          "wordCount": 631,
          "title": "Subjective evaluation of traditional and learning-based image coding methods. (arXiv:2107.13122v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenjiang Liu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chengji Shen</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Ou_K/0/1/0/all/0/1\">Kairi Ou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haihong Tang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a> (2) ((1) Alibaba Group, (2) Zhejiang University)",
          "description": "Image virtual try-on task has abundant applications and has become a hot\nresearch topic recently. Existing 2D image-based virtual try-on methods aim to\ntransfer a target clothing image onto a reference person, which has two main\ndisadvantages: cannot control the size and length precisely; unable to\naccurately estimate the user's figure in the case of users wearing thick\nclothes, resulting in inaccurate dressing effect. In this paper, we put forward\nan akin task that aims to dress clothing for underwear models. %, which is also\nan urgent need in e-commerce scenarios. To solve the above drawbacks, we\npropose a Shape Controllable Virtual Try-On Network (SC-VTON), where a graph\nattention network integrates the information of model and clothing to generate\nthe warped clothing image. In addition, the control points are incorporated\ninto SC-VTON for the desired clothing shape. Furthermore, by adding a Splitting\nNetwork and a Synthesis Network, we can use clothing/model pair data to help\noptimize the deformation module and generalize the task to the typical virtual\ntry-on task. Extensive experiments show that the proposed method can achieve\naccurate shape control. Meanwhile, compared with other methods, our method can\ngenerate high-resolution results with detailed textures.",
          "link": "http://arxiv.org/abs/2107.13156",
          "publishedOn": "2021-07-29T02:00:10.105Z",
          "wordCount": 658,
          "title": "Shape Controllable Virtual Try-on for Underwear Models. (arXiv:2107.13156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanda Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongrun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuesheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yalin Zheng</a>",
          "description": "Semi-supervised approaches for crowd counting attract attention, as the fully\nsupervised paradigm is expensive and laborious due to its request for a large\nnumber of images of dense crowd scenarios and their annotations. This paper\nproposes a spatial uncertainty-aware semi-supervised approach via regularized\nsurrogate task (binary segmentation) for crowd counting problems. Different\nfrom existing semi-supervised learning-based crowd counting methods, to exploit\nthe unlabeled data, our proposed spatial uncertainty-aware teacher-student\nframework focuses on high confident regions' information while addressing the\nnoisy supervision from the unlabeled data in an end-to-end manner.\nSpecifically, we estimate the spatial uncertainty maps from the teacher model's\nsurrogate task to guide the feature learning of the main task (density\nregression) and the surrogate task of the student model at the same time.\nBesides, we introduce a simple yet effective differential transformation layer\nto enforce the inherent spatial consistency regularization between the main\ntask and the surrogate task in the student model, which helps the surrogate\ntask to yield more reliable predictions and generates high-quality uncertainty\nmaps. Thus, our model can also address the task-level perturbation problems\nthat occur spatial inconsistency between the primary and surrogate tasks in the\nstudent model. Experimental results on four challenging crowd counting datasets\ndemonstrate that our method achieves superior performance to the\nstate-of-the-art semi-supervised methods.",
          "link": "http://arxiv.org/abs/2107.13271",
          "publishedOn": "2021-07-29T02:00:10.004Z",
          "wordCount": 657,
          "title": "Spatial Uncertainty-Aware Semi-Supervised Crowd Counting. (arXiv:2107.13271v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>",
          "description": "Camera calibration is an important prerequisite towards the solution of 3D\ncomputer vision problems. Traditional methods rely on static images of a\ncalibration pattern. This raises interesting challenges towards the practical\nusage of event cameras, which notably require image change to produce\nsufficient measurements. The current standard for event camera calibration\ntherefore consists of using flashing patterns. They have the advantage of\nsimultaneously triggering events in all reprojected pattern feature locations,\nbut it is difficult to construct or use such patterns in the field. We present\nthe first dynamic event camera calibration algorithm. It calibrates directly\nfrom events captured during relative motion between camera and calibration\npattern. The method is propelled by a novel feature extraction mechanism for\ncalibration patterns, and leverages existing calibration tools before\noptimizing all parameters through a multi-segment continuous-time formulation.\nAs demonstrated through our results on real data, the obtained calibration\nmethod is highly convenient and reliably calibrates from data sequences\nspanning less than 10 seconds.",
          "link": "http://arxiv.org/abs/2107.06749",
          "publishedOn": "2021-07-29T02:00:08.913Z",
          "wordCount": 629,
          "title": "Dynamic Event Camera Calibration. (arXiv:2107.06749v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub. To the best of our knowledge we believe\nthis the first works to try out semi-supervised learning to improve flood\nsegmentation models.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-29T02:00:08.850Z",
          "wordCount": 774,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:08.842Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1\">Alex Rogozhnikov</a>",
          "description": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2106.03143",
          "publishedOn": "2021-07-29T02:00:08.818Z",
          "wordCount": 607,
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Di Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "3D point cloud completion is very challenging because it heavily relies on\nthe accurate understanding of the complex 3D shapes (e.g., high-curvature,\nconcave/convex, and hollowed-out 3D shapes) and the unknown & diverse patterns\nof the partially available point clouds. In this paper, we propose a novel\nsolution,i.e., Point-block Carving (PC), for completing the complex 3D point\ncloud completion. Given the partial point cloud as the guidance, we carve a3D\nblock that contains the uniformly distributed 3D points, yielding the entire\npoint cloud. To achieve PC, we propose a new network architecture, i.e.,\nCarveNet. This network conducts the exclusive convolution on each point of the\nblock, where the convolutional kernels are trained on the 3D shape data.\nCarveNet determines which point should be carved, for effectively recovering\nthe details of the complete shapes. Furthermore, we propose a sensor-aware\nmethod for data augmentation,i.e., SensorAug, for training CarveNet on richer\npatterns of partial point clouds, thus enhancing the completion power of the\nnetwork. The extensive evaluations on the ShapeNet and KITTI datasets\ndemonstrate the generality of our approach on the partial point clouds with\ndiverse patterns. On these datasets, CarveNet successfully outperforms the\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.13452",
          "publishedOn": "2021-07-29T02:00:08.799Z",
          "wordCount": 643,
          "title": "CarveNet: Carving Point-Block for Complex 3D Shape Completion. (arXiv:2107.13452v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09405",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schirris_Y/0/1/0/all/0/1\">Yoni Schirris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nederlof_I/0/1/0/all/0/1\">Iris Nederlof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horlings_H/0/1/0/all/0/1\">Hugo Mark Horlings</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a>",
          "description": "We propose a Deep learning-based weak label learning method for analysing\nwhole slide images (WSIs) of Hematoxylin and Eosin (H&E) stained tumorcells not\nrequiring pixel-level or tile-level annotations using Self-supervised\npre-training and heterogeneity-aware deep Multiple Instance LEarning\n(DeepSMILE). We apply DeepSMILE to the task of Homologous recombination\ndeficiency (HRD) and microsatellite instability (MSI) prediction. We utilize\ncontrastive self-supervised learning to pre-train a feature extractor on\nhistopathology tiles of cancer tissue. Additionally, we use variability-aware\ndeep multiple instance learning to learn the tile feature aggregation function\nwhile modeling tumor heterogeneity. Compared to state-of-the-art genomic label\nclassification methods, DeepSMILE improves classification performance for HRD\nfrom $70.43\\pm4.10\\%$ to $83.79\\pm1.25\\%$ AUC and MSI from $78.56\\pm6.24\\%$ to\n$90.32\\pm3.58\\%$ AUC in a multi-center breast and colorectal cancer dataset,\nrespectively. These improvements suggest we can improve genomic label\nclassification performance without collecting larger datasets. In the future,\nthis may reduce the need for expensive genome sequencing techniques, provide\npersonalized therapy recommendations based on widely available WSIs of cancer\ntissue, and improve patient care with quicker treatment decisions - also in\nmedical centers without access to genome sequencing resources.",
          "link": "http://arxiv.org/abs/2107.09405",
          "publishedOn": "2021-07-29T02:00:08.791Z",
          "wordCount": 673,
          "title": "DeepSMILE: Self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&E whole-slide images. (arXiv:2107.09405v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Edward S. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>",
          "description": "Training visuomotor robot controllers from scratch on a new robot typically\nrequires generating large amounts of robot-specific data. Could we leverage\ndata previously collected on another robot to reduce or even completely remove\nthis need for robot-specific data? We propose a \"robot-aware\" solution paradigm\nthat exploits readily available robot \"self-knowledge\" such as proprioception,\nkinematics, and camera calibration to achieve this. First, we learn modular\ndynamics models that pair a transferable, robot-agnostic world dynamics module\nwith a robot-specific, analytical robot dynamics module. Next, we set up visual\nplanning costs that draw a distinction between the robot self and the world.\nOur experiments on tabletop manipulation tasks in simulation and on real robots\ndemonstrate that these plug-in improvements dramatically boost the\ntransferability of visuomotor controllers, even permitting zero-shot transfer\nonto new robots for the very first time. Project website:\nhttps://hueds.github.io/rac/",
          "link": "http://arxiv.org/abs/2107.09047",
          "publishedOn": "2021-07-29T02:00:08.783Z",
          "wordCount": 603,
          "title": "Know Thyself: Transferable Visuomotor Control Through Robot-Awareness. (arXiv:2107.09047v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Domain adaptation is to transfer the shared knowledge learned from the source\ndomain to a new environment, i.e., target domain. One common practice is to\ntrain the model on both labeled source-domain data and unlabeled target-domain\ndata. Yet the learned models are usually biased due to the strong supervision\nof the source domain. Most researchers adopt the early-stopping strategy to\nprevent over-fitting, but when to stop training remains a challenging problem\nsince the lack of the target-domain validation set. In this paper, we propose\none efficient bootstrapping method, called Adaboost Student, explicitly\nlearning complementary models during training and liberating users from\nempirical early stopping. Adaboost Student combines the deep model learning\nwith the conventional training strategy, i.e., adaptive boosting, and enables\ninteractions between learned models and the data sampler. We adopt one adaptive\ndata sampler to progressively facilitate learning on hard samples and aggregate\n\"weak\" models to prevent over-fitting. Extensive experiments show that (1)\nWithout the need to worry about the stopping time, AdaBoost Student provides\none robust solution by efficient complementary model learning during training.\n(2) AdaBoost Student is orthogonal to most domain adaptation methods, which can\nbe combined with existing approaches to further improve the state-of-the-art\nperformance. We have achieved competitive results on three widely-used scene\nsegmentation domain adaptation benchmarks.",
          "link": "http://arxiv.org/abs/2103.15685",
          "publishedOn": "2021-07-29T02:00:08.776Z",
          "wordCount": 684,
          "title": "Adaptive Boosting for Domain Adaptation: Towards Robust Predictions in Scene Segmentation. (arXiv:2103.15685v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowakowski_A/0/1/0/all/0/1\">Artur Nowakowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puglisi_E/0/1/0/all/0/1\">Erika Puglisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mifdal_J/0/1/0/all/0/1\">Jamila Mifdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirri_F/0/1/0/all/0/1\">Fiora Pirri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "The abundance of clouds, located both spatially and temporally, often makes\nremote sensing (RS) applications with optical images difficult or even\nimpossible to perform. Traditional cloud removing techniques have been studied\nfor years, and recently, Machine Learning (ML)-based approaches have also been\nconsidered. In this manuscript, a novel method for the restoration of\nclouds-corrupted optical images is presented, able to generate the whole\noptical scene of interest, not only the cloudy pixels, and based on a Joint\nData Fusion paradigm, where three deep neural networks are hierarchically\ncombined. Spatio-temporal features are separately extracted by a conditional\nGenerative Adversarial Network (cGAN) and by a Convolutional Long Short-Term\nMemory (ConvLSTM), from Synthetic Aperture Radar (SAR) data and optical\ntime-series of data respectively, and then combined with a U-shaped network.\nThe use of time-series of data has been rarely explored in the state of the art\nfor this peculiar objective, and moreover existing models do not combine both\nspatio-temporal domains and SAR-optical imagery. Quantitative and qualitative\nresults have shown a good ability of the proposed method in producing\ncloud-free images, by also preserving the details and outperforming the cGAN\nand the ConvLSTM when individually used. Both the code and the dataset have\nbeen implemented from scratch and made available to interested researchers for\nfurther analysis and investigation.",
          "link": "http://arxiv.org/abs/2106.12226",
          "publishedOn": "2021-07-29T02:00:08.768Z",
          "wordCount": 710,
          "title": "Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengbo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zitang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weilian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamata_S/0/1/0/all/0/1\">Sei-ichiro Kamata</a>",
          "description": "Various deep neural network architectures (DNNs) maintain massive vital\nrecords in computer vision. While drawing attention worldwide, the design of\nthe overall structure lacks general guidance. Based on the relationship between\nDNN design and numerical differential equations, we performed a fair comparison\nof the residual design with higher-order perspectives. We show that the widely\nused DNN design strategy, constantly stacking a small design (usually 2-3\nlayers), could be easily improved, supported by solid theoretical knowledge and\nwith no extra parameters needed. We reorganise the residual design in\nhigher-order ways, which is inspired by the observation that many effective\nnetworks can be interpreted as different numerical discretisations of\ndifferential equations. The design of ResNet follows a relatively simple\nscheme, which is Euler forward; however, the situation becomes complicated\nrapidly while stacking. We suppose that stacked ResNet is somehow equalled to a\nhigher-order scheme; then, the current method of forwarding propagation might\nbe relatively weak compared with a typical high-order method such as\nRunge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV\nbenchmarks with sufficient experiments. Stable and noticeable increases in\nperformance are observed, and convergence and robustness are also improved. Our\nstacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per\ncent on CIFAR-10, with the same settings and parameters. The proposed strategy\nis fundamental and theoretical and can therefore be applied to any network as a\ngeneral guideline.",
          "link": "http://arxiv.org/abs/2103.15244",
          "publishedOn": "2021-07-29T02:00:08.760Z",
          "wordCount": 727,
          "title": "Rethinking ResNets: Improved Stacking Strategies With High Order Schemes. (arXiv:2103.15244v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1\">J. K. Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1\">Mario Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwis_K/0/1/0/all/0/1\">Kusal De Alwis</a>",
          "description": "The general approach taken when training deep learning classifiers is to save\nthe parameters after every few iterations, train until either a human observer\nor a simple metric-based heuristic decides the network isn't learning anymore,\nand then backtrack and pick the saved parameters with the best validation\naccuracy. Simple methods are used to determine if a neural network isn't\nlearning anymore because, as long as it's well after the optimal values are\nfound, the condition doesn't impact the final accuracy of the model. However\nfrom a runtime perspective, this is of great significance to the many cases\nwhere numerous neural networks are trained simultaneously (e.g. hyper-parameter\ntuning). Motivated by this, we introduce a statistical significance test to\ndetermine if a neural network has stopped learning. This stopping criterion\nappears to represent a happy medium compared to other popular stopping\ncriterions, achieving comparable accuracy to the criterions that achieve the\nhighest final accuracies in 77% or fewer epochs, while the criterions which\nstop sooner do so with an appreciable loss to final accuracy. Additionally, we\nuse this as the basis of a new learning rate scheduler, removing the need to\nmanually choose learning rate schedules and acting as a quasi-line search,\nachieving superior or comparable empirical performance to existing methods.",
          "link": "http://arxiv.org/abs/2103.01205",
          "publishedOn": "2021-07-29T02:00:08.752Z",
          "wordCount": 691,
          "title": "Statistically Significant Stopping of Neural Network Training. (arXiv:2103.01205v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Weakly-supervised temporal action localization (WS-TAL) aims to localize\nactions in untrimmed videos with only video-level labels. Most existing models\nfollow the \"localization by classification\" procedure: locate temporal regions\ncontributing most to the video-level classification. Generally, they process\neach snippet (or frame) individually and thus overlook the fruitful temporal\ncontext relation. Here arises the single snippet cheating issue: \"hard\"\nsnippets are too vague to be classified. In this paper, we argue that learning\nby comparing helps identify these hard snippets and we propose to utilize\nsnippet Contrastive learning to Localize Actions, CoLA for short. Specifically,\nwe propose a Snippet Contrast (SniCo) Loss to refine the hard snippet\nrepresentation in feature space, which guides the network to perceive precise\ntemporal boundaries and avoid the temporal interval interruption. Besides,\nsince it is infeasible to access frame-level annotations, we introduce a Hard\nSnippet Mining algorithm to locate the potential hard snippets. Substantial\nanalyses verify that this mining strategy efficaciously captures the hard\nsnippets and SniCo Loss leads to more informative feature representation.\nExtensive experiments show that CoLA achieves state-of-the-art results on\nTHUMOS'14 and ActivityNet v1.2 datasets. CoLA code is publicly available at\nhttps://github.com/zhang-can/CoLA.",
          "link": "http://arxiv.org/abs/2103.16392",
          "publishedOn": "2021-07-29T02:00:08.728Z",
          "wordCount": 669,
          "title": "CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning. (arXiv:2103.16392v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morales_D/0/1/0/all/0/1\">David Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1\">Estefania Talavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remeseiro_B/0/1/0/all/0/1\">Beatriz Remeseiro</a>",
          "description": "The field of deep learning is evolving in different directions, with still\nthe need for more efficient training strategies. In this work, we present a\nnovel and robust training scheme that integrates visual explanation techniques\nin the learning process. Unlike the attention mechanisms that focus on the\nrelevant parts of images, we aim to improve the robustness of the model by\nmaking it pay attention to other regions as well. Broadly speaking, the idea is\nto distract the classifier in the learning process to force it to focus not\nonly on relevant regions but also on those that, a priori, are not so\ninformative for the discrimination of the class. We tested the proposed\napproach by embedding it into the learning process of a convolutional neural\nnetwork for the analysis and classification of two well-known datasets, namely\nStanford cars and FGVC-Aircraft. Furthermore, we evaluated our model on a\nreal-case scenario for the classification of egocentric images, allowing us to\nobtain relevant information about peoples' lifestyles. In particular, we work\non the challenging EgoFoodPlaces dataset, achieving state-of-the-art results\nwith a lower level of complexity. The obtained results indicate the suitability\nof our proposed training scheme for image classification, improving the\nrobustness of the final model.",
          "link": "http://arxiv.org/abs/2012.14173",
          "publishedOn": "2021-07-29T02:00:08.714Z",
          "wordCount": 696,
          "title": "Playing to distraction: towards a robust training of CNN classifiers through visual explanation techniques. (arXiv:2012.14173v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>",
          "description": "Crowd counting has drawn much attention due to its importance in\nsafety-critical surveillance systems. Especially, deep neural network (DNN)\nmethods have significantly reduced estimation errors for crowd counting\nmissions. Recent studies have demonstrated that DNNs are vulnerable to\nadversarial attacks, i.e., normal images with human-imperceptible perturbations\ncould mislead DNNs to make false predictions. In this work, we propose a robust\nattack strategy called Adversarial Patch Attack with Momentum (APAM) to\nsystematically evaluate the robustness of crowd counting models, where the\nattacker's goal is to create an adversarial perturbation that severely degrades\ntheir performances, thus leading to public safety accidents (e.g., stampede\naccidents). Especially, the proposed attack leverages the extreme-density\nbackground information of input images to generate robust adversarial patches\nvia a series of transformations (e.g., interpolation, rotation, etc.). We\nobserve that by perturbing less than 6\\% of image pixels, our attacks severely\ndegrade the performance of crowd counting systems, both digitally and\nphysically. To better enhance the adversarial robustness of crowd counting\nmodels, we propose the first regression model-based Randomized Ablation (RA),\nwhich is more sufficient than Adversarial Training (ADT) (Mean Absolute Error\nof RA is 5 lower than ADT on clean samples and 30 lower than ADT on adversarial\nexamples). Extensive experiments on five crowd counting models demonstrate the\neffectiveness and generality of the proposed method. The supplementary\nmaterials and certificate retrained models are available at\n\\url{https://www.dropbox.com/s/hc4fdx133vht0qb/ACM_MM2021_Supp.pdf?dl=0}",
          "link": "http://arxiv.org/abs/2104.10868",
          "publishedOn": "2021-07-29T02:00:08.692Z",
          "wordCount": 719,
          "title": "Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting. (arXiv:2104.10868v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1\">Gowri Somanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>",
          "description": "We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2011.10687",
          "publishedOn": "2021-07-29T02:00:08.676Z",
          "wordCount": 658,
          "title": "HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xuefei Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Changcheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenshuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shuang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huazhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>",
          "description": "Conducting efficient performance estimations of neural architectures is a\nmajor challenge in neural architecture search (NAS). To reduce the architecture\ntraining costs in NAS, one-shot estimators (OSEs) amortize the architecture\ntraining costs by sharing the parameters of one supernet between all\narchitectures. Recently, zero-shot estimators (ZSEs) that involve no training\nare proposed to further reduce the architecture evaluation cost. Despite the\nhigh efficiency of these estimators, the quality of such estimations has not\nbeen thoroughly studied. In this paper, we conduct an extensive and organized\nassessment of OSEs and ZSEs on three NAS benchmarks: NAS-Bench-101/201/301.\nSpecifically, we employ a set of NAS-oriented criteria to study the behavior of\nOSEs and ZSEs and reveal that they have certain biases and variances. After\nanalyzing how and why the OSE estimations are unsatisfying, we explore how to\nmitigate the correlation gap of OSEs from several perspectives. For ZSEs, we\nfind that current ZSEs are not satisfying enough in these benchmark search\nspaces, and analyze their biases. Through our analysis, we give out suggestions\nfor future application and development of efficient architecture performance\nestimators. Furthermore, the analysis framework proposed in our work could be\nutilized in future research to give a more comprehensive understanding of newly\ndesigned architecture performance estimators. All codes and analysis scripts\nare available at https://github.com/walkerning/aw_nas.",
          "link": "http://arxiv.org/abs/2008.03064",
          "publishedOn": "2021-07-29T02:00:08.658Z",
          "wordCount": 706,
          "title": "Evaluating Efficient Performance Estimators of Neural Architectures. (arXiv:2008.03064v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiunn-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chenxi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achar_M/0/1/0/all/0/1\">Madhav Achar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grizzle_J/0/1/0/all/0/1\">Jessy W. Grizzle</a>",
          "description": "Sensor calibration, which can be intrinsic or extrinsic, is an essential step\nto achieve the measurement accuracy required for modern perception and\nnavigation systems deployed on autonomous robots. To date, intrinsic\ncalibration models for spinning LiDARs have been based on hypothesized based on\ntheir physical mechanisms, resulting in anywhere from three to ten parameters\nto be estimated from data, while no phenomenological models have yet been\nproposed for solid-state LiDARs. Instead of going down that road, we propose to\nabstract away from the physics of a LiDAR type (spinning vs solid-state, for\nexample), and focus on the spatial geometry of the point cloud generated by the\nsensor. By modeling the calibration parameters as an element of a special\nmatrix Lie Group, we achieve a unifying view of calibration for different types\nof LiDARs. We further prove mathematically that the proposed model is\nwell-constrained (has a unique answer) given four appropriately orientated\ntargets. The proof provides a guideline for target positioning in the form of a\ntetrahedron. Moreover, an existing Semidefinite programming global solver for\nSE(3) can be modified to compute efficiently the optimal calibration\nparameters. For solid state LiDARs, we illustrate how the method works in\nsimulation. For spinning LiDARs, we show with experimental data that the\nproposed matrix Lie Group model performs equally well as physics-based models\nin terms of reducing the P2P distance, while being more robust to noise.",
          "link": "http://arxiv.org/abs/2012.03321",
          "publishedOn": "2021-07-29T02:00:08.644Z",
          "wordCount": 701,
          "title": "Global Unifying Intrinsic Calibration for Spinning and Solid-State LiDARs. (arXiv:2012.03321v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13542",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wyburd_M/0/1/0/all/0/1\">Madeleine K. Wyburd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dinsdale_N/0/1/0/all/0/1\">Nicola K. Dinsdale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Namburete_A/0/1/0/all/0/1\">Ana I.L. Namburete</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jenkinson_M/0/1/0/all/0/1\">Mark Jenkinson</a>",
          "description": "Accurate topology is key when performing meaningful anatomical segmentations,\nhowever, it is often overlooked in traditional deep learning methods. In this\nwork we propose TEDS-Net: a novel segmentation method that guarantees accurate\ntopology. Our method is built upon a continuous diffeomorphic framework, which\nenforces topology preservation. However, in practice, diffeomorphic fields are\nrepresented using a finite number of parameters and sampled using methods such\nas linear interpolation, violating the theoretical guarantees. We therefore\nintroduce additional modifications to more strictly enforce it. Our network\nlearns how to warp a binary prior, with the desired topological\ncharacteristics, to complete the segmentation task. We tested our method on\nmyocardium segmentation from an open-source 2D heart dataset. TEDS-Net\npreserved topology in 100% of the cases, compared to 90% from the U-Net,\nwithout sacrificing on Hausdorff Distance or Dice performance. Code will be\nmade available at: www.github.com/mwyburd/TEDS-Net",
          "link": "http://arxiv.org/abs/2107.13542",
          "publishedOn": "2021-07-29T02:00:08.636Z",
          "wordCount": 613,
          "title": "TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations. (arXiv:2107.13542v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.08032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Timothy Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozier_J/0/1/0/all/0/1\">Jamell Dozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1\">Helen Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1\">Nishchal Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fr&#xe9;do Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Object recognition and viewpoint estimation lie at the heart of visual\nunderstanding. Recent works suggest that convolutional neural networks (CNNs)\nfail to generalize to out-of-distribution (OOD) category-viewpoint\ncombinations, ie. combinations not seen during training. In this paper, we\ninvestigate when and how such OOD generalization may be possible by evaluating\nCNNs trained to classify both object category and 3D viewpoint on OOD\ncombinations, and identifying the neural mechanisms that facilitate such OOD\ngeneralization. We show that increasing the number of in-distribution\ncombinations (ie. data diversity) substantially improves generalization to OOD\ncombinations, even with the same amount of training data. We compare learning\ncategory and viewpoint in separate and shared network architectures, and\nobserve starkly different trends on in-distribution and OOD combinations, ie.\nwhile shared networks are helpful in-distribution, separate networks\nsignificantly outperform shared ones at OOD combinations. Finally, we\ndemonstrate that such OOD generalization is facilitated by the neural mechanism\nof specialization, ie. the emergence of two types of neurons -- neurons\nselective to category and invariant to viewpoint, and vice versa.",
          "link": "http://arxiv.org/abs/2007.08032",
          "publishedOn": "2021-07-29T02:00:08.629Z",
          "wordCount": 654,
          "title": "When and how do CNNs generalize to out-of-distribution category-viewpoint combinations?. (arXiv:2007.08032v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pengyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "There has been a growing interest in unsupervised domain adaptation (UDA) to\nalleviate the data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is proposed for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vectors that violate the poset constraint by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.",
          "link": "http://arxiv.org/abs/2107.13467",
          "publishedOn": "2021-07-29T02:00:08.588Z",
          "wordCount": 624,
          "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eun_K/0/1/0/all/0/1\">Kim Ji Eun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>",
          "description": "Deep Generative Models (DGMs) are known for their superior capability in\ngenerating realistic data. Extending purely data-driven approaches, recent\nspecialized DGMs may satisfy additional controllable requirements such as\nembedding a traffic sign in a driving scene, by manipulating patterns\n\\textit{implicitly} in the neuron or feature level. In this paper, we introduce\na novel method to incorporate domain knowledge \\textit{explicitly} in the\ngeneration process to achieve semantically controllable scene generation. We\ncategorize our knowledge into two types to be consistent with the composition\nof natural scenes, where the first type represents the property of objects and\nthe second type represents the relationship among objects. We then propose a\ntree-structured generative model to learn complex scene representation, whose\nnodes and edges are naturally corresponding to the two types of knowledge\nrespectively. Knowledge can be explicitly integrated to enable semantically\ncontrollable scene generation by imposing semantic rules on properties of nodes\nand edges in the tree structure. We construct a synthetic example to illustrate\nthe controllability and explainability of our method in a clean setting. We\nfurther extend the synthetic example to realistic autonomous vehicle driving\nenvironments and conduct extensive experiments to show that our method\nefficiently identifies adversarial traffic scenes against different\nstate-of-the-art 3D point cloud segmentation models satisfying the traffic\nrules specified as the explicit knowledge.",
          "link": "http://arxiv.org/abs/2106.04066",
          "publishedOn": "2021-07-29T02:00:08.567Z",
          "wordCount": 703,
          "title": "Semantically Controllable Scene Generation with Guidance of Explicit Knowledge. (arXiv:2106.04066v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13407",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mora_Martin_G/0/1/0/all/0/1\">Germ&#xe1;n Mora-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turpin_A/0/1/0/all/0/1\">Alex Turpin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruget_A/0/1/0/all/0/1\">Alice Ruget</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halimi_A/0/1/0/all/0/1\">Abderrahim Halimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henderson_R/0/1/0/all/0/1\">Robert Henderson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leach_J/0/1/0/all/0/1\">Jonathan Leach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gyongy_I/0/1/0/all/0/1\">Istvan Gyongy</a>",
          "description": "3D time-of-flight (ToF) imaging is used in a variety of applications such as\naugmented reality (AR), computer interfaces, robotics and autonomous systems.\nSingle-photon avalanche diodes (SPADs) are one of the enabling technologies\nproviding accurate depth data even over long ranges. By developing SPADs in\narray format with integrated processing combined with pulsed, flood-type\nillumination, high-speed 3D capture is possible. However, array sizes tend to\nbe relatively small, limiting the lateral resolution of the resulting depth\nmaps, and, consequently, the information that can be extracted from the image\nfor applications such as object detection. In this paper, we demonstrate that\nthese limitations can be overcome through the use of convolutional neural\nnetworks (CNNs) for high-performance object detection. We present outdoor\nresults from a portable SPAD camera system that outputs 16-bin photon timing\nhistograms with 64x32 spatial resolution. The results, obtained with exposure\ntimes down to 2 ms (equivalent to 500 FPS) and in signal-to-background (SBR)\nratios as low as 0.05, point to the advantages of providing the CNN with full\nhistogram data rather than point clouds alone. Alternatively, a combination of\npoint cloud and active intensity data may be used as input, for a similar level\nof performance. In either case, the GPU-accelerated processing time is less\nthan 1 ms per frame, leading to an overall latency (image acquisition plus\nprocessing) in the millisecond range, making the results relevant for\nsafety-critical computer vision applications which would benefit from faster\nthan human reaction times.",
          "link": "http://arxiv.org/abs/2107.13407",
          "publishedOn": "2021-07-29T02:00:08.547Z",
          "wordCount": 705,
          "title": "High-speed object detection with a single-photon time-of-flight image sensor. (arXiv:2107.13407v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15564",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_Q/0/1/0/all/0/1\">Qingcheng Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_L/0/1/0/all/0/1\">Lin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">He Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_J/0/1/0/all/0/1\">Jiezhen Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jicong Zhang</a>",
          "description": "The novel Coronavirus disease (COVID-19) is a highly contagious virus and has\nspread all over the world, posing an extremely serious threat to all countries.\nAutomatic lung infection segmentation from computed tomography (CT) plays an\nimportant role in the quantitative analysis of COVID-19. However, the major\nchallenge lies in the inadequacy of annotated COVID-19 datasets. Currently,\nthere are several public non-COVID lung lesion segmentation datasets, providing\nthe potential for generalizing useful information to the related COVID-19\nsegmentation task. In this paper, we propose a novel relation-driven\ncollaborative learning model to exploit shared knowledge from non-COVID lesions\nfor annotation-efficient COVID-19 CT lung infection segmentation. The model\nconsists of a general encoder to capture general lung lesion features based on\nmultiple non-COVID lesions, and a target encoder to focus on task-specific\nfeatures based on COVID-19 infections. Features extracted from the two parallel\nencoders are concatenated for the subsequent decoder part. We develop a\ncollaborative learning scheme to regularize feature-level relation consistency\nof given input and encourage the model to learn more general and discriminative\nrepresentation of COVID-19 infections. Extensive experiments demonstrate that\ntrained with limited COVID-19 data, exploiting shared knowledge from non-COVID\nlesions can further improve state-of-the-art performance with up to 3.0% in\ndice similarity coefficient and 4.2% in normalized surface dice. Our proposed\nmethod promotes new insights into annotation-efficient deep learning for\nCOVID-19 infection segmentation and illustrates strong potential for real-world\napplications in the global fight against COVID-19 in the absence of sufficient\nhigh-quality annotations.",
          "link": "http://arxiv.org/abs/2012.15564",
          "publishedOn": "2021-07-29T02:00:08.536Z",
          "wordCount": 768,
          "title": "Exploiting Shared Knowledge from Non-COVID Lesions for Annotation-Efficient COVID-19 CT Lung Infection Segmentation. (arXiv:2012.15564v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Son_W/0/1/0/all/0/1\">Wonchul Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_J/0/1/0/all/0/1\">Jaemin Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Junyong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonjun Hwang</a>",
          "description": "With the success of deep neural networks, knowledge distillation which guides\nthe learning of a small student network from a large teacher network is being\nactively studied for model compression and transfer learning. However, few\nstudies have been performed to resolve the poor learning issue of the student\nnetwork when the student and teacher model sizes significantly differ. In this\npaper, we propose a densely guided knowledge distillation using multiple\nteacher assistants that gradually decreases the model size to efficiently\nbridge the large gap between the teacher and student networks. To stimulate\nmore efficient learning of the student network, we guide each teacher assistant\nto every other smaller teacher assistants iteratively. Specifically, when\nteaching a smaller teacher assistant at the next step, the existing larger\nteacher assistants from the previous step are used as well as the teacher\nnetwork. Moreover, we design stochastic teaching where, for each mini-batch, a\nteacher or teacher assistants are randomly dropped. This acts as a regularizer\nto improve the efficiency of teaching of the student network. Thus, the student\ncan always learn salient distilled knowledge from the multiple sources. We\nverified the effectiveness of the proposed method for a classification task\nusing CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant\nperformance improvements with various backbone architectures such as ResNet,\nWideResNet, and VGG.",
          "link": "http://arxiv.org/abs/2009.08825",
          "publishedOn": "2021-07-29T02:00:08.529Z",
          "wordCount": 684,
          "title": "Densely Guided Knowledge Distillation using Multiple Teacher Assistants. (arXiv:2009.08825v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weiherer_M/0/1/0/all/0/1\">Maximilian Weiherer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eigenberger_A/0/1/0/all/0/1\">Andreas Eigenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brebant_V/0/1/0/all/0/1\">Vanessa Br&#xe9;bant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prantl_L/0/1/0/all/0/1\">Lukas Prantl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palm_C/0/1/0/all/0/1\">Christoph Palm</a>",
          "description": "We present the Regensburg Breast Shape Model (RBSM) - a 3D statistical shape\nmodel of the female breast built from 110 breast scans, and the first ever\npublicly available. Together with the model, a fully automated, pairwise\nsurface registration pipeline used to establish correspondence among 3D breast\nscans is introduced. Our method is computationally efficient and requires only\nfour landmarks to guide the registration process. In order to weaken the strong\ncoupling between breast and thorax, we propose to minimize the variance outside\nthe breast region as much as possible. To achieve this goal, a novel concept\ncalled breast probability masks (BPMs) is introduced. A BPM assigns\nprobabilities to each point of a 3D breast scan, telling how likely it is that\na particular point belongs to the breast area. During registration, we use BPMs\nto align the template to the target as accurately as possible inside the breast\nregion and only roughly outside. This simple yet effective strategy\nsignificantly reduces the unwanted variance outside the breast region, leading\nto better statistical shape models in which breast shapes are quite well\ndecoupled from the thorax. The RBSM is thus able to produce a variety of\ndifferent breast shapes as independently as possible from the shape of the\nthorax. Our systematic experimental evaluation reveals a generalization ability\nof 0.17 mm and a specificity of 2.8 mm for the RBSM. Ultimately, our model is\nseen as a first step towards combining physically motivated deformable models\nof the breast and statistical approaches in order to enable more realistic\nsurgical outcome simulation.",
          "link": "http://arxiv.org/abs/2107.13463",
          "publishedOn": "2021-07-29T02:00:08.502Z",
          "wordCount": 745,
          "title": "Learning the shape of female breasts: an open-access 3D statistical shape model of the female breast built from 110 breast scans. (arXiv:2107.13463v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13431",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shuang Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Q/0/1/0/all/0/1\">Qiongyu Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Wenquan Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_D/0/1/0/all/0/1\">Desheng Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huabin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaobo Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_K/0/1/0/all/0/1\">Kehong Yuan</a>",
          "description": "Ultrasound is the preferred choice for early screening of dense breast\ncancer. Clinically, doctors have to manually write the screening report which\nis time-consuming and laborious, and it is easy to miss and miswrite.\nTherefore, this paper proposes a method for efficiently generating personalized\nbreast ultrasound screening preliminary reports by AI, especially for benign\nand normal cases which account for the majority. Doctors then make simple\nadjustments or corrections to quickly generate final reports. The proposed\napproach has been tested using a database of 1133 breast tumor instances.\nExperimental results indicate this pipeline improves doctors' work efficiency\nby up to 90%, which greatly reduces repetitive work.",
          "link": "http://arxiv.org/abs/2107.13431",
          "publishedOn": "2021-07-29T02:00:08.487Z",
          "wordCount": 559,
          "title": "AI assisted method for efficiently generating breast ultrasound screening reports. (arXiv:2107.13431v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaodan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xingxing Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dekui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Ying He</a>",
          "description": "The existing 3D deep learning methods adopt either individual point-based\nfeatures or local-neighboring voxel-based features, and demonstrate great\npotential for processing 3D data. However, the point based models are\ninefficient due to the unordered nature of point clouds and the voxel-based\nmodels suffer from large information loss. Motivated by the success of recent\npoint-voxel representation, such as PVCNN, we propose a new convolutional\nneural network, called Multi Point-Voxel Convolution (MPVConv), for deep\nlearning on point clouds. Integrating both the advantages of voxel and\npoint-based methods, MPVConv can effectively increase the neighboring\ncollection between point-based features and also promote independence among\nvoxel-based features. Moreover, most of the existing approaches aim at solving\none specific task, and only a few of them can handle a variety of tasks. Simply\nreplacing the corresponding convolution module with MPVConv, we show that\nMPVConv can fit in different backbones to solve a wide range of 3D tasks.\nExtensive experiments on benchmark datasets such as ShapeNet Part, S3DIS and\nKITTI for various tasks show that MPVConv improves the accuracy of the backbone\n(PointNet) by up to \\textbf{36\\%}, and achieves higher accuracy than the\nvoxel-based model with up to \\textbf{34}$\\times$ speedups. In addition, MPVConv\noutperforms the state-of-the-art point-based models with up to\n\\textbf{8}$\\times$ speedups. Notably, our MPVConv achieves better accuracy than\nthe newest point-voxel-based model PVCNN (a model more efficient than PointNet)\nwith lower latency.",
          "link": "http://arxiv.org/abs/2107.13152",
          "publishedOn": "2021-07-29T02:00:08.460Z",
          "wordCount": 679,
          "title": "Multi Point-Voxel Convolution (MPVConv) for Deep Learning on Point Clouds. (arXiv:2107.13152v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>",
          "description": "The computational vision community has recently paid attention to continual\nlearning for blind image quality assessment (BIQA). The primary challenge is to\ncombat catastrophic forgetting of previously-seen IQA datasets (i.e., tasks).\nIn this paper, we present a simple yet effective continual learning method for\nBIQA with improved quality prediction accuracy, plasticity-stability trade-off,\nand task-order/length robustness. The key step in our approach is to freeze all\nconvolution filters of a pre-trained deep neural network (DNN) for an explicit\npromise of stability, and learn task-specific normalization parameters for\nplasticity. We assign each new task a prediction head, and load the\ncorresponding normalization parameters to produce a quality score. The final\nquality estimate is computed by feature fusion and adaptive weighting using\nhierarchical representations, without leveraging the test-time oracle.\nExtensive experiments on six IQA datasets demonstrate the advantages of the\nproposed method in comparison to previous training techniques for BIQA.",
          "link": "http://arxiv.org/abs/2107.13429",
          "publishedOn": "2021-07-29T02:00:08.441Z",
          "wordCount": 591,
          "title": "Task-Specific Normalization for Continual Learning of Blind Image Quality Models. (arXiv:2107.13429v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14331",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1\">Abhranil Das</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Geisler_W/0/1/0/all/0/1\">Wilson S Geisler</a>",
          "description": "Univariate and multivariate normal probability distributions are widely used\nwhen modeling decisions under uncertainty. Computing the performance of such\nmodels requires integrating these distributions over specific domains, which\ncan vary widely across models. Besides some special cases where these integrals\nare easy to calculate, there exist no general analytical expressions, standard\nnumerical methods or software for these integrals. Here we present mathematical\nresults and open-source software that provide (i) the probability in any domain\nof a normal in any dimensions with any parameters, (ii) the probability\ndensity, cumulative distribution, and inverse cumulative distribution of any\nfunction of a normal vector, (iii) the classification errors among any number\nof normal distributions, the Bayes-optimal discriminability index and relation\nto the operating characteristic, (iv) dimension reduction and visualizations\nfor such problems, and (v) tests for how reliably these methods may be used on\ngiven data. We demonstrate these tools with vision research applications of\ndetecting occluding objects in natural scenes, and detecting camouflage.",
          "link": "http://arxiv.org/abs/2012.14331",
          "publishedOn": "2021-07-29T02:00:08.434Z",
          "wordCount": 681,
          "title": "A method to integrate and classify normal distributions. (arXiv:2012.14331v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_F/0/1/0/all/0/1\">Frank P.-W. Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yingnan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>",
          "description": "Accurate prediction of future person location and movement trajectory from an\negocentric wearable camera can benefit a wide range of applications, such as\nassisting visually impaired people in navigation, and the development of\nmobility assistance for people with disability. In this work, a new egocentric\ndataset was constructed using a wearable camera, with 8,250 short clips of a\ntargeted person either walking 1) toward, 2) away, or 3) across the camera\nwearer in indoor environments, or 4) staying still in the scene, and 13,817\nperson bounding boxes were manually labelled. Apart from the bounding boxes,\nthe dataset also contains the estimated pose of the targeted person as well as\nthe IMU signal of the wearable camera at each time point. An LSTM-based\nencoder-decoder framework was designed to predict the future location and\nmovement trajectory of the targeted person in this egocentric setting.\nExtensive experiments have been conducted on the new dataset, and have shown\nthat the proposed method is able to reliably and better predict future person\nlocation and trajectory in egocentric videos captured by the wearable camera\ncompared to three baselines.",
          "link": "http://arxiv.org/abs/2103.04019",
          "publishedOn": "2021-07-29T02:00:08.406Z",
          "wordCount": 671,
          "title": "Indoor Future Person Localization from an Egocentric Wearable Camera. (arXiv:2103.04019v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1\">Akshat Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "In this paper, we demonstrated a practical application of realistic river\nimage generation using deep learning. Specifically, we explored a generative\nadversarial network (GAN) model capable of generating high-resolution and\nrealistic river images that can be used to support modeling and analysis in\nsurface water estimation, river meandering, wetland loss, and other\nhydrological research studies. First, we have created an extensive repository\nof overhead river images to be used in training. Second, we incorporated the\nProgressive Growing GAN (PGGAN), a network architecture that iteratively trains\nsmaller-resolution GANs to gradually build up to a very high resolution to\ngenerate high quality (i.e., 1024x1024) synthetic river imagery. With simpler\nGAN architectures, difficulties arose in terms of exponential increase of\ntraining time and vanishing/exploding gradient issues, which the PGGAN\nimplementation seemed to significantly reduce. The results presented in this\nstudy show great promise in generating high-quality images and capturing the\ndetails of river structure and flow to support hydrological research, which\noften requires extensive imagery for model performance.",
          "link": "http://arxiv.org/abs/2003.00826",
          "publishedOn": "2021-07-29T02:00:08.397Z",
          "wordCount": 644,
          "title": "Realistic River Image Synthesis using Deep Generative Adversarial Networks. (arXiv:2003.00826v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>",
          "description": "In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose new explainability approaches for point cloud deep\nneural networks based on local surrogate model-based methods to show which\ncomponents make the main contribution to the classification. Moreover, we\npropose a quantitative validation method for explainability methods of point\nclouds which enhances the persuasive power of explainability by dropping the\nmost positive or negative contributing features and monitoring how the\nclassification scores of specific categories change. To enable an intuitive\nexplanation of misclassified instances, we display features with confounding\ncontributions. Our new explainability approach provides a fairly accurate, more\nintuitive and widely applicable explanation for point cloud classification\ntasks. Our code is available at https://github.com/Explain3D/Explainable3D",
          "link": "http://arxiv.org/abs/2107.13459",
          "publishedOn": "2021-07-29T02:00:08.384Z",
          "wordCount": 610,
          "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Libo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>",
          "description": "Road detection is a critically important task for self-driving cars. By\nemploying LiDAR data, recent works have significantly improved the accuracy of\nroad detection. Relying on LiDAR sensors limits the wide application of those\nmethods when only cameras are available. In this paper, we propose a novel road\ndetection approach with RGB being the only input during inference.\nSpecifically, we exploit pseudo-LiDAR using depth estimation, and propose a\nfeature fusion network where RGB and learned depth information are fused for\nimproved road detection. To further optimize the network structure and improve\nthe efficiency of the network. we search for the network structure of the\nfeature fusion module using NAS techniques. Finally, be aware of that\ngenerating pseudo-LiDAR from RGB via depth estimation introduces extra\ncomputational costs and relies on depth estimation networks, we design a\nmodality distillation strategy and leverage it to further free our network from\nthese extra computational cost and dependencies during inference. The proposed\nmethod achieves state-of-the-art performance on two challenging benchmarks,\nKITTI and R2D.",
          "link": "http://arxiv.org/abs/2107.13279",
          "publishedOn": "2021-07-29T02:00:08.377Z",
          "wordCount": 596,
          "title": "Pseudo-LiDAR Based Road Detection. (arXiv:2107.13279v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13200",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_B/0/1/0/all/0/1\">Bo Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1\">Pengfei Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a> (Alzheimer&#x27;s Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing), <a href=\"http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1\">Shuwei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1\">Peng Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Ronald X. Xu</a>",
          "description": "Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal\nperiod mild cognitive impairment (MCI) is essential for the delayed disease\nprogression and the improved quality of patients'life. The emerging\ncomputer-aided diagnostic methods that combine deep learning with structural\nmagnetic resonance imaging (sMRI) have achieved encouraging results, but some\nof them are limit of issues such as data leakage and unexplainable diagnosis.\nIn this research, we propose a novel end-to-end deep learning approach for\nautomated diagnosis of AD and localization of important brain regions related\nto the disease from sMRI data. This approach is based on a 2D single model\nstrategy and has the following differences from the current approaches: 1)\nConvolutional Neural Network (CNN) models of different structures and\ncapacities are evaluated systemically and the most suitable model is adopted\nfor AD diagnosis; 2) a data augmentation strategy named Two-stage Random\nRandAugment (TRRA) is proposed to alleviate the overfitting issue caused by\nlimited training data and to improve the classification performance in AD\ndiagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the\nvisually explainable heatmaps that localize and highlight the brain regions\nthat our model focuses on and to make our model more transparent. Our approach\nhas been evaluated on two publicly accessible datasets for two classification\ntasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable\nMCI (sMCI). The experimental results indicate that our approach outperforms the\nstate-of-the-art approaches, including those using multi-model and 3D CNN\nmethods. The resultant localization heatmaps from our approach also highlight\nthe lateral ventricle and some disease-relevant regions of cortex, coincident\nwith the commonly affected regions during the development of AD.",
          "link": "http://arxiv.org/abs/2107.13200",
          "publishedOn": "2021-07-29T02:00:08.356Z",
          "wordCount": 760,
          "title": "An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization. (arXiv:2107.13200v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1907.01845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijun Xu</a>",
          "description": "One of the most critical problems in weight-sharing neural architecture\nsearch is the evaluation of candidate models within a predefined search space.\nIn practice, a one-shot supernet is trained to serve as an evaluator. A\nfaithful ranking certainly leads to more accurate searching results. However,\ncurrent methods are prone to making misjudgments. In this paper, we prove that\ntheir biased evaluation is due to inherent unfairness in the supernet training.\nIn view of this, we propose two levels of constraints: expectation fairness and\nstrict fairness. Particularly, strict fairness ensures equal optimization\nopportunities for all choice blocks throughout the training, which neither\noverestimates nor underestimates their capacity. We demonstrate that this is\ncrucial for improving the confidence of models' ranking. Incorporating the\none-shot supernet trained under the proposed fairness constraints with a\nmulti-objective evolutionary search algorithm, we obtain various\nstate-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation\naccuracy on ImageNet. The models and their evaluation codes are made publicly\navailable online this http URL .",
          "link": "http://arxiv.org/abs/1907.01845",
          "publishedOn": "2021-07-29T02:00:08.349Z",
          "wordCount": 672,
          "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. (arXiv:1907.01845v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.01446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chongwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yulong Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Caifei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>",
          "description": "To boost the object grabbing capability of underwater robots for open-sea\nfarming, we propose a new dataset (UDD) consisting of three categories\n(seacucumber, seaurchin, and scallop) with 2,227 images. To the best of our\nknowledge, it is the first 4K HD dataset collected in a real open-sea farm. We\nalso propose a novel Poisson-blending Generative Adversarial Network (Poisson\nGAN) and an efficient object detection network (AquaNet) to address two common\nissues within related datasets: the class-imbalance problem and the problem of\nmass small object, respectively. Specifically, Poisson GAN combines Poisson\nblending into its generator and employs a new loss called Dual Restriction loss\n(DR loss), which supervises both implicit space features and image-level\nfeatures during training to generate more realistic images. By utilizing\nPoisson GAN, objects of minority class like seacucumber or scallop could be\nadded into an image naturally and annotated automatically, which could increase\nthe loss of minority classes during training detectors to eliminate the\nclass-imbalance problem; AquaNet is a high-efficiency detector to address the\nproblem of detecting mass small objects from cloudy underwater pictures. Within\nit, we design two efficient components: a depth-wise-convolution-based\nMulti-scale Contextual Features Fusion (MFF) block and a Multi-scale\nBlursampling (MBP) module to reduce the parameters of the network to 1.3\nmillion. Both two components could provide multi-scale features of small\nobjects under a short backbone configuration without any loss of accuracy. In\naddition, we construct a large-scale augmented dataset (AUDD) and a\npre-training dataset via Poisson GAN from UDD. Extensive experiments show the\neffectiveness of the proposed Poisson GAN, AquaNet, UDD, AUDD, and pre-training\ndataset.",
          "link": "http://arxiv.org/abs/2003.01446",
          "publishedOn": "2021-07-29T02:00:08.335Z",
          "wordCount": 750,
          "title": "A New Dataset, Poisson GAN and AquaNet for Underwater Object Grabbing. (arXiv:2003.01446v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Widya_A/0/1/0/all/0/1\">Aji Resindra Widya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monno_Y/0/1/0/all/0/1\">Yusuke Monno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1\">Masatoshi Okutomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1\">Sho Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotoda_T/0/1/0/all/0/1\">Takuji Gotoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miki_K/0/1/0/all/0/1\">Kenji Miki</a>",
          "description": "Gastroendoscopy has been a clinical standard for diagnosing and treating\nconditions that affect a part of a patient's digestive system, such as the\nstomach. Despite the fact that gastroendoscopy has a lot of advantages for\npatients, there exist some challenges for practitioners, such as the lack of 3D\nperception, including the depth and the endoscope pose information. Such\nchallenges make navigating the endoscope and localizing any found lesion in a\ndigestive tract difficult. To tackle these problems, deep learning-based\napproaches have been proposed to provide monocular gastroendoscopy with\nadditional yet important depth and pose information. In this paper, we propose\na novel supervised approach to train depth and pose estimation networks using\nconsecutive endoscopy images to assist the endoscope navigation in the stomach.\nWe firstly generate real depth and pose training data using our previously\nproposed whole stomach 3D reconstruction pipeline to avoid poor generalization\nability between computer-generated (CG) models and real data for the stomach.\nIn addition, we propose a novel generalized photometric loss function to avoid\nthe complicated process of finding proper weights for balancing the depth and\nthe pose loss terms, which is required for existing direct depth and pose\nsupervision approaches. We then experimentally show that our proposed\ngeneralized loss performs better than existing direct supervision losses.",
          "link": "http://arxiv.org/abs/2107.13263",
          "publishedOn": "2021-07-29T02:00:08.327Z",
          "wordCount": 664,
          "title": "Learning-Based Depth and Pose Estimation for Monocular Endoscope with Loss Generalization. (arXiv:2107.13263v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1\">Rindra Ramamonjison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xinyu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiaolong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "This paper presents a Simple and effective unsupervised adaptation method for\nRobust Object Detection (SimROD). To overcome the challenging issues of domain\nshift and pseudo-label noise, our method integrates a novel domain-centric\naugmentation method, a gradual self-labeling adaptation procedure, and a\nteacher-guided fine-tuning mechanism. Using our method, target domain samples\ncan be leveraged to adapt object detection models without changing the model\narchitecture or generating synthetic data. When applied to image corruptions\nand high-level cross-domain adaptation benchmarks, our method outperforms prior\nbaselines on multiple domain adaptation benchmarks. SimROD achieves new\nstate-of-the-art on standard real-to-synthetic and cross-camera setup\nbenchmarks. On the image corruption benchmark, models adapted with our method\nachieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%\nAP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method\noutperformed the best baseline performance by up to 8% AP50 on Comic dataset\nand up to 4% on Watercolor dataset.",
          "link": "http://arxiv.org/abs/2107.13389",
          "publishedOn": "2021-07-29T02:00:08.319Z",
          "wordCount": 612,
          "title": "SimROD: A Simple Adaptation Method for Robust Object Detection. (arXiv:2107.13389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunxia Xiao</a>",
          "description": "Generating photo-realistic images from a text description is a challenging\nproblem in computer vision. Previous works have shown promising performance to\ngenerate synthetic images conditional on text by Generative Adversarial\nNetworks (GANs). In this paper, we focus on the category-consistent and\nrelativistic diverse constraints to optimize the diversity of synthetic images.\nBased on those constraints, a category-consistent and relativistic diverse\nconditional GAN (CRD-CGAN) is proposed to synthesize $K$ photo-realistic images\nsimultaneously. We use the attention loss and diversity loss to improve the\nsensitivity of the GAN to word attention and noises. Then, we employ the\nrelativistic conditional loss to estimate the probability of relatively real or\nfake for synthetic images, which can improve the performance of basic\nconditional loss. Finally, we introduce a category-consistent loss to alleviate\nthe over-category issues between K synthetic images. We evaluate our approach\nusing the Birds-200-2011, Oxford-102 flower and MSCOCO 2014 datasets, and the\nextensive experiments demonstrate superiority of the proposed method in\ncomparison with state-of-the-art methods in terms of photorealistic and\ndiversity of the generated synthetic images.",
          "link": "http://arxiv.org/abs/2107.13516",
          "publishedOn": "2021-07-29T02:00:08.298Z",
          "wordCount": 609,
          "title": "CRD-CGAN: Category-Consistent and Relativistic Constraints for Diverse Text-to-Image Generation. (arXiv:2107.13516v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashlesha Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangwan_K/0/1/0/all/0/1\">Kuldip Singh Sangwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhiraj/0/1/0/all/0/1\">Dhiraj</a>",
          "description": "As the proportion of road accidents increases each year, driver distraction\ncontinues to be an important risk component in road traffic injuries and\ndeaths. The distractions caused by the increasing use of mobile phones and\nother wireless devices pose a potential risk to road safety. Our current study\naims to aid the already existing techniques in driver posture recognition by\nimproving the performance in the driver distraction classification problem. We\npresent an approach using a genetic algorithm-based ensemble of six independent\ndeep neural architectures, namely, AlexNet, VGG-16, EfficientNet B0, Vanilla\nCNN, Modified DenseNet, and InceptionV3 + BiLSTM. We test it on two\ncomprehensive datasets, the AUC Distracted Driver Dataset, on which our\ntechnique achieves an accuracy of 96.37%, surpassing the previously obtained\n95.98%, and on the State Farm Driver Distraction Dataset, on which we attain an\naccuracy of 99.75%. The 6-Model Ensemble gave an inference time of 0.024\nseconds as measured on our machine with Ubuntu 20.04(64-bit) and GPU as GeForce\nGTX 1080.",
          "link": "http://arxiv.org/abs/2107.13355",
          "publishedOn": "2021-07-29T02:00:08.291Z",
          "wordCount": 633,
          "title": "A Computer Vision-Based Approach for Driver Distraction Recognition using Deep Learning and Genetic Algorithm Based Ensemble. (arXiv:2107.13355v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang-Zhong Yang</a>",
          "description": "In this report, the technical details of our submission to the EPIC-Kitchens\nAction Anticipation Challenge 2021 are given. We developed a hierarchical\nattention model for action anticipation, which leverages Transformer-based\nattention mechanism to aggregate features across temporal dimension,\nmodalities, symbiotic branches respectively. In terms of Mean Top-5 Recall of\naction, our submission with team name ICL-SJTU achieved 13.39% for overall\ntesting set, 10.05% for unseen subsets and 11.88% for tailed subsets.\nAdditionally, it is noteworthy that our submission ranked 1st in terms of verb\nclass in all three (sub)sets.",
          "link": "http://arxiv.org/abs/2107.13259",
          "publishedOn": "2021-07-29T02:00:08.283Z",
          "wordCount": 529,
          "title": "TransAction: ICL-SJTU Submission to EPIC-Kitchens Action Anticipation Challenge 2021. (arXiv:2107.13259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>",
          "description": "Modelling long-range contextual relationships is critical for pixel-wise\nprediction tasks such as semantic segmentation. However, convolutional neural\nnetworks (CNNs) are inherently limited to model such dependencies due to the\nnaive structure in its building modules (\\eg, local convolution kernel). While\nrecent global aggregation methods are beneficial for long-range structure\ninformation modelling, they would oversmooth and bring noise to the regions\ncontaining fine details (\\eg,~boundaries and small objects), which are very\nmuch cared for the semantic segmentation task. To alleviate this problem, we\npropose to explore the local context for making the aggregated long-range\nrelationship being distributed more accurately in local regions. In particular,\nwe design a novel local distribution module which models the affinity map\nbetween global and local relationship for each pixel adaptively. Integrating\nexisting global aggregation modules, we show that our approach can be\nmodularized as an end-to-end trainable block and easily plugged into existing\nsemantic segmentation networks, giving rise to the \\emph{GALD} networks.\nDespite its simplicity and versatility, our approach allows us to build new\nstate of the art on major semantic segmentation benchmarks including\nCityscapes, ADE20K, Pascal Context, Camvid and COCO-stuff. Code and trained\nmodels are released at \\url{https://github.com/lxtGH/GALD-DGCNet} to foster\nfurther research.",
          "link": "http://arxiv.org/abs/2107.13154",
          "publishedOn": "2021-07-29T02:00:08.276Z",
          "wordCount": 652,
          "title": "Global Aggregation then Local Distribution for Scene Parsing. (arXiv:2107.13154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeesoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Junsuk Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>",
          "description": "Weakly-supervised object localization (WSOL) enables finding an object using\na dataset without any localization information. By simply training a\nclassification model using only image-level annotations, the feature map of the\nmodel can be utilized as a score map for localization. In spite of many WSOL\nmethods proposing novel strategies, there has not been any de facto standard\nabout how to normalize the class activation map (CAM). Consequently, many WSOL\nmethods have failed to fully exploit their own capacity because of the misuse\nof a normalization method. In this paper, we review many existing normalization\nmethods and point out that they should be used according to the property of the\ngiven dataset. Additionally, we propose a new normalization method which\nsubstantially enhances the performance of any CAM-based WSOL methods. Using the\nproposed normalization method, we provide a comprehensive evaluation over three\ndatasets (CUB, ImageNet and OpenImages) on three different architectures and\nobserve significant performance gains over the conventional min-max\nnormalization method in all the evaluated cases.",
          "link": "http://arxiv.org/abs/2107.13221",
          "publishedOn": "2021-07-29T02:00:08.264Z",
          "wordCount": 608,
          "title": "Normalization Matters in Weakly Supervised Object Localization. (arXiv:2107.13221v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lomurno_E/0/1/0/all/0/1\">Eugenio Lomurno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanoni_A/0/1/0/all/0/1\">Andrea Romanoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1\">Matteo Matteucci</a>",
          "description": "Today, Multi-View Stereo techniques are able to reconstruct robust and\ndetailed 3D models, especially when starting from high-resolution images.\nHowever, there are cases in which the resolution of input images is relatively\nlow, for instance, when dealing with old photos, or when hardware constrains\nthe amount of data that can be acquired. In this paper, we investigate if, how,\nand how much increasing the resolution of such input images through\nSuper-Resolution techniques reflects in quality improvements of the\nreconstructed 3D models, despite the artifacts that sometimes this may\ngenerate. We show that applying a Super-Resolution step before recovering the\ndepth maps in most cases leads to a better 3D model both in the case of\nPatchMatch-based and deep-learning-based algorithms. The use of\nSuper-Resolution improves especially the completeness of reconstructed models\nand turns out to be particularly effective in the case of textured scenes.",
          "link": "http://arxiv.org/abs/2107.13261",
          "publishedOn": "2021-07-29T02:00:08.240Z",
          "wordCount": 574,
          "title": "Improving Multi-View Stereo via Super-Resolution. (arXiv:2107.13261v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Ti Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balagopal_A/0/1/0/all/0/1\">Anjali Balagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohopolski_M/0/1/0/all/0/1\">Michael Dohopolski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_H/0/1/0/all/0/1\">Howard E. Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McBeth_R/0/1/0/all/0/1\">Rafe McBeth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mu-Han Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sher_D/0/1/0/all/0/1\">David J. Sher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Steve Jiang</a>",
          "description": "Automatic segmentation of anatomical structures is critical for many medical\napplications. However, the results are not always clinically acceptable and\nrequire tedious manual revision. Here, we present a novel concept called\nartificial intelligence assisted contour revision (AIACR) and demonstrate its\nfeasibility. The proposed clinical workflow of AIACR is as follows given an\ninitial contour that requires a clinicians revision, the clinician indicates\nwhere a large revision is needed, and a trained deep learning (DL) model takes\nthis input to update the contour. This process repeats until a clinically\nacceptable contour is achieved. The DL model is designed to minimize the\nclinicians input at each iteration and to minimize the number of iterations\nneeded to reach acceptance. In this proof-of-concept study, we demonstrated the\nconcept on 2D axial images of three head-and-neck cancer datasets, with the\nclinicians input at each iteration being one mouse click on the desired\nlocation of the contour segment. The performance of the model is quantified\nwith Dice Similarity Coefficient (DSC) and 95th percentile of Hausdorff\nDistance (HD95). The average DSC/HD95 (mm) of the auto-generated initial\ncontours were 0.82/4.3, 0.73/5.6 and 0.67/11.4 for three datasets, which were\nimproved to 0.91/2.1, 0.86/2.4 and 0.86/4.7 with three mouse clicks,\nrespectively. Each DL-based contour update requires around 20 ms. We proposed a\nnovel AIACR concept that uses DL models to assist clinicians in revising\ncontours in an efficient and effective way, and we demonstrated its feasibility\nby using 2D axial CT images from three head-and-neck cancer datasets.",
          "link": "http://arxiv.org/abs/2107.13465",
          "publishedOn": "2021-07-29T02:00:08.231Z",
          "wordCount": 704,
          "title": "A Proof-of-Concept Study of Artificial Intelligence Assisted Contour Revision. (arXiv:2107.13465v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Garcia_A/0/1/0/all/0/1\">Aaron Lopez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Francesc J. Ferri</a>",
          "description": "The use of multiple and semantically correlated sources can provide\ncomplementary information to each other that may not be evident when working\nwith individual modalities on their own. In this context, multi-modal models\ncan help producing more accurate and robust predictions in machine learning\ntasks where audio-visual data is available. This paper presents a multi-modal\nmodel for automatic scene classification that exploits simultaneously auditory\nand visual information. The proposed approach makes use of two separate\nnetworks which are respectively trained in isolation on audio and visual data,\nso that each network specializes in a given modality. The visual subnetwork is\na pre-trained VGG16 model followed by a bidiretional recurrent layer, while the\nresidual audio subnetwork is based on stacked squeeze-excitation convolutional\nblocks trained from scratch. After training each subnetwork, the fusion of\ninformation from the audio and visual streams is performed at two different\nstages. The early fusion stage combines features resulting from the last\nconvolutional block of the respective subnetworks at different time steps to\nfeed a bidirectional recurrent structure. The late fusion stage combines the\noutput of the early fusion stage with the independent predictions provided by\nthe two subnetworks, resulting in the final prediction. We evaluate the method\nusing the recently published TAU Audio-Visual Urban Scenes 2021, which contains\nsynchronized audio and video recordings from 12 European cities in 10 different\nscene classes. The proposed model has been shown to provide an excellent\ntrade-off between prediction performance (86.5%) and system complexity (15M\nparameters) in the evaluation results of the DCASE 2021 Challenge.",
          "link": "http://arxiv.org/abs/2107.13180",
          "publishedOn": "2021-07-29T02:00:08.204Z",
          "wordCount": 712,
          "title": "Squeeze-Excitation Convolutional Recurrent Neural Networks for Audio-Visual Scene Classification. (arXiv:2107.13180v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barquero_G/0/1/0/all/0/1\">Germ&#xe1;n Barquero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupont_I/0/1/0/all/0/1\">Isabelle Hupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_C/0/1/0/all/0/1\">Carles Fern&#xe1;ndez</a>",
          "description": "Most current multi-object trackers focus on short-term tracking, and are\nbased on deep and complex systems that often cannot operate in real-time,\nmaking them impractical for video-surveillance. In this paper we present a\nlong-term, multi-face tracking architecture conceived for working in crowded\ncontexts where faces are often the only visible part of a person. Our system\nbenefits from advances in the fields of face detection and face recognition to\nachieve long-term tracking, and is particularly unconstrained to the motion and\nocclusions of people. It follows a tracking-by-detection approach, combining a\nfast short-term visual tracker with a novel online tracklet reconnection\nstrategy grounded on rank-based face verification. The proposed rank-based\nconstraint favours higher inter-class distance among tracklets, and reduces the\npropagation of errors due to wrong reconnections. Additionally, a correction\nmodule is included to correct past assignments with no extra computational\ncost. We present a series of experiments introducing novel specialized metrics\nfor the evaluation of long-term tracking capabilities, and publicly release a\nvideo dataset with 10 manually annotated videos and a total length of 8' 54\".\nOur findings validate the robustness of each of the proposed modules, and\ndemonstrate that, in these challenging contexts, our approach yields up to 50%\nlonger tracks than state-of-the-art deep learning trackers.",
          "link": "http://arxiv.org/abs/2107.13273",
          "publishedOn": "2021-07-29T02:00:08.192Z",
          "wordCount": 664,
          "title": "Rank-based verification for long-term face tracking in crowded scenes. (arXiv:2107.13273v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodin_I/0/1/0/all/0/1\">Ivan Rodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavroedis_D/0/1/0/all/0/1\">Dimitrios Mavroedis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>",
          "description": "Egocentric videos can bring a lot of information about how humans perceive\nthe world and interact with the environment, which can be beneficial for the\nanalysis of human behaviour. The research in egocentric video analysis is\ndeveloping rapidly thanks to the increasing availability of wearable devices\nand the opportunities offered by new large-scale egocentric datasets. As\ncomputer vision techniques continue to develop at an increasing pace, the tasks\nrelated to the prediction of future are starting to evolve from the need of\nunderstanding the present. Predicting future human activities, trajectories and\ninteractions with objects is crucial in applications such as human-robot\ninteraction, assistive wearable technologies for both industrial and daily\nliving scenarios, entertainment and virtual or augmented reality. This survey\nsummarises the evolution of studies in the context of future prediction from\negocentric vision making an overview of applications, devices, existing\nproblems, commonly used datasets, models and input modalities. Our analysis\nhighlights that methods for future prediction from egocentric vision can have a\nsignificant impact in a range of applications and that further research efforts\nshould be devoted to the standardisation of tasks and the proposal of datasets\nconsidering real-world scenarios such as the ones with an industrial vocation.",
          "link": "http://arxiv.org/abs/2107.13411",
          "publishedOn": "2021-07-29T02:00:08.160Z",
          "wordCount": 647,
          "title": "Predicting the Future from First Person (Egocentric) Vision: A Survey. (arXiv:2107.13411v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagong_M/0/1/0/all/0/1\">Min-Cheol Sagong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_Y/0/1/0/all/0/1\">Yoon-Jae Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seung-Won Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Sung-Jea Ko</a>",
          "description": "Convolutional neural networks (CNNs) have been not only widespread but also\nachieved noticeable results on numerous applications including image\nclassification, restoration, and generation. Although the weight-sharing\nproperty of convolutions makes them widely adopted in various tasks, its\ncontent-agnostic characteristic can also be considered a major drawback. To\nsolve this problem, in this paper, we propose a novel operation, called pixel\nadaptive kernel attention (PAKA). PAKA provides directivity to the filter\nweights by multiplying spatially varying attention from learnable features. The\nproposed method infers pixel-adaptive attention maps along the channel and\nspatial directions separately to address the decomposed model with fewer\nparameters. Our method is trainable in an end-to-end manner and applicable to\nany CNN-based models. In addition, we propose an improved information\naggregation module with PAKA, called the hierarchical PAKA module (HPM). We\ndemonstrate the superiority of our HPM by presenting state-of-the-art\nperformance on semantic segmentation compared to the conventional information\naggregation modules. We validate the proposed method through additional\nablation studies and visualizing the effect of PAKA providing directivity to\nthe weights of convolutions. We also show the generalizability of the proposed\nmethod by applying it to multi-modal tasks especially color-guided depth map\nsuper-resolution.",
          "link": "http://arxiv.org/abs/2107.13144",
          "publishedOn": "2021-07-29T02:00:08.140Z",
          "wordCount": 647,
          "title": "Content-aware Directed Propagation Network with Pixel Adaptive Kernel Attention. (arXiv:2107.13144v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_G/0/1/0/all/0/1\">Guohua Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xingxing Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>",
          "description": "The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum\nSite Museum is handcrafted by experts, and the increasing amounts of unearthed\npieces of terracotta warriors make the archaeologists too challenging to\nconduct the restoration of terracotta warriors efficiently. We hope to segment\nthe 3D point cloud data of the terracotta warriors automatically and store the\nfragment data in the database to assist the archaeologists in matching the\nactual fragments with the ones in the database, which could result in higher\nrepairing efficiency of terracotta warriors. Moreover, the existing 3D neural\nnetwork research is mainly focusing on supervised classification, clustering,\nunsupervised representation, and reconstruction. There are few pieces of\nresearches concentrating on unsupervised point cloud part segmentation. In this\npaper, we present SRG-Net for 3D point clouds of terracotta warriors to address\nthese problems. Firstly, we adopt a customized seed-region-growing algorithm to\nsegment the point cloud coarsely. Then we present a supervised segmentation and\nunsupervised reconstruction networks to learn the characteristics of 3D point\nclouds. Finally, we combine the SRG algorithm with our improved CNN using a\nrefinement method. This pipeline is called SRG-Net, which aims at conducting\nsegmentation tasks on the terracotta warriors. Our proposed SRG-Net is\nevaluated on the terracotta warriors data and ShapeNet dataset by measuring the\naccuracy and the latency. The experimental results show that our SRG-Net\noutperforms the state-of-the-art methods. Our code is shown in Code File\n1~\\cite{Srgnet_2021}.",
          "link": "http://arxiv.org/abs/2107.13167",
          "publishedOn": "2021-07-29T02:00:08.132Z",
          "wordCount": 681,
          "title": "Unsupervised Segmentation for Terracotta Warrior with Seed-Region-Growing CNN(SRG-Net). (arXiv:2107.13167v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrido_L/0/1/0/all/0/1\">Llu&#xed;s Garrido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Corominas_G/0/1/0/all/0/1\">Guillem Rodriguez-Corominas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_H/0/1/0/all/0/1\">Herwig Wendt</a>",
          "description": "Recently, transfer subspace learning based approaches have shown to be a\nvalid alternative to unsupervised subspace clustering and temporal data\nclustering for human motion segmentation (HMS). These approaches leverage prior\nknowledge from a source domain to improve clustering performance on a target\ndomain, and currently they represent the state of the art in HMS. Bucking this\ntrend, in this paper, we propose a novel unsupervised model that learns a\nrepresentation of the data and digs clustering information from the data\nitself. Our model is reminiscent of temporal subspace clustering, but presents\ntwo critical differences. First, we learn an auxiliary data matrix that can\ndeviate from the initial data, hence confer more degrees of freedom to the\ncoding matrix. Second, we introduce a regularization term for this auxiliary\ndata matrix that preserves the local geometrical structure present in the\nhigh-dimensional space. The proposed model is efficiently optimized by using an\noriginal Alternating Direction Method of Multipliers (ADMM) formulation\nallowing to learn jointly the auxiliary data representation, a nonnegative\ndictionary and a coding matrix. Experimental results on four benchmark datasets\nfor HMS demonstrate that our approach achieves significantly better clustering\nperformance then state-of-the-art methods, including both unsupervised and more\nrecent semi-supervised transfer learning approaches.",
          "link": "http://arxiv.org/abs/2107.13362",
          "publishedOn": "2021-07-29T02:00:08.124Z",
          "wordCount": 645,
          "title": "Graph Constrained Data Representation Learning for Human Motion Segmentation. (arXiv:2107.13362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13237",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mukherjee_U/0/1/0/all/0/1\">Uddipan Mukherjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pancholi_S/0/1/0/all/0/1\">Sidharth Pancholi</a>",
          "description": "Heart disease is the most common reason for human mortality that causes\nalmost one-third of deaths throughout the world. Detecting the disease early\nincreases the chances of survival of the patient and there are several ways a\nsign of heart disease can be detected early. This research proposes to convert\ncleansed and normalized heart sound into visual mel scale spectrograms and then\nusing visual domain transfer learning approaches to automatically extract\nfeatures and categorize between heart sounds. Some of the previous studies\nfound that the spectrogram of various types of heart sounds is visually\ndistinguishable to human eyes, which motivated this study to experiment on\nvisual domain classification approaches for automated heart sound\nclassification. It will use convolution neural network-based architectures i.e.\nResNet, MobileNetV2, etc as the automated feature extractors from spectrograms.\nThese well-accepted models in the image domain showed to learn generalized\nfeature representations of cardiac sounds collected from different environments\nwith varying amplitude and noise levels. Model evaluation criteria used were\ncategorical accuracy, precision, recall, and AUROC as the chosen dataset is\nunbalanced. The proposed approach has been implemented on datasets A and B of\nthe PASCAL heart sound collection and resulted in ~ 90% categorical accuracy\nand AUROC of ~0.97 for both sets.",
          "link": "http://arxiv.org/abs/2107.13237",
          "publishedOn": "2021-07-29T02:00:08.116Z",
          "wordCount": 652,
          "title": "A Visual Domain Transfer Learning Approach for Heartbeat Sound Classification. (arXiv:2107.13237v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1\">Geetika Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1\">Rohit K Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1\">Kamlesh Tiwari</a>",
          "description": "This paper proposes teeth-photo, a new biometric modality for human\nauthentication on mobile and hand held devices. Biometrics samples are acquired\nusing the camera mounted on mobile device with the help of a mobile application\nhaving specific markers to register the teeth area. Region of interest (RoI) is\nthen extracted using the markers and the obtained sample is enhanced using\ncontrast limited adaptive histogram equalization (CLAHE) for better visual\nclarity. We propose a deep learning architecture and novel regularization\nscheme to obtain highly discriminative embedding for small size RoI. Proposed\ncustom loss function was able to achieve perfect classification for the tiny\nRoI of $75\\times 75$ size. The model is end-to-end and few-shot and therefore\nis very efficient in terms of time and energy requirements. The system can be\nused in many ways including device unlocking and secure authentication. To the\nbest of our understanding, this is the first work on teeth-photo based\nauthentication for mobile device. Experiments have been conducted on an\nin-house teeth-photo database collected using our application. The database is\nmade publicly available. Results have shown that the proposed system has\nperfect accuracy.",
          "link": "http://arxiv.org/abs/2107.13217",
          "publishedOn": "2021-07-29T02:00:08.109Z",
          "wordCount": 632,
          "title": "DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile and Hand-held Devices. (arXiv:2107.13217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>",
          "description": "Video Instance Segmentation (VIS) is a new and inherently multi-task problem,\nwhich aims to detect, segment and track each instance in a video sequence.\nExisting approaches are mainly based on single-frame features or single-scale\nfeatures of multiple frames, where temporal information or multi-scale\ninformation is ignored. To incorporate both temporal and scale information, we\npropose a Temporal Pyramid Routing (TPR) strategy to conditionally align and\nconduct pixel-level aggregation from a feature pyramid pair of two adjacent\nframes. Specifically, TPR contains two novel components, including Dynamic\nAligned Cell Routing (DACR) and Cross Pyramid Routing (CPR), where DACR is\ndesigned for aligning and gating pyramid features across temporal dimension,\nwhile CPR transfers temporally aggregated features across scale dimension.\nMoreover, our approach is a plug-and-play module and can be easily applied to\nexisting instance segmentation methods. Extensive experiments on YouTube-VIS\ndataset demonstrate the effectiveness and efficiency of the proposed approach\non several state-of-the-art instance segmentation methods. Codes and trained\nmodels will be publicly available to facilitate future\nresearch.(\\url{https://github.com/lxtGH/TemporalPyramidRouting}).",
          "link": "http://arxiv.org/abs/2107.13155",
          "publishedOn": "2021-07-29T02:00:08.090Z",
          "wordCount": 609,
          "title": "Improving Video Instance Segmentation via Temporal Pyramid Routing. (arXiv:2107.13155v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "We present a new neural representation, called Neural Ray (NeuRay), for the\nnovel view synthesis (NVS) task with multi-view images as input. Existing\nneural scene representations for solving the NVS problem, such as NeRF, cannot\ngeneralize to new scenes and take an excessively long time on training on each\nnew scene from scratch. The other subsequent neural rendering methods based on\nstereo matching, such as PixelNeRF, SRF and IBRNet are designed to generalize\nto unseen scenes but suffer from view inconsistency in complex scenes with\nself-occlusions. To address these issues, our NeuRay method represents every\nscene by encoding the visibility of rays associated with the input views. This\nneural representation can efficiently be initialized from depths estimated by\nexternal MVS methods, which is able to generalize to new scenes and achieves\nsatisfactory rendering images without any training on the scene. Then, the\ninitialized NeuRay can be further optimized on every scene with little training\ntiming to enforce spatial coherence to ensure view consistency in the presence\nof severe self-occlusion. Experiments demonstrate that NeuRay can quickly\ngenerate high-quality novel view images of unseen scenes with little finetuning\nand can handle complex scenes with severe self-occlusions which previous\nmethods struggle with.",
          "link": "http://arxiv.org/abs/2107.13421",
          "publishedOn": "2021-07-29T02:00:08.082Z",
          "wordCount": 647,
          "title": "Neural Rays for Occlusion-aware Image-based Rendering. (arXiv:2107.13421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kyrkou_C/0/1/0/all/0/1\">Christos Kyrkou</a>",
          "description": "The need for automated real-time visual systems in applications such as smart\ncamera surveillance, smart environments, and drones necessitates the\nimprovement of methods for visual active monitoring and control. Traditionally,\nthe active monitoring task has been handled through a pipeline of modules such\nas detection, filtering, and control. However, such methods are difficult to\njointly optimize and tune their various parameters for real-time processing in\nresource constraint systems. In this paper a deep Convolutional Camera\nController Neural Network is proposed to go directly from visual information to\ncamera movement to provide an efficient solution to the active vision problem.\nIt is trained end-to-end without bounding box annotations to control a camera\nand follow multiple targets from raw pixel values. Evaluation through both a\nsimulation framework and real experimental setup, indicate that the proposed\nsolution is robust to varying conditions and able to achieve better monitoring\nperformance than traditional approaches both in terms of number of targets\nmonitored as well as in effective monitoring time. The advantage of the\nproposed approach is that it is computationally less demanding and can run at\nover 10 FPS (~4x speedup) on an embedded smart camera providing a practical and\naffordable solution to real-time active monitoring.",
          "link": "http://arxiv.org/abs/2107.13233",
          "publishedOn": "2021-07-29T02:00:08.071Z",
          "wordCount": 673,
          "title": "C^3Net: End-to-End deep learning for efficient real-time visual active camera control. (arXiv:2107.13233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ze Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>",
          "description": "Remote photoplethysmography (rPPG) monitors heart rate without requiring\nphysical contact, which allows for a wide variety of applications. Deep\nlearning-based rPPG have demonstrated superior performance over the traditional\napproaches in controlled context. However, the lighting situation in indoor\nspace is typically complex, with uneven light distribution and frequent\nvariations in illumination. It lacks a fair comparison of different methods\nunder different illuminations using the same dataset. In this paper, we present\na public dataset, namely the BH-rPPG dataset, which contains data from twelve\nsubjects under three illuminations: low, medium, and high illumination. We also\nprovide the ground truth heart rate measured by an oximeter. We evaluate the\nperformance of three deep learning-based methods to that of four traditional\nmethods using two public datasets: the UBFC-rPPG dataset and the BH-rPPG\ndataset. The experimental results demonstrate that traditional methods are\ngenerally more resistant to fluctuating illuminations. We found that the\nrPPGNet achieves lowest MAE among deep learning-based method under medium\nillumination, whereas the CHROM achieves 1.5 beats per minute (BPM),\noutperforming the rPPGNet by 60%. These findings suggest that while developing\ndeep learning-based heart rate estimation algorithms, illumination variation\nshould be taken into account. This work serves as a benchmark for rPPG\nperformance evaluation and it opens a pathway for future investigation into\ndeep learning-based rPPG under illumination variations.",
          "link": "http://arxiv.org/abs/2107.13193",
          "publishedOn": "2021-07-29T02:00:08.022Z",
          "wordCount": 666,
          "title": "Assessment of Deep Learning-based Heart Rate Estimation using Remote Photoplethysmography under Different Illuminations. (arXiv:2107.13193v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13048",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1\">Richard J. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shaban_M/0/1/0/all/0/1\">Muhammad Shaban</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chengkuan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>",
          "description": "Cancer prognostication is a challenging task in computational pathology that\nrequires context-aware representations of histology features to adequately\ninfer patient survival. Despite the advancements made in weakly-supervised deep\nlearning, many approaches are not context-aware and are unable to model\nimportant morphological feature interactions between cell identities and tissue\ntypes that are prognostic for patient survival. In this work, we present\nPatch-GCN, a context-aware, spatially-resolved patch-based graph convolutional\nnetwork that hierarchically aggregates instance-level histology features to\nmodel local- and global-level topological structures in the tumor\nmicroenvironment. We validate Patch-GCN with 4,370 gigapixel WSIs across five\ndifferent cancer types from the Cancer Genome Atlas (TCGA), and demonstrate\nthat Patch-GCN outperforms all prior weakly-supervised approaches by\n3.58-9.46%. Our code and corresponding models are publicly available at\nhttps://github.com/mahmoodlab/Patch-GCN.",
          "link": "http://arxiv.org/abs/2107.13048",
          "publishedOn": "2021-07-29T02:00:07.969Z",
          "wordCount": 606,
          "title": "Whole Slide Images are 2D Point Clouds: Context-Aware Survival Prediction using Patch-based Graph Convolutional Networks. (arXiv:2107.13048v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chenhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "Current geometry-based monocular 3D object detection models can efficiently\ndetect objects by leveraging perspective geometry, but their performance is\nlimited due to the absence of accurate depth information. Though this issue can\nbe alleviated in a depth-based model where a depth estimation module is plugged\nto predict depth information before 3D box reasoning, the introduction of such\nmodule dramatically reduces the detection speed. Instead of training a costly\ndepth estimator, we propose a rendering module to augment the training data by\nsynthesizing images with virtual-depths. The rendering module takes as input\nthe RGB image and its corresponding sparse depth image, outputs a variety of\nphoto-realistic synthetic images, from which the detection model can learn more\ndiscriminative features to adapt to the depth changes of the objects. Besides,\nwe introduce an auxiliary module to improve the detection model by jointly\noptimizing it through a depth estimation task. Both modules are working in the\ntraining time and no extra computation will be introduced to the detection\nmodel. Experiments show that by working with our proposed modules, a\ngeometry-based model can represent the leading accuracy on the KITTI 3D\ndetection benchmark.",
          "link": "http://arxiv.org/abs/2107.13269",
          "publishedOn": "2021-07-29T02:00:07.928Z",
          "wordCount": 636,
          "title": "Aug3D-RPN: Improving Monocular 3D Object Detection by Synthetic Images with Virtual Depth. (arXiv:2107.13269v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13157",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Trivedi_A/0/1/0/all/0/1\">Anusua Trivedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desbiens_J/0/1/0/all/0/1\">Jocelyn Desbiens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gross_R/0/1/0/all/0/1\">Ron Gross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Purpose: To demonstrate that retinal microvasculature per se is a reliable\nbiomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular\ndiseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to\ncolor fundus images for semantic segmentation of the blood vessels and severity\nclassification on both vascular and full images. Vessel reconstruction through\nharmonic descriptors is also used as a smoothing and de-noising tool. The\nmathematical background of the theory is also outlined. Results: For diabetic\npatients, at least 93.8% of DR No-Refer vs. Refer classification can be related\nto vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening\ncase, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the\ndisease biomarkers are related topologically to the vasculature. Translational\nRelevance: Experiments conducted on eye blood vasculature reconstruction as a\nbiomarker shows a strong correlation between vasculature shape and later stages\nof DR.",
          "link": "http://arxiv.org/abs/2107.13157",
          "publishedOn": "2021-07-29T02:00:07.920Z",
          "wordCount": 603,
          "title": "Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases. (arXiv:2107.13157v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1\">Keld T. Lundgaard</a>",
          "description": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.",
          "link": "http://arxiv.org/abs/2107.13054",
          "publishedOn": "2021-07-29T02:00:07.913Z",
          "wordCount": 623,
          "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>",
          "description": "In this paper, we present a set of extremely efficient and high throughput\nmodels for accurate face verification, MixFaceNets which are inspired by Mixed\nDepthwise Convolutional Kernels. Extensive experiment evaluations on Label Face\nin the Wild (LFW), Age-DB, MegaFace, and IARPA Janus Benchmarks IJB-B and IJB-C\ndatasets have shown the effectiveness of our MixFaceNets for applications\nrequiring extremely low computational complexity. Under the same level of\ncomputation complexity (< 500M FLOPs), our MixFaceNets outperform\nMobileFaceNets on all the evaluated datasets, achieving 99.60% accuracy on LFW,\n97.05% accuracy on AgeDB-30, 93.60 TAR (at FAR1e-6) on MegaFace, 90.94 TAR (at\nFAR1e-4) on IJB-B and 93.08 TAR (at FAR1e-4) on IJB-C. With computational\ncomplexity between 500M and 1G FLOPs, our MixFaceNets achieved results\ncomparable to the top-ranked models, while using significantly fewer FLOPs and\nless computation overhead, which proves the practical value of our proposed\nMixFaceNets. All training codes, pre-trained models, and training logs have\nbeen made available https://github.com/fdbtrs/mixfacenets.",
          "link": "http://arxiv.org/abs/2107.13046",
          "publishedOn": "2021-07-29T02:00:07.905Z",
          "wordCount": 602,
          "title": "MixFaceNets: Extremely Efficient Face Recognition Networks. (arXiv:2107.13046v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaojie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Video prediction methods generally consume substantial computing resources in\ntraining and deployment, among which keypoint-based approaches show promising\nimprovement in efficiency by simplifying dense image prediction to light\nkeypoint prediction. However, keypoint locations are often modeled only as\ncontinuous coordinates, so noise from semantically insignificant deviations in\nvideos easily disrupt learning stability, leading to inaccurate keypoint\nmodeling. In this paper, we design a new grid keypoint learning framework,\naiming at a robust and explainable intermediate keypoint representation for\nlong-term efficient video prediction. We have two major technical\ncontributions. First, we detect keypoints by jumping among candidate locations\nin our raised grid space and formulate a condensation loss to encourage\nmeaningful keypoints with strong representative capability. Second, we\nintroduce a 2D binary map to represent the detected grid keypoints and then\nsuggest propagating keypoint locations with stochasticity by selecting entries\nin the discrete grid space, thus preserving the spatial structure of keypoints\nin the longterm horizon for better future frame generation. Extensive\nexperiments verify that our method outperforms the state-ofthe-art stochastic\nvideo prediction methods while saves more than 98% of computing resources. We\nalso demonstrate our method on a robotic-assisted surgery dataset with\npromising results. Our code is available at\nhttps://github.com/xjgaocs/Grid-Keypoint-Learning.",
          "link": "http://arxiv.org/abs/2107.13170",
          "publishedOn": "2021-07-29T02:00:07.898Z",
          "wordCount": 642,
          "title": "Accurate Grid Keypoint Learning for Efficient Video Prediction. (arXiv:2107.13170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadaoui_K/0/1/0/all/0/1\">Karima Kadaoui</a>",
          "description": "Image Captioning is a task that combines computer vision and natural language\nprocessing, where it aims to generate descriptive legends for images. It is a\ntwo-fold process relying on accurate image understanding and correct language\nunderstanding both syntactically and semantically. It is becoming increasingly\ndifficult to keep up with the latest research and findings in the field of\nimage captioning due to the growing amount of knowledge available on the topic.\nThere is not, however, enough coverage of those findings in the available\nreview papers. We perform in this paper a run-through of the current\ntechniques, datasets, benchmarks and evaluation metrics used in image\ncaptioning. The current research on the field is mostly focused on deep\nlearning-based methods, where attention mechanisms along with deep\nreinforcement and adversarial learning appear to be in the forefront of this\nresearch topic. In this paper, we review recent methodologies such as UpDown,\nOSCAR, VIVO, Meta Learning and a model that uses conditional generative\nadversarial nets. Although the GAN-based model achieves the highest score,\nUpDown represents an important basis for image captioning and OSCAR and VIVO\nare more useful as they use novel object captioning. This review paper serves\nas a roadmap for researchers to keep up to date with the latest contributions\nmade in the field of image caption generation.",
          "link": "http://arxiv.org/abs/2107.13114",
          "publishedOn": "2021-07-29T02:00:07.877Z",
          "wordCount": 653,
          "title": "A Thorough Review on Recent Deep Learning Methodologies for Image Captioning. (arXiv:2107.13114v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chaoqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiyu Sun</a>",
          "description": "Previous unsupervised monocular depth estimation methods mainly focus on the\nday-time scenario, and their frameworks are driven by warped photometric\nconsistency. While in some challenging environments, like night, rainy night or\nsnowy winter, the photometry of the same pixel on different frames is\ninconsistent because of the complex lighting and reflection, so that the\nday-time unsupervised frameworks cannot be directly applied to these complex\nscenarios. In this paper, we investigate the problem of unsupervised monocular\ndepth estimation in certain highly complex scenarios. We address this\nchallenging problem by using domain adaptation, and a unified image\ntransfer-based adaptation framework is proposed based on monocular videos in\nthis paper. The depth model trained on day-time scenarios is adapted to\ndifferent complex scenarios. Instead of adapting the whole depth network, we\njust consider the encoder network for lower computational complexity. The depth\nmodels adapted by the proposed framework to different scenarios share the same\ndecoder, which is practical. Constraints on both feature space and output space\npromote the framework to learn the key features for depth decoding, and the\nsmoothness loss is introduced into the adaptation framework for better depth\nestimation performance. Extensive experiments show the effectiveness of the\nproposed unsupervised framework in estimating the dense depth map from the\nnight-time, rainy night-time and snowy winter images.",
          "link": "http://arxiv.org/abs/2107.13137",
          "publishedOn": "2021-07-29T02:00:07.868Z",
          "wordCount": 653,
          "title": "Unsupervised Monocular Depth Estimation in Highly Complex Environments. (arXiv:2107.13137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadaoui_K/0/1/0/all/0/1\">Karima Kadaoui</a>",
          "description": "Image captioning is a task in the field of Artificial Intelligence that\nmerges between computer vision and natural language processing. It is\nresponsible for generating legends that describe images, and has various\napplications like descriptions used by assistive technology or indexing images\n(for search engines for instance). This makes it a crucial topic in AI that is\nundergoing a lot of research. This task however, like many others, is trained\non large images labeled via human annotation, which can be very cumbersome: it\nneeds manual effort, both financial and temporal costs, it is error-prone and\npotentially difficult to execute in some cases (e.g. medical images). To\nmitigate the need for labels, we attempt to use self-supervised learning, a\ntype of learning where models use the data contained within the images\nthemselves as labels. It is challenging to accomplish though, since the task is\ntwo-fold: the images and captions come from two different modalities and\nusually handled by different types of networks. It is thus not obvious what a\ncompletely self-supervised solution would look like. How it would achieve\ncaptioning in a comparable way to how self-supervision is applied today on\nimage recognition tasks is still an ongoing research topic. In this project, we\nare using an encoder-decoder architecture where the encoder is a convolutional\nneural network (CNN) trained on OpenImages dataset and learns image features in\na self-supervised fashion using the rotation pretext task. The decoder is a\nLong Short-Term Memory (LSTM), and it is trained, along within the image\ncaptioning model, on MS COCO dataset and is responsible of generating captions.\nOur GitHub repository can be found:\nhttps://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction",
          "link": "http://arxiv.org/abs/2107.13111",
          "publishedOn": "2021-07-29T02:00:07.855Z",
          "wordCount": 704,
          "title": "Experimenting with Self-Supervision using Rotation Prediction for Image Captioning. (arXiv:2107.13111v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1\">Daniel D&#x27;souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1\">Zach Nussbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>",
          "description": "As machine learning models are increasingly employed to assist human\ndecision-makers, it becomes critical to communicate the uncertainty associated\nwith these model predictions. However, the majority of work on uncertainty has\nfocused on traditional probabilistic or ranking approaches - where the model\nassigns low probabilities or scores to uncertain examples. While this captures\nwhat examples are challenging for the model, it does not capture the underlying\nsource of the uncertainty. In this work, we seek to identify examples the model\nis uncertain about and characterize the source of said uncertainty. We explore\nthe benefits of designing a targeted intervention - targeted data augmentation\nof the examples where the model is uncertain over the course of training. We\ninvestigate whether the rate of learning in the presence of additional\ninformation differs between atypical and noisy examples? Our results show that\nthis is indeed the case, suggesting that well-designed interventions over the\ncourse of training can be an effective way to characterize and distinguish\nbetween different sources of uncertainty.",
          "link": "http://arxiv.org/abs/2107.13098",
          "publishedOn": "2021-07-29T02:00:07.847Z",
          "wordCount": 618,
          "title": "A Tale Of Two Long Tails. (arXiv:2107.13098v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jinlei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qiaoyong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Di Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>",
          "description": "Reconstruction-based methods play an important role in unsupervised anomaly\ndetection in images. Ideally, we expect a perfect reconstruction for normal\nsamples and poor reconstruction for abnormal samples. Since the\ngeneralizability of deep neural networks is difficult to control, existing\nmodels such as autoencoder do not work well. In this work, we interpret the\nreconstruction of an image as a divide-and-assemble procedure. Surprisingly, by\nvarying the granularity of division on feature maps, we are able to modulate\nthe reconstruction capability of the model for both normal and abnormal\nsamples. That is, finer granularity leads to better reconstruction, while\ncoarser granularity leads to poorer reconstruction. With proper granularity,\nthe gap between the reconstruction error of normal and abnormal samples can be\nmaximized. The divide-and-assemble framework is implemented by embedding a\nnovel multi-scale block-wise memory module into an autoencoder network.\nBesides, we introduce adversarial learning and explore the semantic latent\nrepresentation of the discriminator, which improves the detection of subtle\nanomaly. We achieve state-of-the-art performance on the challenging MVTec AD\ndataset. Remarkably, we improve the vanilla autoencoder model by 10.1% in terms\nof the AUROC score.",
          "link": "http://arxiv.org/abs/2107.13118",
          "publishedOn": "2021-07-29T02:00:07.831Z",
          "wordCount": 628,
          "title": "Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection. (arXiv:2107.13118v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walsh_R/0/1/0/all/0/1\">Reece Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelpakey_M/0/1/0/all/0/1\">Mohamed H. Abdelpakey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed S. Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mostafa M.Mohamed</a>",
          "description": "Classifying and analyzing human cells is a lengthy procedure, often involving\na trained professional. In an attempt to expedite this process, an active area\nof research involves automating cell classification through use of deep\nlearning-based techniques. In practice, a large amount of data is required to\naccurately train these deep learning models. However, due to the sparse human\ncell datasets currently available, the performance of these models is typically\nlow. This study investigates the feasibility of using few-shot learning-based\ntechniques to mitigate the data requirements for accurate training. The study\nis comprised of three parts: First, current state-of-the-art few-shot learning\ntechniques are evaluated on human cell classification. The selected techniques\nare trained on a non-medical dataset and then tested on two out-of-domain,\nhuman cell datasets. The results indicate that, overall, the test accuracy of\nstate-of-the-art techniques decreased by at least 30% when transitioning from a\nnon-medical dataset to a medical dataset. Second, this study evaluates the\npotential benefits, if any, to varying the backbone architecture and training\nschemes in current state-of-the-art few-shot learning techniques when used in\nhuman cell classification. Even with these variations, the overall test\naccuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the\nmedical datasets. Third, this study presents future directions for using\nfew-shot learning in human cell classification. In general, few-shot learning\nin its current state performs poorly on human cell classification. The study\nproves that attempts to modify existing network architectures are not effective\nand concludes that future research effort should be focused on improving\nrobustness towards out-of-domain testing using optimization-based or\nself-supervised few-shot learning techniques.",
          "link": "http://arxiv.org/abs/2107.13093",
          "publishedOn": "2021-07-29T02:00:07.805Z",
          "wordCount": 720,
          "title": "Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning. (arXiv:2107.13093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13136",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1\">Joseph Marino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>",
          "description": "While recent machine learning research has revealed connections between deep\ngenerative models such as VAEs and rate-distortion losses used in learned\ncompression, most of this work has focused on images. In a similar spirit, we\nview recently proposed neural video coding algorithms through the lens of deep\nautoregressive and latent variable modeling. We present recent neural video\ncodecs as instances of a generalized stochastic temporal autoregressive\ntransform, and propose new avenues for further improvements inspired by\nnormalizing flows and structured priors. We propose several architectures that\nyield state-of-the-art video compression performance on full-resolution video\nand discuss their tradeoffs and ablations. In particular, we propose (i)\nimproved temporal autoregressive transforms, (ii) improved entropy models with\nstructured and temporal dependencies, and (iii) variable bitrate versions of\nour algorithms. Since our improvements are compatible with a large class of\nexisting models, we provide further evidence that the generative modeling\nviewpoint can advance the neural video coding field.",
          "link": "http://arxiv.org/abs/2107.13136",
          "publishedOn": "2021-07-29T02:00:07.797Z",
          "wordCount": 637,
          "title": "Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>",
          "description": "This thesis presents methods and approaches to image color correction, color\nenhancement, and color editing. To begin, we study the color correction problem\nfrom the standpoint of the camera's image signal processor (ISP). A camera's\nISP is hardware that applies a series of in-camera image processing and color\nmanipulation steps, many of which are nonlinear in nature, to render the\ninitial sensor image to its final photo-finished representation saved in the\n8-bit standard RGB (sRGB) color space. As white balance (WB) is one of the\nmajor procedures applied by the ISP for color correction, this thesis presents\ntwo different methods for ISP white balancing. Afterward, we discuss another\nscenario of correcting and editing image colors, where we present a set of\nmethods to correct and edit WB settings for images that have been improperly\nwhite-balanced by the ISP. Then, we explore another factor that has a\nsignificant impact on the quality of camera-rendered colors, in which we\noutline two different methods to correct exposure errors in camera-rendered\nimages. Lastly, we discuss post-capture auto color editing and manipulation. In\nparticular, we propose auto image recoloring methods to generate different\nrealistic versions of the same camera-rendered image with new colors. Through\nextensive evaluations, we demonstrate that our methods provide superior\nsolutions compared to existing alternatives targeting color correction, color\nenhancement, and color editing.",
          "link": "http://arxiv.org/abs/2107.13117",
          "publishedOn": "2021-07-29T02:00:07.789Z",
          "wordCount": 649,
          "title": "Image color correction, enhancement, and editing. (arXiv:2107.13117v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>",
          "description": "This paper presents a neural network built upon Transformers, namely PlaneTR,\nto simultaneously detect and reconstruct planes from a single image. Different\nfrom previous methods, PlaneTR jointly leverages the context information and\nthe geometric structures in a sequence-to-sequence way to holistically detect\nplane instances in one forward pass. Specifically, we represent the geometric\nstructures as line segments and conduct the network with three main components:\n(i) context and line segments encoders, (ii) a structure-guided plane decoder,\n(iii) a pixel-wise plane embedding decoder. Given an image and its detected\nline segments, PlaneTR generates the context and line segment sequences via two\nspecially designed encoders and then feeds them into a Transformers-based\ndecoder to directly predict a sequence of plane instances by simultaneously\nconsidering the context and global structure cues. Finally, the pixel-wise\nembeddings are computed to assign each pixel to one predicted plane instance\nwhich is nearest to it in embedding space. Comprehensive experiments\ndemonstrate that PlaneTR achieves a state-of-the-art performance on the ScanNet\nand NYUv2 datasets.",
          "link": "http://arxiv.org/abs/2107.13108",
          "publishedOn": "2021-07-29T02:00:07.753Z",
          "wordCount": 608,
          "title": "PlaneTR: Structure-Guided Transformers for 3D Plane Recovery. (arXiv:2107.13108v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuefan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>",
          "description": "We describe a method for realistic depth synthesis that learns diverse\nvariations from the real depth scans and ensures geometric consistency for\neffective synthetic-to-real transfer. Unlike general image synthesis pipelines,\nwhere geometries are mostly ignored, we treat geometries carried by the depth\nbased on their own existence. We propose differential contrastive learning that\nexplicitly enforces the underlying geometric properties to be invariant\nregarding the real variations been learned. The resulting depth synthesis\nmethod is task-agnostic and can be used for training any task-specific networks\nwith synthetic labels. We demonstrate the effectiveness of the proposed method\nby extensive evaluations on downstream real-world geometric reasoning tasks. We\nshow our method achieves better synthetic-to-real transfer performance than the\nother state-of-the-art. When fine-tuned on a small number of real-world\nannotations, our method can even surpass the fully supervised baselines.",
          "link": "http://arxiv.org/abs/2107.13087",
          "publishedOn": "2021-07-29T02:00:07.746Z",
          "wordCount": 574,
          "title": "DCL: Differential Contrastive Learning for Geometry-Aware Depth Synthesis. (arXiv:2107.13087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>",
          "description": "This paper revisits human-object interaction (HOI) recognition at image level\nwithout using supervisions of object location and human pose. We name it\ndetection-free HOI recognition, in contrast to the existing\ndetection-supervised approaches which rely on object and keypoint detections to\nachieve state of the art. With our method, not only the detection supervision\nis evitable, but superior performance can be achieved by properly using\nimage-text pre-training (such as CLIP) and the proposed Log-Sum-Exp Sign\n(LSE-Sign) loss function. Specifically, using text embeddings of class labels\nto initialize the linear classifier is essential for leveraging the CLIP\npre-trained image encoder. In addition, LSE-Sign loss facilitates learning from\nmultiple labels on an imbalanced dataset by normalizing gradients over all\nclasses in a softmax format. Surprisingly, our detection-free solution achieves\n60.5 mAP on the HICO dataset, outperforming the detection-supervised state of\nthe art by 13.4 mAP",
          "link": "http://arxiv.org/abs/2107.13083",
          "publishedOn": "2021-07-29T02:00:07.738Z",
          "wordCount": 588,
          "title": "Is Object Detection Necessary for Human-Object Interaction Recognition?. (arXiv:2107.13083v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shreyank N Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1\">Laura Sevilla-Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kiyoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>",
          "description": "Zero-shot action recognition is the task of classifying action categories\nthat are not available in the training set. In this setting, the standard\nevaluation protocol is to use existing action recognition datasets (e.g.\nUCF101) and randomly split the classes into seen and unseen. However, most\nrecent work builds on representations pre-trained on the Kinetics dataset,\nwhere classes largely overlap with classes in the zero-shot evaluation\ndatasets. As a result, classes which are supposed to be unseen, are present\nduring supervised pre-training, invalidating the condition of the zero-shot\nsetting. A similar concern was previously noted several years ago for image\nbased zero-shot recognition, but has not been considered by the zero-shot\naction recognition community. In this paper, we propose a new split for true\nzero-shot action recognition with no overlap between unseen test classes and\ntraining or pre-training classes. We benchmark several recent approaches on the\nproposed True Zero-Shot (TruZe) Split for UCF101 and HMDB51, with zero-shot and\ngeneralized zero-shot evaluation. In our extensive analysis we find that our\nTruZe splits are significantly harder than comparable random splits as nothing\nis leaking from pre-training, i.e. unseen performance is consistently lower, up\nto 9.4% for zero-shot action recognition. In an additional evaluation we also\nfind that similar issues exist in the splits used in few-shot action\nrecognition, here we see differences of up to 14.1%. We publish our splits and\nhope that our benchmark analysis will change how the field is evaluating zero-\nand few-shot action recognition moving forward.",
          "link": "http://arxiv.org/abs/2107.13029",
          "publishedOn": "2021-07-29T02:00:07.688Z",
          "wordCount": 688,
          "title": "A New Split for Evaluating True Zero-Shot Action Recognition. (arXiv:2107.13029v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boshen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>",
          "description": "Recently, the problem of inaccurate learning targets in crowd counting draws\nincreasing attention. Inspired by a few pioneering work, we solve this problem\nby trying to predict the indices of pre-defined interval bins of counts instead\nof the count values themselves. However, an inappropriate interval setting\nmight make the count error contributions from different intervals extremely\nimbalanced, leading to inferior counting performance. Therefore, we propose a\nnovel count interval partition criterion called Uniform Error Partition (UEP),\nwhich always keeps the expected counting error contributions equal for all\nintervals to minimize the prediction risk. Then to mitigate the inevitably\nintroduced discretization errors in the count quantization process, we propose\nanother criterion called Mean Count Proxies (MCP). The MCP criterion selects\nthe best count proxy for each interval to represent its count value during\ninference, making the overall expected discretization error of an image nearly\nnegligible. As far as we are aware, this work is the first to delve into such a\nclassification task and ends up with a promising solution for count interval\npartition. Following the above two theoretically demonstrated criterions, we\npropose a simple yet effective model termed Uniform Error Partition Network\n(UEPNet), which achieves state-of-the-art performance on several challenging\ndatasets. The codes will be available at:\nhttps://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.",
          "link": "http://arxiv.org/abs/2107.12619",
          "publishedOn": "2021-07-28T02:02:34.598Z",
          "wordCount": 674,
          "title": "Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Menghan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Simon X. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>",
          "description": "For people who ardently love painting but unfortunately have visual\nimpairments, holding a paintbrush to create a work is a very difficult task.\nPeople in this special group are eager to pick up the paintbrush, like Leonardo\nda Vinci, to create and make full use of their own talents. Therefore, to\nmaximally bridge this gap, we propose a painting navigation system to assist\nblind people in painting and artistic creation. The proposed system is composed\nof cognitive system and guidance system. The system adopts drawing board\npositioning based on QR code, brush navigation based on target detection and\nbush real-time positioning. Meanwhile, this paper uses human-computer\ninteraction on the basis of voice and a simple but efficient position\ninformation coding rule. In addition, we design a criterion to efficiently\njudge whether the brush reaches the target or not. According to the\nexperimental results, the thermal curves extracted from the faces of testers\nshow that it is relatively well accepted by blindfolded and even blind testers.\nWith the prompt frequency of 1s, the painting navigation system performs best\nwith the completion degree of 89% with SD of 8.37% and overflow degree of 347%\nwith SD of 162.14%. Meanwhile, the excellent and good types of brush tip\ntrajectory account for 74%, and the relative movement distance is 4.21 with SD\nof 2.51. This work demonstrates that it is practicable for the blind people to\nfeel the world through the brush in their hands. In the future, we plan to\ndeploy Angle's Eyes on the phone to make it more portable. The demo video of\nthe proposed painting navigation system is available at:\nhttps://doi.org/10.6084/m9.figshare.9760004.v1.",
          "link": "http://arxiv.org/abs/2107.12921",
          "publishedOn": "2021-07-28T02:02:34.546Z",
          "wordCount": 739,
          "title": "Angel's Girl for Blind Painters: an Efficient Painting Navigation System Validated by Multimodal Evaluation Approach. (arXiv:2107.12921v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockhardt_F/0/1/0/all/0/1\">Fabian Stockhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osorio_Roig_D/0/1/0/all/0/1\">Dail&#xe9; Osorio-Roig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Computationally efficient, accurate, and privacy-preserving data storage and\nretrieval are among the key challenges faced by practical deployments of\nbiometric identification systems worldwide. In this work, a method of protected\nindexing of biometric data is presented. By utilising feature-level fusion of\nintelligently paired templates, a multi-stage search structure is created.\nDuring retrieval, the list of potential candidate identities is successively\npre-filtered, thereby reducing the number of template comparisons necessary for\na biometric identification transaction. Protection of the biometric probe\ntemplates, as well as the stored reference templates and the created index is\ncarried out using homomorphic encryption. The proposed method is extensively\nevaluated in closed-set and open-set identification scenarios on publicly\navailable databases using two state-of-the-art open-source face recognition\nsystems. With respect to a typical baseline algorithm utilising an exhaustive\nsearch-based retrieval algorithm, the proposed method enables a reduction of\nthe computational workload associated with a biometric identification\ntransaction by 90%, while simultaneously suffering no degradation of the\nbiometric performance. Furthermore, by facilitating a seamless integration of\ntemplate protection with open-source homomorphic encryption libraries, the\nproposed method guarantees unlinkability, irreversibility, and renewability of\nthe protected biometric data.",
          "link": "http://arxiv.org/abs/2107.12675",
          "publishedOn": "2021-07-28T02:02:34.530Z",
          "wordCount": 643,
          "title": "Feature Fusion Methods for Indexing and Retrieval of Biometric Data: Application to Face Recognition with Privacy Protection. (arXiv:2107.12675v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12040",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenxi Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrui Zeng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chang_Y/0/1/0/all/0/1\">Yi-Wei Chang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that\nvisualizes the structural and spatial organization of macromolecules at a\nnear-native state in single cells, which has broad applications in life\nscience. However, the systematic structural recognition and recovery of\nmacromolecules captured by cryo-ET are difficult due to high structural\ncomplexity and imaging limits. Deep learning based subtomogram classification\nhave played critical roles for such tasks. As supervised approaches, however,\ntheir performance relies on sufficient and laborious annotation on a large\ntraining dataset.\n\nResults: To alleviate this major labeling burden, we proposed a Hybrid Active\nLearning (HAL) framework for querying subtomograms for labelling from a large\nunlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select\nthe subtomograms that have the most uncertain predictions. Moreover, to\nmitigate the sampling bias caused by such strategy, a discriminator is\nintroduced to judge if a certain subtomogram is labeled or unlabeled and\nsubsequently the model queries the subtomogram that have higher probabilities\nto be unlabeled. Additionally, HAL introduces a subset sampling strategy to\nimprove the diversity of the query set, so that the information overlap is\ndecreased between the queried batches and the algorithmic efficiency is\nimproved. Our experiments on subtomogram classification tasks using both\nsimulated and real data demonstrate that we can achieve comparable testing\nperformance (on average only 3% accuracy drop) by using less than 30% of the\nlabeled subtomograms, which shows a very promising result for subtomogram\nclassification task with limited labeling resources.",
          "link": "http://arxiv.org/abs/2102.12040",
          "publishedOn": "2021-07-28T02:02:34.402Z",
          "wordCount": 783,
          "title": "Active Learning to Classify Macromolecular Structures in situ for Less Supervision in Cryo-Electron Tomography. (arXiv:2102.12040v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Rundong Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loianno_G/0/1/0/all/0/1\">Giuseppe Loianno</a>",
          "description": "Estimating the 6D pose of objects is beneficial for robotics tasks such as\ntransportation, autonomous navigation, manipulation as well as in scenarios\nbeyond robotics like virtual and augmented reality. With respect to single\nimage pose estimation, pose tracking takes into account the temporal\ninformation across multiple frames to overcome possible detection\ninconsistencies and to improve the pose estimation efficiency. In this work, we\nintroduce a novel Deep Neural Network (DNN) called VIPose, that combines\ninertial and camera data to address the object pose tracking problem in\nreal-time. The key contribution is the design of a novel DNN architecture which\nfuses visual and inertial features to predict the objects' relative 6D pose\nbetween consecutive image frames. The overall 6D pose is then estimated by\nconsecutively combining relative poses. Our approach shows remarkable pose\nestimation results for heavily occluded objects that are well known to be very\nchallenging to handle by existing state-of-the-art solutions. The effectiveness\nof the proposed approach is validated on a new dataset called VIYCB with RGB\nimage, IMU data, and accurate 6D pose annotations created by employing an\nautomated labeling technique. The approach presents accuracy performances\ncomparable to state-of-the-art techniques, but with additional benefit to be\nreal-time.",
          "link": "http://arxiv.org/abs/2107.12617",
          "publishedOn": "2021-07-28T02:02:34.381Z",
          "wordCount": 641,
          "title": "VIPose: Real-time Visual-Inertial 6D Object Pose Tracking. (arXiv:2107.12617v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.",
          "link": "http://arxiv.org/abs/2107.12651",
          "publishedOn": "2021-07-28T02:02:34.325Z",
          "wordCount": 604,
          "title": "Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12689",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Byrne_N/0/1/0/all/0/1\">Nick Byrne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clough_J/0/1/0/all/0/1\">James R Clough</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valverde_I/0/1/0/all/0/1\">Isra Valverde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Montana_G/0/1/0/all/0/1\">Giovanni Montana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_A/0/1/0/all/0/1\">Andrew P King</a>",
          "description": "Multi-class segmentation of cardiac magnetic resonance (CMR) images seeks a\nseparation of data into anatomical components with known structure and\nconfiguration. The most popular CNN-based methods are optimised using pixel\nwise loss functions, ignorant of the spatially extended features that\ncharacterise anatomy. Therefore, whilst sharing a high spatial overlap with the\nground truth, inferred CNN-based segmentations can lack coherence, including\nspurious connected components, holes and voids. Such results are implausible,\nviolating anticipated anatomical topology. In response, (single-class)\npersistent homology-based loss functions have been proposed to capture global\nanatomical features. Our work extends these approaches to the task of\nmulti-class segmentation. Building an enriched topological description of all\nclass labels and class label pairs, our loss functions make predictable and\nstatistically significant improvements in segmentation topology using a\nCNN-based post-processing framework. We also present (and make available) a\nhighly efficient implementation based on cubical complexes and parallel\nexecution, enabling practical application within high resolution 3D data for\nthe first time. We demonstrate our approach on 2D short axis and 3D whole heart\nCMR segmentation, advancing a detailed and faithful analysis of performance on\ntwo publicly available datasets.",
          "link": "http://arxiv.org/abs/2107.12689",
          "publishedOn": "2021-07-28T02:02:34.306Z",
          "wordCount": 641,
          "title": "A persistent homology-based topological loss for CNN-based multi-class segmentation of CMR. (arXiv:2107.12689v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Li Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to adapt\na segmentation model trained on the labeled source domain to the unlabeled\ntarget domain. Existing methods try to learn domain invariant features while\nsuffering from large domain gaps that make it difficult to correctly align\ndiscrepant features, especially in the initial training phase. To address this\nissue, we propose a novel Dual Soft-Paste (DSP) method in this paper.\nSpecifically, DSP selects some classes from a source domain image using a\nlong-tail class first sampling strategy and softly pastes the corresponding\nimage patch on both the source and target training images with a fusion weight.\nTechnically, we adopt the mean teacher framework for domain adaptation, where\nthe pasted source and target images go through the student network while the\noriginal target image goes through the teacher network. Output-level alignment\nis carried out by aligning the probability maps of the target fused image from\nboth networks using a weighted cross-entropy loss. In addition, feature-level\nalignment is carried out by aligning the feature maps of the source and target\nimages from student network using a weighted maximum mean discrepancy loss. DSP\nfacilitates the model learning domain-invariant features from the intermediate\ndomains, leading to faster convergence and better performance. Experiments on\ntwo challenging benchmarks demonstrate the superiority of DSP over\nstate-of-the-art methods. Code is available at\n\\url{https://github.com/GaoLii/DSP}.",
          "link": "http://arxiv.org/abs/2107.09600",
          "publishedOn": "2021-07-28T02:02:34.299Z",
          "wordCount": 695,
          "title": "DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation. (arXiv:2107.09600v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramon_E/0/1/0/all/0/1\">Eduard Ramon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triginer_G/0/1/0/all/0/1\">Gil Triginer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escur_J/0/1/0/all/0/1\">Janna Escur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pumarola_A/0/1/0/all/0/1\">Albert Pumarola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1\">Jaime Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Giro-i-Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>",
          "description": "Recent learning approaches that implicitly represent surface geometry using\ncoordinate-based neural representations have shown impressive results in the\nproblem of multi-view 3D reconstruction. The effectiveness of these techniques\nis, however, subject to the availability of a large number (several tens) of\ninput views of the scene, and computationally demanding optimizations. In this\npaper, we tackle these limitations for the specific problem of few-shot full 3D\nhead reconstruction, by endowing coordinate-based representations with a\nprobabilistic shape prior that enables faster convergence and better\ngeneralization when using few input images (down to three). First, we learn a\nshape model of 3D heads from thousands of incomplete raw scans using implicit\nrepresentations. At test time, we jointly overfit two coordinate-based neural\nnetworks to the scene, one modeling the geometry and another estimating the\nsurface radiance, using implicit differentiable rendering. We devise a\ntwo-stage optimization strategy in which the learned prior is used to\ninitialize and constrain the geometry during an initial optimization phase.\nThen, the prior is unfrozen and fine-tuned to the scene. By doing this, we\nachieve high-fidelity head reconstructions, including hair and shoulders, and\nwith a high level of detail that consistently outperforms both state-of-the-art\n3D Morphable Models methods in the few-shot scenario, and non-parametric\nmethods when large sets of views are available.",
          "link": "http://arxiv.org/abs/2107.12512",
          "publishedOn": "2021-07-28T02:02:34.240Z",
          "wordCount": 654,
          "title": "H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction. (arXiv:2107.12512v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuangjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>",
          "description": "Recent deep networks have convincingly demonstrated high capability in crowd\ncounting, which is a critical task attracting widespread attention due to its\nvarious industrial applications. Despite such progress, trained data-dependent\nmodels usually can not generalize well to unseen scenarios because of the\ninherent domain shift. To facilitate this issue, this paper proposes a novel\nadversarial scoring network (ASNet) to gradually bridge the gap across domains\nfrom coarse to fine granularity. In specific, at the coarse-grained stage, we\ndesign a dual-discriminator strategy to adapt source domain to be close to the\ntargets from the perspectives of both global and local feature space via\nadversarial learning. The distributions between two domains can thus be aligned\nroughly. At the fine-grained stage, we explore the transferability of source\ncharacteristics by scoring how similar the source samples are to target ones\nfrom multiple levels based on generative probability derived from coarse stage.\nGuided by these hierarchical scores, the transferable source features are\nproperly selected to enhance the knowledge transfer during the adaptation\nprocess. With the coarse-to-fine design, the generalization bottleneck induced\nfrom the domain discrepancy can be effectively alleviated. Three sets of\nmigration experiments show that the proposed methods achieve state-of-the-art\ncounting performance compared with major unsupervised methods.",
          "link": "http://arxiv.org/abs/2107.12858",
          "publishedOn": "2021-07-28T02:02:33.959Z",
          "wordCount": 657,
          "title": "Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring Network. (arXiv:2107.12858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dam_T/0/1/0/all/0/1\">Tanmoy Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anavatti_S/0/1/0/all/0/1\">Sreenatha G. Anavatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbass_H/0/1/0/all/0/1\">Hussein A. Abbass</a> (Fellow, IEEESchool of Engineering and Information Technology, University of New South Wales Canberra, Australia)",
          "description": "The Latent Space Clustering in Generative adversarial networks (ClusterGAN)\nmethod has been successful with high-dimensional data. However, the method\nassumes uniformlydistributed priors during the generation of modes, which isa\nrestrictive assumption in real-world data and cause loss ofdiversity in the\ngenerated modes. In this paper, we proposeself-augmentation information\nmaximization improved Clus-terGAN (SIMI-ClusterGAN) to learn the distinctive\npriorsfrom the data. The proposed SIMI-ClusterGAN consists offour deep neural\nnetworks: self-augmentation prior network,generator, discriminator and\nclustering inference autoencoder.The proposed method has been validated using\nseven bench-mark data sets and has shown improved performance overstate-of-the\nart methods. To demonstrate the superiority ofSIMI-ClusterGAN performance on\nimbalanced dataset, wehave discussed two imbalanced conditions on MNIST\ndatasetswith one-class imbalance and three classes imbalanced cases.The results\nhighlight the advantages of SIMI-ClusterGAN.",
          "link": "http://arxiv.org/abs/2107.12706",
          "publishedOn": "2021-07-28T02:02:33.823Z",
          "wordCount": 582,
          "title": "Improving ClusterGAN Using Self-AugmentedInformation Maximization of Disentangling LatentSpaces. (arXiv:2107.12706v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhaoyu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Pin Siang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Hsing Wang</a>",
          "description": "In this work, we propose a novel two-stage framework for the efficient 3D\npoint cloud object detection. Instead of transforming point clouds into 2D bird\neye view projections, we parse the raw point cloud data directly in the 3D\nspace yet achieve impressive efficiency and accuracy. To achieve this goal, we\npropose dynamic voxelization, a method that voxellizes points at local scale\non-the-fly. By doing so, we preserve the point cloud geometry with 3D voxels,\nand therefore waive the dependence on expensive MLPs to learn from point\ncoordinates. On the other hand, we inherently still follow the same processing\npattern as point-wise methods (e.g., PointNet) and no longer suffer from the\nquantization issue like conventional convolutions. For further speed\noptimization, we propose the grid-based downsampling and voxelization method,\nand provide different CUDA implementations to accommodate to the discrepant\nrequirements during training and inference phases. We highlight our efficiency\non KITTI 3D object detection dataset with 75 FPS and on Waymo Open dataset with\n25 FPS inference speed with satisfactory accuracy.",
          "link": "http://arxiv.org/abs/2107.12707",
          "publishedOn": "2021-07-28T02:02:33.541Z",
          "wordCount": null,
          "title": "DV-Det: Efficient 3D Point Cloud Object Detection with Dynamic Voxelization. (arXiv:2107.12707v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12978",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1\">Justin Szeto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>",
          "description": "There are many clinical contexts which require accurate detection and\nsegmentation of all focal pathologies (e.g. lesions, tumours) in patient\nimages. In cases where there are a mix of small and large lesions, standard\nbinary cross entropy loss will result in better segmentation of large lesions\nat the expense of missing small ones. Adjusting the operating point to\naccurately detect all lesions generally leads to oversegmentation of large\nlesions. In this work, we propose a novel reweighing strategy to eliminate this\nperformance gap, increasing small pathology detection performance while\nmaintaining segmentation accuracy. We show that our reweighing strategy vastly\noutperforms competing strategies based on experiments on a large scale,\nmulti-scanner, multi-center dataset of Multiple Sclerosis patient images.",
          "link": "http://arxiv.org/abs/2107.12978",
          "publishedOn": "2021-07-28T02:02:33.039Z",
          "wordCount": null,
          "title": "Optimizing Operating Points for High Performance Lesion Detection and Segmentation Using Lesion Size Reweighting. (arXiv:2107.12978v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.05105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhipeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peirong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>",
          "description": "For semantic segmentation, label probabilities are often uncalibrated as they\nare typically only the by-product of a segmentation task. Intersection over\nUnion (IoU) and Dice score are often used as criteria for segmentation success,\nwhile metrics related to label probabilities are not often explored. However,\nprobability calibration approaches have been studied, which match probability\noutputs with experimentally observed errors. These approaches mainly focus on\nclassification tasks, but not on semantic segmentation. Thus, we propose a\nlearning-based calibration method that focuses on multi-label semantic\nsegmentation. Specifically, we adopt a convolutional neural network to predict\nlocal temperature values for probability calibration. One advantage of our\napproach is that it does not change prediction accuracy, hence allowing for\ncalibration as a post-processing step. Experiments on the COCO, CamVid, and\nLPBA40 datasets demonstrate improved calibration performance for a range of\ndifferent metrics. We also demonstrate the good performance of our method for\nmulti-atlas brain segmentation from magnetic resonance images.",
          "link": "http://arxiv.org/abs/2008.05105",
          "publishedOn": "2021-07-28T02:02:33.038Z",
          "wordCount": 619,
          "title": "Local Temperature Scaling for Probability Calibration. (arXiv:2008.05105v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grassucci_E/0/1/0/all/0/1\">Eleonora Grassucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicero_E/0/1/0/all/0/1\">Edoardo Cicero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comminiello_D/0/1/0/all/0/1\">Danilo Comminiello</a>",
          "description": "Latest Generative Adversarial Networks (GANs) are gathering outstanding\nresults through a large-scale training, thus employing models composed of\nmillions of parameters requiring extensive computational capabilities. Building\nsuch huge models undermines their replicability and increases the training\ninstability. Moreover, multi-channel data, such as images or audio, are usually\nprocessed by realvalued convolutional networks that flatten and concatenate the\ninput, often losing intra-channel spatial relations. To address these issues\nrelated to complexity and information loss, we propose a family of\nquaternion-valued generative adversarial networks (QGANs). QGANs exploit the\nproperties of quaternion algebra, e.g., the Hamilton product, that allows to\nprocess channels as a single entity and capture internal latent relations,\nwhile reducing by a factor of 4 the overall number of parameters. We show how\nto design QGANs and to extend the proposed approach even to advanced models.We\ncompare the proposed QGANs with real-valued counterparts on several image\ngeneration benchmarks. Results show that QGANs are able to obtain better FID\nscores than real-valued GANs and to generate visually pleasing images.\nFurthermore, QGANs save up to 75% of the training parameters. We believe these\nresults may pave the way to novel, more accessible, GANs capable of improving\nperformance and saving computational resources.",
          "link": "http://arxiv.org/abs/2104.09630",
          "publishedOn": "2021-07-28T02:02:32.872Z",
          "wordCount": 678,
          "title": "Quaternion Generative Adversarial Networks. (arXiv:2104.09630v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>",
          "description": "We propose a new method to detect deepfake images using the cue of the source\nfeature inconsistency within the forged images. It is based on the hypothesis\nthat images' distinct source features can be preserved and extracted after\ngoing through state-of-the-art deepfake generation processes. We introduce a\nnovel representation learning approach, called pair-wise self-consistency\nlearning (PCL), for training ConvNets to extract these source features and\ndetect deepfake images. It is accompanied by a new image synthesis approach,\ncalled inconsistency image generator (I2G), to provide richly annotated\ntraining data for PCL. Experimental results on seven popular datasets show that\nour models improve averaged AUC over the state of the art from 96.45% to 98.05%\nin the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset\nevaluation.",
          "link": "http://arxiv.org/abs/2012.09311",
          "publishedOn": "2021-07-28T02:02:32.807Z",
          "wordCount": 594,
          "title": "Learning Self-Consistency for Deepfake Detection. (arXiv:2012.09311v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shihao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dylan Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>",
          "description": "Occlusions pose a significant challenge to optical flow algorithms that rely\non local evidences. We consider an occluded point to be one that is imaged in\nthe first frame but not in the next, a slight overloading of the standard\ndefinition since it also includes points that move out-of-frame. Estimating the\nmotion of these points is extremely difficult, particularly in the two-frame\nsetting. Previous work relies on CNNs to learn occlusions, without much\nsuccess, or requires multiple frames to reason about occlusions using temporal\nsmoothness. In this paper, we argue that the occlusion problem can be better\nsolved in the two-frame case by modelling image self-similarities. We introduce\na global motion aggregation module, a transformer-based approach to find\nlong-range dependencies between pixels in the first image, and perform global\naggregation on the corresponding motion features. We demonstrate that the\noptical flow estimates in the occluded regions can be significantly improved\nwithout damaging the performance in non-occluded regions. This approach obtains\nnew state-of-the-art results on the challenging Sintel dataset, improving the\naverage end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. At\nthe time of submission, our method ranks first on these benchmarks among all\npublished and unpublished approaches. Code is available at\nhttps://github.com/zacjiang/GMA",
          "link": "http://arxiv.org/abs/2104.02409",
          "publishedOn": "2021-07-28T02:02:32.750Z",
          "wordCount": 683,
          "title": "Learning to Estimate Hidden Motions with Global Motion Aggregation. (arXiv:2104.02409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1\">Zixuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haizhou Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>",
          "description": "The challenge of the Class Incremental Learning (CIL) lies in difficulty for\na learner to discern the old classes' data from the new while no previous data\nis preserved. Namely, the representation distribution of different phases\noverlaps with each other. In this paper, to alleviate the phenomenon of\nrepresentation overlapping for both memory-based and memory-free methods, we\npropose a new CIL framework, Contrastive Class Concentration for CIL (C4IL).\nOur framework leverages the class concentration effect of contrastive\nrepresentation learning, therefore yielding a representation distribution with\nbetter intra-class compactibility and inter-class separability. Quantitative\nexperiments showcase our framework that is effective in both memory-based and\nmemory-free cases: it outperforms the baseline methods of both cases by 5% in\nterms of the average and top-1 accuracy in 10-phase and 20-phase CIL.\nQualitative results also demonstrate that our method generates a more compact\nrepresentation distribution that alleviates the overlapping problem.",
          "link": "http://arxiv.org/abs/2107.12308",
          "publishedOn": "2021-07-28T02:02:32.735Z",
          "wordCount": 605,
          "title": "Alleviate Representation Overlapping in Class Incremental Learning by Contrastive Class Concentration. (arXiv:2107.12308v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Robin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>",
          "description": "Deep neural networks (DNNs) for the semantic segmentation of images are\nusually trained to operate on a predefined closed set of object classes. This\nis in contrast to the \"open world\" setting where DNNs are envisioned to be\ndeployed to. From a functional safety point of view, the ability to detect\nso-called \"out-of-distribution\" (OoD) samples, i.e., objects outside of a DNN's\nsemantic space, is crucial for many applications such as automated driving. A\nnatural baseline approach to OoD detection is to threshold on the pixel-wise\nsoftmax entropy. We present a two-step procedure that significantly improves\nthat approach. Firstly, we utilize samples from the COCO dataset as OoD proxy\nand introduce a second training objective to maximize the softmax entropy on\nthese samples. Starting from pretrained semantic segmentation networks we\nre-train a number of DNNs on different in-distribution datasets and\nconsistently observe improved OoD detection performance when evaluating on\ncompletely disjoint OoD datasets. Secondly, we perform a transparent\npost-processing step to discard false positive OoD samples by so-called \"meta\nclassification\". To this end, we apply linear models to a set of hand-crafted\nmetrics derived from the DNN's softmax probabilities. In our experiments we\nconsistently observe a clear additional gain in OoD detection performance,\ncutting down the number of detection errors by up to 52% when comparing the\nbest baseline with our results. We achieve this improvement sacrificing only\nmarginally in original segmentation performance. Therefore, our method\ncontributes to safer DNNs with more reliable overall system performance.",
          "link": "http://arxiv.org/abs/2012.06575",
          "publishedOn": "2021-07-28T02:02:32.609Z",
          "wordCount": 728,
          "title": "Entropy Maximization and Meta Classification for Out-Of-Distribution Detection in Semantic Segmentation. (arXiv:2012.06575v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1711.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_M/0/1/0/all/0/1\">Michael Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi-Dong Shen</a>",
          "description": "Matching images and sentences demands a fine understanding of both\nmodalities. In this paper, we propose a new system to discriminatively embed\nthe image and text to a shared visual-textual space. In this field, most\nexisting works apply the ranking loss to pull the positive image / text pairs\nclose and push the negative pairs apart from each other. However, directly\ndeploying the ranking loss is hard for network learning, since it starts from\nthe two heterogeneous features to build inter-modal relationship. To address\nthis problem, we propose the instance loss which explicitly considers the\nintra-modal data distribution. It is based on an unsupervised assumption that\neach image / text group can be viewed as a class. So the network can learn the\nfine granularity from every image/text group. The experiment shows that the\ninstance loss offers better weight initialization for the ranking loss, so that\nmore discriminative embeddings can be learned. Besides, existing works usually\napply the off-the-shelf features, i.e., word2vec and fixed visual feature. So\nin a minor contribution, this paper constructs an end-to-end dual-path\nconvolutional network to learn the image and text representations. End-to-end\nlearning allows the system to directly learn from the data and fully utilize\nthe supervision. On two generic retrieval datasets (Flickr30k and MSCOCO),\nexperiments demonstrate that our method yields competitive accuracy compared to\nstate-of-the-art methods. Moreover, in language based person retrieval, we\nimprove the state of the art by a large margin. The code has been made publicly\navailable.",
          "link": "http://arxiv.org/abs/1711.05535",
          "publishedOn": "2021-07-28T02:02:32.601Z",
          "wordCount": 743,
          "title": "Dual-Path Convolutional Image-Text Embeddings with Instance Loss. (arXiv:1711.05535v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenhongyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiaofei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>",
          "description": "Deep learning-based dense object detectors have achieved great success in the\npast few years and have been applied to numerous multimedia applications such\nas video understanding. However, the current training pipeline for dense\ndetectors is compromised to lots of conjunctions that may not hold. In this\npaper, we investigate three such important conjunctions: 1) only samples\nassigned as positive in classification head are used to train the regression\nhead; 2) classification and regression share the same input feature and\ncomputational fields defined by the parallel head architecture; and 3) samples\ndistributed in different feature pyramid layers are treated equally when\ncomputing the loss. We first carry out a series of pilot experiments to show\ndisentangling such conjunctions can lead to persistent performance improvement.\nThen, based on these findings, we propose Disentangled Dense Object Detector\n(DDOD), in which simple and effective disentanglement mechanisms are designed\nand integrated into the current state-of-the-art dense object detectors.\nExtensive experiments on MS COCO benchmark show that our approach can lead to\n2.0 mAP, 2.4 mAP and 2.2 mAP absolute improvements on RetinaNet, FCOS, and ATSS\nbaselines with negligible extra overhead. Notably, our best model reaches 55.0\nmAP on the COCO test-dev set and 93.5 AP on the hard subset of WIDER FACE,\nachieving new state-of-the-art performance on these two competitive benchmarks.\nCode is available at https://github.com/zehuichen123/DDOD.",
          "link": "http://arxiv.org/abs/2107.02963",
          "publishedOn": "2021-07-28T02:02:32.594Z",
          "wordCount": 676,
          "title": "Disentangle Your Dense Object Detector. (arXiv:2107.02963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dupont_R/0/1/0/all/0/1\">Robin Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_G/0/1/0/all/0/1\">Guillaume Michel</a>",
          "description": "Pruning seeks to design lightweight architectures by removing redundant\nweights in overparameterized networks. Most of the existing techniques first\nremove structured sub-networks (filters, channels,...) and then fine-tune the\nresulting networks to maintain a high accuracy. However, removing a whole\nstructure is a strong topological prior and recovering the accuracy, with\nfine-tuning, is highly cumbersome. In this paper, we introduce an \"end-to-end\"\nlightweight network design that achieves training and pruning simultaneously\nwithout fine-tuning. The design principle of our method relies on\nreparametrization that learns not only the weights but also the topological\nstructure of the lightweight sub-network. This reparametrization acts as a\nprior (or regularizer) that defines pruning masks implicitly from the weights\nof the underlying network, without increasing the number of training\nparameters. Sparsity is induced with a budget loss that provides an accurate\npruning. Extensive experiments conducted on the CIFAR10 and the TinyImageNet\ndatasets, using standard architectures (namely Conv4, VGG19 and ResNet18), show\ncompelling results without fine-tuning.",
          "link": "http://arxiv.org/abs/2107.03909",
          "publishedOn": "2021-07-28T02:02:32.587Z",
          "wordCount": 615,
          "title": "Weight Reparametrization for Budget-Aware Network Pruning. (arXiv:2107.03909v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12461",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zunair_H/0/1/0/all/0/1\">Hasib Zunair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamza_A/0/1/0/all/0/1\">A. Ben Hamza</a>",
          "description": "The U-Net architecture, built upon the fully convolutional network, has\nproven to be effective in biomedical image segmentation. However, U-Net applies\nskip connections to merge semantically different low- and high-level\nconvolutional features, resulting in not only blurred feature maps, but also\nover- and under-segmented target regions. To address these limitations, we\npropose a simple, yet effective end-to-end depthwise encoder-decoder fully\nconvolutional network architecture, called Sharp U-Net, for binary and\nmulti-class biomedical image segmentation. The key rationale of Sharp U-Net is\nthat instead of applying a plain skip connection, a depthwise convolution of\nthe encoder feature map with a sharpening kernel filter is employed prior to\nmerging the encoder and decoder features, thereby producing a sharpened\nintermediate feature map of the same size as the encoder map. Using this\nsharpening filter layer, we are able to not only fuse semantically less\ndissimilar features, but also to smooth out artifacts throughout the network\nlayers during the early stages of training. Our extensive experiments on six\ndatasets show that the proposed Sharp U-Net model consistently outperforms or\nmatches the recent state-of-the-art baselines in both binary and multi-class\nsegmentation tasks, while adding no extra learnable parameters. Furthermore,\nSharp U-Net outperforms baselines that have more than three times the number of\nlearnable parameters.",
          "link": "http://arxiv.org/abs/2107.12461",
          "publishedOn": "2021-07-28T02:02:32.568Z",
          "wordCount": 652,
          "title": "Sharp U-Net: Depthwise Convolutional Network for Biomedical Image Segmentation. (arXiv:2107.12461v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12889",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Felfeliyan_B/0/1/0/all/0/1\">Banafshe Felfeliyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1\">Abhilash Hareendranathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuntze_G/0/1/0/all/0/1\">Gregor Kuntze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1\">Jacob L. Jaremko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ronsky_J/0/1/0/all/0/1\">Janet L. Ronsky</a>",
          "description": "Objective assessment of Magnetic Resonance Imaging (MRI) scans of\nosteoarthritis (OA) can address the limitation of the current OA assessment.\nSegmentation of bone, cartilage, and joint fluid is necessary for the OA\nobjective assessment. Most of the proposed segmentation methods are not\nperforming instance segmentation and suffer from class imbalance problems. This\nstudy deployed Mask R-CNN instance segmentation and improved it (improved-Mask\nR-CNN (iMaskRCNN)) to obtain a more accurate generalized segmentation for\nOA-associated tissues. Training and validation of the method were performed\nusing 500 MRI knees from the Osteoarthritis Initiative (OAI) dataset and 97 MRI\nscans of patients with symptomatic hip OA. Three modifications to Mask R-CNN\nyielded the iMaskRCNN: adding a 2nd ROIAligned block, adding an extra decoder\nlayer to the mask-header, and connecting them by a skip connection. The results\nwere assessed using Hausdorff distance, dice score, and coefficients of\nvariation (CoV). The iMaskRCNN led to improved bone and cartilage segmentation\ncompared to Mask RCNN as indicated with the increase in dice score from 95% to\n98% for the femur, 95% to 97% for tibia, 71% to 80% for femoral cartilage, and\n81% to 82% for tibial cartilage. For the effusion detection, dice improved with\niMaskRCNN 72% versus MaskRCNN 71%. The CoV values for effusion detection\nbetween Reader1 and Mask R-CNN (0.33), Reader1 and iMaskRCNN (0.34), Reader2\nand Mask R-CNN (0.22), Reader2 and iMaskRCNN (0.29) are close to CoV between\ntwo readers (0.21), indicating a high agreement between the human readers and\nboth Mask R-CNN and iMaskRCNN. Mask R-CNN and iMaskRCNN can reliably and\nsimultaneously extract different scale articular tissues involved in OA,\nforming the foundation for automated assessment of OA. The iMaskRCNN results\nshow that the modification improved the network performance around the edges.",
          "link": "http://arxiv.org/abs/2107.12889",
          "publishedOn": "2021-07-28T02:02:32.560Z",
          "wordCount": 755,
          "title": "Improved-Mask R-CNN: Towards an Accurate Generic MSK MRI instance segmentation platform (Data from the Osteoarthritis Initiative). (arXiv:2107.12889v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Da Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_V/0/1/0/all/0/1\">Vincent Chow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popuri_K/0/1/0/all/0/1\">Karteek Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1\">Mirza Faisal Beg</a>",
          "description": "The latest advances in computer-assisted precision medicine are making it\nfeasible to move from population-wide models that are useful to discover\naggregate patterns that hold for group-based analysis to patient-specific\nmodels that can drive patient-specific decisions with regard to treatment\nchoices, and predictions of outcomes of treatment. Body Composition is\nrecognized as an important driver and risk factor for a wide variety of\ndiseases, as well as a predictor of individual patient-specific clinical\noutcomes to treatment choices or surgical interventions. 3D CT images are\nroutinely acquired in the oncological worklows and deliver accurate rendering\nof internal anatomy and therefore can be used opportunistically to assess the\namount of skeletal muscle and adipose tissue compartments. Powerful tools of\nartificial intelligence such as deep learning are making it feasible now to\nsegment the entire 3D image and generate accurate measurements of all internal\nanatomy. These will enable the overcoming of the severe bottleneck that existed\npreviously, namely, the need for manual segmentation, which was prohibitive to\nscale to the hundreds of 2D axial slices that made up a 3D volumetric image.\nAutomated tools such as presented here will now enable harvesting whole-body\nmeasurements from 3D CT or MRI images, leading to a new era of discovery of the\ndrivers of various diseases based on individual tissue, organ volume, shape,\nand functional status. These measurements were hitherto unavailable thereby\nlimiting the field to a very small and limited subset. These discoveries and\nthe potential to perform individual image segmentation with high speed and\naccuracy are likely to lead to the incorporation of these 3D measures into\nindividual specific treatment planning models related to nutrition, aging,\nchemotoxicity, surgery and survival after the onset of a major disease such as\ncancer.",
          "link": "http://arxiv.org/abs/2106.00652",
          "publishedOn": "2021-07-28T02:02:32.553Z",
          "wordCount": 849,
          "title": "Comprehensive Validation of Automated Whole Body Skeletal Muscle, Adipose Tissue, and Bone Segmentation from 3D CT images for Body Composition Analysis: Towards Extended Body Composition. (arXiv:2106.00652v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.10343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Ke Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>",
          "description": "Very deep Convolutional Neural Networks (CNNs) have greatly improved the\nperformance on various image restoration tasks. However, this comes at a price\nof increasing computational burden, hence limiting their practical usages. We\nobserve that some corrupted image regions are inherently easier to restore than\nothers since the distortion and content vary within an image. To leverage this,\nwe propose Path-Restore, a multi-path CNN with a pathfinder that can\ndynamically select an appropriate route for each image region. We train the\npathfinder using reinforcement learning with a difficulty-regulated reward.\nThis reward is related to the performance, complexity and \"the difficulty of\nrestoring a region\". A policy mask is further investigated to jointly process\nall the image regions. We conduct experiments on denoising and mixed\nrestoration tasks. The results show that our method achieves comparable or\nsuperior performance to existing approaches with less computational cost. In\nparticular, Path-Restore is effective for real-world denoising, where the noise\ndistribution varies across different regions on a single image. Compared to the\nstate-of-the-art RIDNet, our method achieves comparable performance and runs\n2.7x faster on the realistic Darmstadt Noise Dataset.",
          "link": "http://arxiv.org/abs/1904.10343",
          "publishedOn": "2021-07-28T02:02:32.545Z",
          "wordCount": 664,
          "title": "Path-Restore: Learning Network Path Selection for Image Restoration. (arXiv:1904.10343v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raumanns_R/0/1/0/all/0/1\">Ralf Raumanns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schouten_G/0/1/0/all/0/1\">Gerard Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joosten_M/0/1/0/all/0/1\">Max Joosten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1\">Josien P. W. Pluim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>",
          "description": "We present ENHANCE, an open dataset with multiple annotations to complement\nthe existing ISIC and PH2 skin lesion classification datasets. This dataset\ncontains annotations of visual ABC (asymmetry, border, colour) features from\nnon-expert annotation sources: undergraduate students, crowd workers from\nAmazon MTurk and classic image processing algorithms. In this paper we first\nanalyse the correlations between the annotations and the diagnostic label of\nthe lesion, as well as study the agreement between different annotation\nsources. Overall we find weak correlations of non-expert annotations with the\ndiagnostic label, and low agreement between different annotation sources. We\nthen study multi-task learning (MTL) with the annotations as additional labels,\nand show that non-expert annotations can improve (ensembles of)\nstate-of-the-art convolutional neural networks via MTL. We hope that our\ndataset can be used in further research into multiple annotations and/or MTL.\nAll data and models are available on Github:\nhttps://github.com/raumannsr/ENHANCE.",
          "link": "http://arxiv.org/abs/2107.12734",
          "publishedOn": "2021-07-28T02:02:32.525Z",
          "wordCount": 614,
          "title": "ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A case study for skin lesion classification. (arXiv:2107.12734v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wimmer_P/0/1/0/all/0/1\">Paul Wimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehnert_J/0/1/0/all/0/1\">Jens Mehnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Condurache</a>",
          "description": "State-of-the-art deep neural network (DNN) pruning techniques, applied\none-shot before training starts, evaluate sparse architectures with the help of\na single criterion -- called pruning score. Pruning weights based on a solitary\nscore works well for some architectures and pruning rates but may also fail for\nother ones. As a common baseline for pruning scores, we introduce the notion of\na generalized synaptic score (GSS). In this work we do not concentrate on a\nsingle pruning criterion, but provide a framework for combining arbitrary GSSs\nto create more powerful pruning strategies. These COmbined Pruning Scores\n(COPS) are obtained by solving a constrained optimization problem. Optimizing\nfor more than one score prevents the sparse network to overly specialize on an\nindividual task, thus COntrols Pruning before training Starts. The\ncombinatorial optimization problem given by COPS is relaxed on a linear program\n(LP). This LP is solved analytically and determines a solution for COPS.\nFurthermore, an algorithm to compute it for two scores numerically is proposed\nand evaluated. Solving COPS in such a way has lower complexity than the best\ngeneral LP solver. In our experiments we compared pruning with COPS against\nstate-of-the-art methods for different network architectures and image\nclassification tasks and obtained improved results.",
          "link": "http://arxiv.org/abs/2107.12673",
          "publishedOn": "2021-07-28T02:02:32.519Z",
          "wordCount": 647,
          "title": "COPS: Controlled Pruning Before Training Starts. (arXiv:2107.12673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Mingyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Honghui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shumin Han</a>",
          "description": "Transformers with remarkable global representation capacities achieve\ncompetitive results for visual tasks, but fail to consider high-level local\npattern information in input images. In this paper, we present a generic\nDual-stream Network (DS-Net) to fully explore the representation capacity of\nlocal and global pattern features for image classification. Our DS-Net can\nsimultaneously calculate fine-grained and integrated features and efficiently\nfuse them. Specifically, we propose an Intra-scale Propagation module to\nprocess two different resolutions in each block and an Inter-Scale Alignment\nmodule to perform information interaction across features at dual scales.\nBesides, we also design a Dual-stream FPN (DS-FPN) to further enhance\ncontextual information for downstream dense predictions. Without bells and\nwhistles, the propsed DS-Net outperforms Deit-Small by 2.4% in terms of top-1\naccuracy on ImageNet-1k and achieves state-of-the-art performance over other\nVision Transformers and ResNets. For object detection and instance\nsegmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5 %\nin terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art\nscheme, which significantly demonstrates its potential to be a general backbone\nin vision tasks. The code will be released soon.",
          "link": "http://arxiv.org/abs/2105.14734",
          "publishedOn": "2021-07-28T02:02:32.504Z",
          "wordCount": 660,
          "title": "Dual-stream Network for Visual Recognition. (arXiv:2105.14734v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kosman_E/0/1/0/all/0/1\">Eitan Kosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Dotan Di Castro</a>",
          "description": "Autonomous driving gained huge traction in recent years, due to its potential\nto change the way we commute. Much effort has been put into trying to estimate\nthe state of a vehicle. Meanwhile, learning to forecast the state of a vehicle\nahead introduces new capabilities, such as predicting dangerous situations.\nMoreover, forecasting brings new supervision opportunities by learning to\npredict richer a context, expressed by multiple horizons. Intuitively, a video\nstream originated from a front-facing camera is necessary because it encodes\ninformation about the upcoming road. Besides, historical traces of the\nvehicle's states give more context. In this paper, we tackle multi-horizon\nforecasting of vehicle states by fusing the two modalities. We design and\nexperiment with 3 end-to-end architectures that exploit 3D convolutions for\nvisual features extraction and 1D convolutions for features extraction from\nspeed and steering angle traces. To demonstrate the effectiveness of our\nmethod, we perform extensive experiments on two publicly available real-world\ndatasets, Comma2k19 and the Udacity challenge. We show that we are able to\nforecast a vehicle's state to various horizons, while outperforming the current\nstate-of-the-art results on the related task of driving state estimation. We\nexamine the contribution of vision features, and find that a model fed with\nvision features achieves an error that is 56.6% and 66.9% of the error of a\nmodel that doesn't use those features, on the Udacity and Comma2k19 datasets\nrespectively.",
          "link": "http://arxiv.org/abs/2107.12674",
          "publishedOn": "2021-07-28T02:02:32.496Z",
          "wordCount": 672,
          "title": "Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time Series Forecasting. (arXiv:2107.12674v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gothankar_R/0/1/0/all/0/1\">Ruchira Gothankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1\">Fabio Di Troia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "YouTube videos often include captivating descriptions and intriguing\nthumbnails designed to increase the number of views, and thereby increase the\nrevenue for the person who posted the video. This creates an incentive for\npeople to post clickbait videos, in which the content might deviate\nsignificantly from the title, description, or thumbnail. In effect, users are\ntricked into clicking on clickbait videos. In this research, we consider the\nchallenging problem of detecting clickbait YouTube videos. We experiment with\nmultiple state-of-the-art machine learning techniques using a variety of\ntextual features.",
          "link": "http://arxiv.org/abs/2107.12791",
          "publishedOn": "2021-07-28T02:02:32.478Z",
          "wordCount": 517,
          "title": "Clickbait Detection in YouTube Videos. (arXiv:2107.12791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Sreyas Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_J/0/1/0/all/0/1\">Joshua L. Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzorro_R/0/1/0/all/0/1\">Ramon Manzorro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crozier_P/0/1/0/all/0/1\">Peter A. Crozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoncelli_E/0/1/0/all/0/1\">Eero P. Simoncelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>",
          "description": "Deep convolutional neural networks (CNNs) for image denoising are usually\ntrained on large datasets. These models achieve the current state of the art,\nbut they have difficulties generalizing when applied to data that deviate from\nthe training distribution. Recent work has shown that it is possible to train\ndenoisers on a single noisy image. These models adapt to the features of the\ntest image, but their performance is limited by the small amount of information\nused to train them. Here we propose \"GainTuning\", in which CNN models\npre-trained on large datasets are adaptively and selectively adjusted for\nindividual test images. To avoid overfitting, GainTuning optimizes a single\nmultiplicative scaling parameter (the \"Gain\") of each channel in the\nconvolutional layers of the CNN. We show that GainTuning improves\nstate-of-the-art CNNs on standard image-denoising benchmarks, boosting their\ndenoising performance on nearly every image in a held-out test set. These\nadaptive improvements are even more substantial for test images differing\nsystematically from the training data, either in noise level or image type. We\nillustrate the potential of adaptive denoising in a scientific application, in\nwhich a CNN is trained on synthetic data, and tested on real\ntransmission-electron-microscope images. In contrast to the existing\nmethodology, GainTuning is able to faithfully reconstruct the structure of\ncatalytic nanoparticles from these data at extremely low signal-to-noise\nratios.",
          "link": "http://arxiv.org/abs/2107.12815",
          "publishedOn": "2021-07-28T02:02:32.472Z",
          "wordCount": 655,
          "title": "Adaptive Denoising via GainTuning. (arXiv:2107.12815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Darafsh_S/0/1/0/all/0/1\">Sahar Darafsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidary_S/0/1/0/all/0/1\">Saeed Shiry Ghidary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_M/0/1/0/all/0/1\">Morteza Saheb Zamani</a>",
          "description": "With the rapid increase in digital technologies, most fields of study include\nrecognition of human activity and intention recognition, which are important in\nsmart environments. In this research, we introduce a real-time activity\nrecognition to recognize people's intentions to pass or not pass a door. This\nsystem, if applied in elevators and automatic doors will save energy and\nincrease efficiency. For this study, data preparation is applied to combine the\nspatial and temporal features with the help of digital image processing\nprinciples. Nevertheless, unlike previous studies, only one AlexNet neural\nnetwork is used instead of two-stream convolutional neural networks. Our\nembedded system was implemented with an accuracy of 98.78% on our Intention\nRecognition dataset. We also examined our data representation approach on other\ndatasets, including HMDB-51, KTH, and Weizmann, and obtained accuracy of\n78.48%, 97.95%, and 100%, respectively. The image recognition and neural\nnetwork models were simulated and implemented using Xilinx simulators for\nZCU102 board. The operating frequency of this embedded system is 333 MHz, and\nit works in real-time with 120 frames per second (fps).",
          "link": "http://arxiv.org/abs/2107.12744",
          "publishedOn": "2021-07-28T02:02:32.464Z",
          "wordCount": 618,
          "title": "Real-Time Activity Recognition and Intention Recognition Using a Vision-based Embedded System. (arXiv:2107.12744v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1\">Lukas Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1\">Lea Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messner_E/0/1/0/all/0/1\">Eva-Maria Me&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>",
          "description": "Emotion is an inherently subjective psychophysiological human-state and to\nproduce an agreed-upon representation (gold standard) for continuous emotion\nrequires a time-consuming and costly training procedure of multiple human\nannotators. There is strong evidence in the literature that physiological\nsignals are sufficient objective markers for states of emotion, particularly\narousal. In this contribution, we utilise a dataset which includes continuous\nemotion and physiological signals - Heartbeats per Minute (BPM), Electrodermal\nActivity (EDA), and Respiration-rate - captured during a stress induced\nscenario (Trier Social Stress Test). We utilise a Long Short-Term Memory,\nRecurrent Neural Network to explore the benefit of fusing these physiological\nsignals with arousal as the target, learning from various audio, video, and\ntextual based features. We utilise the state-of-the-art MuSe-Toolbox to\nconsider both annotation delay and inter-rater agreement weighting when fusing\nthe target signals. An improvement in Concordance Correlation Coefficient (CCC)\nis seen across features sets when fusing EDA with arousal, compared to the\narousal only gold standard results. Additionally, BERT-based textual features'\nresults improved for arousal plus all physiological signals, obtaining up to\n.3344 CCC compared to .2118 CCC for arousal only. Multimodal fusion also\nimproves overall CCC with audio plus video features obtaining up to .6157 CCC\nto recognize arousal plus EDA and BPM.",
          "link": "http://arxiv.org/abs/2107.12964",
          "publishedOn": "2021-07-28T02:02:32.456Z",
          "wordCount": 660,
          "title": "A Physiologically-adapted Gold Standard for Arousal During a Stress Induced Scenario. (arXiv:2107.12964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xizhou Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>",
          "description": "As a kind of generative self-supervised learning methods, generative\nadversarial nets have been widely studied in the field of anomaly detection.\nHowever, the representation learning ability of the generator is limited since\nit pays too much attention to pixel-level details, and generator is difficult\nto learn abstract semantic representations from label prediction pretext tasks\nas effective as discriminator. In order to improve the representation learning\nability of generator, we propose a self-supervised learning framework combining\ngenerative methods and discriminative methods. The generator no longer learns\nrepresentation by reconstruction error, but the guidance of discriminator, and\ncould benefit from pretext tasks designed for discriminative methods. Our\ndiscriminative-generative representation learning method has performance close\nto discriminative methods and has a great advantage in speed. Our method used\nin one-class anomaly detection task significantly outperforms several\nstate-of-the-arts on multiple benchmark data sets, increases the performance of\nthe top-performing GAN-based baseline by 6% on CIFAR-10 and 2% on MVTAD.",
          "link": "http://arxiv.org/abs/2107.12753",
          "publishedOn": "2021-07-28T02:02:32.449Z",
          "wordCount": 598,
          "title": "Discriminative-Generative Representation Learning for One-Class Anomaly Detection. (arXiv:2107.12753v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Harish_A/0/1/0/all/0/1\">Abhinav Narayan Harish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagar_R/0/1/0/all/0/1\">Rajendra Nagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>",
          "description": "Autonomous assembly of objects is an essential task in robotics and 3D\ncomputer vision. It has been studied extensively in robotics as a problem of\nmotion planning, actuator control and obstacle avoidance. However, the task of\ndeveloping a generalized framework for assembly robust to structural variants\nremains relatively unexplored. In this work, we tackle this problem using a\nrecurrent graph learning framework considering inter-part relations and the\nprogressive update of the part pose. Our network can learn more plausible\npredictions of shape structure by accounting for priorly assembled parts.\nCompared to the current state-of-the-art, our network yields up to 10%\nimprovement in part accuracy and up to 15% improvement in connectivity accuracy\non the PartNet dataset. Moreover, our resulting latent space facilitates\nexciting applications such as shape recovery from the point-cloud components.\nWe conduct extensive experiments to justify our design choices and demonstrate\nthe effectiveness of the proposed framework.",
          "link": "http://arxiv.org/abs/2107.12859",
          "publishedOn": "2021-07-28T02:02:32.441Z",
          "wordCount": 598,
          "title": "RGL-NET: A Recurrent Graph Learning framework for Progressive Part Assembly. (arXiv:2107.12859v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaohui Hu</a>",
          "description": "Since the superiority of Transformer in learning long-term dependency, the\nsign language Transformer model achieves remarkable progress in Sign Language\nRecognition (SLR) and Translation (SLT). However, there are several issues with\nthe Transformer that prevent it from better sign language understanding. The\nfirst issue is that the self-attention mechanism learns sign video\nrepresentation in a frame-wise manner, neglecting the temporal semantic\nstructure of sign gestures. Secondly, the attention mechanism with absolute\nposition encoding is direction and distance unaware, thus limiting its ability.\nTo address these issues, we propose a new model architecture, namely PiSLTRc,\nwith two distinctive characteristics: (i) content-aware and position-aware\nconvolution layers. Specifically, we explicitly select relevant features using\na novel content-aware neighborhood gathering method. Then we aggregate these\nfeatures with position-informed temporal convolution layers, thus generating\nrobust neighborhood-enhanced sign representation. (ii) injecting the relative\nposition information to the attention mechanism in the encoder, decoder, and\neven encoder-decoder cross attention. Compared with the vanilla Transformer\nmodel, our model performs consistently better on three large-scale sign\nlanguage benchmarks: PHOENIX-2014, PHOENIX-2014-T and CSL. Furthermore,\nextensive experiments demonstrate that the proposed method achieves\nstate-of-the-art performance on translation quality with $+1.6$ BLEU\nimprovements.",
          "link": "http://arxiv.org/abs/2107.12600",
          "publishedOn": "2021-07-28T02:02:32.420Z",
          "wordCount": 624,
          "title": "PiSLTRc: Position-informed Sign Language Transformer with Content-aware Convolution. (arXiv:2107.12600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.11091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlett_T/0/1/0/all/0/1\">Torsten Schlett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Face recognition can benefit from the utilization of depth data captured\nusing low-cost cameras, in particular for presentation attack detection\npurposes. Depth video output from these capture devices can however contain\ndefects such as holes or general depth inaccuracies. This work proposes a deep\nlearning face depth enhancement method in this context of facial biometrics,\nwhich adds a security aspect to the topic. U-Net-like architectures are\nutilized, and the networks are compared against hand-crafted enhancer types, as\nwell as a similar depth enhancer network from related work trained for an\nadjacent application scenario. All tested enhancer types exclusively use depth\ndata as input, which differs from methods that enhance depth based on\nadditional input data such as visible light color images. Synthetic face depth\nground truth images and degraded forms thereof are created with help of PRNet,\nto train multiple deep learning enhancer models with different network sizes\nand training configurations. Evaluations are carried out on the synthetic data,\non Kinect v1 images from the KinectFaceDB, and on in-house RealSense D435\nimages. These evaluations include an assessment of the falsification for\noccluded face depth input, which is relevant to biometric security. The\nproposed deep learning enhancers yield noticeably better results than the\ntested preexisting enhancers, without overly falsifying depth data when\nnon-face input is provided, and are shown to reduce the error of a simple\nlandmark-based PAD method.",
          "link": "http://arxiv.org/abs/2006.11091",
          "publishedOn": "2021-07-28T02:02:32.413Z",
          "wordCount": 698,
          "title": "Deep Learning-based Single Image Face Depth Data Enhancement. (arXiv:2006.11091v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuda Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xin Du</a>",
          "description": "Image enhancement is a subjective process whose targets vary with user\npreferences. In this paper, we propose a deep learning-based image enhancement\nmethod covering multiple tonal styles using only a single model dubbed\nStarEnhancer. It can transform an image from one tonal style to another, even\nif that style is unseen. With a simple one-time setting, users can customize\nthe model to make the enhanced images more in line with their aesthetics. To\nmake the method more practical, we propose a well-designed enhancer that can\nprocess a 4K-resolution image over 200 FPS but surpasses the contemporaneous\nsingle style image enhancement methods in terms of PSNR, SSIM, and LPIPS.\nFinally, our proposed enhancement method has good interactability, which allows\nthe user to fine-tune the enhanced image using intuitive options.",
          "link": "http://arxiv.org/abs/2107.12898",
          "publishedOn": "2021-07-28T02:02:32.384Z",
          "wordCount": 560,
          "title": "StarEnhancer: Learning Real-Time and Style-Aware Image Enhancement. (arXiv:2107.12898v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1\">Akshay Rangesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1\">Nachiket Deo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1\">Ross Greer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunaratne_P/0/1/0/all/0/1\">Pujitha Gunaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1\">Mohan M. Trivedi</a>",
          "description": "Understanding occupant-vehicle interactions by modeling control transitions\nis important to ensure safe approaches to passenger vehicle automation. Models\nwhich contain contextual, semantically meaningful representations of driver\nstates can be used to determine the appropriate timing and conditions for\ntransfer of control between driver and vehicle. However, such models rely on\nreal-world control take-over data from drivers engaged in distracting\nactivities, which is costly to collect. Here, we introduce a scheme for data\naugmentation for such a dataset. Using the augmented dataset, we develop and\ntrain take-over time (TOT) models that operate sequentially on mid and\nhigh-level features produced by computer vision algorithms operating on\ndifferent driver-facing camera views, showing models trained on the augmented\ndataset to outperform the initial dataset. The demonstrated model features\nencode different aspects of the driver state, pertaining to the face, hands,\nfoot and upper body of the driver. We perform ablative experiments on feature\ncombinations as well as model architectures, showing that a TOT model supported\nby augmented data can be used to produce continuous estimates of take-over\ntimes without delay, suitable for complex real-world scenarios.",
          "link": "http://arxiv.org/abs/2107.12932",
          "publishedOn": "2021-07-28T02:02:32.345Z",
          "wordCount": 639,
          "title": "Predicting Take-over Time for Autonomous Driving with Real-World Data: Robust Data Augmentation, Models, and Evaluation. (arXiv:2107.12932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>",
          "description": "This paper presents the evaluation of effects of image size on deep learning\nperformance via semantic segmentation of magnetic resonance heart images with\nU-net for fully automated quantification of myocardial infarction. Both\nnon-extra pixel and extra pixel interpolation algorithms are used to change the\nsize of images in datasets of interest. Extra class labels, in interpolated\nground truth segmentation images, are removed using thresholding, median\nfiltering, and subtraction strategies. Common class metrics are used to\nevaluate the quality of semantic segmentation with U-net against the ground\ntruth segmentation while arbitrary threshold, comparison of the sums, and sums\nof differences between medical experts and fully automated results are options\nused to estimate the relationship between medical experts-based quantification\nand fully automated quantification results.",
          "link": "http://arxiv.org/abs/2101.11508",
          "publishedOn": "2021-07-28T02:02:32.326Z",
          "wordCount": 592,
          "title": "Effects of Image Size on Deep Learning. (arXiv:2101.11508v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "We consider the problem of estimating frame-level full human body meshes\ngiven a video of a person with natural motion dynamics. While much progress in\nthis field has been in single image-based mesh estimation, there has been a\nrecent uptick in efforts to infer mesh dynamics from video given its role in\nalleviating issues such as depth ambiguity and occlusions. However, a key\nlimitation of existing work is the assumption that all the observed motion\ndynamics can be modeled using one dynamical/recurrent model. While this may\nwork well in cases with relatively simplistic dynamics, inference with\nin-the-wild videos presents many challenges. In particular, it is typically the\ncase that different body parts of a person undergo different dynamics in the\nvideo, e.g., legs may move in a way that may be dynamically different from\nhands (e.g., a person dancing). To address these issues, we present a new\nmethod for video mesh recovery that divides the human mesh into several local\nparts following the standard skeletal model. We then model the dynamics of each\nlocal part with separate recurrent models, with each model conditioned\nappropriately based on the known kinematic structure of the human body. This\nresults in a structure-informed local recurrent learning architecture that can\nbe trained in an end-to-end fashion with available annotations. We conduct a\nvariety of experiments on standard video mesh recovery benchmark datasets such\nas Human3.6M, MPI-INF-3DHP, and 3DPW, demonstrating the efficacy of our design\nof modeling local dynamics as well as establishing state-of-the-art results\nbased on standard evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.12847",
          "publishedOn": "2021-07-28T02:02:32.316Z",
          "wordCount": 708,
          "title": "Learning Local Recurrent Models for Human Mesh Recovery. (arXiv:2107.12847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laousy_O/0/1/0/all/0/1\">Othmane Laousy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1\">Edouard Oyallon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>",
          "description": "Sarcopenia is a medical condition characterized by a reduction in muscle mass\nand function. A quantitative diagnosis technique consists of localizing the CT\nslice passing through the middle of the third lumbar area (L3) and segmenting\nmuscles at this level. In this paper, we propose a deep reinforcement learning\nmethod for accurate localization of the L3 CT slice. Our method trains a\nreinforcement learning agent by incentivizing it to discover the right\nposition. Specifically, a Deep Q-Network is trained to find the best policy to\nfollow for this problem. Visualizing the training process shows that the agent\nmimics the scrolling of an experienced radiologist. Extensive experiments\nagainst other state-of-the-art deep learning based methods for L3 localization\nprove the superiority of our technique which performs well even with limited\namount of data and annotations.",
          "link": "http://arxiv.org/abs/2107.12800",
          "publishedOn": "2021-07-28T02:02:32.301Z",
          "wordCount": 577,
          "title": "Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment. (arXiv:2107.12800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1\">Andr&#xe9;s G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genevois_T/0/1/0/all/0/1\">Thomas Genevois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lussereau_J/0/1/0/all/0/1\">Jerome Lussereau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laugier_C/0/1/0/all/0/1\">Christian Laugier</a>",
          "description": "Object detection is a critical problem for the safe interaction between\nautonomous vehicles and road users. Deep-learning methodologies allowed the\ndevelopment of object detection approaches with better performance. However,\nthere is still the challenge to obtain more characteristics from the objects\ndetected in real-time. The main reason is that more information from the\nenvironment's objects can improve the autonomous vehicle capacity to face\ndifferent urban situations. This paper proposes a new approach to detect static\nand dynamic objects in front of an autonomous vehicle. Our approach can also\nget other characteristics from the objects detected, like their position,\nvelocity, and heading. We develop our proposal fusing results of the\nenvironment's interpretations achieved of YoloV3 and a Bayesian filter. To\ndemonstrate our proposal's performance, we asses it through a benchmark dataset\nand real-world data obtained from an autonomous platform. We compared the\nresults achieved with another approach.",
          "link": "http://arxiv.org/abs/2107.12692",
          "publishedOn": "2021-07-28T02:02:31.716Z",
          "wordCount": 599,
          "title": "Dynamic and Static Object Detection Considering Fusion Regions and Point-wise Features. (arXiv:2107.12692v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cesaire_M/0/1/0/all/0/1\">Manon C&#xe9;saire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajri_H/0/1/0/all/0/1\">Hatem Hajri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>",
          "description": "This paper introduces stochastic sparse adversarial attacks (SSAA), simple,\nfast and purely noise-based targeted and untargeted $L_0$ attacks of neural\nnetwork classifiers (NNC). SSAA are devised by exploiting a simple small-time\nexpansion idea widely used for Markov processes and offer new examples of $L_0$\nattacks whose studies have been limited. They are designed to solve the known\nscalability issue of the family of Jacobian-based saliency maps attacks to\nlarge datasets and they succeed in solving it. Experiments on small and large\ndatasets (CIFAR-10 and ImageNet) illustrate further advantages of SSAA in\ncomparison with the-state-of-the-art methods. For instance, in the untargeted\ncase, our method called Voting Folded Gaussian Attack (VFGA) scales efficiently\nto ImageNet and achieves a significantly lower $L_0$ score than SparseFool (up\nto $\\frac{2}{5}$ lower) while being faster. Moreover, VFGA achieves better\n$L_0$ scores on ImageNet than Sparse-RS when both attacks are fully successful\non a large number of samples. Codes are publicly available through the link\nhttps://github.com/SSAA3/stochastic-sparse-adv-attacks",
          "link": "http://arxiv.org/abs/2011.12423",
          "publishedOn": "2021-07-28T02:02:31.697Z",
          "wordCount": 644,
          "title": "Stochastic sparse adversarial attacks. (arXiv:2011.12423v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)",
          "description": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.",
          "link": "http://arxiv.org/abs/1907.02704",
          "publishedOn": "2021-07-28T02:02:31.671Z",
          "wordCount": 714,
          "title": "Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhi Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jianwei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaohui Hu</a>",
          "description": "Continuous sign language recognition (cSLR) is a public significant task that\ntranscribes a sign language video into an ordered gloss sequence. It is\nimportant to capture the fine-grained gloss-level details, since there is no\nexplicit alignment between sign video frames and the corresponding glosses.\nAmong the past works, one promising way is to adopt a one-dimensional\nconvolutional network (1D-CNN) to temporally fuse the sequential frames.\nHowever, CNNs are agnostic to similarity or dissimilarity, and thus are unable\nto capture local consistent semantics within temporally neighboring frames. To\naddress the issue, we propose to adaptively fuse local features via temporal\nsimilarity for this task. Specifically, we devise a Multi-scale Local-Temporal\nSimilarity Fusion Network (mLTSF-Net) as follows: 1) In terms of a specific\nvideo frame, we firstly select its similar neighbours with multi-scale\nreceptive regions to accommodate different lengths of glosses. 2) To ensure\ntemporal consistency, we then use position-aware convolution to temporally\nconvolve each scale of selected frames. 3) To obtain a local-temporally\nenhanced frame-wise representation, we finally fuse the results of different\nscales using a content-dependent aggregator. We train our model in an\nend-to-end fashion, and the experimental results on RWTH-PHOENIX-Weather 2014\ndatasets (RWTH) demonstrate that our model achieves competitive performance\ncompared with several state-of-the-art models.",
          "link": "http://arxiv.org/abs/2107.12762",
          "publishedOn": "2021-07-28T02:02:31.663Z",
          "wordCount": 649,
          "title": "Multi-Scale Local-Temporal Similarity Fusion for Continuous Sign Language Recognition. (arXiv:2107.12762v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhirui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zekun Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yabang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1\">Andrew Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>",
          "description": "3D deep learning has been increasingly more popular for a variety of tasks\nincluding many safety-critical applications. However, recently several works\nraise the security issues of 3D deep nets. Although most of these works\nconsider adversarial attacks, we identify that backdoor attack is indeed a more\nserious threat to 3D deep learning systems but remains unexplored. We present\nthe backdoor attacks in 3D with a unified framework that exploits the unique\nproperties of 3D data and networks. In particular, we design two attack\napproaches: the poison-label attack and the clean-label attack. The first one\nis straightforward and effective in practice, while the second one is more\nsophisticated assuming there are certain data inspections. The attack\nalgorithms are mainly motivated and developed by 1) the recent discovery of 3D\nadversarial samples which demonstrate the vulnerability of 3D deep nets under\nspatial transformations; 2) the proposed feature disentanglement technique that\nmanipulates the feature of the data through optimization methods and its\npotential to embed a new task. Extensive experiments show the efficacy of the\npoison-label attack with over 95% success rate across several 3D datasets and\nmodels, and the ability of clean-label attack against data filtering with\naround 50% success rate. Our proposed backdoor attack in 3D point cloud is\nexpected to perform as a baseline for improving the robustness of 3D deep\nmodels.",
          "link": "http://arxiv.org/abs/2103.16074",
          "publishedOn": "2021-07-28T02:02:31.649Z",
          "wordCount": 702,
          "title": "PointBA: Towards Backdoor Attacks in 3D Point Cloud. (arXiv:2103.16074v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1\">P.S. Sastry</a>",
          "description": "Deep Neural Networks (DNNs) have been shown to be susceptible to memorization\nor overfitting in the presence of noisily labelled data. For the problem of\nrobust learning under such noisy data, several algorithms have been proposed. A\nprominent class of algorithms rely on sample selection strategies, motivated by\ncurriculum learning. For example, many algorithms use the `small loss trick'\nwherein a fraction of samples with loss values below a certain threshold are\nselected for training. These algorithms are sensitive to such thresholds, and\nit is difficult to fix or learn these thresholds. Often, these algorithms also\nrequire information such as label noise rates which are typically unavailable\nin practice. In this paper, we propose a data-dependent, adaptive sample\nselection strategy that relies only on batch statistics of a given mini-batch\nto provide robustness against label noise. The algorithm does not have any\nadditional hyperparameters for sample selection, does not need any information\non noise rates, and does not need access to separate data with clean labels. We\nempirically demonstrate the effectiveness of our algorithm on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.15292",
          "publishedOn": "2021-07-28T02:02:31.629Z",
          "wordCount": 641,
          "title": "Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "Traditional learning systems are trained in closed-world for a fixed number\nof classes, and need pre-collected datasets in advance. However, new classes\noften emerge in real-world applications and should be learned incrementally.\nFor example, in electronic commerce, new types of products appear daily, and in\na social media community, new topics emerge frequently. Under such\ncircumstances, incremental models should learn several new classes at a time\nwithout forgetting. We find a strong correlation between old and new classes in\nincremental learning, which can be applied to relate and facilitate different\nlearning stages mutually. As a result, we propose CO-transport for class\nIncremental Learning (COIL), which learns to relate across incremental tasks\nwith the class-wise semantic relationship. In detail, co-transport has two\naspects: prospective transport tries to augment the old classifier with optimal\ntransported knowledge as fast model adaptation. Retrospective transport aims to\ntransport new class classifiers backward as old ones to overcome forgetting.\nWith these transports, COIL efficiently adapts to new tasks, and stably resists\nforgetting. Experiments on benchmark and real-world multimedia datasets\nvalidate the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2107.12654",
          "publishedOn": "2021-07-28T02:02:31.622Z",
          "wordCount": 616,
          "title": "Co-Transport for Class-Incremental Learning. (arXiv:2107.12654v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ponomarev_E/0/1/0/all/0/1\">Evgeny Ponomarev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matveev_S/0/1/0/all/0/1\">Sergey Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1\">Ivan Oseledets</a>",
          "description": "A lot of deep learning applications are desired to be run on mobile devices.\nBoth accuracy and inference time are meaningful for a lot of them. While the\nnumber of FLOPs is usually used as a proxy for neural network latency, it may\nbe not the best choice. In order to obtain a better approximation of latency,\nresearch community uses look-up tables of all possible layers for latency\ncalculation for the final prediction of the inference on mobile CPU. It\nrequires only a small number of experiments. Unfortunately, on mobile GPU this\nmethod is not applicable in a straight-forward way and shows low precision. In\nthis work, we consider latency approximation on mobile GPU as a data and\nhardware-specific problem. Our main goal is to construct a convenient latency\nestimation tool for investigation(LETI) of neural network inference and\nbuilding robust and accurate latency prediction models for each specific task.\nTo achieve this goal, we build open-source tools which provide a convenient way\nto conduct massive experiments on different target devices focusing on mobile\nGPU. After evaluation of the dataset, we learn the regression model on\nexperimental data and use it for future latency prediction and analysis. We\nexperimentally demonstrate the applicability of such an approach on a subset of\npopular NAS-Benchmark 101 dataset and also evaluate the most popular neural\nnetwork architectures for two mobile GPUs. As a result, we construct latency\nprediction model with good precision on the target evaluation subset. We\nconsider LETI as a useful tool for neural architecture search or massive\nlatency evaluation. The project is available at https://github.com/leti-ai",
          "link": "http://arxiv.org/abs/2010.02871",
          "publishedOn": "2021-07-28T02:02:31.615Z",
          "wordCount": 758,
          "title": "LETI: Latency Estimation Tool and Investigation of Neural Networks inference on Mobile GPU. (arXiv:2010.02871v2 [cs.PF] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noh_B/0/1/0/all/0/1\">Byeongjoon Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hansaem Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>",
          "description": "Traffic accidents are a threat to human lives, particularly pedestrians\ncausing premature deaths. Therefore, it is necessary to devise systems to\nprevent accidents in advance and respond proactively, using potential risky\nsituations as one of the surrogate safety measurements. This study introduces a\nnew concept of a pedestrian safety system that combines the field and the\ncentralized processes. The system can warn of upcoming risks immediately in the\nfield and improve the safety of risk frequent areas by assessing the safety\nlevels of roads without actual collisions. In particular, this study focuses on\nthe latter by introducing a new analytical framework for a crosswalk safety\nassessment with behaviors of vehicle/pedestrian and environmental features. We\nobtain these behavioral features from actual traffic video footage in the city\nwith complete automatic processing. The proposed framework mainly analyzes\nthese behaviors in multidimensional perspectives by constructing a data cube\nstructure, which combines the LSTM based predictive collision risk estimation\nmodel and the on line analytical processing operations. From the PCR estimation\nmodel, we categorize the severity of risks as four levels and apply the\nproposed framework to assess the crosswalk safety with behavioral features. Our\nanalytic experiments are based on two scenarios, and the various descriptive\nresults are harvested the movement patterns of vehicles and pedestrians by road\nenvironment and the relationships between risk levels and car speeds. Thus, the\nproposed framework can support decision makers by providing valuable\ninformation to improve pedestrian safety for future accidents, and it can help\nus better understand their behaviors near crosswalks proactively. In order to\nconfirm the feasibility and applicability of the proposed framework, we\nimplement and apply it to actual operating CCTVs in Osan City, Korea.",
          "link": "http://arxiv.org/abs/2107.12507",
          "publishedOn": "2021-07-28T02:02:31.608Z",
          "wordCount": 734,
          "title": "Analyzing vehicle pedestrian interactions combining data cube structure and predictive collision risk estimation model. (arXiv:2107.12507v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thai-Son Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stueker_S/0/1/0/all/0/1\">Sebastian Stueker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>",
          "description": "Achieving super-human performance in recognizing human speech has been a goal\nfor several decades, as researchers have worked on increasingly challenging\ntasks. In the 1990's it was discovered, that conversational speech between two\nhumans turns out to be considerably more difficult than read speech as\nhesitations, disfluencies, false starts and sloppy articulation complicate\nacoustic processing and require robust handling of acoustic, lexical and\nlanguage context, jointly. Early attempts with statistical models could only\nreach error rates over 50% and far from human performance (WER of around 5.5%).\nNeural hybrid models and recent attention-based encoder-decoder models have\nconsiderably improved performance as such contexts can now be learned in an\nintegral fashion. However, processing such contexts requires an entire\nutterance presentation and thus introduces unwanted delays before a recognition\nresult can be output. In this paper, we address performance as well as latency.\nWe present results for a system that can achieve super-human performance (at a\nWER of 5.0%, over the Switchboard conversational benchmark) at a word based\nlatency of only 1 second behind a speaker's speech. The system uses multiple\nattention-based encoder-decoder networks integrated within a novel low latency\nincremental inference approach.",
          "link": "http://arxiv.org/abs/2010.03449",
          "publishedOn": "2021-07-28T02:02:31.599Z",
          "wordCount": 689,
          "title": "Super-Human Performance in Online Low-latency Recognition of Conversational Speech. (arXiv:2010.03449v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Secci_F/0/1/0/all/0/1\">Francesco Secci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1\">Andrea Ceccarelli</a>",
          "description": "RGB cameras are one of the most relevant sensors for autonomous driving\napplications. It is undeniable that failures of vehicle cameras may compromise\nthe autonomous driving task, possibly leading to unsafe behaviors when images\nthat are subsequently processed by the driving system are altered. To support\nthe definition of safe and robust vehicle architectures and intelligent\nsystems, in this paper we define the failure modes of a vehicle camera,\ntogether with an analysis of effects and known mitigations. Further, we build a\nsoftware library for the generation of the corresponding failed images and we\nfeed them to six object detectors for mono and stereo cameras and to the\nself-driving agent of an autonomous driving simulator. The resulting\nmisbehaviors with respect to operating with clean images allow a better\nunderstanding of failures effects and the related safety risks in image-based\napplications.",
          "link": "http://arxiv.org/abs/2008.05938",
          "publishedOn": "2021-07-28T02:02:31.581Z",
          "wordCount": 611,
          "title": "RGB cameras failures and their effects in autonomous driving applications. (arXiv:2008.05938v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08158",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gadermayr_M/0/1/0/all/0/1\">Michael Gadermayr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tschuchnig_M/0/1/0/all/0/1\">Maximilian Tschuchnig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stangassinger_L/0/1/0/all/0/1\">Lea Maria Stangassinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreutzer_C/0/1/0/all/0/1\">Christina Kreutzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Couillard_Despres_S/0/1/0/all/0/1\">Sebastien Couillard-Despres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oostingh_G/0/1/0/all/0/1\">Gertie Janneke Oostingh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hittmair_A/0/1/0/all/0/1\">Anton Hittmair</a>",
          "description": "In contrast to paraffin sections, frozen sections can be quickly generated\nduring surgical interventions. This procedure allows surgeons to wait for\nhistological findings during the intervention to base intra-operative decisions\non the outcome of the histology. However, compared to paraffin sections, the\nquality of frozen sections is typically lower, leading to a higher ratio of\nmiss-classification. In this work, we investigated the effect of the section\ntype on automated decision support approaches for classification of thyroid\ncancer. This was enabled by a data set consisting of pairs of sections for\nindividual patients. Moreover, we investigated, whether a frozen-to-paraffin\ntranslation could help to optimize classification scores. Finally, we propose a\nspecific data augmentation strategy to deal with a small amount of training\ndata and to increase classification accuracy even further.",
          "link": "http://arxiv.org/abs/2012.08158",
          "publishedOn": "2021-07-28T02:02:31.574Z",
          "wordCount": 622,
          "title": "Frozen-to-Paraffin: Categorization of Histological Frozen Sections by the Aid of Paraffin Sections and Generative Adversarial Networks. (arXiv:2012.08158v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sohee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungkyu Lee</a>",
          "description": "Continual learning is a concept of online learning with multiple sequential\ntasks. One of the critical barriers of continual learning is that a network\nshould learn a new task keeping the knowledge of old tasks without access to\nany data of the old tasks. In this paper, we propose a neuron activation\nimportance-based regularization method for stable continual learning regardless\nof the order of tasks. We conduct comprehensive experiments on existing\nbenchmark data sets to evaluate not just the stability and plasticity of our\nmethod with improved classification accuracy also the robustness of the\nperformance along the changes of task order.",
          "link": "http://arxiv.org/abs/2107.12657",
          "publishedOn": "2021-07-28T02:02:31.567Z",
          "wordCount": 529,
          "title": "Continual Learning with Neuron Activation Importance. (arXiv:2107.12657v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xia Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chunxia Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>",
          "description": "The combination of a small unmanned ground vehicle (UGV) and a large unmanned\ncarrier vehicle allows more flexibility in real applications such as rescue in\ndangerous scenarios. The autonomous recovery system, which is used to guide the\nsmall UGV back to the carrier vehicle, is an essential component to achieve a\nseamless combination of the two vehicles. This paper proposes a novel\nautonomous recovery framework with a low-cost monocular vision system to\nprovide accurate positioning and attitude estimation of the UGV during\nnavigation. First, we introduce a light-weight convolutional neural network\ncalled UGV-KPNet to detect the keypoints of the small UGV from the images\ncaptured by a monocular camera. UGV-KPNet is computationally efficient with a\nsmall number of parameters and provides pixel-level accurate keypoints\ndetection results in real-time. Then, six degrees of freedom pose is estimated\nusing the detected keypoints to obtain positioning and attitude information of\nthe UGV. Besides, we are the first to create a large-scale real-world keypoints\ndataset of the UGV. The experimental results demonstrate that the proposed\nsystem achieves state-of-the-art performance in terms of both accuracy and\nspeed on UGV keypoint detection, and can further boost the 6-DoF pose\nestimation for the UGV.",
          "link": "http://arxiv.org/abs/2107.12852",
          "publishedOn": "2021-07-28T02:02:31.560Z",
          "wordCount": 653,
          "title": "Real-time Keypoints Detection for Autonomous Recovery of the Unmanned Ground Vehicle. (arXiv:2107.12852v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benny_Y/0/1/0/all/0/1\">Yaniv Benny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pekar_N/0/1/0/all/0/1\">Niv Pekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "We consider the abstract relational reasoning task, which is commonly used as\nan intelligence test. Since some patterns have spatial rationales, while others\nare only semantic, we propose a multi-scale architecture that processes each\nquery in multiple resolutions. We show that indeed different rules are solved\nby different resolutions and a combined multi-scale approach outperforms the\nexisting state of the art in this task on all benchmarks by 5-54%. The success\nof our method is shown to arise from multiple novelties. First, it searches for\nrelational patterns in multiple resolutions, which allows it to readily detect\nvisual relations, such as location, in higher resolution, while allowing the\nlower resolution module to focus on semantic relations, such as shape type.\nSecond, we optimize the reasoning network of each resolution proportionally to\nits performance, hereby we motivate each resolution to specialize on the rules\nfor which it performs better than the others and ignore cases that are already\nsolved by the other resolutions. Third, we propose a new way to pool\ninformation along the rows and the columns of the illustration-grid of the\nquery. Our work also analyses the existing benchmarks, demonstrating that the\nRAVEN dataset selects the negative examples in a way that is easily exploited.\nWe, therefore, propose a modified version of the RAVEN dataset, named\nRAVEN-FAIR. Our code and pretrained models are available at\nhttps://github.com/yanivbenny/MRNet.",
          "link": "http://arxiv.org/abs/2009.09405",
          "publishedOn": "2021-07-28T02:02:31.553Z",
          "wordCount": 689,
          "title": "Scale-Localized Abstract Reasoning. (arXiv:2009.09405v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhaoyu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Pin Siang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_J/0/1/0/all/0/1\">Junkang Chow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jimmy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheong_Y/0/1/0/all/0/1\">Yehur Cheong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Hsing Wang</a>",
          "description": "3D point cloud interpretation is a challenging task due to the randomness and\nsparsity of the component points. Many of the recently proposed methods like\nPointNet and PointCNN have been focusing on learning shape descriptions from\npoint coordinates as point-wise input features, which usually involves\ncomplicated network architectures. In this work, we draw attention back to the\nstandard 3D convolutions towards an efficient 3D point cloud interpretation.\nInstead of converting the entire point cloud into voxel representations like\nthe other volumetric methods, we voxelize the sub-portions of the point cloud\nonly at necessary locations within each convolution layer on-the-fly, using our\ndynamic voxelization operation with self-adaptive voxelization resolution. In\naddition, we incorporate 3D group convolution into our dense convolution kernel\nimplementation to further exploit the rotation invariant features of point\ncloud. Benefiting from its simple fully-convolutional architecture, our network\nis able to run and converge at a considerably fast speed, while yields on-par\nor even better performance compared with the state-of-the-art methods on\nseveral benchmark datasets.",
          "link": "http://arxiv.org/abs/2009.02918",
          "publishedOn": "2021-07-28T02:02:31.534Z",
          "wordCount": 647,
          "title": "DV-ConvNet: Fully Convolutional Deep Learning on Point Clouds with Dynamic Voxelization and 3D Group Convolution. (arXiv:2009.02918v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabo_A/0/1/0/all/0/1\">Andrea Sabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdizadeh_S/0/1/0/all/0/1\">Sina Mehdizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iaboni_A/0/1/0/all/0/1\">Andrea Iaboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taati_B/0/1/0/all/0/1\">Babak Taati</a>",
          "description": "Drug-induced parkinsonism affects many older adults with dementia, often\ncausing gait disturbances. New advances in vision-based human pose-estimation\nhave opened possibilities for frequent and unobtrusive analysis of gait in\nresidential settings. This work proposes novel spatial-temporal graph\nconvolutional network (ST-GCN) architectures and training procedures to predict\nclinical scores of parkinsonism in gait from video of individuals with\ndementia. We propose a two-stage training approach consisting of a\nself-supervised pretraining stage that encourages the ST-GCN model to learn\nabout gait patterns before predicting clinical scores in the finetuning stage.\nThe proposed ST-GCN models are evaluated on joint trajectories extracted from\nvideo and are compared against traditional (ordinal, linear, random forest)\nregression models and temporal convolutional network baselines. Three 2D human\npose-estimation libraries (OpenPose, Detectron, AlphaPose) and the Microsoft\nKinect (2D and 3D) are used to extract joint trajectories of 4787 natural\nwalking bouts from 53 older adults with dementia. A subset of 399 walks from 14\nparticipants is annotated with scores of parkinsonism severity on the gait\ncriteria of the Unified Parkinson's Disease Rating Scale (UPDRS) and the\nSimpson-Angus Scale (SAS). Our results demonstrate that ST-GCN models operating\non 3D joint trajectories extracted from the Kinect consistently outperform all\nother models and feature sets. Prediction of parkinsonism scores in natural\nwalking bouts of unseen participants remains a challenging task, with the best\nmodels achieving macro-averaged F1-scores of 0.53 +/- 0.03 and 0.40 +/- 0.02\nfor UPDRS-gait and SAS-gait, respectively. Pre-trained model and demo code for\nthis work is available:\nhttps://github.com/TaatiTeam/stgcn_parkinsonism_prediction.",
          "link": "http://arxiv.org/abs/2105.03464",
          "publishedOn": "2021-07-28T02:02:31.527Z",
          "wordCount": 723,
          "title": "Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults with Dementia. (arXiv:2105.03464v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-07-28T02:02:31.518Z",
          "wordCount": 654,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zefeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhiyin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Text-to-image person re-identification (ReID) aims to search for images\ncontaining a person of interest using textual descriptions. However, due to the\nsignificant modality gap and the large intra-class variance in textual\ndescriptions, text-to-image ReID remains a challenging problem. Accordingly, in\nthis paper, we propose a Semantically Self-Aligned Network (SSAN) to handle the\nabove problems. First, we propose a novel method that automatically extracts\nsemantically aligned part-level features from the two modalities. Second, we\ndesign a multi-view non-local network that captures the relationships between\nbody parts, thereby establishing better correspondences between body parts and\nnoun phrases. Third, we introduce a Compound Ranking (CR) loss that makes use\nof textual descriptions for other images of the same identity to provide extra\nsupervision, thereby effectively reducing the intra-class variance in textual\nfeatures. Finally, to expedite future research in text-to-image ReID, we build\na new database named ICFG-PEDES. Extensive experiments demonstrate that SSAN\noutperforms state-of-the-art approaches by significant margins. Both the new\nICFG-PEDES database and the SSAN code are available at\nhttps://github.com/zifyloo/SSAN.",
          "link": "http://arxiv.org/abs/2107.12666",
          "publishedOn": "2021-07-28T02:02:31.511Z",
          "wordCount": 616,
          "title": "Semantically Self-Aligned Network for Text-to-Image Part-aware Person Re-identification. (arXiv:2107.12666v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shi-Xue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>",
          "description": "Arbitrary shape text detection is a challenging task due to the high\ncomplexity and variety of scene texts. In this work, we propose a novel\nadaptive boundary proposal network for arbitrary shape text detection, which\ncan learn to directly produce accurate boundary for arbitrary shape text\nwithout any post-processing. Our method mainly consists of a boundary proposal\nmodel and an innovative adaptive boundary deformation model. The boundary\nproposal model constructed by multi-layer dilated convolutions is adopted to\nproduce prior information (including classification map, distance field, and\ndirection field) and coarse boundary proposals. The adaptive boundary\ndeformation model is an encoder-decoder network, in which the encoder mainly\nconsists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network\n(RNN). It aims to perform boundary deformation in an iterative way for\nobtaining text instance shape guided by prior information from the boundary\nproposal model.In this way, our method can directly and efficiently generate\naccurate text boundaries without complex post-processing. Extensive experiments\non publicly available datasets demonstrate the state-of-the-art performance of\nour method.",
          "link": "http://arxiv.org/abs/2107.12664",
          "publishedOn": "2021-07-28T02:02:31.489Z",
          "wordCount": 633,
          "title": "Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection. (arXiv:2107.12664v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>",
          "description": "Localizing individuals in crowds is more in accordance with the practical\ndemands of subsequent high-level crowd analysis tasks than simply counting.\nHowever, existing localization based methods relying on intermediate\nrepresentations (\\textit{i.e.}, density maps or pseudo boxes) serving as\nlearning targets are counter-intuitive and error-prone. In this paper, we\npropose a purely point-based framework for joint crowd counting and individual\nlocalization. For this framework, instead of merely reporting the absolute\ncounting error at image level, we propose a new metric, called density\nNormalized Average Precision (nAP), to provide more comprehensive and more\nprecise performance evaluation. Moreover, we design an intuitive solution under\nthis framework, which is called Point to Point Network (P2PNet). P2PNet\ndiscards superfluous steps and directly predicts a set of point proposals to\nrepresent heads in an image, being consistent with the human annotation\nresults. By thorough analysis, we reveal the key step towards implementing such\na novel idea is to assign optimal learning targets for these proposals.\nTherefore, we propose to conduct this crucial association in an one-to-one\nmatching manner using the Hungarian algorithm. The P2PNet not only\nsignificantly surpasses state-of-the-art methods on popular counting\nbenchmarks, but also achieves promising localization accuracy. The codes will\nbe available at: https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet.",
          "link": "http://arxiv.org/abs/2107.12746",
          "publishedOn": "2021-07-28T02:02:31.462Z",
          "wordCount": 658,
          "title": "Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework. (arXiv:2107.12746v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiayu Cao</a>",
          "description": "Agriculture is an essential industry in the both society and economy of a\ncountry. However, the pests and diseases cause a great amount of reduction in\nagricultural production while there is not sufficient guidance for farmers to\navoid this disaster. To address this problem, we apply CNNs to plant disease\nrecognition by building a classification model. Within the dataset of 3,642\nimages of apple leaves, We use a pre-trained image classification model\nRestnet34 based on a Convolutional neural network (CNN) with the Fastai\nframework in order to save the training time. Overall, the accuracy of\nclassification is 93.765%.",
          "link": "http://arxiv.org/abs/2107.12598",
          "publishedOn": "2021-07-28T02:02:31.454Z",
          "wordCount": 535,
          "title": "Identify Apple Leaf Diseases Using Deep Learning Algorithm. (arXiv:2107.12598v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gudovskiy_D/0/1/0/all/0/1\">Denis Gudovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishizaka_S/0/1/0/all/0/1\">Shun Ishizaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1\">Kazuki Kozuka</a>",
          "description": "Unsupervised anomaly detection with localization has many practical\napplications when labeling is infeasible and, moreover, when anomaly examples\nare completely missing in the train data. While recently proposed models for\nsuch data setup achieve high accuracy metrics, their complexity is a limiting\nfactor for real-time processing. In this paper, we propose a real-time model\nand analytically derive its relationship to prior methods. Our CFLOW-AD model\nis based on a conditional normalizing flow framework adopted for anomaly\ndetection with localization. In particular, CFLOW-AD consists of a\ndiscriminatively pretrained encoder followed by a multi-scale generative\ndecoders where the latter explicitly estimate likelihood of the encoded\nfeatures. Our approach results in a computationally and memory-efficient model:\nCFLOW-AD is faster and smaller by a factor of 10x than prior state-of-the-art\nwith the same input setting. Our experiments on the MVTec dataset show that\nCFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by\n1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source\nour code with fully reproducible experiments.",
          "link": "http://arxiv.org/abs/2107.12571",
          "publishedOn": "2021-07-28T02:02:31.447Z",
          "wordCount": 620,
          "title": "CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows. (arXiv:2107.12571v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zixin Zhu</a> (Xi&#x27;an jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a> (University of Illinois at Chicago), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a> (Xi&#x27;an Jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a> (Xi&#x27;an Jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a> (Wormpex AI Research)",
          "description": "Effectively tackling the problem of temporal action localization (TAL)\nnecessitates a visual representation that jointly pursues two confounding\ngoals, i.e., fine-grained discrimination for temporal localization and\nsufficient visual invariance for action classification. We address this\nchallenge by enriching both the local and global contexts in the popular\ntwo-stage temporal localization framework, where action proposals are first\ngenerated followed by action classification and temporal boundary regression.\nOur proposed model, dubbed ContextLoc, can be divided into three sub-networks:\nL-Net, G-Net and P-Net. L-Net enriches the local context via fine-grained\nmodeling of snippet-level features, which is formulated as a\nquery-and-retrieval process. G-Net enriches the global context via higher-level\nmodeling of the video-level representation. In addition, we introduce a novel\ncontext adaptation module to adapt the global context to different proposals.\nP-Net further models the context-aware inter-proposal relations. We explore two\nexisting models to be the P-Net in our experiments. The efficacy of our\nproposed method is validated by experimental results on the THUMOS14 (54.3\\% at\nIoU@0.5) and ActivityNet v1.3 (51.24\\% at IoU@0.5) datasets, which outperforms\nrecent states of the art.",
          "link": "http://arxiv.org/abs/2107.12960",
          "publishedOn": "2021-07-28T02:02:31.433Z",
          "wordCount": 639,
          "title": "Enriching Local and Global Contexts for Temporal Action Localization. (arXiv:2107.12960v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Riqiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mirza S. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaiwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deppen_S/0/1/0/all/0/1\">Steve Deppen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandler_K/0/1/0/all/0/1\">Kim L. Sandler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massion_P/0/1/0/all/0/1\">Pierre P. Massion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>",
          "description": "Image Quality Assessment (IQA) is important for scientific inquiry,\nespecially in medical imaging and machine learning. Potential data quality\nissues can be exacerbated when human-based workflows use limited views of the\ndata that may obscure digital artifacts. In practice, multiple factors such as\nnetwork issues, accelerated acquisitions, motion artifacts, and imaging\nprotocol design can impede the interpretation of image collections. The medical\nimage processing community has developed a wide variety of tools for the\ninspection and validation of imaging data. Yet, IQA of computed tomography (CT)\nremains an under-recognized challenge, and no user-friendly tool is commonly\navailable to address these potential issues. Here, we create and illustrate a\npipeline specifically designed to identify and resolve issues encountered with\nlarge-scale data mining of clinically acquired CT data. Using the widely\nstudied National Lung Screening Trial (NLST), we have identified approximately\n4% of image volumes with quality concerns out of 17,392 scans. To assess\nrobustness, we applied the proposed pipeline to our internal datasets where we\nfind our tool is generalizable to clinically acquired medical images. In\nconclusion, the tool has been useful and time-saving for research study of\nclinical data, and the code and tutorials are publicly available at\nhttps://github.com/MASILab/QA_tool.",
          "link": "http://arxiv.org/abs/2107.12842",
          "publishedOn": "2021-07-28T02:02:31.423Z",
          "wordCount": 662,
          "title": "Technical Report: Quality Assessment Tool for Machine Learning with Clinical CT. (arXiv:2107.12842v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sungmin Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dogyoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junhyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sangwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Woojin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>",
          "description": "Despite the remarkable success of deep learning, optimal convolution\noperation on point cloud remains indefinite due to its irregular data\nstructure. In this paper, we present Cubic Kernel Convolution (CKConv) that\nlearns to voxelize the features of local points by exploiting both continuous\nand discrete convolutions. Our continuous convolution uniquely employs a 3D\ncubic form of kernel weight representation that splits a feature into voxels in\nembedding space. By consecutively applying discrete 3D convolutions on the\nvoxelized features in a spatial manner, preceding continuous convolution is\nforced to learn spatial feature mapping, i.e., feature voxelization. In this\nway, geometric information can be detailed by encoding with subdivided\nfeatures, and our 3D convolutions on these fixed structured data do not suffer\nfrom discretization artifacts thanks to voxelization in embedding space.\nFurthermore, we propose a spatial attention module, Local Set Attention (LSA),\nto provide comprehensive structure awareness within the local point set and\nhence produce representative features. By learning feature voxelization with\nLSA, CKConv can extract enriched features for effective point cloud analysis.\nWe show that CKConv has great applicability to point cloud processing tasks\nincluding object classification, object part segmentation, and scene semantic\nsegmentation with state-of-the-art results.",
          "link": "http://arxiv.org/abs/2107.12655",
          "publishedOn": "2021-07-28T02:02:31.412Z",
          "wordCount": 636,
          "title": "CKConv: Learning Feature Voxelization for Point Cloud Analysis. (arXiv:2107.12655v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1\">Alessio Xompero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donaher_S/0/1/0/all/0/1\">Santiago Donaher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iashin_V/0/1/0/all/0/1\">Vladimir Iashin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palermo_F/0/1/0/all/0/1\">Francesca Palermo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solak_G/0/1/0/all/0/1\">G&#xf6;khan Solak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppola_C/0/1/0/all/0/1\">Claudio Coppola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_R/0/1/0/all/0/1\">Reina Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagao_Y/0/1/0/all/0/1\">Yuichi Nagao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1\">Ryo Hachiuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chuanlin Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Rosa H. M. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christmann_G/0/1/0/all/0/1\">Guilherme Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jyun-Ting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neeharika_G/0/1/0/all/0/1\">Gonuguntla Neeharika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chinnakotla Krishna Teja Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Dinesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehman_B/0/1/0/all/0/1\">Bakhtawar Ur Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>",
          "description": "Acoustic and visual sensing can support the contactless estimation of the\nweight of a container and the amount of its content when the container is\nmanipulated by a person. However, transparencies (both of the container and of\nthe content) and the variability of materials, shapes and sizes make this\nproblem challenging. In this paper, we present an open benchmarking framework\nand an in-depth comparative analysis of recent methods that estimate the\ncapacity of a container, as well as the type, mass, and amount of its content.\nThese methods use learned and handcrafted features, such as mel-frequency\ncepstrum coefficients, zero-crossing rate, spectrograms, with different types\nof classifiers to estimate the type and amount of the content with acoustic\ndata, and geometric approaches with visual data to determine the capacity of\nthe container. Results on a newly distributed dataset show that audio alone is\na strong modality and methods achieves a weighted average F1-score up to 81%\nand 97% for content type and level classification, respectively. Estimating the\ncontainer capacity with vision-only approaches and filling mass with\nmulti-modal, multi-stage algorithms reaches up to 65% weighted average capacity\nand mass scores.",
          "link": "http://arxiv.org/abs/2107.12719",
          "publishedOn": "2021-07-28T02:02:31.386Z",
          "wordCount": 692,
          "title": "Multi-modal estimation of the properties of containers and their content: survey and evaluation. (arXiv:2107.12719v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>",
          "description": "Image manipulation with natural language, which aims to manipulate images\nwith the guidance of language descriptions, has been a challenging problem in\nthe fields of computer vision and natural language processing (NLP). Currently,\na number of efforts have been made for this task, but their performances are\nstill distant away from generating realistic and text-conformed manipulated\nimages. Therefore, in this paper, we propose a memory-based Image Manipulation\nNetwork (MIM-Net), where a set of memories learned from images is introduced to\nsynthesize the texture information with the guidance of the textual\ndescription. We propose a two-stage network with an additional reconstruction\nstage to learn the latent memories efficiently. To avoid the unnecessary\nbackground changes, we propose a Target Localization Unit (TLU) to focus on the\nmanipulation of the region mentioned by the text. Moreover, to learn a robust\nmemory, we further propose a novel randomized memory training loss. Experiments\non the four popular datasets show the better performance of our method compared\nto the existing ones.",
          "link": "http://arxiv.org/abs/2107.12579",
          "publishedOn": "2021-07-28T02:02:31.377Z",
          "wordCount": 607,
          "title": "Remember What You have drawn: Semantic Image Manipulation with Memory. (arXiv:2107.12579v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tong Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Confidence calibration is of great importance to the reliability of decisions\nmade by machine learning systems. However, discriminative classifiers based on\ndeep neural networks are often criticized for producing overconfident\npredictions that fail to reflect the true correctness likelihood of\nclassification accuracy. We argue that such an inability to model uncertainty\nis mainly caused by the closed-world nature in softmax: a model trained by the\ncross-entropy loss will be forced to classify input into one of $K$ pre-defined\ncategories with high probability. To address this problem, we for the first\ntime propose a novel $K$+1-way softmax formulation, which incorporates the\nmodeling of open-world uncertainty as the extra dimension. To unify the\nlearning of the original $K$-way classification task and the extra dimension\nthat models uncertainty, we propose a novel energy-based objective function,\nand moreover, theoretically prove that optimizing such an objective essentially\nforces the extra dimension to capture the marginal data distribution. Extensive\nexperiments show that our approach, Energy-based Open-World Softmax\n(EOW-Softmax), is superior to existing state-of-the-art methods in improving\nconfidence calibration.",
          "link": "http://arxiv.org/abs/2107.12628",
          "publishedOn": "2021-07-28T02:02:31.369Z",
          "wordCount": 614,
          "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration. (arXiv:2107.12628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huyan_N/0/1/0/all/0/1\">Ning Huyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_D/0/1/0/all/0/1\">Dou Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xuefeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>",
          "description": "Outlier detection is one of the most important processes taken to create\ngood, reliable data in machine learning. The most methods of outlier detection\nleverage an auxiliary reconstruction task by assuming that outliers are more\ndifficult to be recovered than normal samples (inliers). However, it is not\nalways true, especially for auto-encoder (AE) based models. They may recover\ncertain outliers even outliers are not in the training data, because they do\nnot constrain the feature learning. Instead, we think outlier detection can be\ndone in the feature space by measuring the feature distance between outliers\nand inliers. We then propose a framework, MCOD, using a memory module and a\ncontrastive learning module. The memory module constrains the consistency of\nfeatures, which represent the normal data. The contrastive learning module\nlearns more discriminating features, which boosts the distinction between\noutliers and inliers. Extensive experiments on four benchmark datasets show\nthat our proposed MCOD achieves a considerable performance and outperforms nine\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.12642",
          "publishedOn": "2021-07-28T02:02:31.350Z",
          "wordCount": 601,
          "title": "Unsupervised Outlier Detection using Memory and Contrastive Learning. (arXiv:2107.12642v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.",
          "link": "http://arxiv.org/abs/2107.12636",
          "publishedOn": "2021-07-28T02:02:31.341Z",
          "wordCount": 693,
          "title": "Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turkoz_E/0/1/0/all/0/1\">Erkin T&#xfc;rk&#xf6;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olcay_E/0/1/0/all/0/1\">Ertug Olcay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oksanen_T/0/1/0/all/0/1\">Timo Oksanen</a>",
          "description": "This paper proposes a concept of computer vision-based guidance assistance\nfor agricultural vehicles to increase the accuracy in plowing and reduce\ndriver's cognitive burden in long-lasting tillage operations. Plowing is a\ncommon agricultural practice to prepare the soil for planting in many countries\nand it can take place both in the spring and the fall. Since plowing operation\nrequires high traction forces, it causes increased energy consumption.\nMoreover, longer operation time due to unnecessary maneuvers leads to higher\nfuel consumption. To provide necessary information for the driver and the\ncontrol unit of the tractor, a first concept of furrow detection system based\non an RGB-D camera was developed.",
          "link": "http://arxiv.org/abs/2107.12646",
          "publishedOn": "2021-07-28T02:02:31.318Z",
          "wordCount": 566,
          "title": "Computer Vision-Based Guidance Assistance Concept for Plowing Using RGB-D Camera. (arXiv:2107.12646v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompella_R/0/1/0/all/0/1\">Ramana Kompella</a>",
          "description": "Deep Neural Network (DNN) trained object detectors are widely deployed in\nmany mission-critical systems for real time video analytics at the edge, such\nas autonomous driving and video surveillance. A common performance requirement\nin these mission-critical edge services is the near real-time latency of online\nobject detection on edge devices. However, even with well-trained DNN object\ndetectors, the online detection quality at edge may deteriorate for a number of\nreasons, such as limited capacity to run DNN object detection models on\nheterogeneous edge devices, and detection quality degradation due to random\nframe dropping when the detection processing rate is significantly slower than\nthe incoming video frame rate. This paper addresses these problems by\nexploiting multi-model multi-device detection parallelism for fast object\ndetection in edge systems with heterogeneous edge devices. First, we analyze\nthe performance bottleneck of running a well-trained DNN model at edge for real\ntime online object detection. We use the offline detection as a reference\nmodel, and examine the root cause by analyzing the mismatch among the incoming\nvideo streaming rate, video processing rate for object detection, and output\nrate for real time detection visualization of video streaming. Second, we study\nperformance optimizations by exploiting multi-model detection parallelism. We\nshow that the model-parallel detection approach can effectively speed up the\nFPS detection processing rate, minimizing the FPS disparity with the incoming\nvideo frame rate on heterogeneous edge devices. We evaluate the proposed\napproach using SSD300 and YOLOv3 on benchmark videos of different video stream\nrates. The results show that exploiting multi-model detection parallelism can\nspeed up the online object detection processing rate and deliver near real-time\nobject detection performance for efficient video analytics at edge.",
          "link": "http://arxiv.org/abs/2107.12563",
          "publishedOn": "2021-07-28T02:02:31.304Z",
          "wordCount": 720,
          "title": "Parallel Detection for Efficient Video Analytics at the Edge. (arXiv:2107.12563v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1\">Qi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_R/0/1/0/all/0/1\">Runmin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_R/0/1/0/all/0/1\">Ronghui Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lingzhi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>",
          "description": "Depth map super-resolution is a task with high practical application\nrequirements in the industry. Existing color-guided depth map super-resolution\nmethods usually necessitate an extra branch to extract high-frequency detail\ninformation from RGB image to guide the low-resolution depth map\nreconstruction. However, because there are still some differences between the\ntwo modalities, direct information transmission in the feature dimension or\nedge map dimension cannot achieve satisfactory result, and may even trigger\ntexture copying in areas where the structures of the RGB-D pair are\ninconsistent. Inspired by the multi-task learning, we propose a joint learning\nnetwork of depth map super-resolution (DSR) and monocular depth estimation\n(MDE) without introducing additional supervision labels. For the interaction of\ntwo subnetworks, we adopt a differentiated guidance strategy and design two\nbridges correspondingly. One is the high-frequency attention bridge (HABdg)\ndesigned for the feature encoding process, which learns the high-frequency\ninformation of the MDE task to guide the DSR task. The other is the content\nguidance bridge (CGBdg) designed for the depth map reconstruction process,\nwhich provides the content guidance learned from DSR task for MDE task. The\nentire network architecture is highly portable and can provide a paradigm for\nassociating the DSR and MDE tasks. Extensive experiments on benchmark datasets\ndemonstrate that our method achieves competitive performance. Our code and\nmodels are available at https://rmcong.github.io/proj_BridgeNet.html.",
          "link": "http://arxiv.org/abs/2107.12541",
          "publishedOn": "2021-07-28T02:02:31.295Z",
          "wordCount": 682,
          "title": "BridgeNet: A Joint Learning Network of Depth Map Super-Resolution and Monocular Depth Estimation. (arXiv:2107.12541v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Song Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrich_N/0/1/0/all/0/1\">Norman Hendrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1\">Fanyu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shuzhi Sam Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianwei Zhang</a>",
          "description": "In the classic setting of unsupervised domain adaptation (UDA), the labeled\nsource data are available in the training phase. However, in many real-world\nscenarios, owing to some reasons such as privacy protection and information\nsecurity, the source data is inaccessible, and only a model trained on the\nsource domain is available. This paper proposes a novel deep clustering method\nfor this challenging task. Aiming at the dynamical clustering at feature-level,\nwe introduce extra constraints hidden in the geometric structure between data\nto assist the process. Concretely, we propose a geometry-based constraint,\nnamed semantic consistency on the nearest neighborhood (SCNNH), and use it to\nencourage robust clustering. To reach this goal, we construct the nearest\nneighborhood for every target data and take it as the fundamental clustering\nunit by building our objective on the geometry. Also, we develop a more\nSCNNH-compliant structure with an additional semantic credibility constraint,\nnamed semantic hyper-nearest neighborhood (SHNNH). After that, we extend our\nmethod to this new geometry. Extensive experiments on three challenging UDA\ndatasets indicate that our method achieves state-of-the-art results. The\nproposed method has significant improvement on all datasets (as we adopt SHNNH,\nthe average accuracy increases by over 3.0\\% on the large-scaled dataset). Code\nis available at https://github.com/tntek/N2DCX.",
          "link": "http://arxiv.org/abs/2107.12585",
          "publishedOn": "2021-07-28T02:02:31.288Z",
          "wordCount": 660,
          "title": "Nearest Neighborhood-Based Deep Clustering for Source Data-absent Unsupervised Domain Adaptation. (arXiv:2107.12585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junnan Liu</a>",
          "description": "Effective fusion of different types of features is the key to salient object\ndetection. The majority of existing network structure design is based on the\nsubjective experience of scholars and the process of feature fusion does not\nconsider the relationship between the fused features and highest-level\nfeatures. In this paper, we focus on the feature relationship and propose a\nnovel global attention unit, which we term the \"perception- and-regulation\"\n(PR) block, that adaptively regulates the feature fusion process by explicitly\nmodeling interdependencies between features. The perception part uses the\nstructure of fully-connected layers in classification networks to learn the\nsize and shape of objects. The regulation part selectively strengthens and\nweakens the features to be fused. An imitating eye observation module (IEO) is\nfurther employed for improving the global perception ability of the network.\nThe imitation of foveal vision and peripheral vision enables IEO to scrutinize\nhighly detailed objects and to organize the broad spatial scene to better\nsegment objects. Sufficient experiments conducted on SOD datasets demonstrate\nthat the proposed method performs favorably against 22 state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2107.12560",
          "publishedOn": "2021-07-28T02:02:31.238Z",
          "wordCount": 609,
          "title": "Perception-and-Regulation Network for Salient Object Detection. (arXiv:2107.12560v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santamaria_Pang_A/0/1/0/all/0/1\">Alberto Santamaria-Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianwei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Aritra Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubricht_J/0/1/0/all/0/1\">James Kubricht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1\">Peter Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naresh_I/0/1/0/all/0/1\">Iyer Naresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virani_N/0/1/0/all/0/1\">Nurali Virani</a>",
          "description": "We propose a novel framework for real-time black-box universal attacks which\ndisrupts activations of early convolutional layers in deep learning models. Our\nhypothesis is that perturbations produced in the wavelet space disrupt early\nconvolutional layers more effectively than perturbations performed in the time\ndomain. The main challenge in adversarial attacks is to preserve low frequency\nimage content while minimally changing the most meaningful high frequency\ncontent. To address this, we formulate an optimization problem using time-scale\n(wavelet) representations as a dual space in three steps. First, we project\noriginal images into orthonormal sub-spaces for low and high scales via wavelet\ncoefficients. Second, we perturb wavelet coefficients for high scale projection\nusing a generator network. Third, we generate new adversarial images by\nprojecting back the original coefficients from the low scale and the perturbed\ncoefficients from the high scale sub-space. We provide a theoretical framework\nthat guarantees a dual mapping from time and time-scale domain representations.\nWe compare our results with state-of-the-art black-box attacks from\ngenerative-based and gradient-based models. We also verify efficacy against\nmultiple defense methods such as JPEG compression, Guided Denoiser and\nComdefend. Our results show that wavelet-based perturbations consistently\noutperform time-based attacks thus providing new insights into vulnerabilities\nof deep learning models and could potentially lead to robust architectures or\nnew defense and attack mechanisms by leveraging time-scale representations.",
          "link": "http://arxiv.org/abs/2107.12473",
          "publishedOn": "2021-07-28T02:02:31.230Z",
          "wordCount": 665,
          "title": "Adversarial Attacks with Time-Scale Representations. (arXiv:2107.12473v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>",
          "description": "Self-supervised depth estimation for indoor environments is more challenging\nthan its outdoor counterpart in at least the following two aspects: (i) the\ndepth range of indoor sequences varies a lot across different frames, making it\ndifficult for the depth network to induce consistent depth cues, whereas the\nmaximum distance in outdoor scenes mostly stays the same as the camera usually\nsees the sky; (ii) the indoor sequences contain much more rotational motions,\nwhich cause difficulties for the pose network, while the motions of outdoor\nsequences are pre-dominantly translational, especially for driving datasets\nsuch as KITTI. In this paper, special considerations are given to those\nchallenges and a set of good practices are consolidated for improving the\nperformance of self-supervised monocular depth estimation in indoor\nenvironments. The proposed method mainly consists of two novel modules, \\ie, a\ndepth factorization module and a residual pose estimation module, each of which\nis designed to respectively tackle the aforementioned challenges. The\neffectiveness of each module is shown through a carefully conducted ablation\nstudy and the demonstration of the state-of-the-art performance on two indoor\ndatasets, \\ie, EuRoC and NYUv2.",
          "link": "http://arxiv.org/abs/2107.12429",
          "publishedOn": "2021-07-28T02:02:31.221Z",
          "wordCount": 632,
          "title": "MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments. (arXiv:2107.12429v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1\">Bo Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>",
          "description": "We propose a self-supervised spatio-temporal matching method coined\nMotion-Aware Mask Propagation (MAMP) for semi-supervised video object\nsegmentation. During training, MAMP leverages the frame reconstruction task to\ntrain the model without the need for annotations. During inference, MAMP\nextracts high-resolution features from each frame to build a memory bank from\nthe features as well as the predicted masks of selected past frames. MAMP then\npropagates the masks from the memory bank to subsequent frames according to our\nmotion-aware spatio-temporal matching module, also proposed in this paper.\nEvaluation on DAVIS-2017 and YouTube-VOS datasets show that MAMP achieves\nstate-of-the-art performance with stronger generalization ability compared to\nexisting self-supervised methods, i.e. 4.9\\% higher mean\n$\\mathcal{J}\\&\\mathcal{F}$ on DAVIS-2017 and 4.85\\% higher mean\n$\\mathcal{J}\\&\\mathcal{F}$ on the unseen categories of YouTube-VOS than the\nnearest competitor. Moreover, MAMP performs on par with many supervised video\nobject segmentation methods. Our code is available at:\n\\url{https://github.com/bo-miao/MAMP}.",
          "link": "http://arxiv.org/abs/2107.12569",
          "publishedOn": "2021-07-28T02:02:31.204Z",
          "wordCount": 588,
          "title": "Self-Supervised Video Object Segmentation by Motion-Aware Mask Propagation. (arXiv:2107.12569v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azari_B/0/1/0/all/0/1\">Bahar Azari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1\">Deniz Erdogmus</a>",
          "description": "Despite the vast success of standard planar convolutional neural networks,\nthey are not the most efficient choice for analyzing signals that lie on an\narbitrarily curved manifold, such as a cylinder. The problem arises when one\nperforms a planar projection of these signals and inevitably causes them to be\ndistorted or broken where there is valuable information. We propose a\nCircular-symmetric Correlation Layer (CCL) based on the formalism of\nroto-translation equivariant correlation on the continuous group $S^1 \\times\n\\mathbb{R}$, and implement it efficiently using the well-known Fast Fourier\nTransform (FFT) algorithm. We showcase the performance analysis of a general\nnetwork equipped with CCL on various recognition and classification tasks and\ndatasets. The PyTorch package implementation of CCL is provided online.",
          "link": "http://arxiv.org/abs/2107.12480",
          "publishedOn": "2021-07-28T02:02:30.923Z",
          "wordCount": 552,
          "title": "Circular-Symmetric Correlation Layer based on FFT. (arXiv:2107.12480v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" When viewing\nmice on the shelf, the number of buttons or presence of a wire may not be\nvisible from certain angles or positions. Flat images of candidate mice may not\nprovide the discriminative information needed for \"wireless\". The world, and\nobjects in it, are not flat images but complex 3D shapes. If a human requests\nan object based on any of its basic properties, such as color, shape, or\ntexture, robots should perform the necessary exploration to accomplish the\ntask. In particular, while substantial effort and progress has been made on\nunderstanding explicitly visual attributes like color and category,\ncomparatively little progress has been made on understanding language about\nshapes and contours. In this work, we introduce a novel reasoning task that\ntargets both visual and non-visual language about 3D objects. Our new\nbenchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a\nmodel to choose which of two objects is being referenced by a natural language\ndescription. We introduce several CLIP-based models for distinguishing objects\nand demonstrate that while recent advances in jointly modeling vision and\nlanguage are useful for robotic language understanding, it is still the case\nthat these models are weaker at understanding the 3D nature of objects --\nproperties which play a key role in manipulation. In particular, we find that\nadding view estimation to language grounding models improves accuracy on both\nSNARE and when identifying objects referred to in language on a robot platform.",
          "link": "http://arxiv.org/abs/2107.12514",
          "publishedOn": "2021-07-28T02:02:30.915Z",
          "wordCount": 709,
          "title": "Language Grounding with 3D Objects. (arXiv:2107.12514v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Haisheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_P/0/1/0/all/0/1\">Peiqin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yukun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Weihao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>",
          "description": "This technical report presents an overview of our solution used in the\nsubmission to 2021 HACS Temporal Action Localization Challenge on both\nSupervised Learning Track and Weakly-Supervised Learning Track. Temporal Action\nLocalization (TAL) requires to not only precisely locate the temporal\nboundaries of action instances, but also accurately classify the untrimmed\nvideos into specific categories. However, Weakly-Supervised TAL indicates\nlocating the action instances using only video-level class labels. In this\npaper, to train a supervised temporal action localizer, we adopt Temporal\nContext Aggregation Network (TCANet) to generate high-quality action proposals\nthrough ``local and global\" temporal context aggregation and complementary as\nwell as progressive boundary refinement. As for the WSTAL, a novel framework is\nproposed to handle the poor quality of CAS generated by simple classification\nnetwork, which can only focus on local discriminative parts, rather than locate\nthe entire interval of target actions. Further inspired by the transfer\nlearning method, we also adopt an additional module to transfer the knowledge\nfrom trimmed videos (HACS Clips dataset) to untrimmed videos (HACS Segments\ndataset), aiming at promoting the classification performance on untrimmed\nvideos. Finally, we employ a boundary regression module embedded with\nOuter-Inner-Contrastive (OIC) loss to automatically predict the boundaries\nbased on the enhanced CAS. Our proposed scheme achieves 39.91 and 29.78 average\nmAP on the challenge testing set of supervised and weakly-supervised temporal\naction localization track respectively.",
          "link": "http://arxiv.org/abs/2107.12618",
          "publishedOn": "2021-07-28T02:02:30.805Z",
          "wordCount": 699,
          "title": "Transferable Knowledge-Based Multi-Granularity Aggregation Network for Temporal Action Localization: Submission to ActivityNet Challenge 2021. (arXiv:2107.12618v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yilin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "6D pose estimation of rigid objects from a single RGB image has seen\ntremendous improvements recently by using deep learning to combat complex\nreal-world variations, but a majority of methods build models on the per-object\nlevel, failing to scale to multiple objects simultaneously. In this paper, we\npresent a novel approach for scalable 6D pose estimation, by self-supervised\nlearning on synthetic data of multiple objects using a single autoencoder. To\nhandle multiple objects and generalize to unseen objects, we disentangle the\nlatent object shape and pose representations, so that the latent shape space\nmodels shape similarities, and the latent pose code is used for rotation\nretrieval by comparison with canonical rotations. To encourage shape space\nconstruction, we apply contrastive metric learning and enable the processing of\nunseen objects by referring to similar training objects. The different\nsymmetries across objects induce inconsistent latent pose spaces, which we\ncapture with a conditioned block producing shape-dependent pose codebooks by\nre-entangling shape and pose representations. We test our method on two\nmulti-object benchmarks with real data, T-LESS and NOCS REAL275, and show it\noutperforms existing RGB-based methods in terms of pose estimation accuracy and\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.12549",
          "publishedOn": "2021-07-28T02:02:30.796Z",
          "wordCount": 639,
          "title": "Disentangled Implicit Shape and Pose Learning for Scalable 6D Pose Estimation. (arXiv:2107.12549v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Miao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1\">Yang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Siyu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>",
          "description": "Advanced tensor decomposition, such as Tensor train (TT) and Tensor ring\n(TR), has been widely studied for deep neural network (DNN) model compression,\nespecially for recurrent neural networks (RNNs). However, compressing\nconvolutional neural networks (CNNs) using TT/TR always suffers significant\naccuracy loss. In this paper, we propose a systematic framework for tensor\ndecomposition-based model compression using Alternating Direction Method of\nMultipliers (ADMM). By formulating TT decomposition-based model compression to\nan optimization problem with constraints on tensor ranks, we leverage ADMM\ntechnique to systemically solve this optimization problem in an iterative way.\nDuring this procedure, the entire DNN model is trained in the original\nstructure instead of TT format, but gradually enjoys the desired low tensor\nrank characteristics. We then decompose this uncompressed model to TT format\nand fine-tune it to finally obtain a high-accuracy TT-format DNN model. Our\nframework is very general, and it works for both CNNs and RNNs, and can be\neasily modified to fit other tensor decomposition approaches. We evaluate our\nproposed framework on different DNN models for image classification and video\nrecognition tasks. Experimental results show that our ADMM-based TT-format\nmodels demonstrate very high compression performance with high accuracy.\nNotably, on CIFAR-100, with 2.3X and 2.4X compression ratios, our models have\n1.96% and 2.21% higher top-1 accuracy than the original ResNet-20 and\nResNet-32, respectively. For compressing ResNet-18 on ImageNet, our model\nachieves 2.47X FLOPs reduction without accuracy loss.",
          "link": "http://arxiv.org/abs/2107.12422",
          "publishedOn": "2021-07-28T02:02:30.772Z",
          "wordCount": 682,
          "title": "Towards Efficient Tensor Decomposition-Based DNN Model Compression with Optimization Framework. (arXiv:2107.12422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pakhomov_D/0/1/0/all/0/1\">Daniil Pakhomov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hira_S/0/1/0/all/0/1\">Sanchit Hira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagle_N/0/1/0/all/0/1\">Narayani Wagle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_K/0/1/0/all/0/1\">Kemar E. Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>",
          "description": "We introduce a method that allows to automatically segment images into\nsemantically meaningful regions without human supervision. Derived regions are\nconsistent across different images and coincide with human-defined semantic\nclasses on some datasets. In cases where semantic regions might be hard for\nhuman to define and consistently label, our method is still able to find\nmeaningful and consistent semantic classes. In our work, we use pretrained\nStyleGAN2~\\cite{karras2020analyzing} generative model: clustering in the\nfeature space of the generative model allows to discover semantic classes. Once\nclasses are discovered, a synthetic dataset with generated images and\ncorresponding segmentation masks can be created. After that a segmentation\nmodel is trained on the synthetic dataset and is able to generalize to real\nimages. Additionally, by using CLIP~\\cite{radford2021learning} we are able to\nuse prompts defined in a natural language to discover some desired semantic\nclasses. We test our method on publicly available datasets and show\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2107.12518",
          "publishedOn": "2021-07-28T02:02:30.749Z",
          "wordCount": 597,
          "title": "Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP. (arXiv:2107.12518v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1\">Fa-Ting Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jia-Chang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>",
          "description": "Weakly supervised temporal action localization (WS-TAL) is a challenging task\nthat aims to localize action instances in the given video with video-level\ncategorical supervision. Both appearance and motion features are used in\nprevious works, while they do not utilize them in a proper way but apply simple\nconcatenation or score-level fusion. In this work, we argue that the features\nextracted from the pretrained extractor, e.g., I3D, are not the\nWS-TALtask-specific features, thus the feature re-calibration is needed for\nreducing the task-irrelevant information redundancy. Therefore, we propose a\ncross-modal consensus network (CO2-Net) to tackle this problem. In CO2-Net, we\nmainly introduce two identical proposed cross-modal consensus modules (CCM)\nthat design a cross-modal attention mechanism to filter out the task-irrelevant\ninformation redundancy using the global information from the main modality and\nthe cross-modal local information of the auxiliary modality. Moreover, we treat\nthe attention weights derived from each CCMas the pseudo targets of the\nattention weights derived from another CCM to maintain the consistency between\nthe predictions derived from two CCMs, forming a mutual learning manner.\nFinally, we conduct extensive experiments on two common used temporal action\nlocalization datasets, THUMOS14 and ActivityNet1.2, to verify our method and\nachieve the state-of-the-art results. The experimental results show that our\nproposed cross-modal consensus module can produce more representative features\nfor temporal action localization.",
          "link": "http://arxiv.org/abs/2107.12589",
          "publishedOn": "2021-07-28T02:02:30.724Z",
          "wordCount": 665,
          "title": "Cross-modal Consensus Network for Weakly Supervised Temporal Action Localization. (arXiv:2107.12589v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaotian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Houdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>",
          "description": "There is a surge of interest in image scene graph generation (object,\nattribute and relationship detection) due to the need of building fine-grained\nimage understanding models that go beyond object detection. Due to the lack of\na good benchmark, the reported results of different scene graph generation\nmodels are not directly comparable, impeding the research progress. We have\ndeveloped a much-needed scene graph generation benchmark based on the\nmaskrcnn-benchmark and several popular models. This paper presents main\nfeatures of our benchmark and a comprehensive ablation study of scene graph\ngeneration models using the Visual Genome and OpenImages Visual relationship\ndetection datasets. Our codebase is made publicly available at\nhttps://github.com/microsoft/scene_graph_benchmark.",
          "link": "http://arxiv.org/abs/2107.12604",
          "publishedOn": "2021-07-28T02:02:30.684Z",
          "wordCount": 546,
          "title": "Image Scene Graph Generation (SGG) Benchmark. (arXiv:2107.12604v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smedsrud_P/0/1/0/all/0/1\">Pia H. Smedsrud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_D/0/1/0/all/0/1\">Dag Johansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_T/0/1/0/all/0/1\">Thomas de Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_H/0/1/0/all/0/1\">H&#xe5;vard D. Johansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>",
          "description": "Colonoscopy is considered the gold standard for detection of colorectal\ncancer and its precursors. Existing examination methods are, however, hampered\nby high overall miss-rate, and many abnormalities are left undetected.\nComputer-Aided Diagnosis systems based on advanced machine learning algorithms\nare touted as a game-changer that can identify regions in the colon overlooked\nby the physicians during endoscopic examinations, and help detect and\ncharacterize lesions. In previous work, we have proposed the ResUNet++\narchitecture and demonstrated that it produces more efficient results compared\nwith its counterparts U-Net and ResUNet. In this paper, we demonstrate that\nfurther improvements to the overall prediction performance of the ResUNet++\narchitecture can be achieved by using conditional random field and test-time\naugmentation. We have performed extensive evaluations and validated the\nimprovements using six publicly available datasets: Kvasir-SEG, CVC-ClinicDB,\nCVC-ColonDB, ETIS-Larib Polyp DB, ASU-Mayo Clinic Colonoscopy Video Database,\nand CVC-VideoClinicDB. Moreover, we compare our proposed architecture and\nresulting model with other State-of-the-art methods. To explore the\ngeneralization capability of ResUNet++ on different publicly available polyp\ndatasets, so that it could be used in a real-world setting, we performed an\nextensive cross-dataset evaluation. The experimental results show that applying\nCRF and TTA improves the performance on various polyp segmentation datasets\nboth on the same dataset and cross-dataset.",
          "link": "http://arxiv.org/abs/2107.12435",
          "publishedOn": "2021-07-28T02:02:30.676Z",
          "wordCount": 678,
          "title": "A Comprehensive Study on Colorectal Polyp Segmentation with ResUNet++, Conditional Random Field and Test-Time Augmentation. (arXiv:2107.12435v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rahul Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravirathinam_P/0/1/0/all/0/1\">Praveen Ravirathinam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Ankush Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulla_D/0/1/0/all/0/1\">David Mulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>",
          "description": "Mapping and monitoring crops is a key step towards sustainable\nintensification of agriculture and addressing global food security. A dataset\nlike ImageNet that revolutionized computer vision applications can accelerate\ndevelopment of novel crop mapping techniques. Currently, the United States\nDepartment of Agriculture (USDA) annually releases the Cropland Data Layer\n(CDL) which contains crop labels at 30m resolution for the entire United States\nof America. While CDL is state of the art and is widely used for a number of\nagricultural applications, it has a number of limitations (e.g., pixelated\nerrors, labels carried over from previous errors and absence of input imagery\nalong with class labels). In this work, we create a new semantic segmentation\nbenchmark dataset, which we call CalCROP21, for the diverse crops in the\nCentral Valley region of California at 10m spatial resolution using a Google\nEarth Engine based robust image processing pipeline and a novel attention based\nspatio-temporal semantic segmentation algorithm STATT. STATT uses re-sampled\n(interpolated) CDL labels for training, but is able to generate a better\nprediction than CDL by leveraging spatial and temporal patterns in Sentinel2\nmulti-spectral image series to effectively capture phenologic differences\namongst crops and uses attention to reduce the impact of clouds and other\natmospheric disturbances. We also present a comprehensive evaluation to show\nthat STATT has significantly better results when compared to the resampled CDL\nlabels. We have released the dataset and the processing pipeline code for\ngenerating the benchmark dataset.",
          "link": "http://arxiv.org/abs/2107.12499",
          "publishedOn": "2021-07-28T02:02:30.668Z",
          "wordCount": 690,
          "title": "CalCROP21: A Georeferenced multi-spectral dataset of Satellite Imagery and Crop Labels. (arXiv:2107.12499v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12469",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thoreau_M/0/1/0/all/0/1\">Michael Thoreau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilson_F/0/1/0/all/0/1\">Frazer Wilson</a>",
          "description": "Access to high resolution satellite imagery has dramatically increased in\nrecent years as several new constellations have entered service. High revisit\nfrequencies as well as improved resolution has widened the use cases of\nsatellite imagery to areas such as humanitarian relief and even Search and\nRescue (SaR). We propose a novel remote sensing object detection dataset for\ndeep learning assisted SaR. This dataset contains only small objects that have\nbeen identified as potential targets as part of a live SaR response. We\nevaluate the application of popular object detection models to this dataset as\na baseline to inform further research. We also propose a novel object detection\nmetric, specifically designed to be used in a deep learning assisted SaR\nsetting.",
          "link": "http://arxiv.org/abs/2107.12469",
          "publishedOn": "2021-07-28T02:02:30.657Z",
          "wordCount": 571,
          "title": "SaRNet: A Dataset for Deep Learning Assisted Search and Rescue with Satellite Imagery. (arXiv:2107.12469v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garnot_V/0/1/0/all/0/1\">Vivien Sainte Fare Garnot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>",
          "description": "Unprecedented access to multi-temporal satellite imagery has opened new\nperspectives for a variety of Earth observation tasks. Among them,\npixel-precise panoptic segmentation of agricultural parcels has major economic\nand environmental implications. While researchers have explored this problem\nfor single images, we argue that the complex temporal patterns of crop\nphenology are better addressed with temporal sequences of images. In this\npaper, we present the first end-to-end, single-stage method for panoptic\nsegmentation of Satellite Image Time Series (SITS). This module can be combined\nwith our novel image sequence encoding network which relies on temporal\nself-attention to extract rich and adaptive multi-scale spatio-temporal\nfeatures. We also introduce PASTIS, the first open-access SITS dataset with\npanoptic annotations. We demonstrate the superiority of our encoder for\nsemantic segmentation against multiple competing architectures, and set up the\nfirst state-of-the-art of panoptic segmentation of SITS. Our implementation and\nPASTIS are publicly available.",
          "link": "http://arxiv.org/abs/2107.07933",
          "publishedOn": "2021-07-27T02:03:39.359Z",
          "wordCount": null,
          "title": "Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. (arXiv:2107.07933v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meltzer_P/0/1/0/all/0/1\">Peter Meltzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayani_H/0/1/0/all/0/1\">Hooman Shayani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1\">Amir Khasahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_P/0/1/0/all/0/1\">Pradeep Kumar Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1\">Aditya Sanghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1\">Joseph Lambourne</a>",
          "description": "Boundary Representations (B-Reps) are the industry standard in 3D Computer\nAided Design/Manufacturing (CAD/CAM) and industrial design due to their\nfidelity in representing stylistic details. However, they have been ignored in\nthe 3D style research. Existing 3D style metrics typically operate on meshes or\npointclouds, and fail to account for end-user subjectivity by adopting fixed\ndefinitions of style, either through crowd-sourcing for style labels or\nhand-crafted features. We propose UVStyle-Net, a style similarity measure for\nB-Reps that leverages the style signals in the second order statistics of the\nactivations in a pre-trained (unsupervised) 3D encoder, and learns their\nrelative importance to a subjective end-user through few-shot learning. Our\napproach differs from all existing data-driven 3D style methods since it may be\nused in completely unsupervised settings, which is desirable given the lack of\npublicly available labelled B-Rep datasets. More importantly, the few-shot\nlearning accounts for the inherent subjectivity associated with style. We show\nquantitatively that our proposed method with B-Reps is able to capture stronger\nstyle signals than alternative methods on meshes and pointclouds despite its\nsignificantly greater computational efficiency. We also show it is able to\ngenerate meaningful style gradients with respect to the input shape, and that\nfew-shot learning with as few as two positive examples selected by an end-user\nis sufficient to significantly improve the style measure. Finally, we\ndemonstrate its efficacy on a large unlabeled public dataset of CAD models.\nSource code and data will be released in the future.",
          "link": "http://arxiv.org/abs/2105.02961",
          "publishedOn": "2021-07-27T02:03:39.347Z",
          "wordCount": null,
          "title": "UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity Measure for B-Reps. (arXiv:2105.02961v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengbo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiaqi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>",
          "description": "The key challenge of image manipulation detection is how to learn\ngeneralizable features that are sensitive to manipulations in novel data,\nwhilst specific to prevent false alarms on authentic images. Current research\nemphasizes the sensitivity, with the specificity overlooked. In this paper we\naddress both aspects by multi-view feature learning and multi-scale\nsupervision. By exploiting noise distribution and boundary artifact surrounding\ntampered regions, the former aims to learn semantic-agnostic and thus more\ngeneralizable features. The latter allows us to learn from authentic images\nwhich are nontrivial to be taken into account by current semantic segmentation\nnetwork based methods. Our thoughts are realized by a new network which we term\nMVSS-Net. Extensive experiments on five benchmark sets justify the viability of\nMVSS-Net for both pixel-level and image-level manipulation detection.",
          "link": "http://arxiv.org/abs/2104.06832",
          "publishedOn": "2021-07-27T02:03:39.344Z",
          "wordCount": null,
          "title": "Image Manipulation Detection by Multi-View Multi-Scale Supervision. (arXiv:2104.06832v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.",
          "link": "http://arxiv.org/abs/2106.11342",
          "publishedOn": "2021-07-27T02:03:39.342Z",
          "wordCount": null,
          "title": "Dive into Deep Learning. (arXiv:2106.11342v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiequan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhisheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "In this paper, we propose Parametric Contrastive Learning (PaCo) to tackle\nlong-tailed recognition. Based on theoretical analysis, we observe supervised\ncontrastive loss tends to bias on high-frequency classes and thus increases the\ndifficulty of imbalance learning. We introduce a set of parametric class-wise\nlearnable centers to rebalance from an optimization perspective. Further, we\nanalyze our PaCo loss under a balanced setting. Our analysis demonstrates that\nPaCo can adaptively enhance the intensity of pushing samples of the same class\nclose as more samples are pulled together with their corresponding centers and\nbenefit hard example learning. Experiments on long-tailed CIFAR, ImageNet,\nPlaces, and iNaturalist 2018 manifest the new state-of-the-art for long-tailed\nrecognition. On full ImageNet, models trained with PaCo loss surpass supervised\ncontrastive learning across various ResNet backbones. Our code is available at\n\\url{https://github.com/jiequancui/Parametric-Contrastive-Learning}.",
          "link": "http://arxiv.org/abs/2107.12028",
          "publishedOn": "2021-07-27T02:03:36.138Z",
          "wordCount": 564,
          "title": "Parametric Contrastive Learning. (arXiv:2107.12028v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yue Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prugel_Bennett_A/0/1/0/all/0/1\">Adam Pr&#xfc;gel-Bennett</a>",
          "description": "Visual Semantic Embedding (VSE) models, which map images into a rich semantic\nembedding space, have been a milestone in object recognition and zero-shot\nlearning. Current approaches to VSE heavily rely on static word em-bedding\ntechniques. In this work, we propose a Visual Se-mantic Embedding Probe (VSEP)\ndesigned to probe the semantic information of contextualized word embeddings in\nvisual semantic understanding tasks. We show that the knowledge encoded in\ntransformer language models can be exploited for tasks requiring visual\nsemantic understanding.The VSEP with contextual representations can distinguish\nword-level object representations in complicated scenes as a compositional\nzero-shot learner. We further introduce a zero-shot setting with VSEPs to\nevaluate a model's ability to associate a novel word with a novel visual\ncategory. We find that contextual representations in language mod-els\noutperform static word embeddings, when the compositional chain of object is\nshort. We notice that current visual semantic embedding models lack a mutual\nexclusivity bias which limits their performance.",
          "link": "http://arxiv.org/abs/2107.12021",
          "publishedOn": "2021-07-27T02:03:35.877Z",
          "wordCount": 588,
          "title": "Language Models as Zero-shot Visual Semantic Learners. (arXiv:2107.12021v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Ping-Chia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "Despite the success of deep learning on supervised point cloud semantic\nsegmentation, obtaining large-scale point-by-point manual annotations is still\na significant challenge. To reduce the huge annotation burden, we propose a\nRegion-based and Diversity-aware Active Learning (ReDAL), a general framework\nfor many deep learning approaches, aiming to automatically select only\ninformative and diverse sub-scene regions for label acquisition. Observing that\nonly a small portion of annotated regions are sufficient for 3D scene\nunderstanding with deep learning, we use softmax entropy, color discontinuity,\nand structural complexity to measure the information of sub-scene regions. A\ndiversity-aware selection algorithm is also developed to avoid redundant\nannotations resulting from selecting informative but similar regions in a\nquerying batch. Extensive experiments show that our method highly outperforms\nprevious active learning strategies, and we achieve the performance of 90%\nfully supervised learning, while less than 15% and 5% annotations are required\non S3DIS and SemanticKITTI datasets, respectively.",
          "link": "http://arxiv.org/abs/2107.11769",
          "publishedOn": "2021-07-27T02:03:35.792Z",
          "wordCount": 608,
          "title": "ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jingjing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1\">Zhixiong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>",
          "description": "Encouraging progress has been made towards Visual Question Answering (VQA) in\nrecent years, but it is still challenging to enable VQA models to adaptively\ngeneralize to out-of-distribution (OOD) samples. Intuitively, recompositions of\nexisting visual concepts (i.e., attributes and objects) can generate unseen\ncompositions in the training set, which will promote VQA models to generalize\nto OOD samples. In this paper, we formulate OOD generalization in VQA as a\ncompositional generalization problem and propose a graph generative\nmodeling-based training scheme (X-GGM) to handle the problem implicitly. X-GGM\nleverages graph generative modeling to iteratively generate a relation matrix\nand node representations for the predefined graph that utilizes\nattribute-object pairs as nodes. Furthermore, to alleviate the unstable\ntraining issue in graph generative modeling, we propose a gradient distribution\nconsistency loss to constrain the data distribution with adversarial\nperturbations and the generated distribution. The baseline VQA model (LXMERT)\ntrained with the X-GGM scheme achieves state-of-the-art OOD performance on two\nstandard VQA OOD benchmarks, i.e., VQA-CP v2 and GQA-OOD. Extensive ablation\nstudies demonstrate the effectiveness of X-GGM components.",
          "link": "http://arxiv.org/abs/2107.11576",
          "publishedOn": "2021-07-27T02:03:35.568Z",
          "wordCount": 624,
          "title": "X-GGM: Graph Generative Modeling for Out-of-Distribution Generalization in Visual Question Answering. (arXiv:2107.11576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fang-Yi Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Exploring to what humans pay attention in dynamic panoramic scenes is useful\nfor many fundamental applications, including augmented reality (AR) in retail,\nAR-powered recruitment, and visual language navigation. With this goal in mind,\nwe propose PV-SOD, a new task that aims to segment salient objects from\npanoramic videos. In contrast to existing fixation-level or object-level\nsaliency detection tasks, we focus on multi-modal salient object detection\n(SOD), which mimics human attention mechanism by segmenting salient objects\nwith the guidance of audio-visual cues. To support this task, we collect the\nfirst large-scale dataset, named ASOD60K, which contains 4K-resolution video\nframes annotated with a six-level hierarchy, thus distinguishing itself with\nrichness, diversity and quality. Specifically, each sequence is marked with\nboth its super-/sub-class, with objects of each sub-class being further\nannotated with human eye fixations, bounding boxes, object-/instance-level\nmasks, and associated attributes (e.g., geometrical distortion). These\ncoarse-to-fine annotations enable detailed analysis for PV-SOD modeling, e.g.,\ndetermining the major challenges for existing SOD models, and predicting\nscanpaths to study the long-term eye fixation behaviors of humans. We\nsystematically benchmark 11 representative approaches on ASOD60K and derive\nseveral interesting findings. We hope this study could serve as a good starting\npoint for advancing SOD research towards panoramic videos.",
          "link": "http://arxiv.org/abs/2107.11629",
          "publishedOn": "2021-07-27T02:03:35.422Z",
          "wordCount": 654,
          "title": "ASOD60K: Audio-Induced Salient Object Detection in Panoramic Videos. (arXiv:2107.11629v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zi-Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liang-Jian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tai-Xiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tian-Jing Zhang</a>",
          "description": "The convolution operation is a powerful tool for feature extraction and plays\na prominent role in the field of computer vision. However, when targeting the\npixel-wise tasks like image fusion, it would not fully perceive the\nparticularity of each pixel in the image if the uniform convolution kernel is\nused on different patches. In this paper, we propose a local adaptive\nconvolution (LAConv), which is dynamically adjusted to different spatial\nlocations. LAConv enables the network to pay attention to every specific local\narea in the learning process. Besides, the dynamic bias (DYB) is introduced to\nprovide more possibilities for the depiction of features and make the network\nmore flexible. We further design a residual structure network equipped with the\nproposed LAConv and DYB modules, and apply it to two image fusion tasks.\nExperiments for pansharpening and hyperspectral image super-resolution (HISR)\ndemonstrate the superiority of our method over other state-of-the-art methods.\nIt is worth mentioning that LAConv can also be competent for other\nsuper-resolution tasks with less computation effort.",
          "link": "http://arxiv.org/abs/2107.11617",
          "publishedOn": "2021-07-27T02:03:35.375Z",
          "wordCount": 608,
          "title": "LAConv: Local Adaptive Convolution for Image Fusion. (arXiv:2107.11617v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Weizhi An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hehuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuzhi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>",
          "description": "Recent studies imply that deep neural networks are vulnerable to adversarial\nexamples -- inputs with a slight but intentional perturbation are incorrectly\nclassified by the network. Such vulnerability makes it risky for some\nsecurity-related applications (e.g., semantic segmentation in autonomous cars)\nand triggers tremendous concerns on the model reliability. For the first time,\nwe comprehensively evaluate the robustness of existing UDA methods and propose\na robust UDA approach. It is rooted in two observations: (i) the robustness of\nUDA methods in semantic segmentation remains unexplored, which pose a security\nconcern in this field; and (ii) although commonly used self-supervision (e.g.,\nrotation and jigsaw) benefits image tasks such as classification and\nrecognition, they fail to provide the critical supervision signals that could\nlearn discriminative representation for segmentation tasks. These observations\nmotivate us to propose adversarial self-supervision UDA (or ASSUDA) that\nmaximizes the agreement between clean images and their adversarial examples by\na contrastive loss in the output space. Extensive empirical studies on commonly\nused benchmarks demonstrate that ASSUDA is resistant to adversarial attacks.",
          "link": "http://arxiv.org/abs/2105.10843",
          "publishedOn": "2021-07-27T02:03:35.246Z",
          "wordCount": 652,
          "title": "Exploring Robustness of Unsupervised Domain Adaptation in Semantic Segmentation. (arXiv:2105.10843v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1\">Larisa Markeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1\">Ilya Tolstikhin</a>",
          "description": "We introduce a generalization to the lottery ticket hypothesis in which the\nnotion of \"sparsity\" is relaxed by choosing an arbitrary basis in the space of\nparameters. We present evidence that the original results reported for the\ncanonical basis continue to hold in this broader setting. We describe how\nstructured pruning methods, including pruning units or factorizing\nfully-connected layers into products of low-rank matrices, can be cast as\nparticular instances of this \"generalized\" lottery ticket hypothesis. The\ninvestigations reported here are preliminary and are provided to encourage\nfurther research along this direction.",
          "link": "http://arxiv.org/abs/2107.06825",
          "publishedOn": "2021-07-27T02:03:35.213Z",
          "wordCount": 571,
          "title": "A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08751",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1\">Marius Memmel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Deep learning for medical imaging suffers from temporal and privacy-related\nrestrictions on data availability. To still obtain viable models, continual\nlearning aims to train in sequential order, as and when data is available. The\nmain challenge that continual learning methods face is to prevent catastrophic\nforgetting, i.e., a decrease in performance on the data encountered earlier.\nThis issue makes continuous training of segmentation models for medical\napplications extremely difficult. Yet, often, data from at least two different\ndomains is available which we can exploit to train the model in a way that it\ndisregards domain-specific information. We propose an architecture that\nleverages the simultaneous availability of two or more datasets to learn a\ndisentanglement between the content and domain in an adversarial fashion. The\ndomain-invariant content representation then lays the base for continual\nsemantic segmentation. Our approach takes inspiration from domain adaptation\nand combines it with continual learning for hippocampal segmentation in brain\nMRI. We showcase that our method reduces catastrophic forgetting and\noutperforms state-of-the-art continual learning methods.",
          "link": "http://arxiv.org/abs/2107.08751",
          "publishedOn": "2021-07-27T02:03:35.207Z",
          "wordCount": 647,
          "title": "Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02771",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kazemimoghadam_M/0/1/0/all/0/1\">Mahdieh Kazemimoghadam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chi_W/0/1/0/all/0/1\">Weicheng Chi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahimi_A/0/1/0/all/0/1\">Asal Rahimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_N/0/1/0/all/0/1\">Nathan Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alluri_P/0/1/0/all/0/1\">Prasanna Alluri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nwachukwu_C/0/1/0/all/0/1\">Chika Nwachukwu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1\">Weiguo Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_X/0/1/0/all/0/1\">Xuejun Gu</a>",
          "description": "Efficient, reliable and reproducible target volume delineation is a key step\nin the effective planning of breast radiotherapy. However, post-operative\nbreast target delineation is challenging as the contrast between the tumor bed\nvolume (TBV) and normal breast tissue is relatively low in CT images. In this\nstudy, we propose to mimic the marker-guidance procedure in manual target\ndelineation. We developed a saliency-based deep learning segmentation (SDL-Seg)\nalgorithm for accurate TBV segmentation in post-operative breast irradiation.\nThe SDL-Seg algorithm incorporates saliency information in the form of markers'\nlocation cues into a U-Net model. The design forces the model to encode the\nlocation-related features, which underscores regions with high saliency levels\nand suppresses low saliency regions. The saliency maps were generated by\nidentifying markers on CT images. Markers' locations were then converted to\nprobability maps using a distance-transformation coupled with a Gaussian\nfilter. Subsequently, the CT images and the corresponding saliency maps formed\na multi-channel input for the SDL-Seg network. Our in-house dataset was\ncomprised of 145 prone CT images from 29 post-operative breast cancer patients,\nwho received 5-fraction partial breast irradiation (PBI) regimen on GammaPod.\nThe performance of the proposed method was compared against basic U-Net. Our\nmodel achieved mean (standard deviation) of 76.4 %, 6.76 mm, and 1.9 mm for\nDSC, HD95, and ASD respectively on the test set with computation time of below\n11 seconds per one CT volume. SDL-Seg showed superior performance relative to\nbasic U-Net for all the evaluation metrics while preserving low computation\ncost. The findings demonstrate that SDL-Seg is a promising approach for\nimproving the efficiency and accuracy of the on-line treatment planning\nprocedure of PBI, such as GammaPod based PBI.",
          "link": "http://arxiv.org/abs/2105.02771",
          "publishedOn": "2021-07-27T02:03:35.200Z",
          "wordCount": 770,
          "title": "Saliency-Guided Deep Learning Network for Automatic Tumor Bed Volume Delineation in Post-operative Breast Irradiation. (arXiv:2105.02771v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1\">Mengyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_H/0/1/0/all/0/1\">Haibin Hang</a>",
          "description": "We propose a manifold matching approach to generative models which includes a\ndistribution generator (or data generator) and a metric generator. In our\nframework, we view the real data set as some manifold embedded in a\nhigh-dimensional Euclidean space. The distribution generator aims at generating\nsamples that follow some distribution condensed around the real data manifold.\nIt is achieved by matching two sets of points using their geometric shape\ndescriptors, such as centroid and $p$-diameter, with learned distance metric;\nthe metric generator utilizes both real data and generated samples to learn a\ndistance metric which is close to some intrinsic geodesic distance on the real\ndata manifold. The produced distance metric is further used for manifold\nmatching. The two networks are learned simultaneously during the training\nprocess. We apply the approach on both unsupervised and supervised learning\ntasks: in unconditional image generation task, the proposed method obtains\ncompetitive results compared with existing generative models; in\nsuper-resolution task, we incorporate the framework in perception-based models\nand improve visual qualities by producing samples with more natural textures.\nBoth theoretical analysis and real data experiments demonstrate the feasibility\nand effectiveness of the proposed framework.",
          "link": "http://arxiv.org/abs/2106.10777",
          "publishedOn": "2021-07-27T02:03:35.176Z",
          "wordCount": 655,
          "title": "Manifold Matching via Deep Metric Learning for Generative Modeling. (arXiv:2106.10777v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1\">Ian Colbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1\">Ken Kreutz-Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srinjoy Das</a>",
          "description": "A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.",
          "link": "http://arxiv.org/abs/2107.07647",
          "publishedOn": "2021-07-27T02:03:35.169Z",
          "wordCount": 681,
          "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tata_G/0/1/0/all/0/1\">Gautam Tata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Royer_S/0/1/0/all/0/1\">Sarah-Jeanne Royer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poirion_O/0/1/0/all/0/1\">Olivier Poirion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_J/0/1/0/all/0/1\">Jay Lowe</a>",
          "description": "The quantification of positively buoyant marine plastic debris is critical to\nunderstanding how concentrations of trash from across the world's ocean and\nidentifying high concentration garbage hotspots in dire need of trash removal.\nCurrently, the most common monitoring method to quantify floating plastic\nrequires the use of a manta trawl. Techniques requiring manta trawls (or\nsimilar surface collection devices) utilize physical removal of marine plastic\ndebris as the first step and then analyze collected samples as a second step.\nThe need for physical removal before analysis incurs high costs and requires\nintensive labor preventing scalable deployment of a real-time marine plastic\nmonitoring service across the entirety of Earth's ocean bodies. Without better\nmonitoring and sampling methods, the total impact of plastic pollution on the\nenvironment as a whole, and details of impact within specific oceanic regions,\nwill remain unknown. This study presents a highly scalable workflow that\nutilizes images captured within the epipelagic layer of the ocean as an input.\nIt produces real-time quantification of marine plastic debris for accurate\nquantification and physical removal. The workflow includes creating and\npreprocessing a domain-specific dataset, building an object detection model\nutilizing a deep neural network, and evaluating the model's performance.\nYOLOv5-S was the best performing model, which operates at a Mean Average\nPrecision (mAP) of 0.851 and an F1-Score of 0.89 while maintaining\nnear-real-time speed.",
          "link": "http://arxiv.org/abs/2105.01882",
          "publishedOn": "2021-07-27T02:03:35.162Z",
          "wordCount": 725,
          "title": "DeepPlastic: A Novel Approach to Detecting Epipelagic Bound Plastic Using Deep Visual Models. (arXiv:2105.01882v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>",
          "description": "Unsupervised domain adaptation (UDA) involves a supervised loss in a labeled\nsource domain and an unsupervised loss in an unlabeled target domain, which\noften faces more severe overfitting (than classical supervised learning) as the\nsupervised source loss has clear domain gap and the unsupervised target loss is\noften noisy due to the lack of annotations. This paper presents RDA, a robust\ndomain adaptation technique that introduces adversarial attacking to mitigate\noverfitting in UDA. We achieve robust domain adaptation by a novel Fourier\nadversarial attacking (FAA) method that allows large magnitude of perturbation\nnoises but has minimal modification of image semantics, the former is critical\nto the effectiveness of its generated adversarial samples due to the existence\nof 'domain gaps'. Specifically, FAA decomposes images into multiple frequency\ncomponents (FCs) and generates adversarial samples by just perturbating certain\nFCs that capture little semantic information. With FAA-generated samples, the\ntraining can continue the 'random walk' and drift into an area with a flat loss\nlandscape, leading to more robust domain adaptation. Extensive experiments over\nmultiple domain adaptation tasks show that RDA can work with different computer\nvision tasks with superior performance.",
          "link": "http://arxiv.org/abs/2106.02874",
          "publishedOn": "2021-07-27T02:03:35.155Z",
          "wordCount": 661,
          "title": "RDA: Robust Domain Adaptation via Fourier Adversarial Attacking. (arXiv:2106.02874v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1\">Emilian Radoi</a>",
          "description": "The use of gait for person identification has important advantages such as\nbeing non-invasive, unobtrusive, not requiring cooperation and being less\nlikely to be obscured compared to other biometrics. Existing methods for gait\nrecognition require cooperative gait scenarios, in which a single person is\nwalking multiple times in a straight line in front of a camera. We aim to\naddress the hard challenges of real-world scenarios in which camera feeds\ncapture multiple people, who in most cases pass in front of the camera only\nonce. We address privacy concerns by using only the motion information of\nwalking individuals, with no identifiable appearance-based information. As\nsuch, we propose a novel weakly supervised learning framework, WildGait, which\nconsists of training a Spatio-Temporal Graph Convolutional Network on a large\nnumber of automatically annotated skeleton sequences obtained from raw,\nreal-world, surveillance streams to learn useful gait signatures. Our results\nshow that, with fine-tuning, we surpass in terms of recognition accuracy the\ncurrent state-of-the-art pose-based gait recognition solutions. Our proposed\nmethod is reliable in training gait recognition methods in unconstrained\nenvironments, especially in settings with scarce amounts of annotated data.",
          "link": "http://arxiv.org/abs/2105.05528",
          "publishedOn": "2021-07-27T02:03:35.148Z",
          "wordCount": 677,
          "title": "WildGait: Learning Gait Representations from Raw Surveillance Streams. (arXiv:2105.05528v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Savchenko_A/0/1/0/all/0/1\">Andrey V. Savchenko</a>",
          "description": "In this paper, the multi-task learning of lightweight convolutional neural\nnetworks is studied for face identification and classification of facial\nattributes (age, gender, ethnicity) trained on cropped faces without margins.\nThe necessity to fine-tune these networks to predict facial expressions is\nhighlighted. Several models are presented based on MobileNet, EfficientNet and\nRexNet architectures. It was experimentally demonstrated that they lead to near\nstate-of-the-art results in age, gender and race recognition on the UTKFace\ndataset and emotion classification on the AffectNet dataset. Moreover, it is\nshown that the usage of the trained models as feature extractors of facial\nregions in video frames leads to 4.5% higher accuracy than the previously known\nstate-of-the-art single models for the AFEW and the VGAF datasets from the\nEmotiW challenges. The models and source code are publicly available at\nhttps://github.com/HSE-asavchenko/face-emotion-recognition.",
          "link": "http://arxiv.org/abs/2103.17107",
          "publishedOn": "2021-07-27T02:03:35.140Z",
          "wordCount": 617,
          "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks. (arXiv:2103.17107v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Visca_M/0/1/0/all/0/1\">Marco Visca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1\">Sampo Kuutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1\">Roger Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1\">Saber Fallah</a>",
          "description": "Terrain traversability analysis plays a major role in ensuring safe robotic\nnavigation in unstructured environments. However, real-time constraints\nfrequently limit the accuracy of online tests especially in scenarios where\nrealistic robot-terrain interactions are complex to model. In this context, we\npropose a deep learning framework trained in an end-to-end fashion from\nelevation maps and trajectories to estimate the occurrence of failure events.\nThe network is first trained and tested in simulation over synthetic maps\ngenerated by the OpenSimplex algorithm. The prediction performance of the Deep\nLearning framework is illustrated by being able to retain over 94% recall of\nthe original simulator at 30% of the computational time. Finally, the network\nis transferred and tested on real elevation maps collected by the SEEKER\nconsortium during the Martian rover test trial in the Atacama desert in Chile.\nWe show that transferring and fine-tuning of an application-independent\npre-trained model retains better performance than training uniquely on scarcely\navailable real data.",
          "link": "http://arxiv.org/abs/2105.10937",
          "publishedOn": "2021-07-27T02:03:35.133Z",
          "wordCount": 639,
          "title": "Deep Learning Traversability Estimator for Mobile Robots in Unstructured Environments. (arXiv:2105.10937v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gera_D/0/1/0/all/0/1\">Darshan Gera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1\">S Balasubramanian</a>",
          "description": "Facial expression recognition (FER) in the wild is crucial for building\nreliable human-computer interactive systems. However, annotations of large\nscale datasets in FER has been a key challenge as these datasets suffer from\nnoise due to various factors like crowd sourcing, subjectivity of annotators,\npoor quality of images, automatic labelling based on key word search etc. Such\nnoisy annotations impede the performance of FER due to the memorization ability\nof deep networks. During early learning stage, deep networks fit on clean data.\nThen, eventually, they start overfitting on noisy labels due to their\nmemorization ability, which limits FER performance. This report presents\nConsensual Collaborative Training (CCT) framework used in our submission to\nexpression recognition track of the Affective Behaviour Analysis in-the-wild\n(ABAW) 2021 competition. CCT co-trains three networks jointly using a convex\ncombination of supervision loss and consistency loss, without making any\nassumption about the noise distribution. A dynamic transition mechanism is used\nto move from supervision loss in early learning to consistency loss for\nconsensus of predictions among networks in the later stage. Co-training reduces\noverall error, and consistency loss prevents overfitting to noisy samples. The\nperformance of the model is validated on challenging Aff-Wild2 dataset for\ncategorical expression classification. Our code is made publicly available at\nhttps://github.com/1980x/ABAW2021DMACS.",
          "link": "http://arxiv.org/abs/2107.05736",
          "publishedOn": "2021-07-27T02:03:35.121Z",
          "wordCount": 696,
          "title": "Affect Expression Behaviour Analysis in the Wild using Consensual Collaborative Training. (arXiv:2107.05736v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+wang_S/0/1/0/all/0/1\">Song wang</a>",
          "description": "Image inpainting aims to restore the missing regions and make the recovery\nresults identical to the originally complete image, which is different from the\ncommon generative task emphasizing the naturalness of generated images.\nNevertheless, existing works usually regard it as a pure generation problem and\nemploy cutting-edge generative techniques to address it. The generative\nnetworks fill the main missing parts with realistic contents but usually\ndistort the local structures. In this paper, we formulate image inpainting as a\nmix of two problems, i.e., predictive filtering and deep generation. Predictive\nfiltering is good at preserving local structures and removing artifacts but\nfalls short to complete the large missing regions. The deep generative network\ncan fill the numerous missing pixels based on the understanding of the whole\nscene but hardly restores the details identical to the original ones. To make\nuse of their respective advantages, we propose the joint predictive filtering\nand generative network (JPGNet) that contains three branches: predictive\nfiltering & uncertainty network (PFUNet), deep generative network, and\nuncertainty-aware fusion network (UAFNet). The PFUNet can adaptively predict\npixel-wise kernels for filtering-based inpainting according to the input image\nand output an uncertainty map. This map indicates the pixels should be\nprocessed by filtering or generative networks, which is further fed to the\nUAFNet for a smart combination between filtering and generative results. Note\nthat, our method as a novel framework for the image inpainting problem can\nbenefit any existing generation-based methods. We validate our method on three\npublic datasets, i.e., Dunhuang, Places2, and CelebA, and demonstrate that our\nmethod can enhance three state-of-the-art generative methods (i.e., StructFlow,\nEdgeConnect, and RFRNet) significantly with the slightly extra time cost.",
          "link": "http://arxiv.org/abs/2107.04281",
          "publishedOn": "2021-07-27T02:03:35.098Z",
          "wordCount": 747,
          "title": "JPGNet: Joint Predictive Filtering and Generative Network for Image Inpainting. (arXiv:2107.04281v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael Ng</a>",
          "description": "Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.",
          "link": "http://arxiv.org/abs/2107.07058",
          "publishedOn": "2021-07-27T02:03:35.070Z",
          "wordCount": 724,
          "title": "A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1\">Lin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zongyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Q/0/1/0/all/0/1\">Qianyan Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yehansen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lijing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihang Li</a>",
          "description": "RGB-Infrared (IR) person re-identification aims to retrieve\nperson-of-interest from heterogeneous cameras, easily suffering from large\nimage modality discrepancy caused by different sensing wavelength ranges.\nExisting work usually minimizes such discrepancy by aligning domain\ndistribution of global features, while neglecting the intra-modality structural\nrelations between semantic parts. This could result in the network overly\nfocusing on local cues, without considering long-range body part dependencies,\nleading to meaningless region representations. In this paper, we propose a\ngraph-enabled distribution matching solution, dubbed Geometry-Guided\nDual-Alignment (G2DA) learning, for RGB-IR ReID. It can jointly encourage the\ncross-modal consistency between part semantics and structural relations for\nfine-grained modality alignment by solving a graph matching task within a\nmulti-scale skeleton graph that embeds human topology information.\nSpecifically, we propose to build a semantic-aligned complete graph into which\nall cross-modality images can be mapped via a pose-adaptive graph construction\nmechanism. This graph represents extracted whole-part features by nodes and\nexpresses the node-wise similarities with associated edges. To achieve the\ngraph-based dual-alignment learning, an Optimal Transport (OT) based structured\nmetric is further introduced to simultaneously measure point-wise relations and\ngroup-wise structural similarities across modalities. By minimizing the cost of\nan inter-modality transport plan, G2DA can learn a consistent and\ndiscriminative feature subspace for cross-modality image retrieval.\nFurthermore, we advance a Message Fusion Attention (MFA) mechanism to\nadaptively reweight the information flow of semantic propagation, effectively\nstrengthening the discriminability of extracted semantic features.",
          "link": "http://arxiv.org/abs/2106.07853",
          "publishedOn": "2021-07-27T02:03:35.063Z",
          "wordCount": 712,
          "title": "G2DA: Geometry-Guided Dual-Alignment Learning for RGB-Infrared Person Re-Identification. (arXiv:2106.07853v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Miaomiao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Peiran Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>",
          "description": "This paper proposes a novel deep learning-based video object matting method\nthat can achieve temporally coherent matting results. Its key component is an\nattention-based temporal aggregation module that maximizes image matting\nnetworks' strength for video matting networks. This module computes temporal\ncorrelations for pixels adjacent to each other along the time axis in feature\nspace, which is robust against motion noises. We also design a novel loss term\nto train the attention weights, which drastically boosts the video matting\nperformance. Besides, we show how to effectively solve the trimap generation\nproblem by fine-tuning a state-of-the-art video object segmentation network\nwith a sparse set of user-annotated keyframes. To facilitate video matting and\ntrimap generation networks' training, we construct a large-scale video matting\ndataset with 80 training and 28 validation foreground video clips with\nground-truth alpha mattes. Experimental results show that our method can\ngenerate high-quality alpha mattes for various videos featuring appearance\nchange, occlusion, and fast motion. Our code and dataset can be found at:\nhttps://github.com/yunkezhang/TCVOM",
          "link": "http://arxiv.org/abs/2105.11427",
          "publishedOn": "2021-07-27T02:03:35.056Z",
          "wordCount": 650,
          "title": "Attention-guided Temporally Coherent Video Object Matting. (arXiv:2105.11427v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>",
          "description": "We propose ST-DETR, a Spatio-Temporal Transformer-based architecture for\nobject detection from a sequence of temporal frames. We treat the temporal\nframes as sequences in both space and time and employ the full attention\nmechanisms to take advantage of the features correlations over both dimensions.\nThis treatment enables us to deal with frames sequence as temporal object\nfeatures traces over every location in the space. We explore two possible\napproaches; the early spatial features aggregation over the temporal dimension,\nand the late temporal aggregation of object query spatial features. Moreover,\nwe propose a novel Temporal Positional Embedding technique to encode the time\nsequence information. To evaluate our approach, we choose the Moving Object\nDetection (MOD)task, since it is a perfect candidate to showcase the importance\nof the temporal dimension. Results show a significant 5% mAP improvement on the\nKITTI MOD dataset over the 1-step spatial baseline.",
          "link": "http://arxiv.org/abs/2107.05887",
          "publishedOn": "2021-07-27T02:03:35.027Z",
          "wordCount": 601,
          "title": "ST-DETR: Spatio-Temporal Object Traces Attention Detection Transformer. (arXiv:2107.05887v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shavit_Y/0/1/0/all/0/1\">Yoli Shavit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferens_R/0/1/0/all/0/1\">Ron Ferens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1\">Yosi Keller</a>",
          "description": "Absolute camera pose regressors estimate the position and orientation of a\ncamera from the captured image alone. Typically, a convolutional backbone with\na multi-layer perceptron head is trained with images and pose labels to embed a\nsingle reference scene at a time. Recently, this scheme was extended for\nlearning multiple scenes by replacing the MLP head with a set of fully\nconnected layers. In this work, we propose to learn multi-scene absolute camera\npose regression with Transformers, where encoders are used to aggregate\nactivation maps with self-attention and decoders transform latent features and\nscenes encoding into candidate pose predictions. This mechanism allows our\nmodel to focus on general features that are informative for localization while\nembedding multiple scenes in parallel. We evaluate our method on commonly\nbenchmarked indoor and outdoor datasets and show that it surpasses both\nmulti-scene and state-of-the-art single-scene absolute pose regressors. We make\nour code publicly available from\nhttps://github.com/yolish/multi-scene-pose-transformer.",
          "link": "http://arxiv.org/abs/2103.11468",
          "publishedOn": "2021-07-27T02:03:35.021Z",
          "wordCount": 613,
          "title": "Learning Multi-Scene Absolute Pose Regression with Transformers. (arXiv:2103.11468v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Vision and Language Navigation (VLN) requires an agent to navigate to a\ntarget location by following natural language instructions. Most of existing\nworks represent a navigation candidate by the feature of the corresponding\nsingle view where the candidate lies in. However, an instruction may mention\nlandmarks out of the single view as references, which might lead to failures of\ntextual-visual matching of existing methods. In this work, we propose a\nmulti-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate\nvisual contexts from neighbor views for better textual-visual matching.\nSpecifically, our NvEM utilizes a subject module and a reference module to\ncollect contexts from neighbor views. The subject module fuses neighbor views\nat a global level, and the reference module fuses neighbor objects at a local\nlevel. Subjects and references are adaptively determined via attention\nme'chanisms. Our model also includes an action module to utilize the strong\norientation guidance (e.g., \"turn left\") in instructions. Each module predicts\nnavigation action separately and their weighted sum is used for predicting the\nfinal action. Extensive experimental results demonstrate the effectiveness of\nthe proposed method on the R2R and R4R benchmarks against several\nstate-of-the-art navigators, and NvEM even beats some pre-training ones. Our\ncode is available at https://github.com/MarSaKi/NvEM.",
          "link": "http://arxiv.org/abs/2107.07201",
          "publishedOn": "2021-07-27T02:03:35.002Z",
          "wordCount": 669,
          "title": "Neighbor-view Enhanced Model for Vision and Language Navigation. (arXiv:2107.07201v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tingting Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "Modern top-performing object detectors depend heavily on backbone networks,\nwhose advances bring consistent performance gains through exploring more\neffective network structures. In this paper, we propose a novel and flexible\nbackbone framework, namely CBNetV2, to construct high-performance detectors\nusing existing open-sourced pre-trained backbones under the pre-training\nfine-tuning paradigm. In particular, CBNetV2 architecture groups multiple\nidentical backbones, which are connected through composite connections.\nSpecifically, it integrates the high- and low-level features of multiple\nbackbone networks and gradually expands the receptive field to more efficiently\nperform object detection. We also propose a better training strategy with\nassistant supervision for CBNet-based detectors. Without additional\npre-training of the composite backbone, CBNetV2 can be adapted to various\nbackbones (CNN-based vs. Transformer-based) and head designs of most mainstream\ndetectors (one-stage vs. two-stage, anchor-based vs. anchor-free-based).\nExperiments provide strong evidence that, compared with simply increasing the\ndepth and width of the network, CBNetV2 introduces a more efficient, effective,\nand resource-friendly way to build high-performance backbone networks.\nParticularly, our Dual-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO\ntest-dev under the single-model and single-scale testing protocol, which is\nsignificantly better than the state-of-the-art result (57.7% box AP and 50.2%\nmask AP) achieved by Swin-L, while the training schedule is reduced by\n6$\\times$. With multi-scale testing, we push the current best single model\nresult to a new record of 60.1% box AP and 52.3% mask AP without using extra\ntraining data. Code is available at https://github.com/VDIGPKU/CBNetV2.",
          "link": "http://arxiv.org/abs/2107.00420",
          "publishedOn": "2021-07-27T02:03:34.995Z",
          "wordCount": 735,
          "title": "CBNetV2: A Composite Backbone Network Architecture for Object Detection. (arXiv:2107.00420v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Romanengo_C/0/1/0/all/0/1\">Chiara Romanengo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffo_A/0/1/0/all/0/1\">Andrea Raffo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_Y/0/1/0/all/0/1\">Yifan Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_N/0/1/0/all/0/1\">Nabil Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcidieno_B/0/1/0/all/0/1\">Bianca Falcidieno</a>",
          "description": "We propose Fit4CAD, a benchmark for the evaluation and comparison of methods\nfor fitting simple geometric primitives in point clouds representing CAD\nmodels. This benchmark is meant to help both method developers and those who\nwant to identify the best performing tools. The Fit4CAD dataset is composed by\n225 high quality point clouds, each of which has been obtained by sampling a\nCAD model. The way these elements were created by using existing platforms and\ndatasets makes the benchmark easily expandable. The dataset is already split\ninto a training set and a test set. To assess performance and accuracy of the\ndifferent primitive fitting methods, various measures are defined. To\ndemonstrate the effective use of Fit4CAD, we have tested it on two methods\nbelonging to two different categories of approaches to the primitive fitting\nproblem: a clustering method based on a primitive growing framework and a\nparametric method based on the Hough transform.",
          "link": "http://arxiv.org/abs/2105.06858",
          "publishedOn": "2021-07-27T02:03:34.987Z",
          "wordCount": 632,
          "title": "Fit4CAD: A point cloud benchmark for fitting simple geometric primitives in CAD models. (arXiv:2105.06858v2 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14758",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuwei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Ying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Daoye Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1\">Jingwei Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1\">Zhaoxiang Ye</a>",
          "description": "Low-dose CT has been a key diagnostic imaging modality to reduce the\npotential risk of radiation overdose to patient health. Despite recent\nadvances, CNN-based approaches typically apply filters in a spatially invariant\nway and adopt similar pixel-level losses, which treat all regions of the CT\nimage equally and can be inefficient when fine-grained structures coexist with\nnon-uniformly distributed noises. To address this issue, we propose a\nStructure-preserving Kernel Prediction Network (StructKPN) that combines the\nkernel prediction network with a structure-aware loss function that utilizes\nthe pixel gradient statistics and guides the model towards spatially-variant\nfilters that enhance noise removal, prevent over-smoothing and preserve\ndetailed structures for different regions in CT imaging. Extensive experiments\ndemonstrated that our approach achieved superior performance on both synthetic\nand non-synthetic datasets, and better preserves structures that are highly\ndesired in clinical screening and low-dose protocol optimization.",
          "link": "http://arxiv.org/abs/2105.14758",
          "publishedOn": "2021-07-27T02:03:34.961Z",
          "wordCount": 623,
          "title": "Low-Dose CT Denoising Using a Structure-Preserving Kernel Prediction Network. (arXiv:2105.14758v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuechao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruzhansky_M/0/1/0/all/0/1\">Michael Ruzhansky</a>",
          "description": "In this paper, we propose an interesting semi-sparsity smoothing algorithm\nbased on a novel sparsity-inducing optimization framework. This method is\nderived from the multiple observations, that is, semi-sparsity prior knowledge\nis more universally applicable, especially in areas where sparsity is not fully\nadmitted, such as polynomial-smoothing surfaces. We illustrate that this\nsemi-sparsity can be identified into a generalized $L_0$-norm minimization in\nhigher-order gradient domains, thereby giving rise to a new \"feature-aware\"\nfiltering method with a powerful simultaneous-fitting ability in both sparse\nfeatures (singularities and sharpening edges) and non-sparse regions\n(polynomial-smoothing surfaces). Notice that a direct solver is always\nunavailable due to the non-convexity and combinatorial nature of $L_0$-norm\nminimization. Instead, we solve the model based on an efficient half-quadratic\nsplitting minimization with fast Fourier transforms (FFTs) for acceleration. We\nfinally demonstrate its versatility and many benefits to a series of\nsignal/image processing and computer vision applications.",
          "link": "http://arxiv.org/abs/2107.00627",
          "publishedOn": "2021-07-27T02:03:34.955Z",
          "wordCount": 590,
          "title": "Semi-Sparsity for Smoothing Filters. (arXiv:2107.00627v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07761",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mascolini_A/0/1/0/all/0/1\">Alessio Mascolini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardamone_D/0/1/0/all/0/1\">Dario Cardamone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponzio_F/0/1/0/all/0/1\">Francesco Ponzio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cataldo_S/0/1/0/all/0/1\">Santa Di Cataldo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ficarra_E/0/1/0/all/0/1\">Elisa Ficarra</a>",
          "description": "Computer-aided analysis of biological images typically requires extensive\ntraining on large-scale annotated datasets, which is not viable in many\nsituations. In this paper we present GAN-DL, a Discriminator Learner based on\nthe StyleGAN2 architecture, which we employ for self-supervised image\nrepresentation learning in the case of fluorescent biological images. We show\nthat Wasserstein Generative Adversarial Networks combined with linear Support\nVector Machines enable high-throughput compound screening based on raw images.\nWe demonstrate this by classifying active and inactive compounds tested for the\ninhibition of SARS-CoV-2 infection in VERO and HRCE cell lines. In contrast to\nprevious methods, our deep learning based approach does not require any\nannotation besides the one that is normally collected during the sample\npreparation process. We test our technique on the RxRx19a Sars-CoV-2 image\ncollection. The dataset consists of fluorescent images that were generated to\nassess the ability of regulatory-approved or in late-stage clinical trials\ncompound to modulate the in vitro infection from SARS-CoV-2 in both VERO and\nHRCE cell lines. We show that our technique can be exploited not only for\nclassification tasks, but also to effectively derive a dose response curve for\nthe tested treatments, in a self-supervised manner. Lastly, we demonstrate its\ngeneralization capabilities by successfully addressing a zero-shot learning\ntask, consisting in the categorization of four different cell types of the\nRxRx1 fluorescent images collection.",
          "link": "http://arxiv.org/abs/2107.07761",
          "publishedOn": "2021-07-27T02:03:34.948Z",
          "wordCount": 750,
          "title": "Exploiting generative self-supervised learning for the assessment of biological images with lack of annotations: a COVID-19 case-study. (arXiv:2107.07761v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Um_S/0/1/0/all/0/1\">Se-Yun Um</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sangshin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_K/0/1/0/all/0/1\">Kyungguen Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hong-Goo Kang</a>",
          "description": "In this paper, we propose an effective method to synthesize speaker-specific\nspeech waveforms by conditioning on videos of an individual's face. Using a\ngenerative adversarial network (GAN) with linguistic and speaker characteristic\nfeatures as auxiliary conditions, our method directly converts face images into\nspeech waveforms under an end-to-end training framework. The linguistic\nfeatures are extracted from lip movements using a lip-reading model, and the\nspeaker characteristic features are predicted from face images using\ncross-modal learning with a pre-trained acoustic model. Since these two\nfeatures are uncorrelated and controlled independently, we can flexibly\nsynthesize speech waveforms whose speaker characteristics vary depending on the\ninput face images. Therefore, our method can be regarded as a multi-speaker\nface-to-speech waveform model. We show the superiority of our proposed model\nover conventional methods in terms of both objective and subjective evaluation\nresults. Specifically, we evaluate the performances of the linguistic feature\nand the speaker characteristic generation modules by measuring the accuracy of\nautomatic speech recognition and automatic speaker/gender recognition tasks,\nrespectively. We also evaluate the naturalness of the synthesized speech\nwaveforms using a mean opinion score (MOS) test.",
          "link": "http://arxiv.org/abs/2107.12003",
          "publishedOn": "2021-07-27T02:03:34.509Z",
          "wordCount": 646,
          "title": "Facetron: Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations. (arXiv:2107.12003v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Peng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shi Gu</a>",
          "description": "We study the challenging task of neural network quantization without\nend-to-end retraining, called Post-training Quantization (PTQ). PTQ usually\nrequires a small subset of training data but produces less powerful quantized\nmodels than Quantization-Aware Training (QAT). In this work, we propose a novel\nPTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to\nINT2 for the first time. BRECQ leverages the basic building blocks in neural\nnetworks and reconstructs them one-by-one. In a comprehensive theoretical study\nof the second-order error, we show that BRECQ achieves a good balance between\ncross-layer dependency and generalization error. To further employ the power of\nquantization, the mixed precision technique is incorporated in our framework by\napproximating the inter-layer and intra-layer sensitivity. Extensive\nexperiments on various handcrafted and searched neural architectures are\nconducted for both image classification and object detection tasks. And for the\nfirst time we prove that, without bells and whistles, PTQ can attain 4-bit\nResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster\nproduction of quantized models. Codes are available at\nhttps://github.com/yhhhli/BRECQ.",
          "link": "http://arxiv.org/abs/2102.05426",
          "publishedOn": "2021-07-27T02:03:34.429Z",
          "wordCount": 658,
          "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction. (arXiv:2102.05426v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1\">Tianmin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1\">Abhishek Bhandwaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kevin A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shari Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1\">Dan Gutfreund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spelke_E/0/1/0/all/0/1\">Elizabeth Spelke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1\">Tomer D. Ullman</a>",
          "description": "For machine agents to successfully interact with humans in real-world\nsettings, they will need to develop an understanding of human mental life.\nIntuitive psychology, the ability to reason about hidden mental variables that\ndrive observable actions, comes naturally to people: even pre-verbal infants\ncan tell agents from objects, expecting agents to act efficiently to achieve\ngoals given constraints. Despite recent interest in machine agents that reason\nabout other agents, it is not clear if such agents learn or hold the core\npsychology principles that drive human reasoning. Inspired by cognitive\ndevelopment studies on intuitive psychology, we present a benchmark consisting\nof a large dataset of procedurally generated 3D animations, AGENT (Action,\nGoal, Efficiency, coNstraint, uTility), structured around four scenarios (goal\npreferences, action efficiency, unobserved constraints, and cost-reward\ntrade-offs) that probe key concepts of core intuitive psychology. We validate\nAGENT with human-ratings, propose an evaluation protocol emphasizing\ngeneralization, and compare two strong baselines built on Bayesian inverse\nplanning and a Theory of Mind neural network. Our results suggest that to pass\nthe designed tests of core intuitive psychology at human levels, a model must\nacquire or have built-in representations of how agents plan, combining utility\ncomputations and core knowledge of objects and physics.",
          "link": "http://arxiv.org/abs/2102.12321",
          "publishedOn": "2021-07-27T02:03:34.422Z",
          "wordCount": 712,
          "title": "AGENT: A Benchmark for Core Psychological Reasoning. (arXiv:2102.12321v4 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14659",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Andrews_M/0/1/0/all/0/1\">Michael Andrews</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Burkle_B/0/1/0/all/0/1\">Bjorn Burkle</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-fan Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+DiCroce_D/0/1/0/all/0/1\">Davide DiCroce</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gleyzer_S/0/1/0/all/0/1\">Sergei Gleyzer</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Heintz_U/0/1/0/all/0/1\">Ulrich Heintz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Narain_M/0/1/0/all/0/1\">Meenakshi Narain</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Paulini_M/0/1/0/all/0/1\">Manfred Paulini</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pervan_N/0/1/0/all/0/1\">Nikolas Pervan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shafi_Y/0/1/0/all/0/1\">Yusef Shafi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Usai_E/0/1/0/all/0/1\">Emanuele Usai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_K/0/1/0/all/0/1\">Kun Yang</a>",
          "description": "We describe a novel application of the end-to-end deep learning technique to\nthe task of discriminating top quark-initiated jets from those originating from\nthe hadronization of a light quark or a gluon. The end-to-end deep learning\ntechnique combines deep learning algorithms and low-level detector\nrepresentation of the high-energy collision event. In this study, we use\nlow-level detector information from the simulated CMS Open Data samples to\nconstruct the top jet classifiers. To optimize classifier performance we\nprogressively add low-level information from the CMS tracking detector,\nincluding pixel detector reconstructed hits and impact parameters, and\ndemonstrate the value of additional tracking information even when no new\nspatial structures are added. Relying only on calorimeter energy deposits and\nreconstructed pixel detector hits, the end-to-end classifier achieves an AUC\nscore of 0.975$\\pm$0.002 for the task of classifying boosted top quark jets.\nAfter adding derived track quantities, the classifier AUC score increases to\n0.9824$\\pm$0.0013, serving as the first performance benchmark for these CMS\nOpen Data samples. We additionally provide a timing performance comparison of\ndifferent processor unit architectures for training the network.",
          "link": "http://arxiv.org/abs/2104.14659",
          "publishedOn": "2021-07-27T02:03:34.413Z",
          "wordCount": 685,
          "title": "End-to-End Jet Classification of Boosted Top Quarks with the CMS Open Data. (arXiv:2104.14659v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1\">Abdelrahman Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadhoud_M/0/1/0/all/0/1\">Mayada Hadhoud</a>",
          "description": "Instance segmentation has gained recently huge attention in various computer\nvision applications. It aims at providing different IDs to different objects of\nthe scene, even if they belong to the same class. Instance segmentation is\nusually performed as a two-stage pipeline. First, an object is detected, then\nsemantic segmentation within the detected box area is performed which involves\ncostly up-sampling. In this paper, we propose Insta-YOLO, a novel one-stage\nend-to-end deep learning model for real-time instance segmentation. Instead of\npixel-wise prediction, our model predicts instances as object contours\nrepresented by 2D points in Cartesian space. We evaluate our model on three\ndatasets, namely, Carvana,Cityscapes and Airbus. We compare our results to the\nstate-of-the-art models for instance segmentation. The results show our model\nachieves competitive accuracy in terms of mAP at twice the speed on GTX-1080\nGPU.",
          "link": "http://arxiv.org/abs/2102.06777",
          "publishedOn": "2021-07-27T02:03:34.406Z",
          "wordCount": 596,
          "title": "INSTA-YOLO: Real-Time Instance Segmentation. (arXiv:2102.06777v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1\">Inigo Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferstl_D/0/1/0/all/0/1\">David Ferstl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>",
          "description": "This work presents a novel approach for semi-supervised semantic\nsegmentation. The key element of this approach is our contrastive learning\nmodule that enforces the segmentation network to yield similar pixel-level\nfeature representations for same-class samples across the whole dataset. To\nachieve this, we maintain a memory bank continuously updated with relevant and\nhigh-quality feature vectors from labeled data. In an end-to-end training, the\nfeatures from both labeled and unlabeled data are optimized to be similar to\nsame-class samples from the memory bank. Our approach outperforms the current\nstate-of-the-art for semi-supervised semantic segmentation and semi-supervised\ndomain adaptation on well-known public benchmarks, with larger improvements on\nthe most challenging scenarios, i.e., less available labeled data.\nhttps://github.com/Shathe/SemiSeg-Contrastive",
          "link": "http://arxiv.org/abs/2104.13415",
          "publishedOn": "2021-07-27T02:03:34.374Z",
          "wordCount": 601,
          "title": "Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank. (arXiv:2104.13415v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Z/0/1/0/all/0/1\">Zhibin Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_M/0/1/0/all/0/1\">Mu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wankou Yang</a>",
          "description": "While CNN-based models have made remarkable progress on human pose\nestimation, what spatial dependencies they capture to localize keypoints\nremains unclear. In this work, we propose a model called \\textbf{TransPose},\nwhich introduces Transformer for human pose estimation. The attention layers\nbuilt in Transformer enable our model to capture long-range relationships\nefficiently and also can reveal what dependencies the predicted keypoints rely\non. To predict keypoint heatmaps, the last attention layer specially acts as an\naggregator, which collects contributions from image clues and forms maximum\npositions of keypoints. Such a heatmap-based localization approach via\nTransformer conforms to the principle of Activation Maximization\n\\cite{erhan2009visualizing}. And the revealed dependencies are image-specific\nand fine-grained, which also can provide evidence of how the model handles\nspecial cases, e.g., occlusion. The experiments show that TransPose achieves\n75.8 AP and 75.0 AP on COCO validation and test-dev sets with 256 $\\times$ 192\ninput resolution, while being more lightweight and faster than mainstream CNN\narchitectures. The TransPose model also transfers very well on MPII benchmark,\nyielding 93.9\\% accuracy on test set when fine-tuned with small training costs.\nCode and pre-trained models are publicly available at\n\\url{https://github.com/yangsenius/TransPose}.",
          "link": "http://arxiv.org/abs/2012.14214",
          "publishedOn": "2021-07-27T02:03:34.367Z",
          "wordCount": 662,
          "title": "TransPose: Keypoint Localization via Transformer. (arXiv:2012.14214v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Anchal Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moharana_S/0/1/0/all/0/1\">Sukumar Moharana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_D/0/1/0/all/0/1\">Debi Prasanna Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panwar_A/0/1/0/all/0/1\">Archit Panwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Dewang Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thota_S/0/1/0/all/0/1\">Siva Prasad Thota</a>",
          "description": "With the advent of internet, not safe for work(NSFW) content moderation is a\nmajor problem today. Since,smartphones are now part of daily life of billions\nof people,it becomes even more important to have a solution which coulddetect\nand suggest user about potential NSFW content present ontheir phone. In this\npaper we present a novel on-device solutionfor detecting NSFW images. In\naddition to conventional porno-graphic content moderation, we have also\nincluded semi-nudecontent moderation as it is still NSFW in a large\ndemography.We have curated a dataset comprising of three major\ncategories,namely nude, semi-nude and safe images. We have created anensemble\nof object detector and classifier for filtering of nudeand semi-nude contents.\nThe solution provides unsafe body partannotations along with identification of\nsemi-nude images. Weextensively tested our proposed solution on several public\ndatasetand also on our custom dataset. The model achieves F1 scoreof 0.91 with\n95% precision and 88% recall on our customNSFW16k dataset and 0.92 MAP on NPDI\ndataset. Moreover itachieves average 0.002 false positive rate on a collection\nof safeimage open datasets.",
          "link": "http://arxiv.org/abs/2107.11845",
          "publishedOn": "2021-07-27T02:03:34.345Z",
          "wordCount": 609,
          "title": "On-Device Content Moderation. (arXiv:2107.11845v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Fan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinlong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Sanqing Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Rongqi Gu</a>",
          "description": "Point cloud registration is a fundamental problem in 3D computer vision.\nOutdoor LiDAR point clouds are typically large-scale and complexly distributed,\nwhich makes the registration challenging. In this paper, we propose an\nefficient hierarchical network named HRegNet for large-scale outdoor LiDAR\npoint cloud registration. Instead of using all points in the point clouds,\nHRegNet performs registration on hierarchically extracted keypoints and\ndescriptors. The overall framework combines the reliable features in deeper\nlayer and the precise position information in shallower layers to achieve\nrobust and precise registration. We present a correspondence network to\ngenerate correct and accurate keypoints correspondences. Moreover, bilateral\nconsensus and neighborhood consensus are introduced for keypoints matching and\nnovel similarity features are designed to incorporate them into the\ncorrespondence network, which significantly improves the registration\nperformance. Besides, the whole network is also highly efficient since only a\nsmall number of keypoints are used for registration. Extensive experiments are\nconducted on two large-scale outdoor LiDAR point cloud datasets to demonstrate\nthe high accuracy and efficiency of the proposed HRegNet. The project website\nis https://ispc-group.github.io/hregnet.",
          "link": "http://arxiv.org/abs/2107.11992",
          "publishedOn": "2021-07-27T02:03:34.339Z",
          "wordCount": 629,
          "title": "HRegNet: A Hierarchical Network for Large-scale Outdoor LiDAR Point Cloud Registration. (arXiv:2107.11992v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.04902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Lun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wei-Sheng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Yu Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>",
          "description": "We present a learning-based approach for removing unwanted obstructions, such\nas window reflections, fence occlusions, or adherent raindrops, from a short\nsequence of images captured by a moving camera. Our method leverages motion\ndifferences between the background and obstructing elements to recover both\nlayers. Specifically, we alternate between estimating dense optical flow fields\nof the two layers and reconstructing each layer from the flow-warped images via\na deep convolutional neural network. This learning-based layer reconstruction\nmodule facilitates accommodating potential errors in the flow estimation and\nbrittle assumptions, such as brightness consistency. We show that the proposed\napproach learned from synthetically generated data performs well to real\nimages. Experimental results on numerous challenging scenarios of reflection\nand fence removal demonstrate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2008.04902",
          "publishedOn": "2021-07-27T02:03:34.319Z",
          "wordCount": 625,
          "title": "Learning to See Through Obstructions with Layered Decomposition. (arXiv:2008.04902v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nandy_J/0/1/0/all/0/1\">Jay Nandy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wynne Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mong Li Lee</a>",
          "description": "Deep learning-based models are developed to automatically detect if a retina\nimage is `referable' in diabetic retinopathy (DR) screening. However, their\nclassification accuracy degrades as the input images distributionally shift\nfrom their training distribution. Further, even if the input is not a retina\nimage, a standard DR classifier produces a high confident prediction that the\nimage is `referable'. Our paper presents a Dirichlet Prior Network-based\nframework to address this issue. It utilizes an out-of-distribution (OOD)\ndetector model and a DR classification model to improve generalizability by\nidentifying OOD images. Experiments on real-world datasets indicate that the\nproposed framework can eliminate the unknown non-retina images and identify the\ndistributionally shifted retina images for human intervention.",
          "link": "http://arxiv.org/abs/2107.11822",
          "publishedOn": "2021-07-27T02:03:34.304Z",
          "wordCount": 560,
          "title": "Distributional Shifts in Automated Diabetic Retinopathy Screening. (arXiv:2107.11822v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qiang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shichao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhida Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>",
          "description": "The performance of face recognition system degrades when the variability of\nthe acquired faces increases. Prior work alleviates this issue by either\nmonitoring the face quality in pre-processing or predicting the data\nuncertainty along with the face feature. This paper proposes MagFace, a\ncategory of losses that learn a universal feature embedding whose magnitude can\nmeasure the quality of the given face. Under the new loss, it can be proven\nthat the magnitude of the feature embedding monotonically increases if the\nsubject is more likely to be recognized. In addition, MagFace introduces an\nadaptive mechanism to learn a wellstructured within-class feature distributions\nby pulling easy samples to class centers while pushing hard samples away. This\nprevents models from overfitting on noisy low-quality samples and improves face\nrecognition in the wild. Extensive experiments conducted on face recognition,\nquality assessments as well as clustering demonstrate its superiority over\nstate-of-the-arts. The code is available at\nhttps://github.com/IrvingMeng/MagFace.",
          "link": "http://arxiv.org/abs/2103.06627",
          "publishedOn": "2021-07-27T02:03:34.267Z",
          "wordCount": 659,
          "title": "MagFace: A Universal Representation for Face Recognition and Quality Assessment. (arXiv:2103.06627v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keren Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qijun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>",
          "description": "Salient object detection (SOD) is a long-standing research topic in computer\nvision and has drawn an increasing amount of research interest in the past\ndecade. This paper provides the first comprehensive review and benchmark for\nlight field SOD, which has long been lacking in the saliency community.\nFirstly, we introduce preliminary knowledge on light fields, including theory\nand data forms, and then review existing studies on light field SOD, covering\nten traditional models, seven deep learning-based models, one comparative\nstudy, and one brief review. Existing datasets for light field SOD are also\nsummarized with detailed information and statistical analyses. Secondly, we\nbenchmark nine representative light field SOD models together with several\ncutting-edge RGB-D SOD models on four widely used light field datasets, from\nwhich insightful discussions and analyses, including a comparison between light\nfield SOD and RGB-D SOD models, are achieved. Besides, due to the inconsistency\nof datasets in their current forms, we further generate complete data and\nsupplement focal stacks, depth maps and multi-view images for the inconsistent\ndatasets, making them consistent and unified. Our supplemental data makes a\nuniversal benchmark possible. Lastly, because light field SOD is quite a\nspecial problem attributed to its diverse data representations and high\ndependency on acquisition hardware, making it differ greatly from other\nsaliency detection tasks, we provide nine hints into the challenges and future\ndirections, and outline several open issues. We hope our review and\nbenchmarking could help advance research in this field. All the materials\nincluding collected models, datasets, benchmarking results, and supplemented\nlight field datasets will be publicly available on our project site\nhttps://github.com/kerenfu/LFSOD-Survey.",
          "link": "http://arxiv.org/abs/2010.04968",
          "publishedOn": "2021-07-27T02:03:34.219Z",
          "wordCount": 756,
          "title": "Light Field Salient Object Detection: A Review and Benchmark. (arXiv:2010.04968v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sixiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiachen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zekun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>",
          "description": "Most recent semantic segmentation methods adopt a fully-convolutional network\n(FCN) with an encoder-decoder architecture. The encoder progressively reduces\nthe spatial resolution and learns more abstract/semantic visual concepts with\nlarger receptive fields. Since context modeling is critical for segmentation,\nthe latest efforts have been focused on increasing the receptive field, through\neither dilated/atrous convolutions or inserting attention modules. However, the\nencoder-decoder based FCN architecture remains unchanged. In this paper, we aim\nto provide an alternative perspective by treating semantic segmentation as a\nsequence-to-sequence prediction task. Specifically, we deploy a pure\ntransformer (ie, without convolution and resolution reduction) to encode an\nimage as a sequence of patches. With the global context modeled in every layer\nof the transformer, this encoder can be combined with a simple decoder to\nprovide a powerful segmentation model, termed SEgmentation TRansformer (SETR).\nExtensive experiments show that SETR achieves new state of the art on ADE20K\n(50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on\nCityscapes. Particularly, we achieve the first position in the highly\ncompetitive ADE20K test server leaderboard on the day of submission.",
          "link": "http://arxiv.org/abs/2012.15840",
          "publishedOn": "2021-07-27T02:03:34.197Z",
          "wordCount": 679,
          "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. (arXiv:2012.15840v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qihui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1\">Qi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">MengChu Zhou</a>",
          "description": "In most interactive image generation tasks, given regions of interest (ROI)\nby users, the generated results are expected to have adequate diversities in\nappearance while maintaining correct and reasonable structures in original\nimages. Such tasks become more challenging if only limited data is available.\nRecently proposed generative models complete training based on only one image.\nThey pay much attention to the monolithic feature of the sample while ignoring\nthe actual semantic information of different objects inside the sample. As a\nresult, for ROI-based generation tasks, they may produce inappropriate samples\nwith excessive randomicity and without maintaining the related objects' correct\nstructures. To address this issue, this work introduces a\nMOrphologic-structure-aware Generative Adversarial Network named MOGAN that\nproduces random samples with diverse appearances and reliable structures based\non only one image. For training for ROI, we propose to utilize the data coming\nfrom the original image being augmented and bring in a novel module to\ntransform such augmented data into knowledge containing both structures and\nappearances, thus enhancing the model's comprehension of the sample. To learn\nthe rest areas other than ROI, we employ binary masks to ensure the generation\nisolated from ROI. Finally, we set parallel and hierarchical branches of the\nmentioned learning process. Compared with other single image GAN schemes, our\napproach focuses on internal features including the maintenance of rational\nstructures and variation on appearance. Experiments confirm a better capacity\nof our model on ROI-based image generation tasks than its competitive peers.",
          "link": "http://arxiv.org/abs/2103.02997",
          "publishedOn": "2021-07-27T02:03:34.190Z",
          "wordCount": 706,
          "title": "MOGAN: Morphologic-structure-aware Generative Learning from a Single Image. (arXiv:2103.02997v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imran_S/0/1/0/all/0/1\">Saif Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1\">Daniel Morris</a>",
          "description": "Depth completion starts from a sparse set of known depth values and estimates\nthe unknown depths for the remaining image pixels. Most methods model this as\ndepth interpolation and erroneously interpolate depth pixels into the empty\nspace between spatially distinct objects, resulting in depth-smearing across\nocclusion boundaries. Here we propose a multi-hypothesis depth representation\nthat explicitly models both foreground and background depths in the difficult\nocclusion-boundary regions. Our method can be thought of as performing\ntwin-surface extrapolation, rather than interpolation, in these regions. Next\nour method fuses these extrapolated surfaces into a single depth image\nleveraging the image data. Key to our method is the use of an asymmetric loss\nfunction that operates on a novel twin-surface representation. This enables us\nto train a network to simultaneously do surface extrapolation and surface\nfusion. We characterize our loss function and compare with other common losses.\nFinally, we validate our method on three different datasets; KITTI, an outdoor\nreal-world dataset, NYU2, indoor real-world depth dataset and Virtual KITTI, a\nphoto-realistic synthetic dataset with dense groundtruth, and demonstrate\nimprovement over the state of the art.",
          "link": "http://arxiv.org/abs/2104.02253",
          "publishedOn": "2021-07-27T02:03:34.182Z",
          "wordCount": 669,
          "title": "Depth Completion with Twin Surface Extrapolation at Occlusion Boundaries. (arXiv:2104.02253v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Pranjal Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goenka_S/0/1/0/all/0/1\">Shreyas Goenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Saurabh Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaterji_S/0/1/0/all/0/1\">Somali Chaterji</a>",
          "description": "Federated learning allows a large number of devices to jointly learn a model\nwithout sharing data. In this work, we enable clients with limited computing\npower to perform action recognition, a computationally heavy task. We first\nperform model compression at the central server through knowledge distillation\non a large dataset. This allows the model to learn complex features and serves\nas an initialization for model fine-tuning. The fine-tuning is required because\nthe limited data present in smaller datasets is not adequate for action\nrecognition models to learn complex spatio-temporal features. Because the\nclients present are often heterogeneous in their computing resources, we use an\nasynchronous federated optimization and we further show a convergence bound. We\ncompare our approach to two baseline approaches: fine-tuning at the central\nserver (no clients) and fine-tuning using (heterogeneous) clients using\nsynchronous federated averaging. We empirically show on a testbed of\nheterogeneous embedded devices that we can perform action recognition with\ncomparable accuracy to the two baselines above, while our asynchronous learning\nstrategy reduces the training time by 40%, relative to synchronous learning.",
          "link": "http://arxiv.org/abs/2107.12147",
          "publishedOn": "2021-07-27T02:03:34.173Z",
          "wordCount": 632,
          "title": "Federated Action Recognition on Heterogeneous Embedded Devices. (arXiv:2107.12147v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neimark_D/0/1/0/all/0/1\">Daniel Neimark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_O/0/1/0/all/0/1\">Omri Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zohar_M/0/1/0/all/0/1\">Maya Zohar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asselmann_D/0/1/0/all/0/1\">Dotan Asselmann</a>",
          "description": "This paper presents VTN, a transformer-based framework for video recognition.\nInspired by recent developments in vision transformers, we ditch the standard\napproach in video action recognition that relies on 3D ConvNets and introduce a\nmethod that classifies actions by attending to the entire video sequence\ninformation. Our approach is generic and builds on top of any given 2D spatial\nnetwork. In terms of wall runtime, it trains $16.1\\times$ faster and runs\n$5.1\\times$ faster during inference while maintaining competitive accuracy\ncompared to other state-of-the-art methods. It enables whole video analysis,\nvia a single end-to-end pass, while requiring $1.5\\times$ fewer GFLOPs. We\nreport competitive results on Kinetics-400 and present an ablation study of VTN\nproperties and the trade-off between accuracy and inference speed. We hope our\napproach will serve as a new baseline and start a fresh line of research in the\nvideo recognition domain. Code and models are available at:\nhttps://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md",
          "link": "http://arxiv.org/abs/2102.00719",
          "publishedOn": "2021-07-27T02:03:34.155Z",
          "wordCount": 604,
          "title": "Video Transformer Network. (arXiv:2102.00719v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "Learning a good representation for space-time correspondence is the key for\nvarious computer vision tasks, including tracking object bounding boxes and\nperforming video object pixel segmentation. To learn generalizable\nrepresentation for correspondence in large-scale, a variety of self-supervised\npretext tasks are proposed to explicitly perform object-level or patch-level\nsimilarity learning. Instead of following the previous literature, we propose\nto learn correspondence using Video Frame-level Similarity (VFS) learning, i.e,\nsimply learning from comparing video frames. Our work is inspired by the recent\nsuccess in image-level contrastive learning and similarity learning for visual\nrecognition. Our hypothesis is that if the representation is good for\nrecognition, it requires the convolutional features to find correspondence\nbetween similar objects or parts. Our experiments show surprising results that\nVFS surpasses state-of-the-art self-supervised approaches for both OTB visual\nobject tracking and DAVIS video object segmentation. We perform detailed\nanalysis on what matters in VFS and reveals new properties on image and frame\nlevel similarity learning. Project page is available at https://jerryxu.net/VFS",
          "link": "http://arxiv.org/abs/2103.17263",
          "publishedOn": "2021-07-27T02:03:34.148Z",
          "wordCount": 654,
          "title": "Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective. (arXiv:2103.17263v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Adversarial examples contain carefully crafted perturbations that can fool\ndeep neural networks (DNNs) into making wrong predictions. Enhancing the\nadversarial robustness of DNNs has gained considerable interest in recent\nyears. Although image transformation-based defenses were widely considered at\nan earlier time, most of them have been defeated by adaptive attacks. In this\npaper, we propose a new image transformation defense based on error diffusion\nhalftoning, and combine it with adversarial training to defend against\nadversarial examples. Error diffusion halftoning projects an image into a 1-bit\nspace and diffuses quantization error to neighboring pixels. This process can\nremove adversarial perturbations from a given image while maintaining\nacceptable image quality in the meantime in favor of recognition. Experimental\nresults demonstrate that the proposed method is able to improve adversarial\nrobustness even under advanced adaptive attacks, while most of the other image\ntransformation-based defenses do not. We show that a proper image\ntransformation can still be an effective defense approach. Code:\nhttps://github.com/shaoyuanlo/Halftoning-Defense",
          "link": "http://arxiv.org/abs/2101.09451",
          "publishedOn": "2021-07-27T02:03:34.140Z",
          "wordCount": 654,
          "title": "Error Diffusion Halftoning Against Adversarial Examples. (arXiv:2101.09451v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05245",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1\">Delong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_S/0/1/0/all/0/1\">Shunhui Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zewen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xinyu Zhou</a>",
          "description": "The pandemic of COVID-19 has caused millions of infections, which has led to\na great loss all over the world, socially and economically. Due to the\nfalse-negative rate and the time-consuming of the conventional Reverse\nTranscription Polymerase Chain Reaction (RT-PCR) tests, diagnosing based on\nX-ray images and Computed Tomography (CT) images has been widely adopted.\nTherefore, researchers of the computer vision area have developed many\nautomatic diagnosing models based on machine learning or deep learning to\nassist the radiologists and improve the diagnosing accuracy. In this paper, we\npresent a review of these recently emerging automatic diagnosing models. 70\nmodels proposed from February 14, 2020, to July 21, 2020, are involved. We\nanalyzed the models from the perspective of preprocessing, feature extraction,\nclassification, and evaluation. Based on the limitation of existing models, we\npointed out that domain adaption in transfer learning and interpretability\npromotion would be the possible future directions.",
          "link": "http://arxiv.org/abs/2006.05245",
          "publishedOn": "2021-07-27T02:03:34.133Z",
          "wordCount": 691,
          "title": "A Review of Automated Diagnosis of COVID-19 Based on Scanning Images. (arXiv:2006.05245v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1\">Fangbo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Haonan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bly_R/0/1/0/all/0/1\">Randall A. Bly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moe_K/0/1/0/all/0/1\">Kris S. Moe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hannaford_B/0/1/0/all/0/1\">Blake Hannaford</a>",
          "description": "Deep learning-based methods have achieved promising results on surgical\ninstrument segmentation. However, the high computation cost may limit the\napplication of deep models to time-sensitive tasks such as online surgical\nvideo analysis for robotic-assisted surgery. Moreover, current methods may\nstill suffer from challenging conditions in surgical images such as various\nlighting conditions and the presence of blood. We propose a novel Multi-frame\nFeature Aggregation (MFFA) module to aggregate video frame features temporally\nand spatially in a recurrent mode. By distributing the computation load of deep\nfeature extraction over sequential frames, we can use a lightweight encoder to\nreduce the computation costs at each time step. Moreover, public surgical\nvideos usually are not labeled frame by frame, so we develop a method that can\nrandomly synthesize a surgical frame sequence from a single labeled frame to\nassist network training. We demonstrate that our approach achieves superior\nperformance to corresponding deeper segmentation models on two public surgery\ndatasets.",
          "link": "http://arxiv.org/abs/2011.08752",
          "publishedOn": "2021-07-27T02:03:34.126Z",
          "wordCount": 643,
          "title": "Multi-frame Feature Aggregation for Real-time Instrument Segmentation in Endoscopic Video. (arXiv:2011.08752v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1\">Bart Smets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1\">Jim Portegies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1\">Remco Duits</a>",
          "description": "We present a PDE-based framework that generalizes Group equivariant\nConvolutional Neural Networks (G-CNNs). In this framework, a network layer is\nseen as a set of PDE-solvers where geometrically meaningful PDE-coefficients\nbecome the layer's trainable weights. Formulating our PDEs on homogeneous\nspaces allows these networks to be designed with built-in symmetries such as\nrotation in addition to the standard translation equivariance of CNNs.\n\nHaving all the desired symmetries included in the design obviates the need to\ninclude them by means of costly techniques such as data augmentation. We will\ndiscuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space\nsetting while also going into the specifics of our primary case of interest:\nroto-translation equivariance.\n\nWe solve the PDE of interest by a combination of linear group convolutions\nand non-linear morphological group convolutions with analytic kernel\napproximations that we underpin with formal theorems. Our kernel approximations\nallow for fast GPU-implementation of the PDE-solvers, we release our\nimplementation with this article. Just like for linear convolution a\nmorphological convolution is specified by a kernel that we train in our\nPDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling\nand ReLUs as they are already subsumed by morphological convolutions.\n\nWe present a set of experiments to demonstrate the strength of the proposed\nPDE-G-CNNs in increasing the performance of deep learning based imaging\napplications with far fewer parameters than traditional CNNs.",
          "link": "http://arxiv.org/abs/2001.09046",
          "publishedOn": "2021-07-27T02:03:34.102Z",
          "wordCount": 767,
          "title": "PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03769",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schutte_A/0/1/0/all/0/1\">August DuMont Sch&#xfc;tte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hetzel_J/0/1/0/all/0/1\">J&#xfc;rgen Hetzel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hepp_T/0/1/0/all/0/1\">Tobias Hepp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dietz_B/0/1/0/all/0/1\">Benedikt Dietz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwab_P/0/1/0/all/0/1\">Patrick Schwab</a>",
          "description": "Privacy concerns around sharing personally identifiable information are a\nmajor practical barrier to data sharing in medical research. However, in many\ncases, researchers have no interest in a particular individual's information\nbut rather aim to derive insights at the level of cohorts. Here, we utilize\nGenerative Adversarial Networks (GANs) to create derived medical imaging\ndatasets consisting entirely of synthetic patient data. The synthetic images\nideally have, in aggregate, similar statistical properties to those of a source\ndataset but do not contain sensitive personal information. We assess the\nquality of synthetic data generated by two GAN models for chest radiographs\nwith 14 different radiology findings and brain computed tomography (CT) scans\nwith six types of intracranial hemorrhages. We measure the synthetic image\nquality by the performance difference of predictive models trained on either\nthe synthetic or the real dataset. We find that synthetic data performance\ndisproportionately benefits from a reduced number of unique label combinations.\nOur open-source benchmark also indicates that at low number of samples per\nclass, label overfitting effects start to dominate GAN training. We\nadditionally conducted a reader study in which trained radiologists do not\nperform better than random on discriminating between synthetic and real medical\nimages for intermediate levels of resolutions. In accordance with our benchmark\nresults, the classification accuracy of radiologists increases at higher\nspatial resolution levels. Our study offers valuable guidelines and outlines\npractical conditions under which insights derived from synthetic medical images\nare similar to those that would have been derived from real imaging data. Our\nresults indicate that synthetic data sharing may be an attractive and\nprivacy-preserving alternative to sharing real patient-level data in the right\nsettings.",
          "link": "http://arxiv.org/abs/2012.03769",
          "publishedOn": "2021-07-27T02:03:34.095Z",
          "wordCount": 752,
          "title": "Overcoming Barriers to Data Sharing with Medical Image Generation: A Comprehensive Evaluation. (arXiv:2012.03769v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diao_Y/0/1/0/all/0/1\">Yunfeng Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1\">Tianjia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong-Liang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>",
          "description": "Skeletal motion plays a vital role in human activity recognition as either an\nindependent data source or a complement. The robustness of skeleton-based\nactivity recognizers has been questioned recently, which shows that they are\nvulnerable to adversarial attacks when the full-knowledge of the recognizer is\naccessible to the attacker. However, this white-box requirement is overly\nrestrictive in most scenarios and the attack is not truly threatening. In this\npaper, we show that such threats do exist under black-box settings too. To this\nend, we propose the first black-box adversarial attack method BASAR. Through\nBASAR, we show that adversarial attack is not only truly a threat but also can\nbe extremely deceitful, because on-manifold adversarial samples are rather\ncommon in skeletal motions, in contrast to the common belief that adversarial\nsamples only exist off-manifold. Through exhaustive evaluation and comparison,\nwe show that BASAR can deliver successful attacks across models, data, and\nattack modes. Through harsh perceptual studies, we show that it achieves\neffective yet imperceptible attacks. By analyzing the attack on different\nactivity recognizers, BASAR helps identify the potential causes of their\nvulnerability and provides insights on what classifiers are likely to be more\nrobust against attack. Code is available at\nhttps://github.com/realcrane/BASAR-Black-box-Attack-on-Skeletal-Action-Recognition.",
          "link": "http://arxiv.org/abs/2103.05266",
          "publishedOn": "2021-07-27T02:03:34.087Z",
          "wordCount": 709,
          "title": "BASAR:Black-box Attack on Skeletal Action Recognition. (arXiv:2103.05266v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.11265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1\">Ashkan Abbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monadjemi_A/0/1/0/all/0/1\">Amirhassan Monadjemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Leyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbani_H/0/1/0/all/0/1\">Hossein Rabbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noormohammadi_N/0/1/0/all/0/1\">Neda Noormohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>",
          "description": "The data-driven sparse methods such as synthesis dictionary learning (e.g.,\nK-SVD) and sparsifying transform learning have been proven effective in image\ndenoising. However, they are intrinsically single-scale which can lead to\nsuboptimal results. We propose two methods developed based on wavelet subbands\nmixing to efficiently combine the merits of both single and multiscale methods.\nWe show that an efficient multiscale method can be devised without the need for\ndenoising detail subbands which substantially reduces the runtime. The proposed\nmethods are initially derived within the framework of sparsifying transform\nlearning denoising, and then, they are generalized to propose our multiscale\nextensions for the well-known K-SVD and SAIST image denoising methods. We\nanalyze and assess the studied methods thoroughly and compare them with the\nwell-known and state-of-the-art methods. The experiments show that our methods\nare able to offer good trade-offs between performance and complexity.",
          "link": "http://arxiv.org/abs/2003.11265",
          "publishedOn": "2021-07-27T02:03:34.067Z",
          "wordCount": 643,
          "title": "Multiscale Sparsifying Transform Learning for Image Denoising. (arXiv:2003.11265v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Friedland_S/0/1/0/all/0/1\">Shmuel Friedland</a>",
          "description": "We study the optimal transport problem for $d>2$ discrete measures. This is a\nlinear programming problem on $d$-tensors. It gives a way to compute a\n\"distance\" between two sets of discrete measures. We introduce an entropic\nregularization term, which gives rise to a scaling of tensors. We give a\nvariation of the celebrated Sinkhorn scaling algorithm. We show that this\nalgorithm can be viewed as a partial minimization algorithm of a strictly\nconvex function. Under appropriate conditions the rate of convergence is\ngeometric and we estimate the rate. Our results are generalizations of known\nresults for the classical case of two discrete measures.",
          "link": "http://arxiv.org/abs/2005.00945",
          "publishedOn": "2021-07-27T02:03:34.053Z",
          "wordCount": 594,
          "title": "Tensor optimal transport, distance between sets of measures and tensor scaling. (arXiv:2005.00945v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sudipan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebel_P/0/1/0/all/0/1\">Patrick Ebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Most change detection methods assume that pre-change and post-change images\nare acquired by the same sensor. However, in many real-life scenarios, e.g.,\nnatural disaster, it is more practical to use the latest available images\nbefore and after the occurrence of incidence, which may be acquired using\ndifferent sensors. In particular, we are interested in the combination of the\nimages acquired by optical and Synthetic Aperture Radar (SAR) sensors. SAR\nimages appear vastly different from the optical images even when capturing the\nsame scene. Adding to this, change detection methods are often constrained to\nuse only target image-pair, no labeled data, and no additional unlabeled data.\nSuch constraints limit the scope of traditional supervised machine learning and\nunsupervised generative approaches for multi-sensor change detection. Recent\nrapid development of self-supervised learning methods has shown that some of\nthem can even work with only few images. Motivated by this, in this work we\npropose a method for multi-sensor change detection using only the unlabeled\ntarget bi-temporal images that are used for training a network in\nself-supervised fashion by using deep clustering and contrastive learning. The\nproposed method is evaluated on four multi-modal bi-temporal scenes showing\nchange and the benefits of our self-supervised approach are demonstrated.",
          "link": "http://arxiv.org/abs/2103.05102",
          "publishedOn": "2021-07-27T02:03:33.911Z",
          "wordCount": 667,
          "title": "Self-supervised Multisensor Change Detection. (arXiv:2103.05102v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiaxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "Bi-Level Optimization (BLO) is originated from the area of economic game\ntheory and then introduced into the optimization community. BLO is able to\nhandle problems with a hierarchical structure, involving two levels of\noptimization tasks, where one task is nested inside the other. In machine\nlearning and computer vision fields, despite the different motivations and\nmechanisms, a lot of complex problems, such as hyper-parameter optimization,\nmulti-task and meta-learning, neural architecture search, adversarial learning\nand deep reinforcement learning, actually all contain a series of closely\nrelated subproblms. In this paper, we first uniformly express these complex\nlearning and vision problems from the perspective of BLO. Then we construct a\nbest-response-based single-level reformulation and establish a unified\nalgorithmic framework to understand and formulate mainstream gradient-based BLO\nmethodologies, covering aspects ranging from fundamental automatic\ndifferentiation schemes to various accelerations, simplifications, extensions\nand their convergence and complexity properties. Last but not least, we discuss\nthe potentials of our unified BLO framework for designing new algorithms and\npoint out some promising directions for future research.",
          "link": "http://arxiv.org/abs/2101.11517",
          "publishedOn": "2021-07-27T02:03:33.881Z",
          "wordCount": 664,
          "title": "Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond. (arXiv:2101.11517v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hung-Yueh Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chin-Tang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1\">Ching-Yu Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "Most 3D neural networks are trained from scratch owing to the lack of\nlarge-scale labeled datasets. In this paper, we present a novel 3D pretraining\nmethod by leveraging 2D networks learned from rich 2D datasets. We propose the\ncontrastive pixel-to-point knowledge transfer to effectively utilize the 2D\ninformation by mapping the pixel-level and point-level features into the same\nembedding space. Due to the heterogeneous nature between 2D and 3D networks, we\nintroduce the back-projection function to align the features between 2D and 3D\nto make the transfer possible. Additionally, we devise an upsampling feature\nprojection layer to increase the spatial resolution of high-level 2D feature\nmaps, which helps learning fine-grained 3D representations. With a pretrained\n2D network, the proposed pretraining process requires no additional 2D or 3D\nlabeled data, further alleviating the expansive 3D data annotation cost. To the\nbest of our knowledge, we are the first to exploit existing 2D trained weights\nto pretrain 3D deep neural networks. Our intensive experiments show that the 3D\nmodels pretrained with 2D knowledge boost the performances across various\nreal-world 3D downstream tasks.",
          "link": "http://arxiv.org/abs/2104.04687",
          "publishedOn": "2021-07-27T02:03:33.862Z",
          "wordCount": 660,
          "title": "Learning from 2D: Contrastive Pixel-to-Point Knowledge Transfer for 3D Pretraining. (arXiv:2104.04687v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1\">Biswadeep Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_X/0/1/0/all/0/1\">Xueyuan She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_S/0/1/0/all/0/1\">Saibal Mukhopadhyay</a>",
          "description": "This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for\nenergy-efficient and robust object detection in resource-constrained platforms.\nThe network architecture is based on Convolutional SNN using\nleaky-integrate-fire neuron models. The model combines unsupervised Spike\nTime-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning\nmethods and also uses Monte Carlo Dropout to get an estimate of the uncertainty\nerror. FSHNN provides better accuracy compared to DNN based object detectors\nwhile being 150X energy-efficient. It also outperforms these object detectors,\nwhen subjected to noisy input data and less labeled training data with a lower\nuncertainty error.",
          "link": "http://arxiv.org/abs/2104.10719",
          "publishedOn": "2021-07-27T02:03:33.813Z",
          "wordCount": 576,
          "title": "A Fully Spiking Hybrid Neural Network for Energy-Efficient Object Detection. (arXiv:2104.10719v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>",
          "description": "One-class novelty detection is to identify anomalous instances that do not\nconform to the expected normal instances. In this paper, the Generative\nAdversarial Networks (GANs) based on encoder-decoder-encoder pipeline are used\nfor detection and achieve state-of-the-art performance. However, deep neural\nnetworks are too over-parameterized to deploy on resource-limited devices.\nTherefore, Progressive Knowledge Distillation with GANs (PKDGAN) is proposed to\nlearn compact and fast novelty detection networks. The P-KDGAN is a novel\nattempt to connect two standard GANs by the designed distillation loss for\ntransferring knowledge from the teacher to the student. The progressive\nlearning of knowledge distillation is a two-step approach that continuously\nimproves the performance of the student GAN and achieves better performance\nthan single step methods. In the first step, the student GAN learns the basic\nknowledge totally from the teacher via guiding of the pretrained teacher GAN\nwith fixed weights. In the second step, joint fine-training is adopted for the\nknowledgeable teacher and student GANs to further improve the performance and\nstability. The experimental results on CIFAR-10, MNIST, and FMNIST show that\nour method improves the performance of the student GAN by 2.44%, 1.77%, and\n1.73% when compressing the computation at ratios of 24.45:1, 311.11:1, and\n700:1, respectively.",
          "link": "http://arxiv.org/abs/2007.06963",
          "publishedOn": "2021-07-27T02:03:33.804Z",
          "wordCount": 674,
          "title": "P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection. (arXiv:2007.06963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1812.04275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hangyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>",
          "description": "This paper proposes a novel approach for Sketch-Based Image Retrieval (SBIR),\nfor which the key is to bridge the gap between sketches and photos in terms of\nthe data representation. Inspired by channel-wise attention explored in recent\nyears, we present a Domain-Aware Squeeze-and-Excitation (DASE) network, which\nseamlessly incorporates the prior knowledge of sample sketch or photo into SE\nmodule and make the SE module capable of emphasizing appropriate channels\naccording to domain signal. Accordingly, the proposed network can switch its\nmode to achieve a better domain feature with lower intra-class discrepancy.\nMoreover, while previous works simply focus on minimizing intra-class distance\nand maximizing inter-class distance, we introduce a loss function, named\nMultiplicative Euclidean Margin Softmax (MEMS), which introduces multiplicative\nEuclidean margin into feature space and ensure that the maximum intra-class\ndistance is smaller than the minimum inter-class distance. This facilitates\nlearning a highly discriminative feature space and ensures a more accurate\nimage retrieval result. Extensive experiments are conducted on two widely used\nSBIR benchmark datasets. Our approach achieves better results on both datasets,\nsurpassing the state-of-the-art methods by a large margin.",
          "link": "http://arxiv.org/abs/1812.04275",
          "publishedOn": "2021-07-27T02:03:33.798Z",
          "wordCount": 664,
          "title": "Domain-Aware SE Network for Sketch-based Image Retrieval with Multiplicative Euclidean Margin Softmax. (arXiv:1812.04275v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.00113",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kelly_D/0/1/0/all/0/1\">Damien Kelly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dorado_I/0/1/0/all/0/1\">Ignacio Garcia Dorado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>",
          "description": "Could we compress images via standard codecs while avoiding visible\nartifacts? The answer is obvious -- this is doable as long as the bit budget is\ngenerous enough. What if the allocated bit-rate for compression is\ninsufficient? Then unfortunately, artifacts are a fact of life. Many attempts\nwere made over the years to fight this phenomenon, with various degrees of\nsuccess. In this work we aim to break the unholy connection between bit-rate\nand image quality, and propose a way to circumvent compression artifacts by\npre-editing the incoming image and modifying its content to fit the given bits.\nWe design this editing operation as a learned convolutional neural network, and\nformulate an optimization problem for its training. Our loss takes into account\na proximity between the original image and the edited one, a bit-budget penalty\nover the proposed image, and a no-reference image quality measure for forcing\nthe outcome to be visually pleasing. The proposed approach is demonstrated on\nthe popular JPEG compression, showing savings in bits and/or improvements in\nvisual quality, obtained with intricate editing effects.",
          "link": "http://arxiv.org/abs/2002.00113",
          "publishedOn": "2021-07-27T02:03:33.790Z",
          "wordCount": 649,
          "title": "Better Compression with Deep Pre-Editing. (arXiv:2002.00113v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.12618",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Golts_A/0/1/0/all/0/1\">Alex Golts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schechner_Y/0/1/0/all/0/1\">Yoav Y. Schechner</a>",
          "description": "Computer vision tasks are often expected to be executed on compressed images.\nClassical image compression standards like JPEG 2000 are widely used. However,\nthey do not account for the specific end-task at hand. Motivated by works on\nrecurrent neural network (RNN)-based image compression and three-dimensional\n(3D) reconstruction, we propose unified network architectures to solve both\ntasks jointly. These joint models provide image compression tailored for the\nspecific task of 3D reconstruction. Images compressed by our proposed models,\nyield 3D reconstruction performance superior as compared to using JPEG 2000\ncompression. Our models significantly extend the range of compression rates for\nwhich 3D reconstruction is possible. We also show that this can be done highly\nefficiently at almost no additional cost to obtain compression on top of the\ncomputation already required for performing the 3D reconstruction task.",
          "link": "http://arxiv.org/abs/2003.12618",
          "publishedOn": "2021-07-27T02:03:33.772Z",
          "wordCount": 605,
          "title": "Image compression optimized for 3D reconstruction by utilizing deep neural networks. (arXiv:2003.12618v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.05541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_I/0/1/0/all/0/1\">Icaro O. de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1\">Rayson Laroca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_K/0/1/0/all/0/1\">Keiko V. O. Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1\">Rodrigo Minetto</a>",
          "description": "This work addresses the problem of vehicle identification through\nnon-overlapping cameras. As our main contribution, we introduce a novel dataset\nfor vehicle identification, called Vehicle-Rear, that contains more than three\nhours of high-resolution videos, with accurate information about the make,\nmodel, color and year of nearly 3,000 vehicles, in addition to the position and\nidentification of their license plates. To explore our dataset we design a\ntwo-stream CNN that simultaneously uses two of the most distinctive and\npersistent features available: the vehicle's appearance and its license plate.\nThis is an attempt to tackle a major problem: false alarms caused by vehicles\nwith similar designs or by very close license plate identifiers. In the first\nnetwork stream, shape similarities are identified by a Siamese CNN that uses a\npair of low-resolution vehicle patches recorded by two different cameras. In\nthe second stream, we use a CNN for OCR to extract textual information,\nconfidence scores, and string similarities from a pair of high-resolution\nlicense plate patches. Then, features from both streams are merged by a\nsequence of fully connected layers for decision. In our experiments, we\ncompared the two-stream network against several well-known CNN architectures\nusing single or multiple vehicle features. The architectures, trained models,\nand dataset are publicly available at https://github.com/icarofua/vehicle-rear.",
          "link": "http://arxiv.org/abs/1911.05541",
          "publishedOn": "2021-07-27T02:03:33.765Z",
          "wordCount": 721,
          "title": "Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle Identification Using Convolutional Neural Networks. (arXiv:1911.05541v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aftab_A/0/1/0/all/0/1\">Abdul Rafey Aftab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beeck_M/0/1/0/all/0/1\">Michael von der Beeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrhirsch_S/0/1/0/all/0/1\">Steven Rohrhirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diotte_B/0/1/0/all/0/1\">Benoit Diotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feld_M/0/1/0/all/0/1\">Michael Feld</a>",
          "description": "There is a growing interest in more intelligent natural user interaction with\nthe car. Hand gestures and speech are already being applied for driver-car\ninteraction. Moreover, multimodal approaches are also showing promise in the\nautomotive industry. In this paper, we utilize deep learning for a multimodal\nfusion network for referencing objects outside the vehicle. We use features\nfrom gaze, head pose and finger pointing simultaneously to precisely predict\nthe referenced objects in different car poses. We demonstrate the practical\nlimitations of each modality when used for a natural form of referencing,\nspecifically inside the car. As evident from our results, we overcome the\nmodality specific limitations, to a large extent, by the addition of other\nmodalities. This work highlights the importance of multimodal sensing,\nespecially when moving towards natural user interaction. Furthermore, our user\nbased analysis shows noteworthy differences in recognition of user behavior\ndepending upon the vehicle pose.",
          "link": "http://arxiv.org/abs/2107.12167",
          "publishedOn": "2021-07-27T02:03:33.756Z",
          "wordCount": 603,
          "title": "Multimodal Fusion Using Deep Learning Applied to Driver's Referencing of Outside-Vehicle Objects. (arXiv:2107.12167v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xinyu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kailin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>",
          "description": "Modeling the hand-object (HO) interaction not only requires estimation of the\nHO pose, but also pays attention to the contact due to their interaction.\nSignificant progress has been made in estimating hand and object separately\nwith deep learning methods, simultaneous HO pose estimation and contact\nmodeling has not yet been fully explored. In this paper, we present an explicit\ncontact representation namely Contact Potential Field (CPF), and a\nlearning-fitting hybrid framework namely MIHO to Modeling the Interaction of\nHand and Object. In CPF, we treat each contacting HO vertex pair as a\nspring-mass system. Hence the whole system forms a potential field with minimal\nelastic energy at the grasp position. Extensive experiments on the two commonly\nused benchmarks have demonstrated that our method can achieve state-of-the-art\nin several reconstruction metrics, and allow us to produce more physically\nplausible HO pose even when the ground-truth exhibits severe interpenetration\nor disjointedness. Our code is available at https://github.com/lixiny/CPF.",
          "link": "http://arxiv.org/abs/2012.00924",
          "publishedOn": "2021-07-27T02:03:33.725Z",
          "wordCount": 643,
          "title": "CPF: Learning a Contact Potential Field to Model the Hand-object Interaction. (arXiv:2012.00924v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_N/0/1/0/all/0/1\">Norman Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Frank Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorundo_E/0/1/0/all/0/1\">Evan Dorundo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_R/0/1/0/all/0/1\">Rahul Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tyler Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1\">Samyak Parajuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mike Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilmer_J/0/1/0/all/0/1\">Justin Gilmer</a>",
          "description": "We introduce four new real-world distribution shift datasets consisting of\nchanges in image style, image blurriness, geographic location, camera\noperation, and more. With our new datasets, we take stock of previously\nproposed methods for improving out-of-distribution robustness and put them to\nthe test. We find that using larger models and artificial data augmentations\ncan improve robustness on real-world distribution shifts, contrary to claims in\nprior work. We find improvements in artificial robustness benchmarks can\ntransfer to real-world distribution shifts, contrary to claims in prior work.\nMotivated by our observation that data augmentations can help with real-world\ndistribution shifts, we also introduce a new data augmentation method which\nadvances the state-of-the-art and outperforms models pretrained with 1000 times\nmore labeled data. Overall we find that some methods consistently help with\ndistribution shifts in texture and local image statistics, but these methods do\nnot help with some other distribution shifts like geographic changes. Our\nresults show that future research must study multiple distribution shifts\nsimultaneously, as we demonstrate that no evaluated method consistently\nimproves robustness.",
          "link": "http://arxiv.org/abs/2006.16241",
          "publishedOn": "2021-07-27T02:03:33.719Z",
          "wordCount": 694,
          "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. (arXiv:2006.16241v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11687",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hahne_C/0/1/0/all/0/1\">Christopher Hahne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aggoun_A/0/1/0/all/0/1\">Amar Aggoun</a>",
          "description": "Light-field cameras play a vital role for rich 3-D information retrieval in\nnarrow range depth sensing applications. The key obstacle in composing\nlight-fields from exposures taken by a plenoptic camera is to computationally\ncalibrate, align and rearrange four-dimensional image data. Several attempts\nhave been proposed to enhance the overall image quality by tailoring pipelines\ndedicated to particular plenoptic cameras and improving the consistency across\nviewpoints at the expense of high computational loads. The framework presented\nherein advances prior outcomes thanks to its novel micro image scale-space\nanalysis for generic camera calibration independent of the lens specifications\nand its parallax-invariant, cost-effective viewpoint color equalization from\noptimal transport theory. Artifacts from the sensor and micro lens grid are\ncompensated in an innovative way to enable superior quality in sub-aperture\nimage extraction, computational refocusing and Scheimpflug rendering with\nsub-sampling capabilities. Benchmark comparisons using established image\nmetrics suggest that our proposed pipeline outperforms state-of-the-art tool\nchains in the majority of cases. Results from a Wasserstein distance further\nshow that our color transfer outdoes the existing transport methods. Our\nalgorithms are released under an open-source license, offer cross-platform\ncompatibility with few dependencies and different user interfaces. This makes\nthe reproduction of results and experimentation with plenoptic camera\ntechnology convenient for peer researchers, developers, photographers, data\nscientists and others working in this field.",
          "link": "http://arxiv.org/abs/2010.11687",
          "publishedOn": "2021-07-27T02:03:33.711Z",
          "wordCount": 704,
          "title": "PlenoptiCam v1.0: A light-field imaging framework. (arXiv:2010.11687v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Ye Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zehua Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>",
          "description": "Coarsely-labeled semantic segmentation annotations are easy to obtain, but\ntherefore bear the risk of losing edge details and introducing background\npixels. Impeded by the inherent noise, existing coarse annotations are only\ntaken as a bonus for model pre-training. In this paper, we try to exploit their\npotentials with a confidence-based reweighting strategy. To expand, loss-based\nreweighting strategies usually take the high loss value to identify two\ncompletely different types of pixels, namely, valuable pixels in noise-free\nannotations and mislabeled pixels in noisy annotations. This makes it\nimpossible to perform two tasks of mining valuable pixels and suppressing\nmislabeled pixels at the same time. However, with the help of the prediction\nconfidence, we successfully solve this dilemma and simultaneously perform two\nsubtasks with a single reweighting strategy. Furthermore, we generalize this\nstrategy into an Adversarial Reweighting Module (ARM) and prove its convergence\nstrictly. Experiments on standard datasets shows our ARM can bring consistent\nimprovements for both coarse annotations and fine annotations. Specifically,\nbuilt on top of DeepLabv3+, ARM improves the mIoU on the coarsely-labeled\nCityscapes by a considerable margin and increases the mIoU on the ADE20K\ndataset to 47.50.",
          "link": "http://arxiv.org/abs/2009.05205",
          "publishedOn": "2021-07-27T02:03:33.697Z",
          "wordCount": 659,
          "title": "ARM: A Confidence-Based Adversarial Reweighting Module for Coarse Semantic Segmentation. (arXiv:2009.05205v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milani_F/0/1/0/all/0/1\">Federico Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraternali_P/0/1/0/all/0/1\">Piero Fraternali</a>",
          "description": "Iconography in art is the discipline that studies the visual content of\nartworks to determine their motifs and themes andto characterize the way these\nare represented. It is a subject of active research for a variety of purposes,\nincluding the interpretation of meaning, the investigation of the origin and\ndiffusion in time and space of representations, and the study of influences\nacross artists and art works. With the proliferation of digital archives of art\nimages, the possibility arises of applying Computer Vision techniques to the\nanalysis of art images at an unprecedented scale, which may support iconography\nresearch and education. In this paper we introduce a novel paintings data set\nfor iconography classification and present the quantitativeand qualitative\nresults of applying a Convolutional Neural Network (CNN) classifier to the\nrecognition of the iconography of artworks. The proposed classifier achieves\ngood performances (71.17% Precision, 70.89% Recall, 70.25% F1-Score and 72.73%\nAverage Precision) in the task of identifying saints in Christian religious\npaintings, a task made difficult by the presence of classes with very similar\nvisual features. Qualitative analysis of the results shows that the CNN focuses\non the traditional iconic motifs that characterize the representation of each\nsaint and exploits such hints to attain correct identification. The ultimate\ngoal of our work is to enable the automatic extraction, decomposition, and\ncomparison of iconography elements to support iconographic studies and\nautomatic art work annotation.",
          "link": "http://arxiv.org/abs/2010.11697",
          "publishedOn": "2021-07-27T02:03:33.679Z",
          "wordCount": 739,
          "title": "A Data Set and a Convolutional Model for Iconography Classification in Paintings. (arXiv:2010.11697v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunfeng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Ying Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>",
          "description": "Graph convolutional networks (GCNs) have been widely used and achieved\nremarkable results in skeleton-based action recognition. In GCNs, graph\ntopology dominates feature aggregation and therefore is the key to extracting\nrepresentative features. In this work, we propose a novel Channel-wise Topology\nRefinement Graph Convolution (CTR-GC) to dynamically learn different topologies\nand effectively aggregate joint features in different channels for\nskeleton-based action recognition. The proposed CTR-GC models channel-wise\ntopologies through learning a shared topology as a generic prior for all\nchannels and refining it with channel-specific correlations for each channel.\nOur refinement method introduces few extra parameters and significantly reduces\nthe difficulty of modeling channel-wise topologies. Furthermore, via\nreformulating graph convolutions into a unified form, we find that CTR-GC\nrelaxes strict constraints of graph convolutions, leading to stronger\nrepresentation capability. Combining CTR-GC with temporal modeling modules, we\ndevelop a powerful graph convolutional network named CTR-GCN which notably\noutperforms state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and\nNW-UCLA datasets.",
          "link": "http://arxiv.org/abs/2107.12213",
          "publishedOn": "2021-07-27T02:03:33.672Z",
          "wordCount": 611,
          "title": "Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition. (arXiv:2107.12213v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardaoui_D/0/1/0/all/0/1\">Dina Mardaoui</a>",
          "description": "The performance of modern algorithms on certain computer vision tasks such as\nobject recognition is now close to that of humans. This success was achieved at\nthe price of complicated architectures depending on millions of parameters and\nit has become quite challenging to understand how particular predictions are\nmade. Interpretability methods propose to give us this understanding. In this\npaper, we study LIME, perhaps one of the most popular. On the theoretical side,\nwe show that when the number of generated examples is large, LIME explanations\nare concentrated around a limit explanation for which we give an explicit\nexpression. We further this study for elementary shape detectors and linear\nmodels. As a consequence of this analysis, we uncover a connection between LIME\nand integrated gradients, another explanation method. More precisely, the LIME\nexplanations are similar to the sum of integrated gradients over the\nsuperpixels used in the preprocessing step of LIME.",
          "link": "http://arxiv.org/abs/2102.06307",
          "publishedOn": "2021-07-27T02:03:33.665Z",
          "wordCount": 617,
          "title": "What does LIME really see in images?. (arXiv:2102.06307v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>",
          "description": "When humans solve complex problems, they rarely come up with a decision\nright-away. Instead, they start with an intuitive decision, reflect upon it,\nspot mistakes, resolve contradictions and jump between different hypotheses.\nThus, they create a sequence of ideas and follow a train of thought that\nultimately reaches a conclusive decision. Contrary to this, today's neural\nclassification models are mostly trained to map an input to one single and\nfixed output. In this paper, we investigate how we can give models the\nopportunity of a second, third and $k$-th thought. We take inspiration from\nHegel's dialectics and propose a method that turns an existing classifier's\nclass prediction (such as the image class forest) into a sequence of\npredictions (such as forest $\\rightarrow$ tree $\\rightarrow$ mushroom).\nConcretely, we propose a correction module that is trained to estimate the\nmodel's correctness as well as an iterative prediction update based on the\nprediction's gradient. Our approach results in a dynamic system over class\nprobability distributions $\\unicode{x2014}$ the thought flow. We evaluate our\nmethod on diverse datasets and tasks from computer vision and natural language\nprocessing. We observe surprisingly complex but intuitive behavior and\ndemonstrate that our method (i) can correct misclassifications, (ii)\nstrengthens model performance, (iii) is robust to high levels of adversarial\nattacks, (iv) can increase accuracy up to 4% in a label-distribution-shift\nsetting and (iv) provides a tool for model interpretability that uncovers model\nknowledge which otherwise remains invisible in a single distribution\nprediction.",
          "link": "http://arxiv.org/abs/2107.12220",
          "publishedOn": "2021-07-27T02:03:33.658Z",
          "wordCount": 694,
          "title": "Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.07999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Angelina Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ryan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleiman_A/0/1/0/all/0/1\">Anat Kleiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_L/0/1/0/all/0/1\">Leslie Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dora Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirai_I/0/1/0/all/0/1\">Iroha Shirai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Arvind Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>",
          "description": "Machine learning models are known to perpetuate and even amplify the biases\npresent in the data. However, these data biases frequently do not become\napparent until after the models are deployed. Our work tackles this issue and\nenables the preemptive analysis of large-scale datasets. REVISE (REvealing\nVIsual biaSEs) is a tool that assists in the investigation of a visual dataset,\nsurfacing potential biases along three dimensions: (1) object-based, (2)\nperson-based, and (3) geography-based. Object-based biases relate to the size,\ncontext, or diversity of the depicted objects. Person-based metrics focus on\nanalyzing the portrayal of people within the dataset. Geography-based analyses\nconsider the representation of different geographic locations. These three\ndimensions are deeply intertwined in how they interact to bias a dataset, and\nREVISE sheds light on this; the responsibility then lies with the user to\nconsider the cultural and historical context, and to determine which of the\nrevealed biases may be problematic. The tool further assists the user by\nsuggesting actionable steps that may be taken to mitigate the revealed biases.\nOverall, the key aim of our work is to tackle the machine learning bias problem\nearly in the pipeline. REVISE is available at\nhttps://github.com/princetonvisualai/revise-tool",
          "link": "http://arxiv.org/abs/2004.07999",
          "publishedOn": "2021-07-27T02:03:33.652Z",
          "wordCount": 702,
          "title": "REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets. (arXiv:2004.07999v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_B/0/1/0/all/0/1\">Bilal Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Baz_A/0/1/0/all/0/1\">Ayman El-Baz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>",
          "description": "Prostate cancer (PCa) is the second deadliest form of cancer in males, and it\ncan be clinically graded by examining the structural representations of Gleason\ntissues. This paper proposes \\RV{a new method} for segmenting the Gleason\ntissues \\RV{(patch-wise) in order to grade PCa from the whole slide images\n(WSI).} Also, the proposed approach encompasses two main contributions: 1) A\nsynergy of hybrid dilation factors and hierarchical decomposition of latent\nspace representation for effective Gleason tissues extraction, and 2) A\nthree-tiered loss function which can penalize different semantic segmentation\nmodels for accurately extracting the highly correlated patterns. In addition to\nthis, the proposed framework has been extensively evaluated on a large-scale\nPCa dataset containing 10,516 whole slide scans (with around 71.7M patches),\nwhere it outperforms state-of-the-art schemes by 3.22% (in terms of mean\nintersection-over-union) for extracting the Gleason tissues and 6.91% (in terms\nof F1 score) for grading the progression of PCa.",
          "link": "http://arxiv.org/abs/2011.00527",
          "publishedOn": "2021-07-27T02:03:33.632Z",
          "wordCount": 682,
          "title": "A Dilated Residual Hierarchically Fashioned Segmentation Framework for Extracting Gleason Tissues and Grading Prostate Cancer from Whole Slide Images. (arXiv:2011.00527v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12535",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kang_H/0/1/0/all/0/1\">Hongtao Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_D/0/1/0/all/0/1\">Die Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_W/0/1/0/all/0/1\">Weihua Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1\">Junbo Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_S/0/1/0/all/0/1\">Shaoqun Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quan_T/0/1/0/all/0/1\">Tingwei Quan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiuli Liu</a>",
          "description": "Stain normalization often refers to transferring the color distribution of\nthe source image to that of the target image and has been widely used in\nbiomedical image analysis. The conventional stain normalization is regarded as\nconstructing a pixel-by-pixel color mapping model, which only depends on one\nreference image, and can not accurately achieve the style transformation\nbetween image datasets. In principle, this style transformation can be well\nsolved by the deep learning-based methods due to its complicated network\nstructure, whereas, its complicated structure results in the low computational\nefficiency and artifacts in the style transformation, which has restricted the\npractical application. Here, we use distillation learning to reduce the\ncomplexity of deep learning methods and a fast and robust network called\nStainNet to learn the color mapping between the source image and target image.\nStainNet can learn the color mapping relationship from a whole dataset and\nadjust the color value in a pixel-to-pixel manner. The pixel-to-pixel manner\nrestricts the network size and avoids artifacts in the style transformation.\nThe results on the cytopathology and histopathology datasets show that StainNet\ncan achieve comparable performance to the deep learning-based methods.\nComputation results demonstrate StainNet is more than 40 times faster than\nStainGAN and can normalize a 100,000x100,000 whole slide image in 40 seconds.",
          "link": "http://arxiv.org/abs/2012.12535",
          "publishedOn": "2021-07-27T02:03:33.626Z",
          "wordCount": 718,
          "title": "StainNet: a fast and robust stain normalization network. (arXiv:2012.12535v6 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aiger_D/0/1/0/all/0/1\">Dror Aiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynen_S/0/1/0/all/0/1\">Simon Lynen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosang_J/0/1/0/all/0/1\">Jan Hosang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeisl_B/0/1/0/all/0/1\">Bernhard Zeisl</a>",
          "description": "Outlier rejection and equivalently inlier set optimization is a key\ningredient in numerous applications in computer vision such as filtering\npoint-matches in camera pose estimation or plane and normal estimation in point\nclouds. Several approaches exist, yet at large scale we face a combinatorial\nexplosion of possible solutions and state-of-the-art methods like RANSAC, Hough\ntransform or Branch\\&Bound require a minimum inlier ratio or prior knowledge to\nremain practical. In fact, for problems such as camera posing in very large\nscenes these approaches become useless as they have exponential runtime growth\nif these conditions aren't met. To approach the problem we present a efficient\nand general algorithm for outlier rejection based on \"intersecting\"\n$k$-dimensional surfaces in $R^d$. We provide a recipe for casting a variety of\ngeometric problems as finding a point in $R^d$ which maximizes the number of\nnearby surfaces (and thus inliers). The resulting algorithm has linear\nworst-case complexity with a better runtime dependency in the approximation\nfactor than competing algorithms while not requiring domain specific bounds.\nThis is achieved by introducing a space decomposition scheme that bounds the\nnumber of computations by successively rounding and grouping samples. Our\nrecipe (and open-source code) enables anybody to derive such fast approaches to\nnew problems across a wide range of domains. We demonstrate the versatility of\nthe approach on several camera posing problems with a high number of matches at\nlow inlier ratio achieving state-of-the-art results at significantly lower\nprocessing times.",
          "link": "http://arxiv.org/abs/2107.11810",
          "publishedOn": "2021-07-27T02:03:33.619Z",
          "wordCount": 678,
          "title": "Efficient Large Scale Inlier Voting for Geometric Vision Problems. (arXiv:2107.11810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ung_H/0/1/0/all/0/1\">Hieu T. Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ung_H/0/1/0/all/0/1\">Huy Q. Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>",
          "description": "Accurate insect pest recognition is significant to protect the crop or take\nthe early treatment on the infected yield, and it helps reduce the loss for the\nagriculture economy. Design an automatic pest recognition system is necessary\nbecause manual recognition is slow, time-consuming, and expensive. The\nImage-based pest classifier using the traditional computer vision method is not\nefficient due to the complexity. Insect pest classification is a difficult task\nbecause of various kinds, scales, shapes, complex backgrounds in the field, and\nhigh appearance similarity among insect species. With the rapid development of\ndeep learning technology, the CNN-based method is the best way to develop a\nfast and accurate insect pest classifier. We present different convolutional\nneural network-based models in this work, including attention, feature pyramid,\nand fine-grained models. We evaluate our methods on two public datasets: the\nlarge-scale insect pest dataset, the IP102 benchmark dataset, and a smaller\ndataset, namely D0 in terms of the macro-average precision (MPre), the\nmacro-average recall (MRec), the macro-average F1- score (MF1), the accuracy\n(Acc), and the geometric mean (GM). The experimental results show that\ncombining these convolutional neural network-based models can better perform\nthan the state-of-the-art methods on these two datasets. For instance, the\nhighest accuracy we obtained on IP102 and D0 is $74.13\\%$ and $99.78\\%$,\nrespectively, bypassing the corresponding state-of-the-art accuracy: $67.1\\%$\n(IP102) and $98.8\\%$ (D0). We also publish our codes for contributing to the\ncurrent research related to the insect pest classification problem.",
          "link": "http://arxiv.org/abs/2107.12189",
          "publishedOn": "2021-07-27T02:03:33.612Z",
          "wordCount": 695,
          "title": "An Efficient Insect Pest Classification Using Multiple Convolutional Neural Network Based Models. (arXiv:2107.12189v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marek_M/0/1/0/all/0/1\">Martin Marek</a>",
          "description": "We introduce a new dataset for image-based parking space occupancy\nclassification: ACPDS. Unlike in prior datasets, each image is taken from a\nunique view, systematically annotated, and the parking lots in the train,\nvalidation, and test sets are unique. We use this dataset to propose a simple\nbaseline model for parking space occupancy classification, which achieves 98%\naccuracy on unseen parking lots, significantly outperforming existing models.\nWe share our dataset, code, and trained models under the MIT license.",
          "link": "http://arxiv.org/abs/2107.12207",
          "publishedOn": "2021-07-27T02:03:33.605Z",
          "wordCount": 508,
          "title": "Image-Based Parking Space Occupancy Classification: Dataset and Baseline. (arXiv:2107.12207v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>",
          "description": "We propose an efficient inference framework for semi-supervised video object\nsegmentation by exploiting the temporal redundancy of the video. Our method\nperforms inference on selected keyframes and makes predictions for other frames\nvia propagation based on motion vectors and residuals from the compressed video\nbitstream. Specifically, we propose a new motion vector-based warping method\nfor propagating segmentation masks from keyframes to other frames in a\nmulti-reference manner. Additionally, we propose a residual-based refinement\nmodule that can correct and add detail to the block-wise propagated\nsegmentation masks. Our approach is flexible and can be added on top of\nexisting video object segmentation algorithms. With STM with top-k filtering as\nour base model, we achieved highly competitive results on DAVIS16 and\nYouTube-VOS with substantial speedups of up to 4.9X with little loss in\naccuracy.",
          "link": "http://arxiv.org/abs/2107.12192",
          "publishedOn": "2021-07-27T02:03:33.599Z",
          "wordCount": 567,
          "title": "Efficient Video Object Segmentation with Compressed Video. (arXiv:2107.12192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12205",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bhandary_A/0/1/0/all/0/1\">Abhir Bhandary</a>, <a href=\"http://arxiv.org/find/eess/1/au:+G_A/0/1/0/all/0/1\">Ananth Prabhu G</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Basthikodi_M/0/1/0/all/0/1\">Mustafa Basthikodi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+M_C/0/1/0/all/0/1\">Chaitra K M</a>",
          "description": "Lung cancer begins in the lungs and leading to the reason of cancer demise\namid population in the creation. According to the American Cancer Society,\nwhich estimates about 27% of the deaths because of cancer. In the early phase\nof its evolution, lung cancer does not cause any symptoms usually. Many of the\npatients have been diagnosed in a developed phase where symptoms become more\nprominent, that results in poor curative treatment and high mortality rate.\nComputer Aided Detection systems are used to achieve greater accuracies for the\nlung cancer diagnosis. In this research exertion, we proposed a novel\nmethodology for lung Segmentation on the basis of Fuzzy C-Means Clustering,\nAdaptive Thresholding, and Segmentation of Active Contour Model. The\nexperimental results are analysed and presented.",
          "link": "http://arxiv.org/abs/2107.12205",
          "publishedOn": "2021-07-27T02:03:33.582Z",
          "wordCount": 608,
          "title": "Early Diagnosis of Lung Cancer Using Computer Aided Detection via Lung Segmentation Approach. (arXiv:2107.12205v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loukas_C/0/1/0/all/0/1\">C. Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gazis_A/0/1/0/all/0/1\">A. Gazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schizas_D/0/1/0/all/0/1\">D. Schizas</a>",
          "description": "An important task at the onset of a laparoscopic cholecystectomy (LC)\noperation is the inspection of gallbladder (GB) to evaluate the thickness of\nits wall, presence of inflammation and extent of fat. Difficulty in\nvisualization of the GB wall vessels may be due to the previous factors,\npotentially as a result of chronic inflammation or other diseases. In this\npaper we propose a multiple-instance learning (MIL) technique for assessment of\nthe GB wall vascularity via computer-vision analysis of images from LC\noperations. The bags correspond to a labeled (low vs. high) vascularity dataset\nof 181 GB images, from 53 operations. The instances correspond to unlabeled\npatches extracted from these images. Each patch is represented by a vector with\ncolor, texture and statistical features. We compare various state-of-the-art\nMIL and single-instance learning approaches, as well as a proposed MIL\ntechnique based on variational Bayesian inference. The methods were compared\nfor two experimental tasks: image-based and video-based (i.e. patient-based)\nclassification. The proposed approach presents the best performance with\naccuracy 92.1% and 90.3% for the first and second task, respectively. A\nsignificant advantage of the proposed technique is that it does not require the\ntime-consuming task of manual labelling the instances.",
          "link": "http://arxiv.org/abs/2107.12093",
          "publishedOn": "2021-07-27T02:03:33.573Z",
          "wordCount": 647,
          "title": "A Multiple-Instance Learning Approach for the Assessment of Gallbladder Vascularity from Laparoscopic Images. (arXiv:2107.12093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuedong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cham_T/0/1/0/all/0/1\">Tat-Jen Cham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>",
          "description": "Although much progress has been made in visual emotion recognition,\nresearchers have realized that modern deep networks tend to exploit dataset\ncharacteristics to learn spurious statistical associations between the input\nand the target. Such dataset characteristics are usually treated as dataset\nbias, which damages the robustness and generalization performance of these\nrecognition systems. In this work, we scrutinize this problem from the\nperspective of causal inference, where such dataset characteristic is termed as\na confounder which misleads the system to learn the spurious correlation. To\nalleviate the negative effects brought by the dataset bias, we propose a novel\nInterventional Emotion Recognition Network (IERN) to achieve the backdoor\nadjustment, which is one fundamental deconfounding technique in causal\ninference. A series of designed tests validate the effectiveness of IERN, and\nexperiments on three emotion benchmarks demonstrate that IERN outperforms other\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2107.12096",
          "publishedOn": "2021-07-27T02:03:33.566Z",
          "wordCount": 580,
          "title": "Towards Unbiased Visual Emotion Recognition via Causal Intervention. (arXiv:2107.12096v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zindancioglu_A/0/1/0/all/0/1\">Alara Zindanc&#x131;o&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezgin_T/0/1/0/all/0/1\">T. Metin Sezgin</a>",
          "description": "The ability to edit facial expressions has a wide range of applications in\ncomputer graphics. The ideal facial expression editing algorithm needs to\nsatisfy two important criteria. First, it should allow precise and targeted\nediting of individual facial actions. Second, it should generate high fidelity\noutputs without artifacts. We build a solution based on StyleGAN, which has\nbeen used extensively for semantic manipulation of faces. As we do so, we add\nto our understanding of how various semantic attributes are encoded in\nStyleGAN. In particular, we show that a naive strategy to perform editing in\nthe latent space results in undesired coupling between certain action units,\neven if they are conceptually distinct. For example, although brow lowerer and\nlip tightener are distinct action units, they appear correlated in the training\ndata. Hence, StyleGAN has difficulty in disentangling them. We allow\ndisentangled editing of such action units by computing detached regions of\ninfluence for each action unit, and restrict editing to these regions. We\nvalidate the effectiveness of our local editing method through perception\nexperiments conducted with 23 subjects. The results show that our method\nprovides higher control over local editing and produces images with superior\nfidelity compared to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.12143",
          "publishedOn": "2021-07-27T02:03:33.542Z",
          "wordCount": 648,
          "title": "Perceptually Validated Precise Local Editing for Facial Action Units with StyleGAN. (arXiv:2107.12143v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we address the problem of 3D object detection from point cloud\ndata in real time. For autonomous vehicles to work, it is very important for\nthe perception component to detect the real world objects with both high\naccuracy and fast inference. We propose a novel neural network architecture\nalong with the training and optimization details for detecting 3D objects using\npoint cloud data. We present anchor design along with custom loss functions\nused in this work. A combination of spatial and channel wise attention module\nis used in this work. We use the Kitti 3D Birds Eye View dataset for\nbenchmarking and validating our results. Our method surpasses previous state of\nthe art in this domain both in terms of average precision and speed running at\n> 30 FPS. Finally, we present the ablation study to demonstrate that the\nperformance of our network is generalizable. This makes it a feasible option to\nbe deployed in real time applications like self driving cars.",
          "link": "http://arxiv.org/abs/2107.12137",
          "publishedOn": "2021-07-27T02:03:33.536Z",
          "wordCount": 605,
          "title": "AA3DNet: Attention Augmented Real Time 3D Object Detection. (arXiv:2107.12137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1\">Xi Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weiji Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaomei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weiwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xiaobo Lai</a>",
          "description": "Background: Glioma is the most common brain malignant tumor, with a high\nmorbidity rate and a mortality rate of more than three percent, which seriously\nendangers human health. The main method of acquiring brain tumors in the clinic\nis MRI. Segmentation of brain tumor regions from multi-modal MRI scan images is\nhelpful for treatment inspection, post-diagnosis monitoring, and effect\nevaluation of patients. However, the common operation in clinical brain tumor\nsegmentation is still manual segmentation, lead to its time-consuming and large\nperformance difference between different operators, a consistent and accurate\nautomatic segmentation method is urgently needed. Methods: To meet the above\nchallenges, we propose an automatic brain tumor MRI data segmentation framework\nwhich is called AGSE-VNet. In our study, the Squeeze and Excite (SE) module is\nadded to each encoder, the Attention Guide Filter (AG) module is added to each\ndecoder, using the channel relationship to automatically enhance the useful\ninformation in the channel to suppress the useless information, and use the\nattention mechanism to guide the edge information and remove the influence of\nirrelevant information such as noise. Results: We used the BraTS2020 challenge\nonline verification tool to evaluate our approach. The focus of verification is\nthat the Dice scores of the whole tumor (WT), tumor core (TC) and enhanced\ntumor (ET) are 0.68, 0.85 and 0.70, respectively. Conclusion: Although MRI\nimages have different intensities, AGSE-VNet is not affected by the size of the\ntumor, and can more accurately extract the features of the three regions, it\nhas achieved impressive results and made outstanding contributions to the\nclinical diagnosis and treatment of brain tumor patients.",
          "link": "http://arxiv.org/abs/2107.12046",
          "publishedOn": "2021-07-27T02:03:33.448Z",
          "wordCount": 725,
          "title": "3D AGSE-VNet: An Automatic Brain Tumor MRI Data Segmentation Framework. (arXiv:2107.12046v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangteng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yiliang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>",
          "description": "Video-text retrieval is an important yet challenging task in vision-language\nunderstanding, which aims to learn a joint embedding space where related video\nand text instances are close to each other. Most current works simply measure\nthe video-text similarity based on video-level and text-level embeddings.\nHowever, the neglect of more fine-grained or local information causes the\nproblem of insufficient representation. Some works exploit the local details by\ndisentangling sentences, but overlook the corresponding videos, causing the\nasymmetry of video-text representation. To address the above limitations, we\npropose a Hierarchical Alignment Network (HANet) to align different level\nrepresentations for video-text matching. Specifically, we first decompose video\nand text into three semantic levels, namely event (video and text), action\n(motion and verb), and entity (appearance and noun). Based on these, we\nnaturally construct hierarchical representations in the individual-local-global\nmanner, where the individual level focuses on the alignment between frame and\nword, local level focuses on the alignment between video clip and textual\ncontext, and global level focuses on the alignment between the whole video and\ntext. Different level alignments capture fine-to-coarse correlations between\nvideo and text, as well as take the advantage of the complementary information\namong three semantic levels. Besides, our HANet is also richly interpretable by\nexplicitly learning key semantic concepts. Extensive experiments on two public\ndatasets, namely MSR-VTT and VATEX, show the proposed HANet outperforms other\nstate-of-the-art methods, which demonstrates the effectiveness of hierarchical\nrepresentation and alignment. Our code is publicly available.",
          "link": "http://arxiv.org/abs/2107.12059",
          "publishedOn": "2021-07-27T02:03:33.440Z",
          "wordCount": 687,
          "title": "HANet: Hierarchical Alignment Networks for Video-Text Retrieval. (arXiv:2107.12059v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yue Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prugel_Bennett_A/0/1/0/all/0/1\">Adam Pr&#xfc;gel-Bennett</a>",
          "description": "Zero shot learning (ZSL) has seen a surge in interest over the decade for its\ntight links with the mechanism making young children recognize novel objects.\nAlthough different paradigms of visual semantic embedding models are designed\nto align visual features and distributed word representations, it is unclear to\nwhat extent current ZSL models encode semantic information from distributed\nword representations. In this work, we introduce the split of tiered-ImageNet\nto the ZSL task, in order to avoid the structural flaws in the standard\nImageNet benchmark. We build a unified framework for ZSL with contrastive\nlearning as pre-training, which guarantees no semantic information leakage and\nencourages linearly separable visual features. Our work makes it fair for\nevaluating visual semantic embedding models on a ZSL setting in which semantic\ninference is decisive. With this framework, we show that current ZSL models\nstruggle with encoding semantic relationships from word analogy and word\nhierarchy. Our analyses provide motivation for exploring the role of context\nlanguage representations in ZSL tasks.",
          "link": "http://arxiv.org/abs/2107.11991",
          "publishedOn": "2021-07-27T02:03:33.433Z",
          "wordCount": 594,
          "title": "What Remains of Visual Semantic Embeddings. (arXiv:2107.11991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12038",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mentzer_F/0/1/0/all/0/1\">Fabian Mentzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agustsson_E/0/1/0/all/0/1\">Eirikur Agustsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balle_J/0/1/0/all/0/1\">Johannes Ball&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minnen_D/0/1/0/all/0/1\">David Minnen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johnston_N/0/1/0/all/0/1\">Nick Johnston</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toderici_G/0/1/0/all/0/1\">George Toderici</a>",
          "description": "We present a neural video compression method based on generative adversarial\nnetworks (GANs) that outperforms previous neural video compression methods and\nis comparable to HEVC in a user study. We propose a technique to mitigate\ntemporal error accumulation caused by recursive frame compression that uses\nrandomized shifting and un-shifting, motivated by a spectral analysis. We\npresent in detail the network design choices, their relative importance, and\nelaborate on the challenges of evaluating video compression methods in user\nstudies.",
          "link": "http://arxiv.org/abs/2107.12038",
          "publishedOn": "2021-07-27T02:03:33.425Z",
          "wordCount": 519,
          "title": "Towards Generative Video Compression. (arXiv:2107.12038v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Convolutional neural networks use regular quadrilateral convolution kernels\nto extract features. Since the number of parameters increases quadratically\nwith the size of the convolution kernel, many popular models use small\nconvolution kernels, resulting in small local receptive fields in lower layers.\nThis paper proposes a novel log-polar space convolution (LPSC) method, where\nthe convolution kernel is elliptical and adaptively divides its local receptive\nfield into different regions according to the relative directions and\nlogarithmic distances. The local receptive field grows exponentially with the\nnumber of distance levels. Therefore, the proposed LPSC not only naturally\nencodes local spatial structures, but also greatly increases the single-layer\nreceptive field while maintaining the number of parameters. We show that LPSC\ncan be implemented with conventional convolution via log-polar space pooling\nand can be applied in any network architecture to replace conventional\nconvolutions. Experiments on different tasks and datasets demonstrate the\neffectiveness of the proposed LPSC. Code is available at\nhttps://github.com/BingSu12/Log-Polar-Space-Convolution.",
          "link": "http://arxiv.org/abs/2107.11943",
          "publishedOn": "2021-07-27T02:03:33.372Z",
          "wordCount": 588,
          "title": "Log-Polar Space Convolution for Convolutional Neural Networks. (arXiv:2107.11943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuqian Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>",
          "description": "A recent study finds that existing few-shot learning methods, trained on the\nsource domain, fail to generalize to the novel target domain when a domain gap\nis observed. This motivates the task of Cross-Domain Few-Shot Learning\n(CD-FSL). In this paper, we realize that the labeled target data in CD-FSL has\nnot been leveraged in any way to help the learning process. Thus, we advocate\nutilizing few labeled target data to guide the model learning. Technically, a\nnovel meta-FDMixup network is proposed. We tackle this problem mainly from two\naspects. Firstly, to utilize the source and the newly introduced target data of\ntwo different class sets, a mixup module is re-proposed and integrated into the\nmeta-learning mechanism. Secondly, a novel disentangle module together with a\ndomain classifier is proposed to extract the disentangled domain-irrelevant and\ndomain-specific features. These two modules together enable our model to narrow\nthe domain gap thus generalizing well to the target datasets. Additionally, a\ndetailed feasibility and pilot study is conducted to reflect the intuitive\nunderstanding of CD-FSL under our new setting. Experimental results show the\neffectiveness of our new setting and the proposed method. Codes and models are\navailable at https://github.com/lovelyqian/Meta-FDMixup.",
          "link": "http://arxiv.org/abs/2107.11978",
          "publishedOn": "2021-07-27T02:03:33.355Z",
          "wordCount": 641,
          "title": "Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data. (arXiv:2107.11978v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1\">Aneeshan Sain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>",
          "description": "Visual text recognition is undoubtedly one of the most extensively researched\ntopics in computer vision. Great progress have been made to date, with the\nlatest models starting to focus on the more practical \"in-the-wild\" setting.\nHowever, a salient problem still hinders practical deployment -- prior arts\nmostly struggle with recognising unseen (or rarely seen) character sequences.\nIn this paper, we put forward a novel framework to specifically tackle this\n\"unseen\" problem. Our framework is iterative in nature, in that it utilises\npredicted knowledge of character sequences from a previous iteration, to\naugment the main network in improving the next prediction. Key to our success\nis a unique cross-modal variational autoencoder to act as a feedback module,\nwhich is trained with the presence of textual error distribution data. This\nmodule importantly translate a discrete predicted character space, to a\ncontinuous affine transformation parameter space used to condition the visual\nfeature map at next iteration. Experiments on common datasets have shown\ncompetitive performance over state-of-the-arts under the conventional setting.\nMost importantly, under the new disjoint setup where train-test labels are\nmutually exclusive, ours offers the best performance thus showcasing the\ncapability of generalising onto unseen words.",
          "link": "http://arxiv.org/abs/2107.12081",
          "publishedOn": "2021-07-27T02:03:33.335Z",
          "wordCount": 645,
          "title": "Towards the Unseen: Iterative Text Recognition by Distilling from Errors. (arXiv:2107.12081v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11882",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1\">Riqiang Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1\">Kaiwen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Ho Hin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deppen_S/0/1/0/all/0/1\">Steve Deppen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sandler_K/0/1/0/all/0/1\">Kim Sandler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Massion_P/0/1/0/all/0/1\">Pierre Massion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lasko_T/0/1/0/all/0/1\">Thomas A. Lasko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>",
          "description": "Data from multi-modality provide complementary information in clinical\nprediction, but missing data in clinical cohorts limits the number of subjects\nin multi-modal learning context. Multi-modal missing imputation is challenging\nwith existing methods when 1) the missing data span across heterogeneous\nmodalities (e.g., image vs. non-image); or 2) one modality is largely missing.\nIn this paper, we address imputation of missing data by modeling the joint\ndistribution of multi-modal data. Motivated by partial bidirectional generative\nadversarial net (PBiGAN), we propose a new Conditional PBiGAN (C-PBiGAN) method\nthat imputes one modality combining the conditional knowledge from another\nmodality. Specifically, C-PBiGAN introduces a conditional latent space in a\nmissing imputation framework that jointly encodes the available multi-modal\ndata, along with a class regularization loss on imputed data to recover\ndiscriminative information. To our knowledge, it is the first generative\nadversarial model that addresses multi-modal missing imputation by modeling the\njoint distribution of image and non-image data. We validate our model with both\nthe national lung screening trial (NLST) dataset and an external clinical\nvalidation cohort. The proposed C-PBiGAN achieves significant improvements in\nlung cancer risk estimation compared with representative imputation methods\n(e.g., AUC values increase in both NLST (+2.9\\%) and in-house dataset (+4.3\\%)\ncompared with PBiGAN, p$<$0.05).",
          "link": "http://arxiv.org/abs/2107.11882",
          "publishedOn": "2021-07-27T02:03:33.322Z",
          "wordCount": 683,
          "title": "Lung Cancer Risk Estimation with Incomplete Data: A Joint Missing Imputation Perspective. (arXiv:2107.11882v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wentian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heda Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinxiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Entity-aware image captioning aims to describe named entities and events\nrelated to the image by utilizing the background knowledge in the associated\narticle. This task remains challenging as it is difficult to learn the\nassociation between named entities and visual cues due to the long-tail\ndistribution of named entities. Furthermore, the complexity of the article\nbrings difficulty in extracting fine-grained relationships between entities to\ngenerate informative event descriptions about the image. To tackle these\nchallenges, we propose a novel approach that constructs a multi-modal knowledge\ngraph to associate the visual objects with named entities and capture the\nrelationship between entities simultaneously with the help of external\nknowledge collected from the web. Specifically, we build a text sub-graph by\nextracting named entities and their relationships from the article, and build\nan image sub-graph by detecting the objects in the image. To connect these two\nsub-graphs, we propose a cross-modal entity matching module trained using a\nknowledge base that contains Wikipedia entries and the corresponding images.\nFinally, the multi-modal knowledge graph is integrated into the captioning\nmodel via a graph attention mechanism. Extensive experiments on both GoodNews\nand NYTimes800k datasets demonstrate the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2107.11970",
          "publishedOn": "2021-07-27T02:03:33.315Z",
          "wordCount": 632,
          "title": "Boosting Entity-aware Image Captioning with Multi-modal Knowledge Graph. (arXiv:2107.11970v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laugros_A/0/1/0/all/0/1\">Alfred Laugros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caplier_A/0/1/0/all/0/1\">Alice Caplier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ospici_M/0/1/0/all/0/1\">Matthieu Ospici</a>",
          "description": "Synthetic corruptions gathered into a benchmark are frequently used to\nmeasure neural network robustness to distribution shifts. However, robustness\nto synthetic corruption benchmarks is not always predictive of robustness to\ndistribution shifts encountered in real-world applications. In this paper, we\npropose a methodology to build synthetic corruption benchmarks that make\nrobustness estimations more correlated with robustness to real-world\ndistribution shifts. Using the overlapping criterion, we split synthetic\ncorruptions into categories that help to better understand neural network\nrobustness. Based on these categories, we identify three parameters that are\nrelevant to take into account when constructing a corruption benchmark: number\nof represented categories, balance among categories and size of benchmarks.\nApplying the proposed methodology, we build a new benchmark called\nImageNet-Syn2Nat to predict image classifier robustness.",
          "link": "http://arxiv.org/abs/2107.12052",
          "publishedOn": "2021-07-27T02:03:33.308Z",
          "wordCount": 563,
          "title": "Using Synthetic Corruptions to Measure Robustness to Natural Distribution Shifts. (arXiv:2107.12052v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lujia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Road curb detection is important for autonomous driving. It can be used to\ndetermine road boundaries to constrain vehicles on roads, so that potential\naccidents could be avoided. Most of the current methods detect road curbs\nonline using vehicle-mounted sensors, such as cameras or 3-D Lidars. However,\nthese methods usually suffer from severe occlusion issues. Especially in\nhighly-dynamic traffic environments, most of the field of view is occupied by\ndynamic objects. To alleviate this issue, we detect road curbs offline using\nhigh-resolution aerial images in this paper. Moreover, the detected road curbs\ncan be used to create high-definition (HD) maps for autonomous vehicles.\nSpecifically, we first predict the pixel-wise segmentation map of road curbs,\nand then conduct a series of post-processing steps to extract the graph\nstructure of road curbs. To tackle the disconnectivity issue in the\nsegmentation maps, we propose an innovative connectivity-preserving loss\n(CP-loss) to improve the segmentation performance. The experimental results on\na public dataset demonstrate the effectiveness of our proposed loss function.\nThis paper is accompanied with a demonstration video and a supplementary\ndocument, which are available at\n\\texttt{\\url{https://sites.google.com/view/cp-loss}}.",
          "link": "http://arxiv.org/abs/2107.11920",
          "publishedOn": "2021-07-27T02:03:33.290Z",
          "wordCount": 648,
          "title": "CP-loss: Connectivity-preserving Loss for Road Curb Detection in Autonomous Driving with Aerial Images. (arXiv:2107.11920v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunlei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>",
          "description": "Few-shot learning aims to train a classifier that can generalize well when\njust a small number of labeled samples per class are given. We introduce\nTransductive Maximum Margin Classifier (TMMC) for few-shot learning. The basic\nidea of the classical maximum margin classifier is to solve an optimal\nprediction function that the corresponding separating hyperplane can correctly\ndivide the training data and the resulting classifier has the largest geometric\nmargin. In few-shot learning scenarios, the training samples are scarce, not\nenough to find a separating hyperplane with good generalization ability on\nunseen data. TMMC is constructed using a mixture of the labeled support set and\nthe unlabeled query set in a given task. The unlabeled samples in the query set\ncan adjust the separating hyperplane so that the prediction function is optimal\non both the labeled and unlabeled samples. Furthermore, we leverage an\nefficient and effective quasi-Newton algorithm, the L-BFGS method to optimize\nTMMC. Experimental results on three standard few-shot learning benchmarks\nincluding miniImagenet, tieredImagenet and CUB suggest that our TMMC achieves\nstate-of-the-art accuracies.",
          "link": "http://arxiv.org/abs/2107.11975",
          "publishedOn": "2021-07-27T02:03:33.284Z",
          "wordCount": 607,
          "title": "Transductive Maximum Margin Classifier for Few-Shot Learning. (arXiv:2107.11975v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11945",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Heran Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Liwei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zongben Xu</a>",
          "description": "Cross-contrast image translation is an important task for completing missing\ncontrasts in clinical diagnosis. However, most existing methods learn separate\ntranslator for each pair of contrasts, which is inefficient due to many\npossible contrast pairs in real scenarios. In this work, we propose a unified\nHyper-GAN model for effectively and efficiently translating between different\ncontrast pairs. Hyper-GAN consists of a pair of hyper-encoder and hyper-decoder\nto first map from the source contrast to a common feature space, and then\nfurther map to the target contrast image. To facilitate the translation between\ndifferent contrast pairs, contrast-modulators are designed to tune the\nhyper-encoder and hyper-decoder adaptive to different contrasts. We also design\na common space loss to enforce that multi-contrast images of a subject share a\ncommon feature space, implicitly modeling the shared underlying anatomical\nstructures. Experiments on two datasets of IXI and BraTS 2019 show that our\nHyper-GAN achieves state-of-the-art results in both accuracy and efficiency,\ne.g., improving more than 1.47 and 1.09 dB in PSNR on two datasets with less\nthan half the amount of parameters.",
          "link": "http://arxiv.org/abs/2107.11945",
          "publishedOn": "2021-07-27T02:03:33.275Z",
          "wordCount": 634,
          "title": "A Unified Hyper-GAN Model for Unpaired Multi-contrast MR Image Translation. (arXiv:2107.11945v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>",
          "description": "In spite of the successful application in many fields, machine learning\nalgorithms today suffer from notorious problems like vulnerability to\nadversarial examples. Beyond falling into the cat-and-mouse game between\nadversarial attack and defense, this paper provides alternative perspective to\nconsider adversarial example and explore whether we can exploit it in benign\napplications. We first propose a novel taxonomy of visual information along\ntask-relevance and semantic-orientation. The emergence of adversarial example\nis attributed to algorithm's utilization of task-relevant non-semantic\ninformation. While largely ignored in classical machine learning mechanisms,\ntask-relevant non-semantic information enjoys three interesting characteristics\nas (1) exclusive to algorithm, (2) reflecting common weakness, and (3)\nutilizable as features. Inspired by this, we present brave new idea called\nbenign adversarial attack to exploit adversarial examples for goodness in three\ndirections: (1) adversarial Turing test, (2) rejecting malicious algorithm, and\n(3) adversarial data augmentation. Each direction is positioned with motivation\nelaboration, justification analysis and prototype applications to showcase its\npotential.",
          "link": "http://arxiv.org/abs/2107.11986",
          "publishedOn": "2021-07-27T02:03:33.268Z",
          "wordCount": 597,
          "title": "Benign Adversarial Attack: Tricking Algorithm for Goodness. (arXiv:2107.11986v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziyi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaofei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianjun Zhao</a>",
          "description": "Motion blur caused by the moving of the object or camera during the exposure\ncan be a key challenge for visual object tracking, affecting tracking accuracy\nsignificantly. In this work, we explore the robustness of visual object\ntrackers against motion blur from a new angle, i.e., adversarial blur attack\n(ABA). Our main objective is to online transfer input frames to their natural\nmotion-blurred counterparts while misleading the state-of-the-art trackers\nduring the tracking process. To this end, we first design the motion blur\nsynthesizing method for visual tracking based on the generation principle of\nmotion blur, considering the motion information and the light accumulation\nprocess. With this synthetic method, we propose \\textit{optimization-based ABA\n(OP-ABA)} by iteratively optimizing an adversarial objective function against\nthe tracking w.r.t. the motion and light accumulation parameters. The OP-ABA is\nable to produce natural adversarial examples but the iteration can cause heavy\ntime cost, making it unsuitable for attacking real-time trackers. To alleviate\nthis issue, we further propose \\textit{one-step ABA (OS-ABA)} where we design\nand train a joint adversarial motion and accumulation predictive network\n(JAMANet) with the guidance of OP-ABA, which is able to efficiently estimate\nthe adversarial motion and accumulation parameters in a one-step way. The\nexperiments on four popular datasets (\\eg, OTB100, VOT2018, UAV123, and LaSOT)\ndemonstrate that our methods are able to cause significant accuracy drops on\nfour state-of-the-art trackers with high transferability. Please find the\nsource code at https://github.com/tsingqguo/ABA",
          "link": "http://arxiv.org/abs/2107.12085",
          "publishedOn": "2021-07-27T02:03:33.261Z",
          "wordCount": 694,
          "title": "Learning to Adversarially Blur Visual Object Tracking. (arXiv:2107.12085v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1\">Aneeshan Sain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Amandeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Shuvozit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>",
          "description": "Although text recognition has significantly evolved over the years,\nstate-of-the-art (SOTA) models still struggle in the wild scenarios due to\ncomplex backgrounds, varying fonts, uncontrolled illuminations, distortions and\nother artefacts. This is because such models solely depend on visual\ninformation for text recognition, thus lacking semantic reasoning capabilities.\nIn this paper, we argue that semantic information offers a complementary role\nin addition to visual only. More specifically, we additionally utilize semantic\ninformation by proposing a multi-stage multi-scale attentional decoder that\nperforms joint visual-semantic reasoning. Our novelty lies in the intuition\nthat for text recognition, the prediction should be refined in a stage-wise\nmanner. Therefore our key contribution is in designing a stage-wise unrolling\nattentional decoder where non-differentiability, invoked by discretely\npredicted character labels, needs to be bypassed for end-to-end training. While\nthe first stage predicts using visual features, subsequent stages refine on top\nof it using joint visual-semantic information. Additionally, we introduce\nmulti-scale 2D attention along with dense and residual connections between\ndifferent stages to deal with varying scales of character sizes, for better\nperformance and faster convergence during training. Experimental results show\nour approach to outperform existing SOTA methods by a considerable margin.",
          "link": "http://arxiv.org/abs/2107.12090",
          "publishedOn": "2021-07-27T02:03:33.242Z",
          "wordCount": 647,
          "title": "Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition. (arXiv:2107.12090v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yalong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mohan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "Data augmentation is practically helpful for visual recognition, especially\nat the time of data scarcity. However, such success is only limited to quite a\nfew light augmentations (e.g., random crop, flip). Heavy augmentations (e.g.,\ngray, grid shuffle) are either unstable or show adverse effects during\ntraining, owing to the big gap between the original and augmented images. This\npaper introduces a novel network design, noted as Augmentation Pathways (AP),\nto systematically stabilize training on a much wider range of augmentation\npolicies. Notably, AP tames heavy data augmentations and stably boosts\nperformance without a careful selection among augmentation policies. Unlike\ntraditional single pathway, augmented images are processed in different neural\npaths. The main pathway handles light augmentations, while other pathways focus\non heavy augmentations. By interacting with multiple paths in a dependent\nmanner, the backbone network robustly learns from shared visual patterns among\naugmentations, and suppresses noisy patterns at the same time. Furthermore, we\nextend AP to a homogeneous version and a heterogeneous version for high-order\nscenarios, demonstrating its robustness and flexibility in practical usage.\nExperimental results on ImageNet benchmarks demonstrate the compatibility and\neffectiveness on a much wider range of augmentations (e.g., Crop, Gray, Grid\nShuffle, RandAugment), while consuming fewer parameters and lower computational\ncosts at inference time. Source code:https://github.com/ap-conv/ap-net.",
          "link": "http://arxiv.org/abs/2107.11990",
          "publishedOn": "2021-07-27T02:03:33.236Z",
          "wordCount": 648,
          "title": "Augmentation Pathways Network for Visual Recognition. (arXiv:2107.11990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12009",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cahan_N/0/1/0/all/0/1\">Noa Cahan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marom_E/0/1/0/all/0/1\">Edith M. Marom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soffer_S/0/1/0/all/0/1\">Shelly Soffer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barash_Y/0/1/0/all/0/1\">Yiftach Barash</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Konen_E/0/1/0/all/0/1\">Eli Konen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klang_E/0/1/0/all/0/1\">Eyal Klang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greenspan_H/0/1/0/all/0/1\">Hayit Greenspan</a>",
          "description": "Pulmonary embolus (PE) refers to obstruction of pulmonary arteries by blood\nclots. PE accounts for approximately 100,000 deaths per year in the United\nStates alone. The clinical presentation of PE is often nonspecific, making the\ndiagnosis challenging. Thus, rapid and accurate risk stratification is of\nparamount importance. High-risk PE is caused by right ventricular (RV)\ndysfunction from acute pressure overload, which in return can help identify\nwhich patients require more aggressive therapy. Reconstructed four-chamber\nviews of the heart on chest CT can detect right ventricular enlargement. CT\npulmonary angiography (CTPA) is the golden standard in the diagnostic workup of\nsuspected PE. Therefore, it can link between diagnosis and risk stratification\nstrategies. We developed a weakly supervised deep learning algorithm, with an\nemphasis on a novel attention mechanism, to automatically classify RV strain on\nCTPA. Our method is a 3D DenseNet model with integrated 3D residual attention\nblocks. We evaluated our model on a dataset of CTPAs of emergency department\n(ED) PE patients. This model achieved an area under the receiver operating\ncharacteristic curve (AUC) of 0.88 for classifying RV strain. The model showed\na sensitivity of 87% and specificity of 83.7%. Our solution outperforms\nstate-of-the-art 3D CNN networks. The proposed design allows for a fully\nautomated network that can be trained easily in an end-to-end manner without\nrequiring computationally intensive and time-consuming preprocessing or\nstrenuous labeling of the data.We infer that unmarked CTPAs can be used for\neffective RV strain classification. This could be used as a second reader,\nalerting for high-risk PE patients. To the best of our knowledge, there are no\nprevious deep learning-based studies that attempted to solve this problem.",
          "link": "http://arxiv.org/abs/2107.12009",
          "publishedOn": "2021-07-27T02:03:33.229Z",
          "wordCount": 751,
          "title": "Weakly Supervised Attention Model for RV StrainClassification from volumetric CTPA Scans. (arXiv:2107.12009v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maureira_J/0/1/0/all/0/1\">Jose Maureira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1\">Juan Tapia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arellano_C/0/1/0/all/0/1\">Claudia Arellano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Biometric has been increasing in relevance these days since it can be used\nfor several applications such as access control for instance. Unfortunately,\nwith the increased deployment of biometric applications, we observe an increase\nof attacks. Therefore, algorithms to detect such attacks (Presentation Attack\nDetection (PAD)) have been increasing in relevance. The LivDet-2020 competition\nwhich focuses on Presentation Attacks Detection (PAD) algorithms have shown\nstill open problems, specially for unknown attacks scenarios. In order to\nimprove the robustness of biometric systems, it is crucial to improve PAD\nmethods. This can be achieved by augmenting the number of presentation attack\ninstruments (PAI) and bona fide images that are used to train such algorithms.\nUnfortunately, the capture and creation of presentation attack instruments and\neven the capture of bona fide images is sometimes complex to achieve. This\npaper proposes a novel PAI synthetically created (SPI-PAI) using four\nstate-of-the-art GAN algorithms (cGAN, WGAN, WGAN-GP, and StyleGAN2) and a\nsmall set of periocular NIR images. A benchmark between GAN algorithms is\nperformed using the Frechet Inception Distance (FID) between the generated\nimages and the original images used for training. The best PAD algorithm\nreported by the LivDet-2020 competition was tested for us using the synthetic\nPAI which was obtained with the StyleGAN2 algorithm. Surprisingly, The PAD\nalgorithm was not able to detect the synthetic images as a Presentation Attack,\ncategorizing all of them as bona fide. Such results demonstrated the\nfeasibility of synthetic images to fool presentation attacks detection\nalgorithms and the need for such algorithms to be constantly updated and\ntrained with a larger number of images and PAI scenarios.",
          "link": "http://arxiv.org/abs/2107.12014",
          "publishedOn": "2021-07-27T02:03:33.216Z",
          "wordCount": 707,
          "title": "Synthetic Periocular Iris PAI from a Small Set of Near-Infrared-Images. (arXiv:2107.12014v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1\">Aneeshan Sain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>",
          "description": "Text recognition remains a fundamental and extensively researched topic in\ncomputer vision, largely owing to its wide array of commercial applications.\nThe challenging nature of the very problem however dictated a fragmentation of\nresearch efforts: Scene Text Recognition (STR) that deals with text in everyday\nscenes, and Handwriting Text Recognition (HTR) that tackles hand-written text.\nIn this paper, for the first time, we argue for their unification -- we aim for\na single model that can compete favourably with two separate state-of-the-art\nSTR and HTR models. We first show that cross-utilisation of STR and HTR models\ntrigger significant performance drops due to differences in their inherent\nchallenges. We then tackle their union by introducing a knowledge distillation\n(KD) based framework. This is however non-trivial, largely due to the\nvariable-length and sequential nature of text sequences, which renders\noff-the-shelf KD techniques that mostly works with global fixed-length data\ninadequate. For that, we propose three distillation losses all of which are\nspecifically designed to cope with the aforementioned unique characteristics of\ntext recognition. Empirical evidence suggests that our proposed unified model\nperforms on par with individual models, even surpassing them in certain cases.\nAblative studies demonstrate that naive baselines such as a two-stage\nframework, and domain adaption/generalisation alternatives do not work as well,\nfurther verifying the appropriateness of our design.",
          "link": "http://arxiv.org/abs/2107.12087",
          "publishedOn": "2021-07-27T02:03:33.208Z",
          "wordCount": 673,
          "title": "Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation. (arXiv:2107.12087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aich_A/0/1/0/all/0/1\">Abhishek Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "Despite much recent progress in video-based person re-identification (re-ID),\nthe current state-of-the-art still suffers from common real-world challenges\nsuch as appearance similarity among various people, occlusions, and frame\nmisalignment. To alleviate these problems, we propose Spatio-Temporal\nRepresentation Factorization module (STRF), a flexible new computational unit\nthat can be used in conjunction with most existing 3D convolutional neural\nnetwork architectures for re-ID. The key innovations of STRF over prior work\ninclude explicit pathways for learning discriminative temporal and spatial\nfeatures, with each component further factorized to capture complementary\nperson-specific appearance and motion information. Specifically, temporal\nfactorization comprises two branches, one each for static features (e.g., the\ncolor of clothes) that do not change much over time, and dynamic features\n(e.g., walking patterns) that change over time. Further, spatial factorization\nalso comprises two branches to learn both global (coarse segments) as well as\nlocal (finer segments) appearance features, with the local features\nparticularly useful in cases of occlusion or spatial misalignment. These two\nfactorization operations taken together result in a modular architecture for\nour parameter-wise economic STRF unit that can be plugged in between any two 3D\nconvolutional layers, resulting in an end-to-end learning framework. We\nempirically show that STRF improves performance of various existing baseline\narchitectures while demonstrating new state-of-the-art results using standard\nperson re-identification evaluation protocols on three benchmarks.",
          "link": "http://arxiv.org/abs/2107.11878",
          "publishedOn": "2021-07-27T02:03:33.173Z",
          "wordCount": 666,
          "title": "Spatio-Temporal Representation Factorization for Video-based Person Re-Identification. (arXiv:2107.11878v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunlei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>",
          "description": "The goal of few-shot video classification is to learn a classification model\nwith good generalization ability when trained with only a few labeled videos.\nHowever, it is difficult to learn discriminative feature representations for\nvideos in such a setting. In this paper, we propose Temporal Alignment\nPrediction (TAP) based on sequence similarity learning for few-shot video\nclassification. In order to obtain the similarity of a pair of videos, we\npredict the alignment scores between all pairs of temporal positions in the two\nvideos with the temporal alignment prediction function. Besides, the inputs to\nthis function are also equipped with the context information in the temporal\ndomain. We evaluate TAP on two video classification benchmarks including\nKinetics and Something-Something V2. The experimental results verify the\neffectiveness of TAP and show its superiority over state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.11960",
          "publishedOn": "2021-07-27T02:03:33.165Z",
          "wordCount": 568,
          "title": "Temporal Alignment Prediction for Few-Shot Video Classification. (arXiv:2107.11960v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1\">Baorui Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuigeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>",
          "description": "Scene video text spotting (SVTS) is a very important research topic because\nof many real-life applications. However, only a little effort has put to\nspotting scene video text, in contrast to massive studies of scene text\nspotting in static images. Due to various environmental interferences like\nmotion blur, spotting scene video text becomes very challenging. To promote\nthis research area, this competition introduces a new challenge dataset\ncontaining 129 video clips from 21 natural scenarios in full annotations. The\ncompetition containts three tasks, that is, video text detection (Task 1),\nvideo text tracking (Task 2) and end-to-end video text spotting (Task3). During\nthe competition period (opened on 1st March, 2021 and closed on 11th April,\n2021), a total of 24 teams participated in the three proposed tasks with 46\nvalid submissions, respectively. This paper includes dataset descriptions, task\ndefinitions, evaluation protocols and results summaries of the ICDAR 2021 on\nSVTS competition. Thanks to the healthy number of teams as well as submissions,\nwe consider that the SVTS competition has been successfully held, drawing much\nattention from the community and promoting the field research and its\ndevelopment.",
          "link": "http://arxiv.org/abs/2107.11919",
          "publishedOn": "2021-07-27T02:03:33.092Z",
          "wordCount": 631,
          "title": "ICDAR 2021 Competition on Scene Video Text Spotting. (arXiv:2107.11919v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1\">Oscar Mendez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vowels_M/0/1/0/all/0/1\">Matthew Vowels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>",
          "description": "Attention is an important component of modern deep learning. However, less\nemphasis has been put on its inverse: ignoring distraction. Our daily lives\nrequire us to explicitly avoid giving attention to salient visual features that\nconfound the task we are trying to accomplish. This visual prioritisation\nallows us to concentrate on important tasks while ignoring visual distractors.\n\nIn this work, we introduce Neural Blindness, which gives an agent the ability\nto completely ignore objects or classes that are deemed distractors. More\nexplicitly, we aim to render a neural network completely incapable of\nrepresenting specific chosen classes in its latent space. In a very real sense,\nthis makes the network \"blind\" to certain classes, allowing and agent to focus\non what is important for a given task, and demonstrates how this can be used to\nimprove localisation.",
          "link": "http://arxiv.org/abs/2107.11857",
          "publishedOn": "2021-07-27T02:03:33.067Z",
          "wordCount": 578,
          "title": "Improving Robot Localisation by Ignoring Visual Distraction. (arXiv:2107.11857v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zilun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shihao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichun Zhang</a>",
          "description": "Most few-shot learning models utilize only one modality of data. We would\nlike to investigate qualitatively and quantitatively how much will the model\nimprove if we add an extra modality (i.e. text description of the image), and\nhow it affects the learning procedure. To achieve this goal, we propose four\ntypes of fusion method to combine the image feature and text feature. To verify\nthe effectiveness of improvement, we test the fusion methods with two classical\nfew-shot learning models - ProtoNet and MAML, with image feature extractors\nsuch as ConvNet and ResNet12. The attention-based fusion method works best,\nwhich improves the classification accuracy by a large margin around 30%\ncomparing to the baseline result.",
          "link": "http://arxiv.org/abs/2107.11853",
          "publishedOn": "2021-07-27T02:03:33.061Z",
          "wordCount": 550,
          "title": "Will Multi-modal Data Improves Few-shot Learning?. (arXiv:2107.11853v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hanxi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinxiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "How to model fine-grained spatial-temporal dynamics in videos has been a\nchallenging problem for action recognition. It requires learning deep and rich\nfeatures with superior distinctiveness for the subtle and abstract motions.\nMost existing methods generate features of a layer in a pure feedforward\nmanner, where the information moves in one direction from inputs to outputs.\nAnd they rely on stacking more layers to obtain more powerful features,\nbringing extra non-negligible overheads. In this paper, we propose an Adaptive\nRecursive Circle (ARC) framework, a fine-grained decorator for pure feedforward\nlayers. It inherits the operators and parameters of the original layer but is\nslightly different in the use of those operators and parameters. Specifically,\nthe input of the layer is treated as an evolving state, and its update is\nalternated with the feature generation. At each recursive step, the input state\nis enriched by the previously generated features and the feature generation is\nmade with the newly updated input state. We hope the ARC framework can\nfacilitate fine-grained action recognition by introducing deeply refined\nfeatures and multi-scale receptive fields at a low cost. Significant\nimprovements over feedforward baselines are observed on several benchmarks. For\nexample, an ARC-equipped TSM-ResNet18 outperforms TSM-ResNet50 with 48% fewer\nFLOPs and 52% model parameters on Something-Something V1 and Diving48.",
          "link": "http://arxiv.org/abs/2107.11813",
          "publishedOn": "2021-07-27T02:03:33.042Z",
          "wordCount": 646,
          "title": "Adaptive Recursive Circle Framework for Fine-grained Action Recognition. (arXiv:2107.11813v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "Among numerous videos shared on the web, well-edited ones always attract more\nattention. However, it is difficult for inexperienced users to make well-edited\nvideos because it requires professional expertise and immense manual labor. To\nmeet the demands for non-experts, we present Transcript-to-Video -- a\nweakly-supervised framework that uses texts as input to automatically create\nvideo sequences from an extensive collection of shots. Specifically, we propose\na Content Retrieval Module and a Temporal Coherent Module to learn\nvisual-language representations and model shot sequencing styles, respectively.\nFor fast inference, we introduce an efficient search strategy for real-time\nvideo clip sequencing. Quantitative results and user studies demonstrate\nempirically that the proposed learning framework can retrieve content-relevant\nshots while creating plausible video sequences in terms of style. Besides, the\nrun-time performance analysis shows that our framework can support real-world\napplications.",
          "link": "http://arxiv.org/abs/2107.11851",
          "publishedOn": "2021-07-27T02:03:33.035Z",
          "wordCount": 581,
          "title": "Transcript to Video: Efficient Clip Sequencing from Texts. (arXiv:2107.11851v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abedin_T/0/1/0/all/0/1\">Thasin Abedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prottoy_K/0/1/0/all/0/1\">Khondokar S. S. Prottoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshruba_A/0/1/0/all/0/1\">Ayana Moshruba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakim_S/0/1/0/all/0/1\">Safayat Bin Hakim</a>",
          "description": "Sign language is the only medium of communication for the hearing impaired\nand the deaf and dumb community. Communication with the general mass is thus\nalways a challenge for this minority group. Especially in Bangla sign language\n(BdSL), there are 38 alphabets with some having nearly identical symbols. As a\nresult, in BdSL recognition, the posture of hand is an important factor in\naddition to visual features extracted from traditional Convolutional Neural\nNetwork (CNN). In this paper, a novel architecture \"Concatenated BdSL Network\"\nis proposed which consists of a CNN based image network and a pose estimation\nnetwork. While the image network gets the visual features, the relative\npositions of hand keypoints are taken by the pose estimation network to obtain\nthe additional features to deal with the complexity of the BdSL symbols. A\nscore of 91.51% was achieved by this novel approach in test set and the\neffectiveness of the additional pose estimation network is suggested by the\nexperimental results.",
          "link": "http://arxiv.org/abs/2107.11818",
          "publishedOn": "2021-07-27T02:03:33.028Z",
          "wordCount": 602,
          "title": "Bangla sign language recognition using concatenated BdSL network. (arXiv:2107.11818v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussaid_F/0/1/0/all/0/1\">Farid Boussaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohel_F/0/1/0/all/0/1\">Ferdous Sohel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>",
          "description": "Semantic segmentation is a challenging task in the absence of densely\nlabelled data. Only relying on class activation maps (CAM) with image-level\nlabels provides deficient segmentation supervision. Prior works thus consider\npre-trained models to produce coarse saliency maps to guide the generation of\npseudo segmentation labels. However, the commonly used off-line heuristic\ngeneration process cannot fully exploit the benefits of these coarse saliency\nmaps. Motivated by the significant inter-task correlation, we propose a novel\nweakly supervised multi-task framework termed as AuxSegNet, to leverage\nsaliency detection and multi-label image classification as auxiliary tasks to\nimprove the primary task of semantic segmentation using only image-level\nground-truth labels. Inspired by their similar structured semantics, we also\npropose to learn a cross-task global pixel-level affinity map from the saliency\nand segmentation representations. The learned cross-task affinity can be used\nto refine saliency predictions and propagate CAM maps to provide improved\npseudo labels for both tasks. The mutual boost between pseudo label updating\nand cross-task affinity learning enables iterative improvements on segmentation\nperformance. Extensive experiments demonstrate the effectiveness of the\nproposed auxiliary learning network structure and the cross-task affinity\nlearning method. The proposed approach achieves state-of-the-art weakly\nsupervised segmentation performance on the challenging PASCAL VOC 2012 and MS\nCOCO benchmarks.",
          "link": "http://arxiv.org/abs/2107.11787",
          "publishedOn": "2021-07-27T02:03:33.021Z",
          "wordCount": 656,
          "title": "Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation. (arXiv:2107.11787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Pengwen Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>",
          "description": "Numerous scene text detection methods have been proposed in recent years.\nMost of them declare they have achieved state-of-the-art performances. However,\nthe performance comparison is unfair, due to lots of inconsistent settings\n(e.g., training data, backbone network, multi-scale feature fusion, evaluation\nprotocols, etc.). These various settings would dissemble the pros and cons of\nthe proposed core techniques. In this paper, we carefully examine and analyze\nthe inconsistent settings, and propose a unified framework for the bottom-up\nbased scene text detection methods. Under the unified framework, we ensure the\nconsistent settings for non-core modules, and mainly investigate the\nrepresentations of describing arbitrary-shape scene texts, e.g., regressing\npoints on text contours, clustering pixels with predicted auxiliary\ninformation, grouping connected components with learned linkages, etc. With the\ncomprehensive investigations and elaborate analyses, it not only cleans up the\nobstacle of understanding the performance differences between existing methods\nbut also reveals the advantages and disadvantages of previous models under fair\ncomparisons.",
          "link": "http://arxiv.org/abs/2107.11800",
          "publishedOn": "2021-07-27T02:03:33.014Z",
          "wordCount": 586,
          "title": "Comprehensive Studies for Arbitrary-shape Scene Text Detection. (arXiv:2107.11800v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Preethi_P/0/1/0/all/0/1\">P Preethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1\">Hrishikesh Viswanath</a>",
          "description": "This paper is a presentation of a new method for denoising images using\nHaralick features and further segmenting the characters using artificial neural\nnetworks. The image is divided into kernels, each of which is converted to a\nGLCM (Gray Level Co-Occurrence Matrix) on which a Haralick Feature generation\nfunction is called, the result of which is an array with fourteen elements\ncorresponding to fourteen features The Haralick values and the corresponding\nnoise/text classification form a dictionary, which is then used to de-noise the\nimage through kernel comparison. Segmentation is the process of extracting\ncharacters from a document and can be used when letters are separated by white\nspace, which is an explicit boundary marker. Segmentation is the first step in\nmany Natural Language Processing problems. This paper explores the process of\nsegmentation using Neural Networks. While there have been numerous methods to\nsegment characters of a document, this paper is only concerned with the\naccuracy of doing so using neural networks. It is imperative that the\ncharacters be segmented correctly, for failing to do so will lead to incorrect\nrecognition by Natural language processing tools. Artificial Neural Networks\nwas used to attain accuracy of upto 89%. This method is suitable for languages\nwhere the characters are delimited by white space. However, this method will\nfail to provide acceptable results when the language heavily uses connected\nletters. An example would be the Devanagari script, which is predominantly used\nin northern India.",
          "link": "http://arxiv.org/abs/2107.11801",
          "publishedOn": "2021-07-27T02:03:32.994Z",
          "wordCount": 677,
          "title": "Denoising and Segmentation of Epigraphical Scripts. (arXiv:2107.11801v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Preethi_P/0/1/0/all/0/1\">P Preethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1\">Hrishikesh Viswanath</a>",
          "description": "This work presents a comparison of machine learning algorithms that are\nimplemented to segment the characters of text presented as an image. The\nalgorithms are designed to work on degraded documents with text that is not\naligned in an organized fashion. The paper investigates the use of Support\nVector Machines, K-Nearest Neighbor algorithm and an Encoder Network to perform\nthe operation of character spotting. Character Spotting involves extracting\npotential characters from a stream of text by selecting regions bound by white\nspace.",
          "link": "http://arxiv.org/abs/2107.11795",
          "publishedOn": "2021-07-27T02:03:32.987Z",
          "wordCount": 518,
          "title": "Character Spotting Using Machine Learning Techniques. (arXiv:2107.11795v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11786",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ozyoruk_K/0/1/0/all/0/1\">Kutsev Bengisu Ozyoruk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Can_S/0/1/0/all/0/1\">Sermet Can</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gokceler_G/0/1/0/all/0/1\">Guliz Irem Gokceler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Basak_K/0/1/0/all/0/1\">Kayhan Basak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demir_D/0/1/0/all/0/1\">Derya Demir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Serin_G/0/1/0/all/0/1\">Gurdeniz Serin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hacisalihoglu_U/0/1/0/all/0/1\">Uguray Payam Hacisalihoglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Darbaz_B/0/1/0/all/0/1\">Berkan Darbaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_F/0/1/0/all/0/1\">Funda Yilmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turan_M/0/1/0/all/0/1\">Mehmet Turan</a>",
          "description": "Frozen sectioning (FS) is the preparation method of choice for microscopic\nevaluation of tissues during surgical operations. The high speed of procedure\nallows pathologists to rapidly assess the key microscopic features, such as\ntumor margins and malignant status to guide surgical decision-making and\nminimise disruptions to the course of the operation. However, FS is prone to\nintroducing many misleading artificial structures (histological artefacts),\nsuch as nuclear ice crystals, compression, and cutting artefacts, hindering\ntimely and accurate diagnostic judgement of the pathologist. On the other hand,\nthe gold standard tissue preparation technique of formalin-fixation and\nparaffin-embedding (FFPE) provides significantly superior image quality, but is\na very time-consuming process (12-48 hours), making it unsuitable for\nintra-operative use. In this paper, we propose an artificial intelligence (AI)\nmethod that improves FS image quality by computationally transforming\nfrozen-sectioned whole-slide images (FS-WSIs) into whole-slide FFPE-style\nimages in minutes. AI-FFPE rectifies FS artefacts with the guidance of an\nattention-mechanism that puts a particular emphasis on artefacts while\nutilising a self-regularization mechanism established between FS input image\nand synthesized FFPE-style image that preserves clinically relevant features.\nAs a result, AI-FFPE method successfully generates FFPE-style images without\nsignificantly extending tissue processing time and consequently improves\ndiagnostic accuracy.",
          "link": "http://arxiv.org/abs/2107.11786",
          "publishedOn": "2021-07-27T02:03:32.980Z",
          "wordCount": 669,
          "title": "Deep Learning-based Frozen Section to FFPE Translation. (arXiv:2107.11786v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Habibpour_M/0/1/0/all/0/1\">Maryam Habibpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharoun_H/0/1/0/all/0/1\">Hassan Gharoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajally_A/0/1/0/all/0/1\">AmirReza Tajally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsi_A/0/1/0/all/0/1\">Afshar Shamsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgharnezhad_H/0/1/0/all/0/1\">Hamzeh Asgharnezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>",
          "description": "Defects are unavoidable in casting production owing to the complexity of the\ncasting process. While conventional human-visual inspection of casting products\nis slow and unproductive in mass productions, an automatic and reliable defect\ndetection not just enhances the quality control process but positively improves\nproductivity. However, casting defect detection is a challenging task due to\ndiversity and variation in defects' appearance. Convolutional neural networks\n(CNNs) have been widely applied in both image classification and defect\ndetection tasks. Howbeit, CNNs with frequentist inference require a massive\namount of data to train on and still fall short in reporting beneficial\nestimates of their predictive uncertainty. Accordingly, leveraging the transfer\nlearning paradigm, we first apply four powerful CNN-based models (VGG16,\nResNet50, DenseNet121, and InceptionResNetV2) on a small dataset to extract\nmeaningful features. Extracted features are then processed by various machine\nlearning algorithms to perform the classification task. Simulation results\ndemonstrate that linear support vector machine (SVM) and multi-layer perceptron\n(MLP) show the finest performance in defect detection of casting images.\nSecondly, to achieve a reliable classification and to measure epistemic\nuncertainty, we employ an uncertainty quantification (UQ) technique (ensemble\nof MLP models) using features extracted from four pre-trained CNNs. UQ\nconfusion matrix and uncertainty accuracy metric are also utilized to evaluate\nthe predictive uncertainty estimates. Comprehensive comparisons reveal that UQ\nmethod based on VGG16 outperforms others to fetch uncertainty. We believe an\nuncertainty-aware automatic defect detection solution will reinforce casting\nproductions quality assurance.",
          "link": "http://arxiv.org/abs/2107.11643",
          "publishedOn": "2021-07-27T02:03:32.974Z",
          "wordCount": 695,
          "title": "An Uncertainty-Aware Deep Learning Framework for Defect Detection in Casting Products. (arXiv:2107.11643v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Ziji Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>",
          "description": "The transformer has recently achieved impressive results on various tasks. To\nfurther improve the effectiveness and efficiency of the transformer, there are\ntwo trains of thought among existing works: (1) going wider by scaling to more\ntrainable parameters; (2) going shallower by parameter sharing or model\ncompressing along with the depth. However, larger models usually do not scale\nwell when fewer tokens are available to train, and advanced parallelisms are\nrequired when the model is extremely large. Smaller models usually achieve\ninferior performance compared to the original transformer model due to the loss\nof representation power. In this paper, to achieve better performance with\nfewer trainable parameters, we propose a framework to deploy trainable\nparameters efficiently, by going wider instead of deeper. Specially, we scale\nalong model width by replacing feed-forward network (FFN) with\nmixture-of-experts (MoE). We then share the MoE layers across transformer\nblocks using individual layer normalization. Such deployment plays the role to\ntransform various semantic representations, which makes the model more\nparameter-efficient and effective. To evaluate our framework, we design WideNet\nand evaluate it on ImageNet-1K. Our best model outperforms Vision Transformer\n(ViT) by $1.46\\%$ with $0.72 \\times$ trainable parameters. Using $0.46 \\times$\nand $0.13 \\times$ parameters, our WideNet can still surpass ViT and ViT-MoE by\n$0.83\\%$ and $2.08\\%$, respectively.",
          "link": "http://arxiv.org/abs/2107.11817",
          "publishedOn": "2021-07-27T02:03:32.966Z",
          "wordCount": 650,
          "title": "Go Wider Instead of Deeper. (arXiv:2107.11817v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abou_Kreisha_M/0/1/0/all/0/1\">Mohamed Taha Abou-Kreisha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elnashar_H/0/1/0/all/0/1\">Hany Elnashar</a>",
          "description": "Automated Vehicle License Plate (VLP) detection and recognition have ended up\nbeing a significant research issue as of late. VLP localization and recognition\nare some of the most essential techniques for managing traffic using digital\ntechniques. In this paper, four smart systems are developed to recognize\nEgyptian vehicles license plates. Two systems are based on character\nrecognition, which are (System1, Characters Recognition with Classical Machine\nLearning) and (System2, Characters Recognition with Deep Machine Learning). The\nother two systems are based on the whole plate recognition which are (System3,\nWhole License Plate Recognition with Classical Machine Learning) and (System4,\nWhole License Plate Recognition with Deep Machine Learning). We use object\ndetection algorithms, and machine learning based object recognition algorithms.\nThe performance of the developed systems has been tested on real images, and\nthe experimental results demonstrate that the best detection accuracy rate for\nVLP is provided by using the deep learning method. Where the VLP detection\naccuracy rate is better than the classical system by 32%. However, the best\ndetection accuracy rate for Vehicle License Plate Arabic Character (VLPAC) is\nprovided by using the classical method. Where VLPAC detection accuracy rate is\nbetter than the deep learning-based system by 6%. Also, the results show that\ndeep learning is better than the classical technique used in VLP recognition\nprocesses. Where the recognition accuracy rate is better than the classical\nsystem by 8%. Finally, the paper output recommends a robust VLP recognition\nsystem based on both statistical and deep machine learning.",
          "link": "http://arxiv.org/abs/2107.11640",
          "publishedOn": "2021-07-27T02:03:32.941Z",
          "wordCount": 722,
          "title": "Deep Machine Learning Based Egyptian Vehicle License Plate Recognition Systems. (arXiv:2107.11640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bose_R/0/1/0/all/0/1\">Rupak Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pande_S/0/1/0/all/0/1\">Shivam Pande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>",
          "description": "As the field of remote sensing is evolving, we witness the accumulation of\ninformation from several modalities, such as multispectral (MS), hyperspectral\n(HSI), LiDAR etc. Each of these modalities possess its own distinct\ncharacteristics and when combined synergistically, perform very well in the\nrecognition and classification tasks. However, fusing multiple modalities in\nremote sensing is cumbersome due to highly disparate domains. Furthermore, the\nexisting methods do not facilitate cross-modal interactions. To this end, we\npropose a novel transformer based fusion method for HSI and LiDAR modalities.\nThe model is composed of stacked auto encoders that harness the cross key-value\npairs for HSI and LiDAR, thus establishing a communication between the two\nmodalities, while simultaneously using the CNNs to extract the spectral and\nspatial information from HSI and LiDAR. We test our model on Houston (Data\nFusion Contest - 2013) and MUUFL Gulfport datasets and achieve competitive\nresults.",
          "link": "http://arxiv.org/abs/2107.11585",
          "publishedOn": "2021-07-27T02:03:32.933Z",
          "wordCount": 605,
          "title": "Two Headed Dragons: Multimodal Fusion and Cross Modal Transactions. (arXiv:2107.11585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhaohui Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiyong Gao</a>",
          "description": "Bicubic downscaling is a prevalent technique used to reduce the video storage\nburden or to accelerate the downstream processing speed. However, the inverse\nupscaling step is non-trivial, and the downscaled video may also deteriorate\nthe performance of downstream tasks. In this paper, we propose a\nself-conditioned probabilistic framework for video rescaling to learn the\npaired downscaling and upscaling procedures simultaneously. During the\ntraining, we decrease the entropy of the information lost in the downscaling by\nmaximizing its probability conditioned on the strong spatial-temporal prior\ninformation within the downscaled video. After optimization, the downscaled\nvideo by our framework preserves more meaningful information, which is\nbeneficial for both the upscaling step and the downstream tasks, e.g., video\naction recognition task. We further extend the framework to a lossy video\ncompression system, in which a gradient estimator for non-differential\nindustrial lossy codecs is proposed for the end-to-end training of the whole\nsystem. Extensive experimental results demonstrate the superiority of our\napproach on video rescaling, video compression, and efficient action\nrecognition tasks.",
          "link": "http://arxiv.org/abs/2107.11639",
          "publishedOn": "2021-07-27T02:03:32.919Z",
          "wordCount": 610,
          "title": "Self-Conditioned Probabilistic Learning of Video Rescaling. (arXiv:2107.11639v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Man_Y/0/1/0/all/0/1\">Yunze Man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1\">Xinshuo Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakuma_P/0/1/0/all/0/1\">Prasanna Kumar Sivakuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OToole_M/0/1/0/all/0/1\">Matthew O&#x27;Toole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>",
          "description": "LiDAR sensors can be used to obtain a wide range of measurement signals other\nthan a simple 3D point cloud, and those signals can be leveraged to improve\nperception tasks like 3D object detection. A single laser pulse can be\npartially reflected by multiple objects along its path, resulting in multiple\nmeasurements called echoes. Multi-echo measurement can provide information\nabout object contours and semi-transparent surfaces which can be used to better\nidentify and locate objects. LiDAR can also measure surface reflectance\n(intensity of laser pulse return), as well as ambient light of the scene\n(sunlight reflected by objects). These signals are already available in\ncommercial LiDAR devices but have not been used in most LiDAR-based detection\nmodels. We present a 3D object detection model which leverages the full\nspectrum of measurement signals provided by LiDAR. First, we propose a\nmulti-signal fusion (MSF) module to combine (1) the reflectance and ambient\nfeatures extracted with a 2D CNN, and (2) point cloud features extracted using\na 3D graph neural network (GNN). Second, we propose a multi-echo aggregation\n(MEA) module to combine the information encoded in different set of echo\npoints. Compared with traditional single echo point cloud methods, our proposed\nMulti-Signal LiDAR Detector (MSLiD) extracts richer context information from a\nwider range of sensing measurements and achieves more accurate 3D object\ndetection. Experiments show that by incorporating the multi-modality of LiDAR,\nour method outperforms the state-of-the-art by up to 9.1%.",
          "link": "http://arxiv.org/abs/2107.11470",
          "publishedOn": "2021-07-27T02:03:32.901Z",
          "wordCount": 673,
          "title": "Multi-Echo LiDAR for 3D Object Detection. (arXiv:2107.11470v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiabo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>",
          "description": "Video activity localisation has recently attained increasing attention due to\nits practical values in automatically localising the most salient visual\nsegments corresponding to their language descriptions (sentences) from\nuntrimmed and unstructured videos. For supervised model training, a temporal\nannotation of both the start and end time index of each video segment for a\nsentence (a video moment) must be given. This is not only very expensive but\nalso sensitive to ambiguity and subjective annotation bias, a much harder task\nthan image labelling. In this work, we develop a more accurate\nweakly-supervised solution by introducing Cross-Sentence Relations Mining (CRM)\nin video moment proposal generation and matching when only a paragraph\ndescription of activities without per-sentence temporal annotation is\navailable. Specifically, we explore two cross-sentence relational constraints:\n(1) Temporal ordering and (2) semantic consistency among sentences in a\nparagraph description of video activities. Existing weakly-supervised\ntechniques only consider within-sentence video segment correlations in training\nwithout considering cross-sentence paragraph context. This can mislead due to\nambiguous expressions of individual sentences with visually indiscriminate\nvideo moment proposals in isolation. Experiments on two publicly available\nactivity localisation datasets show the advantages of our approach over the\nstate-of-the-art weakly supervised methods, especially so when the video\nactivity descriptions become more complex.",
          "link": "http://arxiv.org/abs/2107.11443",
          "publishedOn": "2021-07-27T02:03:32.893Z",
          "wordCount": 650,
          "title": "Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation. (arXiv:2107.11443v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>",
          "description": "Non-Euclidean geometry with constant negative curvature, i.e., hyperbolic\nspace, has attracted sustained attention in the community of machine learning.\nHyperbolic space, owing to its ability to embed hierarchical structures\ncontinuously with low distortion, has been applied for learning data with\ntree-like structures. Hyperbolic Neural Networks (HNNs) that operate directly\nin hyperbolic space have also been proposed recently to further exploit the\npotential of hyperbolic representations. While HNNs have achieved better\nperformance than Euclidean neural networks (ENNs) on datasets with implicit\nhierarchical structure, they still perform poorly on standard classification\nbenchmarks such as CIFAR and ImageNet. The traditional wisdom is that it is\ncritical for the data to respect the hyperbolic geometry when applying HNNs. In\nthis paper, we first conduct an empirical study showing that the inferior\nperformance of HNNs on standard recognition datasets can be attributed to the\nnotorious vanishing gradient problem. We further discovered that this problem\nstems from the hybrid architecture of HNNs. Our analysis leads to a simple yet\neffective solution called Feature Clipping, which regularizes the hyperbolic\nembedding whenever its norm exceeding a given threshold. Our thorough\nexperiments show that the proposed method can successfully avoid the vanishing\ngradient problem when training HNNs with backpropagation. The improved HNNs are\nable to achieve comparable performance with ENNs on standard image recognition\ndatasets including MNIST, CIFAR10, CIFAR100 and ImageNet, while demonstrating\nmore adversarial robustness and stronger out-of-distribution detection\ncapability.",
          "link": "http://arxiv.org/abs/2107.11472",
          "publishedOn": "2021-07-27T02:03:32.886Z",
          "wordCount": 670,
          "title": "Free Hyperbolic Neural Networks with Limited Radii. (arXiv:2107.11472v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalande_A/0/1/0/all/0/1\">Alain Lalande</a>",
          "description": "Deep learning methods are the de-facto solutions to a multitude of medical\nimage analysis tasks. Cardiac MRI segmentation is one such application which,\nlike many others, requires a large number of annotated data so a trained\nnetwork can generalize well. Unfortunately, the process of having a large\nnumber of manually curated images by medical experts is both slow and utterly\nexpensive. In this paper, we set out to explore whether expert knowledge is a\nstrict requirement for the creation of annotated datasets that machine learning\ncan successfully train on. To do so, we gauged the performance of three\nsegmentation models, namely U-Net, Attention U-Net, and ENet, trained with\ndifferent loss functions on expert and non-expert groundtruth for cardiac\ncine-MRI segmentation. Evaluation was done with classic segmentation metrics\n(Dice index and Hausdorff distance) as well as clinical measurements, such as\nthe ventricular ejection fractions and the myocardial mass. Results reveal that\ngeneralization performances of a segmentation neural network trained on\nnon-expert groundtruth data is, to all practical purposes, as good as on expert\ngroundtruth data, in particular when the non-expert gets a decent level of\ntraining, highlighting an opportunity for the efficient and cheap creation of\nannotations for cardiac datasets.",
          "link": "http://arxiv.org/abs/2107.11447",
          "publishedOn": "2021-07-27T02:03:32.855Z",
          "wordCount": 648,
          "title": "Deep Learning Based Cardiac MRI Segmentation: Do We Need Experts?. (arXiv:2107.11447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wuzhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>",
          "description": "Accurate image segmentation plays a crucial role in medical image analysis,\nyet it faces great challenges of various shapes, diverse sizes, and blurry\nboundaries. To address these difficulties, square kernel-based encoder-decoder\narchitecture has been proposed and widely used, but its performance remains\nstill unsatisfactory. To further cope with these challenges, we present a novel\ndouble-branch encoder architecture. Our architecture is inspired by two\nobservations: 1) Since the discrimination of features learned via square\nconvolutional kernels needs to be further improved, we propose to utilize\nnon-square vertical and horizontal convolutional kernels in the double-branch\nencoder, so features learned by the two branches can be expected to complement\neach other. 2) Considering that spatial attention can help models to better\nfocus on the target region in a large-sized image, we develop an attention loss\nto further emphasize the segmentation on small-sized targets. Together, the\nabove two schemes give rise to a novel double-branch encoder segmentation\nframework for medical image segmentation, namely Crosslink-Net. The experiments\nvalidate the effectiveness of our model on four datasets. The code is released\nat https://github.com/Qianyu1226/Crosslink-Net.",
          "link": "http://arxiv.org/abs/2107.11517",
          "publishedOn": "2021-07-27T02:03:32.846Z",
          "wordCount": 644,
          "title": "Crosslink-Net: Double-branch Encoder Segmentation Network via Fusing Vertical and Horizontal Convolutions. (arXiv:2107.11517v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dao_S/0/1/0/all/0/1\">Son D.Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_E/0/1/0/all/0/1\">Ethan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>",
          "description": "Recently, as an effective way of learning latent representations, contrastive\nlearning has been increasingly popular and successful in various domains. The\nsuccess of constrastive learning in single-label classifications motivates us\nto leverage this learning framework to enhance distinctiveness for better\nperformance in multi-label image classification. In this paper, we show that a\ndirect application of contrastive learning can hardly improve in multi-label\ncases. Accordingly, we propose a novel framework for multi-label classification\nwith contrastive learning in a fully supervised setting, which learns multiple\nrepresentations of an image under the context of different labels. This\nfacilities a simple yet intuitive adaption of contrastive learning into our\nmodel to boost its performance in multi-label image classification. Extensive\nexperiments on two benchmark datasets show that the proposed framework achieves\nstate-of-the-art performance in the comparison with the advanced methods in\nmulti-label classification.",
          "link": "http://arxiv.org/abs/2107.11626",
          "publishedOn": "2021-07-27T02:03:32.823Z",
          "wordCount": 570,
          "title": "Multi-Label Image Classification with Contrastive Learning. (arXiv:2107.11626v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jongseok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seunghwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+GunheeKim/0/1/0/all/0/1\">GunheeKim</a>",
          "description": "We present an approach named the Cycled Composition Network that can measure\nthe semantic distance of the composition of image-text embedding. First, the\nComposition Network transit a reference image to target image in an embedding\nspace using relative caption. Second, the Correction Network calculates a\ndifference between reference and retrieved target images in the embedding space\nand match it with a relative caption. Our goal is to learn a Composition\nmapping with the Composition Network. Since this one-way mapping is highly\nunder-constrained, we couple it with an inverse relation learning with the\nCorrection Network and introduce a cycled relation for given Image We\nparticipate in Fashion IQ 2020 challenge and have won the first place with the\nensemble of our model.",
          "link": "http://arxiv.org/abs/2107.11509",
          "publishedOn": "2021-07-27T02:03:32.816Z",
          "wordCount": 566,
          "title": "Cycled Compositional Learning between Images and Text. (arXiv:2107.11509v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_I/0/1/0/all/0/1\">Ian E. Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dera_D/0/1/0/all/0/1\">Dimah Dera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal Bouaynaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_R/0/1/0/all/0/1\">Ravi P. Ramachandran</a>",
          "description": "With the rise of deep neural networks, the challenge of explaining the\npredictions of these networks has become increasingly recognized. While many\nmethods for explaining the decisions of deep neural networks exist, there is\ncurrently no consensus on how to evaluate them. On the other hand, robustness\nis a popular topic for deep learning research; however, it is hardly talked\nabout in explainability until very recently. In this tutorial paper, we start\nby presenting gradient-based interpretability methods. These techniques use\ngradient signals to assign the burden of the decision on the input features.\nLater, we discuss how gradient-based methods can be evaluated for their\nrobustness and the role that adversarial robustness plays in having meaningful\nexplanations. We also discuss the limitations of gradient-based methods.\nFinally, we present the best practices and attributes that should be examined\nbefore choosing an explainability method. We conclude with the future\ndirections for research in the area at the convergence of robustness and\nexplainability.",
          "link": "http://arxiv.org/abs/2107.11400",
          "publishedOn": "2021-07-27T02:03:32.809Z",
          "wordCount": 617,
          "title": "Robust Explainability: A Tutorial on Gradient-Based Attribution Methods for Deep Neural Networks. (arXiv:2107.11400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11627",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiyuan Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chimitt_N/0/1/0/all/0/1\">Nicholas Chimitt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1\">Stanley H. Chan</a>",
          "description": "Fast and accurate simulation of imaging through atmospheric turbulence is\nessential for developing turbulence mitigation algorithms. Recognizing the\nlimitations of previous approaches, we introduce a new concept known as the\nphase-to-space (P2S) transform to significantly speed up the simulation. P2S is\nbuild upon three ideas: (1) reformulating the spatially varying convolution as\na set of invariant convolutions with basis functions, (2) learning the basis\nfunction via the known turbulence statistics models, (3) implementing the P2S\ntransform via a light-weight network that directly convert the phase\nrepresentation to spatial representation. The new simulator offers 300x --\n1000x speed up compared to the mainstream split-step simulators while\npreserving the essential turbulence statistics.",
          "link": "http://arxiv.org/abs/2107.11627",
          "publishedOn": "2021-07-27T02:03:32.798Z",
          "wordCount": 569,
          "title": "Accelerating Atmospheric Turbulence Simulation via Learned Phase-to-Space Transform. (arXiv:2107.11627v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11574",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lai_X/0/1/0/all/0/1\">Xuetian Lai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiongyao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_X/0/1/0/all/0/1\">Xiaopeng Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pu_J/0/1/0/all/0/1\">Jixiong Pu</a>",
          "description": "Reconstruction of image by using convolutional neural networks (CNNs) has\nbeen vigorously studied in the last decade. Until now, there have being\ndeveloped several techniques for imaging of a single object through scattering\nmedium by using neural networks, however how to reconstruct images of more than\none object simultaneously seems hard to realize. In this paper, we demonstrate\nan approach by using generative adversarial network (GAN) to reconstruct images\nof two adjacent objects through scattering media. We construct an imaging\nsystem for imaging of two adjacent objects behind the scattering media. In\ngeneral, as the light field of two adjacent object images pass through the\nscattering slab, a speckle pattern is obtained. The designed adversarial\nnetwork, which is called as YGAN, is employed to reconstruct the images\nsimultaneously. It is shown that based on the trained YGAN, we can reconstruct\nimages of two adjacent objects from one speckle pattern with high fidelity. In\naddition, we study the influence of the object image types, and the distance\nbetween the two adjacent objects on the fidelity of the reconstructed images.\nMoreover even if another scattering medium is inserted between the two objects,\nwe can also reconstruct the images of two objects from a speckle with high\nquality. The technique presented in this work can be used for applications in\nareas of medical image analysis, such as medical image classification,\nsegmentation, and studies of multi-object scattering imaging etc.",
          "link": "http://arxiv.org/abs/2107.11574",
          "publishedOn": "2021-07-27T02:03:32.791Z",
          "wordCount": 694,
          "title": "Reconstructing Images of Two Adjacent Objects through Scattering Medium Using Generative Adversarial Network. (arXiv:2107.11574v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiujun Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1\">Weijian Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Cloth-changing person re-identification (re-ID) is a new rising research\ntopic that aims at retrieving pedestrians whose clothes are changed. This task\nis quite challenging and has not been fully studied to date. Current works\nmainly focus on body shape or contour sketch, but they are not robust enough\ndue to view and posture variations. The key to this task is to exploit\ncloth-irrelevant cues. This paper proposes a semantic-guided pixel sampling\napproach for the cloth-changing person re-ID task. We do not explicitly define\nwhich feature to extract but force the model to automatically learn\ncloth-irrelevant cues. Specifically, we first recognize the pedestrian's upper\nclothes and pants, then randomly change them by sampling pixels from other\npedestrians. The changed samples retain the identity labels but exchange the\npixels of clothes or pants among different pedestrians. Besides, we adopt a\nloss function to constrain the learned features to keep consistent before and\nafter changes. In this way, the model is forced to learn cues that are\nirrelevant to upper clothes and pants. We conduct extensive experiments on the\nlatest released PRCC dataset. Our method achieved 65.8% on Rank1 accuracy,\nwhich outperforms previous methods with a large margin. The code is available\nat https://github.com/shuxjweb/pixel_sampling.git.",
          "link": "http://arxiv.org/abs/2107.11522",
          "publishedOn": "2021-07-27T02:03:32.773Z",
          "wordCount": 656,
          "title": "Semantic-guided Pixel Sampling for Cloth-Changing Person Re-identification. (arXiv:2107.11522v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moskvyak_O/0/1/0/all/0/1\">Olga Moskvyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maire_F/0/1/0/all/0/1\">Frederic Maire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1\">Feras Dayoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1\">Mahsa Baktashmotlagh</a>",
          "description": "Person re-identification is the challenging task of identifying a person\nacross different camera views. Training a convolutional neural network (CNN)\nfor this task requires annotating a large dataset, and hence, it involves the\ntime-consuming manual matching of people across cameras. To reduce the need for\nlabeled data, we focus on a semi-supervised approach that requires only a\nsubset of the training data to be labeled. We conduct a comprehensive survey in\nthe area of person re-identification with limited labels. Existing works in\nthis realm are limited in the sense that they utilize features from multiple\nCNNs and require the number of identities in the unlabeled data to be known. To\novercome these limitations, we propose to employ part-based features from a\nsingle CNN without requiring the knowledge of the label space (i.e., the number\nof identities). This makes our approach more suitable for practical scenarios,\nand it significantly reduces the need for computational resources. We also\npropose a PartMixUp loss that improves the discriminative ability of learned\npart-based features for pseudo-labeling in semi-supervised settings. Our method\noutperforms the state-of-the-art results on three large-scale person re-id\ndatasets and achieves the same level of performance as fully supervised methods\nwith only one-third of labeled identities.",
          "link": "http://arxiv.org/abs/2107.11566",
          "publishedOn": "2021-07-27T02:03:32.766Z",
          "wordCount": 634,
          "title": "Going Deeper into Semi-supervised Person Re-identification. (arXiv:2107.11566v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11645",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cao_W/0/1/0/all/0/1\">Wenming Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_P/0/1/0/all/0/1\">Philip L.H. Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lui_G/0/1/0/all/0/1\">Gilbert C.S. Lui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_K/0/1/0/all/0/1\">Keith W.H. Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1\">Ho-Ming Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yanwen Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuen_M/0/1/0/all/0/1\">Man-Fung Yuen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seto_W/0/1/0/all/0/1\">Wai-Kay Seto</a>",
          "description": "In this work, we propose a new segmentation network by integrating DenseUNet\nand bidirectional LSTM together with attention mechanism, termed as\nDA-BDense-UNet. DenseUNet allows learning enough diverse features and enhancing\nthe representative power of networks by regulating the information flow.\nBidirectional LSTM is responsible to explore the relationships between the\nencoded features and the up-sampled features in the encoding and decoding\npaths. Meanwhile, we introduce attention gates (AG) into DenseUNet to diminish\nresponses of unrelated background regions and magnify responses of salient\nregions progressively. Besides, the attention in bidirectional LSTM takes into\naccount the contribution differences of the encoded features and the up-sampled\nfeatures in segmentation improvement, which can in turn adjust proper weights\nfor these two kinds of features. We conduct experiments on liver CT image data\nsets collected from multiple hospitals by comparing them with state-of-the-art\nsegmentation models. Experimental results indicate that our proposed method\nDA-BDense-UNet has achieved comparative performance in terms of dice\ncoefficient, which demonstrates its effectiveness.",
          "link": "http://arxiv.org/abs/2107.11645",
          "publishedOn": "2021-07-27T02:03:32.757Z",
          "wordCount": 618,
          "title": "Dual-Attention Enhanced BDense-UNet for Liver Lesion Segmentation. (arXiv:2107.11645v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tirupattur_P/0/1/0/all/0/1\">Praveen Tirupattur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1\">Aayush J Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangam_T/0/1/0/all/0/1\">Tushar Sangam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1\">Shruti Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh S Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "This paper summarizes the TinyAction challenge which was organized in\nActivityNet workshop at CVPR 2021. This challenge focuses on recognizing\nreal-world low-resolution activities present in videos. Action recognition task\nis currently focused around classifying the actions from high-quality videos\nwhere the actors and the action is clearly visible. While various approaches\nhave been shown effective for recognition task in recent works, they often do\nnot deal with videos of lower resolution where the action is happening in a\ntiny region. However, many real world security videos often have the actual\naction captured in a small resolution, making action recognition in a tiny\nregion a challenging task. In this work, we propose a benchmark dataset,\nTinyVIRAT-v2, which is comprised of naturally occuring low-resolution actions.\nThis is an extension of the TinyVIRAT dataset and consists of actions with\nmultiple labels. The videos are extracted from security videos which makes them\nrealistic and more challenging. We use current state-of-the-art action\nrecognition methods on the dataset as a benchmark, and propose the TinyAction\nChallenge.",
          "link": "http://arxiv.org/abs/2107.11494",
          "publishedOn": "2021-07-27T02:03:32.750Z",
          "wordCount": 621,
          "title": "TinyAction Challenge: Recognizing Real-world Low-resolution Activities in Videos. (arXiv:2107.11494v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1\">Kien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "We propose a novel framework for image clustering that incorporates joint\nrepresentation learning and clustering. Our method consists of two heads that\nshare the same backbone network - a \"representation learning\" head and a\n\"clustering\" head. The \"representation learning\" head captures fine-grained\npatterns of objects at the instance level which serve as clues for the\n\"clustering\" head to extract coarse-grain information that separates objects\ninto clusters. The whole model is trained in an end-to-end manner by minimizing\nthe weighted sum of two sample-oriented contrastive losses applied to the\noutputs of the two heads. To ensure that the contrastive loss corresponding to\nthe \"clustering\" head is optimal, we introduce a novel critic function called\n\"log-of-dot-product\". Extensive experimental results demonstrate that our\nmethod significantly outperforms state-of-the-art single-stage clustering\nmethods across a variety of image datasets, improving over the best baseline by\nabout 5-7% in accuracy on CIFAR10/20, STL10, and ImageNet-Dogs. Further, the\n\"two-stage\" variant of our method also achieves better results than baselines\non three challenging ImageNet subsets.",
          "link": "http://arxiv.org/abs/2107.11635",
          "publishedOn": "2021-07-27T02:03:32.743Z",
          "wordCount": 606,
          "title": "Clustering by Maximizing Mutual Information Across Views. (arXiv:2107.11635v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blumer_K/0/1/0/all/0/1\">Katy Blumer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1\">Michael P. Brenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1\">Jon Kleinberg</a>",
          "description": "We analyze a dataset of retinal images using linear probes: linear regression\nmodels trained on some \"target\" task, using embeddings from a deep\nconvolutional (CNN) model trained on some \"source\" task as input. We use this\nmethod across all possible pairings of 93 tasks in the UK Biobank dataset of\nretinal images, leading to ~164k different models. We analyze the performance\nof these linear probes by source and target task and by layer depth. We observe\nthat representations from the middle layers of the network are more\ngeneralizable. We find that some target tasks are easily predicted irrespective\nof the source task, and that some other target tasks are more accurately\npredicted from correlated source tasks than from embeddings trained on the same\ntask.",
          "link": "http://arxiv.org/abs/2107.11468",
          "publishedOn": "2021-07-27T02:03:32.674Z",
          "wordCount": 596,
          "title": "Using a Cross-Task Grid of Linear Probes to Interpret CNN Model Predictions On Retinal Images. (arXiv:2107.11468v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1\">Lucas Liebenwein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maalouf_A/0/1/0/all/0/1\">Alaa Maalouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_O/0/1/0/all/0/1\">Oren Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1\">Dan Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "We present a novel global compression framework for deep neural networks that\nautomatically analyzes each layer to identify the optimal per-layer compression\nratio, while simultaneously achieving the desired overall compression. Our\nalgorithm hinges on the idea of compressing each convolutional (or\nfully-connected) layer by slicing its channels into multiple groups and\ndecomposing each group via low-rank decomposition. At the core of our algorithm\nis the derivation of layer-wise error bounds from the Eckart Young Mirsky\ntheorem. We then leverage these bounds to frame the compression problem as an\noptimization problem where we wish to minimize the maximum compression error\nacross layers and propose an efficient algorithm towards a solution. Our\nexperiments indicate that our method outperforms existing low-rank compression\napproaches across a wide range of networks and data sets. We believe that our\nresults open up new avenues for future research into the global\nperformance-size trade-offs of modern neural networks. Our code is available at\nhttps://github.com/lucaslie/torchprune.",
          "link": "http://arxiv.org/abs/2107.11442",
          "publishedOn": "2021-07-27T02:03:32.639Z",
          "wordCount": 602,
          "title": "Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition. (arXiv:2107.11442v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yeli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1\">Daniel Jun Xian Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Easwaran_A/0/1/0/all/0/1\">Arvind Easwaran</a>",
          "description": "Uncertainties in machine learning are a significant roadblock for its\napplication in safety-critical cyber-physical systems (CPS). One source of\nuncertainty arises from distribution shifts in the input data between training\nand test scenarios. Detecting such distribution shifts in real-time is an\nemerging approach to address the challenge. The high dimensional input space in\nCPS applications involving imaging adds extra difficulty to the task.\nGenerative learning models are widely adopted for the task, namely\nout-of-distribution (OoD) detection. To improve the state-of-the-art, we\nstudied existing proposals from both machine learning and CPS fields. In the\nlatter, safety monitoring in real-time for autonomous driving agents has been a\nfocus. Exploiting the spatiotemporal correlation of motion in videos, we can\nrobustly detect hazardous motion around autonomous driving agents. Inspired by\nthe latest advances in the Variational Autoencoder (VAE) theory and practice,\nwe tapped into the prior knowledge in data to further boost OoD detection's\nrobustness. Comparison studies over nuScenes and Synthia data sets show our\nmethods significantly improve detection capabilities of OoD factors unique to\ndriving scenarios, 42% better than state-of-the-art approaches. Our model also\ngeneralized near-perfectly, 97% better than the state-of-the-art across the\nreal-world and simulation driving data sets experimented. Finally, we\ncustomized one proposed method into a twin-encoder model that can be deployed\nto resource limited embedded devices for real-time OoD detection. Its execution\ntime was reduced over four times in low-precision 8-bit integer inference,\nwhile detection capability is comparable to its corresponding floating-point\nmodel.",
          "link": "http://arxiv.org/abs/2107.11750",
          "publishedOn": "2021-07-27T02:03:32.602Z",
          "wordCount": 686,
          "title": "Improving Variational Autoencoder based Out-of-Distribution Detection for Embedded Real-time Applications. (arXiv:2107.11750v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1\">Nasibullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1\">Partha Pratim Mohanta</a>",
          "description": "Video captioning is one of the challenging problems at the intersection of\nvision and language, having many real-life applications in video retrieval,\nvideo surveillance, assisting visually challenged people, Human-machine\ninterface, and many more. Recent deep learning-based methods have shown\npromising results but are still on the lower side than other vision tasks (such\nas image classification, object detection). A significant drawback with\nexisting video captioning methods is that they are optimized over cross-entropy\nloss function, which is uncorrelated to the de facto evaluation metrics (BLEU,\nMETEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate\nof the true loss function for video captioning. This paper addresses the\ndrawback by introducing a dynamic loss network (DLN), which provides an\nadditional feedback signal that directly reflects the evaluation metrics. Our\nresults on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to\nText (MSRVTT) datasets outperform previous methods.",
          "link": "http://arxiv.org/abs/2107.11707",
          "publishedOn": "2021-07-27T02:03:32.583Z",
          "wordCount": 585,
          "title": "Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuqian Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>",
          "description": "Given a video demonstration, can we imitate the action contained in this\nvideo? In this paper, we introduce a novel task, dubbed mesh-based action\nimitation. The goal of this task is to enable an arbitrary target human mesh to\nperform the same action shown on the video demonstration. To achieve this, a\nnovel Mesh-based Video Action Imitation (M-VAI) method is proposed by us. M-VAI\nfirst learns to reconstruct the meshes from the given source image frames, then\nthe initial recovered mesh sequence is fed into mesh2mesh, a mesh sequence\nsmooth module proposed by us, to improve the temporal consistency. Finally, we\nimitate the actions by transferring the pose from the constructed human body to\nour target identity mesh. High-quality and detailed human body meshes can be\ngenerated by using our M-VAI. Extensive experiments demonstrate the feasibility\nof our task and the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2107.11756",
          "publishedOn": "2021-07-27T02:03:32.576Z",
          "wordCount": 599,
          "title": "Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics from Videos. (arXiv:2107.11756v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Man Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Huanhuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangshe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaoxu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>",
          "description": "How to effectively and efficiently deal with spatio-temporal event streams,\nwhere the events are generally sparse and non-uniform and have the microsecond\ntemporal resolution, is of great value and has various real-life applications.\nSpiking neural network (SNN), as one of the brain-inspired event-triggered\ncomputing models, has the potential to extract effective spatio-temporal\nfeatures from the event streams. However, when aggregating individual events\ninto frames with a new higher temporal resolution, existing SNN models do not\nattach importance to that the serial frames have different signal-to-noise\nratios since event streams are sparse and non-uniform. This situation\ninterferes with the performance of existing SNNs. In this work, we propose a\ntemporal-wise attention SNN (TA-SNN) model to learn frame-based representation\nfor processing event streams. Concretely, we extend the attention concept to\ntemporal-wise input to judge the significance of frames for the final decision\nat the training stage, and discard the irrelevant frames at the inference\nstage. We demonstrate that TA-SNN models improve the accuracy of event streams\nclassification tasks. We also study the impact of multiple-scale temporal\nresolutions for frame-based representation. Our approach is tested on three\ndifferent classification tasks: gesture recognition, image classification, and\nspoken digit recognition. We report the state-of-the-art results on these\ntasks, and get the essential improvement of accuracy (almost 19\\%) for gesture\nrecognition with only 60 ms.",
          "link": "http://arxiv.org/abs/2107.11711",
          "publishedOn": "2021-07-27T02:03:32.569Z",
          "wordCount": 668,
          "title": "Temporal-wise Attention Spiking Neural Networks for Event Streams Classification. (arXiv:2107.11711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>",
          "description": "In this paper, we focus on the challenging multicategory instance\nsegmentation problem in remote sensing images (RSIs), which aims at predicting\nthe categories of all instances and localizing them with pixel-level masks.\nAlthough many landmark frameworks have demonstrated promising performance in\ninstance segmentation, the complexity in the background and scale variability\ninstances still remain challenging for instance segmentation of RSIs. To\naddress the above problems, we propose an end-to-end multi-category instance\nsegmentation model, namely Semantic Attention and Scale Complementary Network,\nwhich mainly consists of a Semantic Attention (SEA) module and a Scale\nComplementary Mask Branch (SCMB). The SEA module contains a simple fully\nconvolutional semantic segmentation branch with extra supervision to strengthen\nthe activation of interest instances on the feature map and reduce the\nbackground noise's interference. To handle the under-segmentation of geospatial\ninstances with large varying scales, we design the SCMB that extends the\noriginal single mask branch to trident mask branches and introduces\ncomplementary mask supervision at different scales to sufficiently leverage the\nmulti-scale information. We conduct comprehensive experiments to evaluate the\neffectiveness of our proposed method on the iSAID dataset and the NWPU Instance\nSegmentation dataset and achieve promising performance.",
          "link": "http://arxiv.org/abs/2107.11758",
          "publishedOn": "2021-07-27T02:03:32.551Z",
          "wordCount": 648,
          "title": "Semantic Attention and Scale Complementary Network for Instance Segmentation in Remote Sensing Images. (arXiv:2107.11758v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_A/0/1/0/all/0/1\">Ali Rahmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1\">Seyed-Mohsen Moosavi-Dezfooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Huaiyu Dai</a>",
          "description": "Adversarial training has been shown as an effective approach to improve the\nrobustness of image classifiers against white-box attacks. However, its\neffectiveness against black-box attacks is more nuanced. In this work, we\ndemonstrate that some geometric consequences of adversarial training on the\ndecision boundary of deep networks give an edge to certain types of black-box\nattacks. In particular, we define a metric called robustness gain to show that\nwhile adversarial training is an effective method to dramatically improve the\nrobustness in white-box scenarios, it may not provide such a good robustness\ngain against the more realistic decision-based black-box attacks. Moreover, we\nshow that even the minimal perturbation white-box attacks can converge faster\nagainst adversarially-trained neural networks compared to the regular ones.",
          "link": "http://arxiv.org/abs/2107.11671",
          "publishedOn": "2021-07-27T02:03:32.544Z",
          "wordCount": 569,
          "title": "Adversarial training may be a double-edged sword. (arXiv:2107.11671v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qiang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yang Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yunxiao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenxu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>",
          "description": "Despite the great success achieved by deep learning methods in face\nrecognition, severe performance drops are observed for large pose variations in\nunconstrained environments (e.g., in cases of surveillance and photo-tagging).\nTo address it, current methods either deploy pose-specific models or frontalize\nfaces by additional modules. Still, they ignore the fact that identity\ninformation should be consistent across poses and are not realizing the data\nimbalance between frontal and profile face images during training. In this\npaper, we propose an efficient PoseFace framework which utilizes the facial\nlandmarks to disentangle the pose-invariant features and exploits a\npose-adaptive loss to handle the imbalance issue adaptively. Extensive\nexperimental results on the benchmarks of Multi-PIE, CFP, CPLFW and IJB have\ndemonstrated the superiority of our method over the state-of-the-arts.",
          "link": "http://arxiv.org/abs/2107.11721",
          "publishedOn": "2021-07-27T02:03:32.537Z",
          "wordCount": 574,
          "title": "PoseFace: Pose-Invariant Features and Pose-Adaptive Loss for Face Recognition. (arXiv:2107.11721v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhang Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongsheng_H/0/1/0/all/0/1\">Huang Hongsheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jianchao_T/0/1/0/all/0/1\">Tan Jianchao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongmin_X/0/1/0/all/0/1\">Xu Hongmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guozhu_P/0/1/0/all/0/1\">Peng Guozhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Liu Ji</a>",
          "description": "Analyzing and understanding hand information from multimedia materials like\nimages or videos is important for many real world applications and remains\nactive in research community. There are various works focusing on recovering\nhand information from single image, however, they usually solve a single task,\nfor example, hand mask segmentation, 2D/3D hand pose estimation, or hand mesh\nreconstruction and perform not well in challenging scenarios. To further\nimprove the performance of these tasks, we propose a novel Hand Image\nUnderstanding (HIU) framework to extract comprehensive information of the hand\nobject from a single RGB image, by jointly considering the relationships\nbetween these tasks. To achieve this goal, a cascaded multi-task learning (MTL)\nbackbone is designed to estimate the 2D heat maps, to learn the segmentation\nmask, and to generate the intermediate 3D information encoding, followed by a\ncoarse-to-fine learning paradigm and a self-supervised learning strategy.\nQualitative experiments demonstrate that our approach is capable of recovering\nreasonable mesh representations even in challenging situations. Quantitatively,\nour method significantly outperforms the state-of-the-art approaches on various\nwidely-used datasets, in terms of diverse evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.11646",
          "publishedOn": "2021-07-27T02:03:32.530Z",
          "wordCount": 626,
          "title": "Hand Image Understanding via Deep Multi-Task Learning. (arXiv:2107.11646v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11696",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Calderon_Ramirez_S/0/1/0/all/0/1\">Saul Calderon-Ramirez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murillo_Hernandez_D/0/1/0/all/0/1\">Diego Murillo-Hernandez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rojas_Salazar_K/0/1/0/all/0/1\">Kevin Rojas-Salazar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elizondo_D/0/1/0/all/0/1\">David Elizondo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shengxiang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Molina_Cabello_M/0/1/0/all/0/1\">Miguel Molina-Cabello</a>",
          "description": "The implementation of deep learning based computer aided diagnosis systems\nfor the classification of mammogram images can help in improving the accuracy,\nreliability, and cost of diagnosing patients. However, training a deep learning\nmodel requires a considerable amount of labeled images, which can be expensive\nto obtain as time and effort from clinical practitioners is required. A number\nof publicly available datasets have been built with data from different\nhospitals and clinics. However, using models trained on these datasets for\nlater work on images sampled from a different hospital or clinic might result\nin lower performance. This is due to the distribution mismatch of the datasets,\nwhich include different patient populations and image acquisition protocols.\nThe scarcity of labeled data can also bring a challenge towards the application\nof transfer learning with models trained using these source datasets. In this\nwork, a real world scenario is evaluated where a novel target dataset sampled\nfrom a private Costa Rican clinic is used, with few labels and heavily\nimbalanced data. The use of two popular and publicly available datasets\n(INbreast and CBIS-DDSM) as source data, to train and test the models on the\nnovel target dataset, is evaluated. The use of the semi-supervised deep\nlearning approach known as MixMatch, to leverage the usage of unlabeled data\nfrom the target dataset, is proposed and evaluated. In the tests, the\nperformance of models is extensively measured, using different metrics to\nassess the performance of a classifier under heavy data imbalance conditions.\nIt is shown that the use of semi-supervised deep learning combined with\nfine-tuning can provide a meaningful advantage when using scarce labeled\nobservations. We make available the novel dataset for the benefit of the\ncommunity.",
          "link": "http://arxiv.org/abs/2107.11696",
          "publishedOn": "2021-07-27T02:03:32.522Z",
          "wordCount": 749,
          "title": "A Real Use Case of Semi-Supervised Learning for Mammogram Classification in a Local Clinic of Costa Rica. (arXiv:2107.11696v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oksuz_K/0/1/0/all/0/1\">Kemal Oksuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cam_B/0/1/0/all/0/1\">Baris Can Cam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1\">Emre Akbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalkan_S/0/1/0/all/0/1\">Sinan Kalkan</a>",
          "description": "We propose Rank & Sort (RS) Loss, as a ranking-based loss function to train\ndeep object detection and instance segmentation methods (i.e. visual\ndetectors). RS Loss supervises the classifier, a sub-network of these methods,\nto rank each positive above all negatives as well as to sort positives among\nthemselves with respect to (wrt.) their continuous localisation qualities (e.g.\nIntersection-over-Union - IoU). To tackle the non-differentiable nature of\nranking and sorting, we reformulate the incorporation of error-driven update\nwith backpropagation as Identity Update, which enables us to model our novel\nsorting error among positives. With RS Loss, we significantly simplify\ntraining: (i) Thanks to our sorting objective, the positives are prioritized by\nthe classifier without an additional auxiliary head (e.g. for centerness, IoU,\nmask-IoU), (ii) due to its ranking-based nature, RS Loss is robust to class\nimbalance, and thus, no sampling heuristic is required, and (iii) we address\nthe multi-task nature of visual detectors using tuning-free task-balancing\ncoefficients. Using RS Loss, we train seven diverse visual detectors only by\ntuning the learning rate, and show that it consistently outperforms baselines:\ne.g. our RS Loss improves (i) Faster R-CNN by ~ 3 box AP and aLRP Loss\n(ranking-based baseline) by ~ 2 box AP on COCO dataset, (ii) Mask R-CNN with\nrepeat factor sampling (RFS) by 3.5 mask AP (~ 7 AP for rare classes) on LVIS\ndataset; and also outperforms all counterparts. Code available at\nhttps://github.com/kemaloksuz/RankSortLoss",
          "link": "http://arxiv.org/abs/2107.11669",
          "publishedOn": "2021-07-27T02:03:32.503Z",
          "wordCount": 682,
          "title": "Rank & Sort Loss for Object Detection and Instance Segmentation. (arXiv:2107.11669v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2012.14415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chris Junchi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Independent component analysis (ICA) has been a popular dimension reduction\ntool in statistical machine learning and signal processing. In this paper, we\npresent a convergence analysis for an online tensorial ICA algorithm, by\nviewing the problem as a nonconvex stochastic approximation problem. For\nestimating one component, we provide a dynamics-based analysis to prove that\nour online tensorial ICA algorithm with a specific choice of stepsize achieves\na sharp finite-sample error bound. In particular, under a mild assumption on\nthe data-generating distribution and a scaling condition such that $d^4/T$ is\nsufficiently small up to a polylogarithmic factor of data dimension $d$ and\nsample size $T$, a sharp finite-sample error bound of $\\tilde{O}(\\sqrt{d/T})$\ncan be obtained.",
          "link": "http://arxiv.org/abs/2012.14415",
          "publishedOn": "2021-07-30T02:13:30.636Z",
          "wordCount": 592,
          "title": "Stochastic Approximation for Online Tensorial Independent Component Analysis. (arXiv:2012.14415v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mingyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanbeig_M/0/1/0/all/0/1\">Mohammadhosein Hasanbeig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shaoping Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1\">Alessandro Abate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_Z/0/1/0/all/0/1\">Zhen Kan</a>",
          "description": "This paper investigates the motion planning of autonomous dynamical systems\nmodeled by Markov decision processes (MDP) with unknown transition\nprobabilities over continuous state and action spaces. Linear temporal logic\n(LTL) is used to specify high-level tasks over infinite horizon, which can be\nconverted into a limit deterministic generalized B\\\"uchi automaton (LDGBA) with\nseveral accepting sets. The novelty is to design an embedded product MDP\n(EP-MDP) between the LDGBA and the MDP by incorporating a synchronous\ntracking-frontier function to record unvisited accepting sets of the automaton,\nand to facilitate the satisfaction of the accepting conditions. The proposed\nLDGBA-based reward shaping and discounting schemes for the model-free\nreinforcement learning (RL) only depend on the EP-MDP states and can overcome\nthe issues of sparse rewards. Rigorous analysis shows that any RL method that\noptimizes the expected discounted return is guaranteed to find an optimal\npolicy whose traces maximize the satisfaction probability. A modular deep\ndeterministic policy gradient (DDPG) is then developed to generate such\npolicies over continuous state and action spaces. The performance of our\nframework is evaluated via an array of OpenAI gym environments.",
          "link": "http://arxiv.org/abs/2102.12855",
          "publishedOn": "2021-07-30T02:13:30.616Z",
          "wordCount": 682,
          "title": "Modular Deep Reinforcement Learning for Continuous Motion Planning with Temporal Logic. (arXiv:2102.12855v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Christopher D. Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">Heejin Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1\">Pratik Chaudhari</a>",
          "description": "We develop a Multi-Agent Reinforcement Learning (MARL) method to learn\nscalable control policies for target tracking. Our method can handle an\narbitrary number of pursuers and targets; we show results for tasks consisting\nup to 1000 pursuers tracking 1000 targets. We use a decentralized,\npartially-observable Markov Decision Process framework to model pursuers as\nagents receiving partial observations (range and bearing) about targets which\nmove using fixed, unknown policies. An attention mechanism is used to\nparameterize the value function of the agents; this mechanism allows us to\nhandle an arbitrary number of targets. Entropy-regularized off-policy RL\nmethods are used to train a stochastic policy, and we discuss how it enables a\nhedging behavior between pursuers that leads to a weak form of cooperation in\nspite of completely decentralized control execution. We further develop a\nmasking heuristic that allows training on smaller problems with few\npursuers-targets and execution on much larger problems. Thorough simulation\nexperiments, ablation studies, and comparisons to state of the art algorithms\nare performed to study the scalability of the approach and robustness of\nperformance to varying numbers of agents and targets.",
          "link": "http://arxiv.org/abs/2011.08055",
          "publishedOn": "2021-07-30T02:13:30.599Z",
          "wordCount": 656,
          "title": "Scalable Reinforcement Learning Policies for Multi-Agent Control. (arXiv:2011.08055v2 [cs.MA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheinert_D/0/1/0/all/0/1\">Dominik Scheinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acker_A/0/1/0/all/0/1\">Alexander Acker</a>",
          "description": "Deployment, operation and maintenance of large IT systems becomes\nincreasingly complex and puts human experts under extreme stress when problems\noccur. Therefore, utilization of machine learning (ML) and artificial\nintelligence (AI) is applied on IT system operation and maintenance -\nsummarized in the term AIOps. One specific direction aims at the recognition of\nre-occurring anomaly types to enable remediation automation. However, due to IT\nsystem specific properties, especially their frequent changes (e.g. software\nupdates, reconfiguration or hardware modernization), recognition of reoccurring\nanomaly types is challenging. Current methods mainly assume a static\ndimensionality of provided data. We propose a method that is invariant to\ndimensionality changes of given data. Resource metric data such as CPU\nutilization, allocated memory and others are modelled as multivariate time\nseries. The extraction of temporal and spatial features together with the\nsubsequent anomaly classification is realized by utilizing TELESTO, our novel\ngraph convolutional neural network (GCNN) architecture. The experimental\nevaluation is conducted in a real-world cloud testbed deployment that is\nhosting two applications. Classification results of injected anomalies on a\ncassandra database node show that TELESTO outperforms the alternative GCNNs and\nachieves an overall classification accuracy of 85.1%. Classification results\nfor the other nodes show accuracy values between 85% and 60%.",
          "link": "http://arxiv.org/abs/2102.12877",
          "publishedOn": "2021-07-30T02:13:30.516Z",
          "wordCount": 684,
          "title": "TELESTO: A Graph Neural Network Model for Anomaly Classification in Cloud Services. (arXiv:2102.12877v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14151",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rao_A/0/1/0/all/0/1\">Aniruddha Rajendra Rao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Reimherr_M/0/1/0/all/0/1\">Matthew Reimherr</a>",
          "description": "We introduce a new class of non-linear function-on-function regression models\nfor functional data using neural networks. We propose a framework using a\nhidden layer consisting of continuous neurons, called a continuous hidden\nlayer, for functional response modeling and give two model fitting strategies,\nFunctional Direct Neural Network (FDNN) and Functional Basis Neural Network\n(FBNN). Both are designed explicitly to exploit the structure inherent in\nfunctional data and capture the complex relations existing between the\nfunctional predictors and the functional response. We fit these models by\nderiving functional gradients and implement regularization techniques for more\nparsimonious results. We demonstrate the power and flexibility of our proposed\nmethod in handling complex functional models through extensive simulation\nstudies as well as real data examples.",
          "link": "http://arxiv.org/abs/2107.14151",
          "publishedOn": "2021-07-30T02:13:30.509Z",
          "wordCount": 566,
          "title": "Modern Non-Linear Function-on-Function Regression. (arXiv:2107.14151v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iranfar_A/0/1/0/all/0/1\">Arman Iranfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arza_A/0/1/0/all/0/1\">Adriana Arza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atienza_D/0/1/0/all/0/1\">David Atienza</a>",
          "description": "Continuous and multimodal stress detection has been performed recently\nthrough wearable devices and machine learning algorithms. However, a well-known\nand important challenge of working on physiological signals recorded by\nconventional monitoring devices is missing data due to sensors insufficient\ncontact and interference by other equipment. This challenge becomes more\nproblematic when the user/patient is mentally or physically active or stressed\nbecause of more frequent conscious or subconscious movements. In this paper, we\npropose ReLearn, a robust machine learning framework for stress detection from\nbiomarkers extracted from multimodal physiological signals. ReLearn effectively\ncopes with missing data and outliers both at training and inference phases.\nReLearn, composed of machine learning models for feature selection, outlier\ndetection, data imputation, and classification, allows us to classify all\nsamples, including those with missing values at inference. In particular,\naccording to our experiments and stress database, while by discarding all\nmissing data, as a simplistic yet common approach, no prediction can be made\nfor 34% of the data at inference, our approach can achieve accurate\npredictions, as high as 78%, for missing samples. Also, our experiments show\nthat the proposed framework obtains a cross-validation accuracy of 86.8% even\nif more than 50% of samples within the features are missing.",
          "link": "http://arxiv.org/abs/2104.14278",
          "publishedOn": "2021-07-30T02:13:30.484Z",
          "wordCount": 680,
          "title": "ReLearn: A Robust Machine Learning Framework in Presence of Missing Data for Multimodal Stress Detection from Physiological Signals. (arXiv:2104.14278v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1801.02982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1\">Zeyuan Allen-Zhu</a>",
          "description": "Stochastic gradient descent (SGD) gives an optimal convergence rate when\nminimizing convex stochastic objectives $f(x)$. However, in terms of making the\ngradients small, the original SGD does not give an optimal rate, even when\n$f(x)$ is convex.\n\nIf $f(x)$ is convex, to find a point with gradient norm $\\varepsilon$, we\ndesign an algorithm SGD3 with a near-optimal rate\n$\\tilde{O}(\\varepsilon^{-2})$, improving the best known rate\n$O(\\varepsilon^{-8/3})$ of [18].\n\nIf $f(x)$ is nonconvex, to find its $\\varepsilon$-approximate local minimum,\nwe design an algorithm SGD5 with rate $\\tilde{O}(\\varepsilon^{-3.5})$, where\npreviously SGD variants only achieve $\\tilde{O}(\\varepsilon^{-4})$ [6, 15, 33].\nThis is no slower than the best known stochastic version of Newton's method in\nall parameter regimes [30].",
          "link": "http://arxiv.org/abs/1801.02982",
          "publishedOn": "2021-07-30T02:13:30.469Z",
          "wordCount": 620,
          "title": "How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD. (arXiv:1801.02982v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhanpeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qirui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "This paper aims to formulate the problem of estimating the optimal baseline\nvalues for the Shapley value in game theory. The Shapley value measures the\nattribution of each input variable of a complex model, which is computed as the\nmarginal benefit from the presence of this variable w.r.t.its absence under\ndifferent contexts. To this end, people usually set the input variable to its\nbaseline value to represent the absence of this variable (i.e.the no-signal\nstate of this variable). Previous studies usually determine the baseline values\nin an empirical manner, which hurts the trustworthiness of the Shapley value.\nIn this paper, we revisit the feature representation of a deep model from the\nperspective of game theory, and define the multi-variate interaction patterns\nof input variables to define the no-signal state of an input variable. Based on\nthe multi-variate interaction, we learn the optimal baseline value of each\ninput variable. Experimental results have demonstrated the effectiveness of our\nmethod.",
          "link": "http://arxiv.org/abs/2105.10719",
          "publishedOn": "2021-07-30T02:13:30.463Z",
          "wordCount": 614,
          "title": "Learning Baseline Values for Shapley Values. (arXiv:2105.10719v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thoma_N/0/1/0/all/0/1\">Nils Thoma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhongjie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1\">Fabrizio Ventola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "Time series forecasting is a relevant task that is performed in several\nreal-world scenarios such as product sales analysis and prediction of energy\ndemand. Given their accuracy performance, currently, Recurrent Neural Networks\n(RNNs) are the models of choice for this task. Despite their success in time\nseries forecasting, less attention has been paid to make the RNNs trustworthy.\nFor example, RNNs can not naturally provide an uncertainty measure to their\npredictions. This could be extremely useful in practice in several cases e.g.\nto detect when a prediction might be completely wrong due to an unusual pattern\nin the time series. Whittle Sum-Product Networks (WSPNs), prominent deep\ntractable probabilistic circuits (PCs) for time series, can assist an RNN with\nproviding meaningful probabilities as uncertainty measure. With this aim, we\npropose RECOWN, a novel architecture that employs RNNs and a discriminant\nvariant of WSPNs called Conditional WSPNs (CWSPNs). We also formulate a\nLog-Likelihood Ratio Score as better estimation of uncertainty that is tailored\nto time series and Whittle likelihoods. In our experiments, we show that\nRECOWNs are accurate and trustworthy time series predictors, able to \"know when\nthey do not know\".",
          "link": "http://arxiv.org/abs/2106.04148",
          "publishedOn": "2021-07-30T02:13:30.458Z",
          "wordCount": 662,
          "title": "RECOWNs: Probabilistic Circuits for Trustworthy Time Series Forecasting. (arXiv:2106.04148v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.03194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenbo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1\">Ness B. Shroff</a>",
          "description": "This paper studies the problem of finding the exact ranking from noisy\ncomparisons. A comparison over a set of $m$ items produces a noisy outcome\nabout the most preferred item, and reveals some information about the ranking.\nBy repeatedly and adaptively choosing items to compare, we want to fully rank\nthe items with a certain confidence, and use as few comparisons as possible.\nDifferent from most previous works, in this paper, we have three main\nnovelties: (i) compared to prior works, our upper bounds (algorithms) and lower\nbounds on the sample complexity (aka number of comparisons) require the minimal\nassumptions on the instances, and are not restricted to specific models; (ii)\nwe give lower bounds and upper bounds on instances with unequal noise levels;\nand (iii) this paper aims at the exact ranking without knowledge on the\ninstances, while most of the previous works either focus on approximate\nrankings or study exact ranking but require prior knowledge. We first derive\nlower bounds for pairwise ranking (i.e., compare two items each time), and then\npropose (nearly) optimal pairwise ranking algorithms. We further make\nextensions to listwise ranking (i.e., comparing multiple items each time).\nNumerical results also show our improvements against the state of the art.",
          "link": "http://arxiv.org/abs/1909.03194",
          "publishedOn": "2021-07-30T02:13:30.452Z",
          "wordCount": 686,
          "title": "On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons. (arXiv:1909.03194v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.00814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peng-Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>",
          "description": "k-nearest neighbor graph is a fundamental data structure in many disciplines\nsuch as information retrieval, data-mining, pattern recognition, and machine\nlearning, etc. In the literature, considerable research has been focusing on\nhow to efficiently build an approximate k-nearest neighbor graph (k-NN graph)\nfor a fixed dataset. Unfortunately, a closely related issue of how to merge two\nexisting k-NN graphs has been overlooked. In this paper, we address the issue\nof k-NN graph merging in two different scenarios. In the first scenario, a\nsymmetric merge algorithm is proposed to combine two approximate k-NN graphs.\nThe algorithm facilitates large-scale processing by the efficient merging of\nk-NN graphs that are produced in parallel. In the second scenario, a joint\nmerge algorithm is proposed to expand an existing k-NN graph with a raw\ndataset. The algorithm enables the incremental construction of a hierarchical\napproximate k-NN graph. Superior performance is attained when leveraging the\nhierarchy for NN search of various data types, dimensionality, and distance\nmeasures.",
          "link": "http://arxiv.org/abs/1908.00814",
          "publishedOn": "2021-07-30T02:13:30.446Z",
          "wordCount": 660,
          "title": "On the Merge of k-NN Graph. (arXiv:1908.00814v6 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsiaousis_M/0/1/0/all/0/1\">Michail Tsiaousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1\">Gertjan Burghouts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillerstrom_F/0/1/0/all/0/1\">Fieke Hillerstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>",
          "description": "The dominant paradigm in spatiotemporal action detection is to classify\nactions using spatiotemporal features learned by 2D or 3D Convolutional\nNetworks. We argue that several actions are characterized by their context,\nsuch as relevant objects and actors present in the video. To this end, we\nintroduce an architecture based on self-attention and Graph Convolutional\nNetworks in order to model contextual cues, such as actor-actor and\nactor-object interactions, to improve human action detection in video. We are\ninterested in achieving this in a weakly-supervised setting, i.e. using as less\nannotations as possible in terms of action bounding boxes. Our model aids\nexplainability by visualizing the learned context as an attention map, even for\nactions and objects unseen during training. We evaluate how well our model\nhighlights the relevant context by introducing a quantitative metric based on\nrecall of objects retrieved by attention maps. Our model relies on a 3D\nconvolutional RGB stream, and does not require expensive optical flow\ncomputation. We evaluate our models on the DALY dataset, which consists of\nhuman-object interaction actions. Experimental results show that our\ncontextualized approach outperforms a baseline action detection approach by\nmore than 2 points in Video-mAP. Code is available at\n\\url{https://github.com/micts/acgcn}",
          "link": "http://arxiv.org/abs/2107.13648",
          "publishedOn": "2021-07-30T02:13:30.432Z",
          "wordCount": 678,
          "title": "Spot What Matters: Learning Context Using Graph Convolutional Networks for Weakly-Supervised Action Detection. (arXiv:2107.13648v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1\">Nergis Tomen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>",
          "description": "Convolutional layers in CNNs implement linear filters which decompose the\ninput into different frequency bands. However, most modern architectures\nneglect standard principles of filter design when optimizing their model\nchoices regarding the size and shape of the convolutional kernel. In this work,\nwe consider the well-known problem of spectral leakage caused by windowing\nartifacts in filtering operations in the context of CNNs. We show that the\nsmall size of CNN kernels make them susceptible to spectral leakage, which may\ninduce performance-degrading artifacts. To address this issue, we propose the\nuse of larger kernel sizes along with the Hamming window function to alleviate\nleakage in CNN architectures. We demonstrate improved classification accuracy\non multiple benchmark datasets including Fashion-MNIST, CIFAR-10, CIFAR-100 and\nImageNet with the simple use of a standard window function in convolutional\nlayers. Finally, we show that CNNs employing the Hamming window display\nincreased robustness against various adversarial attacks.",
          "link": "http://arxiv.org/abs/2101.10143",
          "publishedOn": "2021-07-30T02:13:30.426Z",
          "wordCount": 615,
          "title": "Spectral Leakage and Rethinking the Kernel Size in CNNs. (arXiv:2101.10143v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Ternary Neural Networks (TNNs) have received much attention due to being\npotentially orders of magnitude faster in inference, as well as more power\nefficient, than full-precision counterparts. However, 2 bits are required to\nencode the ternary representation with only 3 quantization levels leveraged. As\na result, conventional TNNs have similar memory consumption and speed compared\nwith the standard 2-bit models, but have worse representational capability.\nMoreover, there is still a significant gap in accuracy between TNNs and\nfull-precision networks, hampering their deployment to real applications. To\ntackle these two challenges, in this work, we first show that, under some mild\nconstraints, computational complexity of the ternary inner product can be\nreduced by a factor of 2. Second, to mitigate the performance gap, we\nelaborately design an implementation-dependent ternary quantization algorithm.\nThe proposed framework is termed Fast and Accurate Ternary Neural Networks\n(FATNN). Experiments on image classification demonstrate that our FATNN\nsurpasses the state-of-the-arts by a significant margin in accuracy. More\nimportantly, speedup evaluation compared with various precisions is analyzed on\nseveral platforms, which serves as a strong benchmark for further research.",
          "link": "http://arxiv.org/abs/2008.05101",
          "publishedOn": "2021-07-30T02:13:30.421Z",
          "wordCount": 669,
          "title": "FATNN: Fast and Accurate Ternary Neural Networks. (arXiv:2008.05101v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acevedo_Viloria_J/0/1/0/all/0/1\">Jaime D. Acevedo-Viloria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roa_L/0/1/0/all/0/1\">Luisa Roa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeshina_S/0/1/0/all/0/1\">Soji Adeshina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olazo_C/0/1/0/all/0/1\">Cesar Charalla Olazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Rey_A/0/1/0/all/0/1\">Andr&#xe9;s Rodr&#xed;guez-Rey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_J/0/1/0/all/0/1\">Jose Alberto Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correa_Bahnsen_A/0/1/0/all/0/1\">Alejandro Correa-Bahnsen</a>",
          "description": "Large digital platforms create environments where different types of user\ninteractions are captured, these relationships offer a novel source of\ninformation for fraud detection problems. In this paper we propose a framework\nof relational graph convolutional networks methods for fraudulent behaviour\nprevention in the financial services of a Super-App. To this end, we apply the\nframework on different heterogeneous graphs of users, devices, and credit\ncards; and finally use an interpretability algorithm for graph neural networks\nto determine the most important relations to the classification task of the\nusers. Our results show that there is an added value when considering models\nthat take advantage of the alternative data of the Super-App and the\ninteractions found in their high connectivity, further proofing how they can\nleverage that into better decisions and fraud detection strategies.",
          "link": "http://arxiv.org/abs/2107.13673",
          "publishedOn": "2021-07-30T02:13:30.415Z",
          "wordCount": 597,
          "title": "Relational Graph Neural Networks for Fraud Detection in a Super-Appe nvironment. (arXiv:2107.13673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1\">Kushankur Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellinger_C/0/1/0/all/0/1\">Colin Bellinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corizzo_R/0/1/0/all/0/1\">Roberto Corizzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krawczyk_B/0/1/0/all/0/1\">Bartosz Krawczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Japkowicz_N/0/1/0/all/0/1\">Nathalie Japkowicz</a>",
          "description": "Structural concept complexity, class overlap, and data scarcity are some of\nthe most important factors influencing the performance of classifiers under\nclass imbalance conditions. When these effects were uncovered in the early\n2000s, understandably, the classifiers on which they were demonstrated belonged\nto the classical rather than Deep Learning categories of approaches. As Deep\nLearning is gaining ground over classical machine learning and is beginning to\nbe used in critical applied settings, it is important to assess systematically\nhow well they respond to the kind of challenges their classical counterparts\nhave struggled with in the past two decades. The purpose of this paper is to\nstudy the behavior of deep learning systems in settings that have previously\nbeen deemed challenging to classical machine learning systems to find out\nwhether the depth of the systems is an asset in such settings. The results in\nboth artificial and real-world image datasets (MNIST Fashion, CIFAR-10) show\nthat these settings remain mostly challenging for Deep Learning systems and\nthat deeper architectures seem to help with structural concept complexity but\nnot with overlap challenges in simple artificial domains. Data scarcity is not\novercome by deeper layers, either. In the real-world image domains, where\noverfitting is a greater concern than in the artificial domains, the advantage\nof deeper architectures is less obvious: while it is observed in certain cases,\nit is quickly cancelled as models get deeper and perform worse than their\nshallower counterparts.",
          "link": "http://arxiv.org/abs/2107.14194",
          "publishedOn": "2021-07-30T02:13:30.410Z",
          "wordCount": 680,
          "title": "On the combined effect of class imbalance and concept complexity in deep learning. (arXiv:2107.14194v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14094",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Krishnakumari_P/0/1/0/all/0/1\">Panchamy Krishnakumari</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cats_O/0/1/0/all/0/1\">Oded Cats</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lint_H/0/1/0/all/0/1\">Hans van Lint</a>",
          "description": "In an effort to improve user satisfaction and transit image, transit service\nproviders worldwide offer delay compensations. Smart card data enables the\nestimation of passenger delays throughout the network and aid in monitoring\nservice performance. Notwithstanding, in order to prioritize measures for\nimproving service reliability and hence reducing passenger delays, it is\nparamount to identify the system components - stations and track segments -\nwhere most passenger delay occurs. To this end, we propose a novel method for\nestimating network passenger delay from individual trajectories. We decompose\nthe delay along a passenger trajectory into its corresponding track segment\ndelay, initial waiting time and transfer delay. We distinguish between two\ndifferent types of passenger delay in relation to the public transit network:\naverage passenger delay and total passenger delay. We employ temporal\nclustering on these two quantities to reveal daily and seasonal regularity in\ndelay patterns of the transit network. The estimation and clustering methods\nare demonstrated on one year of data from Washington metro network. The data\nconsists of schedule information and smart card data which includes\npassenger-train assignment of the metro network for the months of August 2017\nto August 2018. Our findings show that the average passenger delay is\nrelatively stable throughout the day. The temporal clustering reveals\npronounced and recurrent and thus predictable daily and weekly patterns with\ndistinct characteristics for certain months.",
          "link": "http://arxiv.org/abs/2107.14094",
          "publishedOn": "2021-07-30T02:13:30.393Z",
          "wordCount": 674,
          "title": "Day-to-day and seasonal regularity of network passenger delay for metro networks. (arXiv:2107.14094v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strouse_D/0/1/0/all/0/1\">DJ Strouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumli_K/0/1/0/all/0/1\">Kate Baumli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warde_Farley_D/0/1/0/all/0/1\">David Warde-Farley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mnih_V/0/1/0/all/0/1\">Vlad Mnih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_S/0/1/0/all/0/1\">Steven Hansen</a>",
          "description": "Unsupervised skill learning objectives (Gregor et al., 2016, Eysenbach et\nal., 2018) allow agents to learn rich repertoires of behavior in the absence of\nextrinsic rewards. They work by simultaneously training a policy to produce\ndistinguishable latent-conditioned trajectories, and a discriminator to\nevaluate distinguishability by trying to infer latents from trajectories. The\nhope is for the agent to explore and master the environment by encouraging each\nskill (latent) to reliably reach different states. However, an inherent\nexploration problem lingers: when a novel state is actually encountered, the\ndiscriminator will necessarily not have seen enough training data to produce\naccurate and confident skill classifications, leading to low intrinsic reward\nfor the agent and effective penalization of the sort of exploration needed to\nactually maximize the objective. To combat this inherent pessimism towards\nexploration, we derive an information gain auxiliary objective that involves\ntraining an ensemble of discriminators and rewarding the policy for their\ndisagreement. Our objective directly estimates the epistemic uncertainty that\ncomes from the discriminator not having seen enough training examples, thus\nproviding an intrinsic reward more tailored to the true objective compared to\npseudocount-based methods (Burda et al., 2019). We call this exploration bonus\ndiscriminator disagreement intrinsic reward, or DISDAIN. We demonstrate\nempirically that DISDAIN improves skill learning both in a tabular grid world\n(Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we\nencourage researchers to treat pessimism with DISDAIN.",
          "link": "http://arxiv.org/abs/2107.14226",
          "publishedOn": "2021-07-30T02:13:30.387Z",
          "wordCount": 684,
          "title": "Learning more skills through optimistic exploration. (arXiv:2107.14226v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guoliang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Ting Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jin Song Dong</a>",
          "description": "Although deep learning has demonstrated astonishing performance in many\napplications, there are still concerns about its dependability. One desirable\nproperty of deep learning applications with societal impact is fairness (i.e.,\nnon-discrimination). Unfortunately, discrimination might be intrinsically\nembedded into the models due to the discrimination in the training data. As a\ncountermeasure, fairness testing systemically identifies discriminatory\nsamples, which can be used to retrain the model and improve the model's\nfairness. Existing fairness testing approaches however have two major\nlimitations. Firstly, they only work well on traditional machine learning\nmodels and have poor performance (e.g., effectiveness and efficiency) on deep\nlearning models. Secondly, they only work on simple structured (e.g., tabular)\ndata and are not applicable for domains such as text. In this work, we bridge\nthe gap by proposing a scalable and effective approach for systematically\nsearching for discriminatory samples while extending existing fairness testing\napproaches to address a more challenging domain, i.e., text classification.\nCompared with state-of-the-art methods, our approach only employs lightweight\nprocedures like gradient computation and clustering, which is significantly\nmore scalable and effective. Experimental results show that on average, our\napproach explores the search space much more effectively (9.62 and 2.38 times\nmore than the state-of-the-art methods respectively on tabular and text\ndatasets) and generates much more discriminatory samples (24.95 and 2.68 times)\nwithin a same reasonable time. Moreover, the retrained models reduce\ndiscrimination by 57.2% and 60.2% respectively on average.",
          "link": "http://arxiv.org/abs/2107.08176",
          "publishedOn": "2021-07-30T02:13:30.380Z",
          "wordCount": 702,
          "title": "Automatic Fairness Testing of Neural Classifiers through Adversarial Sampling. (arXiv:2107.08176v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1\">Raman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravorty_S/0/1/0/all/0/1\">Suman Chakravorty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mohamed Naveed Gul Mohamed</a>",
          "description": "We consider the problem of Reinforcement Learning for nonlinear stochastic\ndynamical systems. We show that in the RL setting, there is an inherent ``Curse\nof Variance\" in addition to Bellman's infamous ``Curse of Dimensionality\", in\nparticular, we show that the variance in the solution grows\nfactorial-exponentially in the order of the approximation. A fundamental\nconsequence is that this precludes the search for anything other than ``local\"\nfeedback solutions in RL, in order to control the explosive variance growth,\nand thus, ensure accuracy. We further show that the deterministic optimal\ncontrol has a perturbation structure, in that the higher order terms do not\naffect the calculation of lower order terms, which can be utilized in RL to get\naccurate local solutions.",
          "link": "http://arxiv.org/abs/2011.10829",
          "publishedOn": "2021-07-30T02:13:30.374Z",
          "wordCount": 593,
          "title": "On the Convergence of Reinforcement Learning in Nonlinear Continuous State Space Problems. (arXiv:2011.10829v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14135",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">YunPeng Li</a>",
          "description": "Convolutive blind source separation (BSS) is intended to recover the unknown\ncomponents from their convolutive mixtures. Contrary to the contrast functions\nused in instantaneous cases, the spatial-temporal prewhitening stage and the\npara-unitary filters constraint are difficult to implement in a convolutive\ncontext. In this paper, we propose several modifications of FastICA to\nalleviate these difficulties. Our method performs the simple prewhitening step\non convolutive mixtures prior to the separation and optimizes the contrast\nfunction under the diagonalization constraint implemented by single value\ndecomposition (SVD). Numerical simulations are implemented to verify the\nperformance of the proposed method.",
          "link": "http://arxiv.org/abs/2107.14135",
          "publishedOn": "2021-07-30T02:13:30.369Z",
          "wordCount": 527,
          "title": "Modifications of FastICA in Convolutive Blind Source Separation. (arXiv:2107.14135v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11815",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liang_T/0/1/0/all/0/1\">Tengyuan Liang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Recht_B/0/1/0/all/0/1\">Benjamin Recht</a>",
          "description": "This paper provides elementary analyses of the regret and generalization of\nminimum-norm interpolating classifiers (MNIC). The MNIC is the function of\nsmallest Reproducing Kernel Hilbert Space norm that perfectly interpolates a\nlabel pattern on a finite data set. We derive a mistake bound for MNIC and a\nregularized variant that holds for all data sets. This bound follows from\nelementary properties of matrix inverses. Under the assumption that the data is\nindependently and identically distributed, the mistake bound implies that MNIC\ngeneralizes at a rate proportional to the norm of the interpolating solution\nand inversely proportional to the number of data points. This rate matches\nsimilar rates derived for margin classifiers and perceptrons. We derive several\nplausible generative models where the norm of the interpolating classifier is\nbounded or grows at a rate sublinear in $n$. We also show that as long as the\npopulation class conditional distributions are sufficiently separable in total\nvariation, then MNIC generalizes with a fast rate.",
          "link": "http://arxiv.org/abs/2101.11815",
          "publishedOn": "2021-07-30T02:13:30.350Z",
          "wordCount": 618,
          "title": "Interpolating Classifiers Make Few Mistakes. (arXiv:2101.11815v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.01845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drori_Y/0/1/0/all/0/1\">Yoel Drori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>",
          "description": "We study the iteration complexity of stochastic gradient descent (SGD) for\nminimizing the gradient norm of smooth, possibly nonconvex functions. We\nprovide several results, implying that the $\\mathcal{O}(\\epsilon^{-4})$ upper\nbound of Ghadimi and Lan~\\cite{ghadimi2013stochastic} (for making the average\ngradient norm less than $\\epsilon$) cannot be improved upon, unless a\ncombination of additional assumptions is made. Notably, this holds even if we\nlimit ourselves to convex quadratic functions. We also show that for nonconvex\nfunctions, the feasibility of minimizing gradients with SGD is surprisingly\nsensitive to the choice of optimality criteria.",
          "link": "http://arxiv.org/abs/1910.01845",
          "publishedOn": "2021-07-30T02:13:30.345Z",
          "wordCount": 577,
          "title": "The Complexity of Finding Stationary Points with Stochastic Gradient Descent. (arXiv:1910.01845v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gaurav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chenzhong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1\">Jyotirmoy V. Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogdan_P/0/1/0/all/0/1\">Paul Bogdan</a>",
          "description": "Reinforcement learning (RL) is a technique to learn the control policy for an\nagent that interacts with a stochastic environment. In any given state, the\nagent takes some action, and the environment determines the probability\ndistribution over the next state as well as gives the agent some reward. Most\nRL algorithms typically assume that the environment satisfies Markov\nassumptions (i.e. the probability distribution over the next state depends only\non the current state). In this paper, we propose a model-based RL technique for\na system that has non-Markovian dynamics. Such environments are common in many\nreal-world applications such as in human physiology, biological systems,\nmaterial science, and population dynamics. Model-based RL (MBRL) techniques\ntypically try to simultaneously learn a model of the environment from the data,\nas well as try to identify an optimal policy for the learned model. We propose\na technique where the non-Markovianity of the system is modeled through a\nfractional dynamical system. We show that we can quantify the difference in the\nperformance of an MBRL algorithm that uses bounded horizon model predictive\ncontrol from the optimal policy. Finally, we demonstrate our proposed framework\non a pharmacokinetic model of human blood glucose dynamics and show that our\nfractional models can capture distant correlations on real-world datasets.",
          "link": "http://arxiv.org/abs/2107.13790",
          "publishedOn": "2021-07-30T02:13:30.336Z",
          "wordCount": 642,
          "title": "Non-Markovian Reinforcement Learning using Fractional Dynamics. (arXiv:2107.13790v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>",
          "description": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.",
          "link": "http://arxiv.org/abs/2107.08661",
          "publishedOn": "2021-07-30T02:13:30.331Z",
          "wordCount": 631,
          "title": "Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14203",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1\">Lingjiao Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1\">Tracy Cai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>",
          "description": "Machine learning (ML) prediction APIs are increasingly widely used. An ML API\ncan change over time due to model updates or retraining. This presents a key\nchallenge in the usage of the API because it is often not clear to the user if\nand how the ML model has changed. Model shifts can affect downstream\napplication performance and also create oversight issues (e.g. if consistency\nis desired). In this paper, we initiate a systematic investigation of ML API\nshifts. We first quantify the performance shifts from 2020 to 2021 of popular\nML APIs from Google, Microsoft, Amazon, and others on a variety of datasets. We\nidentified significant model shifts in 12 out of 36 cases we investigated.\nInterestingly, we found several datasets where the API's predictions became\nsignificantly worse over time. This motivated us to formulate the API shift\nassessment problem at a more fine-grained level as estimating how the API\nmodel's confusion matrix changes over time when the data distribution is\nconstant. Monitoring confusion matrix shifts using standard random sampling can\nrequire a large number of samples, which is expensive as each API call costs a\nfee. We propose a principled adaptive sampling algorithm, MASA, to efficiently\nestimate confusion matrix shifts. MASA can accurately estimate the confusion\nmatrix shifts in commercial ML APIs using up to 90% fewer samples compared to\nrandom sampling. This work establishes ML API shifts as an important problem to\nstudy and provides a cost-effective approach to monitor such shifts.",
          "link": "http://arxiv.org/abs/2107.14203",
          "publishedOn": "2021-07-30T02:13:30.326Z",
          "wordCount": 697,
          "title": "Did the Model Change? Efficiently Assessing Machine Learning API Shifts. (arXiv:2107.14203v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rex Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramli_A/0/1/0/all/0/1\">Albara Ah Ramli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanle Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_E/0/1/0/all/0/1\">Esha Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henricson_E/0/1/0/all/0/1\">Erik Henricson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>",
          "description": "With the rapid development of the internet of things (IoT) and artificial\nintelligence (AI) technologies, human activity recognition (HAR) has been\napplied in a variety of domains such as security and surveillance, human-robot\ninteraction, and entertainment. Even though a number of surveys and review\npapers have been published, there is a lack of HAR overview papers focusing on\nhealthcare applications that use wearable sensors. Therefore, we fill in the\ngap by presenting this overview paper. In particular, we present our projects\nto illustrate the system design of HAR applications for healthcare. Our\nprojects include early mobility identification of human activities for\nintensive care unit (ICU) patients and gait analysis of Duchenne muscular\ndystrophy (DMD) patients. We cover essential components of designing HAR\nsystems including sensor factors (e.g., type, number, and placement location),\nAI model selection (e.g., classical machine learning models versus deep\nlearning models), and feature engineering. In addition, we highlight the\nchallenges of such healthcare-oriented HAR systems and propose several research\nopportunities for both the medical and the computer science community.",
          "link": "http://arxiv.org/abs/2103.15990",
          "publishedOn": "2021-07-30T02:13:30.313Z",
          "wordCount": 671,
          "title": "An Overview of Human Activity Recognition Using Wearable Sensors: Healthcare and Artificial Intelligence. (arXiv:2103.15990v4 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1\">Kin Wai Cheuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li Su</a>",
          "description": "Most of the current supervised automatic music transcription (AMT) models\nlack the ability to generalize. This means that they have trouble transcribing\nreal-world music recordings from diverse musical genres that are not presented\nin the labelled training data. In this paper, we propose a semi-supervised\nframework, ReconVAT, which solves this issue by leveraging the huge amount of\navailable unlabelled music recordings. The proposed ReconVAT uses\nreconstruction loss and virtual adversarial training. When combined with\nexisting U-net models for AMT, ReconVAT achieves competitive results on common\nbenchmark datasets such as MAPS and MusicNet. For example, in the few-shot\nsetting for the string part version of MusicNet, ReconVAT achieves F1-scores of\n61.0% and 41.6% for the note-wise and note-with-offset-wise metrics\nrespectively, which translates into an improvement of 22.2% and 62.5% compared\nto the supervised baseline model. Our proposed framework also demonstrates the\npotential of continual learning on new data, which could be useful in\nreal-world applications whereby new data is constantly available.",
          "link": "http://arxiv.org/abs/2107.04954",
          "publishedOn": "2021-07-30T02:13:30.307Z",
          "wordCount": 632,
          "title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan C. P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeanneret_G/0/1/0/all/0/1\">Guillaume Jeanneret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueda_L/0/1/0/all/0/1\">Laura Rueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbel&#xe1;ez</a>",
          "description": "Deep learning models are prone to being fooled by imperceptible perturbations\nknown as adversarial attacks. In this work, we study how equipping models with\nTest-time Transformation Ensembling (TTE) can work as a reliable defense\nagainst such attacks. While transforming the input data, both at train and test\ntimes, is known to enhance model performance, its effects on adversarial\nrobustness have not been studied. Here, we present a comprehensive empirical\nstudy of the impact of TTE, in the form of widely-used image transforms, on\nadversarial robustness. We show that TTE consistently improves model robustness\nagainst a variety of powerful attacks without any need for re-training, and\nthat this improvement comes at virtually no trade-off with accuracy on clean\nsamples. Finally, we show that the benefits of TTE transfer even to the\ncertified robustness domain, in which TTE provides sizable and consistent\nimprovements.",
          "link": "http://arxiv.org/abs/2107.14110",
          "publishedOn": "2021-07-30T02:13:30.302Z",
          "wordCount": 588,
          "title": "Enhancing Adversarial Robustness via Test-time Transformation Ensembling. (arXiv:2107.14110v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_R/0/1/0/all/0/1\">Roger Alexander M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laflamme_Janssen_J/0/1/0/all/0/1\">Jonathan Laflamme-Janssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacaro_J/0/1/0/all/0/1\">Jaime Camacaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessega_C/0/1/0/all/0/1\">Carolina Bessega</a>",
          "description": "In this article we address the question whether it is possible to learn the\ndifferential equations describing the physical properties of a dynamical\nsystem, subject to non-conservative forces, from observations of its realspace\ntrajectory(ies) only. We introduce a network that incorporates a difference\napproximation for the second order derivative in terms of residual connections\nbetween convolutional blocks, whose shared weights represent the coefficients\nof a second order ordinary differential equation. We further combine this\nsolver-like architecture with a convolutional network, capable of learning the\nrelation between trajectories of coupled oscillators and therefore allows us to\nmake a stable forecast even if the system is only partially observed. We\noptimize this map together with the solver network, while sharing their\nweights, to form a powerful framework capable of learning the complex physical\nproperties of a dissipative dynamical system.",
          "link": "http://arxiv.org/abs/2010.11270",
          "publishedOn": "2021-07-30T02:13:30.296Z",
          "wordCount": 605,
          "title": "Learning second order coupled differential equations that are subject to non-conservative forces. (arXiv:2010.11270v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerri_P/0/1/0/all/0/1\">Pietro Cerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>",
          "description": "Image-to-image translation (i2i) networks suffer from entanglement effects in\npresence of physics-related phenomena in target domain (such as occlusions,\nfog, etc), thus lowering the translation quality and variability. In this\npaper, we present a comprehensive method for disentangling physics-based traits\nin the translation, guiding the learning process with neural or physical\nmodels. For the latter, we integrate adversarial estimation and genetic\nalgorithms to correctly achieve disentanglement. The results show our approach\ndramatically increase performances in many challenging scenarios for image\ntranslation.",
          "link": "http://arxiv.org/abs/2107.14229",
          "publishedOn": "2021-07-30T02:13:30.290Z",
          "wordCount": 524,
          "title": "Guided Disentanglement in Generative Networks. (arXiv:2107.14229v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Valerie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jeffrey Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joon Sik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1\">Gregory Plumb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>",
          "description": "Despite increasing interest in the field of Interpretable Machine Learning\n(IML), a significant gap persists between the technical objectives targeted by\nresearchers' methods and the high-level goals of consumers' use cases. In this\nwork, we synthesize foundational work on IML methods and evaluation into an\nactionable taxonomy. This taxonomy serves as a tool to conceptualize the gap\nbetween researchers and consumers, illustrated by the lack of connections\nbetween its methods and use cases components. It also provides the foundation\nfrom which we describe a three-step workflow to better enable researchers and\nconsumers to work together to discover what types of methods are useful for\nwhat use cases. Eventually, by building on the results generated from this\nworkflow, a more complete version of the taxonomy will increasingly allow\nconsumers to find relevant methods for their target use cases and researchers\nto identify applicable use cases for their proposed methods.",
          "link": "http://arxiv.org/abs/2103.06254",
          "publishedOn": "2021-07-30T02:13:30.276Z",
          "wordCount": 617,
          "title": "Interpretable Machine Learning: Moving From Mythos to Diagnostics. (arXiv:2103.06254v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eugene Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Cheng-Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yi Lee</a>",
          "description": "Deep neural networks (DNNs) are known to perform well when deployed to test\ndistributions that shares high similarity with the training distribution.\nFeeding DNNs with new data sequentially that were unseen in the training\ndistribution has two major challenges -- fast adaptation to new tasks and\ncatastrophic forgetting of old tasks. Such difficulties paved way for the\non-going research on few-shot learning and continual learning. To tackle these\nproblems, we introduce Attentive Independent Mechanisms (AIM). We incorporate\nthe idea of learning using fast and slow weights in conjunction with the\ndecoupling of the feature extraction and higher-order conceptual learning of a\nDNN. AIM is designed for higher-order conceptual learning, modeled by a mixture\nof experts that compete to learn independent concepts to solve a new task. AIM\nis a modular component that can be inserted into existing deep learning\nframeworks. We demonstrate its capability for few-shot learning by adding it to\nSIB and trained on MiniImageNet and CIFAR-FS, showing significant improvement.\nAIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and\nMiniImageNet to demonstrate its capability in continual learning. Code made\npublicly available at https://github.com/huang50213/AIM-Fewshot-Continual.",
          "link": "http://arxiv.org/abs/2107.14053",
          "publishedOn": "2021-07-30T02:13:30.270Z",
          "wordCount": 643,
          "title": "Few-Shot and Continual Learning with Attentive Independent Mechanisms. (arXiv:2107.14053v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.02469",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Subramani_K/0/1/0/all/0/1\">Krishna Subramani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "Most audio processing pipelines involve transformations that act on\nfixed-dimensional input representations of audio. For example, when using the\nShort Time Fourier Transform (STFT) the DFT size specifies a fixed dimension\nfor the input representation. As a consequence, most audio machine learning\nmodels are designed to process fixed-size vector inputs which often prohibits\nthe repurposing of learned models on audio with different sampling rates or\nalternative representations. We note, however, that the intrinsic spectral\ninformation in the audio signal is invariant to the choice of the input\nrepresentation or the sampling rate. Motivated by this, we introduce a novel\nway of processing audio signals by treating them as a collection of points in\nfeature space, and we use point cloud machine learning models that give us\ninvariance to the choice of representation parameters, such as DFT size or the\nsampling rate. Additionally, we observe that these methods result in smaller\nmodels, and allow us to significantly subsample the input representation with\nminimal effects to a trained model performance.",
          "link": "http://arxiv.org/abs/2105.02469",
          "publishedOn": "2021-07-30T02:13:30.177Z",
          "wordCount": 627,
          "title": "Point Cloud Audio Processing. (arXiv:2105.02469v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13822",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Esche_E/0/1/0/all/0/1\">Erik Esche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Talis_T/0/1/0/all/0/1\">Torben Talis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weigert_J/0/1/0/all/0/1\">Joris Weigert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brand_Rihm_G/0/1/0/all/0/1\">Gerardo Brand-Rihm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_B/0/1/0/all/0/1\">Byungjun You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_C/0/1/0/all/0/1\">Christian Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Repke_J/0/1/0/all/0/1\">Jens-Uwe Repke</a>",
          "description": "Continuously operated (bio-)chemical processes increasingly suffer from\nexternal disturbances, such as feed fluctuations or changes in market\nconditions. Product quality often hinges on control of rarely measured\nconcentrations, which are expensive to measure. Semi-supervised regression is a\npossible building block and method from machine learning to construct\nsoft-sensors for such infrequently measured states. Using two case studies,\ni.e., the Williams-Otto process and a bioethanol production process,\nsemi-supervised regression is compared against standard regression to evaluate\nits merits and its possible scope of application for process control in the\n(bio-)chemical industry.",
          "link": "http://arxiv.org/abs/2107.13822",
          "publishedOn": "2021-07-30T02:13:30.166Z",
          "wordCount": 545,
          "title": "Semi-supervised Learning for Data-driven Soft-sensing of Biological and Chemical Processes. (arXiv:2107.13822v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1\">Eugene Valassakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1\">Norman Di Palo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>",
          "description": "In this paper, we study the problem of zero-shot sim-to-real when the task\nrequires both highly precise control with sub-millimetre error tolerance, and\nwide task space generalisation. Our framework involves a coarse-to-fine\ncontroller, where trajectories begin with classical motion planning using\nICP-based pose estimation, and transition to a learned end-to-end controller\nwhich maps images to actions and is trained in simulation with domain\nrandomisation. In this way, we achieve precise control whilst also generalising\nthe controller across wide task spaces, and keeping the robustness of\nvision-based, end-to-end control. Real-world experiments on a range of\ndifferent tasks show that, by exploiting the best of both worlds, our framework\nsignificantly outperforms purely motion planning methods, and purely\nlearning-based methods. Furthermore, we answer a range of questions on best\npractices for precise sim-to-real transfer, such as how different image sensor\nmodalities and image feature representations perform.",
          "link": "http://arxiv.org/abs/2105.11283",
          "publishedOn": "2021-07-30T02:13:30.159Z",
          "wordCount": 626,
          "title": "Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide Task Spaces. (arXiv:2105.11283v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huan_J/0/1/0/all/0/1\">Jun Huan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "While deep learning succeeds in a wide range of tasks, it highly depends on\nthe massive collection of annotated data which is expensive and time-consuming.\nTo lower the cost of data annotation, active learning has been proposed to\ninteractively query an oracle to annotate a small proportion of informative\nsamples in an unlabeled dataset. Inspired by the fact that the samples with\nhigher loss are usually more informative to the model than the samples with\nlower loss, in this paper we present a novel deep active learning approach that\nqueries the oracle for data annotation when the unlabeled sample is believed to\nincorporate high loss. The core of our approach is a measurement Temporal\nOutput Discrepancy (TOD) that estimates the sample loss by evaluating the\ndiscrepancy of outputs given by models at different optimization steps. Our\ntheoretical investigation shows that TOD lower-bounds the accumulated sample\nloss thus it can be used to select informative unlabeled samples. On basis of\nTOD, we further develop an effective unlabeled data sampling strategy as well\nas an unsupervised learning criterion that enhances model performance by\nincorporating the unlabeled data. Due to the simplicity of TOD, our active\nlearning approach is efficient, flexible, and task-agnostic. Extensive\nexperimental results demonstrate that our approach achieves superior\nperformances than the state-of-the-art active learning methods on image\nclassification and semantic segmentation tasks.",
          "link": "http://arxiv.org/abs/2107.14153",
          "publishedOn": "2021-07-30T02:13:30.124Z",
          "wordCount": 673,
          "title": "Semi-Supervised Active Learning with Temporal Output Discrepancy. (arXiv:2107.14153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1\">Eddie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>",
          "description": "Data augmentations are important ingredients in the recipe for training\nrobust neural networks, especially in computer vision. A fundamental question\nis whether neural network features encode data augmentation transformations. To\nanswer this question, we introduce a systematic approach to investigate which\nlayers of neural networks are the most predictive of augmentation\ntransformations. Our approach uses features in pre-trained vision models with\nminimal additional processing to predict common properties transformed by\naugmentation (scale, aspect ratio, hue, saturation, contrast, and brightness).\nSurprisingly, neural network features not only predict data augmentation\ntransformations, but they predict many transformations with high accuracy.\nAfter validating that neural networks encode features corresponding to\naugmentation transformations, we show that these features are encoded in the\nearly layers of modern CNNs, though the augmentation signal fades in deeper\nlayers.",
          "link": "http://arxiv.org/abs/2003.08773",
          "publishedOn": "2021-07-30T02:13:30.095Z",
          "wordCount": 601,
          "title": "Do CNNs Encode Data Augmentations?. (arXiv:2003.08773v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oueida_S/0/1/0/all/0/1\">Soraia Oueida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1\">Soaad Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotb_Y/0/1/0/all/0/1\">Yehia Kotb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Syed Ishtiaque Ahmed</a>",
          "description": "This paper presents a new approach to prevent transportation accidents and\nmonitor driver's behavior using a healthcare AI system that incorporates\nfairness and ethics. Dangerous medical cases and unusual behavior of the driver\nare detected. Fairness algorithm is approached in order to improve\ndecision-making and address ethical issues such as privacy issues, and to\nconsider challenges that appear in the wild within AI in healthcare and\ndriving. A healthcare professional will be alerted about any unusual activity,\nand the driver's location when necessary, is provided in order to enable the\nhealthcare professional to immediately help to the unstable driver. Therefore,\nusing the healthcare AI system allows for accidents to be predicted and thus\nprevented and lives may be saved based on the built-in AI system inside the\nvehicle which interacts with the ER system.",
          "link": "http://arxiv.org/abs/2107.14077",
          "publishedOn": "2021-07-30T02:13:30.090Z",
          "wordCount": 618,
          "title": "A Fair and Ethical Healthcare Artificial Intelligence System for Monitoring Driver Behavior and Preventing Road Accidents. (arXiv:2107.14077v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daghaghi_S/0/1/0/all/0/1\">Shabnam Daghaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medini_T/0/1/0/all/0/1\">Tharun Medini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meisburger_N/0/1/0/all/0/1\">Nicholas Meisburger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beidi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengnan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>",
          "description": "Softmax classifiers with a very large number of classes naturally occur in\nmany applications such as natural language processing and information\nretrieval. The calculation of full softmax is costly from the computational and\nenergy perspective. There have been various sampling approaches to overcome\nthis challenge, popularly known as negative sampling (NS). Ideally, NS should\nsample negative classes from a distribution that is dependent on the input\ndata, the current parameters, and the correct positive class. Unfortunately,\ndue to the dynamically updated parameters and data samples, there is no\nsampling scheme that is provably adaptive and samples the negative classes\nefficiently. Therefore, alternative heuristics like random sampling, static\nfrequency-based sampling, or learning-based biased sampling, which primarily\ntrade either the sampling cost or the adaptivity of samples per iteration are\nadopted. In this paper, we show two classes of distributions where the sampling\nscheme is truly adaptive and provably generates negative samples in\nnear-constant time. Our implementation in C++ on CPU is significantly superior,\nboth in terms of wall-clock time and accuracy, compared to the most optimized\nTensorFlow implementations of other popular negative sampling approaches on\npowerful NVIDIA V100 GPU.",
          "link": "http://arxiv.org/abs/2012.15843",
          "publishedOn": "2021-07-30T02:13:30.077Z",
          "wordCount": 674,
          "title": "A Tale of Two Efficient and Informative Negative Sampling Distributions. (arXiv:2012.15843v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13833",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Noort_F/0/1/0/all/0/1\">Frieda van den Noort</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sirmacek_B/0/1/0/all/0/1\">Beril Sirmacek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slump_C/0/1/0/all/0/1\">Cornelis H. Slump</a>",
          "description": "The prevalance of pelvic floor problems is high within the female population.\nTransperineal ultrasound (TPUS) is the main imaging modality used to\ninvestigate these problems. Automating the analysis of TPUS data will help in\ngrowing our understanding of pelvic floor related problems. In this study we\npresent a U-net like neural network with some convolutional long short term\nmemory (CLSTM) layers to automate the 3D segmentation of the levator ani muscle\n(LAM) in TPUS volumes. The CLSTM layers are added to preserve the inter-slice\n3D information. We reach human level performance on this segmentation task.\nTherefore, we conclude that we successfully automated the segmentation of the\nLAM on 3D TPUS data. This paves the way towards automatic in-vivo analysis of\nthe LAM mechanics in the context of large study populations.",
          "link": "http://arxiv.org/abs/2107.13833",
          "publishedOn": "2021-07-30T02:13:30.062Z",
          "wordCount": 587,
          "title": "Recurrent U-net for automatic pelvic floor muscle segmentation on 3D ultrasound. (arXiv:2107.13833v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.05261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>",
          "description": "Previous studies have demonstrated that end-to-end learning enables\nsignificant shaping gains over additive white Gaussian noise (AWGN) channels.\nHowever, its benefits have not yet been quantified over realistic wireless\nchannel models. This work aims to fill this gap by exploring the gains of\nend-to-end learning over a frequency- and time-selective fading channel using\northogonal frequency division multiplexing (OFDM). With imperfect channel\nknowledge at the receiver, the shaping gains observed on AWGN channels vanish.\nNonetheless, we identify two other sources of performance improvements. The\nfirst comes from a neural network (NN)-based receiver operating over a large\nnumber of subcarriers and OFDM symbols which allows to significantly reduce the\nnumber of orthogonal pilots without loss of bit error rate (BER). The second\ncomes from entirely eliminating orthognal pilots by jointly learning a neural\nreceiver together with either superimposed pilots (SIPs), linearly combined\nwith conventional quadrature amplitude modulation (QAM), or an optimized\nconstellation geometry. The learned geometry works for a wide range of\nsignal-to-noise ratios (SNRs), Doppler and delay spreads, has zero mean and\ndoes hence not contain any form of superimposed pilots. Both schemes achieve\nthe same BER as the pilot-based baseline with around 7% higher throughput.\nThus, we believe that a jointly learned transmitter and receiver are a very\ninteresting component for beyond-5G communication systems which could remove\nthe need and associated control overhead for demodulation reference signals\n(DMRSs).",
          "link": "http://arxiv.org/abs/2009.05261",
          "publishedOn": "2021-07-30T02:13:30.056Z",
          "wordCount": 708,
          "title": "End-to-end Learning for OFDM: From Neural Receivers to Pilotless Communication. (arXiv:2009.05261v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1\">Peter Kairouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1\">Thomas Steinke</a>",
          "description": "We consider training models on private data that are distributed across user\ndevices. To ensure privacy, we add on-device noise and use secure aggregation\nso that only the noisy sum is revealed to the server. We present a\ncomprehensive end-to-end system, which appropriately discretizes the data and\nadds discrete Gaussian noise before performing secure aggregation. We provide a\nnovel privacy analysis for sums of discrete Gaussians and carefully analyze the\neffects of data quantization and modular summation arithmetic. Our theoretical\nguarantees highlight the complex tension between communication, privacy, and\naccuracy. Our extensive experimental results demonstrate that our solution is\nessentially able to match the accuracy to central differential privacy with\nless than 16 bits of precision per value.",
          "link": "http://arxiv.org/abs/2102.06387",
          "publishedOn": "2021-07-30T02:13:30.006Z",
          "wordCount": 604,
          "title": "The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation. (arXiv:2102.06387v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1\">Lauro Langosco di Langosco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strathmann_H/0/1/0/all/0/1\">Heiko Strathmann</a>",
          "description": "Particle-based approximate Bayesian inference approaches such as Stein\nVariational Gradient Descent (SVGD) combine the flexibility and convergence\nguarantees of sampling methods with the computational benefits of variational\ninference. In practice, SVGD relies on the choice of an appropriate kernel\nfunction, which impacts its ability to model the target distribution -- a\nchallenging problem with only heuristic solutions. We propose Neural\nVariational Gradient Descent (NVGD), which is based on parameterizing the\nwitness function of the Stein discrepancy by a deep neural network whose\nparameters are learned in parallel to the inference, mitigating the necessity\nto make any kernel choices whatsoever. We empirically evaluate our method on\npopular synthetic inference problems, real-world Bayesian linear regression,\nand Bayesian neural network inference.",
          "link": "http://arxiv.org/abs/2107.10731",
          "publishedOn": "2021-07-30T02:13:29.987Z",
          "wordCount": 567,
          "title": "Neural Variational Gradient Descent. (arXiv:2107.10731v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weichert_D/0/1/0/all/0/1\">Dorina Weichert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kister_A/0/1/0/all/0/1\">Alexander Kister</a>",
          "description": "A solution that is only reliable under favourable conditions is hardly a safe\nsolution. Min Max Optimization is an approach that returns optima that are\nrobust against worst case conditions. We propose algorithms that perform Min\nMax Optimization in a setting where the function that should be optimized is\nnot known a priori and hence has to be learned by experiments. Therefore we\nextend the Bayesian Optimization setting, which is tailored to maximization\nproblems, to Min Max Optimization problems. While related work extends the two\nacquisition functions Expected Improvement and Gaussian Process Upper\nConfidence Bound; we extend the two acquisition functions Entropy Search and\nKnowledge Gradient. These acquisition functions are able to gain knowledge\nabout the optimum instead of just looking for points that are supposed to be\noptimal. In our evaluation we show that these acquisition functions allow for\nbetter solutions - converging faster to the optimum than the benchmark\nsettings.",
          "link": "http://arxiv.org/abs/2107.13772",
          "publishedOn": "2021-07-30T02:13:29.981Z",
          "wordCount": 586,
          "title": "Bayesian Optimization for Min Max Optimization. (arXiv:2107.13772v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaini_P/0/1/0/all/0/1\">Priyank Jaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holdijk_L/0/1/0/all/0/1\">Lars Holdijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>",
          "description": "We focus on the problem of efficient sampling and learning of probability\ndensities by incorporating symmetries in probabilistic models. We first\nintroduce Equivariant Stein Variational Gradient Descent algorithm -- an\nequivariant sampling method based on Stein's identity for sampling from\ndensities with symmetries. Equivariant SVGD explicitly incorporates symmetry\ninformation in a density through equivariant kernels which makes the resultant\nsampler efficient both in terms of sample complexity and the quality of\ngenerated samples. Subsequently, we define equivariant energy based models to\nmodel invariant densities that are learned using contrastive divergence. By\nutilizing our equivariant SVGD for training equivariant EBMs, we propose new\nways of improving and scaling up training of energy based models. We apply\nthese equivariant energy models for modelling joint densities in regression and\nclassification tasks for image datasets, many-body particle systems and\nmolecular structure generation.",
          "link": "http://arxiv.org/abs/2106.07832",
          "publishedOn": "2021-07-30T02:13:29.922Z",
          "wordCount": 608,
          "title": "Learning Equivariant Energy Based Models with Equivariant Stein Variational Gradient Descent. (arXiv:2106.07832v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Smaranjit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Kanishka Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nethaji_N/0/1/0/all/0/1\">Niketha Nethaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Shivam Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_A/0/1/0/all/0/1\">Arnab Dutta Purkayastha</a>",
          "description": "Tourism in India plays a quintessential role in the country's economy with an\nestimated 9.2% GDP share for the year 2018. With a yearly growth rate of 6.2%,\nthe industry holds a huge potential for being the primary driver of the economy\nas observed in the nations of the Middle East like the United Arab Emirates.\nThe historical and cultural diversity exhibited throughout the geography of the\nnation is a unique spectacle for people around the world and therefore serves\nto attract tourists in tens of millions in number every year. Traditionally,\ntour guides or academic professionals who study these heritage monuments were\nresponsible for providing information to the visitors regarding their\narchitectural and historical significance. However, unfortunately this system\nhas several caveats when considered on a large scale such as unavailability of\nsufficient trained people, lack of accurate information, failure to convey the\nrichness of details in an attractive format etc. Recently, machine learning\napproaches revolving around the usage of monument pictures have been shown to\nbe useful for rudimentary analysis of heritage sights. This paper serves as a\nsurvey of the research endeavors undertaken in this direction which would\neventually provide insights for building an automated decision system that\ncould be utilized to make the experience of tourism in India more modernized\nfor visitors.",
          "link": "http://arxiv.org/abs/2107.14070",
          "publishedOn": "2021-07-30T02:13:29.905Z",
          "wordCount": 690,
          "title": "Machine Learning Advances aiding Recognition and Classification of Indian Monuments and Landmarks. (arXiv:2107.14070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheinert_D/0/1/0/all/0/1\">Dominik Scheinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thamsen_L/0/1/0/all/0/1\">Lauritz Thamsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Houkun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Will_J/0/1/0/all/0/1\">Jonathan Will</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acker_A/0/1/0/all/0/1\">Alexander Acker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wittkopp_T/0/1/0/all/0/1\">Thorsten Wittkopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_O/0/1/0/all/0/1\">Odej Kao</a>",
          "description": "Distributed dataflow systems enable the use of clusters for scalable data\nanalytics. However, selecting appropriate cluster resources for a processing\njob is often not straightforward. Performance models trained on historical\nexecutions of a concrete job are helpful in such situations, yet they are\nusually bound to a specific job execution context (e.g. node type, software\nversions, job parameters) due to the few considered input parameters. Even in\ncase of slight context changes, such supportive models need to be retrained and\ncannot benefit from historical execution data from related contexts.\n\nThis paper presents Bellamy, a novel modeling approach that combines\nscale-outs, dataset sizes, and runtimes with additional descriptive properties\nof a dataflow job. It is thereby able to capture the context of a job\nexecution. Moreover, Bellamy is realizing a two-step modeling approach. First,\na general model is trained on all the available data for a specific scalable\nanalytics algorithm, hereby incorporating data from different contexts.\nSubsequently, the general model is optimized for the specific situation at\nhand, based on the available data for the concrete context. We evaluate our\napproach on two publicly available datasets consisting of execution data from\nvarious dataflow jobs carried out in different environments, showing that\nBellamy outperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.13921",
          "publishedOn": "2021-07-30T02:13:29.887Z",
          "wordCount": 661,
          "title": "Bellamy: Reusing Performance Models for Distributed Dataflow Jobs Across Contexts. (arXiv:2107.13921v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1\">Puoya Tabaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianhao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1\">Ivan Dokmani&#x107;</a>",
          "description": "Many data analysis problems can be cast as distance geometry problems in\n\\emph{space forms} -- Euclidean, spherical, or hyperbolic spaces. Often,\nabsolute distance measurements are often unreliable or simply unavailable and\nonly proxies to absolute distances in the form of similarities are available.\nHence we ask the following: Given only \\emph{comparisons} of similarities\namongst a set of entities, what can be said about the geometry of the\nunderlying space form? To study this question, we introduce the notions of the\n\\textit{ordinal capacity} of a target space form and \\emph{ordinal spread} of\nthe similarity measurements. The latter is an indicator of complex patterns in\nthe measurements, while the former quantifies the capacity of a space form to\naccommodate a set of measurements with a specific ordinal spread profile. We\nprove that the ordinal capacity of a space form is related to its dimension and\nthe sign of its curvature. This leads to a lower bound on the Euclidean and\nspherical embedding dimension of what we term similarity graphs. More\nimportantly, we show that the statistical behavior of the ordinal spread random\nvariables defined on a similarity graph can be used to identify its underlying\nspace form. We support our theoretical claims with experiments on weighted\ntrees, single-cell RNA expression data and spherical cartographic measurements.",
          "link": "http://arxiv.org/abs/2006.09858",
          "publishedOn": "2021-07-30T02:13:29.875Z",
          "wordCount": 690,
          "title": "Geometry of Similarity Comparisons. (arXiv:2006.09858v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_i/0/1/0/all/0/1\">ing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huaxiong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaoshuang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shixin Xu</a>",
          "description": "Stroke is the top leading causes of death in China (Zhou et al. The Lancet\n2019). A dataset from Shanxi Province is used to identify the risk of each\npatient's at four states low/medium/high/attack and provide the state\ntransition tendency through a SHAP DeepExplainer. To improve the accuracy on an\nimbalance sample set, the Quadratic Interactive Deep Neural Network (QIDNN)\nmodel is first proposed by flexible selecting and appending of quadratic\ninteractive features. The experimental results showed that the QIDNN model with\n7 interactive features achieve the state-of-art accuracy $83.25\\%$. Blood\npressure, physical inactivity, smoking, weight and total cholesterol are the\ntop five important features. Then, for the sake of high recall on the most\nurgent state, attack state, the stroke occurrence prediction is taken as an\nauxiliary objective to benefit from multi-objective optimization. The\nprediction accuracy was promoted, meanwhile the recall of the attack state was\nimproved by $24.9\\%$ (to $84.83\\%$) compared to QIDNN (from $67.93\\%$) with\nsame features. The prediction model and analysis tool in this paper not only\ngave the theoretical optimized prediction method, but also provided the\nattribution explanation of risk states and transition direction of each\npatient, which provided a favorable tool for doctors to analyze and diagnose\nthe disease.",
          "link": "http://arxiv.org/abs/2107.14060",
          "publishedOn": "2021-07-30T02:13:29.869Z",
          "wordCount": 651,
          "title": "Multi-objective optimization and explanation for stroke risk assessment in Shanxi province. (arXiv:2107.14060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farhat_H/0/1/0/all/0/1\">Hikmat Farhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rammouz_V/0/1/0/all/0/1\">Veronica Rammouz</a>",
          "description": "With the rapid growth of the number of devices on the Internet, malware poses\na threat not only to the affected devices but also their ability to use said\ndevices to launch attacks on the Internet ecosystem. Rapid malware\nclassification is an important tools to combat that threat. One of the\nsuccessful approaches to classification is based on malware images and deep\nlearning. While many deep learning architectures are very accurate they usually\ntake a long time to train. In this work we perform experiments on multiple well\nknown, pre-trained, deep network architectures in the context of transfer\nlearning. We show that almost all them classify malware accurately with a very\nshort training period.",
          "link": "http://arxiv.org/abs/2107.13743",
          "publishedOn": "2021-07-30T02:13:29.853Z",
          "wordCount": 543,
          "title": "Malware Classification Using Transfer Learning. (arXiv:2107.13743v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13735",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1\">Yubin Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maulik_R/0/1/0/all/0/1\">Romit Maulik</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_T/0/1/0/all/0/1\">Ting Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dietrich_F/0/1/0/all/0/1\">Felix Dietrich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kevrekidis_I/0/1/0/all/0/1\">Ioannis G. Kevrekidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duan_J/0/1/0/all/0/1\">Jinqiao Duan</a>",
          "description": "In this work, we propose a method to learn probability distributions using\nsample path data from stochastic differential equations. Specifically, we\nconsider temporally evolving probability distributions (e.g., those produced by\nintegrating local or nonlocal Fokker-Planck equations). We analyze this\nevolution through machine learning assisted construction of a time-dependent\nmapping that takes a reference distribution (say, a Gaussian) to each and every\ninstance of our evolving distribution. If the reference distribution is the\ninitial condition of a Fokker-Planck equation, what we learn is the time-T map\nof the corresponding solution. Specifically, the learned map is a normalizing\nflow that deforms the support of the reference density to the support of each\nand every density snapshot in time. We demonstrate that this approach can learn\nsolutions to non-local Fokker-Planck equations, such as those arising in\nsystems driven by both Brownian and L\\'evy noise. We present examples with two-\nand three-dimensional, uni- and multimodal distributions to validate the\nmethod.",
          "link": "http://arxiv.org/abs/2107.13735",
          "publishedOn": "2021-07-30T02:13:29.848Z",
          "wordCount": 607,
          "title": "Learning the temporal evolution of multivariate densities via normalizing flows. (arXiv:2107.13735v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banna_V/0/1/0/all/0/1\">Vishnu Banna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinnakotla_A/0/1/0/all/0/1\">Akhil Chinnakotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhengxin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vegesana_A/0/1/0/all/0/1\">Anirudh Vegesana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivek_N/0/1/0/all/0/1\">Naveen Vivek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnappa_K/0/1/0/all/0/1\">Kruthi Krishnappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yung-Hsiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1\">George K. Thiruvathukal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">James C. Davis</a>",
          "description": "Machine learning techniques are becoming a fundamental tool for scientific\nand engineering progress. These techniques are applied in contexts as diverse\nas astronomy and spam filtering. However, correctly applying these techniques\nrequires careful engineering. Much attention has been paid to the technical\npotential; relatively little attention has been paid to the software\nengineering process required to bring research-based machine learning\ntechniques into practical utility. Technology companies have supported the\nengineering community through machine learning frameworks such as TensorFLow\nand PyTorch, but the details of how to engineer complex machine learning models\nin these frameworks have remained hidden.\n\nTo promote best practices within the engineering community, academic\ninstitutions and Google have partnered to launch a Special Interest Group on\nMachine Learning Models (SIGMODELS) whose goal is to develop exemplary\nimplementations of prominent machine learning models in community locations\nsuch as the TensorFlow Model Garden (TFMG). The purpose of this report is to\ndefine a process for reproducing a state-of-the-art machine learning model at a\nlevel of quality suitable for inclusion in the TFMG. We define the engineering\nprocess and elaborate on each step, from paper analysis to model release. We\nreport on our experiences implementing the YOLO model family with a team of 26\nstudent researchers, share the tools we developed, and describe the lessons we\nlearned along the way.",
          "link": "http://arxiv.org/abs/2107.00821",
          "publishedOn": "2021-07-30T02:13:29.842Z",
          "wordCount": 705,
          "title": "An Experience Report on Machine Learning Reproducibility: Guidance for Practitioners and TensorFlow Model Garden Contributors. (arXiv:2107.00821v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.14119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_N/0/1/0/all/0/1\">Nadav Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_I/0/1/0/all/0/1\">Itamar Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1\">Matan Protter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>",
          "description": "In a typical multi-label setting, a picture contains on average few positive\nlabels, and many negative ones. This positive-negative imbalance dominates the\noptimization process, and can lead to under-emphasizing gradients from positive\nlabels during training, resulting in poor accuracy. In this paper, we introduce\na novel asymmetric loss (\"ASL\"), which operates differently on positive and\nnegative samples. The loss enables to dynamically down-weights and\nhard-thresholds easy negative samples, while also discarding possibly\nmislabeled samples. We demonstrate how ASL can balance the probabilities of\ndifferent samples, and how this balancing is translated to better mAP scores.\nWith ASL, we reach state-of-the-art results on multiple popular multi-label\ndatasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate\nASL applicability for other tasks, such as single-label classification and\nobject detection. ASL is effective, easy to implement, and does not increase\nthe training time or complexity.\n\nImplementation is available at: https://github.com/Alibaba-MIIL/ASL.",
          "link": "http://arxiv.org/abs/2009.14119",
          "publishedOn": "2021-07-30T02:13:29.837Z",
          "wordCount": 650,
          "title": "Asymmetric Loss For Multi-Label Classification. (arXiv:2009.14119v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>",
          "description": "For over hundreds of millions of years, sea turtles and their ancestors have\nswum in the vast expanses of the ocean. They have undergone a number of\nevolutionary changes, leading to speciation and sub-speciation. However, in the\npast few decades, some of the most notable forces driving the genetic variance\nand population decline have been global warming and anthropogenic impact\nranging from large-scale poaching, collecting turtle eggs for food, besides\ndumping trash including plastic waste into the ocean. This leads to severe\ndetrimental effects in the sea turtle population, driving them to extinction.\nThis research focusses on the forces causing the decline in sea turtle\npopulation, the necessity for the global conservation efforts along with its\nsuccesses and failures, followed by an in-depth analysis of the modern advances\nin detection and recognition of sea turtles, involving Machine Learning and\nComputer Vision systems, aiding the conservation efforts.",
          "link": "http://arxiv.org/abs/2107.14061",
          "publishedOn": "2021-07-30T02:13:29.830Z",
          "wordCount": 610,
          "title": "The Need and Status of Sea Turtle Conservation and Survey of Associated Computer Vision Advances. (arXiv:2107.14061v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.01005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jordan_I/0/1/0/all/0/1\">Ian D. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokol_P/0/1/0/all/0/1\">Piotr Aleksander Sokol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_I/0/1/0/all/0/1\">Il Memming Park</a>",
          "description": "Gated recurrent units (GRUs) are specialized memory elements for building\nrecurrent neural networks. Despite their incredible success on various tasks,\nincluding extracting dynamics underlying neural data, little is understood\nabout the specific dynamics representable in a GRU network. As a result, it is\nboth difficult to know a priori how successful a GRU network will perform on a\ngiven task, and also their capacity to mimic the underlying behavior of their\nbiological counterparts. Using a continuous time analysis, we gain intuition on\nthe inner workings of GRU networks. We restrict our presentation to low\ndimensions, allowing for a comprehensive visualization. We found a surprisingly\nrich repertoire of dynamical features that includes stable limit cycles\n(nonlinear oscillations), multi-stable dynamics with various topologies, and\nhomoclinic bifurcations. At the same time we were unable to train GRU networks\nto produce continuous attractors, which are hypothesized to exist in biological\nneural networks. We contextualize the usefulness of different kinds of observed\ndynamics and support our claims experimentally.",
          "link": "http://arxiv.org/abs/1906.01005",
          "publishedOn": "2021-07-30T02:13:29.816Z",
          "wordCount": 647,
          "title": "Gated recurrent units viewed through the lens of continuous time dynamical systems. (arXiv:1906.01005v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10848",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1\">Hangjian Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Padilla_O/0/1/0/all/0/1\">Oscar Hernan Madrid Padilla</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_Q/0/1/0/all/0/1\">Qing Zhou</a>",
          "description": "Structural learning of directed acyclic graphs (DAGs) or Bayesian networks\nhas been studied extensively under the assumption that data are independent. We\npropose a new Gaussian DAG model for dependent data which assumes the\nobservations are correlated according to an undirected network. Under this\nmodel, we develop a method to estimate the DAG structure given a topological\nordering of the nodes. The proposed method jointly estimates the Bayesian\nnetwork and the correlations among observations by optimizing a scoring\nfunction based on penalized likelihood. We show that under some mild\nconditions, the proposed method produces consistent estimators after one\niteration. Extensive numerical experiments also demonstrate that by jointly\nestimating the DAG structure and the sample correlation, our method achieves\nmuch higher accuracy in structure learning. When the node ordering is unknown,\nthrough experiments on synthetic and real data, we show that our algorithm can\nbe used to estimate the correlations between samples, with which we can\nde-correlate the dependent data to significantly improve the performance of\nclassical DAG learning methods.",
          "link": "http://arxiv.org/abs/1905.10848",
          "publishedOn": "2021-07-30T02:13:29.810Z",
          "wordCount": 622,
          "title": "Learning Gaussian DAGs from Network Data. (arXiv:1905.10848v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.01656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Disabato_S/0/1/0/all/0/1\">Simone Disabato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roveri_M/0/1/0/all/0/1\">Manuel Roveri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1\">Cesare Alippi</a>",
          "description": "Severe constraints on memory and computation characterizing the\nInternet-of-Things (IoT) units may prevent the execution of Deep Learning\n(DL)-based solutions, which typically demand large memory and high processing\nload. In order to support a real-time execution of the considered DL model at\nthe IoT unit level, DL solutions must be designed having in mind constraints on\nmemory and processing capability exposed by the chosen IoT technology. In this\npaper, we introduce a design methodology aiming at allocating the execution of\nConvolutional Neural Networks (CNNs) on a distributed IoT application. Such a\nmethodology is formalized as an optimization problem where the latency between\nthe data-gathering phase and the subsequent decision-making one is minimized,\nwithin the given constraints on memory and processing load at the units level.\nThe methodology supports multiple sources of data as well as multiple CNNs in\nexecution on the same IoT system allowing the design of CNN-based applications\ndemanding autonomy, low decision-latency, and high Quality-of-Service.",
          "link": "http://arxiv.org/abs/1908.01656",
          "publishedOn": "2021-07-30T02:13:29.804Z",
          "wordCount": 643,
          "title": "Distributed Deep Convolutional Neural Networks for the Internet-of-Things. (arXiv:1908.01656v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.10692",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Harris_K/0/1/0/all/0/1\">Kameron Decker Harris</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yizhe Zhu</a>",
          "description": "We provide a novel analysis of low-rank tensor completion based on hypergraph\nexpanders. As a proxy for rank, we minimize the max-quasinorm of the tensor,\nwhich generalizes the max-norm for matrices. Our analysis is deterministic and\nshows that the number of samples required to approximately recover an order-$t$\ntensor with at most $n$ entries per dimension is linear in $n$, under the\nassumption that the rank and order of the tensor are $O(1)$. As steps in our\nproof, we find a new expander mixing lemma for a $t$-partite, $t$-uniform\nregular hypergraph model, and prove several new properties about tensor\nmax-quasinorm. To the best of our knowledge, this is the first deterministic\nanalysis of tensor completion. We develop a practical algorithm that solves a\nrelaxed version of the max-quasinorm minimization problem, and we demonstrate\nits efficacy with numerical experiments.",
          "link": "http://arxiv.org/abs/1910.10692",
          "publishedOn": "2021-07-30T02:13:29.798Z",
          "wordCount": 626,
          "title": "Deterministic tensor completion with hypergraph expanders. (arXiv:1910.10692v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "We introduce a new image segmentation task, termed Entity Segmentation (ES)\nwith the aim to segment all visual entities in an image without considering\nsemantic category labels. It has many practical applications in image\nmanipulation/editing where the segmentation mask quality is typically crucial\nbut category labels are less important. In this setting, all\nsemantically-meaningful segments are equally treated as categoryless entities\nand there is no thing-stuff distinction. Based on our unified entity\nrepresentation, we propose a center-based entity segmentation framework with\ntwo novel modules to improve mask quality. Experimentally, both our new task\nand framework demonstrate superior advantages as against existing work. In\nparticular, ES enables the following: (1) merging multiple datasets to form a\nlarge training set without the need to resolve label conflicts; (2) any model\ntrained on one dataset can generalize exceptionally well to other datasets with\nunseen domains. Our code is made publicly available at\nhttps://github.com/dvlab-research/Entity.",
          "link": "http://arxiv.org/abs/2107.14228",
          "publishedOn": "2021-07-30T02:13:29.792Z",
          "wordCount": 595,
          "title": "Open-World Entity Segmentation. (arXiv:2107.14228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raman_C/0/1/0/all/0/1\">Chirag Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1\">Hayley Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1\">Marco Loog</a>",
          "description": "The default paradigm for the forecasting of human behavior in social\nconversations is characterized by top-down approaches. These involve\nidentifying predictive relationships between low level nonverbal cues and\nfuture semantic events of interest (e.g. turn changes, group leaving). A common\nhurdle however, is the limited availability of labeled data for supervised\nlearning. In this work, we take the first step in the direction of a bottom-up\nself-supervised approach in the domain. We formulate the task of Social Cue\nForecasting to leverage the larger amount of unlabeled low-level behavior cues,\nand characterize the modeling challenges involved. To address these, we take a\nmeta-learning approach and propose the Social Process (SP) models--socially\naware sequence-to-sequence (Seq2Seq) models within the Neural Process (NP)\nfamily. SP models learn extractable representations of non-semantic future cues\nfor each participant, while capturing global uncertainty by jointly reasoning\nabout the future for all members of the group. Evaluation on synthesized and\nreal-world behavior data shows that our SP models achieve higher log-likelihood\nthan the NP baselines, and also highlights important considerations for\napplying such techniques within the domain of social human interactions.",
          "link": "http://arxiv.org/abs/2107.13576",
          "publishedOn": "2021-07-30T02:13:29.711Z",
          "wordCount": 628,
          "title": "Social Processes: Self-Supervised Forecasting of Nonverbal Cues in Social Conversations. (arXiv:2107.13576v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13721",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Saparbayeva_B/0/1/0/all/0/1\">Bayan Saparbayeva</a>",
          "description": "Mainfold-valued functional data analysis (FDA) recently becomes an active\narea of research motivated by the raising availability of trajectories or\nlongitudinal data observed on non-linear manifolds. The challenges of analyzing\nsuch data comes from many aspects, including infinite dimensionality and\nnonlinearity, as well as time domain or phase variability. In this paper, we\nstudy the amplitude part of manifold-valued functions on $\\S^2$, which is\ninvariant to random time warping or re-parameterization of the function.\nUtilizing the nice geometry of $\\S^2$, we develop a set of efficient and\naccurate tools for temporal alignment of functions, geodesic and sample mean\ncalculation. At the heart of these tools, they rely on gradient descent\nalgorithms with carefully derived gradients. We show the advantages of these\nnewly developed tools over its competitors with extensive simulations and real\ndata, and demonstrate the importance of considering the amplitude part of\nfunctions instead of mixing it with phase variability in mainfold-valued FDA.",
          "link": "http://arxiv.org/abs/2107.13721",
          "publishedOn": "2021-07-30T02:13:29.706Z",
          "wordCount": 590,
          "title": "Amplitude Mean of Functional Data on $\\mathbb{S}^2$. (arXiv:2107.13721v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Philippe Burlina</a>",
          "description": "When deploying artificial intelligence (AI) in the real world, being able to\ntrust the operation of the AI by characterizing how it performs is an\never-present and important topic. An important and still largely unexplored\ntask in this characterization is determining major factors within the real\nworld that affect the AI's behavior, such as weather conditions or lighting,\nand either a) being able to give justification for why it may have failed or b)\neliminating the influence the factor has. Determining these sensitive factors\nheavily relies on collected data that is diverse enough to cover numerous\ncombinations of these factors, which becomes more onerous when having many\npotential sensitive factors or operating in complex environments. This paper\ninvestigates methods that discover and separate out individual semantic\nsensitive factors from a given dataset to conduct this characterization as well\nas addressing mitigation of these factors' sensitivity. We also broaden\nremediation of fairness, which normally only addresses socially relevant\nfactors, and widen it to deal with the desensitization of AI with regard to all\npossible aspects of variation in the domain. The proposed methods which\ndiscover these major factors reduce the potentially onerous demands of\ncollecting a sufficiently diverse dataset. In experiments using the road sign\n(GTSRB) and facial imagery (CelebA) datasets, we show the promise of using this\nscheme to perform this characterization and remediation and demonstrate that\nour approach outperforms state of the art approaches.",
          "link": "http://arxiv.org/abs/2107.13625",
          "publishedOn": "2021-07-30T02:13:29.700Z",
          "wordCount": 674,
          "title": "Generalizing Fairness: Discovery and Mitigation of Unknown Sensitive Attributes. (arXiv:2107.13625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elwood_A/0/1/0/all/0/1\">Adam Elwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasparin_A/0/1/0/all/0/1\">Alberto Gasparin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozza_A/0/1/0/all/0/1\">Alessandro Rozza</a>",
          "description": "With the rise in use of social media to promote branded products, the demand\nfor effective influencer marketing has increased. Brands are looking for\nimproved ways to identify valuable influencers among a vast catalogue; this is\neven more challenging with \"micro-influencers\", which are more affordable than\nmainstream ones but difficult to discover. In this paper, we propose a novel\nmulti-task learning framework to improve the state of the art in\nmicro-influencer ranking based on multimedia content. Moreover, since the\nvisual congruence between a brand and influencer has been shown to be good\nmeasure of compatibility, we provide an effective visual method for\ninterpreting our models' decisions, which can also be used to inform brands'\nmedia strategies. We compare with the current state-of-the-art on a recently\nconstructed public dataset and we show significant improvement both in terms of\naccuracy and model complexity. The techniques for ranking and interpretation\npresented in this work can be generalised to arbitrary multimedia ranking tasks\nthat have datasets with a similar structure.",
          "link": "http://arxiv.org/abs/2107.13943",
          "publishedOn": "2021-07-30T02:13:29.659Z",
          "wordCount": 599,
          "title": "Ranking Micro-Influencers: a Novel Multi-Task Learning and Interpretable Framework. (arXiv:2107.13943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13841",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Goetz_A/0/1/0/all/0/1\">Aur&#xe8;le Goetz</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Durmaz_A/0/1/0/all/0/1\">Ali Riza Durmaz</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Muller_M/0/1/0/all/0/1\">Martin M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Thomas_A/0/1/0/all/0/1\">Akhil Thomas</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Britz_D/0/1/0/all/0/1\">Dominik Britz</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kerfriden_P/0/1/0/all/0/1\">Pierre Kerfriden</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Eberl_C/0/1/0/all/0/1\">Chris Eberl</a>",
          "description": "Materials' microstructures are signatures of their alloying composition and\nprocessing history. Therefore, microstructures exist in a wide variety. As\nmaterials become increasingly complex to comply with engineering demands,\nadvanced computer vision (CV) approaches such as deep learning (DL) inevitably\ngain relevance for quantifying microstrucutures' constituents from micrographs.\nWhile DL can outperform classical CV techniques for many tasks, shortcomings\nare poor data efficiency and generalizability across datasets. This is\ninherently in conflict with the expense associated with annotating materials\ndata through experts and extensive materials diversity. To tackle poor domain\ngeneralizability and the lack of labeled data simultaneously, we propose to\napply a sub-class of transfer learning methods called unsupervised domain\nadaptation (UDA). These algorithms address the task of finding domain-invariant\nfeatures when supplied with annotated source data and unannotated target data,\nsuch that performance on the latter distribution is optimized despite the\nabsence of annotations. Exemplarily, this study is conducted on a lath-shaped\nbainite segmentation task in complex phase steel micrographs. Here, the domains\nto bridge are selected to be different metallographic specimen preparations\n(surface etchings) and distinct imaging modalities. We show that a\nstate-of-the-art UDA approach surpasses the na\\\"ive application of source\ndomain trained models on the target domain (generalization baseline) to a large\nextent. This holds true independent of the domain shift, despite using little\ndata, and even when the baseline models were pre-trained or employed data\naugmentation. Through UDA, mIoU was improved over generalization baselines from\n82.2%, 61.0%, 49.7% to 84.7%, 67.3%, 73.3% on three target datasets,\nrespectively. This underlines this techniques' potential to cope with materials\nvariance.",
          "link": "http://arxiv.org/abs/2107.13841",
          "publishedOn": "2021-07-30T02:13:29.652Z",
          "wordCount": 705,
          "title": "Addressing materials' microstructure diversity using transfer learning. (arXiv:2107.13841v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14038",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kashefi_A/0/1/0/all/0/1\">Ali Kashefi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukerji_T/0/1/0/all/0/1\">Tapan Mukerji</a>",
          "description": "We propose a novel deep learning framework for predicting permeability of\nporous media from their digital images. Unlike convolutional neural networks,\ninstead of feeding the whole image volume as inputs to the network, we model\nthe boundary between solid matrix and pore spaces as point clouds and feed them\nas inputs to a neural network based on the PointNet architecture. This approach\novercomes the challenge of memory restriction of graphics processing units and\nits consequences on the choice of batch size, and convergence. Compared to\nconvolutional neural networks, the proposed deep learning methodology provides\nfreedom to select larger batch sizes, due to reducing significantly the size of\nnetwork inputs. Specifically, we use the classification branch of PointNet and\nadjust it for a regression task. As a test case, two and three dimensional\nsynthetic digital rock images are considered. We investigate the effect of\ndifferent components of our neural network on its performance. We compare our\ndeep learning strategy with a convolutional neural network from various\nperspectives, specifically for maximum possible batch size. We inspect the\ngeneralizability of our network by predicting the permeability of real-world\nrock samples as well as synthetic digital rocks that are statistically\ndifferent from the samples used during training. The network predicts the\npermeability of digital rocks a few thousand times faster than a Lattice\nBoltzmann solver with a high level of prediction accuracy.",
          "link": "http://arxiv.org/abs/2107.14038",
          "publishedOn": "2021-07-30T02:13:29.632Z",
          "wordCount": 668,
          "title": "Point-Cloud Deep Learning of Porous Media for Permeability Prediction. (arXiv:2107.14038v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Jiayi Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huayu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1\">Kaichao You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duburcq_A/0/1/0/all/0/1\">Alexis Duburcq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "We present Tianshou, a highly modularized python library for deep\nreinforcement learning (DRL) that uses PyTorch as its backend. Tianshou aims to\nprovide building blocks to replicate common RL experiments and has officially\nsupported more than 15 classic algorithms succinctly. To facilitate related\nresearch and prove Tianshou's reliability, we release Tianshou's benchmark of\nMuJoCo environments, covering 9 classic algorithms and 9/13 Mujoco tasks with\nstate-of-the-art performance. We open-sourced Tianshou at\nhttps://github.com/thu-ml/tianshou/, which has received over 3k stars and\nbecome one of the most popular PyTorch-based DRL libraries.",
          "link": "http://arxiv.org/abs/2107.14171",
          "publishedOn": "2021-07-30T02:13:29.614Z",
          "wordCount": 538,
          "title": "Tianshou: a Highly Modularized Deep Reinforcement Learning Library. (arXiv:2107.14171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zarafshan_P/0/1/0/all/0/1\">Pejman Zarafshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javadi_S/0/1/0/all/0/1\">Saman Javadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roozbahani_A/0/1/0/all/0/1\">Abbas Roozbahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemy_S/0/1/0/all/0/1\">Seyed Mehdi Hashemy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarafshan_P/0/1/0/all/0/1\">Payam Zarafshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etezadi_H/0/1/0/all/0/1\">Hamed Etezadi</a>",
          "description": "Groundwater is the largest storage of freshwater resources, which serves as\nthe major inventory for most of the human consumption through agriculture,\nindustrial, and domestic water supply. In the fields of hydrological, some\nresearchers applied a neural network to forecast rainfall intensity in\nspace-time and introduced the advantages of neural networks compared to\nnumerical models. Then, many researches have been conducted applying\ndata-driven models. Some of them extended an Artificial Neural Networks (ANNs)\nmodel to forecast groundwater level in semi-confined glacial sand and gravel\naquifer under variable state, pumping extraction and climate conditions with\nsignificant accuracy. In this paper, a multi-layer perceptron is applied to\nsimulate groundwater level. The adaptive moment estimation optimization\nalgorithm is also used to this matter. The root mean squared error, mean\nabsolute error, mean squared error and the coefficient of determination ( ) are\nused to evaluate the accuracy of the simulated groundwater level. Total value\nof and RMSE are 0.9458 and 0.7313 respectively which are obtained from the\nmodel output. Results indicate that deep learning algorithms can demonstrate a\nhigh accuracy prediction. Although the optimization of parameters is\ninsignificant in numbers, but due to the value of time in modelling setup, it\nis highly recommended to apply an optimization algorithm in modelling.",
          "link": "http://arxiv.org/abs/2107.13870",
          "publishedOn": "2021-07-30T02:13:29.589Z",
          "wordCount": 653,
          "title": "Artificial Intelligence Hybrid Deep Learning Model for Groundwater Level Prediction Using MLP-ADAM. (arXiv:2107.13870v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeddi_A/0/1/0/all/0/1\">Ashkan B. Jeddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_N/0/1/0/all/0/1\">Nariman L. Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafieezadeh_A/0/1/0/all/0/1\">Abdollah Shafieezadeh</a>",
          "description": "Reinforcement learning (RL) has shown a promising performance in learning\noptimal policies for a variety of sequential decision-making tasks. However, in\nmany real-world RL problems, besides optimizing the main objectives, the agent\nis expected to satisfy a certain level of safety (e.g., avoiding collisions in\nautonomous driving). While RL problems are commonly formalized as Markov\ndecision processes (MDPs), safety constraints are incorporated via constrained\nMarkov decision processes (CMDPs). Although recent advances in safe RL have\nenabled learning safe policies in CMDPs, these safety requirements should be\nsatisfied during both training and in the deployment process. Furthermore, it\nis shown that in memory-based and partially observable environments, these\nmethods fail to maintain safety over unseen out-of-distribution observations.\nTo address these limitations, we propose a Lyapunov-based uncertainty-aware\nsafe RL model. The introduced model adopts a Lyapunov function that converts\ntrajectory-based constraints to a set of local linear constraints. Furthermore,\nto ensure the safety of the agent in highly uncertain environments, an\nuncertainty quantification method is developed that enables identifying\nrisk-averse actions through estimating the probability of constraint\nviolations. Moreover, a Transformers model is integrated to provide the agent\nwith memory to process long time horizons of information via the self-attention\nmechanism. The proposed model is evaluated in grid-world navigation tasks where\nsafety is defined as avoiding static and dynamic obstacles in fully and\npartially observable environments. The results of these experiments show a\nsignificant improvement in the performance of the agent both in achieving\noptimality and satisfying safety constraints.",
          "link": "http://arxiv.org/abs/2107.13944",
          "publishedOn": "2021-07-30T02:13:29.584Z",
          "wordCount": 691,
          "title": "Lyapunov-based uncertainty-aware safe reinforcement learning. (arXiv:2107.13944v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1\">Sai Saketh Rambhatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Michael Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>",
          "description": "Boosting is a method for finding a highly accurate hypothesis by linearly\ncombining many ``weak\" hypotheses, each of which may be only moderately\naccurate. Thus, boosting is a method for learning an ensemble of classifiers.\nWhile boosting has been shown to be very effective for decision trees, its\nimpact on neural networks has not been extensively studied. We prove one\nimportant difference between sums of decision trees compared to sums of\nconvolutional neural networks (CNNs) which is that a sum of decision trees\ncannot be represented by a single decision tree with the same number of\nparameters while a sum of CNNs can be represented by a single CNN. Next, using\nstandard object recognition datasets, we verify experimentally the well-known\nresult that a boosted ensemble of decision trees usually generalizes much\nbetter on testing data than a single decision tree with the same number of\nparameters. In contrast, using the same datasets and boosting algorithms, our\nexperiments show the opposite to be true when using neural networks (both CNNs\nand multilayer perceptrons (MLPs)). We find that a single neural network\nusually generalizes better than a boosted ensemble of smaller neural networks\nwith the same total number of parameters.",
          "link": "http://arxiv.org/abs/2107.13600",
          "publishedOn": "2021-07-30T02:13:29.572Z",
          "wordCount": 636,
          "title": "To Boost or not to Boost: On the Limits of Boosted Neural Networks. (arXiv:2107.13600v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahate_A/0/1/0/all/0/1\">Anil Rahate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walambe_R/0/1/0/all/0/1\">Rahee Walambe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanna_S/0/1/0/all/0/1\">Sheela Ramanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotecha_K/0/1/0/all/0/1\">Ketan Kotecha</a>",
          "description": "Multimodal deep learning systems which employ multiple modalities like text,\nimage, audio, video, etc., are showing better performance in comparison with\nindividual modalities (i.e., unimodal) systems. Multimodal machine learning\ninvolves multiple aspects: representation, translation, alignment, fusion, and\nco-learning. In the current state of multimodal machine learning, the\nassumptions are that all modalities are present, aligned, and noiseless during\ntraining and testing time. However, in real-world tasks, typically, it is\nobserved that one or more modalities are missing, noisy, lacking annotated\ndata, have unreliable labels, and are scarce in training or testing and or\nboth. This challenge is addressed by a learning paradigm called multimodal\nco-learning. The modeling of a (resource-poor) modality is aided by exploiting\nknowledge from another (resource-rich) modality using transfer of knowledge\nbetween modalities, including their representations and predictive models.\nCo-learning being an emerging area, there are no dedicated reviews explicitly\nfocusing on all challenges addressed by co-learning. To that end, in this work,\nwe provide a comprehensive survey on the emerging area of multimodal\nco-learning that has not been explored in its entirety yet. We review\nimplementations that overcome one or more co-learning challenges without\nexplicitly considering them as co-learning challenges. We present the\ncomprehensive taxonomy of multimodal co-learning based on the challenges\naddressed by co-learning and associated implementations. The various techniques\nemployed to include the latest ones are reviewed along with some of the\napplications and datasets. Our final goal is to discuss challenges and\nperspectives along with the important ideas and directions for future work that\nwe hope to be beneficial for the entire research community focusing on this\nexciting domain.",
          "link": "http://arxiv.org/abs/2107.13782",
          "publishedOn": "2021-07-30T02:13:29.552Z",
          "wordCount": 713,
          "title": "Multimodal Co-learning: Challenges, Applications with Datasets, Recent Advances and Future Directions. (arXiv:2107.13782v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdary_G/0/1/0/all/0/1\">G Jignesh Chowdary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1\">Suganya G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_P/0/1/0/all/0/1\">Premalatha M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Y_A/0/1/0/all/0/1\">Asnath Victy Phamila Y</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karunamurthy K</a>",
          "description": "With the advancements in computer technology, there is a rapid development of\nintelligent systems to understand the complex relationships in data to make\npredictions and classifications. Artificail Intelligence based framework is\nrapidly revolutionizing the healthcare industry. These intelligent systems are\nbuilt with machine learning and deep learning based robust models for early\ndiagnosis of diseases and demonstrates a promising supplementary diagnostic\nmethod for frontline clinical doctors and surgeons. Machine Learning and Deep\nLearning based systems can streamline and simplify the steps involved in\ndiagnosis of diseases from clinical and image-based data, thus providing\nsignificant clinician support and workflow optimization. They mimic human\ncognition and are even capable of diagnosing diseases that cannot be diagnosed\nwith human intelligence. This paper focuses on the survey of machine learning\nand deep learning applications in across 16 medical specialties, namely Dental\nmedicine, Haematology, Surgery, Cardiology, Pulmonology, Orthopedics,\nRadiology, Oncology, General medicine, Psychiatry, Endocrinology, Neurology,\nDermatology, Hepatology, Nephrology, Ophthalmology, and Drug discovery. In this\npaper along with the survey, we discuss the advancements of medical practices\nwith these systems and also the impact of these systems on medical\nprofessionals.",
          "link": "http://arxiv.org/abs/2107.14037",
          "publishedOn": "2021-07-30T02:13:29.521Z",
          "wordCount": 644,
          "title": "Machine Learning and Deep Learning Methods for Building Intelligent Systems in Medicine and Drug Discovery: A Comprehensive Survey. (arXiv:2107.14037v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozkara_K/0/1/0/all/0/1\">Kaan Ozkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Navjot Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1\">Deepesh Data</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1\">Suhas Diggavi</a>",
          "description": "Traditionally, federated learning (FL) aims to train a single global model\nwhile collaboratively using multiple clients and a server. Two natural\nchallenges that FL algorithms face are heterogeneity in data across clients and\ncollaboration of clients with {\\em diverse resources}. In this work, we\nintroduce a \\textit{quantized} and \\textit{personalized} FL algorithm QuPeD\nthat facilitates collective (personalized model compression) training via\n\\textit{knowledge distillation} (KD) among clients who have access to\nheterogeneous data and resources. For personalization, we allow clients to\nlearn \\textit{compressed personalized models} with different quantization\nparameters and model dimensions/structures. Towards this, first we propose an\nalgorithm for learning quantized models through a relaxed optimization problem,\nwhere quantization values are also optimized over. When each client\nparticipating in the (federated) learning process has different requirements\nfor the compressed model (both in model dimension and precision), we formulate\na compressed personalization framework by introducing knowledge distillation\nloss for local client objectives collaborating through a global model. We\ndevelop an alternating proximal gradient update for solving this compressed\npersonalization problem, and analyze its convergence properties. Numerically,\nwe validate that QuPeD outperforms competing personalized FL methods, FedAvg,\nand local training of clients in various heterogeneous settings.",
          "link": "http://arxiv.org/abs/2107.13892",
          "publishedOn": "2021-07-30T02:13:29.461Z",
          "wordCount": 635,
          "title": "QuPeD: Quantized Personalization via Distillation with Applications to Federated Learning. (arXiv:2107.13892v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piech_C/0/1/0/all/0/1\">Chris Piech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "High-quality computer science education is limited by the difficulty of\nproviding instructor feedback to students at scale. While this feedback could\nin principle be automated, supervised approaches to predicting the correct\nfeedback are bottlenecked by the intractability of annotating large quantities\nof student code. In this paper, we instead frame the problem of providing\nfeedback as few-shot classification, where a meta-learner adapts to give\nfeedback to student code on a new programming question from just a few examples\nannotated by instructors. Because data for meta-training is limited, we propose\na number of amendments to the typical few-shot learning framework, including\ntask augmentation to create synthetic tasks, and additional side information to\nbuild stronger priors about each task. These additions are combined with a\ntransformer architecture to embed discrete sequences (e.g. code) to a\nprototypical representation of a feedback class label. On a suite of few-shot\nnatural language processing tasks, we match or outperform state-of-the-art\nperformance. Then, on a collection of student solutions to exam questions from\nan introductory university course, we show that our approach reaches an average\nprecision of 88% on unseen questions, surpassing the 82% precision of teaching\nassistants. Our approach was successfully deployed to deliver feedback to\n16,000 student exam-solutions in a programming course offered by a tier 1\nuniversity. This is, to the best of our knowledge, the first successful\ndeployment of a machine learning based feedback to open-ended student code.",
          "link": "http://arxiv.org/abs/2107.14035",
          "publishedOn": "2021-07-30T02:13:29.453Z",
          "wordCount": 677,
          "title": "ProtoTransformer: A Meta-Learning Approach to Providing Student Feedback. (arXiv:2107.14035v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scabini_L/0/1/0/all/0/1\">Leonardo F. S. Scabini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_O/0/1/0/all/0/1\">Odemir M. Bruno</a>",
          "description": "Understanding the behavior of Artificial Neural Networks is one of the main\ntopics in the field recently, as black-box approaches have become usual since\nthe widespread of deep learning. Such high-dimensional models may manifest\ninstabilities and weird properties that resemble complex systems. Therefore, we\npropose Complex Network (CN) techniques to analyze the structure and\nperformance of fully connected neural networks. For that, we build a dataset\nwith 4 thousand models and their respective CN properties. They are employed in\na supervised classification setup considering four vision benchmarks. Each\nneural network is approached as a weighted and undirected graph of neurons and\nsynapses, and centrality measures are computed after training. Results show\nthat these measures are highly related to the network classification\nperformance. We also propose the concept of Bag-Of-Neurons (BoN), a CN-based\napproach for finding topological signatures linking similar neurons. Results\nsuggest that six neuronal types emerge in such networks, independently of the\ntarget domain, and are distributed differently according to classification\naccuracy. We also tackle specific CN properties related to performance, such as\nhigher subgraph centrality on lower-performing models. Our findings suggest\nthat CN properties play a critical role in the performance of fully connected\nneural networks, with topological patterns emerging independently on a wide\nrange of models.",
          "link": "http://arxiv.org/abs/2107.14062",
          "publishedOn": "2021-07-30T02:13:29.434Z",
          "wordCount": 686,
          "title": "Structure and Performance of Fully Connected Neural Networks: Emerging Complex Network Properties. (arXiv:2107.14062v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaodian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wanhang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuihai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>",
          "description": "In recent years, federated learning (FL) has been widely applied for\nsupporting decentralized collaborative learning scenarios. Among existing FL\nmodels, federated logistic regression (FLR) is a widely used statistic model\nand has been used in various industries. To ensure data security and user\nprivacy, FLR leverages homomorphic encryption (HE) to protect the exchanged\ndata among different collaborative parties. However, HE introduces significant\ncomputational overhead (i.e., the cost of data encryption/decryption and\ncalculation over encrypted data), which eventually becomes the performance\nbottleneck of the whole system. In this paper, we propose HAFLO, a GPU-based\nsolution to improve the performance of FLR. The core idea of HAFLO is to\nsummarize a set of performance-critical homomorphic operators (HO) used by FLR\nand accelerate the execution of these operators through a joint optimization of\nstorage, IO, and computation. The preliminary results show that our\nacceleration on FATE, a popular FL framework, achieves a 49.9$\\times$ speedup\nfor heterogeneous LR and 88.4$\\times$ for homogeneous LR.",
          "link": "http://arxiv.org/abs/2107.13797",
          "publishedOn": "2021-07-30T02:13:29.428Z",
          "wordCount": 589,
          "title": "HAFLO: GPU-Based Acceleration for Federated Logistic Regression. (arXiv:2107.13797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simeunovic_J/0/1/0/all/0/1\">Jelena Simeunovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubnel_B/0/1/0/all/0/1\">Baptiste Schubnel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alet_P/0/1/0/all/0/1\">Pierre-Jean Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrillo_R/0/1/0/all/0/1\">Rafael E. Carrillo</a>",
          "description": "Accurate forecasting of solar power generation with fine temporal and spatial\nresolution is vital for the operation of the power grid. However,\nstate-of-the-art approaches that combine machine learning with numerical\nweather predictions (NWP) have coarse resolution. In this paper, we take a\ngraph signal processing perspective and model multi-site photovoltaic (PV)\nproduction time series as signals on a graph to capture their spatio-temporal\ndependencies and achieve higher spatial and temporal resolution forecasts. We\npresent two novel graph neural network models for deterministic multi-site PV\nforecasting dubbed the graph-convolutional long short term memory (GCLSTM) and\nthe graph-convolutional transformer (GCTrafo) models. These methods rely solely\non production data and exploit the intuition that PV systems provide a dense\nnetwork of virtual weather stations. The proposed methods were evaluated in two\ndata sets for an entire year: 1) production data from 304 real PV systems, and\n2) simulated production of 1000 PV systems, both distributed over Switzerland.\nThe proposed models outperform state-of-the-art multi-site forecasting methods\nfor prediction horizons of six hours ahead. Furthermore, the proposed models\noutperform state-of-the-art single-site methods with NWP as inputs on horizons\nup to four hours ahead.",
          "link": "http://arxiv.org/abs/2107.13875",
          "publishedOn": "2021-07-30T02:13:29.421Z",
          "wordCount": 638,
          "title": "Spatio-temporal graph neural networks for multi-site PV power forecasting. (arXiv:2107.13875v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13657",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Goel_G/0/1/0/all/0/1\">Gautam Goel</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hassibi_B/0/1/0/all/0/1\">Babak Hassibi</a>",
          "description": "We consider control from the perspective of competitive analysis. Unlike much\nprior work on learning-based control, which focuses on minimizing regret\nagainst the best controller selected in hindsight from some specific class, we\nfocus on designing an online controller which competes against a clairvoyant\noffline optimal controller. A natural performance metric in this setting is\ncompetitive ratio, which is the ratio between the cost incurred by the online\ncontroller and the cost incurred by the offline optimal controller. Using\noperator-theoretic techniques from robust control, we derive a computationally\nefficient state-space description of the the controller with optimal\ncompetitive ratio in both finite-horizon and infinite-horizon settings. We\nextend competitive control to nonlinear systems using Model Predictive Control\n(MPC) and present numerical experiments which show that our competitive\ncontroller can significantly outperform standard $H_2$ and $H_{\\infty}$\ncontrollers in the MPC setting.",
          "link": "http://arxiv.org/abs/2107.13657",
          "publishedOn": "2021-07-30T02:13:29.415Z",
          "wordCount": 559,
          "title": "Competitive Control. (arXiv:2107.13657v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14112",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Baucas_M/0/1/0/all/0/1\">Marc Jayson Baucas</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Spachos_P/0/1/0/all/0/1\">Petros Spachos</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gregori_S/0/1/0/all/0/1\">Stefano Gregori</a>",
          "description": "Medical conditions and cases are growing at a rapid pace, where physical\nspace is starting to be constrained. Hospitals and clinics no longer have the\nability to accommodate large numbers of incoming patients. It is clear that the\ncurrent state of the health industry needs to improve its valuable and limited\nresources. The evolution of the Internet of Things (IoT) devices along with\nassistive technologies can alleviate the problem in healthcare, by being a\nconvenient and easy means of accessing healthcare services wirelessly. There is\na plethora of IoT devices and potential applications that can take advantage of\nthe unique characteristics that these technologies can offer. However, at the\nsame time, these services pose novel challenges that need to be properly\naddressed. In this article, we review some popular categories of IoT-based\napplications for healthcare along with their devices. Then, we describe the\nchallenges and discuss how research can properly address the open issues and\nimprove the already existing implementations in healthcare. Further possible\nsolutions are also discussed to show their potential in being viable solutions\nfor future healthcare applications",
          "link": "http://arxiv.org/abs/2107.14112",
          "publishedOn": "2021-07-30T02:13:29.409Z",
          "wordCount": 647,
          "title": "Internet-of-Things Devices and Assistive Technologies for Healthcare: Applications, Challenges, and Opportunities. (arXiv:2107.14112v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "This paper surveys and organizes research works in a new paradigm in natural\nlanguage processing, which we dub \"prompt-based learning\". Unlike traditional\nsupervised learning, which trains a model to take in an input x and predict an\noutput y as P(y|x), prompt-based learning is based on language models that\nmodel the probability of text directly. To use these models to perform\nprediction tasks, the original input x is modified using a template into a\ntextual string prompt x' that has some unfilled slots, and then the language\nmodel is used to probabilistically fill the unfilled information to obtain a\nfinal string x, from which the final output y can be derived. This framework is\npowerful and attractive for a number of reasons: it allows the language model\nto be pre-trained on massive amounts of raw text, and by defining a new\nprompting function the model is able to perform few-shot or even zero-shot\nlearning, adapting to new scenarios with few or no labeled data. In this paper\nwe introduce the basics of this promising paradigm, describe a unified set of\nmathematical notations that can cover a wide variety of existing work, and\norganize existing work along several dimensions, e.g.the choice of pre-trained\nmodels, prompts, and tuning strategies. To make the field more accessible to\ninterested beginners, we not only make a systematic review of existing works\nand a highly structured typology of prompt-based concepts, but also release\nother resources, e.g., a website this http URL including\nconstantly-updated survey, and paperlist.",
          "link": "http://arxiv.org/abs/2107.13586",
          "publishedOn": "2021-07-30T02:13:29.395Z",
          "wordCount": 710,
          "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (arXiv:2107.13586v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raschka_S/0/1/0/all/0/1\">Sebastian Raschka</a>",
          "description": "Machine learning has seen a vast increase of interest in recent years, along\nwith an abundance of learning resources. While conventional lectures provide\nstudents with important information and knowledge, we also believe that\nadditional project-based learning components can motivate students to engage in\ntopics more deeply. In addition to incorporating project-based learning in our\ncourses, we aim to develop project-based learning components aligned with\nreal-world tasks, including experimental design and execution, report writing,\noral presentation, and peer-reviewing. This paper describes the organization of\nour project-based machine learning courses with a particular emphasis on the\nclass project components and shares our resources with instructors who would\nlike to include similar elements in their courses.",
          "link": "http://arxiv.org/abs/2107.13671",
          "publishedOn": "2021-07-30T02:13:29.389Z",
          "wordCount": 579,
          "title": "Deeper Learning By Doing: Integrating Hands-On Research Projects Into a Machine Learning Course. (arXiv:2107.13671v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyons_M/0/1/0/all/0/1\">Michael J. Lyons</a>",
          "description": "Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I\ndesigned and photographed JAFFE, a set of facial expression images intended for\nuse in a study of face perception. In 2019, without seeking permission or\ninforming us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely\npublicized art shows. In addition, they published a nonfactual account of the\nimages in the essay \"Excavating AI: The Politics of Images in Machine Learning\nTraining Sets.\" The present article recounts the creation of the JAFFE dataset\nand unravels each of Crawford and Paglen's fallacious statements. I also\ndiscuss JAFFE more broadly in connection with research on facial expression,\naffective computing, and human-computer interaction.",
          "link": "http://arxiv.org/abs/2107.13998",
          "publishedOn": "2021-07-30T02:13:29.381Z",
          "wordCount": 572,
          "title": "\"Excavating AI\" Re-excavated: Debunking a Fallacious Account of the JAFFE Dataset. (arXiv:2107.13998v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lordelo_C/0/1/0/all/0/1\">Carlos Lordelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1\">Emmanouil Benetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1\">Simon Dixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1\">Sven Ahlb&#xe4;ck</a>",
          "description": "This paper proposes a deep convolutional neural network for performing\nnote-level instrument assignment. Given a polyphonic multi-instrumental music\nsignal along with its ground truth or predicted notes, the objective is to\nassign an instrumental source for each note. This problem is addressed as a\npitch-informed classification task where each note is analysed individually. We\nalso propose to utilise several kernel shapes in the convolutional layers in\norder to facilitate learning of efficient timbre-discriminative feature maps.\nExperiments on the MusicNet dataset using 7 instrument classes show that our\napproach is able to achieve an average F-score of 0.904 when the original\nmulti-pitch annotations are used as the pitch information for the system, and\nthat it also excels if the note information is provided using third-party\nmulti-pitch estimation algorithms. We also include ablation studies\ninvestigating the effects of the use of multiple kernel shapes and comparing\ndifferent input representations for the audio and the note-related information.",
          "link": "http://arxiv.org/abs/2107.13617",
          "publishedOn": "2021-07-30T02:13:29.376Z",
          "wordCount": 625,
          "title": "Pitch-Informed Instrument Assignment Using a Deep Convolutional Network with Multiple Kernel Shapes. (arXiv:2107.13617v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Koushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishmam_A/0/1/0/all/0/1\">Abtahi Ishmam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taher_K/0/1/0/all/0/1\">Kazi Abu Taher</a>",
          "description": "Demand forecasting in power sector has become an important part of modern\ndemand management and response systems with the rise of smart metering enabled\ngrids. Long Short-Term Memory (LSTM) shows promising results in predicting time\nseries data which can also be applied to power load demand in smart grids. In\nthis paper, an LSTM based model using neural network architecture is proposed\nto forecast power demand. The model is trained with hourly energy and power\nusage data of four years from a smart grid. After training and prediction, the\naccuracy of the model is compared against the traditional statistical time\nseries analysis algorithms, such as Auto-Regressive (AR), to determine the\nefficiency. The mean absolute percentile error is found to be 1.22 in the\nproposed LSTM model, which is the lowest among the other models. From the\nfindings, it is clear that the inclusion of neural network in predicting power\ndemand reduces the error of prediction significantly. Thus, the application of\nLSTM can enable a more efficient demand response system.",
          "link": "http://arxiv.org/abs/2107.13653",
          "publishedOn": "2021-07-30T02:13:29.370Z",
          "wordCount": 621,
          "title": "Demand Forecasting in Smart Grid Using Long Short-Term Memory. (arXiv:2107.13653v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antaris_S/0/1/0/all/0/1\">Stefanos Antaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafailidis_D/0/1/0/all/0/1\">Dimitrios Rafailidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdzijauskas_S/0/1/0/all/0/1\">Sarunas Girdzijauskas</a>",
          "description": "In this paper we present a deep graph reinforcement learning model to predict\nand improve the user experience during a live video streaming event,\norchestrated by an agent/tracker. We first formulate the user experience\nprediction problem as a classification task, accounting for the fact that most\nof the viewers at the beginning of an event have poor quality of experience due\nto low-bandwidth connections and limited interactions with the tracker. In our\nmodel we consider different factors that influence the quality of user\nexperience and train the proposed model on diverse state-action transitions\nwhen viewers interact with the tracker. In addition, provided that past events\nhave various user experience characteristics we follow a gradient boosting\nstrategy to compute a global model that learns from different events. Our\nexperiments with three real-world datasets of live video streaming events\ndemonstrate the superiority of the proposed model against several baseline\nstrategies. Moreover, as the majority of the viewers at the beginning of an\nevent has poor experience, we show that our model can significantly increase\nthe number of viewers with high quality experience by at least 75% over the\nfirst streaming minutes. Our evaluation datasets and implementation are\npublicly available at https://publicresearch.z13.web.core.windows.net",
          "link": "http://arxiv.org/abs/2107.13619",
          "publishedOn": "2021-07-30T02:13:29.355Z",
          "wordCount": 644,
          "title": "A Deep Graph Reinforcement Learning Model for Improving User Experience in Live Video Streaming. (arXiv:2107.13619v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaulwar_A/0/1/0/all/0/1\">Amit Chaulwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_M/0/1/0/all/0/1\">Michael Huth</a>",
          "description": "Federated analytics has many applications in edge computing, its use can lead\nto better decision making for service provision, product development, and user\nexperience. We propose a Bayesian approach to trend detection in which the\nprobability of a keyword being trendy, given a dataset, is computed via Bayes'\nTheorem; the probability of a dataset, given that a keyword is trendy, is\ncomputed through secure aggregation of such conditional probabilities over\nlocal datasets of users. We propose a protocol, named SAFE, for Bayesian\nfederated analytics that offers sufficient privacy for production grade use\ncases and reduces the computational burden of users and an aggregator. We\nillustrate this approach with a trend detection experiment and discuss how this\napproach could be extended further to make it production-ready.",
          "link": "http://arxiv.org/abs/2107.13640",
          "publishedOn": "2021-07-30T02:13:29.348Z",
          "wordCount": 566,
          "title": "Secure Bayesian Federated Analytics for Privacy-Preserving Trend Detection. (arXiv:2107.13640v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1\">Yashar Deldjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merra_F/0/1/0/all/0/1\">Felice Antonio Merra</a>",
          "description": "Recommender systems (RSs) employ user-item feedback, e.g., ratings, to match\ncustomers to personalized lists of products. Approaches to top-k recommendation\nmainly rely on Learning-To-Rank algorithms and, among them, the most widely\nadopted is Bayesian Personalized Ranking (BPR), which bases on a pair-wise\noptimization approach. Recently, BPR has been found vulnerable against\nadversarial perturbations of its model parameters. Adversarial Personalized\nRanking (APR) mitigates this issue by robustifying BPR via an adversarial\ntraining procedure. The empirical improvements of APR's accuracy performance on\nBPR have led to its wide use in several recommender models. However, a key\noverlooked aspect has been the beyond-accuracy performance of APR, i.e.,\nnovelty, coverage, and amplification of popularity bias, considering that\nrecent results suggest that BPR, the building block of APR, is sensitive to the\nintensification of biases and reduction of recommendation novelty. In this\nwork, we model the learning characteristics of the BPR and APR optimization\nframeworks to give mathematical evidence that, when the feedback data have a\ntailed distribution, APR amplifies the popularity bias more than BPR due to an\nunbalanced number of received positive updates from short-head items. Using\nmatrix factorization (MF), we empirically validate the theoretical results by\nperforming preliminary experiments on two public datasets to compare BPR-MF and\nAPR-MF performance on accuracy and beyond-accuracy metrics. The experimental\nresults consistently show the degradation of novelty and coverage measures and\na worrying amplification of bias.",
          "link": "http://arxiv.org/abs/2107.13876",
          "publishedOn": "2021-07-30T02:13:29.341Z",
          "wordCount": 687,
          "title": "Understanding the Effects of Adversarial Personalized Ranking Optimization Method on Recommendation Quality. (arXiv:2107.13876v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "Pre-trained language models (PLMs) have achieved great success in natural\nlanguage processing. Most of PLMs follow the default setting of architecture\nhyper-parameters (e.g., the hidden dimension is a quarter of the intermediate\ndimension in feed-forward sub-networks) in BERT (Devlin et al., 2019). Few\nstudies have been conducted to explore the design of architecture\nhyper-parameters in BERT, especially for the more efficient PLMs with tiny\nsizes, which are essential for practical deployment on resource-constrained\ndevices. In this paper, we adopt the one-shot Neural Architecture Search (NAS)\nto automatically search architecture hyper-parameters. Specifically, we\ncarefully design the techniques of one-shot learning and the search space to\nprovide an adaptive and efficient development way of tiny PLMs for various\nlatency constraints. We name our method AutoTinyBERT and evaluate its\neffectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show\nthat our method outperforms both the SOTA search-based baseline (NAS-BERT) and\nthe SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM and\nMobileBERT). In addition, based on the obtained architectures, we propose a\nmore efficient development method that is even faster than the development of a\nsingle PLM.",
          "link": "http://arxiv.org/abs/2107.13686",
          "publishedOn": "2021-07-30T02:13:29.334Z",
          "wordCount": 639,
          "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models. (arXiv:2107.13686v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aitio_A/0/1/0/all/0/1\">Antti Aitio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howey_D/0/1/0/all/0/1\">David A. Howey</a>",
          "description": "Hundreds of millions of people lack access to electricity. Decentralised\nsolar-battery systems are key for addressing this whilst avoiding carbon\nemissions and air pollution, but are hindered by relatively high costs and\nrural locations that inhibit timely preventative maintenance. Accurate\ndiagnosis of battery health and prediction of end of life from operational data\nimproves user experience and reduces costs. But lack of controlled validation\ntests and variable data quality mean existing lab-based techniques fail to\nwork. We apply a scaleable probabilistic machine learning approach to diagnose\nhealth in 1027 solar-connected lead-acid batteries, each running for 400-760\ndays, totalling 620 million data rows. We demonstrate 73% accurate prediction\nof end of life, eight weeks in advance, rising to 82% at the point of failure.\nThis work highlights the opportunity to estimate health from existing\nmeasurements using `big data' techniques, without additional equipment,\nextending lifetime and improving performance in real-world applications.",
          "link": "http://arxiv.org/abs/2107.13856",
          "publishedOn": "2021-07-30T02:13:29.328Z",
          "wordCount": 599,
          "title": "Predicting battery end of life from solar off-grid system field data using machine learning. (arXiv:2107.13856v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Han Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thuraisingham_B/0/1/0/all/0/1\">Bhavani Thuraisingham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>",
          "description": "Adversarial training has been empirically proven to be one of the most\neffective and reliable defense methods against adversarial attacks. However,\nalmost all existing studies about adversarial training are focused on balanced\ndatasets, where each class has an equal amount of training examples. Research\non adversarial training with imbalanced training datasets is rather limited. As\nthe initial effort to investigate this problem, we reveal the facts that\nadversarially trained models present two distinguished behaviors from naturally\ntrained models in imbalanced datasets: (1) Compared to natural training,\nadversarially trained models can suffer much worse performance on\nunder-represented classes, when the training dataset is extremely imbalanced.\n(2) Traditional reweighting strategies may lose efficacy to deal with the\nimbalance issue for adversarial training. For example, upweighting the\nunder-represented classes will drastically hurt the model's performance on\nwell-represented classes, and as a result, finding an optimal reweighting value\ncan be tremendously challenging. In this paper, to further understand our\nobservations, we theoretically show that the poor data separability is one key\nreason causing this strong tension between under-represented and\nwell-represented classes. Motivated by this finding, we propose Separable\nReweighted Adversarial Training (SRAT) to facilitate adversarial training under\nimbalanced scenarios, by learning more separable features for different\nclasses. Extensive experiments on various datasets verify the effectiveness of\nthe proposed framework.",
          "link": "http://arxiv.org/abs/2107.13639",
          "publishedOn": "2021-07-30T02:13:29.313Z",
          "wordCount": 643,
          "title": "Imbalanced Adversarial Training with Reweighting. (arXiv:2107.13639v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bachinger_F/0/1/0/all/0/1\">Florian Bachinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>",
          "description": "With the increasing number of created and deployed prediction models and the\ncomplexity of machine learning workflows we require so called model management\nsystems to support data scientists in their tasks. In this work we describe our\ntechnological concept for such a model management system. This concept includes\nversioned storage of data, support for different machine learning algorithms,\nfine tuning of models, subsequent deployment of models and monitoring of model\nperformance after deployment. We describe this concept with a close focus on\nmodel lifecycle requirements stemming from our industry application cases, but\ngeneralize key features that are relevant for all applications of machine\nlearning.",
          "link": "http://arxiv.org/abs/2107.13821",
          "publishedOn": "2021-07-30T02:13:29.306Z",
          "wordCount": 583,
          "title": "Concept for a Technical Infrastructure for Management of Predictive Models in Industrial Applications. (arXiv:2107.13821v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xinyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongjin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuncong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingchao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>",
          "description": "Detecting abnormal activities in real-world surveillance videos is an\nimportant yet challenging task as the prior knowledge about video anomalies is\nusually limited or unavailable. Despite that many approaches have been\ndeveloped to resolve this problem, few of them can capture the normal\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\nseldom explicitly consider the local consistency at frame level and global\ncoherence of temporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative Adversarial\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\nSpecifically, we first present a convolutional transformer to perform future\nframe prediction. It contains three key components, i.e., a convolutional\nencoder to capture the spatial information of the input video clips, a temporal\nself-attention module to encode the temporal dynamics, and a convolutional\ndecoder to integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure, which jointly\nconsiders an image discriminator that can maintain the local consistency at\nframe-level and a video discriminator that can enforce the global coherence of\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\nthe prediction error is used to identify abnormal video frames. Thoroughly\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\nthe proposed adversarial spatio-temporal modeling framework.",
          "link": "http://arxiv.org/abs/2107.13720",
          "publishedOn": "2021-07-30T02:13:29.299Z",
          "wordCount": 708,
          "title": "Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. (arXiv:2107.13720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1\">Hoe-Han Goh</a>",
          "description": "This perspective illustrates some of the AI applications that can accelerate\nthe achievement of SDGs and also highlights some of the considerations that\ncould hinder the efforts towards them. This emphasizes the importance of\nestablishing standard AI guidelines and regulations for the beneficial\napplications of AI.",
          "link": "http://arxiv.org/abs/2107.13966",
          "publishedOn": "2021-07-30T02:13:29.293Z",
          "wordCount": 487,
          "title": "Artificial Intelligence in Achieving Sustainable Development Goals. (arXiv:2107.13966v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aminian_G/0/1/0/all/0/1\">Gholamali Aminian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Y/0/1/0/all/0/1\">Yuheng Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_L/0/1/0/all/0/1\">Laura Toni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1\">Miguel R. D. Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wornell_G/0/1/0/all/0/1\">Gregory Wornell</a>",
          "description": "Bounding the generalization error of a supervised learning algorithm is one\nof the most important problems in learning theory, and various approaches have\nbeen developed. However, existing bounds are often loose and lack of\nguarantees. As a result, they may fail to characterize the exact generalization\nability of a learning algorithm. Our main contribution is an exact\ncharacterization of the expected generalization error of the well-known Gibbs\nalgorithm in terms of symmetrized KL information between the input training\nsamples and the output hypothesis. Such a result can be applied to tighten\nexisting expected generalization error bound. Our analysis provides more\ninsight on the fundamental role the symmetrized KL information plays in\ncontrolling the generalization error of the Gibbs algorithm.",
          "link": "http://arxiv.org/abs/2107.13656",
          "publishedOn": "2021-07-30T02:13:29.277Z",
          "wordCount": 608,
          "title": "Characterizing the Generalization Error of Gibbs Algorithm with Symmetrized KL information. (arXiv:2107.13656v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Otles_E/0/1/0/all/0/1\">Erkin &#xd6;tle&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jeeheh Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Benjamin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bochinski_M/0/1/0/all/0/1\">Michelle Bochinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hyeon Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortwine_J/0/1/0/all/0/1\">Justin Ortwine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_E/0/1/0/all/0/1\">Erica Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washer_L/0/1/0/all/0/1\">Laraine Washer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_V/0/1/0/all/0/1\">Vincent B. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Krishna Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1\">Jenna Wiens</a>",
          "description": "Once integrated into clinical care, patient risk stratification models may\nperform worse compared to their retrospective performance. To date, it is\nwidely accepted that performance will degrade over time due to changes in care\nprocesses and patient populations. However, the extent to which this occurs is\npoorly understood, in part because few researchers report prospective\nvalidation performance. In this study, we compare the 2020-2021 ('20-'21)\nprospective performance of a patient risk stratification model for predicting\nhealthcare-associated infections to a 2019-2020 ('19-'20) retrospective\nvalidation of the same model. We define the difference in retrospective and\nprospective performance as the performance gap. We estimate how i) \"temporal\nshift\", i.e., changes in clinical workflows and patient populations, and ii)\n\"infrastructure shift\", i.e., changes in access, extraction and transformation\nof data, both contribute to the performance gap. Applied prospectively to\n26,864 hospital encounters during a twelve-month period from July 2020 to June\n2021, the model achieved an area under the receiver operating characteristic\ncurve (AUROC) of 0.767 (95% confidence interval (CI): 0.737, 0.801) and a Brier\nscore of 0.189 (95% CI: 0.186, 0.191). Prospective performance decreased\nslightly compared to '19-'20 retrospective performance, in which the model\nachieved an AUROC of 0.778 (95% CI: 0.744, 0.815) and a Brier score of 0.163\n(95% CI: 0.161, 0.165). The resulting performance gap was primarily due to\ninfrastructure shift and not temporal shift. So long as we continue to develop\nand validate models using data stored in large research data warehouses, we\nmust consider differences in how and when data are accessed, measure how these\ndifferences may affect prospective performance, and work to mitigate those\ndifferences.",
          "link": "http://arxiv.org/abs/2107.13964",
          "publishedOn": "2021-07-30T02:13:29.247Z",
          "wordCount": 733,
          "title": "Mind the Performance Gap: Examining Dataset Shift During Prospective Validation. (arXiv:2107.13964v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grespan_M/0/1/0/all/0/1\">Mattia Medina Grespan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ashim Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>",
          "description": "Symbolic knowledge can provide crucial inductive bias for training neural\nmodels, especially in low data regimes. A successful strategy for incorporating\nsuch knowledge involves relaxing logical statements into sub-differentiable\nlosses for optimization. In this paper, we study the question of how best to\nrelax logical expressions that represent labeled examples and knowledge about a\nproblem; we focus on sub-differentiable t-norm relaxations of logic. We present\ntheoretical and empirical criteria for characterizing which relaxation would\nperform best in various scenarios. In our theoretical study driven by the goal\nof preserving tautologies, the Lukasiewicz t-norm performs best. However, in\nour empirical analysis on the text chunking and digit recognition tasks, the\nproduct t-norm achieves best predictive performance. We analyze this apparent\ndiscrepancy, and conclude with a list of best practices for defining loss\nfunctions via logic.",
          "link": "http://arxiv.org/abs/2107.13646",
          "publishedOn": "2021-07-30T02:13:29.191Z",
          "wordCount": 578,
          "title": "Evaluating Relaxations of Logic for Neural Networks: A Comprehensive Study. (arXiv:2107.13646v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trillos_N/0/1/0/all/0/1\">Nicolas Garcia Trillos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengfei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenghui Li</a>",
          "description": "In this work we study statistical properties of graph-based algorithms for\nmulti-manifold clustering (MMC). In MMC the goal is to retrieve the\nmulti-manifold structure underlying a given Euclidean data set when this one is\nassumed to be obtained by sampling a distribution on a union of manifolds\n$\\mathcal{M} = \\mathcal{M}_1 \\cup\\dots \\cup \\mathcal{M}_N$ that may intersect\nwith each other and that may have different dimensions. We investigate\nsufficient conditions that similarity graphs on data sets must satisfy in order\nfor their corresponding graph Laplacians to capture the right geometric\ninformation to solve the MMC problem. Precisely, we provide high probability\nerror bounds for the spectral approximation of a tensorized Laplacian on\n$\\mathcal{M}$ with a suitable graph Laplacian built from the observations; the\nrecovered tensorized Laplacian contains all geometric information of all the\nindividual underlying manifolds. We provide an example of a family of\nsimilarity graphs, which we call annular proximity graphs with angle\nconstraints, satisfying these sufficient conditions. We contrast our family of\ngraphs with other constructions in the literature based on the alignment of\ntangent planes. Extensive numerical experiments expand the insights that our\ntheory provides on the MMC problem.",
          "link": "http://arxiv.org/abs/2107.13610",
          "publishedOn": "2021-07-30T02:13:29.165Z",
          "wordCount": 635,
          "title": "Large sample spectral analysis of graph-based multi-manifold clustering. (arXiv:2107.13610v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Agni Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_V/0/1/0/all/0/1\">Vikramjit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliver_C/0/1/0/all/0/1\">Carolyn Oliver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullal_A/0/1/0/all/0/1\">Adeeti Ullal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biddulph_M/0/1/0/all/0/1\">Matt Biddulph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mance_I/0/1/0/all/0/1\">Irida Mance</a>",
          "description": "Respiratory rate (RR) is a clinical metric used to assess overall health and\nphysical fitness. An individual's RR can change from their baseline due to\nchronic illness symptoms (e.g., asthma, congestive heart failure), acute\nillness (e.g., breathlessness due to infection), and over the course of the day\ndue to physical exhaustion during heightened exertion. Remote estimation of RR\ncan offer a cost-effective method to track disease progression and\ncardio-respiratory fitness over time. This work investigates a model-driven\napproach to estimate RR from short audio segments obtained after physical\nexertion in healthy adults. Data was collected from 21 individuals using\nmicrophone-enabled, near-field headphones before, during, and after strenuous\nexercise. RR was manually annotated by counting perceived inhalations and\nexhalations. A multi-task Long-Short Term Memory (LSTM) network with\nconvolutional layers was implemented to process mel-filterbank energies,\nestimate RR in varying background noise conditions, and predict heavy\nbreathing, indicated by an RR of more than 25 breaths per minute. The\nmulti-task model performs both classification and regression tasks and\nleverages a mixture of loss functions. It was observed that RR can be estimated\nwith a concordance correlation coefficient (CCC) of 0.76 and a mean squared\nerror (MSE) of 0.2, demonstrating that audio can be a viable signal for\napproximating RR.",
          "link": "http://arxiv.org/abs/2107.14028",
          "publishedOn": "2021-07-30T02:13:29.158Z",
          "wordCount": 671,
          "title": "Estimating Respiratory Rate From Breath Audio Obtained Through Wearable Microphones. (arXiv:2107.14028v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dumpala_S/0/1/0/all/0/1\">Sri Harsha Dumpala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1\">Sebastian Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rempel_S/0/1/0/all/0/1\">Sheri Rempel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uher_R/0/1/0/all/0/1\">Rudolf Uher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oore_S/0/1/0/all/0/1\">Sageev Oore</a>",
          "description": "Depression detection from speech has attracted a lot of attention in recent\nyears. However, the significance of speaker-specific information in depression\ndetection has not yet been explored. In this work, we analyze the significance\nof speaker embeddings for the task of depression detection from speech.\nExperimental results show that the speaker embeddings provide important cues to\nachieve state-of-the-art performance in depression detection. We also show that\ncombining conventional OpenSMILE and COVAREP features, which carry\ncomplementary information, with speaker embeddings further improves the\ndepression detection performance. The significance of temporal context in the\ntraining of deep learning models for depression detection is also analyzed in\nthis paper.",
          "link": "http://arxiv.org/abs/2107.13969",
          "publishedOn": "2021-07-30T02:13:29.151Z",
          "wordCount": 560,
          "title": "Significance of Speaker Embeddings and Temporal Context for Depression Detection. (arXiv:2107.13969v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Breiki_F/0/1/0/all/0/1\">Farha Al Breiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridzuan_M/0/1/0/all/0/1\">Muhammad Ridzuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandhe_R/0/1/0/all/0/1\">Rushali Grandhe</a>",
          "description": "Fine-grained image classification involves identifying different\nsubcategories of a class which possess very subtle discriminatory features.\nFine-grained datasets usually provide bounding box annotations along with class\nlabels to aid the process of classification. However, building large scale\ndatasets with such annotations is a mammoth task. Moreover, this extensive\nannotation is time-consuming and often requires expertise, which is a huge\nbottleneck in building large datasets. On the other hand, self-supervised\nlearning (SSL) exploits the freely available data to generate supervisory\nsignals which act as labels. The features learnt by performing some pretext\ntasks on huge unlabelled data proves to be very helpful for multiple downstream\ntasks.\n\nOur idea is to leverage self-supervision such that the model learns useful\nrepresentations of fine-grained image classes. We experimented with 3 kinds of\nmodels: Jigsaw solving as pretext task, adversarial learning (SRGAN) and\ncontrastive learning based (SimCLR) model. The learned features are used for\ndownstream tasks such as fine-grained image classification. Our code is\navailable at\nthis http URL",
          "link": "http://arxiv.org/abs/2107.13973",
          "publishedOn": "2021-07-30T02:13:29.096Z",
          "wordCount": 608,
          "title": "Self-Supervised Learning for Fine-Grained Image Classification. (arXiv:2107.13973v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14033",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Cui_C/0/1/0/all/0/1\">Chaoran Cui</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Li_X/0/1/0/all/0/1\">Xiaojie Li</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Du_J/0/1/0/all/0/1\">Juan Du</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhang_C/0/1/0/all/0/1\">Chunyun Zhang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Nie_X/0/1/0/all/0/1\">Xiushan Nie</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>",
          "description": "Predicting the future price trends of stocks is a challenging yet intriguing\nproblem given its critical role to help investors make profitable decisions. In\nthis paper, we present a collaborative temporal-relational modeling framework\nfor end-to-end stock trend prediction. The temporal dynamics of stocks is\nfirstly captured with an attention-based recurrent neural network. Then,\ndifferent from existing studies relying on the pairwise correlations between\nstocks, we argue that stocks are naturally connected as a collective group, and\nintroduce the hypergraph structures to jointly characterize the stock\ngroup-wise relationships of industry-belonging and fund-holding. A novel\nhypergraph tri-attention network (HGTAN) is proposed to augment the hypergraph\nconvolutional networks with a hierarchical organization of intra-hyperedge,\ninter-hyperedge, and inter-hypergraph attention modules. In this manner, HGTAN\nadaptively determines the importance of nodes, hyperedges, and hypergraphs\nduring the information propagation among stocks, so that the potential\nsynergies between stock movements can be fully exploited. Extensive experiments\non real-world data demonstrate the effectiveness of our approach. Also, the\nresults of investment simulation show that our approach can achieve a more\ndesirable risk-adjusted return. The data and codes of our work have been\nreleased at https://github.com/lixiaojieff/HGTAN.",
          "link": "http://arxiv.org/abs/2107.14033",
          "publishedOn": "2021-07-30T02:13:28.966Z",
          "wordCount": 630,
          "title": "Temporal-Relational Hypergraph Tri-Attention Networks for Stock Trend Prediction. (arXiv:2107.14033v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Prerak Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleforge_A/0/1/0/all/0/1\">Antoine Deleforge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1\">Emmanuel Vincent</a>",
          "description": "Knowing the geometrical and acoustical parameters of a room may benefit\napplications such as audio augmented reality, speech dereverberation or audio\nforensics. In this paper, we study the problem of jointly estimating the total\nsurface area, the volume, as well as the frequency-dependent reverberation time\nand mean surface absorption of a room in a blind fashion, based on two-channel\nnoisy speech recordings from multiple, unknown source-receiver positions. A\nnovel convolutional neural network architecture leveraging both single- and\ninter-channel cues is proposed and trained on a large, realistic simulated\ndataset. Results on both simulated and real data show that using multiple\nobservations in one room significantly reduces estimation errors and variances\non all target quantities, and that using two channels helps the estimation of\nsurface and volume. The proposed model outperforms a recently proposed blind\nvolume estimation method on the considered datasets.",
          "link": "http://arxiv.org/abs/2107.13832",
          "publishedOn": "2021-07-30T02:13:28.859Z",
          "wordCount": 595,
          "title": "Blind Room Parameter Estimation Using Multiple-Multichannel Speech Recordings. (arXiv:2107.13832v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00719",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_P/0/1/0/all/0/1\">Po-Yu Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_S/0/1/0/all/0/1\">Shu-Min Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_N/0/1/0/all/0/1\">Nan-Lan Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chu Lin</a>",
          "description": "Drug-target interaction (DTI) prediction plays a crucial role in drug\ndiscovery, and deep learning approaches have achieved state-of-the-art\nperformance in this field. We introduce an ensemble of deep learning models\n(EnsembleDLM) for DTI prediction. EnsembleDLM only uses the sequence\ninformation of chemical compounds and proteins, and it aggregates the\npredictions from multiple deep neural networks. This approach not only achieves\nstate-of-the-art performance in Davis and KIBA datasets but also reaches\ncutting-edge performance in the cross-domain applications across different\nbio-activity types and different protein classes. We also demonstrate that\nEnsembleDLM achieves a good performance (Pearson correlation coefficient and\nconcordance index > 0.8) in the new domain with approximately 50% transfer\nlearning data, i.e., the training set has twice as much data as the test set.",
          "link": "http://arxiv.org/abs/2107.00719",
          "publishedOn": "2021-07-29T02:00:11.235Z",
          "wordCount": 588,
          "title": "Toward Drug-Target Interaction Prediction via Ensemble Modeling and Transfer Learning. (arXiv:2107.00719v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1\">Daniel D&#x27;souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1\">Zach Nussbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>",
          "description": "As machine learning models are increasingly employed to assist human\ndecision-makers, it becomes critical to communicate the uncertainty associated\nwith these model predictions. However, the majority of work on uncertainty has\nfocused on traditional probabilistic or ranking approaches - where the model\nassigns low probabilities or scores to uncertain examples. While this captures\nwhat examples are challenging for the model, it does not capture the underlying\nsource of the uncertainty. In this work, we seek to identify examples the model\nis uncertain about and characterize the source of said uncertainty. We explore\nthe benefits of designing a targeted intervention - targeted data augmentation\nof the examples where the model is uncertain over the course of training. We\ninvestigate whether the rate of learning in the presence of additional\ninformation differs between atypical and noisy examples? Our results show that\nthis is indeed the case, suggesting that well-designed interventions over the\ncourse of training can be an effective way to characterize and distinguish\nbetween different sources of uncertainty.",
          "link": "http://arxiv.org/abs/2107.13098",
          "publishedOn": "2021-07-29T02:00:11.209Z",
          "wordCount": 618,
          "title": "A Tale Of Two Long Tails. (arXiv:2107.13098v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Edward S. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>",
          "description": "Training visuomotor robot controllers from scratch on a new robot typically\nrequires generating large amounts of robot-specific data. Could we leverage\ndata previously collected on another robot to reduce or even completely remove\nthis need for robot-specific data? We propose a \"robot-aware\" solution paradigm\nthat exploits readily available robot \"self-knowledge\" such as proprioception,\nkinematics, and camera calibration to achieve this. First, we learn modular\ndynamics models that pair a transferable, robot-agnostic world dynamics module\nwith a robot-specific, analytical robot dynamics module. Next, we set up visual\nplanning costs that draw a distinction between the robot self and the world.\nOur experiments on tabletop manipulation tasks in simulation and on real robots\ndemonstrate that these plug-in improvements dramatically boost the\ntransferability of visuomotor controllers, even permitting zero-shot transfer\nonto new robots for the very first time. Project website:\nhttps://hueds.github.io/rac/",
          "link": "http://arxiv.org/abs/2107.09047",
          "publishedOn": "2021-07-29T02:00:11.202Z",
          "wordCount": 603,
          "title": "Know Thyself: Transferable Visuomotor Control Through Robot-Awareness. (arXiv:2107.09047v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuangjia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Ying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jiahua Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuedong Yang</a>",
          "description": "Constructing appropriate representations of molecules lies at the core of\nnumerous tasks such as material science, chemistry and drug designs. Recent\nresearches abstract molecules as attributed graphs and employ graph neural\nnetworks (GNN) for molecular representation learning, which have made\nremarkable achievements in molecular graph modeling. Albeit powerful, current\nmodels either are based on local aggregation operations and thus miss\nhigher-order graph properties or focus on only node information without fully\nusing the edge information. For this sake, we propose a Communicative Message\nPassing Transformer (CoMPT) neural network to improve the molecular graph\nrepresentation by reinforcing message interactions between nodes and edges\nbased on the Transformer architecture. Unlike the previous transformer-style\nGNNs that treat molecules as fully connected graphs, we introduce a message\ndiffusion mechanism to leverage the graph connectivity inductive bias and\nreduce the message enrichment explosion. Extensive experiments demonstrated\nthat the proposed model obtained superior performances (around 4$\\%$ on\naverage) against state-of-the-art baselines on seven chemical property datasets\n(graph-level tasks) and two chemical shift datasets (node-level tasks). Further\nvisualization studies also indicated a better representation capacity achieved\nby our model.",
          "link": "http://arxiv.org/abs/2107.08773",
          "publishedOn": "2021-07-29T02:00:11.168Z",
          "wordCount": 649,
          "title": "Learning Attributed Graph Representations with Communicative Message Passing Transformer. (arXiv:2107.08773v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "How can deep neural networks encode information that corresponds to words in\nhuman speech into raw acoustic data? This paper proposes two neural network\narchitectures for modeling unsupervised lexical learning from raw acoustic\ninputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN),\nthat combine a Deep Convolutional GAN architecture for audio data (WaveGAN;\narXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN\n(arXiv:1606.03657), and propose a new latent space structure that can model\nfeatural learning simultaneously with a higher level classification and allows\nfor a very low-dimension vector representation of lexical items. Lexical\nlearning is modeled as emergent from an architecture that forces a deep neural\nnetwork to output data such that unique information is retrievable from its\nacoustic outputs. The networks trained on lexical items from TIMIT learn to\nencode unique information corresponding to lexical items in the form of\ncategorical variables in their latent space. By manipulating these variables,\nthe network outputs specific lexical items. The network occasionally outputs\ninnovative lexical items that violate training data, but are linguistically\ninterpretable and highly informative for cognitive modeling and neural network\ninterpretability. Innovative outputs suggest that phonetic and phonological\nrepresentations learned by the network can be productively recombined and\ndirectly paralleled to productivity in human speech: a fiwGAN network trained\non `suit' and `dark' outputs innovative `start', even though it never saw\n`start' or even a [st] sequence in the training data. We also argue that\nsetting latent featural codes to values well beyond training range results in\nalmost categorical generation of prototypical lexical items and reveals\nunderlying values of each latent code.",
          "link": "http://arxiv.org/abs/2006.02951",
          "publishedOn": "2021-07-29T02:00:11.091Z",
          "wordCount": 756,
          "title": "CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks. (arXiv:2006.02951v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub. To the best of our knowledge we believe\nthis the first works to try out semi-supervised learning to improve flood\nsegmentation models.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-29T02:00:11.083Z",
          "wordCount": 774,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Navarro_B_J/0/1/0/all/0/1\">J.-Emeterio Navarro-B</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebert_M/0/1/0/all/0/1\">Martin Gebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielig_R/0/1/0/all/0/1\">Ralf Bielig</a>",
          "description": "This article proposes two different approaches to automatically create a map\nfor valid on-street car parking spaces. For this, we use car sharing park-out\nevents data. The first one uses spatial aggregation and the second a machine\nlearning algorithm. For the former, we chose rasterization and road sectioning;\nfor the latter we chose decision trees. We compare the results of these\napproaches and discuss their advantages and disadvantages. Furthermore, we show\nour results for a neighborhood in the city of Berlin and report a\nclassification accuracy of 91.6\\% on the original imbalanced data. Finally, we\ndiscuss further work; from gathering more data over a longer period of time to\nfitting spatial Gaussian densities to the data and the usage of apps for manual\nvalidation and annotation of parking spaces to improve ground truth data.",
          "link": "http://arxiv.org/abs/2102.06758",
          "publishedOn": "2021-07-29T02:00:11.075Z",
          "wordCount": 621,
          "title": "On automatic extraction of on-street parking spaces using park-out events data. (arXiv:2102.06758v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00773",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Visani_G/0/1/0/all/0/1\">Gian Marco Visani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1\">Alexandra Hope Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kent_D/0/1/0/all/0/1\">David M. Kent</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wong_J/0/1/0/all/0/1\">John B. Wong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cohen_J/0/1/0/all/0/1\">Joshua T. Cohen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "We address the problem of modeling constrained hospital resources in the\nmidst of the COVID-19 pandemic in order to inform decision-makers of future\ndemand and assess the societal value of possible interventions. For broad\napplicability, we focus on the common yet challenging scenario where\npatient-level data for a region of interest are not available. Instead, given\ndaily admissions counts, we model aggregated counts of observed resource use,\nsuch as the number of patients in the general ward, in the intensive care unit,\nor on a ventilator. In order to explain how individual patient trajectories\nproduce these counts, we propose an aggregate count explicit-duration hidden\nMarkov model, nicknamed the ACED-HMM, with an interpretable, compact\nparameterization. We develop an Approximate Bayesian Computation approach that\ndraws samples from the posterior distribution over the model's transition and\nduration parameters given aggregate counts from a specific location, thus\nadapting the model to a region or individual hospital site of interest. Samples\nfrom this posterior can then be used to produce future forecasts of any counts\nof interest. Using data from the United States and the United Kingdom, we show\nour mechanistic approach provides competitive probabilistic forecasts for the\nfuture even as the dynamics of the pandemic shift. Furthermore, we show how our\nmodel provides insight about recovery probabilities or length of stay\ndistributions, and we suggest its potential to answer challenging what-if\nquestions about the societal value of possible interventions.",
          "link": "http://arxiv.org/abs/2105.00773",
          "publishedOn": "2021-07-29T02:00:11.068Z",
          "wordCount": 782,
          "title": "Approximate Bayesian Computation for an Explicit-Duration Hidden Markov Model of COVID-19 Hospital Trajectories. (arXiv:2105.00773v2 [stat.AP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00533",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hall_G/0/1/0/all/0/1\">Georgina Hall</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>",
          "description": "In this paper, we consider the graph alignment problem, which is the problem\nof recovering, given two graphs, a one-to-one mapping between nodes that\nmaximizes edge overlap. This problem can be viewed as a noisy version of the\nwell-known graph isomorphism problem and appears in many applications,\nincluding social network deanonymization and cellular biology. Our focus here\nis on partial recovery, i.e., we look for a one-to-one mapping which is correct\non a fraction of the nodes of the graph rather than on all of them, and we\nassume that the two input graphs to the problem are correlated\nErd\\H{o}s-R\\'enyi graphs of parameters $(n,q,s)$. Our main contribution is then\nto give necessary and sufficient conditions on $(n,q,s)$ under which partial\nrecovery is possible with high probability as the number of nodes $n$ goes to\ninfinity. In particular, we show that it is possible to achieve partial\nrecovery in the $nqs=\\Theta(1)$ regime under certain additional assumptions. An\ninteresting byproduct of the analysis techniques we develop to obtain the\nsufficiency result in the partial recovery setting is a tighter analysis of the\nmaximum likelihood estimator for the graph alignment problem, which leads to\nimproved sufficient conditions for exact recovery.",
          "link": "http://arxiv.org/abs/2007.00533",
          "publishedOn": "2021-07-29T02:00:11.061Z",
          "wordCount": 677,
          "title": "Partial Recovery in the Graph Alignment Problem. (arXiv:2007.00533v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamford_C/0/1/0/all/0/1\">Chris Bamford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grela_L/0/1/0/all/0/1\">Lukasz Grela</a>",
          "description": "In recent years, researchers have achieved great success in applying Deep\nReinforcement Learning (DRL) algorithms to Real-time Strategy (RTS) games,\ncreating strong autonomous agents that could defeat professional players in\nStarCraft~II. However, existing approaches to tackle full games have high\ncomputational costs, usually requiring the use of thousands of GPUs and CPUs\nfor weeks. This paper has two main contributions to address this issue: 1) We\nintroduce Gym-$\\mu$RTS (pronounced \"gym-micro-RTS\") as a fast-to-run RL\nenvironment for full-game RTS research and 2) we present a collection of\ntechniques to scale DRL to play full-game $\\mu$RTS as well as ablation studies\nto demonstrate their empirical importance. Our best-trained bot can defeat\nevery $\\mu$RTS bot we tested from the past $\\mu$RTS competitions when working\nin a single-map setting, resulting in a state-of-the-art DRL agent while only\ntaking about 60 hours of training using a single machine (one GPU, three vCPU,\n16GB RAM). See the blog post at\nhttps://wandb.ai/vwxyzjn/gym-microrts-paper/reports/Gym-RTS-Toward-Affordable-Deep-Reinforcement-Learning-Research-in-Real-Time-Strategy-Games--Vmlldzo2MDIzMTg\nand the source code at https://github.com/vwxyzjn/gym-microrts-paper",
          "link": "http://arxiv.org/abs/2105.13807",
          "publishedOn": "2021-07-29T02:00:11.038Z",
          "wordCount": 666,
          "title": "Gym-$\\mu$RTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning. (arXiv:2105.13807v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.11988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fornasier_M/0/1/0/all/0/1\">Massimo Fornasier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pareschi_L/0/1/0/all/0/1\">Lorenzo Pareschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunnen_P/0/1/0/all/0/1\">Philippe S&#xfc;nnen</a>",
          "description": "We investigate the implementation of a new stochastic Kuramoto-Vicsek-type\nmodel for global optimization of nonconvex functions on the sphere. This model\nbelongs to the class of Consensus-Based Optimization. In fact, particles move\non the sphere driven by a drift towards an instantaneous consensus point, which\nis computed as a convex combination of particle locations, weighted by the cost\nfunction according to Laplace's principle, and it represents an approximation\nto a global minimizer. The dynamics is further perturbed by a random vector\nfield to favor exploration, whose variance is a function of the distance of the\nparticles to the consensus point. In particular, as soon as the consensus is\nreached the stochastic component vanishes. The main results of this paper are\nabout the proof of convergence of the numerical scheme to global minimizers\nprovided conditions of well-preparation of the initial datum. The proof\ncombines previous results of mean-field limit with a novel asymptotic analysis,\nand classical convergence results of numerical methods for SDE. We present\nseveral numerical experiments, which show that the algorithm proposed in the\npresent paper scales well with the dimension and is extremely versatile. To\nquantify the performances of the new approach, we show that the algorithm is\nable to perform essentially as good as ad hoc state of the art methods in\nchallenging problems in signal processing and machine learning, namely the\nphase retrieval problem and the robust subspace detection.",
          "link": "http://arxiv.org/abs/2001.11988",
          "publishedOn": "2021-07-29T02:00:11.020Z",
          "wordCount": 747,
          "title": "Consensus-Based Optimization on the Sphere: Convergence to Global Minimizers and Machine Learning. (arXiv:2001.11988v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guesmi_A/0/1/0/all/0/1\">Amira Guesmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1\">Ihsen Alouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasawneh_K/0/1/0/all/0/1\">Khaled Khasawneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baklouti_M/0/1/0/all/0/1\">Mouna Baklouti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frikha_T/0/1/0/all/0/1\">Tarek Frikha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_M/0/1/0/all/0/1\">Mohamed Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1\">Nael Abu-Ghazaleh</a>",
          "description": "In the past few years, an increasing number of machine-learning and deep\nlearning structures, such as Convolutional Neural Networks (CNNs), have been\napplied to solving a wide range of real-life problems. However, these\narchitectures are vulnerable to adversarial attacks. In this paper, we propose\nfor the first time to use hardware-supported approximate computing to improve\nthe robustness of machine learning classifiers. We show that our approximate\ncomputing implementation achieves robustness across a wide range of attack\nscenarios. Specifically, for black-box and grey-box attack scenarios, we show\nthat successful adversarial attacks against the exact classifier have poor\ntransferability to the approximate implementation. Surprisingly, the robustness\nadvantages also apply to white-box attacks where the attacker has access to the\ninternal implementation of the approximate classifier. We explain some of the\npossible reasons for this robustness through analysis of the internal operation\nof the approximate implementation. Furthermore, our approximate computing model\nmaintains the same level in terms of classification accuracy, does not require\nretraining, and reduces resource utilization and energy consumption of the CNN.\nWe conducted extensive experiments on a set of strong adversarial attacks; We\nempirically show that the proposed implementation increases the robustness of a\nLeNet-5 and an Alexnet CNNs by up to 99% and 87%, respectively for strong\ngrey-box adversarial attacks along with up to 67% saving in energy consumption\ndue to the simpler nature of the approximate logic. We also show that a\nwhite-box attack requires a remarkably higher noise budget to fool the\napproximate classifier, causing an average of 4db degradation of the PSNR of\nthe input image relative to the images that succeed in fooling the exact\nclassifier",
          "link": "http://arxiv.org/abs/2006.07700",
          "publishedOn": "2021-07-29T02:00:11.003Z",
          "wordCount": 761,
          "title": "Defensive Approximation: Enhancing CNNs Security through Approximate Computing. (arXiv:2006.07700v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>",
          "description": "Near out-of-distribution detection (OOD) is a major challenge for deep neural\nnetworks. We demonstrate that large-scale pre-trained transformers can\nsignificantly improve the state-of-the-art (SOTA) on a range of near OOD tasks\nacross different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD\ndetection, we improve the AUROC from 85% (current SOTA) to more than 96% using\nVision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD\ndetection benchmark, we improve the AUROC from 66% to 77% using transformers\nand unsupervised pre-training. To further improve performance, we explore the\nfew-shot outlier exposure setting where a few examples from outlier classes may\nbe available; we show that pre-trained transformers are particularly\nwell-suited for outlier exposure, and that the AUROC of OOD detection on\nCIFAR-100 vs CIFAR-10 can be improved to 98.7% with just 1 image per OOD class,\nand 99.46% with 10 images per OOD class. For multi-modal image-text pre-trained\ntransformers such as CLIP, we explore a new way of using just the names of\noutlier classes as a sole source of information without any accompanying\nimages, and show that this outperforms previous SOTA on standard vision OOD\nbenchmark tasks.",
          "link": "http://arxiv.org/abs/2106.03004",
          "publishedOn": "2021-07-29T02:00:10.994Z",
          "wordCount": 646,
          "title": "Exploring the Limits of Out-of-Distribution Detection. (arXiv:2106.03004v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:10.967Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Minping Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1\">Qiuhua Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yudong Cao</a>",
          "description": "The scope of data-driven fault diagnosis models is greatly improved through\ndeep learning (DL). However, the classical convolution and recurrent structure\nhave their defects in computational efficiency and feature representation,\nwhile the latest Transformer architecture based on attention mechanism has not\nbeen applied in this field. To solve these problems, we propose a novel\ntime-frequency Transformer (TFT) model inspired by the massive success of\nstandard Transformer in sequence processing. Specially, we design a fresh\ntokenizer and encoder module to extract effective abstractions from the\ntime-frequency representation (TFR) of vibration signals. On this basis, a new\nend-to-end fault diagnosis framework based on time-frequency Transformer is\npresented in this paper. Through the case studies on bearing experimental\ndatasets, we constructed the optimal Transformer structure and verified the\nperformance of the diagnostic method. The superiority of the proposed method is\ndemonstrated in comparison with the benchmark model and other state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2104.09079",
          "publishedOn": "2021-07-29T02:00:10.915Z",
          "wordCount": 623,
          "title": "A novel Time-frequency Transformer and its Application in Fault Diagnosis of Rolling Bearings. (arXiv:2104.09079v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02604",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Sabando_M/0/1/0/all/0/1\">Mar&#xed;a Virginia Sabando</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ponzoni_I/0/1/0/all/0/1\">Ignacio Ponzoni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Milios_E/0/1/0/all/0/1\">Evangelos E. Milios</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Soto_A/0/1/0/all/0/1\">Axel J. Soto</a>",
          "description": "With the consolidation of deep learning in drug discovery, several novel\nalgorithms for learning molecular representations have been proposed. Despite\nthe interest of the community in developing new methods for learning molecular\nembeddings and their theoretical benefits, comparing molecular embeddings with\neach other and with traditional representations is not straightforward, which\nin turn hinders the process of choosing a suitable representation for QSAR\nmodeling. A reason behind this issue is the difficulty of conducting a fair and\nthorough comparison of the different existing embedding approaches, which\nrequires numerous experiments on various datasets and training scenarios. To\nclose this gap, we reviewed the literature on methods for molecular embeddings\nand reproduced three unsupervised and two supervised molecular embedding\ntechniques recently proposed in the literature. We compared these five methods\nconcerning their performance in QSAR scenarios using different classification\nand regression datasets. We also compared these representations to traditional\nmolecular representations, namely molecular descriptors and fingerprints. As\nopposed to the expected outcome, our experimental setup consisting of over\n25,000 trained models and statistical tests revealed that the predictive\nperformance using molecular embeddings did not significantly surpass that of\ntraditional representations. While supervised embeddings yielded competitive\nresults compared to those using traditional molecular representations,\nunsupervised embeddings tended to perform worse than traditional\nrepresentations. Our results highlight the need for conducting a careful\ncomparison and analysis of the different embedding techniques prior to using\nthem in drug design tasks, and motivate a discussion about the potential of\nmolecular embeddings in computer-aided drug design.",
          "link": "http://arxiv.org/abs/2104.02604",
          "publishedOn": "2021-07-29T02:00:10.908Z",
          "wordCount": 714,
          "title": "Using Molecular Embeddings in QSAR Modeling: Does it Make a Difference?. (arXiv:2104.02604v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_A/0/1/0/all/0/1\">Ananth Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathioudakis_M/0/1/0/all/0/1\">Michael Mathioudakis</a>",
          "description": "Machine unlearning is the task of updating machine learning (ML) models after\na subset of the training data they were trained on is deleted. Methods for the\ntask are desired to combine effectiveness and efficiency, i.e., they should\neffectively \"unlearn\" deleted data, but in a way that does not require\nexcessive computation effort (e.g., a full retraining) for a small amount of\ndeletions. Such a combination is typically achieved by tolerating some amount\nof approximation in the unlearning. In addition, laws and regulations in the\nspirit of \"the right to be forgotten\" have given rise to requirements for\ncertifiability, i.e., the ability to demonstrate that the deleted data has\nindeed been unlearned by the ML model.\n\nIn this paper, we present an experimental study of the three state-of-the-art\napproximate unlearning methods for linear models and demonstrate the trade-offs\nbetween efficiency, effectiveness and certifiability offered by each method. In\nimplementing the study, we extend some of the existing works and describe a\ncommon ML pipeline to compare and evaluate the unlearning methods on six\nreal-world datasets and a variety of settings. We provide insights into the\neffect of the quantity and distribution of the deleted data on ML models and\nthe performance of each unlearning method in different settings. We also\npropose a practical online strategy to determine when the accumulated error\nfrom approximate unlearning is large enough to warrant a full retrain of the ML\nmodel.",
          "link": "http://arxiv.org/abs/2106.15093",
          "publishedOn": "2021-07-29T02:00:10.886Z",
          "wordCount": 683,
          "title": "Certifiable Machine Unlearning for Linear Models. (arXiv:2106.15093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14331",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1\">Abhranil Das</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Geisler_W/0/1/0/all/0/1\">Wilson S Geisler</a>",
          "description": "Univariate and multivariate normal probability distributions are widely used\nwhen modeling decisions under uncertainty. Computing the performance of such\nmodels requires integrating these distributions over specific domains, which\ncan vary widely across models. Besides some special cases where these integrals\nare easy to calculate, there exist no general analytical expressions, standard\nnumerical methods or software for these integrals. Here we present mathematical\nresults and open-source software that provide (i) the probability in any domain\nof a normal in any dimensions with any parameters, (ii) the probability\ndensity, cumulative distribution, and inverse cumulative distribution of any\nfunction of a normal vector, (iii) the classification errors among any number\nof normal distributions, the Bayes-optimal discriminability index and relation\nto the operating characteristic, (iv) dimension reduction and visualizations\nfor such problems, and (v) tests for how reliably these methods may be used on\ngiven data. We demonstrate these tools with vision research applications of\ndetecting occluding objects in natural scenes, and detecting camouflage.",
          "link": "http://arxiv.org/abs/2012.14331",
          "publishedOn": "2021-07-29T02:00:10.854Z",
          "wordCount": 681,
          "title": "A method to integrate and classify normal distributions. (arXiv:2012.14331v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengbo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zitang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weilian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamata_S/0/1/0/all/0/1\">Sei-ichiro Kamata</a>",
          "description": "Various deep neural network architectures (DNNs) maintain massive vital\nrecords in computer vision. While drawing attention worldwide, the design of\nthe overall structure lacks general guidance. Based on the relationship between\nDNN design and numerical differential equations, we performed a fair comparison\nof the residual design with higher-order perspectives. We show that the widely\nused DNN design strategy, constantly stacking a small design (usually 2-3\nlayers), could be easily improved, supported by solid theoretical knowledge and\nwith no extra parameters needed. We reorganise the residual design in\nhigher-order ways, which is inspired by the observation that many effective\nnetworks can be interpreted as different numerical discretisations of\ndifferential equations. The design of ResNet follows a relatively simple\nscheme, which is Euler forward; however, the situation becomes complicated\nrapidly while stacking. We suppose that stacked ResNet is somehow equalled to a\nhigher-order scheme; then, the current method of forwarding propagation might\nbe relatively weak compared with a typical high-order method such as\nRunge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV\nbenchmarks with sufficient experiments. Stable and noticeable increases in\nperformance are observed, and convergence and robustness are also improved. Our\nstacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per\ncent on CIFAR-10, with the same settings and parameters. The proposed strategy\nis fundamental and theoretical and can therefore be applied to any network as a\ngeneral guideline.",
          "link": "http://arxiv.org/abs/2103.15244",
          "publishedOn": "2021-07-29T02:00:10.846Z",
          "wordCount": 727,
          "title": "Rethinking ResNets: Improved Stacking Strategies With High Order Schemes. (arXiv:2103.15244v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarussi_R/0/1/0/all/0/1\">Roei Sarussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brutzkus_A/0/1/0/all/0/1\">Alon Brutzkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>",
          "description": "Can a neural network minimizing cross-entropy learn linearly separable data?\nDespite progress in the theory of deep learning, this question remains\nunsolved. Here we prove that SGD globally optimizes this learning problem for a\ntwo-layer network with Leaky ReLU activations. The learned network can in\nprinciple be very complex. However, empirical evidence suggests that it often\nturns out to be approximately linear. We provide theoretical support for this\nphenomenon by proving that if network weights converge to two weight clusters,\nthis will imply an approximately linear decision boundary. Finally, we show a\ncondition on the optimization that leads to weight clustering. We provide\nempirical results that validate our theoretical analysis.",
          "link": "http://arxiv.org/abs/2101.02533",
          "publishedOn": "2021-07-29T02:00:10.759Z",
          "wordCount": 567,
          "title": "Towards Understanding Learning in Neural Networks with Linear Teachers. (arXiv:2101.02533v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Minto_L/0/1/0/all/0/1\">Lorenzo Minto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_M/0/1/0/all/0/1\">Moritz Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1\">Hamed Haddadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livshits_B/0/1/0/all/0/1\">Benjamin Livshits</a>",
          "description": "Recommender systems are commonly trained on centrally collected user\ninteraction data like views or clicks. This practice however raises serious\nprivacy concerns regarding the recommender's collection and handling of\npotentially sensitive data. Several privacy-aware recommender systems have been\nproposed in recent literature, but comparatively little attention has been\ngiven to systems at the intersection of implicit feedback and privacy. To\naddress this shortcoming, we propose a practical federated recommender system\nfor implicit data under user-level local differential privacy (LDP). The\nprivacy-utility trade-off is controlled by parameters $\\epsilon$ and $k$,\nregulating the per-update privacy budget and the number of $\\epsilon$-LDP\ngradient updates sent by each user respectively. To further protect the user's\nprivacy, we introduce a proxy network to reduce the fingerprinting surface by\nanonymizing and shuffling the reports before forwarding them to the\nrecommender. We empirically demonstrate the effectiveness of our framework on\nthe MovieLens dataset, achieving up to Hit Ratio with K=10 (HR@10) 0.68 on 50k\nusers with 5k items. Even on the full dataset, we show that it is possible to\nachieve reasonable utility with HR@10>0.5 without compromising user privacy.",
          "link": "http://arxiv.org/abs/2105.03941",
          "publishedOn": "2021-07-29T02:00:10.747Z",
          "wordCount": 666,
          "title": "Stronger Privacy for Federated Collaborative Filtering with Implicit Feedback. (arXiv:2105.03941v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_G/0/1/0/all/0/1\">Guannan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yujie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_S/0/1/0/all/0/1\">Steven Low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Na Li</a>",
          "description": "With large-scale integration of renewable generation and distributed energy\nresources (DERs), modern power systems are confronted with new operational\nchallenges, such as growing complexity, increasing uncertainty, and aggravating\nvolatility. Meanwhile, more and more data are becoming available owing to the\nwidespread deployment of smart meters, smart sensors, and upgraded\ncommunication networks. As a result, data-driven control techniques, especially\nreinforcement learning (RL), have attracted surging attention in recent years.\nIn this paper, we provide a tutorial on various RL techniques and how they can\nbe applied to decision-making in power systems. We illustrate RL-based models\nand solutions in three key applications, frequency regulation, voltage control,\nand energy management. We conclude with three critical issues in the\napplication of RL, i.e., safety, scalability, and data. Several potential\nfuture directions are discussed as well.",
          "link": "http://arxiv.org/abs/2102.01168",
          "publishedOn": "2021-07-29T02:00:10.704Z",
          "wordCount": 632,
          "title": "Reinforcement Learning for Decision-Making and Control in Power Systems: Tutorial, Review, and Vision. (arXiv:2102.01168v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramabathiran_A/0/1/0/all/0/1\">Amuthan A. Ramabathiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_P/0/1/0/all/0/1\">Prabhu Ramachandran</a>",
          "description": "We introduce a class of Sparse, Physics-based, and partially Interpretable\nNeural Networks (SPINN) for solving ordinary and partial differential equations\n(PDEs). By reinterpreting a traditional meshless representation of solutions of\nPDEs we develop a class of sparse neural network architectures that are\npartially interpretable. The SPINN model we propose here serves as a seamless\nbridge between two extreme modeling tools for PDEs, namely dense neural network\nbased methods like Physics Informed Neural Networks (PINNs) and traditional\nmesh-free numerical methods, thereby providing a novel means to develop a new\nclass of hybrid algorithms that build on the best of both these viewpoints. A\nunique feature of the SPINN model that distinguishes it from other neural\nnetwork based approximations proposed earlier is that it is (i) interpretable,\nin a particular sense made precise in the work, and (ii) sparse in the sense\nthat it has much fewer connections than typical dense neural networks used for\nPDEs. Further, the SPINN algorithm implicitly encodes mesh adaptivity and is\nable to handle discontinuities in the solutions. In addition, we demonstrate\nthat Fourier series representations can also be expressed as a special class of\nSPINN and propose generalized neural network analogues of Fourier\nrepresentations. We illustrate the utility of the proposed method with a\nvariety of examples involving ordinary differential equations, elliptic,\nparabolic, hyperbolic and nonlinear partial differential equations, and an\nexample in fluid dynamics.",
          "link": "http://arxiv.org/abs/2102.13037",
          "publishedOn": "2021-07-29T02:00:10.684Z",
          "wordCount": 718,
          "title": "SPINN: Sparse, Physics-based, and partially Interpretable Neural Networks for PDEs. (arXiv:2102.13037v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yunsong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stearrett_R/0/1/0/all/0/1\">Ryan Stearrett</a>",
          "description": "A cross-benchmark has been done on three critical aspects, data imputing,\nfeature selection and regression algorithms, for machine learning based\nchemical vapor deposition (CVD) virtual metrology (VM). The result reveals that\nlinear feature selection regression algorithm would extensively under-fit the\nVM data. Data imputing is also necessary to achieve a higher prediction\naccuracy as the data availability is only ~70% when optimal accuracy is\nobtained. This work suggests a nonlinear feature selection and regression\nalgorithm combined with nearest data imputing algorithm would provide a\nprediction accuracy as high as 0.7. This would lead to 70% reduced CVD\nprocessing variation, which is believed to will lead to reduced frequency of\nphysical metrology as well as more reliable mass-produced wafer with improved\nquality.",
          "link": "http://arxiv.org/abs/2107.05071",
          "publishedOn": "2021-07-29T02:00:10.616Z",
          "wordCount": 574,
          "title": "Machine Learning based CVD Virtual Metrology in Mass Produced Semiconductor Process. (arXiv:2107.05071v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>",
          "description": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pre-trained vision models such as VGG16 and ResNet\n\nIndex Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image\nclassification, Natural Language Processing, t-SNE, text classification,\nTransfer Learning",
          "link": "http://arxiv.org/abs/2106.12479",
          "publishedOn": "2021-07-29T02:00:10.346Z",
          "wordCount": 706,
          "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Waller_I/0/1/0/all/0/1\">Isaac Waller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Ashton Anderson</a>",
          "description": "Optimism about the Internet's potential to bring the world together has been\ntempered by concerns about its role in inflaming the 'culture wars'. Via mass\nselection into like-minded groups, online society may be becoming more\nfragmented and polarized, particularly with respect to partisan differences.\nHowever, our ability to measure the social makeup of online communities, and in\nturn understand the social organization of online platforms, is limited by the\npseudonymous, unstructured, and large-scale nature of digital discussion. We\ndevelop a neural embedding methodology to quantify the positioning of online\ncommunities along social dimensions by leveraging large-scale patterns of\naggregate behaviour. Applying our methodology to 5.1B Reddit comments made in\n10K communities over 14 years, we measure how the macroscale community\nstructure is organized with respect to age, gender, and U.S. political\npartisanship. Examining political content, we find Reddit underwent a\nsignificant polarization event around the 2016 U.S. presidential election, and\nremained highly polarized for years afterward. Contrary to conventional wisdom,\nhowever, individual-level polarization is rare; the system-level shift in 2016\nwas disproportionately driven by the arrival of new and newly political users.\nPolitical polarization on Reddit is unrelated to previous activity on the\nplatform, and is instead temporally aligned with external events. We also\nobserve a stark ideological asymmetry, with the sharp increase in 2016 being\nentirely attributable to changes in right-wing activity. Our methodology is\nbroadly applicable to the study of online interaction, and our findings have\nimplications for the design of online platforms, understanding the social\ncontexts of online behaviour, and quantifying the dynamics and mechanisms of\nonline polarization.",
          "link": "http://arxiv.org/abs/2010.00590",
          "publishedOn": "2021-07-29T02:00:10.318Z",
          "wordCount": 746,
          "title": "Quantifying social organization and political polarization in online platforms. (arXiv:2010.00590v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smedt_J/0/1/0/all/0/1\">Johannes De Smedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeshchenko_A/0/1/0/all/0/1\">Anton Yeshchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyvyanyy_A/0/1/0/all/0/1\">Artem Polyvyanyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerdt_J/0/1/0/all/0/1\">Jochen De Weerdt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendling_J/0/1/0/all/0/1\">Jan Mendling</a>",
          "description": "Process analytics is an umbrella of data-driven techniques which includes\nmaking predictions for individual process instances or overall process models.\nAt the instance level, various novel techniques have been recently devised,\ntackling next activity, remaining time, and outcome prediction. At the model\nlevel, there is a notable void. It is the ambition of this paper to fill this\ngap. To this end, we develop a technique to forecast the entire process model\nfrom historical event data. A forecasted model is a will-be process model\nrepresenting a probable future state of the overall process. Such a forecast\nhelps to investigate the consequences of drift and emerging bottlenecks. Our\ntechnique builds on a representation of event data as multiple time series,\neach capturing the evolution of a behavioural aspect of the process model, such\nthat corresponding forecasting techniques can be applied. Our implementation\ndemonstrates the accuracy of our technique on real-world event log data.",
          "link": "http://arxiv.org/abs/2105.01092",
          "publishedOn": "2021-07-29T02:00:10.311Z",
          "wordCount": 634,
          "title": "Process Model Forecasting Using Time Series Analysis of Event Sequence Data. (arXiv:2105.01092v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "This paper argues that training GANs on local and non-local dependencies in\nspeech data offers insights into how deep neural networks discretize continuous\ndata and how symbolic-like rule-based morphophonological processes emerge in a\ndeep convolutional architecture. Acquisition of speech has recently been\nmodeled as a dependency between latent space and data generated by GANs in\nBegu\\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local\nallophonic distribution. We extend this approach to test learning of local and\nnon-local phonological processes that include approximations of morphological\nprocesses. We further parallel outputs of the model to results of a behavioral\nexperiment where human subjects are trained on the data used for training the\nGAN network. Four main conclusions emerge: (i) the networks provide useful\ninformation for computational models of speech acquisition even if trained on a\ncomparatively small dataset of an artificial grammar learning experiment; (ii)\nlocal processes are easier to learn than non-local processes, which matches\nboth behavioral data in human subjects and typology in the world's languages.\nThis paper also proposes (iii) how we can actively observe the network's\nprogress in learning and explore the effect of training steps on learning\nrepresentations by keeping latent space constant across different training\nsteps. Finally, this paper shows that (iv) the network learns to encode the\npresence of a prefix with a single latent variable; by interpolating this\nvariable, we can actively observe the operation of a non-local phonological\nprocess. The proposed technique for retrieving learning representations has\ngeneral implications for our understanding of how GANs discretize continuous\nspeech data and suggests that rule-like generalizations in the training data\nare represented as an interaction between variables in the network's latent\nspace.",
          "link": "http://arxiv.org/abs/2009.12711",
          "publishedOn": "2021-07-29T02:00:10.300Z",
          "wordCount": 761,
          "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks. (arXiv:2009.12711v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07757",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Musso_D/0/1/0/all/0/1\">Daniele Musso</a>",
          "description": "Local entropic loss functions provide a versatile framework to define\narchitecture-aware regularization procedures. Besides the possibility of being\nanisotropic in the synaptic space, the local entropic smoothening of the loss\nfunction can vary during training, thus yielding a tunable model complexity. A\nscoping protocol where the regularization is strong in the early-stage of the\ntraining and then fades progressively away constitutes an alternative to\nstandard initialization procedures for deep convolutional neural networks,\nnonetheless, it has wider applicability. We analyze anisotropic, local entropic\nsmoothenings in the language of statistical physics and information theory,\nproviding insight into both their interpretation and workings. We comment some\naspects related to the physics of renormalization and the spacetime structure\nof convolutional networks.",
          "link": "http://arxiv.org/abs/2107.07757",
          "publishedOn": "2021-07-29T02:00:10.291Z",
          "wordCount": 585,
          "title": "Entropic alternatives to initialization. (arXiv:2107.07757v2 [cond-mat.dis-nn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1\">Akshat Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "In this paper, we demonstrated a practical application of realistic river\nimage generation using deep learning. Specifically, we explored a generative\nadversarial network (GAN) model capable of generating high-resolution and\nrealistic river images that can be used to support modeling and analysis in\nsurface water estimation, river meandering, wetland loss, and other\nhydrological research studies. First, we have created an extensive repository\nof overhead river images to be used in training. Second, we incorporated the\nProgressive Growing GAN (PGGAN), a network architecture that iteratively trains\nsmaller-resolution GANs to gradually build up to a very high resolution to\ngenerate high quality (i.e., 1024x1024) synthetic river imagery. With simpler\nGAN architectures, difficulties arose in terms of exponential increase of\ntraining time and vanishing/exploding gradient issues, which the PGGAN\nimplementation seemed to significantly reduce. The results presented in this\nstudy show great promise in generating high-quality images and capturing the\ndetails of river structure and flow to support hydrological research, which\noften requires extensive imagery for model performance.",
          "link": "http://arxiv.org/abs/2003.00826",
          "publishedOn": "2021-07-29T02:00:10.284Z",
          "wordCount": 644,
          "title": "Realistic River Image Synthesis using Deep Generative Adversarial Networks. (arXiv:2003.00826v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Clarice Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kathryn Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_A/0/1/0/all/0/1\">Andrew Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1\">Rashidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keya_K/0/1/0/all/0/1\">Kamrun Naher Keya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foulds_J/0/1/0/all/0/1\">James Foulds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shimei Pan</a>",
          "description": "Currently, there is a surge of interest in fair Artificial Intelligence (AI)\nand Machine Learning (ML) research which aims to mitigate discriminatory bias\nin AI algorithms, e.g. along lines of gender, age, and race. While most\nresearch in this domain focuses on developing fair AI algorithms, in this work,\nwe show that a fair AI algorithm on its own may be insufficient to achieve its\nintended results in the real world. Using career recommendation as a case\nstudy, we build a fair AI career recommender by employing gender debiasing\nmachine learning techniques. Our offline evaluation showed that the debiased\nrecommender makes fairer career recommendations without sacrificing its\naccuracy. Nevertheless, an online user study of more than 200 college students\nrevealed that participants on average prefer the original biased system over\nthe debiased system. Specifically, we found that perceived gender disparity is\na determining factor for the acceptance of a recommendation. In other words,\nour results demonstrate we cannot fully address the gender bias issue in AI\nrecommendations without addressing the gender bias in humans.",
          "link": "http://arxiv.org/abs/2106.07112",
          "publishedOn": "2021-07-29T02:00:10.232Z",
          "wordCount": 649,
          "title": "User Acceptance of Gender Stereotypes in Automated Career Recommendations. (arXiv:2106.07112v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09747",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Basak_S/0/1/0/all/0/1\">Subhasish Basak</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Petit_S/0/1/0/all/0/1\">S&#xe9;bastien Petit</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bect_J/0/1/0/all/0/1\">Julien Bect</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vazquez_E/0/1/0/all/0/1\">Emmanuel Vazquez</a>",
          "description": "This article investigates the origin of numerical issues in maximum\nlikelihood parameter estimation for Gaussian process (GP) interpolation and\ninvestigates simple but effective strategies for improving commonly used\nopen-source software implementations. This work targets a basic problem but a\nhost of studies, particularly in the literature of Bayesian optimization, rely\non off-the-shelf GP implementations. For the conclusions of these studies to be\nreliable and reproducible, robust GP implementations are critical.",
          "link": "http://arxiv.org/abs/2101.09747",
          "publishedOn": "2021-07-29T02:00:10.225Z",
          "wordCount": 530,
          "title": "Numerical issues in maximum likelihood parameter estimation for Gaussian process interpolation. (arXiv:2101.09747v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1\">Tommaso R. Cesari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vecchia_R/0/1/0/all/0/1\">Riccardo Della Vecchia</a>",
          "description": "In this preliminary (and unpolished) version of the paper, we study an\nasynchronous online learning setting with a network of agents. At each time\nstep, some of the agents are activated, requested to make a prediction, and pay\nthe corresponding loss. Some feedback is then revealed to these agents and is\nlater propagated through the network. We consider the case of full, bandit, and\nsemi-bandit feedback. In particular, we construct a reduction to delayed\nsingle-agent learning that applies to both the full and the bandit feedback\ncase and allows to obtain regret guarantees for both settings. We complement\nthese results with a near-matching lower bound.",
          "link": "http://arxiv.org/abs/2106.04982",
          "publishedOn": "2021-07-29T02:00:10.218Z",
          "wordCount": 556,
          "title": "Cooperative Online Learning. (arXiv:2106.04982v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1\">Gowri Somanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>",
          "description": "We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2011.10687",
          "publishedOn": "2021-07-29T02:00:10.197Z",
          "wordCount": 658,
          "title": "HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.12278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castera_C/0/1/0/all/0/1\">Camille Castera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Bolte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1\">C&#xe9;dric F&#xe9;votte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Edouard Pauwels</a>",
          "description": "We introduce a new second-order inertial optimization method for machine\nlearning called INNA. It exploits the geometry of the loss function while only\nrequiring stochastic approximations of the function values and the generalized\ngradients. This makes INNA fully implementable and adapted to large-scale\noptimization problems such as the training of deep neural networks. The\nalgorithm combines both gradient-descent and Newton-like behaviors as well as\ninertia. We prove the convergence of INNA for most deep learning problems. To\ndo so, we provide a well-suited framework to analyze deep learning loss\nfunctions involving tame optimization in which we study a continuous dynamical\nsystem together with its discrete stochastic approximations. We prove sublinear\nconvergence for the continuous-time differential inclusion which underlies our\nalgorithm. Additionally, we also show how standard optimization mini-batch\nmethods applied to non-smooth non-convex problems can yield a certain type of\nspurious stationary points never discussed before. We address this issue by\nproviding a theoretical framework around the new idea of $D$-criticality; we\nthen give a simple asymptotic analysis of INNA. Our algorithm allows for using\nan aggressive learning rate of $o(1/\\log k)$. From an empirical viewpoint, we\nshow that INNA returns competitive results with respect to state of the art\n(stochastic gradient descent, ADAGRAD, ADAM) on popular deep learning benchmark\nproblems.",
          "link": "http://arxiv.org/abs/1905.12278",
          "publishedOn": "2021-07-29T02:00:10.190Z",
          "wordCount": 733,
          "title": "An Inertial Newton Algorithm for Deep Learning. (arXiv:1905.12278v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.01845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijun Xu</a>",
          "description": "One of the most critical problems in weight-sharing neural architecture\nsearch is the evaluation of candidate models within a predefined search space.\nIn practice, a one-shot supernet is trained to serve as an evaluator. A\nfaithful ranking certainly leads to more accurate searching results. However,\ncurrent methods are prone to making misjudgments. In this paper, we prove that\ntheir biased evaluation is due to inherent unfairness in the supernet training.\nIn view of this, we propose two levels of constraints: expectation fairness and\nstrict fairness. Particularly, strict fairness ensures equal optimization\nopportunities for all choice blocks throughout the training, which neither\noverestimates nor underestimates their capacity. We demonstrate that this is\ncrucial for improving the confidence of models' ranking. Incorporating the\none-shot supernet trained under the proposed fairness constraints with a\nmulti-objective evolutionary search algorithm, we obtain various\nstate-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation\naccuracy on ImageNet. The models and their evaluation codes are made publicly\navailable online this http URL .",
          "link": "http://arxiv.org/abs/1907.01845",
          "publishedOn": "2021-07-29T02:00:10.158Z",
          "wordCount": 672,
          "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. (arXiv:1907.01845v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Habibpour_M/0/1/0/all/0/1\">Maryam Habibpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharoun_H/0/1/0/all/0/1\">Hassan Gharoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdipour_M/0/1/0/all/0/1\">Mohammadreza Mehdipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajally_A/0/1/0/all/0/1\">AmirReza Tajally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgharnezhad_H/0/1/0/all/0/1\">Hamzeh Asgharnezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsi_A/0/1/0/all/0/1\">Afshar Shamsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafie_Khah_M/0/1/0/all/0/1\">Miadreza Shafie-Khah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catalao_J/0/1/0/all/0/1\">Joao P.S. Catalao</a>",
          "description": "Countless research works of deep neural networks (DNNs) in the task of credit\ncard fraud detection have focused on improving the accuracy of point\npredictions and mitigating unwanted biases by building different network\narchitectures or learning models. Quantifying uncertainty accompanied by point\nestimation is essential because it mitigates model unfairness and permits\npractitioners to develop trustworthy systems which abstain from suboptimal\ndecisions due to low confidence. Explicitly, assessing uncertainties associated\nwith DNNs predictions is critical in real-world card fraud detection settings\nfor characteristic reasons, including (a) fraudsters constantly change their\nstrategies, and accordingly, DNNs encounter observations that are not generated\nby the same process as the training distribution, (b) owing to the\ntime-consuming process, very few transactions are timely checked by\nprofessional experts to update DNNs. Therefore, this study proposes three\nuncertainty quantification (UQ) techniques named Monte Carlo dropout, ensemble,\nand ensemble Monte Carlo dropout for card fraud detection applied on\ntransaction data. Moreover, to evaluate the predictive uncertainty estimates,\nUQ confusion matrix and several performance metrics are utilized. Through\nexperimental results, we show that the ensemble is more effective in capturing\nuncertainty corresponding to generated predictions. Additionally, we\ndemonstrate that the proposed UQ methods provide extra insight to the point\npredictions, leading to elevate the fraud prevention process.",
          "link": "http://arxiv.org/abs/2107.13508",
          "publishedOn": "2021-07-29T02:00:10.077Z",
          "wordCount": 663,
          "title": "Uncertainty-Aware Credit Card Fraud Detection Using Deep Learning. (arXiv:2107.13508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13522",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Hasyim_M/0/1/0/all/0/1\">Muhammad R. Hasyim</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Batton_C/0/1/0/all/0/1\">Clay H. Batton</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Mandadapu_K/0/1/0/all/0/1\">Kranthi K. Mandadapu</a>",
          "description": "A central object in the computational studies of rare events is the committor\nfunction. Though costly to compute, the committor function encodes complete\nmechanistic information of the processes involving rare events, including\nreaction rates and transition-state ensembles. Under the framework of\ntransition path theory (TPT), recent work [1] proposes an algorithm where a\nfeedback loop couples a neural network that models the committor function with\nimportance sampling, mainly umbrella sampling, which collects data needed for\nadaptive training. In this work, we show additional modifications are needed to\nimprove the accuracy of the algorithm. The first modification adds elements of\nsupervised learning, which allows the neural network to improve its prediction\nby fitting to sample-mean estimates of committor values obtained from short\nmolecular dynamics trajectories. The second modification replaces the\ncommittor-based umbrella sampling with the finite-temperature string (FTS)\nmethod, which enables homogeneous sampling in regions where transition pathways\nare located. We test our modifications on low-dimensional systems with\nnon-convex potential energy where reference solutions can be found via\nanalytical or the finite element methods, and show how combining supervised\nlearning and the FTS method yields accurate computation of committor functions\nand reaction rates. We also provide an error analysis for algorithms that use\nthe FTS method, using which reaction rates can be accurately estimated during\ntraining with a small number of samples.",
          "link": "http://arxiv.org/abs/2107.13522",
          "publishedOn": "2021-07-29T02:00:10.028Z",
          "wordCount": 682,
          "title": "Supervised Learning and the Finite-Temperature String Method for Computing Committor Functions and Reaction Rates. (arXiv:2107.13522v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "EEG-based emotion recognition often requires sufficient labeled training\nsamples to build an effective computational model. Labeling EEG data, on the\nother hand, is often expensive and time-consuming. To tackle this problem and\nreduce the need for output labels in the context of EEG-based emotion\nrecognition, we propose a semi-supervised pipeline to jointly exploit both\nunlabeled and labeled data for learning EEG representations. Our\nsemi-supervised framework consists of both unsupervised and supervised\ncomponents. The unsupervised part maximizes the consistency between original\nand reconstructed input data using an autoencoder, while simultaneously the\nsupervised part minimizes the cross-entropy between the input and output\nlabels. We evaluate our framework using both a stacked autoencoder and an\nattention-based recurrent autoencoder. We test our framework on the large-scale\nSEED EEG dataset and compare our results with several other popular\nsemi-supervised methods. Our semi-supervised framework with a deep\nattention-based recurrent autoencoder consistently outperforms the benchmark\nmethods, even when small sub-sets (3\\%, 5\\% and 10\\%) of the output labels are\navailable during training, achieving a new state-of-the-art semi-supervised\nperformance.",
          "link": "http://arxiv.org/abs/2107.13505",
          "publishedOn": "2021-07-29T02:00:10.020Z",
          "wordCount": 618,
          "title": "Deep Recurrent Semi-Supervised EEG Representation Learning for Emotion Recognition. (arXiv:2107.13505v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helou_B/0/1/0/all/0/1\">Bassam Helou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusi_A/0/1/0/all/0/1\">Aditya Dusi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collin_A/0/1/0/all/0/1\">Anne Collin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdipour_N/0/1/0/all/0/1\">Noushin Mehdipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lizarazo_C/0/1/0/all/0/1\">Cristhian Lizarazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belta_C/0/1/0/all/0/1\">Calin Belta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wongpiromsarn_T/0/1/0/all/0/1\">Tichakorn Wongpiromsarn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tebbens_R/0/1/0/all/0/1\">Radboud Duintjer Tebbens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>",
          "description": "Autonomous vehicles must balance a complex set of objectives. There is no\nconsensus on how they should do so, nor on a model for specifying a desired\ndriving behavior. We created a dataset to help address some of these questions\nin a limited operating domain. The data consists of 92 traffic scenarios, with\nmultiple ways of traversing each scenario. Multiple annotators expressed their\npreference between pairs of scenario traversals. We used the data to compare an\ninstance of a rulebook, carefully hand-crafted independently of the dataset,\nwith several interpretable machine learning models such as Bayesian networks,\ndecision trees, and logistic regression trained on the dataset. To compare\ndriving behavior, these models use scores indicating by how much different\nscenario traversals violate each of 14 driving rules. The rules are\ninterpretable and designed by subject-matter experts. First, we found that\nthese rules were enough for these models to achieve a high classification\naccuracy on the dataset. Second, we found that the rulebook provides high\ninterpretability without excessively sacrificing performance. Third, the data\npointed to possible improvements in the rulebook and the rules, and to\npotential new rules. Fourth, we explored the interpretability vs performance\ntrade-off by also training non-interpretable models such as a random forest.\nFinally, we make the dataset publicly available to encourage a discussion from\nthe wider community on behavior specification for AVs. Please find it at\ngithub.com/bassam-motional/Reasonable-Crowd.",
          "link": "http://arxiv.org/abs/2107.13507",
          "publishedOn": "2021-07-29T02:00:09.974Z",
          "wordCount": 693,
          "title": "The Reasonable Crowd: Towards evidence-based and interpretable models of driving behavior. (arXiv:2107.13507v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1901.09997",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Berahas_A/0/1/0/all/0/1\">Albert S. Berahas</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jahani_M/0/1/0/all/0/1\">Majid Jahani</a>, <a href=\"http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Takac_M/0/1/0/all/0/1\">Martin Tak&#xe1;&#x10d;</a>",
          "description": "We present two sampled quasi-Newton methods (sampled LBFGS and sampled LSR1)\nfor solving empirical risk minimization problems that arise in machine\nlearning. Contrary to the classical variants of these methods that sequentially\nbuild Hessian or inverse Hessian approximations as the optimization progresses,\nour proposed methods sample points randomly around the current iterate at every\niteration to produce these approximations. As a result, the approximations\nconstructed make use of more reliable (recent and local) information, and do\nnot depend on past iterate information that could be significantly stale. Our\nproposed algorithms are efficient in terms of accessed data points (epochs) and\nhave enough concurrency to take advantage of parallel/distributed computing\nenvironments. We provide convergence guarantees for our proposed methods.\nNumerical tests on a toy classification problem as well as on popular\nbenchmarking binary classification and neural network training tasks reveal\nthat the methods outperform their classical variants.",
          "link": "http://arxiv.org/abs/1901.09997",
          "publishedOn": "2021-07-29T02:00:09.967Z",
          "wordCount": 641,
          "title": "Quasi-Newton Methods for Machine Learning: Forget the Past, Just Sample. (arXiv:1901.09997v5 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Charles Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orbik_J/0/1/0/all/0/1\">J&#x119;drzej Orbik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devin_C/0/1/0/all/0/1\">Coline Devin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Brian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1\">Glen Berseth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "In this paper, we study how robots can autonomously learn skills that require\na combination of navigation and grasping. Learning robotic skills in the real\nworld remains challenging without large-scale data collection and supervision.\nOur aim is to devise a robotic reinforcement learning system for learning\nnavigation and manipulation together, in an \\textit{autonomous} way without\nhuman intervention, enabling continual learning under realistic assumptions.\nSpecifically, our system, ReLMM, can learn continuously on a real-world\nplatform without any environment instrumentation, without human intervention,\nand without access to privileged information, such as maps, objects positions,\nor a global view of the environment. Our method employs a modularized policy\nwith components for manipulation and navigation, where uncertainty over the\nmanipulation success drives exploration for the navigation controller, and the\nmanipulation module provides rewards for navigation. We evaluate our method on\na room cleanup task, where the robot must navigate to and pick up items of\nscattered on the floor. After a grasp curriculum training phase, ReLMM can\nlearn navigation and grasping together fully automatically, in around 40 hours\nof real-world training.",
          "link": "http://arxiv.org/abs/2107.13545",
          "publishedOn": "2021-07-29T02:00:09.959Z",
          "wordCount": 626,
          "title": "ReLMM: Practical RL for Learning Mobile Manipulation Skills Using Only Onboard Sensors. (arXiv:2107.13545v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13473",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Valenchon_N/0/1/0/all/0/1\">Nicolas Valenchon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bouteiller_Y/0/1/0/all/0/1\">Yann Bouteiller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jourde_H/0/1/0/all/0/1\">Hugo R. Jourde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coffey_E/0/1/0/all/0/1\">Emily B.J. Coffey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beltrame_G/0/1/0/all/0/1\">Giovanni Beltrame</a>",
          "description": "Electroencephalography (EEG) is a method of measuring the brain's electrical\nactivity, using non-invasive scalp electrodes. In this article, we propose the\nPortiloop, a deep learning-based portable and low-cost device enabling the\nneuroscience community to capture EEG, process it in real time, detect patterns\nof interest, and respond with precisely-timed stimulation. The core of the\nPortiloop is a System on Chip composed of an Analog to Digital Converter (ADC)\nand a Field-Programmable Gate Array (FPGA). After being converted to digital by\nthe ADC, the EEG signal is processed in the FPGA. The FPGA contains an ad-hoc\nArtificial Neural Network (ANN) with convolutional and recurrent units,\ndirectly implemented in hardware. The output of the ANN is then used to trigger\nthe user-defined feedback. We use the Portiloop to develop a real-time sleep\nspindle stimulating application, as a case study. Sleep spindles are a specific\ntype of transient oscillation ($\\sim$2.5 s, 12-16 Hz) that are observed in EEG\nrecordings, and are related to memory consolidation during sleep. We tested the\nPortiloop's capacity to detect and stimulate sleep spindles in real time using\nan existing database of EEG sleep recordings. With 71% for both precision and\nrecall as compared with expert labels, the system is able to stimulate spindles\nwithin $\\sim$300 ms of their onset, enabling experimental manipulation of early\nthe entire spindle. The Portiloop can be extended to detect and stimulate other\nneural events in EEG. It is fully available to the research community as an\nopen science project.",
          "link": "http://arxiv.org/abs/2107.13473",
          "publishedOn": "2021-07-29T02:00:09.840Z",
          "wordCount": 721,
          "title": "The Portiloop: a deep learning-based open science tool for closed-loop brain stimulation. (arXiv:2107.13473v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/1811.11891",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koelle_S/0/1/0/all/0/1\">Samson Koelle</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1\">Hanyu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meila</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chia Chen</a>",
          "description": "Manifold embedding algorithms map high-dimensional data down to coordinates\nin a much lower-dimensional space. One of the aims of dimension reduction is to\nfind intrinsic coordinates that describe the data manifold. The coordinates\nreturned by the embedding algorithm are abstract, and finding their physical or\ndomain-related meaning is not formalized and often left to domain experts. This\npaper studies the problem of recovering the meaning of the new low-dimensional\nrepresentation in an automatic, principled fashion. We propose a method to\nexplain embedding coordinates of a manifold as non-linear compositions of\nfunctions from a user-defined dictionary. We show that this problem can be set\nup as a sparse linear Group Lasso recovery problem, find sufficient recovery\nconditions, and demonstrate its effectiveness on data.",
          "link": "http://arxiv.org/abs/1811.11891",
          "publishedOn": "2021-07-29T02:00:09.832Z",
          "wordCount": 567,
          "title": "Manifold Coordinates with Physical Meaning. (arXiv:1811.11891v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Ying Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jong-Shi Pang</a>",
          "description": "The non-negative matrix factorization (NMF) model with an additional\northogonality constraint on one of the factor matrices, called the orthogonal\nNMF (ONMF), has been found a promising clustering model and can outperform the\nclassical K-means. However, solving the ONMF model is a challenging\noptimization problem because the coupling of the orthogonality and\nnon-negativity constraints introduces a mixed combinatorial aspect into the\nproblem due to the determination of the correct status of the variables\n(positive or zero). Most of the existing methods directly deal with the\northogonality constraint in its original form via various optimization\ntechniques, but are not scalable for large-scale problems. In this paper, we\npropose a new ONMF based clustering formulation that equivalently transforms\nthe orthogonality constraint into a set of norm-based non-convex equality\nconstraints. We then apply a non-convex penalty (NCP) approach to add them to\nthe objective as penalty terms, leading to a problem that is efficiently\nsolvable. One smooth penalty formulation and one non-smooth penalty formulation\nare respectively studied. We build theoretical conditions for the penalized\nproblems to provide feasible stationary solutions to the ONMF based clustering\nproblem, as well as proposing efficient algorithms for solving the penalized\nproblems of the two NCP methods. Experimental results based on both synthetic\nand real datasets are presented to show that the proposed NCP methods are\ncomputationally time efficient, and either match or outperform the existing\nK-means and ONMF based methods in terms of the clustering performance.",
          "link": "http://arxiv.org/abs/1906.00570",
          "publishedOn": "2021-07-29T02:00:09.825Z",
          "wordCount": 732,
          "title": "Clustering by Orthogonal NMF Model and Non-Convex Penalty Optimization. (arXiv:1906.00570v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polgreen_E/0/1/0/all/0/1\">Elizabeth Polgreen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_A/0/1/0/all/0/1\">Andrew Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1\">Sanjit A. Seshia</a>",
          "description": "In classic program synthesis algorithms, such as counterexample-guided\ninductive synthesis (CEGIS), the algorithms alternate between a synthesis phase\nand an oracle (verification) phase. Many synthesis algorithms use a white-box\noracle based on satisfiability modulo theory (SMT) solvers to provide\ncounterexamples. But what if a white-box oracle is either not available or not\neasy to work with? We present a framework for solving a general class of\noracle-guided synthesis problems which we term synthesis modulo oracles. In\nthis setting, oracles may be black boxes with a query-response interface\ndefined by the synthesis problem. As a necessary component of this framework,\nwe also formalize the problem of satisfiability modulo theories and oracles,\nand present an algorithm for solving this problem. We implement a prototype\nsolver for satisfiability and synthesis modulo oracles and demonstrate that, by\nusing oracles that execute functions not easily modeled in SMT-constraints,\nsuch as recursive functions or oracles that incorporate compilation and\nexecution of code, SMTO and SyMO are able to solve problems beyond the\nabilities of standard SMT and synthesis solvers.",
          "link": "http://arxiv.org/abs/2107.13477",
          "publishedOn": "2021-07-29T02:00:09.813Z",
          "wordCount": 613,
          "title": "Satisfiability and Synthesis Modulo Oracles. (arXiv:2107.13477v1 [cs.LO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kummer_L/0/1/0/all/0/1\">Lorenz Kummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidak_K/0/1/0/all/0/1\">Kevin Sidak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichmann_T/0/1/0/all/0/1\">Tabea Reichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gansterer_W/0/1/0/all/0/1\">Wilfried Gansterer</a>",
          "description": "Quantization is a technique for reducing deep neural networks (DNNs) training\nand inference times, which is crucial for training in resource constrained\nenvironments or time critical inference applications. State-of-the-art (SOTA)\nquantization approaches focus on post-training quantization, i.e. quantization\nof pre-trained DNNs for speeding up inference. Very little work on quantized\ntraining exists, which neither al-lows dynamic intra-epoch precision switches\nnor em-ploys an information theory based switching heuristic. Usually, existing\napproaches require full precision refinement afterwards and enforce a global\nword length across the whole DNN. This leads to suboptimal quantization\nmappings and resource usage. Recognizing these limits, we introduce MARViN, a\nnew quantized training strategy using information theory-based intra-epoch\nprecision switching, which decides on a per-layer basis which precision should\nbe used in order to minimize quantization-induced information loss. Note that\nany quantization must leave enough precision such that future learning steps do\nnot suffer from vanishing gradients. We achieve an average speedup of 1.86\ncompared to a float32 basis while limiting mean accuracy degradation on\nAlexNet/ResNet to only -0.075%.",
          "link": "http://arxiv.org/abs/2107.13490",
          "publishedOn": "2021-07-29T02:00:09.804Z",
          "wordCount": 612,
          "title": "MARViN -- Multiple Arithmetic Resolutions Vacillating in Neural Networks. (arXiv:2107.13490v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1\">Alex Rogozhnikov</a>",
          "description": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2106.03143",
          "publishedOn": "2021-07-29T02:00:09.699Z",
          "wordCount": 607,
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1\">J. K. Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1\">Mario Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwis_K/0/1/0/all/0/1\">Kusal De Alwis</a>",
          "description": "The general approach taken when training deep learning classifiers is to save\nthe parameters after every few iterations, train until either a human observer\nor a simple metric-based heuristic decides the network isn't learning anymore,\nand then backtrack and pick the saved parameters with the best validation\naccuracy. Simple methods are used to determine if a neural network isn't\nlearning anymore because, as long as it's well after the optimal values are\nfound, the condition doesn't impact the final accuracy of the model. However\nfrom a runtime perspective, this is of great significance to the many cases\nwhere numerous neural networks are trained simultaneously (e.g. hyper-parameter\ntuning). Motivated by this, we introduce a statistical significance test to\ndetermine if a neural network has stopped learning. This stopping criterion\nappears to represent a happy medium compared to other popular stopping\ncriterions, achieving comparable accuracy to the criterions that achieve the\nhighest final accuracies in 77% or fewer epochs, while the criterions which\nstop sooner do so with an appreciable loss to final accuracy. Additionally, we\nuse this as the basis of a new learning rate scheduler, removing the need to\nmanually choose learning rate schedules and acting as a quasi-line search,\nachieving superior or comparable empirical performance to existing methods.",
          "link": "http://arxiv.org/abs/2103.01205",
          "publishedOn": "2021-07-29T02:00:09.681Z",
          "wordCount": 691,
          "title": "Statistically Significant Stopping of Neural Network Training. (arXiv:2103.01205v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Timothy Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozier_J/0/1/0/all/0/1\">Jamell Dozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1\">Helen Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1\">Nishchal Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fr&#xe9;do Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Object recognition and viewpoint estimation lie at the heart of visual\nunderstanding. Recent works suggest that convolutional neural networks (CNNs)\nfail to generalize to out-of-distribution (OOD) category-viewpoint\ncombinations, ie. combinations not seen during training. In this paper, we\ninvestigate when and how such OOD generalization may be possible by evaluating\nCNNs trained to classify both object category and 3D viewpoint on OOD\ncombinations, and identifying the neural mechanisms that facilitate such OOD\ngeneralization. We show that increasing the number of in-distribution\ncombinations (ie. data diversity) substantially improves generalization to OOD\ncombinations, even with the same amount of training data. We compare learning\ncategory and viewpoint in separate and shared network architectures, and\nobserve starkly different trends on in-distribution and OOD combinations, ie.\nwhile shared networks are helpful in-distribution, separate networks\nsignificantly outperform shared ones at OOD combinations. Finally, we\ndemonstrate that such OOD generalization is facilitated by the neural mechanism\nof specialization, ie. the emergence of two types of neurons -- neurons\nselective to category and invariant to viewpoint, and vice versa.",
          "link": "http://arxiv.org/abs/2007.08032",
          "publishedOn": "2021-07-29T02:00:09.674Z",
          "wordCount": 654,
          "title": "When and how do CNNs generalize to out-of-distribution category-viewpoint combinations?. (arXiv:2007.08032v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wenxuan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haiping Huang</a>",
          "description": "The geometric structure of an optimization landscape is argued to be\nfundamentally important to support the success of deep neural network learning.\nA direct computation of the landscape beyond two layers is hard. Therefore, to\ncapture the global view of the landscape, an interpretable model of the\nnetwork-parameter (or weight) space must be established. However, the model is\nlacking so far. Furthermore, it remains unknown what the landscape looks like\nfor deep networks of binary synapses, which plays a key role in robust and\nenergy efficient neuromorphic computation. Here, we propose a statistical\nmechanics framework by directly building a least structured model of the\nhigh-dimensional weight space, considering realistic structured data,\nstochastic gradient descent training, and the computational depth of neural\nnetworks. We also consider whether the number of network parameters outnumbers\nthe number of supplied training data, namely, over- or under-parametrization.\nOur least structured model reveals that the weight spaces of the\nunder-parametrization and over-parameterization cases belong to the same class,\nin the sense that these weight spaces are well-connected without any\nhierarchical clustering structure. In contrast, the shallow-network has a\nbroken weight space, characterized by a discontinuous phase transition, thereby\nclarifying the benefit of depth in deep learning from the angle of high\ndimensional geometry. Our effective model also reveals that inside a deep\nnetwork, there exists a liquid-like central part of the architecture in the\nsense that the weights in this part behave as randomly as possible, providing\nalgorithmic implications. Our data-driven model thus provides a statistical\nmechanics insight about why deep learning is unreasonably effective in terms of\nthe high-dimensional weight space, and how deep networks are different from\nshallow ones.",
          "link": "http://arxiv.org/abs/2007.08093",
          "publishedOn": "2021-07-29T02:00:09.666Z",
          "wordCount": 754,
          "title": "Data-driven effective model shows a liquid-like deep learning. (arXiv:2007.08093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "In recent years, the growth of Machine Learning (ML) algorithms has raised\nthe number of studies including their applicability in a variety of different\nscenarios. Among all, one of the hardest ones is the aerospace, due to its\npeculiar physical requirements. In this context, a feasibility study and a\nfirst prototype for an Artificial Intelligence (AI) model to be deployed on\nboard satellites are presented in this work. As a case study, the detection of\nvolcanic eruptions has been investigated as a method to swiftly produce alerts\nand allow immediate interventions. Two Convolutional Neural Networks (CNNs)\nhave been proposed and designed, showing how to efficiently implement them for\nidentifying the eruptions and at the same time adapting their complexity in\norder to fit on board requirements.",
          "link": "http://arxiv.org/abs/2106.15281",
          "publishedOn": "2021-07-29T02:00:09.647Z",
          "wordCount": 602,
          "title": "On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1808.09670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fouillen_E/0/1/0/all/0/1\">Erwan Fouillen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyer_C/0/1/0/all/0/1\">Claire Boyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangnier_M/0/1/0/all/0/1\">Maxime Sangnier</a>",
          "description": "Gradient boosting is a prediction method that iteratively combines weak\nlearners to produce a complex and accurate model. From an optimization point of\nview, the learning procedure of gradient boosting mimics a gradient descent on\na functional variable. This paper proposes to build upon the proximal point\nalgorithm, when the empirical risk to minimize is not differentiable, in order\nto introduce a novel boosting approach, called proximal boosting. Besides being\nmotivated by non-differentiable optimization, the proposed algorithm benefits\nfrom algorithmic improvements such as controlling the approximation error and\nNesterov's acceleration, in the same way as gradient boosting [Grubb and\nBagnell, 2011, Biau et al., 2018]. This leads to two variants, respectively\ncalled residual proximal boosting and accelerated proximal boosting.\nTheoretical convergence is proved for the first two procedures under different\nhypotheses on the empirical risk and advantages of leveraging proximal methods\nfor boosting are illustrated by numerical experiments on simulated and\nreal-world data. In particular, we exhibit a favorable comparison over gradient\nboosting regarding convergence rate and prediction accuracy.",
          "link": "http://arxiv.org/abs/1808.09670",
          "publishedOn": "2021-07-29T02:00:09.640Z",
          "wordCount": 631,
          "title": "Proximal boosting and variants. (arXiv:1808.09670v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammoudeh_A/0/1/0/all/0/1\">Ahmad Hammoudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedmori_S/0/1/0/all/0/1\">Sara Tedmori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obeid_N/0/1/0/all/0/1\">Nadim Obeid</a>",
          "description": "Although learning from data is effective and has achieved significant\nmilestones, it has many challenges and limitations. Learning from data starts\nfrom observations and then proceeds to broader generalizations. This framework\nis controversial in science, yet it has achieved remarkable engineering\nsuccesses. This paper reflects on some epistemological issues and some of the\nlimitations of the knowledge discovered in data. The document discusses the\ncommon perception that getting more data is the key to achieving better machine\nlearning models from theoretical and practical perspectives. The paper sheds\nsome light on the shortcomings of using generic mathematical theories to\ndescribe the process. It further highlights the need for theories specialized\nin learning from data. While more data leverages the performance of machine\nlearning models in general, the relation in practice is shown to be logarithmic\nat its best; After a specific limit, more data stabilize or degrade the machine\nlearning models. Recent work in reinforcement learning showed that the trend is\nshifting away from data-oriented approaches and relying more on algorithms. The\npaper concludes that learning from data is hindered by many limitations. Hence\nan approach that has an intensional orientation is needed.",
          "link": "http://arxiv.org/abs/2107.13270",
          "publishedOn": "2021-07-29T02:00:09.633Z",
          "wordCount": 626,
          "title": "A Reflection on Learning from Data: Epistemology Issues and Limitations. (arXiv:2107.13270v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1\">Sam Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_R/0/1/0/all/0/1\">Raluca Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1\">Ida Momennejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rzepecki_J/0/1/0/all/0/1\">Jaroslaw Rzepecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuniga_E/0/1/0/all/0/1\">Evelyn Zuniga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costello_G/0/1/0/all/0/1\">Gavin Costello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_G/0/1/0/all/0/1\">Guy Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_A/0/1/0/all/0/1\">Ali Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>",
          "description": "A key challenge on the path to developing agents that learn complex\nhuman-like behavior is the need to quickly and accurately quantify\nhuman-likeness. While human assessments of such behavior can be highly\naccurate, speed and scalability are limited. We address these limitations\nthrough a novel automated Navigation Turing Test (ANTT) that learns to predict\nhuman judgments of human-likeness. We demonstrate the effectiveness of our\nautomated NTT on a navigation task in a complex 3D environment. We investigate\nsix classification models to shed light on the types of architectures best\nsuited to this task, and validate them against data collected through a human\nNTT. Our best models achieve high accuracy when distinguishing true human and\nagent behavior. At the same time, we show that predicting finer-grained human\nassessment of agents' progress towards human-like behavior remains unsolved.\nOur work takes an important step towards agents that more effectively learn\ncomplex human-like behavior.",
          "link": "http://arxiv.org/abs/2105.09637",
          "publishedOn": "2021-07-29T02:00:09.626Z",
          "wordCount": 660,
          "title": "Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation. (arXiv:2105.09637v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Farwa K. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanagan_A/0/1/0/all/0/1\">Adrian Flanagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kuan E. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamgir_Z/0/1/0/all/0/1\">Zareen Alamgir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammad_Ud_Din_M/0/1/0/all/0/1\">Muhammad Ammad-Ud-Din</a>",
          "description": "We introduce the payload optimization method for federated recommender\nsystems (FRS). In federated learning (FL), the global model payload that is\nmoved between the server and users depends on the number of items to recommend.\nThe model payload grows when there is an increasing number of items. This\nbecomes challenging for an FRS if it is running in production mode. To tackle\nthe payload challenge, we formulated a multi-arm bandit solution that selected\npart of the global model and transmitted it to all users. The selection process\nwas guided by a novel reward function suitable for FL systems. So far as we are\naware, this is the first optimization method that seeks to address item\ndependent payloads. The method was evaluated using three benchmark\nrecommendation datasets. The empirical validation confirmed that the proposed\nmethod outperforms the simpler methods that do not benefit from the bandits for\nthe purpose of item selection. In addition, we have demonstrated the usefulness\nof our proposed method by rigorously evaluating the effects of a payload\nreduction on the recommendation performance degradation. Our method achieved up\nto a 90\\% reduction in model payload, yielding only a $\\sim$4\\% - 8\\% loss in\nthe recommendation performance for highly sparse datasets",
          "link": "http://arxiv.org/abs/2107.13078",
          "publishedOn": "2021-07-29T02:00:09.606Z",
          "wordCount": 674,
          "title": "A Payload Optimization Method for Federated Recommender Systems. (arXiv:2107.13078v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feeney_P/0/1/0/all/0/1\">Patrick Feeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "The pixelwise reconstruction error of deep autoencoders is often utilized for\nimage novelty detection and localization under the assumption that pixels with\nhigh error indicate which parts of the input image are unfamiliar and therefore\nlikely to be novel. This assumed correlation between pixels with high\nreconstruction error and novel regions of input images has not been verified\nand may limit the accuracy of these methods. In this paper we utilize saliency\nmaps to evaluate whether this correlation exists. Saliency maps reveal directly\nhow much a change in each input pixel would affect reconstruction loss, while\neach pixel's reconstruction error may be attributed to many input pixels when\nlayers are fully connected. We compare saliency maps to reconstruction error\nmaps via qualitative visualizations as well as quantitative correspondence\nbetween the top K elements of the maps for both novel and normal images. Our\nresults indicate that reconstruction error maps do not closely correlate with\nthe importance of pixels in the input images, making them insufficient for\nnovelty localization.",
          "link": "http://arxiv.org/abs/2107.13379",
          "publishedOn": "2021-07-29T02:00:09.599Z",
          "wordCount": 606,
          "title": "Evaluating the Use of Reconstruction Error for Novelty Localization. (arXiv:2107.13379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Liangxiu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleerekoper_A/0/1/0/all/0/1\">Anthony Kleerekoper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tongle Hu</a>",
          "description": "Late blight disease is one of the most destructive diseases in potato crop,\nleading to serious yield losses globally. Accurate diagnosis of the disease at\nearly stage is critical for precision disease control and management. Current\nfarm practices in crop disease diagnosis are based on manual visual inspection,\nwhich is costly, time consuming, subject to individual bias. Recent advances in\nimaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote\nsensing and machine learning offer the opportunity to address this challenge.\nParticularly, hyperspectral imagery (HSI) combining with machine learning/deep\nlearning approaches is preferable for accurately identifying specific plant\ndiseases because the HSI consists of a wide range of high-quality reflectance\ninformation beyond human vision, capable of capturing both spectral-spatial\ninformation. The proposed method considers the potential disease specific\nreflectance radiation variance caused by the canopy structural diversity,\nintroduces the multiple capsule layers to model the hierarchical structure of\nthe spectral-spatial disease attributes with the encapsulated features to\nrepresent the various classes and the rotation invariance of the disease\nattributes in the feature space. We have evaluated the proposed method with the\nreal UAV-based HSI data under the controlled field conditions. The\neffectiveness of the hierarchical features has been quantitatively assessed and\ncompared with the existing representative machine learning/deep learning\nmethods. The experiment results show that the proposed model significantly\nimproves the accuracy performance when considering hierarchical-structure of\nspectral-spatial features, comparing to the existing methods only using\nspectral, or spatial or spectral-spatial features without consider\nhierarchical-structure of spectral-spatial features.",
          "link": "http://arxiv.org/abs/2107.13277",
          "publishedOn": "2021-07-29T02:00:09.592Z",
          "wordCount": 707,
          "title": "A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery. (arXiv:2107.13277v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yihong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1\">Shunmei Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_X/0/1/0/all/0/1\">Xiaoxiao Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chao Yan</a>",
          "description": "Edge computing enabled smart greenhouse is a representative application of\nInternet of Things technology, which can monitor the environmental information\nin real time and employ the information to contribute to intelligent\ndecision-making. In the process, anomaly detection for wireless sensor data\nplays an important role. However, traditional anomaly detection algorithms\noriginally designed for anomaly detection in static data have not properly\nconsidered the inherent characteristics of data stream produced by wireless\nsensor such as infiniteness, correlations and concept drift, which may pose a\nconsiderable challenge on anomaly detection based on data stream, and lead to\nlow detection accuracy and efficiency. First, data stream usually generates\nquickly which means that it is infinite and enormous, so any traditional\noff-line anomaly detection algorithm that attempts to store the whole dataset\nor to scan the dataset multiple times for anomaly detection will run out of\nmemory space. Second, there exist correlations among different data streams,\nwhich traditional algorithms hardly consider. Third, the underlying data\ngeneration process or data distribution may change over time. Thus, traditional\nanomaly detection algorithms with no model update will lose their effects.\nConsidering these issues, a novel method (called DLSHiForest) on basis of\nLocality-Sensitive Hashing and time window technique in this paper is proposed\nto solve these problems while achieving accurate and efficient detection.\nComprehensive experiments are executed using real-world agricultural greenhouse\ndataset to demonstrate the feasibility of our approach. Experimental results\nshow that our proposal is practicable in addressing challenges of traditional\nanomaly detection while ensuring accuracy and efficiency.",
          "link": "http://arxiv.org/abs/2107.13353",
          "publishedOn": "2021-07-29T02:00:09.585Z",
          "wordCount": 715,
          "title": "Fast Wireless Sensor Anomaly Detection based on Data Stream in Edge Computing Enabled Smart Greenhouse. (arXiv:2107.13353v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merlo_E/0/1/0/all/0/1\">Ettore Merlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marhaba_M/0/1/0/all/0/1\">Mira Marhaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braiek_H/0/1/0/all/0/1\">Houssem Ben Braiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1\">Giuliano Antoniol</a>",
          "description": "Neural network test cases are meant to exercise different reasoning paths in\nan architecture and used to validate the prediction outcomes. In this paper, we\nintroduce \"computational profiles\" as vectors of neuron activation levels. We\ninvestigate the distribution of computational profile likelihood of metamorphic\ntest cases with respect to the likelihood distributions of training, test and\nerror control cases. We estimate the non-parametric probability densities of\nneuron activation levels for each distinct output class. Probabilities are\ninferred using training cases only, without any additional knowledge about\nmetamorphic test cases. Experiments are performed by training a network on the\nMNIST Fashion library of images and comparing prediction likelihoods with those\nobtained from error control-data and from metamorphic test cases. Experimental\nresults show that the distributions of computational profile likelihood for\ntraining and test cases are somehow similar, while the distribution of the\nrandom-noise control-data is always remarkably lower than the observed one for\nthe training and testing sets. In contrast, metamorphic test cases show a\nprediction likelihood that lies in an extended range with respect to training,\ntests, and random noise. Moreover, the presented approach allows the\nindependent assessment of different training classes and experiments to show\nthat some of the classes are more sensitive to misclassifying metamorphic test\ncases than other classes. In conclusion, metamorphic test cases represent very\naggressive tests for neural network architectures. Furthermore, since\nmetamorphic test cases force a network to misclassify those inputs whose\nlikelihood is similar to that of training cases, they could also be considered\nas adversarial attacks that evade defenses based on computational profile\nlikelihood evaluation.",
          "link": "http://arxiv.org/abs/2107.13491",
          "publishedOn": "2021-07-29T02:00:09.561Z",
          "wordCount": 717,
          "title": "Models of Computational Profiles to Study the Likelihood of DNN Metamorphic Test Cases. (arXiv:2107.13491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13530",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kessler_S/0/1/0/all/0/1\">Samuel Kessler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_B/0/1/0/all/0/1\">Bethan Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karout_S/0/1/0/all/0/1\">Salah Karout</a>",
          "description": "We present a method for continual learning of speech representations for\nmultiple languages using self-supervised learning (SSL) and applying these for\nautomatic speech recognition. There is an abundance of unannotated speech, so\ncreating self-supervised representations from raw audio and finetuning on a\nsmall annotated datasets is a promising direction to build speech recognition\nsystems. Wav2vec models perform SSL on raw audio in a pretraining phase and\nthen finetune on a small fraction of annotated data. SSL models have produced\nstate of the art results for ASR. However, these models are very expensive to\npretrain with self-supervision. We tackle the problem of learning new language\nrepresentations continually from audio without forgetting a previous language\nrepresentation. We use ideas from continual learning to transfer knowledge from\na previous task to speed up pretraining a new language task. Our\ncontinual-wav2vec2 model can decrease pretraining times by 32% when learning a\nnew language task, and learn this new audio-language representation without\nforgetting previous language representation.",
          "link": "http://arxiv.org/abs/2107.13530",
          "publishedOn": "2021-07-29T02:00:09.543Z",
          "wordCount": 634,
          "title": "Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition. (arXiv:2107.13530v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1\">Daniel Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stapleton_L/0/1/0/all/0/1\">Logan Stapleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1\">Vasilis Syrgkanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Steven Wu</a>",
          "description": "Randomized experiments can be susceptible to selection bias due to potential\nnon-compliance by the participants. While much of the existing work has studied\ncompliance as a static behavior, we propose a game-theoretic model to study\ncompliance as dynamic behavior that may change over time. In rounds, a social\nplanner interacts with a sequence of heterogeneous agents who arrive with their\nunobserved private type that determines both their prior preferences across the\nactions (e.g., control and treatment) and their baseline rewards without taking\nany treatment. The planner provides each agent with a randomized recommendation\nthat may alter their beliefs and their action selection. We develop a novel\nrecommendation mechanism that views the planner's recommendation as a form of\ninstrumental variable (IV) that only affects an agents' action selection, but\nnot the observed rewards. We construct such IVs by carefully mapping the\nhistory -- the interactions between the planner and the previous agents -- to a\nrandom recommendation. Even though the initial agents may be completely\nnon-compliant, our mechanism can incentivize compliance over time, thereby\nenabling the estimation of the treatment effect of each treatment, and\nminimizing the cumulative regret of the planner whose goal is to identify the\noptimal treatment.",
          "link": "http://arxiv.org/abs/2107.10093",
          "publishedOn": "2021-07-29T02:00:09.515Z",
          "wordCount": 671,
          "title": "Incentivizing Compliance with Algorithmic Instruments. (arXiv:2107.10093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lingam_V/0/1/0/all/0/1\">Vijay Lingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragesh_R/0/1/0/all/0/1\">Rahul Ragesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Arun Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellamanickam_S/0/1/0/all/0/1\">Sundararajan Sellamanickam</a>",
          "description": "Graph Neural Networks (GNNs) exhibit excellent performance when graphs have\nstrong homophily property, i.e. connected nodes have the same labels. However,\nthey perform poorly on heterophilic graphs. Several approaches address the\nissue of heterophily by proposing models that adapt the graph by optimizing\ntask-specific loss function using labelled data. These adaptations are made\neither via attention or by attenuating or enhancing various\nlow-frequency/high-frequency signals, as needed for the task at hand. More\nrecent approaches adapt the eigenvalues of the graph. One important\ninterpretation of this adaptation is that these models select/weigh the\neigenvectors of the graph. Based on this interpretation, we present an\neigendecomposition based approach and propose EigenNetwork models that improve\nthe performance of GNNs on heterophilic graphs. Performance improvement is\nachieved by learning flexible graph adaptation functions that modulate the\neigenvalues of the graph. Regularization of these functions via parameter\nsharing helps to improve the performance even more. Our approach achieves up to\n11% improvement in performance over the state-of-the-art methods on\nheterophilic graphs.",
          "link": "http://arxiv.org/abs/2107.13312",
          "publishedOn": "2021-07-29T02:00:09.507Z",
          "wordCount": 610,
          "title": "Effective Eigendecomposition based Graph Adaptation for Heterophilic Networks. (arXiv:2107.13312v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13157",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Trivedi_A/0/1/0/all/0/1\">Anusua Trivedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desbiens_J/0/1/0/all/0/1\">Jocelyn Desbiens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gross_R/0/1/0/all/0/1\">Ron Gross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Purpose: To demonstrate that retinal microvasculature per se is a reliable\nbiomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular\ndiseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to\ncolor fundus images for semantic segmentation of the blood vessels and severity\nclassification on both vascular and full images. Vessel reconstruction through\nharmonic descriptors is also used as a smoothing and de-noising tool. The\nmathematical background of the theory is also outlined. Results: For diabetic\npatients, at least 93.8% of DR No-Refer vs. Refer classification can be related\nto vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening\ncase, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the\ndisease biomarkers are related topologically to the vasculature. Translational\nRelevance: Experiments conducted on eye blood vasculature reconstruction as a\nbiomarker shows a strong correlation between vasculature shape and later stages\nof DR.",
          "link": "http://arxiv.org/abs/2107.13157",
          "publishedOn": "2021-07-29T02:00:09.498Z",
          "wordCount": 603,
          "title": "Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases. (arXiv:2107.13157v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1\">Rindra Ramamonjison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xinyu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiaolong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "This paper presents a Simple and effective unsupervised adaptation method for\nRobust Object Detection (SimROD). To overcome the challenging issues of domain\nshift and pseudo-label noise, our method integrates a novel domain-centric\naugmentation method, a gradual self-labeling adaptation procedure, and a\nteacher-guided fine-tuning mechanism. Using our method, target domain samples\ncan be leveraged to adapt object detection models without changing the model\narchitecture or generating synthetic data. When applied to image corruptions\nand high-level cross-domain adaptation benchmarks, our method outperforms prior\nbaselines on multiple domain adaptation benchmarks. SimROD achieves new\nstate-of-the-art on standard real-to-synthetic and cross-camera setup\nbenchmarks. On the image corruption benchmark, models adapted with our method\nachieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%\nAP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method\noutperformed the best baseline performance by up to 8% AP50 on Comic dataset\nand up to 4% on Watercolor dataset.",
          "link": "http://arxiv.org/abs/2107.13389",
          "publishedOn": "2021-07-29T02:00:09.477Z",
          "wordCount": 612,
          "title": "SimROD: A Simple Adaptation Method for Robust Object Detection. (arXiv:2107.13389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13419",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Devi_T/0/1/0/all/0/1\">Thangjam Clarinda Devi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thaoroijam_K/0/1/0/all/0/1\">Kabita Thaoroijam</a>",
          "description": "This paper presents a vowel-based dialect identification system for\nMeeteilon. For this work, a vowel dataset is created by using Meeteilon Speech\nCorpora available at Linguistic Data Consortium for Indian Languages (LDC-IL).\nSpectral features such as formant frequencies (F1, F1 and F3) and prosodic\nfeatures such as pitch (F0), energy, intensity and segment duration values are\nextracted from monophthong vowel sounds. Random forest classifier, a decision\ntree-based ensemble algorithm is used for classification of three major\ndialects of Meeteilon namely, Imphal, Kakching and Sekmai. Model has shown an\naverage dialect identification performance in terms of accuracy of around\n61.57%. The role of spectral and prosodic features are found to be significant\nin Meeteilon dialect classification.",
          "link": "http://arxiv.org/abs/2107.13419",
          "publishedOn": "2021-07-29T02:00:09.466Z",
          "wordCount": 579,
          "title": "Vowel-based Meeteilon dialect identification using a Random Forest classifier. (arXiv:2107.13419v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patrick_Evans_J/0/1/0/all/0/1\">James Patrick-Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannehl_M/0/1/0/all/0/1\">Moritz Dannehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinder_J/0/1/0/all/0/1\">Johannes Kinder</a>",
          "description": "Reverse engineers would benefit from identifiers like function names, but\nthese are usually unavailable in binaries. Training a machine learning model to\npredict function names automatically is promising but fundamentally hard due to\nthe enormous number of classes. In this paper, we introduce eXtreme Function\nLabeling (XFL), an extreme multi-label learning approach to selecting\nappropriate labels for binary functions. XFL splits function names into tokens,\ntreating each as an informative label akin to the problem of tagging texts in\nnatural language. To capture the semantics of binary code, we introduce DEXTER,\na novel function embedding that combines static analysis-based features with\nlocal context from the call graph and global context from the entire binary. We\ndemonstrate that XFL outperforms state-of-the-art approaches to function\nlabeling on a dataset of over 10,000 binaries from the Debian project,\nachieving a precision of 82.5%. We also study combinations of XFL with\ndifferent published embeddings for binary functions and show that DEXTER\nconsistently improves over the state of the art in information gain. As a\nresult, we are able to show that binary function labeling is best phrased in\nterms of multi-label learning, and that binary function embeddings benefit from\nmoving beyond just learning from syntax.",
          "link": "http://arxiv.org/abs/2107.13404",
          "publishedOn": "2021-07-29T02:00:09.448Z",
          "wordCount": 631,
          "title": "XFL: eXtreme Function Labeling. (arXiv:2107.13404v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_G/0/1/0/all/0/1\">Gary G. Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_V/0/1/0/all/0/1\">Vincent S. Tseng</a>",
          "description": "Arrhythmia detection from ECG is an important research subject in the\nprevention and diagnosis of cardiovascular diseases. The prevailing studies\nformulate arrhythmia detection from ECG as a time series classification\nproblem. Meanwhile, early detection of arrhythmia presents a real-world demand\nfor early prevention and diagnosis. In this paper, we address a problem of\ncardiovascular disease early classification, which is a varied-length and\nlong-length time series early classification problem as well. For solving this\nproblem, we propose a deep reinforcement learning-based framework, namely\nSnippet Policy Network (SPN), consisting of four modules, snippet generator,\nbackbone network, controlling agent, and discriminator. Comparing to the\nexisting approaches, the proposed framework features flexible input length,\nsolves the dual-optimization solution of the earliness and accuracy goals.\nExperimental results demonstrate that SPN achieves an excellent performance of\nover 80\\% in terms of accuracy. Compared to the state-of-the-art methods, at\nleast 7% improvement on different metrics, including the precision, recall,\nF1-score, and harmonic mean, is delivered by the proposed SPN. To the best of\nour knowledge, this is the first work focusing on solving the cardiovascular\nearly classification problem based on varied-length ECG data. Based on these\nexcellent features from SPN, it offers a good exemplification for addressing\nall kinds of varied-length time series early classification problems.",
          "link": "http://arxiv.org/abs/2107.13361",
          "publishedOn": "2021-07-29T02:00:09.439Z",
          "wordCount": 646,
          "title": "Snippet Policy Network for Multi-class Varied-length ECG Early Classification. (arXiv:2107.13361v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13394",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Faruqui_S/0/1/0/all/0/1\">Syed Hasib Akhter Faruqui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Alaeddini_A/0/1/0/all/0/1\">Adel Alaeddini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fisher_Hoch_S/0/1/0/all/0/1\">Susan P Fisher-Hoch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mccormic_J/0/1/0/all/0/1\">Joseph B Mccormic</a>",
          "description": "The emergence and progression of multiple chronic conditions (MCC) over time\noften form a dynamic network that depends on patient's modifiable risk factors\nand their interaction with non-modifiable risk factors and existing conditions.\nContinuous time Bayesian networks (CTBNs) are effective methods for modeling\nthe complex network of MCC relationships over time. However, CTBNs are not able\nto effectively formulate the dynamic impact of patient's modifiable risk\nfactors on the emergence and progression of MCC. Considering a functional CTBN\n(FCTBN) to represent the underlying structure of the MCC relationships with\nrespect to individuals' risk factors and existing conditions, we propose a\nnonlinear state-space model based on Extended Kalman filter (EKF) to capture\nthe dynamics of the patients' modifiable risk factors and existing conditions\non the MCC evolution over time. We also develop a tensor control chart to\ndynamically monitor the effect of changes in the modifiable risk factors of\nindividual patients on the risk of new chronic conditions emergence. We\nvalidate the proposed approach based on a combination of simulation and real\ndata from a dataset of 385 patients from Cameron County Hispanic Cohort (CCHC)\nover multiple years. The dataset examines the emergence of 5 chronic conditions\n(Diabetes, Obesity, Cognitive Impairment, Hyperlipidemia, and Hypertension)\nbased on 4 modifiable risk factors representing lifestyle behaviors (Diet,\nExercise, Smoking Habit, and Drinking Habit) and 3 non-modifiable risk factors,\nincluding demographic information (Age, Gender, Education). The results\ndemonstrate the effectiveness of the proposed methodology for dynamic\nprediction and monitoring of the risk of MCC emergence in individual patients.",
          "link": "http://arxiv.org/abs/2107.13394",
          "publishedOn": "2021-07-29T02:00:09.432Z",
          "wordCount": 736,
          "title": "Nonlinear State Space Modeling and Control of the Impact of Patients' Modifiable Lifestyle Behaviors on the Emergence of Multiple Chronic Conditions. (arXiv:2107.13394v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lattanzi_E/0/1/0/all/0/1\">Emanuele Lattanzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calisti_L/0/1/0/all/0/1\">Lorenzo Calisti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freschi_V/0/1/0/all/0/1\">Valerio Freschi</a>",
          "description": "Current guidelines from the World Health Organization indicate that the\nSARSCoV-2 coronavirus, which results in the novel coronavirus disease\n(COVID-19), is transmitted through respiratory droplets or by contact. Contact\ntransmission occurs when contaminated hands touch the mucous membrane of the\nmouth, nose, or eyes. Moreover, pathogens can also be transferred from one\nsurface to another by contaminated hands, which facilitates transmission by\nindirect contact. Consequently, hands hygiene is extremely important to prevent\nthe spread of the SARSCoV-2 virus. Additionally, hand washing and/or hand\nrubbing disrupts also the transmission of other viruses and bacteria that cause\ncommon colds, flu and pneumonia, thereby reducing the overall disease burden.\nThe vast proliferation of wearable devices, such as smartwatches, containing\nacceleration, rotation, magnetic field sensors, etc., together with the modern\ntechnologies of artificial intelligence, such as machine learning and more\nrecently deep-learning, allow the development of accurate applications for\nrecognition and classification of human activities such as: walking, climbing\nstairs, running, clapping, sitting, sleeping, etc. In this work we evaluate the\nfeasibility of an automatic system, based on current smartwatches, which is\nable to recognize when a subject is washing or rubbing its hands, in order to\nmonitor parameters such as frequency and duration, and to evaluate the\neffectiveness of the gesture. Our preliminary results show a classification\naccuracy of about 95% and of about 94% for respectively deep and standard\nlearning techniques.",
          "link": "http://arxiv.org/abs/2107.13405",
          "publishedOn": "2021-07-29T02:00:09.424Z",
          "wordCount": 722,
          "title": "Automatic Unstructured Handwashing Recognition using Smartwatch to Reduce Contact Transmission of Pathogens. (arXiv:2107.13405v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_A/0/1/0/all/0/1\">Alishiba Dsouza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tempelmeier_N/0/1/0/all/0/1\">Nicolas Tempelmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demidova_E/0/1/0/all/0/1\">Elena Demidova</a>",
          "description": "OpenStreetMap (OSM) is one of the richest openly available sources of\nvolunteered geographic information. Although OSM includes various geographical\nentities, their descriptions are highly heterogeneous, incomplete, and do not\nfollow any well-defined ontology. Knowledge graphs can potentially provide\nvaluable semantic information to enrich OSM entities. However, interlinking OSM\nentities with knowledge graphs is inherently difficult due to the large,\nheterogeneous, ambiguous, and flat OSM schema and the annotation sparsity. This\npaper tackles the alignment of OSM tags with the corresponding knowledge graph\nclasses holistically by jointly considering the schema and instance layers. We\npropose a novel neural architecture that capitalizes upon a shared latent space\nfor tag-to-class alignment created using linked entities in OSM and knowledge\ngraphs. Our experiments performed to align OSM datasets for several countries\nwith two of the most prominent openly available knowledge graphs, namely,\nWikidata and DBpedia, demonstrate that the proposed approach outperforms the\nstate-of-the-art schema alignment baselines by up to 53 percentage points in\nterms of F1-score. The resulting alignment facilitates new semantic annotations\nfor over 10 million OSM entities worldwide, which is more than a 400% increase\ncompared to the existing semantic annotations in OSM.",
          "link": "http://arxiv.org/abs/2107.13257",
          "publishedOn": "2021-07-29T02:00:09.404Z",
          "wordCount": 625,
          "title": "Towards Neural Schema Alignment for OpenStreetMap and Knowledge Graphs. (arXiv:2107.13257v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Struski_L/0/1/0/all/0/1\">&#x141;ukasz Struski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danel_T/0/1/0/all/0/1\">Tomasz Danel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1\">Marek &#x15a;mieja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1\">Jacek Tabor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1\">Bartosz Zieli&#x144;ski</a>",
          "description": "Recent years have seen a surge in research on deep interpretable neural\nnetworks with decision trees as one of the most commonly incorporated tools.\nThere are at least three advantages of using decision trees over logistic\nregression classification models: they are easy to interpret since they are\nbased on binary decisions, they can make decisions faster, and they provide a\nhierarchy of classes. However, one of the well-known drawbacks of decision\ntrees, as compared to decision graphs, is that decision trees cannot reuse the\ndecision nodes. Nevertheless, decision graphs were not commonly used in deep\nlearning due to the lack of efficient gradient-based training techniques. In\nthis paper, we fill this gap and provide a general paradigm based on Markov\nprocesses, which allows for efficient training of the special type of decision\ngraphs, which we call Self-Organizing Neural Graphs (SONG). We provide an\nextensive theoretical study of SONG, complemented by experiments conducted on\nLetter, Connect4, MNIST, CIFAR, and TinyImageNet datasets, showing that our\nmethod performs on par or better than existing decision models.",
          "link": "http://arxiv.org/abs/2107.13214",
          "publishedOn": "2021-07-29T02:00:09.397Z",
          "wordCount": 601,
          "title": "SONG: Self-Organizing Neural Graphs. (arXiv:2107.13214v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1\">Bang Xiang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathy_Y/0/1/0/all/0/1\">Yasmin Fathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "Autoencoders are unsupervised models which have been used for detecting\nanomalies in multi-sensor environments. A typical use includes training a\npredictive model with data from sensors operating under normal conditions and\nusing the model to detect anomalies. Anomalies can come either from real\nchanges in the environment (real drift) or from faulty sensory devices (virtual\ndrift); however, the use of Autoencoders to distinguish between different\nanomalies has not yet been considered. To this end, we first propose the\ndevelopment of Bayesian Autoencoders to quantify epistemic and aleatoric\nuncertainties. We then test the Bayesian Autoencoder using a real-world\nindustrial dataset for hydraulic condition monitoring. The system is injected\nwith noise and drifts, and we have found the epistemic uncertainty to be less\nsensitive to sensor perturbations as compared to the reconstruction loss. By\nobserving the reconstructed signals with the uncertainties, we gain\ninterpretable insights, and these uncertainties offer a potential avenue for\ndistinguishing real and virtual drifts.",
          "link": "http://arxiv.org/abs/2107.13249",
          "publishedOn": "2021-07-29T02:00:09.388Z",
          "wordCount": 601,
          "title": "Bayesian Autoencoders for Drift Detection in Industrial Environments. (arXiv:2107.13249v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiyong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qianqian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Shilong Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>",
          "description": "The Area under the ROC curve (AUC) is a well-known ranking metric for\nproblems such as imbalanced learning and recommender systems. The vast majority\nof existing AUC-optimization-based machine learning methods only focus on\nbinary-class cases, while leaving the multiclass cases unconsidered. In this\npaper, we start an early trial to consider the problem of learning multiclass\nscoring functions via optimizing multiclass AUC metrics. Our foundation is\nbased on the M metric, which is a well-known multiclass extension of AUC. We\nfirst pay a revisit to this metric, showing that it could eliminate the\nimbalance issue from the minority class pairs. Motivated by this, we propose an\nempirical surrogate risk minimization framework to approximately optimize the M\nmetric. Theoretically, we show that: (i) optimizing most of the popular\ndifferentiable surrogate losses suffices to reach the Bayes optimal scoring\nfunction asymptotically; (ii) the training framework enjoys an imbalance-aware\ngeneralization error bound, which pays more attention to the bottleneck samples\nof minority classes compared with the traditional $O(\\sqrt{1/N})$ result.\nPractically, to deal with the low scalability of the computational operations,\nwe propose acceleration methods for three popular surrogate loss functions,\nincluding the exponential loss, squared loss, and hinge loss, to speed up loss\nand gradient evaluations. Finally, experimental results on 11 real-world\ndatasets demonstrate the effectiveness of our proposed framework.",
          "link": "http://arxiv.org/abs/2107.13171",
          "publishedOn": "2021-07-29T02:00:09.381Z",
          "wordCount": 650,
          "title": "Learning with Multiclass AUC: Theory and Algorithms. (arXiv:2107.13171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1\">Geetika Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1\">Rohit K Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1\">Kamlesh Tiwari</a>",
          "description": "This paper proposes teeth-photo, a new biometric modality for human\nauthentication on mobile and hand held devices. Biometrics samples are acquired\nusing the camera mounted on mobile device with the help of a mobile application\nhaving specific markers to register the teeth area. Region of interest (RoI) is\nthen extracted using the markers and the obtained sample is enhanced using\ncontrast limited adaptive histogram equalization (CLAHE) for better visual\nclarity. We propose a deep learning architecture and novel regularization\nscheme to obtain highly discriminative embedding for small size RoI. Proposed\ncustom loss function was able to achieve perfect classification for the tiny\nRoI of $75\\times 75$ size. The model is end-to-end and few-shot and therefore\nis very efficient in terms of time and energy requirements. The system can be\nused in many ways including device unlocking and secure authentication. To the\nbest of our understanding, this is the first work on teeth-photo based\nauthentication for mobile device. Experiments have been conducted on an\nin-house teeth-photo database collected using our application. The database is\nmade publicly available. Results have shown that the proposed system has\nperfect accuracy.",
          "link": "http://arxiv.org/abs/2107.13217",
          "publishedOn": "2021-07-29T02:00:09.373Z",
          "wordCount": 632,
          "title": "DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile and Hand-held Devices. (arXiv:2107.13217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wei-Wei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a>",
          "description": "Analyzing better time series with limited human effort is of interest to\nacademia and industry. Driven by business scenarios, we organized the first\nAutomated Time Series Regression challenge (AutoSeries) for the WSDM Cup 2020.\nWe present its design, analysis, and post-hoc experiments. The code submission\nrequirement precluded participants from any manual intervention, testing\nautomated machine learning capabilities of solutions, across many datasets,\nunder hardware and time limitations. We prepared 10 datasets from diverse\napplication domains (sales, power consumption, air quality, traffic, and\nparking), featuring missing data, mixed continuous and categorical variables,\nand various sampling rates. Each dataset was split into a training and a test\nsequence (which was streamed, allowing models to continuously adapt). The\nsetting of time series regression, differs from classical forecasting in that\ncovariates at the present time are known. Great strides were made by\nparticipants to tackle this AutoSeries problem, as demonstrated by the jump in\nperformance from the sample submission, and post-hoc comparisons with\nAutoGluon. Simple yet effective methods were used, based on feature\nengineering, LightGBM, and random search hyper-parameter tuning, addressing all\naspects of the challenge. Our post-hoc analyses revealed that providing\nadditional time did not yield significant improvements. The winners' code was\nopen-sourced https://www.4paradigm.com/competition/autoseries2020.",
          "link": "http://arxiv.org/abs/2107.13186",
          "publishedOn": "2021-07-29T02:00:09.353Z",
          "wordCount": 644,
          "title": "AutoML Meets Time Series Regression Design and Analysis of the AutoSeries Challenge. (arXiv:2107.13186v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lishuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinting Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok Leung Tsui</a>",
          "description": "Short-term forecasting of passenger flow is critical for transit management\nand crowd regulation. Spatial dependencies, temporal dependencies,\ninter-station correlations driven by other latent factors, and exogenous\nfactors bring challenges to the short-term forecasts of passenger flow of urban\nrail transit networks. An innovative deep learning approach, Multi-Graph\nConvolutional-Recurrent Neural Network (MGC-RNN) is proposed to forecast\npassenger flow in urban rail transit systems to incorporate these complex\nfactors. We propose to use multiple graphs to encode the spatial and other\nheterogenous inter-station correlations. The temporal dynamics of the\ninter-station correlations are also modeled via the proposed multi-graph\nconvolutional-recurrent neural network structure. Inflow and outflow of all\nstations can be collectively predicted with multiple time steps ahead via a\nsequence to sequence(seq2seq) architecture. The proposed method is applied to\nthe short-term forecasts of passenger flow in Shenzhen Metro, China. The\nexperimental results show that MGC-RNN outperforms the benchmark algorithms in\nterms of forecasting accuracy. Besides, it is found that the inter-station\ndriven by network distance, network structure, and recent flow patterns are\nsignificant factors for passenger flow forecasting. Moreover, the architecture\nof LSTM-encoder-decoder can capture the temporal dependencies well. In general,\nthe proposed framework could provide multiple views of passenger flow dynamics\nfor fine prediction and exhibit a possibility for multi-source heterogeneous\ndata fusion in the spatiotemporal forecast tasks.",
          "link": "http://arxiv.org/abs/2107.13226",
          "publishedOn": "2021-07-29T02:00:09.345Z",
          "wordCount": 659,
          "title": "Multi-Graph Convolutional-Recurrent Neural Network (MGC-RNN) for Short-Term Forecasting of Transit Passenger Flow. (arXiv:2107.13226v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruhe_D/0/1/0/all/0/1\">David Ruhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1\">Patrick Forr&#xe9;</a>",
          "description": "We perform approximate inference in state-space models that allow for\nnonlinear higher-order Markov chains in latent space. The conditional\nindependencies of the generative model enable us to parameterize only an\ninference model, which learns to estimate clean states in a self-supervised\nmanner using maximum likelihood. First, we propose a recurrent method that is\ntrained directly on noisy observations. Afterward, we cast the model such that\nthe optimization problem leads to an update scheme that backpropagates through\na recursion similar to the classical Kalman filter and smoother. In scientific\napplications, domain knowledge can give a linear approximation of the latent\ntransition maps. We can easily incorporate this knowledge into our model,\nleading to a hybrid inference approach. In contrast to other methods,\nexperiments show that the hybrid method makes the inferred latent states\nphysically more interpretable and accurate, especially in low-data regimes.\nFurthermore, we do not rely on an additional parameterization of the generative\nmodel or supervision via uncorrupted observations or ground truth latent\nstates. Despite our model's simplicity, we obtain competitive results on the\nchaotic Lorenz system compared to a fully supervised approach and outperform a\nmethod based on variational inference.",
          "link": "http://arxiv.org/abs/2107.13349",
          "publishedOn": "2021-07-29T02:00:09.338Z",
          "wordCount": 617,
          "title": "Self-Supervised Hybrid Inference in State-Space Models. (arXiv:2107.13349v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peuter_S/0/1/0/all/0/1\">Sebastiaan De Peuter</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Oulasvirta_A/0/1/0/all/0/1\">Antti Oulasvirta</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1\">Samuel Kaski</a> (1 and 3) ((1) Department of Computer Science, Aalto University, Finland, (2) Department of Communications and Networking, Aalto University, Finland, (3) Department of Computer Science, University of Manchester, UK)",
          "description": "AI for supporting designers needs to be rethought. It should aim to\ncooperate, not automate, by supporting and leveraging the creativity and\nproblem-solving of designers. The challenge for such AI is how to infer\ndesigners' goals and then help them without being needlessly disruptive. We\npresent AI-assisted design: a framework for creating such AI, built around\ngenerative user models which enable reasoning about designers' goals,\nreasoning, and capabilities.",
          "link": "http://arxiv.org/abs/2107.13074",
          "publishedOn": "2021-07-29T02:00:09.331Z",
          "wordCount": 550,
          "title": "Toward AI Assistants That Let Designers Design. (arXiv:2107.13074v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_E/0/1/0/all/0/1\">Eric Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer J. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1\">Ann Kennedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Swarat Chaudhuri</a>",
          "description": "We present a framework for the unsupervised learning of neurosymbolic\nencoders, i.e., encoders obtained by composing neural networks with symbolic\nprograms from a domain-specific language. Such a framework can naturally\nincorporate symbolic expert knowledge into the learning process and lead to\nmore interpretable and factorized latent representations than fully neural\nencoders. Also, models learned this way can have downstream impact, as many\nanalysis workflows can benefit from having clean programmatic descriptions. We\nground our learning algorithm in the variational autoencoding (VAE) framework,\nwhere we aim to learn a neurosymbolic encoder in conjunction with a standard\ndecoder. Our algorithm integrates standard VAE-style training with modern\nprogram synthesis techniques. We evaluate our method on learning latent\nrepresentations for real-world trajectory data from animal biology and sports\nanalytics. We show that our approach offers significantly better separation\nthan standard VAEs and leads to practical gains on downstream tasks.",
          "link": "http://arxiv.org/abs/2107.13132",
          "publishedOn": "2021-07-29T02:00:09.324Z",
          "wordCount": 576,
          "title": "Unsupervised Learning of Neurosymbolic Encoders. (arXiv:2107.13132v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1\">Bang Xiang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "Recent advancements in predictive machine learning has led to its application\nin various use cases in manufacturing. Most research focused on maximising\npredictive accuracy without addressing the uncertainty associated with it.\nWhile accuracy is important, focusing primarily on it poses an overfitting\ndanger, exposing manufacturers to risk, ultimately hindering the adoption of\nthese techniques. In this paper, we determine the sources of uncertainty in\nmachine learning and establish the success criteria of a machine learning\nsystem to function well under uncertainty in a cyber-physical manufacturing\nsystem (CPMS) scenario. Then, we propose a multi-agent system architecture\nwhich leverages probabilistic machine learning as a means of achieving such\ncriteria. We propose possible scenarios for which our proposed architecture is\nuseful and discuss future work. Experimentally, we implement Bayesian Neural\nNetworks for multi-tasks classification on a public dataset for the real-time\ncondition monitoring of a hydraulic system and demonstrate the usefulness of\nthe system by evaluating the probability of a prediction being accurate given\nits uncertainty. We deploy these models using our proposed agent-based\nframework and integrate web visualisation to demonstrate its real-time\nfeasibility.",
          "link": "http://arxiv.org/abs/2107.13252",
          "publishedOn": "2021-07-29T02:00:09.318Z",
          "wordCount": 635,
          "title": "Multi Agent System for Machine Learning Under Uncertainty in Cyber Physical Manufacturing System. (arXiv:2107.13252v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canessa_G/0/1/0/all/0/1\">Gianpiero Canessa</a>",
          "description": "Support vector machines (SVM) is one of the well known supervised classes of\nlearning algorithms. Furthermore, the conic-segmentation SVM (CS-SVM) is a\nnatural multiclass analogue of the standard binary SVM, as CS-SVM models are\ndealing with the situation where the exact values of the data points are known.\nThis paper studies CS-SVM when the data points are uncertain or mislabelled.\nWith some properties known for the distributions, a chance-constrained CS-SVM\napproach is used to ensure the small probability of misclassification for the\nuncertain data. The geometric interpretation is presented to show how CS-SVM\nworks. Finally, we present experimental results to investigate the chance\nconstrained CS-SVM's performance.",
          "link": "http://arxiv.org/abs/2107.13319",
          "publishedOn": "2021-07-29T02:00:09.297Z",
          "wordCount": 547,
          "title": "Chance constrained conic-segmentation support vector machine with uncertain data. (arXiv:2107.13319v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13393",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Choe_Y/0/1/0/all/0/1\">Yoonsuck Choe</a>",
          "description": "Brain science and artificial intelligence have made great progress toward the\nunderstanding and engineering of the human mind. The progress has accelerated\nsignificantly since the turn of the century thanks to new methods for probing\nthe brain (both structure and function), and rapid development in deep learning\nresearch. However, despite these new developments, there are still many open\nquestions, such as how to understand the brain at the system level, and various\nrobustness issues and limitations of deep learning. In this informal essay, I\nwill talk about some of the concepts that are central to brain science and\nartificial intelligence, such as information and memory, and discuss how a\ndifferent view on these concepts can help us move forward, beyond current\nlimits of our understanding in these fields.",
          "link": "http://arxiv.org/abs/2107.13393",
          "publishedOn": "2021-07-29T02:00:09.291Z",
          "wordCount": 576,
          "title": "Meaning Versus Information, Prediction Versus Memory, and Question Versus Answer. (arXiv:2107.13393v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chubba_e/0/1/0/all/0/1\">ennifer Chubba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Missaouib_S/0/1/0/all/0/1\">Sondess Missaouib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Concannonc_S/0/1/0/all/0/1\">Shauna Concannonc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maloneyb_L/0/1/0/all/0/1\">Liam Maloneyb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">James Alfred Walker</a>",
          "description": "Conversational Artificial Intelligence (CAI) systems and Intelligent Personal\nAssistants (IPA), such as Alexa, Cortana, Google Home and Siri are becoming\nubiquitous in our lives, including those of children, the implications of which\nis receiving increased attention, specifically with respect to the effects of\nthese systems on children's cognitive, social and linguistic development.\nRecent advances address the implications of CAI with respect to privacy,\nsafety, security, and access. However, there is a need to connect and embed the\nethical and technical aspects in the design. Using a case-study of a research\nand development project focused on the use of CAI in storytelling for children,\nthis paper reflects on the social context within a specific case of technology\ndevelopment, as substantiated and supported by argumentation from within the\nliterature. It describes the decision making process behind the recommendations\nmade on this case for their adoption in the creative industries. Further\nresearch that engages with developers and stakeholders in the ethics of\nstorytelling through CAI is highlighted as a matter of urgency.",
          "link": "http://arxiv.org/abs/2107.13076",
          "publishedOn": "2021-07-29T02:00:09.284Z",
          "wordCount": 620,
          "title": "Interactive Storytelling for Children: A Case-study of Design and Development Considerations for Ethical Conversational AI. (arXiv:2107.13076v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1\">Masahiro Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1\">Takaaki Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>",
          "description": "With the recent rapid progress in the study of deep generative models (DGMs),\nthere is a need for a framework that can implement them in a simple and generic\nway. In this research, we focus on two features of the latest DGMs: (1) deep\nneural networks are encapsulated by probability distributions and (2) models\nare designed and learned based on an objective function. Taking these features\ninto account, we propose a new DGM library called Pixyz. We experimentally show\nthat our library is faster than existing probabilistic modeling languages in\nlearning simple DGMs and we show that our library can be used to implement\ncomplex DGMs in a simple and concise manner, which is difficult to do with\nexisting libraries.",
          "link": "http://arxiv.org/abs/2107.13109",
          "publishedOn": "2021-07-29T02:00:09.276Z",
          "wordCount": 553,
          "title": "Pixyz: a library for developing deep generative models. (arXiv:2107.13109v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curth_A/0/1/0/all/0/1\">Alicia Curth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>",
          "description": "The machine learning toolbox for estimation of heterogeneous treatment\neffects from observational data is expanding rapidly, yet many of its\nalgorithms have been evaluated only on a very limited set of semi-synthetic\nbenchmark datasets. In this paper, we show that even in arguably the simplest\nsetting -- estimation under ignorability assumptions -- the results of such\nempirical evaluations can be misleading if (i) the assumptions underlying the\ndata-generating mechanisms in benchmark datasets and (ii) their interplay with\nbaseline algorithms are inadequately discussed. We consider two popular machine\nlearning benchmark datasets for evaluation of heterogeneous treatment effect\nestimators -- the IHDP and ACIC2016 datasets -- in detail. We identify problems\nwith their current use and highlight that the inherent characteristics of the\nbenchmark datasets favor some algorithms over others -- a fact that is rarely\nacknowledged but of immense relevance for interpretation of empirical results.\nWe close by discussing implications and possible next steps.",
          "link": "http://arxiv.org/abs/2107.13346",
          "publishedOn": "2021-07-29T02:00:09.269Z",
          "wordCount": 619,
          "title": "Doing Great at Estimating CATE? On the Neglected Assumptions in Benchmark Comparisons of Treatment Effect Estimators. (arXiv:2107.13346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.",
          "link": "http://arxiv.org/abs/2107.13469",
          "publishedOn": "2021-07-29T02:00:09.251Z",
          "wordCount": 605,
          "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13090",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hambly_B/0/1/0/all/0/1\">Ben Hambly</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xu_R/0/1/0/all/0/1\">Renyuan Xu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yang_H/0/1/0/all/0/1\">Huining Yang</a>",
          "description": "We consider a general-sum N-player linear-quadratic game with stochastic\ndynamics over a finite horizon and prove the global convergence of the natural\npolicy gradient method to the Nash equilibrium. In order to prove the\nconvergence of the method, we require a certain amount of noise in the system.\nWe give a condition, essentially a lower bound on the covariance of the noise\nin terms of the model parameters, in order to guarantee convergence. We\nillustrate our results with numerical experiments to show that even in\nsituations where the policy gradient method may not converge in the\ndeterministic setting, the addition of noise leads to convergence.",
          "link": "http://arxiv.org/abs/2107.13090",
          "publishedOn": "2021-07-29T02:00:09.243Z",
          "wordCount": 559,
          "title": "Policy Gradient Methods Find the Nash Equilibrium in N-player General-sum Linear-quadratic Games. (arXiv:2107.13090v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1\">Wil van der Aalst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockhoff_T/0/1/0/all/0/1\">Tobias Brockhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahfarokhi_A/0/1/0/all/0/1\">Anahita Farhang Ghahfarokhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourbafrani_M/0/1/0/all/0/1\">Mahsa Pourbafrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uysal_M/0/1/0/all/0/1\">Merih Seran Uysal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelst_S/0/1/0/all/0/1\">Sebastiaan van Zelst</a>",
          "description": "Operational processes in production, logistics, material handling,\nmaintenance, etc., are supported by cyber-physical systems combining hardware\nand software components. As a result, the digital and the physical world are\nclosely aligned, and it is possible to track operational processes in detail\n(e.g., using sensors). The abundance of event data generated by today's\noperational processes provides opportunities and challenges for process mining\ntechniques supporting process discovery, performance analysis, and conformance\nchecking. Using existing process mining tools, it is already possible to\nautomatically discover process models and uncover performance and compliance\nproblems. In the DFG-funded Cluster of Excellence \"Internet of Production\"\n(IoP), process mining is used to create \"digital shadows\" to improve a wide\nvariety of operational processes. However, operational processes are dynamic,\ndistributed, and complex. Driven by the challenges identified in the IoP\ncluster, we work on novel techniques for comparative process mining (comparing\nprocess variants for different products at different locations at different\ntimes), object-centric process mining (to handle processes involving different\ntypes of objects that interact), and forward-looking process mining (to explore\n\"What if?\" questions). By addressing these challenges, we aim to develop\nvaluable \"digital shadows\" that can be used to remove operational friction.",
          "link": "http://arxiv.org/abs/2107.13066",
          "publishedOn": "2021-07-29T02:00:09.236Z",
          "wordCount": 660,
          "title": "Removing Operational Friction Using Process Mining: Challenges Provided by the Internet of Production (IoP). (arXiv:2107.13066v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanan Sun</a>",
          "description": "Neural Architecture Search (NAS) can automatically design well-performed\narchitectures of Deep Neural Networks (DNNs) for the tasks at hand. However,\none bottleneck of NAS is the prohibitively computational cost largely due to\nthe expensive performance evaluation. The neural predictors can directly\nestimate the performance without any training of the DNNs to be evaluated, thus\nhave drawn increasing attention from researchers. Despite their popularity,\nthey also suffer a severe limitation: the shortage of annotated DNN\narchitectures for effectively training the neural predictors. In this paper, we\nproposed Homogeneous Architecture Augmentation for Neural Predictor (HAAP) of\nDNN architectures to address the issue aforementioned. Specifically, a\nhomogeneous architecture augmentation algorithm is proposed in HAAP to generate\nsufficient training data taking the use of homogeneous representation.\nFurthermore, the one-hot encoding strategy is introduced into HAAP to make the\nrepresentation of DNN architectures more effective. The experiments have been\nconducted on both NAS-Benchmark-101 and NAS-Bench-201 dataset. The experimental\nresults demonstrate that the proposed HAAP algorithm outperforms the state of\nthe arts compared, yet with much less training data. In addition, the ablation\nstudies on both benchmark datasets have also shown the universality of the\nhomogeneous architecture augmentation.",
          "link": "http://arxiv.org/abs/2107.13153",
          "publishedOn": "2021-07-29T02:00:09.228Z",
          "wordCount": 631,
          "title": "Homogeneous Architecture Augmentation for Neural Predictor. (arXiv:2107.13153v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Sayantini Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivisonno_R/0/1/0/all/0/1\">Riccardo Trivisonno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carle_G/0/1/0/all/0/1\">Georg Carle</a>",
          "description": "Network automation is gaining significant attention in the development of B5G\nnetworks, primarily for reducing operational complexity, expenditures and\nimproving network efficiency. Concurrently operating closed loops aiming for\nindividual optimization targets may cause conflicts which, left unresolved,\nwould lead to significant degradation in network Key Performance Indicators\n(KPIs), thereby resulting in sub-optimal network performance. Centralized\ncoordination, albeit optimal, is impractical in large scale networks and for\ntime-critical applications. Decentralized approaches are therefore envisaged in\nthe evolution to B5G and subsequently, 6G networks. This work explores\npervasive intelligence for conflict resolution in network automation, as an\nalternative to centralized orchestration. A Q-Learning decentralized approach\nto network automation is proposed, and an application to network slice\nauto-scaling is designed and evaluated. Preliminary results highlight the\npotential of the proposed scheme and justify further research work in this\ndirection.",
          "link": "http://arxiv.org/abs/2107.13268",
          "publishedOn": "2021-07-29T02:00:09.218Z",
          "wordCount": 606,
          "title": "Q-Learning for Conflict Resolution in B5G Network Automation. (arXiv:2107.13268v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1\">Claudio Pomo</a>",
          "description": "Collaborative filtering models based on matrix factorization and learned\nsimilarities using Artificial Neural Networks (ANNs) have gained significant\nattention in recent years. This is, in part, because ANNs have demonstrated\ngood results in a wide variety of recommendation tasks. The introduction of\nANNs within the recommendation ecosystem has been recently questioned, raising\nseveral comparisons in terms of efficiency and effectiveness. One aspect most\nof these comparisons have in common is their focus on accuracy, neglecting\nother evaluation dimensions important for the recommendation, such as novelty,\ndiversity, or accounting for biases. We replicate experiments from three papers\nthat compare Neural Collaborative Filtering (NCF) and Matrix Factorization\n(MF), to extend the analysis to other evaluation dimensions. Our contribution\nshows that the experiments are entirely reproducible, and we extend the study\nincluding other accuracy metrics and two statistical hypothesis tests. We\ninvestigated the Diversity and Novelty of the recommendations, showing that MF\nprovides a better accuracy also on the long tail, although NCF provides a\nbetter item coverage and more diversified recommendations. We discuss the bias\neffect generated by the tested methods. They show a relatively small bias, but\nother recommendation baselines, with competitive accuracy performance,\nconsistently show to be less affected by this issue. This is the first work, to\nthe best of our knowledge, where several evaluation dimensions have been\nexplored for an array of SOTA algorithms covering recent adaptations of ANNs\nand MF. Hence, we show the potential these techniques may have on\nbeyond-accuracy evaluation while analyzing the effect on reproducibility these\ncomplementary dimensions may spark. Available at\ngithub.com/sisinflab/Reenvisioning-the-comparison-between-Neural-Collaborative-Filtering-and-Matrix-Factorization",
          "link": "http://arxiv.org/abs/2107.13472",
          "publishedOn": "2021-07-29T02:00:09.198Z",
          "wordCount": 710,
          "title": "Reenvisioning Collaborative Filtering vs Matrix Factorization. (arXiv:2107.13472v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pengyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "There has been a growing interest in unsupervised domain adaptation (UDA) to\nalleviate the data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is proposed for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vectors that violate the poset constraint by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.",
          "link": "http://arxiv.org/abs/2107.13467",
          "publishedOn": "2021-07-29T02:00:09.190Z",
          "wordCount": 624,
          "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuesong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>",
          "description": "Node features and structural information of a graph are both crucial for\nsemi-supervised node classification problems. A variety of graph neural network\n(GNN) based approaches have been proposed to tackle these problems, which\ntypically determine output labels through feature aggregation. This can be\nproblematic, as it implies conditional independence of output nodes given\nhidden representations, despite their direct connections in the graph. To learn\nthe direct influence among output nodes in a graph, we propose the Explicit\nPairwise Factorized Graph Neural Network (EPFGNN), which models the whole graph\nas a partially observed Markov Random Field. It contains explicit pairwise\nfactors to model output-output relations and uses a GNN backbone to model\ninput-output relations. To balance model complexity and expressivity, the\npairwise factors have a shared component and a separate scaling coefficient for\neach edge. We apply the EM algorithm to train our model, and utilize a\nstar-shaped piecewise likelihood for the tractable surrogate objective. We\nconduct experiments on various datasets, which shows that our model can\neffectively improve the performance for semi-supervised node classification on\ngraphs.",
          "link": "http://arxiv.org/abs/2107.13059",
          "publishedOn": "2021-07-29T02:00:09.179Z",
          "wordCount": 613,
          "title": "Explicit Pairwise Factorized Graph Neural Network for Semi-Supervised Node Classification. (arXiv:2107.13059v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Picallo_M/0/1/0/all/0/1\">Mario Alvarez-Picallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghica_D/0/1/0/all/0/1\">Dan R. Ghica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sprunger_D/0/1/0/all/0/1\">David Sprunger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanasi_F/0/1/0/all/0/1\">Fabio Zanasi</a>",
          "description": "We enhance the calculus of string diagrams for monoidal categories with\nhierarchical features in order to capture closed monoidal (and cartesian\nclosed) structure. Using this new syntax we formulate an automatic\ndifferentiation algorithm for (applied) simply typed lambda calculus in the\nstyle of [Pearlmutter and Siskind 2008] and we prove for the first time its\nsoundness. To give an efficient yet principled implementation of the AD\nalgorithm we define a sound and complete representation of hierarchical string\ndiagrams as a class of hierarchical hypergraphs we call hypernets.",
          "link": "http://arxiv.org/abs/2107.13433",
          "publishedOn": "2021-07-29T02:00:09.160Z",
          "wordCount": 523,
          "title": "Functorial String Diagrams for Reverse-Mode Automatic Differentiation. (arXiv:2107.13433v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>",
          "description": "In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose new explainability approaches for point cloud deep\nneural networks based on local surrogate model-based methods to show which\ncomponents make the main contribution to the classification. Moreover, we\npropose a quantitative validation method for explainability methods of point\nclouds which enhances the persuasive power of explainability by dropping the\nmost positive or negative contributing features and monitoring how the\nclassification scores of specific categories change. To enable an intuitive\nexplanation of misclassified instances, we display features with confounding\ncontributions. Our new explainability approach provides a fairly accurate, more\nintuitive and widely applicable explanation for point cloud classification\ntasks. Our code is available at https://github.com/Explain3D/Explainable3D",
          "link": "http://arxiv.org/abs/2107.13459",
          "publishedOn": "2021-07-29T02:00:09.104Z",
          "wordCount": 610,
          "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1\">Guangliang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minglei Li</a>",
          "description": "Channel estimation and signal detection are essential steps to ensure the\nquality of end-to-end communication in orthogonal frequency-division\nmultiplexing (OFDM) systems. In this paper, we develop a DDLSD approach, i.e.,\nData-driven Deep Learning for Signal Detection in OFDM systems. First, the OFDM\nsystem model is established. Then, the long short-term memory (LSTM) is\nintroduced into the OFDM system model. Wireless channel data is generated\nthrough simulation, the preprocessed time series feature information is input\ninto the LSTM to complete the offline training. Finally, the trained model is\nused for online recovery of transmitted signal. The difference between this\nscheme and existing OFDM receiver is that explicit estimated channel state\ninformation (CSI) is transformed into invisible estimated CSI, and the transmit\nsymbol is directly restored. Simulation results show that the DDLSD scheme\noutperforms the existing traditional methods in terms of improving channel\nestimation and signal detection performance.",
          "link": "http://arxiv.org/abs/2107.13423",
          "publishedOn": "2021-07-29T02:00:09.096Z",
          "wordCount": 594,
          "title": "A Signal Detection Scheme Based on Deep Learning in OFDM Systems. (arXiv:2107.13423v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dongchen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-feng Yang</a>",
          "description": "Traditional maximum entropy and sparsity-based algorithms for analytic\ncontinuation often suffer from the ill-posed kernel matrix or demand tremendous\ncomputation time for parameter tuning. Here we propose a neural network method\nby convex optimization and replace the ill-posed inverse problem by a sequence\nof well-conditioned surrogate problems. After training, the learned optimizers\nare able to give a solution of high quality with low time cost and achieve\nhigher parameter efficiency than heuristic full-connected networks. The output\ncan also be used as a neural default model to improve the maximum entropy for\nbetter performance. Our methods may be easily extended to other\nhigh-dimensional inverse problems via large-scale pretraining.",
          "link": "http://arxiv.org/abs/2107.13265",
          "publishedOn": "2021-07-29T02:00:09.087Z",
          "wordCount": 542,
          "title": "Learned Optimizers for Analytic Continuation. (arXiv:2107.13265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13449",
          "author": "<a href=\"http://arxiv.org/find/nucl-th/1/au:+Sarkar_A/0/1/0/all/0/1\">Avik Sarkar</a>, <a href=\"http://arxiv.org/find/nucl-th/1/au:+Lee_D/0/1/0/all/0/1\">Dean Lee</a>",
          "description": "Emulators that can bypass computationally expensive scientific calculations\nwith high accuracy and speed can enable new studies of fundamental science as\nwell as more potential applications. In this work we focus on solving a system\nof constraint equations efficiently using a new machine learning approach that\nwe call self-learning emulation. A self-learning emulator is an active learning\nprotocol that can rapidly solve a system of equations over some range of\ncontrol parameters. The key ingredient is a fast estimate of the emulator error\nthat becomes progressively more accurate as the emulator improves. This\nacceleration is possible because the emulator itself is used to estimate the\nerror, and we illustrate with two examples. The first uses cubic spline\ninterpolation to find the roots of a polynomial with variable coefficients. The\nsecond example uses eigenvector continuation to find the eigenvectors and\neigenvalues of a large Hamiltonian matrix that depends on several control\nparameters. We envision future applications of self-learning emulators for\nsolving systems of algebraic equations, linear and nonlinear differential\nequations, and linear and nonlinear eigenvalue problems.",
          "link": "http://arxiv.org/abs/2107.13449",
          "publishedOn": "2021-07-29T02:00:09.080Z",
          "wordCount": 625,
          "title": "Self-learning Emulators and Eigenvector Continuation. (arXiv:2107.13449v1 [nucl-th])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13148",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Ullah_A/0/1/0/all/0/1\">A. K. M. Amanat Ullah</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Imtiaz_F/0/1/0/all/0/1\">Fahim Imtiaz</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ihsan_M/0/1/0/all/0/1\">Miftah Uddin Md Ihsan</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Alam_M/0/1/0/all/0/1\">Md. Golam Rabiul Alam</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Majumdar_M/0/1/0/all/0/1\">Mahbub Majumdar</a>",
          "description": "The unpredictability and volatility of the stock market render it challenging\nto make a substantial profit using any generalized scheme. This paper intends\nto discuss our machine learning model, which can make a significant amount of\nprofit in the US stock market by performing live trading in the Quantopian\nplatform while using resources free of cost. Our top approach was to use\nensemble learning with four classifiers: Gaussian Naive Bayes, Decision Tree,\nLogistic Regression with L1 regularization and Stochastic Gradient Descent, to\ndecide whether to go long or short on a particular stock. Our best model\nperformed daily trade between July 2011 and January 2019, generating 54.35%\nprofit. Finally, our work showcased that mixtures of weighted classifiers\nperform better than any individual predictor about making trading decisions in\nthe stock market.",
          "link": "http://arxiv.org/abs/2107.13148",
          "publishedOn": "2021-07-29T02:00:09.073Z",
          "wordCount": 594,
          "title": "Combining Machine Learning Classifiers for Stock Trading with Effective Feature Extraction. (arXiv:2107.13148v1 [q-fin.TR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Divi_S/0/1/0/all/0/1\">Siddharth Divi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yi-Shan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrukh_H/0/1/0/all/0/1\">Habiba Farrukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celik_Z/0/1/0/all/0/1\">Z. Berkay Celik</a>",
          "description": "In Federated Learning (FL), the clients learn a single global model (FedAvg)\nthrough a central aggregator. In this setting, the non-IID distribution of the\ndata across clients restricts the global FL model from delivering good\nperformance on the local data of each client. Personalized FL aims to address\nthis problem by finding a personalized model for each client. Recent works\nwidely report the average personalized model accuracy on a particular data\nsplit of a dataset to evaluate the effectiveness of their methods. However,\nconsidering the multitude of personalization approaches proposed, it is\ncritical to study the per-user personalized accuracy and the accuracy\nimprovements among users with an equitable notion of fairness. To address these\nissues, we present a set of performance and fairness metrics intending to\nassess the quality of personalized FL methods. We apply these metrics to four\nrecently proposed personalized FL methods, PersFL, FedPer, pFedMe, and\nPer-FedAvg, on three different data splits of the CIFAR-10 dataset. Our\nevaluations show that the personalized model with the highest average accuracy\nacross users may not necessarily be the fairest. Our code is available at\nhttps://tinyurl.com/1hp9ywfa for public use.",
          "link": "http://arxiv.org/abs/2107.13173",
          "publishedOn": "2021-07-29T02:00:09.053Z",
          "wordCount": 627,
          "title": "New Metrics to Evaluate the Performance and Fairness of Personalized Federated Learning. (arXiv:2107.13173v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1\">Bang Xiang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pearce_T/0/1/0/all/0/1\">Tim Pearce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "After an autoencoder (AE) has learnt to reconstruct one dataset, it might be\nexpected that the likelihood on an out-of-distribution (OOD) input would be\nlow. This has been studied as an approach to detect OOD inputs. Recent work\nshowed this intuitive approach can fail for the dataset pairs FashionMNIST vs\nMNIST. This paper suggests this is due to the use of Bernoulli likelihood and\nanalyses why this is the case, proposing two fixes: 1) Compute the uncertainty\nof likelihood estimate by using a Bayesian version of the AE. 2) Use\nalternative distributions to model the likelihood.",
          "link": "http://arxiv.org/abs/2107.13304",
          "publishedOn": "2021-07-29T02:00:09.045Z",
          "wordCount": 550,
          "title": "Bayesian Autoencoders: Analysing and Fixing the Bernoulli likelihood for Out-of-Distribution Detection. (arXiv:2107.13304v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13430",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nishida_K/0/1/0/all/0/1\">Kiheiji Nishida</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Naito_K/0/1/0/all/0/1\">Kanta Naito</a>",
          "description": "This paper studies kernel density estimation by stagewise minimization\nalgorithm with a simple dictionary on U-divergence. We randomly split an i.i.d.\nsample into the two disjoint sets, one to be used for constructing the kernels\nin the dictionary and the other for evaluating the estimator, and implement the\nalgorithm. The resulting estimator brings us data-adaptive weighting parameters\nand bandwidth matrices, and realizes a sparse representation of kernel density\nestimation. We present the non-asymptotic error bounds of our estimator and\nconfirm its performance by simulations compared with the direct plug-in\nbandwidth matrices and the reduced set density estimator.",
          "link": "http://arxiv.org/abs/2107.13430",
          "publishedOn": "2021-07-29T02:00:09.036Z",
          "wordCount": 538,
          "title": "Kernel Density Estimation by Stagewise Algorithm with a Simple Dictionary. (arXiv:2107.13430v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13136",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1\">Joseph Marino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>",
          "description": "While recent machine learning research has revealed connections between deep\ngenerative models such as VAEs and rate-distortion losses used in learned\ncompression, most of this work has focused on images. In a similar spirit, we\nview recently proposed neural video coding algorithms through the lens of deep\nautoregressive and latent variable modeling. We present recent neural video\ncodecs as instances of a generalized stochastic temporal autoregressive\ntransform, and propose new avenues for further improvements inspired by\nnormalizing flows and structured priors. We propose several architectures that\nyield state-of-the-art video compression performance on full-resolution video\nand discuss their tradeoffs and ablations. In particular, we propose (i)\nimproved temporal autoregressive transforms, (ii) improved entropy models with\nstructured and temporal dependencies, and (iii) variable bitrate versions of\nour algorithms. Since our improvements are compatible with a large class of\nexisting models, we provide further evidence that the generative modeling\nviewpoint can advance the neural video coding field.",
          "link": "http://arxiv.org/abs/2107.13136",
          "publishedOn": "2021-07-29T02:00:09.028Z",
          "wordCount": 637,
          "title": "Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "A common lens to theoretically study neural net architectures is to analyze\nthe functions they can approximate. However, the constructions from\napproximation theory often have unrealistic aspects, for example, reliance on\ninfinite precision to memorize target function values, which make these results\npotentially less meaningful. To address these issues, this work proposes a\nformal definition of statistically meaningful approximation which requires the\napproximating network to exhibit good statistical learnability. We present case\nstudies on statistically meaningful approximation for two classes of functions:\nboolean circuits and Turing machines. We show that overparameterized\nfeedforward neural nets can statistically meaningfully approximate boolean\ncircuits with sample complexity depending only polynomially on the circuit\nsize, not the size of the approximating network. In addition, we show that\ntransformers can statistically meaningfully approximate Turing machines with\ncomputation time bounded by $T$, requiring sample complexity polynomial in the\nalphabet size, state space size, and $\\log (T)$. Our analysis introduces new\ntools for generalization bounds that provide much tighter sample complexity\nguarantees than the typical VC-dimension or norm-based bounds, which may be of\nindependent interest.",
          "link": "http://arxiv.org/abs/2107.13163",
          "publishedOn": "2021-07-29T02:00:09.019Z",
          "wordCount": 619,
          "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers. (arXiv:2107.13163v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13200",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_B/0/1/0/all/0/1\">Bo Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1\">Pengfei Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a> (Alzheimer&#x27;s Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing), <a href=\"http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1\">Shuwei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1\">Peng Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Ronald X. Xu</a>",
          "description": "Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal\nperiod mild cognitive impairment (MCI) is essential for the delayed disease\nprogression and the improved quality of patients'life. The emerging\ncomputer-aided diagnostic methods that combine deep learning with structural\nmagnetic resonance imaging (sMRI) have achieved encouraging results, but some\nof them are limit of issues such as data leakage and unexplainable diagnosis.\nIn this research, we propose a novel end-to-end deep learning approach for\nautomated diagnosis of AD and localization of important brain regions related\nto the disease from sMRI data. This approach is based on a 2D single model\nstrategy and has the following differences from the current approaches: 1)\nConvolutional Neural Network (CNN) models of different structures and\ncapacities are evaluated systemically and the most suitable model is adopted\nfor AD diagnosis; 2) a data augmentation strategy named Two-stage Random\nRandAugment (TRRA) is proposed to alleviate the overfitting issue caused by\nlimited training data and to improve the classification performance in AD\ndiagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the\nvisually explainable heatmaps that localize and highlight the brain regions\nthat our model focuses on and to make our model more transparent. Our approach\nhas been evaluated on two publicly accessible datasets for two classification\ntasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable\nMCI (sMCI). The experimental results indicate that our approach outperforms the\nstate-of-the-art approaches, including those using multi-model and 3D CNN\nmethods. The resultant localization heatmaps from our approach also highlight\nthe lateral ventricle and some disease-relevant regions of cortex, coincident\nwith the commonly affected regions during the development of AD.",
          "link": "http://arxiv.org/abs/2107.13200",
          "publishedOn": "2021-07-29T02:00:08.998Z",
          "wordCount": 760,
          "title": "An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization. (arXiv:2107.13200v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daubechies_I/0/1/0/all/0/1\">Ingrid Daubechies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeVore_R/0/1/0/all/0/1\">Ronald DeVore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dym_N/0/1/0/all/0/1\">Nadav Dym</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faigenbaum_Golovin_S/0/1/0/all/0/1\">Shira Faigenbaum-Golovin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalsky_S/0/1/0/all/0/1\">Shahar Z. Kovalsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kung-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Josiah Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrova_G/0/1/0/all/0/1\">Guergana Petrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sober_B/0/1/0/all/0/1\">Barak Sober</a>",
          "description": "In the desire to quantify the success of neural networks in deep learning and\nother applications, there is a great interest in understanding which functions\nare efficiently approximated by the outputs of neural networks. By now, there\nexists a variety of results which show that a wide range of functions can be\napproximated with sometimes surprising accuracy by these outputs. For example,\nit is known that the set of functions that can be approximated with exponential\naccuracy (in terms of the number of parameters used) includes, on one hand,\nvery smooth functions such as polynomials and analytic functions (see e.g.\n\\cite{E,S,Y}) and, on the other hand, very rough functions such as the\nWeierstrass function (see e.g. \\cite{EPGB,DDFHP}), which is nowhere\ndifferentiable. In this paper, we add to the latter class of rough functions by\nshowing that it also includes refinable functions. Namely, we show that\nrefinable functions are approximated by the outputs of deep ReLU networks with\na fixed width and increasing depth with accuracy exponential in terms of their\nnumber of parameters. Our results apply to functions used in the standard\nconstruction of wavelets as well as to functions constructed via subdivision\nalgorithms in Computer Aided Geometric Design.",
          "link": "http://arxiv.org/abs/2107.13191",
          "publishedOn": "2021-07-29T02:00:08.989Z",
          "wordCount": 635,
          "title": "Neural Network Approximation of Refinable Functions. (arXiv:2107.13191v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bahadori_M/0/1/0/all/0/1\">Mohammad Taha Bahadori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchetgen_E/0/1/0/all/0/1\">Eric Tchetgen Tchetgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David E. Heckerman</a>",
          "description": "We study the problem of observational causal inference with continuous\ntreatment. We focus on the challenge of estimating the causal response curve\nfor infrequently-observed treatment values. We design a new algorithm based on\nthe framework of entropy balancing which learns weights that directly maximize\ncausal inference accuracy using end-to-end optimization. Our weights can be\ncustomized for different datasets and causal inference algorithms. We propose a\nnew theory for consistency of entropy balancing for continuous treatments.\nUsing synthetic and real-world data, we show that our proposed algorithm\noutperforms the entropy balancing in terms of causal inference accuracy.",
          "link": "http://arxiv.org/abs/2107.13068",
          "publishedOn": "2021-07-29T02:00:08.982Z",
          "wordCount": 537,
          "title": "End-to-End Balancing for Causal Continuous Treatment-Effect Estimation. (arXiv:2107.13068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1\">Keld T. Lundgaard</a>",
          "description": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.",
          "link": "http://arxiv.org/abs/2107.13054",
          "publishedOn": "2021-07-29T02:00:08.975Z",
          "wordCount": 623,
          "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesidis_G/0/1/0/all/0/1\">George Kesidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1\">David J. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergeron_M/0/1/0/all/0/1\">Maxime Bergeron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferguson_R/0/1/0/all/0/1\">Ryan Ferguson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_V/0/1/0/all/0/1\">Vladimir Lucic</a>",
          "description": "We describe a gradient-based method to discover local error maximizers of a\ndeep neural network (DNN) used for regression, assuming the availability of an\n\"oracle\" capable of providing real-valued supervision (a regression target) for\nsamples. For example, the oracle could be a numerical solver which,\noperationally, is much slower than the DNN. Given a discovered set of local\nerror maximizers, the DNN is either fine-tuned or retrained in the manner of\nactive learning.",
          "link": "http://arxiv.org/abs/2107.13124",
          "publishedOn": "2021-07-29T02:00:08.954Z",
          "wordCount": 511,
          "title": "Robust and Active Learning for Deep Neural Network Regression. (arXiv:2107.13124v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13067",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ataei_M/0/1/0/all/0/1\">Mohammadmehdi Ataei</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pirmorad_E/0/1/0/all/0/1\">Erfan Pirmorad</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Costa_F/0/1/0/all/0/1\">Franco Costa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Han_S/0/1/0/all/0/1\">Sejin Han</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Park_C/0/1/0/all/0/1\">Chul B Park</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bussmann_M/0/1/0/all/0/1\">Markus Bussmann</a>",
          "description": "Piecewise Linear Interface Construction (PLIC) is frequently used to\ngeometrically reconstruct fluid interfaces in Computational Fluid Dynamics\n(CFD) modeling of two-phase flows. PLIC reconstructs interfaces from a scalar\nfield that represents the volume fraction of each phase in each computational\ncell. Given the volume fraction and interface normal, the location of a linear\ninterface is uniquely defined. For a cubic computational cell (3D), the\nposition of the planar interface is determined by intersecting the cube with a\nplane, such that the volume of the resulting truncated polyhedron cell is equal\nto the volume fraction. Yet it is geometrically complex to find the exact\nposition of the plane, and it involves calculations that can be a computational\nbottleneck of many CFD models. However, while the forward problem of 3D PLIC is\nchallenging, the inverse problem, of finding the volume of the truncated\npolyhedron cell given a defined plane, is simple. In this work, we propose a\ndeep learning model for the solution to the forward problem of PLIC by only\nmaking use of its inverse problem. The proposed model is up to several orders\nof magnitude faster than traditional schemes, which significantly reduces the\ncomputational bottleneck of PLIC in CFD simulations.",
          "link": "http://arxiv.org/abs/2107.13067",
          "publishedOn": "2021-07-29T02:00:08.947Z",
          "wordCount": 651,
          "title": "A Deep Learning Algorithm for Piecewise Linear Interface Construction (PLIC). (arXiv:2107.13067v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dallmann_A/0/1/0/all/0/1\">Alexander Dallmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoller_D/0/1/0/all/0/1\">Daniel Zoller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1\">Andreas Hotho</a>",
          "description": "At the present time, sequential item recommendation models are compared by\ncalculating metrics on a small item subset (target set) to speed up\ncomputation. The target set contains the relevant item and a set of negative\nitems that are sampled from the full item set. Two well-known strategies to\nsample negative items are uniform random sampling and sampling by popularity to\nbetter approximate the item frequency distribution in the dataset. Most\nrecently published papers on sequential item recommendation rely on sampling by\npopularity to compare the evaluated models. However, recent work has already\nshown that an evaluation with uniform random sampling may not be consistent\nwith the full ranking, that is, the model ranking obtained by evaluating a\nmetric using the full item set as target set, which raises the question whether\nthe ranking obtained by sampling by popularity is equal to the full ranking. In\nthis work, we re-evaluate current state-of-the-art sequential recommender\nmodels from the point of view, whether these sampling strategies have an impact\non the final ranking of the models. We therefore train four recently proposed\nsequential recommendation models on five widely known datasets. For each\ndataset and model, we employ three evaluation strategies. First, we compute the\nfull model ranking. Then we evaluate all models on a target set sampled by the\ntwo different sampling strategies, uniform random sampling and sampling by\npopularity with the commonly used target set size of 100, compute the model\nranking for each strategy and compare them with each other. Additionally, we\nvary the size of the sampled target set. Overall, we find that both sampling\nstrategies can produce inconsistent rankings compared with the full ranking of\nthe models. Furthermore, both sampling by popularity and uniform random\nsampling do not consistently produce the same ranking ...",
          "link": "http://arxiv.org/abs/2107.13045",
          "publishedOn": "2021-07-29T02:00:08.940Z",
          "wordCount": 740,
          "title": "A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models. (arXiv:2107.13045v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walsh_R/0/1/0/all/0/1\">Reece Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelpakey_M/0/1/0/all/0/1\">Mohamed H. Abdelpakey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed S. Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mostafa M.Mohamed</a>",
          "description": "Classifying and analyzing human cells is a lengthy procedure, often involving\na trained professional. In an attempt to expedite this process, an active area\nof research involves automating cell classification through use of deep\nlearning-based techniques. In practice, a large amount of data is required to\naccurately train these deep learning models. However, due to the sparse human\ncell datasets currently available, the performance of these models is typically\nlow. This study investigates the feasibility of using few-shot learning-based\ntechniques to mitigate the data requirements for accurate training. The study\nis comprised of three parts: First, current state-of-the-art few-shot learning\ntechniques are evaluated on human cell classification. The selected techniques\nare trained on a non-medical dataset and then tested on two out-of-domain,\nhuman cell datasets. The results indicate that, overall, the test accuracy of\nstate-of-the-art techniques decreased by at least 30% when transitioning from a\nnon-medical dataset to a medical dataset. Second, this study evaluates the\npotential benefits, if any, to varying the backbone architecture and training\nschemes in current state-of-the-art few-shot learning techniques when used in\nhuman cell classification. Even with these variations, the overall test\naccuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the\nmedical datasets. Third, this study presents future directions for using\nfew-shot learning in human cell classification. In general, few-shot learning\nin its current state performs poorly on human cell classification. The study\nproves that attempts to modify existing network architectures are not effective\nand concludes that future research effort should be focused on improving\nrobustness towards out-of-domain testing using optimization-based or\nself-supervised few-shot learning techniques.",
          "link": "http://arxiv.org/abs/2107.13093",
          "publishedOn": "2021-07-29T02:00:08.932Z",
          "wordCount": 720,
          "title": "Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning. (arXiv:2107.13093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Onoufriou_G/0/1/0/all/0/1\">George Onoufriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayfield_P/0/1/0/all/0/1\">Paul Mayfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontidis_G/0/1/0/all/0/1\">Georgios Leontidis</a>",
          "description": "Fully Homomorphic Encryption (FHE) is a relatively recent advancement in the\nfield of privacy-preserving technologies. FHE allows for the arbitrary depth\ncomputation of both addition and multiplication, and thus the application of\nabelian/polynomial equations, like those found in deep learning algorithms.\nThis project investigates, derives, and proves how FHE with deep learning can\nbe used at scale, with relatively low time complexity, the problems that such a\nsystem incurs, and mitigations/solutions for such problems. In addition, we\ndiscuss how this could have an impact on the future of data privacy and how it\ncan enable data sharing across various actors in the agri-food supply chain,\nhence allowing the development of machine learning-based systems. Finally, we\nfind that although FHE incurs a high spatial complexity cost, the time\ncomplexity is within expected reasonable bounds, while allowing for absolutely\nprivate predictions to be made, in our case for milk yield prediction.",
          "link": "http://arxiv.org/abs/2107.12997",
          "publishedOn": "2021-07-29T02:00:08.920Z",
          "wordCount": 585,
          "title": "Fully Homomorphically Encrypted Deep Learning as a Service. (arXiv:2107.12997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Timothy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_R/0/1/0/all/0/1\">Roman Novak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lechao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaehoon Lee</a>",
          "description": "The effectiveness of machine learning algorithms arises from being able to\nextract useful features from large amounts of data. As model and dataset sizes\nincrease, dataset distillation methods that compress large datasets into\nsignificantly smaller yet highly performant ones will become valuable in terms\nof training efficiency and useful feature extraction. To that end, we apply a\nnovel distributed kernel based meta-learning framework to achieve\nstate-of-the-art results for dataset distillation using infinitely wide\nconvolutional neural networks. For instance, using only 10 datapoints (0.02% of\noriginal dataset), we obtain over 64% test accuracy on CIFAR-10 image\nclassification task, a dramatic improvement over the previous best test\naccuracy of 40%. Our state-of-the-art results extend across many other settings\nfor MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and SVHN. Furthermore, we\nperform some preliminary analyses of our distilled datasets to shed light on\nhow they differ from naturally occurring data.",
          "link": "http://arxiv.org/abs/2107.13034",
          "publishedOn": "2021-07-29T02:00:08.867Z",
          "wordCount": 580,
          "title": "Dataset Distillation with Infinitely Wide Convolutional Networks. (arXiv:2107.13034v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.01791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uddin_A/0/1/0/all/0/1\">A. F. M. Shahab Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monira_M/0/1/0/all/0/1\">Mst. Sirazam Monira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Wheemyung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">TaeChoong Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Sung-Ho Bae</a>",
          "description": "Advanced data augmentation strategies have widely been studied to improve the\ngeneralization ability of deep learning models. Regional dropout is one of the\npopular solutions that guides the model to focus on less discriminative parts\nby randomly removing image regions, resulting in improved regularization.\nHowever, such information removal is undesirable. On the other hand, recent\nstrategies suggest to randomly cut and mix patches and their labels among\ntraining images, to enjoy the advantages of regional dropout without having any\npointless pixel in the augmented images. We argue that such random selection\nstrategies of the patches may not necessarily represent sufficient information\nabout the corresponding object and thereby mixing the labels according to that\nuninformative patch enables the model to learn unexpected feature\nrepresentation. Therefore, we propose SaliencyMix that carefully selects a\nrepresentative image patch with the help of a saliency map and mixes this\nindicative patch with the target image, thus leading the model to learn more\nappropriate feature representation. SaliencyMix achieves the best known top-1\nerror of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures on\nImageNet classification, respectively, and also improves the model robustness\nagainst adversarial perturbations. Furthermore, models that are trained with\nSaliencyMix help to improve the object detection performance. Source code is\navailable at https://github.com/SaliencyMix/SaliencyMix.",
          "link": "http://arxiv.org/abs/2006.01791",
          "publishedOn": "2021-07-28T02:02:34.744Z",
          "wordCount": 708,
          "title": "SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization. (arXiv:2006.01791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.14220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mostaani_A/0/1/0/all/0/1\">Arsham Mostaani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thang X. Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzinotas_S/0/1/0/all/0/1\">Symeon Chatzinotas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ottersten_B/0/1/0/all/0/1\">Bj&#xf6;rn Ottersten</a>",
          "description": "A collaborative task is assigned to a multiagent system (MAS) in which agents\nare allowed to communicate. The MAS runs over an underlying Markov decision\nprocess and its task is to maximize the averaged sum of discounted one-stage\nrewards. Although knowing the global state of the environment is necessary for\nthe optimal action selection of the MAS, agents are limited to individual\nobservations. The inter-agent communication can tackle the issue of local\nobservability, however, the limited rate of the inter-agent communication\nprevents the agent from acquiring the precise global state information. To\novercome this challenge, agents need to communicate their observations in a\ncompact way such that the MAS compromises the minimum possible sum of rewards.\nWe show that this problem is equivalent to a form of rate-distortion problem\nwhich we call the task-based information compression. We introduce a scheme for\ntask-based information compression titled State aggregation for information\ncompression (SAIC), for which a state aggregation algorithm is analytically\ndesigned. The SAIC is shown to be capable of achieving near-optimal performance\nin terms of the achieved sum of discounted rewards. The proposed algorithm is\napplied to a rendezvous problem and its performance is compared with several\nbenchmarks. Numerical experiments confirm the superiority of the proposed\nalgorithm.",
          "link": "http://arxiv.org/abs/2005.14220",
          "publishedOn": "2021-07-28T02:02:34.699Z",
          "wordCount": 696,
          "title": "Task-Based Information Compression for Multi-Agent Communication Problems with Channel Rate Constraints. (arXiv:2005.14220v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Secci_F/0/1/0/all/0/1\">Francesco Secci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1\">Andrea Ceccarelli</a>",
          "description": "RGB cameras are one of the most relevant sensors for autonomous driving\napplications. It is undeniable that failures of vehicle cameras may compromise\nthe autonomous driving task, possibly leading to unsafe behaviors when images\nthat are subsequently processed by the driving system are altered. To support\nthe definition of safe and robust vehicle architectures and intelligent\nsystems, in this paper we define the failure modes of a vehicle camera,\ntogether with an analysis of effects and known mitigations. Further, we build a\nsoftware library for the generation of the corresponding failed images and we\nfeed them to six object detectors for mono and stereo cameras and to the\nself-driving agent of an autonomous driving simulator. The resulting\nmisbehaviors with respect to operating with clean images allow a better\nunderstanding of failures effects and the related safety risks in image-based\napplications.",
          "link": "http://arxiv.org/abs/2008.05938",
          "publishedOn": "2021-07-28T02:02:34.652Z",
          "wordCount": 611,
          "title": "RGB cameras failures and their effects in autonomous driving applications. (arXiv:2008.05938v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12975",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Severin_B/0/1/0/all/0/1\">B. Severin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lennon_D/0/1/0/all/0/1\">D. T. Lennon</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Camenzind_L/0/1/0/all/0/1\">L. C. Camenzind</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Vigneau_F/0/1/0/all/0/1\">F. Vigneau</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Fedele_F/0/1/0/all/0/1\">F. Fedele</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Jirovec_D/0/1/0/all/0/1\">D. Jirovec</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ballabio_A/0/1/0/all/0/1\">A. Ballabio</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chrastina_D/0/1/0/all/0/1\">D. Chrastina</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Isella_G/0/1/0/all/0/1\">G. Isella</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kruijf_M/0/1/0/all/0/1\">M. de Kruijf</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Carballido_M/0/1/0/all/0/1\">M. J. Carballido</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Svab_S/0/1/0/all/0/1\">S. Svab</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kuhlmann_A/0/1/0/all/0/1\">A. V. Kuhlmann</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Braakman_F/0/1/0/all/0/1\">F. R. Braakman</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Geyer_S/0/1/0/all/0/1\">S. Geyer</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Froning_F/0/1/0/all/0/1\">F. N. M. Froning</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Moon_H/0/1/0/all/0/1\">H. Moon</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Osborne_M/0/1/0/all/0/1\">M. A. Osborne</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Sejdinovic_D/0/1/0/all/0/1\">D. Sejdinovic</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Katsaros_G/0/1/0/all/0/1\">G. Katsaros</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zumbuhl_D/0/1/0/all/0/1\">D. M. Zumb&#xfc;hl</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Briggs_G/0/1/0/all/0/1\">G. A. D. Briggs</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ares_N/0/1/0/all/0/1\">N. Ares</a>",
          "description": "The potential of Si and SiGe-based devices for the scaling of quantum\ncircuits is tainted by device variability. Each device needs to be tuned to\noperation conditions. We give a key step towards tackling this variability with\nan algorithm that, without modification, is capable of tuning a 4-gate Si\nFinFET, a 5-gate GeSi nanowire and a 7-gate SiGe heterostructure double quantum\ndot device from scratch. We achieve tuning times of 30, 10, and 92 minutes,\nrespectively. The algorithm also provides insight into the parameter space\nlandscape for each of these devices. These results show that overarching\nsolutions for the tuning of quantum devices are enabled by machine learning.",
          "link": "http://arxiv.org/abs/2107.12975",
          "publishedOn": "2021-07-28T02:02:34.625Z",
          "wordCount": 609,
          "title": "Cross-architecture Tuning of Silicon and SiGe-based Quantum Devices Using Machine Learning. (arXiv:2107.12975v1 [cond-mat.mes-hall])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silveira_B/0/1/0/all/0/1\">B&#xe1;rbara Silveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1\">Henrique S. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1\">Fabricio Murai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Ana Paula Couto da Silva</a>",
          "description": "In recent years, Online Social Networks have become an important medium for\npeople who suffer from mental disorders to share moments of hardship, and\nreceive emotional and informational support. In this work, we analyze how\ndiscussions in Reddit communities related to mental disorders can help improve\nthe health conditions of their users. Using the emotional tone of users'\nwriting as a proxy for emotional state, we uncover relationships between user\ninteractions and state changes. First, we observe that authors of negative\nposts often write rosier comments after engaging in discussions, indicating\nthat users' emotional state can improve due to social support. Second, we build\nmodels based on SOTA text embedding techniques and RNNs to predict shifts in\nemotional tone. This differs from most of related work, which focuses primarily\non detecting mental disorders from user activity. We demonstrate the\nfeasibility of accurately predicting the users' reactions to the interactions\nexperienced in these platforms, and present some examples which illustrate that\nthe models are correctly capturing the effects of comments on the author's\nemotional tone. Our models hold promising implications for interventions to\nprovide support for people struggling with mental illnesses.",
          "link": "http://arxiv.org/abs/2005.07473",
          "publishedOn": "2021-07-28T02:02:34.616Z",
          "wordCount": 698,
          "title": "Predicting User Emotional Tone in Mental Disorder Online Communities. (arXiv:2005.07473v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12416",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jing_G/0/1/0/all/0/1\">Gangshan Jing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_H/0/1/0/all/0/1\">He Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+George_J/0/1/0/all/0/1\">Jemin George</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakrabortty_A/0/1/0/all/0/1\">Aranya Chakrabortty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush K. Sharma</a>",
          "description": "Recently introduced distributed zeroth-order optimization (ZOO) algorithms\nhave shown their utility in distributed reinforcement learning (RL).\nUnfortunately, in the gradient estimation process, almost all of them require\nrandom samples with the same dimension as the global variable and/or require\nevaluation of the global cost function, which may induce high estimation\nvariance for large-scale networks. In this paper, we propose a novel\ndistributed zeroth-order algorithm by leveraging the network structure inherent\nin the optimization objective, which allows each agent to estimate its local\ngradient by local cost evaluation independently, without use of any consensus\nprotocol. The proposed algorithm exhibits an asynchronous update scheme, and is\ndesigned for stochastic non-convex optimization with a possibly non-convex\nfeasible domain based on the block coordinate descent method. The algorithm is\nlater employed as a distributed model-free RL algorithm for distributed linear\nquadratic regulator design, where a learning graph is designed to describe the\nrequired interaction relationship among agents in distributed learning. We\nprovide an empirical validation of the proposed algorithm to benchmark its\nperformance on convergence rate and variance against a centralized ZOO\nalgorithm.",
          "link": "http://arxiv.org/abs/2107.12416",
          "publishedOn": "2021-07-28T02:02:34.579Z",
          "wordCount": 641,
          "title": "Asynchronous Distributed Reinforcement Learning for LQR Control via Zeroth-Order Block Coordinate Descent. (arXiv:2107.12416v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Claudia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1\">Victor Veitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1\">David Blei</a>",
          "description": "The defining challenge for causal inference from observational data is the\npresence of `confounders', covariates that affect both treatment assignment and\nthe outcome. To address this challenge, practitioners collect and adjust for\nthe covariates, hoping that they adequately correct for confounding. However,\nincluding every observed covariate in the adjustment runs the risk of including\n`bad controls', variables that induce bias when they are conditioned on. The\nproblem is that we do not always know which variables in the covariate set are\nsafe to adjust for and which are not. To address this problem, we develop\nNearly Invariant Causal Estimation (NICE). NICE uses invariant risk\nminimization (IRM) [Arj19] to learn a representation of the covariates that,\nunder some assumptions, strips out bad controls but preserves sufficient\ninformation to adjust for confounding. Adjusting for the learned\nrepresentation, rather than the covariates themselves, avoids the induced bias\nand provides valid causal inferences. We evaluate NICE on both synthetic and\nsemi-synthetic data. When the covariates contain unknown collider variables and\nother bad controls, NICE performs better than adjusting for all the covariates.",
          "link": "http://arxiv.org/abs/2011.12379",
          "publishedOn": "2021-07-28T02:02:34.572Z",
          "wordCount": 637,
          "title": "Invariant Representation Learning for Treatment Effect Estimation. (arXiv:2011.12379v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_W/0/1/0/all/0/1\">Wendson A. S. Barbosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffith_A/0/1/0/all/0/1\">Aaron Griffith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowlands_G/0/1/0/all/0/1\">Graham E. Rowlands</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govia_L/0/1/0/all/0/1\">Luke C. G. Govia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeill_G/0/1/0/all/0/1\">Guilhem J. Ribeill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Hai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohki_T/0/1/0/all/0/1\">Thomas A. Ohki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1\">Daniel J. Gauthier</a>",
          "description": "We demonstrate that matching the symmetry properties of a reservoir computer\n(RC) to the data being processed dramatically increases its processing power.\nWe apply our method to the parity task, a challenging benchmark problem that\nhighlights inversion and permutation symmetries, and to a chaotic system\ninference task that presents an inversion symmetry rule. For the parity task,\nour symmetry-aware RC obtains zero error using an exponentially reduced neural\nnetwork and training data, greatly speeding up the time to result and\noutperforming hand crafted artificial neural networks. When both symmetries are\nrespected, we find that the network size $N$ necessary to obtain zero error for\n50 different RC instances scales linearly with the parity-order $n$. Moreover,\nsome symmetry-aware RC instances perform a zero error classification with only\n$N=1$ for $n\\leq7$. Furthermore, we show that a symmetry-aware RC only needs a\ntraining data set with size on the order of $(n+n/2)$ to obtain such\nperformance, an exponential reduction in comparison to a regular RC which\nrequires a training data set with size on the order of $n2^n$ to contain all\n$2^n$ possible $n-$bit-long sequences. For the inference task, we show that a\nsymmetry-aware RC presents a normalized root-mean-square error three\norders-of-magnitude smaller than regular RCs. For both tasks, our RC approach\nrespects the symmetries by adjusting only the input and the output layers, and\nnot by problem-based modifications to the neural network. We anticipate that\ngeneralizations of our procedure can be applied in information processing for\nproblems with known symmetries.",
          "link": "http://arxiv.org/abs/2102.00310",
          "publishedOn": "2021-07-28T02:02:34.553Z",
          "wordCount": 739,
          "title": "Symmetry-Aware Reservoir Computing. (arXiv:2102.00310v3 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.05759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Gil_D/0/1/0/all/0/1\">Diego Garc&#xed;a-Gil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1\">Salvador Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1\">Ning Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrera_F/0/1/0/all/0/1\">Francisco Herrera</a>",
          "description": "Differences in data size per class, also known as imbalanced data\ndistribution, have become a common problem affecting data quality. Big Data\nscenarios pose a new challenge to traditional imbalanced classification\nalgorithms, since they are not prepared to work with such amount of data. Split\ndata strategies and lack of data in the minority class due to the use of\nMapReduce paradigm have posed new challenges for tackling the imbalance between\nclasses in Big Data scenarios. Ensembles have shown to be able to successfully\naddress imbalanced data problems. Smart Data refers to data of enough quality\nto achieve high performance models. The combination of ensembles and Smart\nData, achieved through Big Data preprocessing, should be a great synergy. In\nthis paper, we propose a novel methodology based on Decision Trees Ensemble\nwith Smart Data for addressing the imbalanced classification problem in Big\nData domains, namely DeTE_SD methodology. This methodology is based on the\nlearning of different decision trees using distributed quality data for the\nensemble process. This quality data is achieved by fusing Random\nDiscretization, Principal Components Analysis and clustering-based Random\nOversampling for obtaining different Smart Data versions of the original data.\nExperiments carried out in 21 binary adapted datasets have shown that our\nmethodology outperforms Random Forest.",
          "link": "http://arxiv.org/abs/2001.05759",
          "publishedOn": "2021-07-28T02:02:34.537Z",
          "wordCount": 691,
          "title": "A Methodology guided by Decision Trees Ensemble and Smart Data for Imbalanced Big Data. (arXiv:2001.05759v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yuqing Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ricky Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Procedural content generation via machine learning (PCGML) is the process of\nprocedurally generating game content using models trained on existing game\ncontent. PCGML methods can struggle to capture the true variance present in\nunderlying data with a single model. In this paper, we investigated the use of\nensembles of Markov chains for procedurally generating \\emph{Mega Man} levels.\nWe conduct an initial investigation of our approach and evaluate it on measures\nof playability and stylistic similarity in comparison to a non-ensemble,\nexisting Markov chain approach.",
          "link": "http://arxiv.org/abs/2107.12524",
          "publishedOn": "2021-07-28T02:02:34.523Z",
          "wordCount": 540,
          "title": "Ensemble Learning For Mega Man Level Generation. (arXiv:2107.12524v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-07-28T02:02:34.503Z",
          "wordCount": 654,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12783",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Khurana_D/0/1/0/all/0/1\">Drona Khurana</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ravichandran_S/0/1/0/all/0/1\">Srinivasan Ravichandran</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jain_S/0/1/0/all/0/1\">Sparsh Jain</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Edakunni_N/0/1/0/all/0/1\">Narayanan Unny Edakunni</a>",
          "description": "A plug-in algorithm to estimate Bayes Optimal Classifiers for fairness-aware\nbinary classification has been proposed in (Menon & Williamson, 2018). However,\nthe statistical efficacy of their approach has not been established. We prove\nthat the plug-in algorithm is statistically consistent. We also derive finite\nsample guarantees associated with learning the Bayes Optimal Classifiers via\nthe plug-in algorithm. Finally, we propose a protocol that modifies the plug-in\napproach, so as to simultaneously guarantee fairness and differential privacy\nwith respect to a binary feature deemed sensitive.",
          "link": "http://arxiv.org/abs/2107.12783",
          "publishedOn": "2021-07-28T02:02:34.496Z",
          "wordCount": 532,
          "title": "Statistical Guarantees for Fairness Aware Plug-In Algorithms. (arXiv:2107.12783v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12102",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cartis_C/0/1/0/all/0/1\">Coralia Cartis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Massart_E/0/1/0/all/0/1\">Estelle Massart</a>, <a href=\"http://arxiv.org/find/math/1/au:+Otemissov_A/0/1/0/all/0/1\">Adilet Otemissov</a>",
          "description": "We propose a random-subspace algorithmic framework for global optimization of\nLipschitz-continuous objectives, and analyse its convergence using novel tools\nfrom conic integral geometry. X-REGO randomly projects, in a sequential or\nsimultaneous manner, the high-dimensional original problem into low-dimensional\nsubproblems that can then be solved with any global, or even local,\noptimization solver. We estimate the probability that the randomly-embedded\nsubproblem shares (approximately) the same global optimum as the original\nproblem. This success probability is then used to show convergence of X-REGO to\nan approximate global solution of the original problem, under weak assumptions\non the problem (having a strictly feasible global solution) and on the solver\n(guaranteed to find an approximate global solution of the reduced problem with\nsufficiently high probability). In the particular case of unconstrained\nobjectives with low effective dimension, that only vary over a low-dimensional\nsubspace, we propose an X-REGO variant that explores random subspaces of\nincreasing dimension until finding the effective dimension of the problem,\nleading to X-REGO globally converging after a finite number of embeddings,\nproportional to the effective dimension. We show numerically that this variant\nefficiently finds both the effective dimension and an approximate global\nminimizer of the original problem.",
          "link": "http://arxiv.org/abs/2107.12102",
          "publishedOn": "2021-07-28T02:02:34.490Z",
          "wordCount": 631,
          "title": "Global optimization using random embeddings. (arXiv:2107.12102v1 [math.OC] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shifeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>",
          "description": "Cross-speaker style transfer is crucial to the applications of multi-style\nand expressive speech synthesis at scale. It does not require the target\nspeakers to be experts in expressing all styles and to collect corresponding\nrecordings for model training. However, the performances of existing style\ntransfer methods are still far behind real application needs. The root causes\nare mainly twofold. Firstly, the style embedding extracted from single\nreference speech can hardly provide fine-grained and appropriate prosody\ninformation for arbitrary text to synthesize. Secondly, in these models the\ncontent/text, prosody, and speaker timbre are usually highly entangled, it's\ntherefore not realistic to expect a satisfied result when freely combining\nthese components, such as to transfer speaking style between speakers. In this\npaper, we propose a cross-speaker style transfer text-to-speech (TTS) model\nwith explicit prosody bottleneck. The prosody bottleneck builds up the kernels\naccounting for speaking style robustly, and disentangles the prosody from\ncontent and speaker timbre, therefore guarantees high quality cross-speaker\nstyle transfer. Evaluation result shows the proposed method even achieves\non-par performance with source speaker's speaker-dependent (SD) model in\nobjective measurement of prosody, and significantly outperforms the cycle\nconsistency and GMVAE-based baselines in objective and subjective evaluations.",
          "link": "http://arxiv.org/abs/2107.12562",
          "publishedOn": "2021-07-28T02:02:34.483Z",
          "wordCount": 640,
          "title": "Cross-speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis. (arXiv:2107.12562v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuzborskij_I/0/1/0/all/0/1\">Ilja Kuzborskij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivasplata_O/0/1/0/all/0/1\">Omar Rivasplata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rannen_Triki_A/0/1/0/all/0/1\">Amal Rannen-Triki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>",
          "description": "Empirically it has been observed that the performance of deep neural networks\nsteadily improves as we increase model size, contradicting the classical view\non overfitting and generalization. Recently, the double descent phenomena has\nbeen proposed to reconcile this observation with theory, suggesting that the\ntest error has a second descent when the model becomes sufficiently\noverparameterized, as the model size itself acts as an implicit regularizer. In\nthis paper we add to the growing body of work in this space, providing a\ncareful study of learning dynamics as a function of model size for the least\nsquares scenario. We show an excess risk bound for the gradient descent\nsolution of the least squares objective. The bound depends on the smallest\nnon-zero eigenvalue of the covariance matrix of the input features, via a\nfunctional form that has the double descent behavior. This gives a new\nperspective on the double descent curves reported in the literature. Our\nanalysis of the excess risk allows to decouple the effect of optimization and\ngeneralization error. In particular, we find that in case of noiseless\nregression, double descent is explained solely by optimization-related\nquantities, which was missed in studies focusing on the Moore-Penrose\npseudoinverse solution. We believe that our derivation provides an alternative\nview compared to existing work, shedding some light on a possible cause of this\nphenomena, at least in the considered least squares setting. We empirically\nexplore if our predictions hold for neural networks, in particular whether the\ncovariance of intermediary hidden activations has a similar behavior as the one\npredicted by our derivations.",
          "link": "http://arxiv.org/abs/2107.12685",
          "publishedOn": "2021-07-28T02:02:34.475Z",
          "wordCount": 711,
          "title": "On the Role of Optimization in Double Descent: A Least Squares Study. (arXiv:2107.12685v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04896",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Sakthivadivel_D/0/1/0/all/0/1\">Dalton A R Sakthivadivel</a>",
          "description": "We investigate how the activation function can be used to describe neural\nfiring in an abstract way, and in turn, why it works well in artificial neural\nnetworks. We discuss how a spike in a biological neurone belongs to a\nparticular universality class of phase transitions in statistical physics. We\nthen show that the artificial neurone is, mathematically, a mean field model of\nbiological neural membrane dynamics, which arises from modelling spiking as a\nphase transition. This allows us to treat selective neural firing in an\nabstract way, and formalise the role of the activation function in perceptron\nlearning. The resultant statistical physical model allows us to recover the\nexpressions for some known activation functions as various special cases. Along\nwith deriving this model and specifying the analogous neural case, we analyse\nthe phase transition to understand the physics of neural network learning.\nTogether, it is shown that there is not only a biological meaning, but a\nphysical justification, for the emergence and performance of typical activation\nfunctions; implications for neural learning and inference are also discussed.",
          "link": "http://arxiv.org/abs/2102.04896",
          "publishedOn": "2021-07-28T02:02:34.452Z",
          "wordCount": 649,
          "title": "Formalising the Use of the Activation Function in Neural Inference. (arXiv:2102.04896v2 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halle_A/0/1/0/all/0/1\">Alex Halle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campanile_L/0/1/0/all/0/1\">L. Flavio Campanile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasse_A/0/1/0/all/0/1\">Alexander Hasse</a>",
          "description": "Topology optimization is widely used by engineers during the initial product\ndevelopment process to get a first possible geometry design. The\nstate-of-the-art is the iterative calculation, which requires both time and\ncomputational power. Some newly developed methods use artificial intelligence\nto accelerate the topology optimization. These require conventionally\npre-optimized data and therefore are dependent on the quality and number of\navailable data. This paper proposes an AI-assisted design method for topology\noptimization, which does not require pre-optimized data. The designs are\nprovided by an artificial neural network, the predictor, on the basis of\nboundary conditions and degree of filling (the volume percentage filled by\nmaterial) as input data. In the training phase, geometries generated on the\nbasis of random input data are evaluated with respect to given criteria. The\nresults of those evaluations flow into an objective function which is minimized\nby adapting the predictor's parameters. After the training is completed, the\npresented AI-assisted design procedure supplies geometries which are similar to\nthe ones generated by conventional topology optimizers, but requires a small\nfraction of the computational effort required by those algorithms. We\nanticipate our paper to be a starting point for AI-based methods that requires\ndata, that is hard to compute or not available.",
          "link": "http://arxiv.org/abs/2012.06384",
          "publishedOn": "2021-07-28T02:02:34.444Z",
          "wordCount": 664,
          "title": "An AI-Assisted Design Method for Topology Optimization Without Pre-Optimized Training Data. (arXiv:2012.06384v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yejiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Weiming Xiang</a>",
          "description": "In this paper, a robust optimization framework is developed to train shallow\nneural networks based on reachability analysis of neural networks. To\ncharacterize noises of input data, the input training data is disturbed in the\ndescription of interval sets. Interval-based reachability analysis is then\nperformed for the hidden layer. With the reachability analysis results, a\nrobust optimization training method is developed in the framework of robust\nleast-square problems. Then, the developed robust least-square problem is\nrelaxed to a semidefinite programming problem. It has been shown that the\ndeveloped robust learning method can provide better robustness against\nperturbations at the price of loss of training accuracy to some extent. At\nlast, the proposed method is evaluated on a robot arm model learning example.",
          "link": "http://arxiv.org/abs/2107.12801",
          "publishedOn": "2021-07-28T02:02:34.437Z",
          "wordCount": 560,
          "title": "Robust Optimization Framework for Training Shallow Neural Networks Using Reachability Method. (arXiv:2107.12801v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bernini_N/0/1/0/all/0/1\">Nicola Bernini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessa_M/0/1/0/all/0/1\">Mikhail Bessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delmas_R/0/1/0/all/0/1\">R&#xe9;mi Delmas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gold_A/0/1/0/all/0/1\">Arthur Gold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goubault_E/0/1/0/all/0/1\">Eric Goubault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pennec_R/0/1/0/all/0/1\">Romain Pennec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putot_S/0/1/0/all/0/1\">Sylvie Putot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sillion_F/0/1/0/all/0/1\">Fran&#xe7;ois Sillion</a>",
          "description": "We explore the reinforcement learning approach to designing controllers by\nextensively discussing the case of a quadcopter attitude controller. We provide\nall details allowing to reproduce our approach, starting with a model of the\ndynamics of a crazyflie 2.0 under various nominal and non-nominal conditions,\nincluding partial motor failures and wind gusts. We develop a robust form of a\nsignal temporal logic to quantitatively evaluate the vehicle's behavior and\nmeasure the performance of controllers. The paper thoroughly describes the\nchoices in training algorithms, neural net architecture, hyperparameters,\nobservation space in view of the different performance metrics we have\nintroduced. We discuss the robustness of the obtained controllers, both to\npartial loss of power for one rotor and to wind gusts and finish by drawing\nconclusions on practical controller design by reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.12942",
          "publishedOn": "2021-07-28T02:02:34.428Z",
          "wordCount": 595,
          "title": "Reinforcement Learning with Formal Performance Metrics for Quadcopter Attitude Control under Non-nominal Contexts. (arXiv:2107.12942v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12421",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Talgorn_B/0/1/0/all/0/1\">Bastien Talgorn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Alarie_S/0/1/0/all/0/1\">St&#xe9;phane Alarie</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kokkolaras_M/0/1/0/all/0/1\">Michael Kokkolaras</a>",
          "description": "We consider computationally expensive blackbox optimization problems and\npresent a method that employs surrogate models and concurrent computing at the\nsearch step of the mesh adaptive direct search (MADS) algorithm. Specifically,\nwe solve a surrogate optimization problem using locally weighted scatterplot\nsmoothing (LOWESS) models to find promising candidate points to be evaluated by\nthe blackboxes. We consider several methods for selecting promising points from\na large number of points. We conduct numerical experiments to assess the\nperformance of the modified MADS algorithm with respect to available CPU\nresources by means of five engineering design problems.",
          "link": "http://arxiv.org/abs/2107.12421",
          "publishedOn": "2021-07-28T02:02:34.421Z",
          "wordCount": 528,
          "title": "Parallel Surrogate-assisted Optimization Using Mesh Adaptive Direct Search. (arXiv:2107.12421v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2004.06230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiayu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1\">Emma Brunskill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weiwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_S/0/1/0/all/0/1\">Susan Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1\">Finale Doshi-Velez</a>",
          "description": "Contextual bandits often provide simple and effective personalization in\ndecision making problems, making them popular tools to deliver personalized\ninterventions in mobile health as well as other health applications. However,\nwhen bandits are deployed in the context of a scientific study -- e.g. a\nclinical trial to test if a mobile health intervention is effective -- the aim\nis not only to personalize for an individual, but also to determine, with\nsufficient statistical power, whether or not the system's intervention is\neffective. It is essential to assess the effectiveness of the intervention\nbefore broader deployment for better resource allocation. The two objectives\nare often deployed under different model assumptions, making it hard to\ndetermine how achieving the personalization and statistical power affect each\nother. In this work, we develop general meta-algorithms to modify existing\nalgorithms such that sufficient power is guaranteed while still improving each\nuser's well-being. We also demonstrate that our meta-algorithms are robust to\nvarious model mis-specifications possibly appearing in statistical studies,\nthus providing a valuable tool to study designers.",
          "link": "http://arxiv.org/abs/2004.06230",
          "publishedOn": "2021-07-28T02:02:34.395Z",
          "wordCount": 651,
          "title": "Power Constrained Bandits. (arXiv:2004.06230v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gothankar_R/0/1/0/all/0/1\">Ruchira Gothankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1\">Fabio Di Troia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "YouTube videos often include captivating descriptions and intriguing\nthumbnails designed to increase the number of views, and thereby increase the\nrevenue for the person who posted the video. This creates an incentive for\npeople to post clickbait videos, in which the content might deviate\nsignificantly from the title, description, or thumbnail. In effect, users are\ntricked into clicking on clickbait videos. In this research, we consider the\nchallenging problem of detecting clickbait YouTube videos. We experiment with\nmultiple state-of-the-art machine learning techniques using a variety of\ntextual features.",
          "link": "http://arxiv.org/abs/2107.12791",
          "publishedOn": "2021-07-28T02:02:34.388Z",
          "wordCount": 517,
          "title": "Clickbait Detection in YouTube Videos. (arXiv:2107.12791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haibo Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy M. Hospedales</a>",
          "description": "Federated learning (FL) enables distributed participants to collectively\nlearn a strong global model without sacrificing their individual data privacy.\nMainstream FL approaches require each participant to share a common network\narchitecture and further assume that data are are sampled IID across\nparticipants. However, in real-world deployments participants may require\nheterogeneous network architectures; and the data distribution is almost\ncertainly non-uniform across participants. To address these issues we introduce\nFedH2L, which is agnostic to both the model architecture and robust to\ndifferent data distributions across participants. In contrast to approaches\nsharing parameters or gradients, FedH2L relies on mutual distillation,\nexchanging only posteriors on a shared seed set between participants in a\ndecentralized manner. This makes it extremely bandwidth efficient, model\nagnostic, and crucially produces models capable of performing well on the whole\ndata distribution when learning from heterogeneous silos.",
          "link": "http://arxiv.org/abs/2101.11296",
          "publishedOn": "2021-07-28T02:02:34.373Z",
          "wordCount": 615,
          "title": "FedH2L: Federated Learning with Model and Statistical Heterogeneity. (arXiv:2101.11296v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Solares_J/0/1/0/all/0/1\">Jose Roberto Ayala Solares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yajie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassaine_A/0/1/0/all/0/1\">Abdelaali Hassaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Shishir Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamouei_M/0/1/0/all/0/1\">Mohammad Mamouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canoy_D/0/1/0/all/0/1\">Dexter Canoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_K/0/1/0/all/0/1\">Kazem Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimi_Khorshidi_G/0/1/0/all/0/1\">Gholamreza Salimi-Khorshidi</a>",
          "description": "Deep learning models have shown tremendous potential in learning\nrepresentations, which are able to capture some key properties of the data.\nThis makes them great candidates for transfer learning: Exploiting\ncommonalities between different learning tasks to transfer knowledge from one\ntask to another. Electronic health records (EHR) research is one of the domains\nthat has witnessed a growing number of deep learning techniques employed for\nlearning clinically-meaningful representations of medical concepts (such as\ndiseases and medications). Despite this growth, the approaches to benchmark and\nassess such learned representations (or, embeddings) is under-investigated;\nthis can be a big issue when such embeddings are shared to facilitate transfer\nlearning. In this study, we aim to (1) train some of the most prominent disease\nembedding techniques on a comprehensive EHR data from 3.1 million patients, (2)\nemploy qualitative and quantitative evaluation techniques to assess these\nembeddings, and (3) provide pre-trained disease embeddings for transfer\nlearning. This study can be the first comprehensive approach for clinical\nconcept embedding evaluation and can be applied to any embedding techniques and\nfor any EHR concept.",
          "link": "http://arxiv.org/abs/2107.12919",
          "publishedOn": "2021-07-28T02:02:34.352Z",
          "wordCount": 623,
          "title": "Transfer Learning in Electronic Health Records through Clinical Concept Embedding. (arXiv:2107.12919v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12775",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Che_H/0/1/0/all/0/1\">Hui Che</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramanathan_S/0/1/0/all/0/1\">Sumana Ramanathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Foran_D/0/1/0/all/0/1\">David Foran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nosher_J/0/1/0/all/0/1\">John L Nosher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hacihaliloglu_I/0/1/0/all/0/1\">Ilker Hacihaliloglu</a>",
          "description": "With the success of deep learning-based methods applied in medical image\nanalysis, convolutional neural networks (CNNs) have been investigated for\nclassifying liver disease from ultrasound (US) data. However, the scarcity of\navailable large-scale labeled US data has hindered the success of CNNs for\nclassifying liver disease from US data. In this work, we propose a novel\ngenerative adversarial network (GAN) architecture for realistic diseased and\nhealthy liver US image synthesis. We adopt the concept of stacking to\nsynthesize realistic liver US data. Quantitative and qualitative evaluation is\nperformed on 550 in-vivo B-mode liver US images collected from 55 subjects. We\nalso show that the synthesized images, together with real in vivo data, can be\nused to significantly improve the performance of traditional CNN architectures\nfor Nonalcoholic fatty liver disease (NAFLD) classification.",
          "link": "http://arxiv.org/abs/2107.12775",
          "publishedOn": "2021-07-28T02:02:34.345Z",
          "wordCount": 598,
          "title": "Realistic Ultrasound Image Synthesis for Improved Classification of Liver Disease. (arXiv:2107.12775v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Robin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>",
          "description": "Deep neural networks (DNNs) for the semantic segmentation of images are\nusually trained to operate on a predefined closed set of object classes. This\nis in contrast to the \"open world\" setting where DNNs are envisioned to be\ndeployed to. From a functional safety point of view, the ability to detect\nso-called \"out-of-distribution\" (OoD) samples, i.e., objects outside of a DNN's\nsemantic space, is crucial for many applications such as automated driving. A\nnatural baseline approach to OoD detection is to threshold on the pixel-wise\nsoftmax entropy. We present a two-step procedure that significantly improves\nthat approach. Firstly, we utilize samples from the COCO dataset as OoD proxy\nand introduce a second training objective to maximize the softmax entropy on\nthese samples. Starting from pretrained semantic segmentation networks we\nre-train a number of DNNs on different in-distribution datasets and\nconsistently observe improved OoD detection performance when evaluating on\ncompletely disjoint OoD datasets. Secondly, we perform a transparent\npost-processing step to discard false positive OoD samples by so-called \"meta\nclassification\". To this end, we apply linear models to a set of hand-crafted\nmetrics derived from the DNN's softmax probabilities. In our experiments we\nconsistently observe a clear additional gain in OoD detection performance,\ncutting down the number of detection errors by up to 52% when comparing the\nbest baseline with our results. We achieve this improvement sacrificing only\nmarginally in original segmentation performance. Therefore, our method\ncontributes to safer DNNs with more reliable overall system performance.",
          "link": "http://arxiv.org/abs/2012.06575",
          "publishedOn": "2021-07-28T02:02:34.339Z",
          "wordCount": 728,
          "title": "Entropy Maximization and Meta Classification for Out-Of-Distribution Detection in Semantic Segmentation. (arXiv:2012.06575v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08158",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gadermayr_M/0/1/0/all/0/1\">Michael Gadermayr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tschuchnig_M/0/1/0/all/0/1\">Maximilian Tschuchnig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stangassinger_L/0/1/0/all/0/1\">Lea Maria Stangassinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreutzer_C/0/1/0/all/0/1\">Christina Kreutzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Couillard_Despres_S/0/1/0/all/0/1\">Sebastien Couillard-Despres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oostingh_G/0/1/0/all/0/1\">Gertie Janneke Oostingh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hittmair_A/0/1/0/all/0/1\">Anton Hittmair</a>",
          "description": "In contrast to paraffin sections, frozen sections can be quickly generated\nduring surgical interventions. This procedure allows surgeons to wait for\nhistological findings during the intervention to base intra-operative decisions\non the outcome of the histology. However, compared to paraffin sections, the\nquality of frozen sections is typically lower, leading to a higher ratio of\nmiss-classification. In this work, we investigated the effect of the section\ntype on automated decision support approaches for classification of thyroid\ncancer. This was enabled by a data set consisting of pairs of sections for\nindividual patients. Moreover, we investigated, whether a frozen-to-paraffin\ntranslation could help to optimize classification scores. Finally, we propose a\nspecific data augmentation strategy to deal with a small amount of training\ndata and to increase classification accuracy even further.",
          "link": "http://arxiv.org/abs/2012.08158",
          "publishedOn": "2021-07-28T02:02:34.332Z",
          "wordCount": 622,
          "title": "Frozen-to-Paraffin: Categorization of Histological Frozen Sections by the Aid of Paraffin Sections and Generative Adversarial Networks. (arXiv:2012.08158v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.",
          "link": "http://arxiv.org/abs/2107.12651",
          "publishedOn": "2021-07-28T02:02:34.285Z",
          "wordCount": 604,
          "title": "Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Chung-Wei Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yakimenka_Y/0/1/0/all/0/1\">Yauhen Yakimenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsuan-Yin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosnes_E/0/1/0/all/0/1\">Eirik Rosnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kliewer_J/0/1/0/all/0/1\">Joerg Kliewer</a>",
          "description": "We propose to extend the concept of private information retrieval by allowing\nfor distortion in the retrieval process and relaxing the perfect privacy\nrequirement at the same time. In particular, we study the tradeoff between\ndownload rate, distortion, and user privacy leakage, and show that in the limit\nof large file sizes this trade-off can be captured via a novel\ninformation-theoretical formulation for datasets with a known distribution.\nMoreover, for scenarios where the statistics of the dataset is unknown, we\npropose a new deep learning framework by leveraging a generative adversarial\nnetwork approach, which allows the user to learn efficient schemes from the\ndata itself, minimizing the download cost. We evaluate the performance of the\nscheme on a synthetic Gaussian dataset as well as on both the MNIST and\nCIFAR-10 datasets. For the MNIST dataset, the data-driven approach\nsignificantly outperforms a non-learning based scheme which combines source\ncoding with multiple file download, while the CIFAR-10 performance is notably\nbetter.",
          "link": "http://arxiv.org/abs/2012.03902",
          "publishedOn": "2021-07-28T02:02:34.277Z",
          "wordCount": 658,
          "title": "Generative Adversarial User Privacy in Lossy Single-Server Information Retrieval. (arXiv:2012.03902v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tingting Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1\">Ramy E. Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_H/0/1/0/all/0/1\">Hanieh Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangwani_T/0/1/0/all/0/1\">Tynan Gangwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annavaram_M/0/1/0/all/0/1\">Murali Annavaram</a>",
          "description": "Stragglers, Byzantine workers, and data privacy are the main bottlenecks in\ndistributed cloud computing. Several prior works proposed coded computing\nstrategies to jointly address all three challenges. They require either a large\nnumber of workers, a significant communication cost or a significant\ncomputational complexity to tolerate malicious workers. Much of the overhead in\nprior schemes comes from the fact that they tightly couple coding for all three\nproblems into a single framework. In this work, we propose Verifiable Coded\nComputing (VCC) framework that decouples Byzantine node detection challenge\nfrom the straggler tolerance. VCC leverages coded computing just for handling\nstragglers and privacy, and then uses an orthogonal approach of verifiable\ncomputing to tackle Byzantine nodes. Furthermore, VCC dynamically adapts its\ncoding scheme to tradeoff straggler tolerance with Byzantine protection and\nvice-versa. We evaluate VCC on compute intensive distributed logistic\nregression application. Our experiments show that VCC speeds up the\nconventional uncoded implementation of distributed logistic regression by\n$3.2\\times-6.9\\times$, and also improves the test accuracy by up to $12.6\\%$.",
          "link": "http://arxiv.org/abs/2107.12958",
          "publishedOn": "2021-07-28T02:02:34.258Z",
          "wordCount": 630,
          "title": "Verifiable Coded Computing: Towards Fast, Secure and Private Distributed Machine Learning. (arXiv:2107.12958v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghojogh_B/0/1/0/all/0/1\">Benyamin Ghojogh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crowley_M/0/1/0/all/0/1\">Mark Crowley</a>",
          "description": "This is a tutorial and survey paper on Boltzmann Machine (BM), Restricted\nBoltzmann Machine (RBM), and Deep Belief Network (DBN). We start with the\nrequired background on probabilistic graphical models, Markov random field,\nGibbs sampling, statistical physics, Ising model, and the Hopfield network.\nThen, we introduce the structures of BM and RBM. The conditional distributions\nof visible and hidden variables, Gibbs sampling in RBM for generating\nvariables, training BM and RBM by maximum likelihood estimation, and\ncontrastive divergence are explained. Then, we discuss different possible\ndiscrete and continuous distributions for the variables. We introduce\nconditional RBM and how it is trained. Finally, we explain deep belief network\nas a stack of RBM models. This paper on Boltzmann machines can be useful in\nvarious fields including data science, statistics, neural computation, and\nstatistical physics.",
          "link": "http://arxiv.org/abs/2107.12521",
          "publishedOn": "2021-07-28T02:02:34.248Z",
          "wordCount": 604,
          "title": "Restricted Boltzmann Machine and Deep Belief Network: Tutorial and Survey. (arXiv:2107.12521v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ittner_J/0/1/0/all/0/1\">Jan Ittner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolikowski_L/0/1/0/all/0/1\">Lukasz Bolikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemker_K/0/1/0/all/0/1\">Konstantin Hemker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_R/0/1/0/all/0/1\">Ricardo Kennedy</a>",
          "description": "We offer a new formalism for global explanations of pairwise feature\ndependencies and interactions in supervised models. Building upon SHAP values\nand SHAP interaction values, our approach decomposes feature contributions into\nsynergistic, redundant and independent components (S-R-I decomposition of SHAP\nvectors). We propose a geometric interpretation of the components and formally\nprove its basic properties. Finally, we demonstrate the utility of synergy,\nredundancy and independence by applying them to a constructed data set and\nmodel.",
          "link": "http://arxiv.org/abs/2107.12436",
          "publishedOn": "2021-07-28T02:02:34.232Z",
          "wordCount": 523,
          "title": "Feature Synergy, Redundancy, and Independence in Global Model Explanations using SHAP Vector Decomposition. (arXiv:2107.12436v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.05754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baudry_D/0/1/0/all/0/1\">Dorian Baudry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautron_R/0/1/0/all/0/1\">Romain Gautron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufmann_E/0/1/0/all/0/1\">Emilie Kaufmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_O/0/1/0/all/0/1\">Odalric-Ambryn Maillard</a>",
          "description": "In this paper we study a multi-arm bandit problem in which the quality of\neach arm is measured by the Conditional Value at Risk (CVaR) at some level\nalpha of the reward distribution. While existing works in this setting mainly\nfocus on Upper Confidence Bound algorithms, we introduce a new Thompson\nSampling approach for CVaR bandits on bounded rewards that is flexible enough\nto solve a variety of problems grounded on physical resources. Building on a\nrecent work by Riou & Honda (2020), we introduce B-CVTS for continuous bounded\nrewards and M-CVTS for multinomial distributions. On the theoretical side, we\nprovide a non-trivial extension of their analysis that enables to theoretically\nbound their CVaR regret minimization performance. Strikingly, our results show\nthat these strategies are the first to provably achieve asymptotic optimality\nin CVaR bandits, matching the corresponding asymptotic lower bounds for this\nsetting. Further, we illustrate empirically the benefit of Thompson Sampling\napproaches both in a realistic environment simulating a use-case in agriculture\nand on various synthetic examples.",
          "link": "http://arxiv.org/abs/2012.05754",
          "publishedOn": "2021-07-28T02:02:34.225Z",
          "wordCount": 637,
          "title": "Optimal Thompson Sampling strategies for support-aware CVaR bandits. (arXiv:2012.05754v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1711.03639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1\">Thodoris Lykouris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1\">Karthik Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tardos_E/0/1/0/all/0/1\">Eva Tardos</a>",
          "description": "We consider the problem of adversarial (non-stochastic) online learning with\npartial information feedback, where at each round, a decision maker selects an\naction from a finite set of alternatives. We develop a black-box approach for\nsuch problems where the learner observes as feedback only losses of a subset of\nthe actions that includes the selected action. When losses of actions are\nnon-negative, under the graph-based feedback model introduced by Mannor and\nShamir, we offer algorithms that attain the so called \"small-loss\" $o(\\alpha\nL^{\\star})$ regret bounds with high probability, where $\\alpha$ is the\nindependence number of the graph, and $L^{\\star}$ is the loss of the best\naction. Prior to our work, there was no data-dependent guarantee for general\nfeedback graphs even for pseudo-regret (without dependence on the number of\nactions, i.e. utilizing the increased information feedback). Taking advantage\nof the black-box nature of our technique, we extend our results to many other\napplications such as semi-bandits (including routing in networks), contextual\nbandits (even with an infinite comparator class), as well as learning with\nslowly changing (shifting) comparators.\n\nIn the special case of classical bandit and semi-bandit problems, we provide\noptimal small-loss, high-probability guarantees of\n$\\tilde{O}(\\sqrt{dL^{\\star}})$ for actual regret, where $d$ is the number of\nactions, answering open questions of Neu. Previous bounds for bandits and\nsemi-bandits were known only for pseudo-regret and only in expectation. We also\noffer an optimal $\\tilde{O}(\\sqrt{\\kappa L^{\\star}})$ regret guarantee for\nfixed feedback graphs with clique-partition number at most $\\kappa$.",
          "link": "http://arxiv.org/abs/1711.03639",
          "publishedOn": "2021-07-28T02:02:34.208Z",
          "wordCount": 765,
          "title": "Small-loss bounds for online learning with partial information. (arXiv:1711.03639v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.07450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tingyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenbing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1\">Somayeh Sojoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>",
          "description": "Variants of Graph Neural Networks (GNNs) for representation learning have\nbeen proposed recently and achieved fruitful results in various fields. Among\nthem, Graph Attention Network (GAT) first employs a self-attention strategy to\nlearn attention weights for each edge in the spatial domain. However, learning\nthe attentions over edges can only focus on the local information of graphs and\ngreatly increases the computational costs. In this paper, we first introduce\nthe attention mechanism in the spectral domain of graphs and present Spectral\nGraph Attention Network (SpGAT) that learns representations for different\nfrequency components regarding weighted filters and graph wavelets bases. In\nthis way, SpGAT can better capture global patterns of graphs in an efficient\nmanner with much fewer learned parameters than that of GAT. Further, to reduce\nthe computational cost of SpGAT brought by the eigen-decomposition, we propose\na fast approximation variant SpGAT-Cheby. We thoroughly evaluate the\nperformance of SpGAT and SpGAT-Cheby in semi-supervised node classification\ntasks and verify the effectiveness of the learned attentions in the spectral\ndomain.",
          "link": "http://arxiv.org/abs/2003.07450",
          "publishedOn": "2021-07-28T02:02:34.201Z",
          "wordCount": 655,
          "title": "Spectral Graph Attention Network with Fast Eigen-approximation. (arXiv:2003.07450v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonet_D/0/1/0/all/0/1\">David Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_A/0/1/0/all/0/1\">Antonio Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Hidalgo_J/0/1/0/all/0/1\">Javier Ruiz-Hidalgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekkizhar_S/0/1/0/all/0/1\">Sarath Shekkizhar</a>",
          "description": "State-of-the-art neural network architectures continue to scale in size and\ndeliver impressive generalization results, although this comes at the expense\nof limited interpretability. In particular, a key challenge is to determine\nwhen to stop training the model, as this has a significant impact on\ngeneralization. Convolutional neural networks (ConvNets) comprise\nhigh-dimensional feature spaces formed by the aggregation of multiple channels,\nwhere analyzing intermediate data representations and the model's evolution can\nbe challenging owing to the curse of dimensionality. We present channel-wise\nDeepNNK (CW-DeepNNK), a novel channel-wise generalization estimate based on\nnon-negative kernel regression (NNK) graphs with which we perform local\npolytope interpolation on low-dimensional channels. This method leads to\ninstance-based interpretability of both the learned data representations and\nthe relationship between channels. Motivated by our observations, we use\nCW-DeepNNK to propose a novel early stopping criterion that (i) does not\nrequire a validation set, (ii) is based on a task performance metric, and (iii)\nallows stopping to be reached at different points for each channel. Our\nexperiments demonstrate that our proposed method has advantages as compared to\nthe standard criterion based on validation set performance.",
          "link": "http://arxiv.org/abs/2107.12972",
          "publishedOn": "2021-07-28T02:02:34.193Z",
          "wordCount": 631,
          "title": "Channel-Wise Early Stopping without a Validation Set via NNK Polytope Interpolation. (arXiv:2107.12972v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekarchi_Z/0/1/0/all/0/1\">Zahra Shekarchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_S/0/1/0/all/0/1\">Suzanne Stevenson</a>",
          "description": "Children learn word meanings by tapping into the commonalities across\ndifferent situations in which words are used and overcome the high level of\nuncertainty involved in early word learning experiences. We propose a modeling\nframework to investigate the role of mutual exclusivity bias - asserting\none-to-one mappings between words and their meanings - in reducing uncertainty\nin word learning. In a set of computational studies, we show that to\nsuccessfully learn word meanings in the face of uncertainty, a learner needs to\nuse two types of competition: words competing for association to a referent\nwhen learning from an observation and referents competing for a word when the\nword is used. Our work highlights the importance of an algorithmic-level\nanalysis to shed light on the utility of different mechanisms that can\nimplement the same computational-level theory.",
          "link": "http://arxiv.org/abs/2012.03370",
          "publishedOn": "2021-07-28T02:02:34.176Z",
          "wordCount": 619,
          "title": "Competition in Cross-situational Word Learning: A Computational Study. (arXiv:2012.03370v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cesaire_M/0/1/0/all/0/1\">Manon C&#xe9;saire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajri_H/0/1/0/all/0/1\">Hatem Hajri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>",
          "description": "This paper introduces stochastic sparse adversarial attacks (SSAA), simple,\nfast and purely noise-based targeted and untargeted $L_0$ attacks of neural\nnetwork classifiers (NNC). SSAA are devised by exploiting a simple small-time\nexpansion idea widely used for Markov processes and offer new examples of $L_0$\nattacks whose studies have been limited. They are designed to solve the known\nscalability issue of the family of Jacobian-based saliency maps attacks to\nlarge datasets and they succeed in solving it. Experiments on small and large\ndatasets (CIFAR-10 and ImageNet) illustrate further advantages of SSAA in\ncomparison with the-state-of-the-art methods. For instance, in the untargeted\ncase, our method called Voting Folded Gaussian Attack (VFGA) scales efficiently\nto ImageNet and achieves a significantly lower $L_0$ score than SparseFool (up\nto $\\frac{2}{5}$ lower) while being faster. Moreover, VFGA achieves better\n$L_0$ scores on ImageNet than Sparse-RS when both attacks are fully successful\non a large number of samples. Codes are publicly available through the link\nhttps://github.com/SSAA3/stochastic-sparse-adv-attacks",
          "link": "http://arxiv.org/abs/2011.12423",
          "publishedOn": "2021-07-28T02:02:34.166Z",
          "wordCount": 644,
          "title": "Stochastic sparse adversarial attacks. (arXiv:2011.12423v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Archit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1\">Karol Hausman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "Reinforcement learning (RL) promises to enable autonomous acquisition of\ncomplex behaviors for diverse agents. However, the success of current\nreinforcement learning algorithms is predicated on an often under-emphasised\nrequirement -- each trial needs to start from a fixed initial state\ndistribution. Unfortunately, resetting the environment to its initial state\nafter each trial requires substantial amount of human supervision and extensive\ninstrumentation of the environment which defeats the purpose of autonomous\nreinforcement learning. In this work, we propose Value-accelerated Persistent\nReinforcement Learning (VaPRL), which generates a curriculum of initial states\nsuch that the agent can bootstrap on the success of easier tasks to efficiently\nlearn harder tasks. The agent also learns to reach the initial states proposed\nby the curriculum, minimizing the reliance on human interventions into the\nlearning. We observe that VaPRL reduces the interventions required by three\norders of magnitude compared to episodic RL while outperforming prior\nstate-of-the art methods for reset-free RL both in terms of sample efficiency\nand asymptotic performance on a variety of simulated robotics problems.",
          "link": "http://arxiv.org/abs/2107.12931",
          "publishedOn": "2021-07-28T02:02:34.146Z",
          "wordCount": 605,
          "title": "Persistent Reinforcement Learning via Subgoal Curricula. (arXiv:2107.12931v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12869",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Duong_Q/0/1/0/all/0/1\">Quan Duong</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tran_T/0/1/0/all/0/1\">Tan Tran</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pham_D/0/1/0/all/0/1\">Duc-Thinh Pham</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mai_A/0/1/0/all/0/1\">An Mai</a>",
          "description": "The volume of flight traffic gets increasing over the time, which makes the\nstrategic traffic flow management become one of the challenging problems since\nit requires a lot of computational resources to model entire traffic data. On\nthe other hand, Automatic Dependent Surveillance - Broadcast (ADS-B) technology\nhas been considered as a promising data technology to provide both flight crews\nand ground control staff the necessary information safely and efficiently about\nthe position and velocity of the airplanes in a specific area. In the attempt\nto tackle this problem, we presented in this paper a simplified framework that\ncan support to detect the typical air routes between airports based on ADS-B\ndata. Specifically, the flight traffic will be classified into major groups\nbased on similarity measures, which helps to reduce the number of flight paths\nbetween airports. As a matter of fact, our framework can be taken into account\nto reduce practically the computational cost for air flow optimization and\nevaluate the operational performance. Finally, in order to illustrate the\npotential applications of our proposed framework, an experiment was performed\nusing ADS-B traffic flight data of three different pairs of airports. The\ndetected typical routes between each couple of airports show promising results\nby virtue of combining two indices for measuring the clustering performance and\nincorporating human judgment into the visual inspection.",
          "link": "http://arxiv.org/abs/2107.12869",
          "publishedOn": "2021-07-28T02:02:34.139Z",
          "wordCount": 681,
          "title": "A Simplified Framework for Air Route Clustering Based on ADS-B Data. (arXiv:2107.12869v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12970",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifeng Zhao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_P/0/1/0/all/0/1\">Pei Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Galindo_Torres_S/0/1/0/all/0/1\">S.A. Galindo-Torres</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "Longitudinal Dispersion(LD) is the dominant process of scalar transport in\nnatural streams. An accurate prediction on LD coefficient(Dl) can produce a\nperformance leap in related simulation. The emerging machine learning(ML)\ntechniques provide a self-adaptive tool for this problem. However, most of the\nexisting studies utilize an unproved quaternion feature set, obtained through\nsimple theoretical deduction. Few studies have put attention on its reliability\nand rationality. Besides, due to the lack of comparative comparison, the proper\nchoice of ML models in different scenarios still remains unknown. In this\nstudy, the Feature Gradient selector was first adopted to distill the local\noptimal feature sets directly from multivariable data. Then, a global optimal\nfeature set (the channel width, the flow velocity, the channel slope and the\ncross sectional area) was proposed through numerical comparison of the\ndistilled local optimums in performance with representative ML models. The\nchannel slope is identified to be the key parameter for the prediction of LDC.\nFurther, we designed a weighted evaluation metric which enables comprehensive\nmodel comparison. With the simple linear model as the baseline, a benchmark of\nsingle and ensemble learning models was provided. Advantages and disadvantages\nof the methods involved were also discussed. Results show that the support\nvector machine has significantly better performance than other models. Decision\ntree is not suitable for this problem due to poor generalization ability.\nNotably, simple models show superiority over complicated model on this\nlow-dimensional problem, for their better balance between regression and\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.12970",
          "publishedOn": "2021-07-28T02:02:34.133Z",
          "wordCount": 691,
          "title": "A Data-driven feature selection and machine-learning model benchmark for the prediction of longitudinal dispersion coefficient. (arXiv:2107.12970v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12978",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1\">Justin Szeto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>",
          "description": "There are many clinical contexts which require accurate detection and\nsegmentation of all focal pathologies (e.g. lesions, tumours) in patient\nimages. In cases where there are a mix of small and large lesions, standard\nbinary cross entropy loss will result in better segmentation of large lesions\nat the expense of missing small ones. Adjusting the operating point to\naccurately detect all lesions generally leads to oversegmentation of large\nlesions. In this work, we propose a novel reweighing strategy to eliminate this\nperformance gap, increasing small pathology detection performance while\nmaintaining segmentation accuracy. We show that our reweighing strategy vastly\noutperforms competing strategies based on experiments on a large scale,\nmulti-scanner, multi-center dataset of Multiple Sclerosis patient images.",
          "link": "http://arxiv.org/abs/2107.12978",
          "publishedOn": "2021-07-28T02:02:34.125Z",
          "wordCount": 584,
          "title": "Optimizing Operating Points for High Performance Lesion Detection and Segmentation Using Lesion Size Reweighting. (arXiv:2107.12978v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sommer_D/0/1/0/all/0/1\">David M. Sommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abfalterer_L/0/1/0/all/0/1\">Lukas Abfalterer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zingg_S/0/1/0/all/0/1\">Sheila Zingg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_E/0/1/0/all/0/1\">Esfandiar Mohammadi</a>",
          "description": "Differentially private (DP) mechanisms face the challenge of providing\naccurate results while protecting their inputs: the privacy-utility trade-off.\nA simple but powerful technique for DP adds noise to sensitivity-bounded query\noutputs to blur the exact query output: additive mechanisms. While a vast body\nof work considers infinitely wide noise distributions, some applications (e.g.,\nreal-time operating systems) require hard bounds on the deviations from the\nreal query, and only limited work on such mechanisms exist. An additive\nmechanism with truncated noise (i.e., with bounded range) can offer such hard\nbounds. We introduce a gradient-descent-based tool to learn truncated noise for\nadditive mechanisms with strong utility bounds while simultaneously optimizing\nfor differential privacy under sequential composition, i.e., scenarios where\nmultiple noisy queries on the same data are revealed. Our method can learn\ndiscrete noise patterns and not only hyper-parameters of a predefined\nprobability distribution. For sensitivity bounded mechanisms, we show that it\nis sufficient to consider symmetric and that\\new{, for from the mean\nmonotonically falling noise,} ensuring privacy for a pair of representative\nquery outputs guarantees privacy for all pairs of inputs (that differ in one\nelement). We find that the utility-privacy trade-off curves of our generated\nnoise are remarkably close to truncated Gaussians and even replicate their\nshape for $l_2$ utility-loss. For a low number of compositions, we also\nimproved DP-SGD (sub-sampling). Moreover, we extend Moments Accountant to\ntruncated distributions, allowing to incorporate mechanism output events with\nvarying input-dependent zero occurrence probability.",
          "link": "http://arxiv.org/abs/2107.12957",
          "publishedOn": "2021-07-28T02:02:34.118Z",
          "wordCount": 686,
          "title": "Learning Numeric Optimal Differentially Private Truncated Additive Mechanisms. (arXiv:2107.12957v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kawakami_H/0/1/0/all/0/1\">Hiroki Kawakami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_H/0/1/0/all/0/1\">Hirohisa Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Keisuke Sugiura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1\">Hiroki Matsutani</a>",
          "description": "Although high-performance deep neural networks are in high demand in edge\nenvironments, computation resources are strictly limited in edge devices, and\nlight-weight neural network techniques, such as Depthwise Separable Convolution\n(DSC), have been developed. ResNet is one of conventional deep neural network\nmodels that stack a lot of layers and parameters for a higher accuracy. To\nreduce the parameter size of ResNet, by utilizing a similarity to ODE (Ordinary\nDifferential Equation), Neural ODE repeatedly uses most of weight parameters\ninstead of having a lot of different parameters. Thus, Neural ODE becomes\nsignificantly small compared to that of ResNet so that it can be implemented in\nresource-limited edge devices. In this paper, a combination of Neural ODE and\nDSC, called dsODENet, is designed and implemented for FPGAs (Field-Programmable\nGate Arrays). dsODENet is then applied to edge domain adaptation as a practical\nuse case and evaluated with image classification datasets. It is implemented on\nXilinx ZCU104 board and evaluated in terms of domain adaptation accuracy,\ntraining speed, FPGA resource utilization, and speedup rate compared to a\nsoftware execution. The results demonstrate that dsODENet is comparable to or\nslightly better than our baseline Neural ODE implementation in terms of domain\nadaptation accuracy, while the total parameter size without pre- and\npost-processing layers is reduced by 54.2% to 79.8%. The FPGA implementation\naccelerates the prediction tasks by 27.9 times faster than a software\nimplementation.",
          "link": "http://arxiv.org/abs/2107.12824",
          "publishedOn": "2021-07-28T02:02:34.103Z",
          "wordCount": 677,
          "title": "A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs. (arXiv:2107.12824v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12878",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alle_S/0/1/0/all/0/1\">Shanmukh Alle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Priyakumar_U/0/1/0/all/0/1\">U. Deva Priyakumar</a>",
          "description": "Parkinson's Disease (PD) is a chronic and progressive neurological disorder\nthat results in rigidity, tremors and postural instability. There is no\ndefinite medical test to diagnose PD and diagnosis is mostly a clinical\nexercise. Although guidelines exist, about 10-30% of the patients are wrongly\ndiagnosed with PD. Hence, there is a need for an accurate, unbiased and fast\nmethod for diagnosis. In this study, we propose LPGNet, a fast and accurate\nmethod to diagnose PD from gait. LPGNet uses Linear Prediction Residuals (LPR)\nto extract discriminating patterns from gait recordings and then uses a 1D\nconvolution neural network with depth-wise separable convolutions to perform\ndiagnosis. LPGNet achieves an AUC of 0.91 with a 21 times speedup and about 99%\nlesser parameters in the model compared to the state of the art. We also\nundertake an analysis of various cross-validation strategies used in literature\nin PD diagnosis from gait and find that most methods are affected by some form\nof data leakage between various folds which leads to unnecessarily large models\nand inflated performance due to overfitting. The analysis clears the path for\nfuture works in correctly evaluating their methods.",
          "link": "http://arxiv.org/abs/2107.12878",
          "publishedOn": "2021-07-28T02:02:34.096Z",
          "wordCount": 664,
          "title": "Linear Prediction Residual for Efficient Diagnosis of Parkinson's Disease from Gait. (arXiv:2107.12878v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12910",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hongpeng Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ibrahim_C/0/1/0/all/0/1\">Chahine Ibrahim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_W/0/1/0/all/0/1\">Wei Xing Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_W/0/1/0/all/0/1\">Wei Pan</a>",
          "description": "This paper proposes a sparse Bayesian treatment of deep neural networks\n(DNNs) for system identification. Although DNNs show impressive approximation\nability in various fields, several challenges still exist for system\nidentification problems. First, DNNs are known to be too complex that they can\neasily overfit the training data. Second, the selection of the input regressors\nfor system identification is nontrivial. Third, uncertainty quantification of\nthe model parameters and predictions are necessary. The proposed Bayesian\napproach offers a principled way to alleviate the above challenges by marginal\nlikelihood/model evidence approximation and structured group sparsity-inducing\npriors construction. The identification algorithm is derived as an iterative\nregularized optimization procedure that can be solved as efficiently as\ntraining typical DNNs. Furthermore, a practical calculation approach based on\nthe Monte-Carlo integration method is derived to quantify the uncertainty of\nthe parameters and predictions. The effectiveness of the proposed Bayesian\napproach is demonstrated on several linear and nonlinear systems identification\nbenchmarks with achieving good and competitive simulation accuracy.",
          "link": "http://arxiv.org/abs/2107.12910",
          "publishedOn": "2021-07-28T02:02:34.086Z",
          "wordCount": 610,
          "title": "Sparse Bayesian Deep Learning for Dynamic System Identification. (arXiv:2107.12910v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1\">Lukas Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1\">Lea Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messner_E/0/1/0/all/0/1\">Eva-Maria Me&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>",
          "description": "Emotion is an inherently subjective psychophysiological human-state and to\nproduce an agreed-upon representation (gold standard) for continuous emotion\nrequires a time-consuming and costly training procedure of multiple human\nannotators. There is strong evidence in the literature that physiological\nsignals are sufficient objective markers for states of emotion, particularly\narousal. In this contribution, we utilise a dataset which includes continuous\nemotion and physiological signals - Heartbeats per Minute (BPM), Electrodermal\nActivity (EDA), and Respiration-rate - captured during a stress induced\nscenario (Trier Social Stress Test). We utilise a Long Short-Term Memory,\nRecurrent Neural Network to explore the benefit of fusing these physiological\nsignals with arousal as the target, learning from various audio, video, and\ntextual based features. We utilise the state-of-the-art MuSe-Toolbox to\nconsider both annotation delay and inter-rater agreement weighting when fusing\nthe target signals. An improvement in Concordance Correlation Coefficient (CCC)\nis seen across features sets when fusing EDA with arousal, compared to the\narousal only gold standard results. Additionally, BERT-based textual features'\nresults improved for arousal plus all physiological signals, obtaining up to\n.3344 CCC compared to .2118 CCC for arousal only. Multimodal fusion also\nimproves overall CCC with audio plus video features obtaining up to .6157 CCC\nto recognize arousal plus EDA and BPM.",
          "link": "http://arxiv.org/abs/2107.12964",
          "publishedOn": "2021-07-28T02:02:34.080Z",
          "wordCount": 660,
          "title": "A Physiologically-adapted Gold Standard for Arousal During a Stress Induced Scenario. (arXiv:2107.12964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perekrestenko_D/0/1/0/all/0/1\">Dmytro Perekrestenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eberhard_L/0/1/0/all/0/1\">L&#xe9;andre Eberhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolcskei_H/0/1/0/all/0/1\">Helmut B&#xf6;lcskei</a>",
          "description": "We show that every $d$-dimensional probability distribution of bounded\nsupport can be generated through deep ReLU networks out of a $1$-dimensional\nuniform input distribution. What is more, this is possible without incurring a\ncost - in terms of approximation error measured in Wasserstein-distance -\nrelative to generating the $d$-dimensional target distribution from $d$\nindependent random variables. This is enabled by a vast generalization of the\nspace-filling approach discovered in (Bailey & Telgarsky, 2018). The\nconstruction we propose elicits the importance of network depth in driving the\nWasserstein distance between the target distribution and its neural network\napproximation to zero. Finally, we find that, for histogram target\ndistributions, the number of bits needed to encode the corresponding generative\nnetwork equals the fundamental limit for encoding probability distributions as\ndictated by quantization theory.",
          "link": "http://arxiv.org/abs/2107.12466",
          "publishedOn": "2021-07-28T02:02:34.062Z",
          "wordCount": 558,
          "title": "High-Dimensional Distribution Generation Through Deep Neural Networks. (arXiv:2107.12466v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mimi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parnell_A/0/1/0/all/0/1\">Andrew Parnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brabazon_D/0/1/0/all/0/1\">Dermot Brabazon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benavoli_A/0/1/0/all/0/1\">Alessio Benavoli</a>",
          "description": "Bayesian optimization (BO) is an approach to globally optimizing black-box\nobjective functions that are expensive to evaluate. BO-powered experimental\ndesign has found wide application in materials science, chemistry, experimental\nphysics, drug development, etc. This work aims to bring attention to the\nbenefits of applying BO in designing experiments and to provide a BO manual,\ncovering both methodology and software, for the convenience of anyone who wants\nto apply or learn BO. In particular, we briefly explain the BO technique,\nreview all the applications of BO in additive manufacturing, compare and\nexemplify the features of different open BO libraries, unlock new potential\napplications of BO to other types of data (e.g., preferential output). This\narticle is aimed at readers with some understanding of Bayesian methods, but\nnot necessarily with knowledge of additive manufacturing; the software\nperformance overview and implementation instructions are instrumental for any\nexperimental-design practitioner. Moreover, our review in the field of additive\nmanufacturing highlights the current knowledge and technological trends of BO.",
          "link": "http://arxiv.org/abs/2107.12809",
          "publishedOn": "2021-07-28T02:02:34.055Z",
          "wordCount": 606,
          "title": "Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing. (arXiv:2107.12809v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strumke_I/0/1/0/all/0/1\">Inga Str&#xfc;mke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slavkovik_M/0/1/0/all/0/1\">Marija Slavkovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madai_V/0/1/0/all/0/1\">Vince Madai</a>",
          "description": "While the demand for ethical artificial intelligence (AI) systems increases,\nthe number of unethical uses of AI accelerates, even though there is no\nshortage of ethical guidelines. We argue that a main underlying cause for this\nis that AI developers face a social dilemma in AI development ethics,\npreventing the widespread adaptation of ethical best practices. We define the\nsocial dilemma for AI development and describe why the current crisis in AI\ndevelopment ethics cannot be solved without relieving AI developers of their\nsocial dilemma. We argue that AI development must be professionalised to\novercome the social dilemma, and discuss how medicine can be used as a template\nin this process.",
          "link": "http://arxiv.org/abs/2107.12977",
          "publishedOn": "2021-07-28T02:02:34.046Z",
          "wordCount": 558,
          "title": "The social dilemma in AI development and why we have to solve it. (arXiv:2107.12977v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12838",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Noman_F/0/1/0/all/0/1\">Fuad Noman</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ting_C/0/1/0/all/0/1\">Chee-Ming Ting</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kang_H/0/1/0/all/0/1\">Hakmook Kang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Phan_R/0/1/0/all/0/1\">Raphael C.-W. Phan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Boyd_B/0/1/0/all/0/1\">Brian D. Boyd</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Taylor_W/0/1/0/all/0/1\">Warren D. Taylor</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ombao_H/0/1/0/all/0/1\">Hernando Ombao</a>",
          "description": "Brain functional connectivity (FC) reveals biomarkers for identification of\nvarious neuropsychiatric disorders. Recent application of deep neural networks\n(DNNs) to connectome-based classification mostly relies on traditional\nconvolutional neural networks using input connectivity matrices on a regular\nEuclidean grid. We propose a graph deep learning framework to incorporate the\nnon-Euclidean information about graph structure for classifying functional\nmagnetic resonance imaging (fMRI)- derived brain networks in major depressive\ndisorder (MDD). We design a novel graph autoencoder (GAE) architecture based on\nthe graph convolutional networks (GCNs) to embed the topological structure and\nnode content of large-sized fMRI networks into low-dimensional latent\nrepresentations. In network construction, we employ the Ledoit-Wolf (LDW)\nshrinkage method to estimate the high-dimensional FC metrics efficiently from\nfMRI data. We consider both supervised and unsupervised approaches for the\ngraph embedded learning. The learned embeddings are then used as feature inputs\nfor a deep fully-connected neural network (FCNN) to discriminate MDD from\nhealthy controls. Evaluated on a resting-state fMRI MDD dataset with 43\nsubjects, results show that the proposed GAE-FCNN model significantly\noutperforms several state-of-the-art DNN methods for brain connectome\nclassification, achieving accuracy of 72.50% using the LDW-FC metrics as node\nfeatures. The graph embeddings of fMRI FC networks learned by the GAE also\nreveal apparent group differences between MDD and HC. Our new framework\ndemonstrates feasibility of learning graph embeddings on brain networks to\nprovide discriminative information for diagnosis of brain disorders.",
          "link": "http://arxiv.org/abs/2107.12838",
          "publishedOn": "2021-07-28T02:02:34.021Z",
          "wordCount": 691,
          "title": "Graph Autoencoders for Embedding Learning in Brain Networks and Major Depressive Disorder Identification. (arXiv:2107.12838v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12889",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Felfeliyan_B/0/1/0/all/0/1\">Banafshe Felfeliyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1\">Abhilash Hareendranathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuntze_G/0/1/0/all/0/1\">Gregor Kuntze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1\">Jacob L. Jaremko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ronsky_J/0/1/0/all/0/1\">Janet L. Ronsky</a>",
          "description": "Objective assessment of Magnetic Resonance Imaging (MRI) scans of\nosteoarthritis (OA) can address the limitation of the current OA assessment.\nSegmentation of bone, cartilage, and joint fluid is necessary for the OA\nobjective assessment. Most of the proposed segmentation methods are not\nperforming instance segmentation and suffer from class imbalance problems. This\nstudy deployed Mask R-CNN instance segmentation and improved it (improved-Mask\nR-CNN (iMaskRCNN)) to obtain a more accurate generalized segmentation for\nOA-associated tissues. Training and validation of the method were performed\nusing 500 MRI knees from the Osteoarthritis Initiative (OAI) dataset and 97 MRI\nscans of patients with symptomatic hip OA. Three modifications to Mask R-CNN\nyielded the iMaskRCNN: adding a 2nd ROIAligned block, adding an extra decoder\nlayer to the mask-header, and connecting them by a skip connection. The results\nwere assessed using Hausdorff distance, dice score, and coefficients of\nvariation (CoV). The iMaskRCNN led to improved bone and cartilage segmentation\ncompared to Mask RCNN as indicated with the increase in dice score from 95% to\n98% for the femur, 95% to 97% for tibia, 71% to 80% for femoral cartilage, and\n81% to 82% for tibial cartilage. For the effusion detection, dice improved with\niMaskRCNN 72% versus MaskRCNN 71%. The CoV values for effusion detection\nbetween Reader1 and Mask R-CNN (0.33), Reader1 and iMaskRCNN (0.34), Reader2\nand Mask R-CNN (0.22), Reader2 and iMaskRCNN (0.29) are close to CoV between\ntwo readers (0.21), indicating a high agreement between the human readers and\nboth Mask R-CNN and iMaskRCNN. Mask R-CNN and iMaskRCNN can reliably and\nsimultaneously extract different scale articular tissues involved in OA,\nforming the foundation for automated assessment of OA. The iMaskRCNN results\nshow that the modification improved the network performance around the edges.",
          "link": "http://arxiv.org/abs/2107.12889",
          "publishedOn": "2021-07-28T02:02:34.013Z",
          "wordCount": 755,
          "title": "Improved-Mask R-CNN: Towards an Accurate Generic MSK MRI instance segmentation platform (Data from the Osteoarthritis Initiative). (arXiv:2107.12889v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaeckle_F/0/1/0/all/0/1\">Florian Jaeckle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingyue Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>",
          "description": "Many available formal verification methods have been shown to be instances of\na unified Branch-and-Bound (BaB) formulation. We propose a novel machine\nlearning framework that can be used for designing an effective branching\nstrategy as well as for computing better lower bounds. Specifically, we learn\ntwo graph neural networks (GNN) that both directly treat the network we want to\nverify as a graph input and perform forward-backward passes through the GNN\nlayers. We use one GNN to simulate the strong branching heuristic behaviour and\nanother to compute a feasible dual solution of the convex relaxation, thereby\nproviding a valid lower bound.\n\nWe provide a new verification dataset that is more challenging than those\nused in the literature, thereby providing an effective alternative for testing\nalgorithmic improvements for verification. Whilst using just one of the GNNs\nleads to a reduction in verification time, we get optimal performance when\ncombining the two GNN approaches. Our combined framework achieves a 50\\%\nreduction in both the number of branches and the time required for verification\non various convolutional networks when compared to several state-of-the-art\nverification methods. In addition, we show that our GNN models generalize well\nto harder properties on larger unseen networks.",
          "link": "http://arxiv.org/abs/2107.12855",
          "publishedOn": "2021-07-28T02:02:33.965Z",
          "wordCount": 638,
          "title": "Neural Network Branch-and-Bound for Neural Network Verification. (arXiv:2107.12855v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1\">Patrik Joslin Kenfack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Adil Mehmood Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1\">Rasheed Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazmi_S/0/1/0/all/0/1\">S.M. Ahsan Kazmi</a>,",
          "description": "Training machine learning models with the only accuracy as a final goal may\npromote prejudices and discriminatory behaviors embedded in the data. One\nsolution is to learn latent representations that fulfill specific fairness\nmetrics. Different types of learning methods are employed to map data into the\nfair representational space. The main purpose is to learn a latent\nrepresentation of data that scores well on a fairness metric while maintaining\nthe usability for the downstream task. In this paper, we propose a new fair\nrepresentation learning approach that leverages different levels of\nrepresentation of data to tighten the fairness bounds of the learned\nrepresentation. Our results show that stacking different auto-encoders and\nenforcing fairness at different latent spaces result in an improvement of\nfairness compared to other existing approaches.",
          "link": "http://arxiv.org/abs/2107.12826",
          "publishedOn": "2021-07-28T02:02:33.952Z",
          "wordCount": 568,
          "title": "Adversarial Stacked Auto-Encoders for Fair Representation Learning. (arXiv:2107.12826v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12797",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kepler_M/0/1/0/all/0/1\">Michael E. Kepler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1\">Alec Koppel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bedi_A/0/1/0/all/0/1\">Amrit Singh Bedi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Stilwell_D/0/1/0/all/0/1\">Daniel J. Stilwell</a>",
          "description": "Gaussian processes (GPs) are a well-known nonparametric Bayesian inference\ntechnique, but they suffer from scalability problems for large sample sizes,\nand their performance can degrade for non-stationary or spatially heterogeneous\ndata. In this work, we seek to overcome these issues through (i) employing\nvariational free energy approximations of GPs operating in tandem with online\nexpectation propagation steps; and (ii) introducing a local splitting step\nwhich instantiates a new GP whenever the posterior distribution changes\nsignificantly as quantified by the Wasserstein metric over posterior\ndistributions. Over time, then, this yields an ensemble of sparse GPs which may\nbe updated incrementally, and adapts to locality, heterogeneity, and\nnon-stationarity in training data.",
          "link": "http://arxiv.org/abs/2107.12797",
          "publishedOn": "2021-07-28T02:02:33.936Z",
          "wordCount": 548,
          "title": "Wasserstein-Splitting Gaussian Process Regression for Heterogeneous Online Bayesian Inference. (arXiv:2107.12797v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhiwen Pan</a>",
          "description": "Nowadays, multi-sensor technologies are applied in many fields, e.g., Health\nCare (HC), Human Activity Recognition (HAR), and Industrial Control System\n(ICS). These sensors can generate a substantial amount of multivariate\ntime-series data. Unsupervised anomaly detection on multi-sensor time-series\ndata has been proven critical in machine learning researches. The key challenge\nis to discover generalized normal patterns by capturing spatial-temporal\ncorrelation in multi-sensor data. Beyond this challenge, the noisy data is\noften intertwined with the training data, which is likely to mislead the model\nby making it hard to distinguish between the normal, abnormal, and noisy data.\nFew of previous researches can jointly address these two challenges. In this\npaper, we propose a novel deep learning-based anomaly detection algorithm\ncalled Deep Convolutional Autoencoding Memory network (CAE-M). We first build a\nDeep Convolutional Autoencoder to characterize spatial dependence of\nmulti-sensor data with a Maximum Mean Discrepancy (MMD) to better distinguish\nbetween the noisy, normal, and abnormal data. Then, we construct a Memory\nNetwork consisting of linear (Autoregressive Model) and non-linear predictions\n(Bidirectional LSTM with Attention) to capture temporal dependence from\ntime-series data. Finally, CAE-M jointly optimizes these two subnetworks. We\nempirically compare the proposed approach with several state-of-the-art anomaly\ndetection methods on HAR and HC datasets. Experimental results demonstrate that\nour proposed model outperforms these existing methods.",
          "link": "http://arxiv.org/abs/2107.12626",
          "publishedOn": "2021-07-28T02:02:33.901Z",
          "wordCount": 671,
          "title": "Unsupervised Deep Anomaly Detection for Multi-Sensor Time-Series Signals. (arXiv:2107.12626v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ausset_G/0/1/0/all/0/1\">Guillaume Ausset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciffreo_T/0/1/0/all/0/1\">Tom Ciffreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portier_F/0/1/0/all/0/1\">Francois Portier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemencon_S/0/1/0/all/0/1\">Stephan Cl&#xe9;men&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papin_T/0/1/0/all/0/1\">Timoth&#xe9;e Papin</a>",
          "description": "Survival analysis, or time-to-event modelling, is a classical statistical\nproblem that has garnered a lot of interest for its practical use in\nepidemiology, demographics or actuarial sciences. Recent advances on the\nsubject from the point of view of machine learning have been concerned with\nprecise per-individual predictions instead of population studies, driven by the\nrise of individualized medicine. We introduce here a conditional normalizing\nflow based estimate of the time-to-event density as a way to model highly\nflexible and individualized conditional survival distributions. We use a novel\nhierarchical formulation of normalizing flows to enable efficient fitting of\nflexible conditional distributions without overfitting and show how the\nnormalizing flow formulation can be efficiently adapted to the censored\nsetting. We experimentally validate the proposed approach on a synthetic\ndataset as well as four open medical datasets and an example of a common\nfinancial problem.",
          "link": "http://arxiv.org/abs/2107.12825",
          "publishedOn": "2021-07-28T02:02:33.893Z",
          "wordCount": 579,
          "title": "Individual Survival Curves with Conditional Normalizing Flows. (arXiv:2107.12825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuesheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haizhang Zhang</a>",
          "description": "We explore convergence of deep neural networks with the popular ReLU\nactivation function, as the depth of the networks tends to infinity. To this\nend, we introduce the notion of activation domains and activation matrices of a\nReLU network. By replacing applications of the ReLU activation function by\nmultiplications with activation matrices on activation domains, we obtain an\nexplicit expression of the ReLU network. We then identify the convergence of\nthe ReLU networks as convergence of a class of infinite products of matrices.\nSufficient and necessary conditions for convergence of these infinite products\nof matrices are studied. As a result, we establish necessary conditions for\nReLU networks to converge that the sequence of weight matrices converges to the\nidentity matrix and the sequence of the bias vectors converges to zero as the\ndepth of ReLU networks increases to infinity. Moreover, we obtain sufficient\nconditions in terms of the weight matrices and bias vectors at hidden layers\nfor pointwise convergence of deep ReLU networks. These results provide\nmathematical insights to the design strategy of the well-known deep residual\nnetworks in image classification.",
          "link": "http://arxiv.org/abs/2107.12530",
          "publishedOn": "2021-07-28T02:02:33.802Z",
          "wordCount": 607,
          "title": "Convergence of Deep ReLU Networks. (arXiv:2107.12530v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_G/0/1/0/all/0/1\">Gourav Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1\">Massoud Pedram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1\">Peter A. Beerel</a>",
          "description": "Deep spiking neural networks (SNNs) have emerged as a potential alternative\nto traditional deep learning frameworks, due to their promise to provide\nincreased compute efficiency on event-driven neuromorphic hardware. However, to\nperform well on complex vision applications, most SNN training frameworks yield\nlarge inference latency which translates to increased spike activity and\nreduced energy efficiency. Hence,minimizing average spike activity while\npreserving accuracy indeep SNNs remains a significant challenge and\nopportunity.This paper presents a non-iterative SNN training technique\nthatachieves ultra-high compression with reduced spiking activitywhile\nmaintaining high inference accuracy. In particular, our framework first uses\nthe attention-maps of an un compressed meta-model to yield compressed ANNs.\nThis step can be tuned to support both irregular and structured channel pruning\nto leverage computational benefits over a broad range of platforms. The\nframework then performs sparse-learning-based supervised SNN training using\ndirect inputs. During the training, it jointly optimizes the SNN weight,\nthreshold, and leak parameters to drastically minimize the number of time steps\nrequired while retaining compression. To evaluate the merits of our approach,\nwe performed experiments with variants of VGG and ResNet, on both CIFAR-10 and\nCIFAR-100, and VGG16 on Tiny-ImageNet.The SNN models generated through the\nproposed technique yield SOTA compression ratios of up to 33.4x with no\nsignificant drops in accuracy compared to baseline unpruned counterparts.\nCompared to existing SNN pruning methods, we achieve up to 8.3x higher\ncompression with improved accuracy.",
          "link": "http://arxiv.org/abs/2107.12445",
          "publishedOn": "2021-07-28T02:02:33.598Z",
          "wordCount": null,
          "title": "Towards Low-Latency Energy-Efficient Deep SNNs via Attention-Guided Compression. (arXiv:2107.12445v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.",
          "link": "http://arxiv.org/abs/2107.12636",
          "publishedOn": "2021-07-28T02:02:33.591Z",
          "wordCount": null,
          "title": "Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koren_M/0/1/0/all/0/1\">Mark Koren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_A/0/1/0/all/0/1\">Ahmed Nassar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>",
          "description": "Validating the safety of autonomous systems generally requires the use of\nhigh-fidelity simulators that adequately capture the variability of real-world\nscenarios. However, it is generally not feasible to exhaustively search the\nspace of simulation scenarios for failures. Adaptive stress testing (AST) is a\nmethod that uses reinforcement learning to find the most likely failure of a\nsystem. AST with a deep reinforcement learning solver has been shown to be\neffective in finding failures across a range of different systems. This\napproach generally involves running many simulations, which can be very\nexpensive when using a high-fidelity simulator. To improve efficiency, we\npresent a method that first finds failures in a low-fidelity simulator. It then\nuses the backward algorithm, which trains a deep neural network policy using a\nsingle expert demonstration, to adapt the low-fidelity failures to\nhigh-fidelity. We have created a series of autonomous vehicle validation case\nstudies that represent some of the ways low-fidelity and high-fidelity\nsimulators can differ, such as time discretization. We demonstrate in a variety\nof case studies that this new AST approach is able to find failures with\nsignificantly fewer high-fidelity simulation steps than are needed when just\nrunning AST directly in high-fidelity. As a proof of concept, we also\ndemonstrate AST on NVIDIA's DriveSim simulator, an industry state-of-the-art\nhigh-fidelity simulator for finding failures in autonomous vehicles.",
          "link": "http://arxiv.org/abs/2107.12940",
          "publishedOn": "2021-07-28T02:02:33.548Z",
          "wordCount": null,
          "title": "Finding Failures in High-Fidelity Simulation using Adaptive Stress Testing and the Backward Algorithm. (arXiv:2107.12940v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoyt_C/0/1/0/all/0/1\">Christopher R. Hoyt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owen_A/0/1/0/all/0/1\">Art B. Owen</a>",
          "description": "We use graphical methods to probe neural nets that classify images. Plots of\nt-SNE outputs at successive layers in a network reveal increasingly organized\narrangement of the data points. They can also reveal how a network can diminish\nor even forget about within-class structure as the data proceeds through\nlayers. We use class-specific analogues of principal components to visualize\nhow succeeding layers separate the classes. These allow us to sort images from\na given class from most typical to least typical (in the data) and they also\nserve as very useful projection coordinates for data visualization. We find\nthem especially useful when defining versions guided tours for animated data\nvisualization.",
          "link": "http://arxiv.org/abs/2107.12547",
          "publishedOn": "2021-07-28T02:02:33.516Z",
          "wordCount": null,
          "title": "Probing neural networks with t-SNE, class-specific projections and a guided tour. (arXiv:2107.12547v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tong Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Confidence calibration is of great importance to the reliability of decisions\nmade by machine learning systems. However, discriminative classifiers based on\ndeep neural networks are often criticized for producing overconfident\npredictions that fail to reflect the true correctness likelihood of\nclassification accuracy. We argue that such an inability to model uncertainty\nis mainly caused by the closed-world nature in softmax: a model trained by the\ncross-entropy loss will be forced to classify input into one of $K$ pre-defined\ncategories with high probability. To address this problem, we for the first\ntime propose a novel $K$+1-way softmax formulation, which incorporates the\nmodeling of open-world uncertainty as the extra dimension. To unify the\nlearning of the original $K$-way classification task and the extra dimension\nthat models uncertainty, we propose a novel energy-based objective function,\nand moreover, theoretically prove that optimizing such an objective essentially\nforces the extra dimension to capture the marginal data distribution. Extensive\nexperiments show that our approach, Energy-based Open-World Softmax\n(EOW-Softmax), is superior to existing state-of-the-art methods in improving\nconfidence calibration.",
          "link": "http://arxiv.org/abs/2107.12628",
          "publishedOn": "2021-07-28T02:02:33.502Z",
          "wordCount": null,
          "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration. (arXiv:2107.12628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boshen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>",
          "description": "Recently, the problem of inaccurate learning targets in crowd counting draws\nincreasing attention. Inspired by a few pioneering work, we solve this problem\nby trying to predict the indices of pre-defined interval bins of counts instead\nof the count values themselves. However, an inappropriate interval setting\nmight make the count error contributions from different intervals extremely\nimbalanced, leading to inferior counting performance. Therefore, we propose a\nnovel count interval partition criterion called Uniform Error Partition (UEP),\nwhich always keeps the expected counting error contributions equal for all\nintervals to minimize the prediction risk. Then to mitigate the inevitably\nintroduced discretization errors in the count quantization process, we propose\nanother criterion called Mean Count Proxies (MCP). The MCP criterion selects\nthe best count proxy for each interval to represent its count value during\ninference, making the overall expected discretization error of an image nearly\nnegligible. As far as we are aware, this work is the first to delve into such a\nclassification task and ends up with a promising solution for count interval\npartition. Following the above two theoretically demonstrated criterions, we\npropose a simple yet effective model termed Uniform Error Partition Network\n(UEPNet), which achieves state-of-the-art performance on several challenging\ndatasets. The codes will be available at:\nhttps://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.",
          "link": "http://arxiv.org/abs/2107.12619",
          "publishedOn": "2021-07-28T02:02:33.499Z",
          "wordCount": null,
          "title": "Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Menculini_L/0/1/0/all/0/1\">Lorenzo Menculini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marini_A/0/1/0/all/0/1\">Andrea Marini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proietti_M/0/1/0/all/0/1\">Massimiliano Proietti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garinei_A/0/1/0/all/0/1\">Alberto Garinei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozza_A/0/1/0/all/0/1\">Alessio Bozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moretti_C/0/1/0/all/0/1\">Cecilia Moretti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marconi_M/0/1/0/all/0/1\">Marcello Marconi</a>",
          "description": "Setting sale prices correctly is of great importance for firms, and the study\nand forecast of prices time series is therefore a relevant topic not only from\na data science perspective but also from an economic and applicative one. In\nthis paper we exhamine different techniques to forecast the sale prices of\nthree food products applied by an Italian food wholesaler, as a step towards\nthe automation of pricing tasks usually taken care by human workforce. We\nconsider ARIMA models and compare them to Prophet, a scalable forecasting tool\ndeveloped by Facebook and based on a generalized additive model, and to deep\nlearning models based on Long Short--Term Memory (LSTM) and Convolutional\nNeural Networks (CNNs). ARIMA models are frequently used in econometric\nanalyses, providing a good bechmark for the problem under study. Our results\nindicate that ARIMA performs similarly to LSTM neural networks for the problem\nunder study, while the combination of CNNs and LSTMs attains the best overall\naccuracy, but requires more time to be tuned. On the contrary, Prophet is very\nfast to use, but less accurate.",
          "link": "http://arxiv.org/abs/2107.12770",
          "publishedOn": "2021-07-28T02:02:33.496Z",
          "wordCount": null,
          "title": "Comparing Prophet and Deep Learning to ARIMA in Forecasting Wholesale Food Prices. (arXiv:2107.12770v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lesley Tan</a>",
          "description": "In this paper, we investigate data-driven parameterized modeling of insertion\nloss for transmission lines with respect to design parameters. We first show\nthat direct application of neural networks can lead to non-physics models with\nnegative insertion loss. To mitigate this problem, we propose two deep learning\nsolutions. One solution is to add a regulation term, which represents the\npassive condition, to the final loss function to enforce the negative quantity\nof insertion loss. In the second method, a third-order polynomial expression is\ndefined first, which ensures positiveness, to approximate the insertion loss,\nthen DeepONet neural network structure, which was proposed recently for\nfunction and system modeling, was employed to model the coefficients of\npolynomials. The resulting neural network is applied to predict the\ncoefficients of the polynomial expression. The experimental results on an\nopen-sourced SI/PI database of a PCB design show that both methods can ensure\nthe positiveness for the insertion loss. Furthermore, both methods can achieve\nsimilar prediction results, while the polynomial-based DeepONet method is\nfaster than DeepONet based method in training time.",
          "link": "http://arxiv.org/abs/2107.12527",
          "publishedOn": "2021-07-28T02:02:33.494Z",
          "wordCount": null,
          "title": "Physics-Enforced Modeling for Insertion Loss of Transmission Lines by Deep Neural Networks. (arXiv:2107.12527v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1\">Maithra Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1\">Jon Kleinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1\">Samy Bengio</a>",
          "description": "The successes of deep learning critically rely on the ability of neural\nnetworks to output meaningful predictions on unseen data -- generalization. Yet\ndespite its criticality, there remain fundamental open questions on how neural\nnetworks generalize. How much do neural networks rely on memorization -- seeing\nhighly similar training examples -- and how much are they capable of\nhuman-intelligence styled reasoning -- identifying abstract rules underlying\nthe data? In this paper we introduce a novel benchmark, Pointer Value Retrieval\n(PVR) tasks, that explore the limits of neural network generalization. While\nPVR tasks can consist of visual as well as symbolic inputs, each with varying\nlevels of difficulty, they all have a simple underlying rule. One part of the\nPVR task input acts as a pointer, giving the location of a different part of\nthe input, which forms the value (and output). We demonstrate that this task\nstructure provides a rich testbed for understanding generalization, with our\nempirical study showing large variations in neural network performance based on\ndataset size, task complexity and model architecture. The interaction of\nposition, values and the pointer rule also allow the development of nuanced\ntests of generalization, by introducing distribution shift and increasing\nfunctional complexity. These reveal both subtle failures and surprising\nsuccesses, suggesting many promising directions of exploration on this\nbenchmark.",
          "link": "http://arxiv.org/abs/2107.12580",
          "publishedOn": "2021-07-28T02:02:33.449Z",
          "wordCount": null,
          "title": "Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization. (arXiv:2107.12580v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raumanns_R/0/1/0/all/0/1\">Ralf Raumanns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schouten_G/0/1/0/all/0/1\">Gerard Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joosten_M/0/1/0/all/0/1\">Max Joosten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1\">Josien P. W. Pluim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>",
          "description": "We present ENHANCE, an open dataset with multiple annotations to complement\nthe existing ISIC and PH2 skin lesion classification datasets. This dataset\ncontains annotations of visual ABC (asymmetry, border, colour) features from\nnon-expert annotation sources: undergraduate students, crowd workers from\nAmazon MTurk and classic image processing algorithms. In this paper we first\nanalyse the correlations between the annotations and the diagnostic label of\nthe lesion, as well as study the agreement between different annotation\nsources. Overall we find weak correlations of non-expert annotations with the\ndiagnostic label, and low agreement between different annotation sources. We\nthen study multi-task learning (MTL) with the annotations as additional labels,\nand show that non-expert annotations can improve (ensembles of)\nstate-of-the-art convolutional neural networks via MTL. We hope that our\ndataset can be used in further research into multiple annotations and/or MTL.\nAll data and models are available on Github:\nhttps://github.com/raumannsr/ENHANCE.",
          "link": "http://arxiv.org/abs/2107.12734",
          "publishedOn": "2021-07-28T02:02:33.448Z",
          "wordCount": null,
          "title": "ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A case study for skin lesion classification. (arXiv:2107.12734v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13493",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kabeli_O/0/1/0/all/0/1\">Ori Kabeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenyu Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1\">Buye Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>",
          "description": "Deep neural networks have recently shown great success in the task of blind\nsource separation, both under monaural and binaural settings. Although these\nmethods were shown to produce high-quality separations, they were mainly\napplied under offline settings, in which the model has access to the full input\nsignal while separating the signal. In this study, we convert a non-causal\nstate-of-the-art separation model into a causal and real-time model and\nevaluate its performance under both online and offline settings. We compare the\nperformance of the proposed model to several baseline methods under anechoic,\nnoisy, and noisy-reverberant recording conditions while exploring both monaural\nand binaural inputs and outputs. Our findings shed light on the relative\ndifference between causal and non-causal models when performing separation. Our\nstateful implementation for online separation leads to a minor drop in\nperformance compared to the offline model; 0.8dB for monaural inputs and 0.3dB\nfor binaural inputs while reaching a real-time factor of 0.65. Samples can be\nfound under the following link:\nhttps://kwanum.github.io/sagrnnc-stream-results/.",
          "link": "http://arxiv.org/abs/2106.13493",
          "publishedOn": "2021-07-28T02:02:33.412Z",
          "wordCount": 649,
          "title": "Online Self-Attentive Gated RNNs for Real-Time Speaker Separation. (arXiv:2106.13493v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zandieh_A/0/1/0/all/0/1\">Amir Zandieh</a>",
          "description": "The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely wide\nneural nets trained under least squares loss by gradient descent. However,\ndespite its importance, the super-quadratic runtime of kernel methods limits\nthe use of NTK in large-scale learning tasks. To accelerate kernel machines\nwith NTK, we propose a near input sparsity time algorithm that maps the input\ndata to a randomized low-dimensional feature space so that the inner product of\nthe transformed data approximates their NTK evaluation. Our transformation\nworks by sketching the polynomial expansions of arc-cosine kernels.\nFurthermore, we propose a feature map for approximating the convolutional\ncounterpart of the NTK, which can transform any image using a runtime that is\nonly linear in the number of pixels. We show that in standard large-scale\nregression and classification tasks a linear regressor trained on our features\noutperforms trained Neural Nets and Nystrom approximation of NTK kernel.",
          "link": "http://arxiv.org/abs/2104.00415",
          "publishedOn": "2021-07-28T02:02:33.403Z",
          "wordCount": 611,
          "title": "Learning with Neural Tangent Kernels in Near Input Sparsity Time. (arXiv:2104.00415v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhirui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zekun Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yabang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1\">Andrew Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>",
          "description": "3D deep learning has been increasingly more popular for a variety of tasks\nincluding many safety-critical applications. However, recently several works\nraise the security issues of 3D deep nets. Although most of these works\nconsider adversarial attacks, we identify that backdoor attack is indeed a more\nserious threat to 3D deep learning systems but remains unexplored. We present\nthe backdoor attacks in 3D with a unified framework that exploits the unique\nproperties of 3D data and networks. In particular, we design two attack\napproaches: the poison-label attack and the clean-label attack. The first one\nis straightforward and effective in practice, while the second one is more\nsophisticated assuming there are certain data inspections. The attack\nalgorithms are mainly motivated and developed by 1) the recent discovery of 3D\nadversarial samples which demonstrate the vulnerability of 3D deep nets under\nspatial transformations; 2) the proposed feature disentanglement technique that\nmanipulates the feature of the data through optimization methods and its\npotential to embed a new task. Extensive experiments show the efficacy of the\npoison-label attack with over 95% success rate across several 3D datasets and\nmodels, and the ability of clean-label attack against data filtering with\naround 50% success rate. Our proposed backdoor attack in 3D point cloud is\nexpected to perform as a baseline for improving the robustness of 3D deep\nmodels.",
          "link": "http://arxiv.org/abs/2103.16074",
          "publishedOn": "2021-07-28T02:02:33.391Z",
          "wordCount": 702,
          "title": "PointBA: Towards Backdoor Attacks in 3D Point Cloud. (arXiv:2103.16074v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chernyak_B/0/1/0/all/0/1\">Bronya Roni Chernyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazan_T/0/1/0/all/0/1\">Tamir Hazan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshet_J/0/1/0/all/0/1\">Joseph Keshet</a>",
          "description": "This paper proposes an attack-independent (non-adversarial training)\ntechnique for improving adversarial robustness of neural network models, with\nminimal loss of standard accuracy. We suggest creating a neighborhood around\neach training example, such that the label is kept constant for all inputs\nwithin that neighborhood. Unlike previous work that follows a similar\nprinciple, we apply this idea by extending the training set with multiple\nperturbations for each training example, drawn from within the neighborhood.\nThese perturbations are model independent, and remain constant throughout the\nentire training process. We analyzed our method empirically on MNIST, SVHN, and\nCIFAR-10, under different attacks and conditions. Results suggest that the\nproposed approach improves standard accuracy over other defenses while having\nincreased robustness compared to vanilla adversarial training.",
          "link": "http://arxiv.org/abs/2103.08265",
          "publishedOn": "2021-07-28T02:02:33.371Z",
          "wordCount": 594,
          "title": "Constant Random Perturbations Provide Adversarial Robustness with Minimal Effect on Accuracy. (arXiv:2103.08265v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "Traditional learning systems are trained in closed-world for a fixed number\nof classes, and need pre-collected datasets in advance. However, new classes\noften emerge in real-world applications and should be learned incrementally.\nFor example, in electronic commerce, new types of products appear daily, and in\na social media community, new topics emerge frequently. Under such\ncircumstances, incremental models should learn several new classes at a time\nwithout forgetting. We find a strong correlation between old and new classes in\nincremental learning, which can be applied to relate and facilitate different\nlearning stages mutually. As a result, we propose CO-transport for class\nIncremental Learning (COIL), which learns to relate across incremental tasks\nwith the class-wise semantic relationship. In detail, co-transport has two\naspects: prospective transport tries to augment the old classifier with optimal\ntransported knowledge as fast model adaptation. Retrospective transport aims to\ntransport new class classifiers backward as old ones to overcome forgetting.\nWith these transports, COIL efficiently adapts to new tasks, and stably resists\nforgetting. Experiments on benchmark and real-world multimedia datasets\nvalidate the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2107.12654",
          "publishedOn": "2021-07-28T02:02:33.338Z",
          "wordCount": 616,
          "title": "Co-Transport for Class-Incremental Learning. (arXiv:2107.12654v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dam_T/0/1/0/all/0/1\">Tanmoy Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anavatti_S/0/1/0/all/0/1\">Sreenatha G. Anavatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbass_H/0/1/0/all/0/1\">Hussein A. Abbass</a> (Fellow, IEEESchool of Engineering and Information Technology, University of New South Wales Canberra, Australia)",
          "description": "The Latent Space Clustering in Generative adversarial networks (ClusterGAN)\nmethod has been successful with high-dimensional data. However, the method\nassumes uniformlydistributed priors during the generation of modes, which isa\nrestrictive assumption in real-world data and cause loss ofdiversity in the\ngenerated modes. In this paper, we proposeself-augmentation information\nmaximization improved Clus-terGAN (SIMI-ClusterGAN) to learn the distinctive\npriorsfrom the data. The proposed SIMI-ClusterGAN consists offour deep neural\nnetworks: self-augmentation prior network,generator, discriminator and\nclustering inference autoencoder.The proposed method has been validated using\nseven bench-mark data sets and has shown improved performance overstate-of-the\nart methods. To demonstrate the superiority ofSIMI-ClusterGAN performance on\nimbalanced dataset, wehave discussed two imbalanced conditions on MNIST\ndatasetswith one-class imbalance and three classes imbalanced cases.The results\nhighlight the advantages of SIMI-ClusterGAN.",
          "link": "http://arxiv.org/abs/2107.12706",
          "publishedOn": "2021-07-28T02:02:33.331Z",
          "wordCount": 582,
          "title": "Improving ClusterGAN Using Self-AugmentedInformation Maximization of Disentangling LatentSpaces. (arXiv:2107.12706v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kosman_E/0/1/0/all/0/1\">Eitan Kosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Dotan Di Castro</a>",
          "description": "Autonomous driving gained huge traction in recent years, due to its potential\nto change the way we commute. Much effort has been put into trying to estimate\nthe state of a vehicle. Meanwhile, learning to forecast the state of a vehicle\nahead introduces new capabilities, such as predicting dangerous situations.\nMoreover, forecasting brings new supervision opportunities by learning to\npredict richer a context, expressed by multiple horizons. Intuitively, a video\nstream originated from a front-facing camera is necessary because it encodes\ninformation about the upcoming road. Besides, historical traces of the\nvehicle's states give more context. In this paper, we tackle multi-horizon\nforecasting of vehicle states by fusing the two modalities. We design and\nexperiment with 3 end-to-end architectures that exploit 3D convolutions for\nvisual features extraction and 1D convolutions for features extraction from\nspeed and steering angle traces. To demonstrate the effectiveness of our\nmethod, we perform extensive experiments on two publicly available real-world\ndatasets, Comma2k19 and the Udacity challenge. We show that we are able to\nforecast a vehicle's state to various horizons, while outperforming the current\nstate-of-the-art results on the related task of driving state estimation. We\nexamine the contribution of vision features, and find that a model fed with\nvision features achieves an error that is 56.6% and 66.9% of the error of a\nmodel that doesn't use those features, on the Udacity and Comma2k19 datasets\nrespectively.",
          "link": "http://arxiv.org/abs/2107.12674",
          "publishedOn": "2021-07-28T02:02:33.323Z",
          "wordCount": 672,
          "title": "Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time Series Forecasting. (arXiv:2107.12674v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1\">Hannaneh Barahouei Pasandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1\">Tamer Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1\">Hadi Amirpour</a>",
          "description": "Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency\nhave promised to increase data throughput by allowing concurrent communication\nbetween one Access Point and multiple users. However, we are still a long way\nfrom enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry\napplications such as video streaming in practical WiFi network settings due to\nheterogeneous channel conditions and devices, unreliable transmissions, and\nlack of useful feedback exchange among the lower and upper layers'\nrequirements. This paper introduces MuViS, a novel dual-phase optimization\nframework that proposes a Quality of Experience (QoE) aware MU-MIMO\noptimization for multi-user video streaming over IEEE 802.11ac. MuViS first\nemploys reinforcement learning to optimize the MU-MIMO user group and mode\nselection for users based on their PHY/MAC layer characteristics. The video\nbitrate is then optimized based on the user's mode (Multi-User (MU) or\nSingle-User (SU)). We present our design and its evaluation on smartphones and\nlaptops using 802.11ac WiFi. Our experimental results in various indoor\nenvironments and configurations show a scalable framework that can support a\nlarge number of users with streaming at high video rates and satisfying QoE\nrequirements.",
          "link": "http://arxiv.org/abs/2106.15262",
          "publishedOn": "2021-07-28T02:02:33.313Z",
          "wordCount": 637,
          "title": "MU-MIMO Grouping For Real-time Applications. (arXiv:2106.15262v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sohee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungkyu Lee</a>",
          "description": "Continual learning is a concept of online learning with multiple sequential\ntasks. One of the critical barriers of continual learning is that a network\nshould learn a new task keeping the knowledge of old tasks without access to\nany data of the old tasks. In this paper, we propose a neuron activation\nimportance-based regularization method for stable continual learning regardless\nof the order of tasks. We conduct comprehensive experiments on existing\nbenchmark data sets to evaluate not just the stability and plasticity of our\nmethod with improved classification accuracy also the robustness of the\nperformance along the changes of task order.",
          "link": "http://arxiv.org/abs/2107.12657",
          "publishedOn": "2021-07-28T02:02:33.294Z",
          "wordCount": 529,
          "title": "Continual Learning with Neuron Activation Importance. (arXiv:2107.12657v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pahar_M/0/1/0/all/0/1\">Madhurananda Pahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niesler_T/0/1/0/all/0/1\">Thomas Niesler</a>",
          "description": "We present an experimental investigation into the automatic detection of\nCOVID-19 from coughs, breaths and speech as this type of screening is\nnon-contact, does not require specialist medical expertise or laboratory\nfacilities and can easily be deployed on inexpensive consumer hardware.\n\nSmartphone recordings of cough, breath and speech from subjects around the\nglobe are used for classification by seven standard machine learning\nclassifiers using leave-$p$-out cross-validation to provide a promising\nbaseline performance.\n\nThen, a diverse dataset of 10.29 hours of cough, sneeze, speech and noise\naudio recordings are used to pre-train a CNN, LSTM and Resnet50 classifier and\nfine tuned the model to enhance the performance even further.\n\nWe have also extracted the bottleneck features from these pre-trained models\nby removing the final-two layers and used them as an input to the LR, SVM, MLP\nand KNN classifiers to detect COVID-19 signature.\n\nThe highest AUC of 0.98 was achieved using a transfer learning based Resnet50\narchitecture on coughs from Coswara dataset.\n\nThe highest AUC of 0.94 and 0.92 was achieved from an SVM run on the\nbottleneck features extracted from the breaths from Coswara dataset and speech\nrecordings from ComParE dataset.\n\nWe conclude that among all vocal audio, coughs carry the strongest COVID-19\nsignature followed by breath and speech and using transfer learning improves\nthe classifier performance with higher AUC and lower variance across the\ncross-validation folds.\n\nAlthough these signatures are not perceivable by human ear, machine learning\nbased COVID-19 detection is possible from vocal audio recorded via smartphone.",
          "link": "http://arxiv.org/abs/2104.02477",
          "publishedOn": "2021-07-28T02:02:33.288Z",
          "wordCount": 775,
          "title": "Deep Transfer Learning based COVID-19 Detection in Cough, Breath and Speech using Bottleneck Features. (arXiv:2104.02477v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12631",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1\">Jiguang He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wymeersch_H/0/1/0/all/0/1\">Henk Wymeersch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Renzo_M/0/1/0/all/0/1\">Marco Di Renzo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Juntti_M/0/1/0/all/0/1\">Markku Juntti</a>",
          "description": "Inspired by the remarkable learning and prediction performance of deep neural\nnetworks (DNNs), we apply one special type of DNN framework, known as\nmodel-driven deep unfolding neural network, to reconfigurable intelligent\nsurface (RIS)-aided millimeter wave (mmWave) single-input multiple-output\n(SIMO) systems. We focus on uplink cascaded channel estimation, where known and\nfixed base station combining and RIS phase control matrices are considered for\ncollecting observations. To boost the estimation performance and reduce the\ntraining overhead, the inherent channel sparsity of mmWave channels is\nleveraged in the deep unfolding method. It is verified that the proposed deep\nunfolding network architecture can outperform the least squares (LS) method\nwith a relatively smaller training overhead and online computational\ncomplexity.",
          "link": "http://arxiv.org/abs/2107.12631",
          "publishedOn": "2021-07-28T02:02:33.281Z",
          "wordCount": 562,
          "title": "Learning to Estimate RIS-Aided mmWave Channels. (arXiv:2107.12631v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tong_A/0/1/0/all/0/1\">Alexander Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huguet_G/0/1/0/all/0/1\">Guillaume Huguet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natik_A/0/1/0/all/0/1\">Amine Natik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDonald_K/0/1/0/all/0/1\">Kincaid MacDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuchroo_M/0/1/0/all/0/1\">Manik Kuchroo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coifman_R/0/1/0/all/0/1\">Ronald Coifman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1\">Guy Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1\">Smita Krishnaswamy</a>",
          "description": "We propose a new fast method of measuring distances between large numbers of\nrelated high dimensional datasets called the Diffusion Earth Mover's Distance\n(EMD). We model the datasets as distributions supported on common data graph\nthat is derived from the affinity matrix computed on the combined data. In such\ncases where the graph is a discretization of an underlying Riemannian closed\nmanifold, we prove that Diffusion EMD is topologically equivalent to the\nstandard EMD with a geodesic ground distance. Diffusion EMD can be computed in\n$\\tilde{O}(n)$ time and is more accurate than similarly fast algorithms such as\ntree-based EMDs. We also show Diffusion EMD is fully differentiable, making it\namenable to future uses in gradient-descent frameworks such as deep neural\nnetworks. Finally, we demonstrate an application of Diffusion EMD to single\ncell data collected from 210 COVID-19 patient samples at Yale New Haven\nHospital. Here, Diffusion EMD can derive distances between patients on the\nmanifold of cells at least two orders of magnitude faster than equally accurate\nmethods. This distance matrix between patients can be embedded into a higher\nlevel patient manifold which uncovers structure and heterogeneity in patients.\nMore generally, Diffusion EMD is applicable to all datasets that are massively\ncollected in parallel in many medical and biological systems.",
          "link": "http://arxiv.org/abs/2102.12833",
          "publishedOn": "2021-07-28T02:02:33.274Z",
          "wordCount": 727,
          "title": "Diffusion Earth Mover's Distance and Distribution Embeddings. (arXiv:2102.12833v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>",
          "description": "We propose using self-supervised discrete representations for the task of\nspeech resynthesis. To generate disentangled representation, we separately\nextract low-bitrate representations for speech content, prosodic information,\nand speaker identity. This allows to synthesize speech in a controllable\nmanner. We analyze various state-of-the-art, self-supervised representation\nlearning methods and shed light on the advantages of each method while\nconsidering reconstruction quality and disentanglement properties.\nSpecifically, we evaluate the F0 reconstruction, speaker identification\nperformance (for both resynthesis and voice conversion), recordings'\nintelligibility, and overall quality using subjective human evaluation. Lastly,\nwe demonstrate how these representations can be used for an ultra-lightweight\nspeech codec. Using the obtained representations, we can get to a rate of 365\nbits per second while providing better speech quality than the baseline\nmethods. Audio samples can be found under the following link:\nspeechbot.github.io/resynthesis.",
          "link": "http://arxiv.org/abs/2104.00355",
          "publishedOn": "2021-07-28T02:02:33.266Z",
          "wordCount": 627,
          "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations. (arXiv:2104.00355v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhiyong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yixuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huihua Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hsiao-Dong Chiang</a>",
          "description": "Recent progress on deep learning relies heavily on the quality and efficiency\nof training algorithms. In this paper, we develop a fast training method\nmotivated by the nonlinear Conjugate Gradient (CG) framework. We propose the\nConjugate Gradient with Quadratic line-search (CGQ) method. On the one hand, a\nquadratic line-search determines the step size according to current loss\nlandscape. On the other hand, the momentum factor is dynamically updated in\ncomputing the conjugate gradient parameter (like Polak-Ribiere). Theoretical\nresults to ensure the convergence of our method in strong convex settings is\ndeveloped. And experiments in image classification datasets show that our\nmethod yields faster convergence than other local solvers and has better\ngeneralization capability (test set accuracy). One major advantage of the paper\nmethod is that tedious hand tuning of hyperparameters like the learning rate\nand momentum is avoided.",
          "link": "http://arxiv.org/abs/2106.11548",
          "publishedOn": "2021-07-28T02:02:33.247Z",
          "wordCount": 603,
          "title": "Adaptive Learning Rate and Momentum for Training Deep Neural Networks. (arXiv:2106.11548v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_B/0/1/0/all/0/1\">Ben Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1\">Charalampos Saitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully\ncausal approach to neural audio synthesis which operates directly in the\nwaveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU\ninference. The NEWT uses time-distributed multilayer perceptrons with periodic\nactivations to implicitly learn nonlinear transfer functions that encode the\ncharacteristics of a target timbre. Once trained, a NEWT can produce complex\ntimbral evolutions by simple affine transformations of its input and output\nsignals. We paired the NEWT with a differentiable noise synthesiser and reverb\nand found it capable of generating realistic musical instrument performances\nwith only 260k total model parameters, conditioned on F0 and loudness features.\nWe compared our method to state-of-the-art benchmarks with a multi-stimulus\nlistening test and the Fr\\'echet Audio Distance and found it performed\ncompetitively across the tested timbral domains. Our method significantly\noutperformed the benchmarks in terms of generation speed, and achieved\nreal-time performance on a consumer CPU, both with and without FastNEWT,\nsuggesting it is a viable basis for future creative sound design tools.",
          "link": "http://arxiv.org/abs/2107.05050",
          "publishedOn": "2021-07-28T02:02:33.240Z",
          "wordCount": 631,
          "title": "Neural Waveshaping Synthesis. (arXiv:2107.05050v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hunter Lang</a>",
          "description": "Deep learning has proven effective for various application tasks, but its\napplicability is limited by the reliance on annotated examples. Self-supervised\nlearning has emerged as a promising direction to alleviate the supervision\nbottleneck, but existing work focuses on leveraging co-occurrences in unlabeled\ndata for task-agnostic representation learning, as exemplified by masked\nlanguage model pretraining. In this chapter, we explore task-specific\nself-supervision, which leverages domain knowledge to automatically annotate\nnoisy training examples for end applications, either by introducing labeling\nfunctions for annotating individual instances, or by imposing constraints over\ninterdependent label decisions. We first present deep probabilistic logic(DPL),\nwhich offers a unifying framework for task-specific self-supervision by\ncomposing probabilistic logic with deep learning. DPL represents unknown labels\nas latent variables and incorporates diverse self-supervision using\nprobabilistic logic to train a deep neural network end-to-end using variational\nEM. Next, we present self-supervised self-supervision(S4), which adds to DPL\nthe capability to learn new self-supervision automatically. Starting from an\ninitial seed self-supervision, S4 iteratively uses the deep neural network to\npropose new self supervision. These are either added directly (a form of\nstructured self-training) or verified by a human expert (as in feature-based\nactive learning). Experiments on real-world applications such as biomedical\nmachine reading and various text classification tasks show that task-specific\nself-supervision can effectively leverage domain expertise and often match the\naccuracy of supervised methods with a tiny fraction of human effort.",
          "link": "http://arxiv.org/abs/2107.12591",
          "publishedOn": "2021-07-28T02:02:33.232Z",
          "wordCount": 675,
          "title": "Combining Probabilistic Logic and Deep Learning for Self-Supervised Learning. (arXiv:2107.12591v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.09630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grassucci_E/0/1/0/all/0/1\">Eleonora Grassucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicero_E/0/1/0/all/0/1\">Edoardo Cicero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comminiello_D/0/1/0/all/0/1\">Danilo Comminiello</a>",
          "description": "Latest Generative Adversarial Networks (GANs) are gathering outstanding\nresults through a large-scale training, thus employing models composed of\nmillions of parameters requiring extensive computational capabilities. Building\nsuch huge models undermines their replicability and increases the training\ninstability. Moreover, multi-channel data, such as images or audio, are usually\nprocessed by realvalued convolutional networks that flatten and concatenate the\ninput, often losing intra-channel spatial relations. To address these issues\nrelated to complexity and information loss, we propose a family of\nquaternion-valued generative adversarial networks (QGANs). QGANs exploit the\nproperties of quaternion algebra, e.g., the Hamilton product, that allows to\nprocess channels as a single entity and capture internal latent relations,\nwhile reducing by a factor of 4 the overall number of parameters. We show how\nto design QGANs and to extend the proposed approach even to advanced models.We\ncompare the proposed QGANs with real-valued counterparts on several image\ngeneration benchmarks. Results show that QGANs are able to obtain better FID\nscores than real-valued GANs and to generate visually pleasing images.\nFurthermore, QGANs save up to 75% of the training parameters. We believe these\nresults may pave the way to novel, more accessible, GANs capable of improving\nperformance and saving computational resources.",
          "link": "http://arxiv.org/abs/2104.09630",
          "publishedOn": "2021-07-28T02:02:33.225Z",
          "wordCount": 678,
          "title": "Quaternion Generative Adversarial Networks. (arXiv:2104.09630v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sorochan_K/0/1/0/all/0/1\">Kynan Sorochan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jerry Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yakun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Machine learning has been a popular tool in many different fields, including\nprocedural content generation. However, procedural content generation via\nmachine learning (PCGML) approaches can struggle with controllability and\ncoherence. In this paper, we attempt to address these problems by learning to\ngenerate human-like paths, and then generating levels based on these paths. We\nextract player path data from gameplay video, train an LSTM to generate new\npaths based on this data, and then generate game levels based on this path\ndata. We demonstrate that our approach leads to more coherent levels for the\ngame Lode Runner in comparison to an existing PCGML approach.",
          "link": "http://arxiv.org/abs/2107.12532",
          "publishedOn": "2021-07-28T02:02:33.218Z",
          "wordCount": 562,
          "title": "Generating Lode Runner Levels by Learning Player Paths with LSTMs. (arXiv:2107.12532v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadasivan_V/0/1/0/all/0/1\">Vinu Sankar Sadasivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1\">Anirban Dasgupta</a>",
          "description": "Curriculum learning is a training strategy that sorts the training examples\nby some measure of their difficulty and gradually exposes them to the learner\nto improve the network performance. Motivated by our insights from implicit\ncurriculum ordering, we first introduce a simple curriculum learning strategy\nthat uses statistical measures such as standard deviation and entropy values to\nscore the difficulty of data points for real image classification tasks. We\nempirically show its improvements in performance with convolutional and\nfully-connected neural networks on multiple real image datasets. We also\npropose and study the performance of a dynamic curriculum learning algorithm.\nOur dynamic curriculum algorithm tries to reduce the distance between the\nnetwork weight and an optimal weight at any training step by greedily sampling\nexamples with gradients that are directed towards the optimal weight. Further,\nwe use our algorithms to discuss why curriculum learning is helpful.",
          "link": "http://arxiv.org/abs/2103.00147",
          "publishedOn": "2021-07-28T02:02:33.211Z",
          "wordCount": 614,
          "title": "Statistical Measures For Defining Curriculum Scoring Function. (arXiv:2103.00147v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wenlong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhiling Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhang Gu</a>",
          "description": "Generative adversarial networks (GANs) have promoted remarkable advances in\nsingle-image super-resolution (SR) by recovering photo-realistic images.\nHowever, high memory consumption of GAN-based SR (usually generators) causes\nperformance degradation and more energy consumption, hindering the deployment\nof GAN-based SR into resource-constricted mobile devices. In this paper, we\npropose a novel compression framework \\textbf{M}ulti-scale \\textbf{F}eature\n\\textbf{A}ggregation Net based \\textbf{GAN} (MFAGAN) for reducing the memory\naccess cost of the generator. First, to overcome the memory explosion of dense\nconnections, we utilize a memory-efficient multi-scale feature aggregation net\nas the generator. Second, for faster and more stable training, our method\nintroduces the PatchGAN discriminator. Third, to balance the student\ndiscriminator and the compressed generator, we distill both the generator and\nthe discriminator. Finally, we perform a hardware-aware neural architecture\nsearch (NAS) to find a specialized SubGenerator for the target mobile phone.\nBenefiting from these improvements, the proposed MFAGAN achieves up to\n\\textbf{8.3}$\\times$ memory saving and \\textbf{42.9}$\\times$ computation\nreduction, with only minor visual quality degradation, compared with ESRGAN.\nEmpirical studies also show $\\sim$\\textbf{70} milliseconds latency on Qualcomm\nSnapdragon 865 chipset.",
          "link": "http://arxiv.org/abs/2107.12679",
          "publishedOn": "2021-07-28T02:02:33.193Z",
          "wordCount": 610,
          "title": "MFAGAN: A Compression Framework for Memory-Efficient On-Device Super-Resolution GAN. (arXiv:2107.12679v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_B/0/1/0/all/0/1\">Byungjin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yu Xiao</a>",
          "description": "Vehicular fog computing (VFC) pushes the cloud computing capability to the\ndistributed fog nodes at the edge of the Internet, enabling compute-intensive\nand latency-sensitive computing services for vehicles through task offloading.\nHowever, a heterogeneous mobility environment introduces uncertainties in terms\nof resource supply and demand, which are inevitable bottlenecks for the optimal\noffloading decision. Also, these uncertainties bring extra challenges to task\noffloading under the oblivious adversary attack and data privacy risks. In this\narticle, we develop a new adversarial online learning algorithm with bandit\nfeedback based on the adversarial multi-armed bandit theory, to enable scalable\nand low-complexity offloading decision making. Specifically, we focus on\noptimizing fog node selection with the aim of minimizing the offloading service\ncosts in terms of delay and energy. The key is to implicitly tune the\nexploration bonus in the selection process and the assessment rules of the\ndesigned algorithm, taking into account volatile resource supply and demand. We\ntheoretically prove that the input-size dependent selection rule allows to\nchoose a suitable fog node without exploring the sub-optimal actions, and also\nan appropriate score patching rule allows to quickly adapt to evolving\ncircumstances, which reduce variance and bias simultaneously, thereby achieving\na better exploitation-exploration balance. Simulation results verify the\neffectiveness and robustness of the proposed algorithm.",
          "link": "http://arxiv.org/abs/2104.12827",
          "publishedOn": "2021-07-28T02:02:33.186Z",
          "wordCount": 700,
          "title": "Learning-based decentralized offloading decision making in an adversarial environment. (arXiv:2104.12827v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benaroya_L/0/1/0/all/0/1\">Laurent Benaroya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obin_N/0/1/0/all/0/1\">Nicolas Obin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roebel_A/0/1/0/all/0/1\">Axel Roebel</a>",
          "description": "Voice conversion (VC) consists of digitally altering the voice of an\nindividual to manipulate part of its content, primarily its identity, while\nmaintaining the rest unchanged. Research in neural VC has accomplished\nconsiderable breakthroughs with the capacity to falsify a voice identity using\na small amount of data with a highly realistic rendering. This paper goes\nbeyond voice identity and presents a neural architecture that allows the\nmanipulation of voice attributes (e.g., gender and age). Leveraging the latest\nadvances on adversarial learning of structured speech representation, a novel\nstructured neural network is proposed in which multiple auto-encoders are used\nto encode speech as a set of idealistically independent linguistic and\nextra-linguistic representations, which are learned adversariarly and can be\nmanipulated during VC. Moreover, the proposed architecture is time-synchronized\nso that the original voice timing is preserved during conversion which allows\nlip-sync applications. Applied to voice gender conversion on the real-world\nVCTK dataset, our proposed architecture can learn successfully\ngender-independent representation and convert the voice gender with a very high\nefficiency and naturalness.",
          "link": "http://arxiv.org/abs/2107.12346",
          "publishedOn": "2021-07-28T02:02:33.178Z",
          "wordCount": 637,
          "title": "Beyond Voice Identity Conversion: Manipulating Voice Attributes by Adversarial Learning of Structured Disentangled Representations. (arXiv:2107.12346v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Team_Open_Ended_Learning/0/1/0/all/0/1\">Open-Ended Learning Team</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stooke_A/0/1/0/all/0/1\">Adam Stooke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1\">Anuj Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barros_C/0/1/0/all/0/1\">Catarina Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deck_C/0/1/0/all/0/1\">Charlie Deck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_J/0/1/0/all/0/1\">Jakob Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sygnowski_J/0/1/0/all/0/1\">Jakub Sygnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trebacz_M/0/1/0/all/0/1\">Maja Trebacz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaderberg_M/0/1/0/all/0/1\">Max Jaderberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_M/0/1/0/all/0/1\">Michael Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAleese_N/0/1/0/all/0/1\">Nat McAleese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_Schmieg_N/0/1/0/all/0/1\">Nathalie Bradley-Schmieg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Nathaniel Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porcel_N/0/1/0/all/0/1\">Nicolas Porcel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1\">Roberta Raileanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_Fitt_S/0/1/0/all/0/1\">Steph Hughes-Fitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalibard_V/0/1/0/all/0/1\">Valentin Dalibard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czarnecki_W/0/1/0/all/0/1\">Wojciech Marian Czarnecki</a>",
          "description": "In this work we create agents that can perform well beyond a single,\nindividual task, that exhibit much wider generalisation of behaviour to a\nmassive, rich space of challenges. We define a universe of tasks within an\nenvironment domain and demonstrate the ability to train agents that are\ngenerally capable across this vast space and beyond. The environment is\nnatively multi-agent, spanning the continuum of competitive, cooperative, and\nindependent games, which are situated within procedurally generated physical 3D\nworlds. The resulting space is exceptionally diverse in terms of the challenges\nposed to agents, and as such, even measuring the learning progress of an agent\nis an open research problem. We propose an iterative notion of improvement\nbetween successive generations of agents, rather than seeking to maximise a\nsingular objective, allowing us to quantify progress despite tasks being\nincomparable in terms of achievable rewards. We show that through constructing\nan open-ended learning process, which dynamically changes the training task\ndistributions and training objectives such that the agent never stops learning,\nwe achieve consistent learning of new behaviours. The resulting agent is able\nto score reward in every one of our humanly solvable evaluation levels, with\nbehaviour generalising to many held-out points in the universe of tasks.\nExamples of this zero-shot generalisation include good performance on Hide and\nSeek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks\nwe characterise the behaviour of our agent, and find interesting emergent\nheuristic behaviours such as trial-and-error experimentation, simple tool use,\noption switching, and cooperation. Finally, we demonstrate that the general\ncapabilities of this agent could unlock larger scale transfer of behaviour\nthrough cheap finetuning.",
          "link": "http://arxiv.org/abs/2107.12808",
          "publishedOn": "2021-07-28T02:02:33.166Z",
          "wordCount": 738,
          "title": "Open-Ended Learning Leads to Generally Capable Agents. (arXiv:2107.12808v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gudovskiy_D/0/1/0/all/0/1\">Denis Gudovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishizaka_S/0/1/0/all/0/1\">Shun Ishizaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1\">Kazuki Kozuka</a>",
          "description": "Unsupervised anomaly detection with localization has many practical\napplications when labeling is infeasible and, moreover, when anomaly examples\nare completely missing in the train data. While recently proposed models for\nsuch data setup achieve high accuracy metrics, their complexity is a limiting\nfactor for real-time processing. In this paper, we propose a real-time model\nand analytically derive its relationship to prior methods. Our CFLOW-AD model\nis based on a conditional normalizing flow framework adopted for anomaly\ndetection with localization. In particular, CFLOW-AD consists of a\ndiscriminatively pretrained encoder followed by a multi-scale generative\ndecoders where the latter explicitly estimate likelihood of the encoded\nfeatures. Our approach results in a computationally and memory-efficient model:\nCFLOW-AD is faster and smaller by a factor of 10x than prior state-of-the-art\nwith the same input setting. Our experiments on the MVTec dataset show that\nCFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by\n1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source\nour code with fully reproducible experiments.",
          "link": "http://arxiv.org/abs/2107.12571",
          "publishedOn": "2021-07-28T02:02:33.147Z",
          "wordCount": 620,
          "title": "CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows. (arXiv:2107.12571v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zisen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Co-creative Procedural Content Generation via Machine Learning (PCGML) refers\nto systems where a PCGML agent and a human work together to produce output\ncontent. One of the limitations of co-creative PCGML is that it requires\nco-creative training data for a PCGML agent to learn to interact with humans.\nHowever, acquiring this data is a difficult and time-consuming process. In this\nwork, we propose approximating human-AI interaction data and employing transfer\nlearning to adapt learned co-creative knowledge from one game to a different\ngame. We explore this approach for co-creative Zelda dungeon room generation.",
          "link": "http://arxiv.org/abs/2107.12533",
          "publishedOn": "2021-07-28T02:02:33.138Z",
          "wordCount": 545,
          "title": "Toward Co-creative Dungeon Generation via Transfer Learning. (arXiv:2107.12533v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianxin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bing Yao</a>",
          "description": "The rapid developments in advanced sensing and imaging bring about a\ndata-rich environment, facilitating the effective modeling, monitoring, and\ncontrol of complex systems. For example, the body-sensor network captures\nmulti-channel information pertinent to the electrical activity of the heart\n(i.e., electrocardiograms (ECG)), which enables medical scientists to monitor\nand detect abnormal cardiac conditions. However, the high-dimensional sensing\ndata are generally complexly structured and realizing the full data potential\ndepends to a great extent on advanced analytical and predictive methods. This\npaper presents a physics-constrained deep learning (P-DL) framework for\nhigh-dimensional inverse ECG modeling. This method integrates the physical laws\nof the complex system with the advanced deep learning infrastructure for\neffective prediction of the system dynamics. The proposed P-DL approach is\nimplemented to solve the inverse ECG model and predict the time-varying\ndistribution of electric potentials in the heart from the ECG data measured by\nthe body-surface sensor network. Experimental results show that the proposed\nP-DL method significantly outperforms existing methods that are commonly used\nin current practice.",
          "link": "http://arxiv.org/abs/2107.12780",
          "publishedOn": "2021-07-28T02:02:33.131Z",
          "wordCount": 594,
          "title": "Physics-constrained Deep Learning for Robust Inverse ECG Modeling. (arXiv:2107.12780v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cromp_S/0/1/0/all/0/1\">Sonia Cromp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samadian_A/0/1/0/all/0/1\">Alireza Samadian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruhs_K/0/1/0/all/0/1\">Kirk Pruhs</a>",
          "description": "Many tasks use data housed in relational databases to train boosted\nregression tree models. In this paper, we give a relational adaptation of the\ngreedy algorithm for training boosted regression trees. For the subproblem of\ncalculating the sum of squared residuals of the dataset, which dominates the\nruntime of the boosting algorithm, we provide a $(1 + \\epsilon)$-approximation\nusing the tensor sketch technique. Employing this approximation within the\nrelational boosted regression trees algorithm leads to learning similar model\nparameters, but with asymptotically better runtime.",
          "link": "http://arxiv.org/abs/2107.12373",
          "publishedOn": "2021-07-28T02:02:33.124Z",
          "wordCount": 513,
          "title": "Relational Boosted Regression Trees. (arXiv:2107.12373v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1\">P.S. Sastry</a>",
          "description": "Deep Neural Networks (DNNs) have been shown to be susceptible to memorization\nor overfitting in the presence of noisily labelled data. For the problem of\nrobust learning under such noisy data, several algorithms have been proposed. A\nprominent class of algorithms rely on sample selection strategies, motivated by\ncurriculum learning. For example, many algorithms use the `small loss trick'\nwherein a fraction of samples with loss values below a certain threshold are\nselected for training. These algorithms are sensitive to such thresholds, and\nit is difficult to fix or learn these thresholds. Often, these algorithms also\nrequire information such as label noise rates which are typically unavailable\nin practice. In this paper, we propose a data-dependent, adaptive sample\nselection strategy that relies only on batch statistics of a given mini-batch\nto provide robustness against label noise. The algorithm does not have any\nadditional hyperparameters for sample selection, does not need any information\non noise rates, and does not need access to separate data with clean labels. We\nempirically demonstrate the effectiveness of our algorithm on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.15292",
          "publishedOn": "2021-07-28T02:02:33.117Z",
          "wordCount": 641,
          "title": "Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halina_E/0/1/0/all/0/1\">Emily Halina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Generating rhythm game charts from songs via machine learning has been a\nproblem of increasing interest in recent years. However, all existing systems\nstruggle to replicate human-like patterning: the placement of game objects in\nrelation to each other to form congruent patterns based on events in the song.\nPatterning is a key identifier of high quality rhythm game content, seen as a\nnecessary component in human rankings. We establish a new approach for chart\ngeneration that produces charts with more congruent, human-like patterning than\nseen in prior work.",
          "link": "http://arxiv.org/abs/2107.12506",
          "publishedOn": "2021-07-28T02:02:33.105Z",
          "wordCount": 547,
          "title": "TaikoNation: Patterning-focused Chart Generation for Rhythm Action Games. (arXiv:2107.12506v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aouali_I/0/1/0/all/0/1\">Imad Aouali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_S/0/1/0/all/0/1\">Sergey Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gartrell_M/0/1/0/all/0/1\">Mike Gartrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohde_D/0/1/0/all/0/1\">David Rohde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasile_F/0/1/0/all/0/1\">Flavian Vasile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaytsev_V/0/1/0/all/0/1\">Victor Zaytsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Legrand_D/0/1/0/all/0/1\">Diego Legrand</a>",
          "description": "We consider the problem of slate recommendation, where the recommender system\npresents a user with a collection or slate composed of K recommended items at\nonce. If the user finds the recommended items appealing then the user may click\nand the recommender system receives some feedback. Two pieces of information\nare available to the recommender system: was the slate clicked? (the reward),\nand if the slate was clicked, which item was clicked? (rank). In this paper, we\nformulate several Bayesian models that incorporate the reward signal (Reward\nmodel), the rank signal (Rank model), or both (Full model), for\nnon-personalized slate recommendation. In our experiments, we analyze\nperformance gains of the Full model and show that it achieves significantly\nlower error as the number of products in the catalog grows or as the slate size\nincreases.",
          "link": "http://arxiv.org/abs/2107.12455",
          "publishedOn": "2021-07-28T02:02:33.086Z",
          "wordCount": 603,
          "title": "Combining Reward and Rank Signals for Slate Recommendation. (arXiv:2107.12455v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peizhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1\">Gao Cong</a>",
          "description": "Cardinality estimation is a fundamental problem in database systems. To\ncapture the rich joint data distributions of a relational table, most of the\nexisting work either uses data as unsupervised information or uses query\nworkload as supervised information. Very little work has been done to use both\ntypes of information, and cannot fully make use of both types of information to\nlearn the joint data distribution. In this work, we aim to close the gap\nbetween data-driven and query-driven methods by proposing a new unified deep\nautoregressive model, UAE, that learns the joint data distribution from both\nthe data and query workload. First, to enable using the supervised query\ninformation in the deep autoregressive model, we develop differentiable\nprogressive sampling using the Gumbel-Softmax trick. Second, UAE is able to\nutilize both types of information to learn the joint data distribution in a\nsingle model. Comprehensive experimental results demonstrate that UAE achieves\nsingle-digit multiplicative error at tail, better accuracies over\nstate-of-the-art methods, and is both space and time efficient.",
          "link": "http://arxiv.org/abs/2107.12295",
          "publishedOn": "2021-07-28T02:02:33.078Z",
          "wordCount": 615,
          "title": "A Unified Deep Model of Learning from both Data and Queries for Cardinality Estimation. (arXiv:2107.12295v1 [cs.DB] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Varela_J/0/1/0/all/0/1\">Jos&#xe9; Su&#xe1;rez-Varela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferriol_Galmes_M/0/1/0/all/0/1\">Miquel Ferriol-Galm&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Albert L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almasan_P/0/1/0/all/0/1\">Paul Almasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardez_G/0/1/0/all/0/1\">Guillermo Bern&#xe1;rdez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujol_Perich_D/0/1/0/all/0/1\">David Pujol-Perich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusek_K/0/1/0/all/0/1\">Krzysztof Rusek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonniot_L/0/1/0/all/0/1\">Lo&#xef;ck Bonniot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_C/0/1/0/all/0/1\">Christoph Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnitzler_F/0/1/0/all/0/1\">Fran&#xe7;ois Schnitzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taiani_F/0/1/0/all/0/1\">Fran&#xe7;ois Ta&#xef;ani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Happ_M/0/1/0/all/0/1\">Martin Happ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_C/0/1/0/all/0/1\">Christian Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jia Lei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herlich_M/0/1/0/all/0/1\">Matthias Herlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorfinger_P/0/1/0/all/0/1\">Peter Dorfinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hainke_N/0/1/0/all/0/1\">Nick Vincent Hainke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venz_S/0/1/0/all/0/1\">Stefan Venz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegener_J/0/1/0/all/0/1\">Johannes Wegener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wissing_H/0/1/0/all/0/1\">Henrike Wissing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shihan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlet_Ros_P/0/1/0/all/0/1\">Pere Barlet-Ros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabellos_Aparicio_A/0/1/0/all/0/1\">Albert Cabellos-Aparicio</a>",
          "description": "During the last decade, Machine Learning (ML) has increasingly become a hot\ntopic in the field of Computer Networks and is expected to be gradually adopted\nfor a plethora of control, monitoring and management tasks in real-world\ndeployments. This poses the need to count on new generations of students,\nresearchers and practitioners with a solid background in ML applied to\nnetworks. During 2020, the International Telecommunication Union (ITU) has\norganized the \"ITU AI/ML in 5G challenge'', an open global competition that has\nintroduced to a broad audience some of the current main challenges in ML for\nnetworks. This large-scale initiative has gathered 23 different challenges\nproposed by network operators, equipment manufacturers and academia, and has\nattracted a total of 1300+ participants from 60+ countries. This paper narrates\nour experience organizing one of the proposed challenges: the \"Graph Neural\nNetworking Challenge 2020''. We describe the problem presented to participants,\nthe tools and resources provided, some organization aspects and participation\nstatistics, an outline of the top-3 awarded solutions, and a summary with some\nlessons learned during all this journey. As a result, this challenge leaves a\ncurated set of educational resources openly available to anyone interested in\nthe topic.",
          "link": "http://arxiv.org/abs/2107.12433",
          "publishedOn": "2021-07-28T02:02:33.071Z",
          "wordCount": 712,
          "title": "The Graph Neural Networking Challenge: A Worldwide Competition for Education in AI/ML for Networks. (arXiv:2107.12433v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ponomarev_E/0/1/0/all/0/1\">Evgeny Ponomarev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matveev_S/0/1/0/all/0/1\">Sergey Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1\">Ivan Oseledets</a>",
          "description": "A lot of deep learning applications are desired to be run on mobile devices.\nBoth accuracy and inference time are meaningful for a lot of them. While the\nnumber of FLOPs is usually used as a proxy for neural network latency, it may\nbe not the best choice. In order to obtain a better approximation of latency,\nresearch community uses look-up tables of all possible layers for latency\ncalculation for the final prediction of the inference on mobile CPU. It\nrequires only a small number of experiments. Unfortunately, on mobile GPU this\nmethod is not applicable in a straight-forward way and shows low precision. In\nthis work, we consider latency approximation on mobile GPU as a data and\nhardware-specific problem. Our main goal is to construct a convenient latency\nestimation tool for investigation(LETI) of neural network inference and\nbuilding robust and accurate latency prediction models for each specific task.\nTo achieve this goal, we build open-source tools which provide a convenient way\nto conduct massive experiments on different target devices focusing on mobile\nGPU. After evaluation of the dataset, we learn the regression model on\nexperimental data and use it for future latency prediction and analysis. We\nexperimentally demonstrate the applicability of such an approach on a subset of\npopular NAS-Benchmark 101 dataset and also evaluate the most popular neural\nnetwork architectures for two mobile GPUs. As a result, we construct latency\nprediction model with good precision on the target evaluation subset. We\nconsider LETI as a useful tool for neural architecture search or massive\nlatency evaluation. The project is available at https://github.com/leti-ai",
          "link": "http://arxiv.org/abs/2010.02871",
          "publishedOn": "2021-07-28T02:02:33.062Z",
          "wordCount": 758,
          "title": "LETI: Latency Estimation Tool and Investigation of Neural Networks inference on Mobile GPU. (arXiv:2010.02871v2 [cs.PF] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laousy_O/0/1/0/all/0/1\">Othmane Laousy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1\">Edouard Oyallon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>",
          "description": "Sarcopenia is a medical condition characterized by a reduction in muscle mass\nand function. A quantitative diagnosis technique consists of localizing the CT\nslice passing through the middle of the third lumbar area (L3) and segmenting\nmuscles at this level. In this paper, we propose a deep reinforcement learning\nmethod for accurate localization of the L3 CT slice. Our method trains a\nreinforcement learning agent by incentivizing it to discover the right\nposition. Specifically, a Deep Q-Network is trained to find the best policy to\nfollow for this problem. Visualizing the training process shows that the agent\nmimics the scrolling of an experienced radiologist. Extensive experiments\nagainst other state-of-the-art deep learning based methods for L3 localization\nprove the superiority of our technique which performs well even with limited\namount of data and annotations.",
          "link": "http://arxiv.org/abs/2107.12800",
          "publishedOn": "2021-07-28T02:02:33.031Z",
          "wordCount": 577,
          "title": "Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment. (arXiv:2107.12800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12040",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenxi Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrui Zeng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chang_Y/0/1/0/all/0/1\">Yi-Wei Chang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that\nvisualizes the structural and spatial organization of macromolecules at a\nnear-native state in single cells, which has broad applications in life\nscience. However, the systematic structural recognition and recovery of\nmacromolecules captured by cryo-ET are difficult due to high structural\ncomplexity and imaging limits. Deep learning based subtomogram classification\nhave played critical roles for such tasks. As supervised approaches, however,\ntheir performance relies on sufficient and laborious annotation on a large\ntraining dataset.\n\nResults: To alleviate this major labeling burden, we proposed a Hybrid Active\nLearning (HAL) framework for querying subtomograms for labelling from a large\nunlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select\nthe subtomograms that have the most uncertain predictions. Moreover, to\nmitigate the sampling bias caused by such strategy, a discriminator is\nintroduced to judge if a certain subtomogram is labeled or unlabeled and\nsubsequently the model queries the subtomogram that have higher probabilities\nto be unlabeled. Additionally, HAL introduces a subset sampling strategy to\nimprove the diversity of the query set, so that the information overlap is\ndecreased between the queried batches and the algorithmic efficiency is\nimproved. Our experiments on subtomogram classification tasks using both\nsimulated and real data demonstrate that we can achieve comparable testing\nperformance (on average only 3% accuracy drop) by using less than 30% of the\nlabeled subtomograms, which shows a very promising result for subtomogram\nclassification task with limited labeling resources.",
          "link": "http://arxiv.org/abs/2102.12040",
          "publishedOn": "2021-07-28T02:02:33.023Z",
          "wordCount": 783,
          "title": "Active Learning to Classify Macromolecular Structures in situ for Less Supervision in Cryo-Electron Tomography. (arXiv:2102.12040v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>",
          "description": "Federated learning (FL) is an emerging practical framework for effective and\nscalable machine learning among multiple participants, such as end users,\norganizations and companies. However, most existing FL or distributed learning\nframeworks have not well addressed two important issues together: collaborative\nfairness and adversarial robustness (e.g. free-riders and malicious\nparticipants). In conventional FL, all participants receive the global model\n(equal rewards), which might be unfair to the high-contributing participants.\nFurthermore, due to the lack of a safeguard mechanism, free-riders or malicious\nadversaries could game the system to access the global model for free or to\nsabotage it. In this paper, we propose a novel Robust and Fair Federated\nLearning (RFFL) framework to achieve collaborative fairness and adversarial\nrobustness simultaneously via a reputation mechanism. RFFL maintains a\nreputation for each participant by examining their contributions via their\nuploaded gradients (using vector similarity) and thus identifies\nnon-contributing or malicious participants to be removed. Our approach\ndifferentiates itself by not requiring any auxiliary/validation dataset.\nExtensive experiments on benchmark datasets show that RFFL can achieve high\nfairness and is very robust to different types of adversaries while achieving\ncompetitive predictive accuracy.",
          "link": "http://arxiv.org/abs/2011.10464",
          "publishedOn": "2021-07-28T02:02:33.007Z",
          "wordCount": 683,
          "title": "A Reputation Mechanism Is All You Need: Collaborative Fairness and Adversarial Robustness in Federated Learning. (arXiv:2011.10464v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azari_B/0/1/0/all/0/1\">Bahar Azari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1\">Deniz Erdogmus</a>",
          "description": "Despite the vast success of standard planar convolutional neural networks,\nthey are not the most efficient choice for analyzing signals that lie on an\narbitrarily curved manifold, such as a cylinder. The problem arises when one\nperforms a planar projection of these signals and inevitably causes them to be\ndistorted or broken where there is valuable information. We propose a\nCircular-symmetric Correlation Layer (CCL) based on the formalism of\nroto-translation equivariant correlation on the continuous group $S^1 \\times\n\\mathbb{R}$, and implement it efficiently using the well-known Fast Fourier\nTransform (FFT) algorithm. We showcase the performance analysis of a general\nnetwork equipped with CCL on various recognition and classification tasks and\ndatasets. The PyTorch package implementation of CCL is provided online.",
          "link": "http://arxiv.org/abs/2107.12480",
          "publishedOn": "2021-07-28T02:02:33.000Z",
          "wordCount": 552,
          "title": "Circular-Symmetric Correlation Layer based on FFT. (arXiv:2107.12480v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12525",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kang_D/0/1/0/all/0/1\">Daniel Kang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Guibas_J/0/1/0/all/0/1\">John Guibas</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bailis_P/0/1/0/all/0/1\">Peter Bailis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Given a dataset $\\mathcal{D}$, we are interested in computing the mean of a\nsubset of $\\mathcal{D}$ which matches a predicate. \\algname leverages\nstratified sampling and proxy models to efficiently compute this statistic\ngiven a sampling budget $N$. In this document, we theoretically analyze\n\\algname and show that the MSE of the estimate decays at rate $O(N_1^{-1} +\nN_2^{-1} + N_1^{1/2}N_2^{-3/2})$, where $N=K \\cdot N_1+N_2$ for some integer\nconstant $K$ and $K \\cdot N_1$ and $N_2$ represent the number of samples used\nin Stage 1 and Stage 2 of \\algname respectively. Hence, if a constant fraction\nof the total sample budget $N$ is allocated to each stage, we will achieve a\nmean squared error of $O(N^{-1})$ which matches the rate of mean squared error\nof the optimal stratified sampling algorithm given a priori knowledge of the\npredicate positive rate and standard deviation per stratum.",
          "link": "http://arxiv.org/abs/2107.12525",
          "publishedOn": "2021-07-28T02:02:32.982Z",
          "wordCount": 591,
          "title": "Proof: Accelerating Approximate Aggregation Queries with Expensive Predicates. (arXiv:2107.12525v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10205",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Y/0/1/0/all/0/1\">Yongqian Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Q/0/1/0/all/0/1\">QianLi Lin</a>",
          "description": "With the development of end-to-end control based on deep learning, it is\nimportant to study new system modeling techniques to realize dynamics modeling\nwith high-dimensional inputs. In this paper, a novel Koopman-based deep\nconvolutional network, called CKNet, is proposed to identify latent dynamics\nfrom raw pixels. CKNet learns an encoder and decoder to play the role of the\nKoopman eigenfunctions and modes, respectively. The Koopman eigenvalues can be\napproximated by eigenvalues of the learned state transition matrix. The\ndeterministic convolutional Koopman network (DCKNet) and the variational\nconvolutional Koopman network (VCKNet) are proposed to span some subspace for\napproximating the Koopman operator respectively. Because CKNet is trained under\nthe constraints of the Koopman theory, the identified latent dynamics is in a\nlinear form and has good interpretability. Besides, the state transition and\ncontrol matrices are trained as trainable tensors so that the identified\ndynamics is also time-invariant. We also design an auxiliary weight term for\nreducing multi-step linearity and prediction losses. Experiments were conducted\non two offline trained and four online trained nonlinear forced dynamical\nsystems with continuous action spaces in Gym and Mujoco environment\nrespectively, and the results show that identified dynamics are adequate for\napproximating the latent dynamics and generating clear images. Especially for\noffline trained cases, this work confirms CKNet from a novel perspective that\nwe visualize the evolutionary processes of the latent states and the Koopman\neigenfunctions with DCKNet and VCKNet separately to each task based on the same\nepisode and results demonstrate that different approaches learn similar\nfeatures in shapes.",
          "link": "http://arxiv.org/abs/2102.10205",
          "publishedOn": "2021-07-28T02:02:32.974Z",
          "wordCount": 729,
          "title": "CKNet: A Convolutional Neural Network Based on Koopman Operator for Modeling Latent Dynamics from Pixels. (arXiv:2102.10205v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabo_A/0/1/0/all/0/1\">Andrea Sabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdizadeh_S/0/1/0/all/0/1\">Sina Mehdizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iaboni_A/0/1/0/all/0/1\">Andrea Iaboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taati_B/0/1/0/all/0/1\">Babak Taati</a>",
          "description": "Drug-induced parkinsonism affects many older adults with dementia, often\ncausing gait disturbances. New advances in vision-based human pose-estimation\nhave opened possibilities for frequent and unobtrusive analysis of gait in\nresidential settings. This work proposes novel spatial-temporal graph\nconvolutional network (ST-GCN) architectures and training procedures to predict\nclinical scores of parkinsonism in gait from video of individuals with\ndementia. We propose a two-stage training approach consisting of a\nself-supervised pretraining stage that encourages the ST-GCN model to learn\nabout gait patterns before predicting clinical scores in the finetuning stage.\nThe proposed ST-GCN models are evaluated on joint trajectories extracted from\nvideo and are compared against traditional (ordinal, linear, random forest)\nregression models and temporal convolutional network baselines. Three 2D human\npose-estimation libraries (OpenPose, Detectron, AlphaPose) and the Microsoft\nKinect (2D and 3D) are used to extract joint trajectories of 4787 natural\nwalking bouts from 53 older adults with dementia. A subset of 399 walks from 14\nparticipants is annotated with scores of parkinsonism severity on the gait\ncriteria of the Unified Parkinson's Disease Rating Scale (UPDRS) and the\nSimpson-Angus Scale (SAS). Our results demonstrate that ST-GCN models operating\non 3D joint trajectories extracted from the Kinect consistently outperform all\nother models and feature sets. Prediction of parkinsonism scores in natural\nwalking bouts of unseen participants remains a challenging task, with the best\nmodels achieving macro-averaged F1-scores of 0.53 +/- 0.03 and 0.40 +/- 0.02\nfor UPDRS-gait and SAS-gait, respectively. Pre-trained model and demo code for\nthis work is available:\nhttps://github.com/TaatiTeam/stgcn_parkinsonism_prediction.",
          "link": "http://arxiv.org/abs/2105.03464",
          "publishedOn": "2021-07-28T02:02:32.967Z",
          "wordCount": 723,
          "title": "Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults with Dementia. (arXiv:2105.03464v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>",
          "description": "This paper presents the evaluation of effects of image size on deep learning\nperformance via semantic segmentation of magnetic resonance heart images with\nU-net for fully automated quantification of myocardial infarction. Both\nnon-extra pixel and extra pixel interpolation algorithms are used to change the\nsize of images in datasets of interest. Extra class labels, in interpolated\nground truth segmentation images, are removed using thresholding, median\nfiltering, and subtraction strategies. Common class metrics are used to\nevaluate the quality of semantic segmentation with U-net against the ground\ntruth segmentation while arbitrary threshold, comparison of the sums, and sums\nof differences between medical experts and fully automated results are options\nused to estimate the relationship between medical experts-based quantification\nand fully automated quantification results.",
          "link": "http://arxiv.org/abs/2101.11508",
          "publishedOn": "2021-07-28T02:02:32.957Z",
          "wordCount": 592,
          "title": "Effects of Image Size on Deep Learning. (arXiv:2101.11508v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12438",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Gupta_V/0/1/0/all/0/1\">Vishal Gupta</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_M/0/1/0/all/0/1\">Michael Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rusmevichientong_P/0/1/0/all/0/1\">Paat Rusmevichientong</a>",
          "description": "Motivated by the poor performance of cross-validation in settings where data\nare scarce, we propose a novel estimator of the out-of-sample performance of a\npolicy in data-driven optimization.Our approach exploits the optimization\nproblem's sensitivity analysis to estimate the gradient of the optimal\nobjective value with respect to the amount of noise in the data and uses the\nestimated gradient to debias the policy's in-sample performance. Unlike\ncross-validation techniques, our approach avoids sacrificing data for a test\nset, utilizes all data when training and, hence, is well-suited to settings\nwhere data are scarce. We prove bounds on the bias and variance of our\nestimator for optimization problems with uncertain linear objectives but known,\npotentially non-convex, feasible regions. For more specialized optimization\nproblems where the feasible region is ``weakly-coupled\" in a certain sense, we\nprove stronger results. Specifically, we provide explicit high-probability\nbounds on the error of our estimator that hold uniformly over a policy class\nand depends on the problem's dimension and policy class's complexity. Our\nbounds show that under mild conditions, the error of our estimator vanishes as\nthe dimension of the optimization problem grows, even if the amount of\navailable data remains small and constant. Said differently, we prove our\nestimator performs well in the small-data, large-scale regime. Finally, we\nnumerically compare our proposed method to state-of-the-art approaches through\na case-study on dispatching emergency medical response services using real\ndata. Our method provides more accurate estimates of out-of-sample performance\nand learns better-performing policies.",
          "link": "http://arxiv.org/abs/2107.12438",
          "publishedOn": "2021-07-28T02:02:32.948Z",
          "wordCount": 683,
          "title": "Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization. (arXiv:2107.12438v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_R/0/1/0/all/0/1\">Raz Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_Y/0/1/0/all/0/1\">Yuval Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_K/0/1/0/all/0/1\">Kobi Cohen</a>",
          "description": "We consider a distributed learning problem in a wireless network, consisting\nof N distributed edge devices and a parameter server (PS). The objective\nfunction is a sum of the edge devices' local loss functions, who aim to train a\nshared model by communicating with the PS over multiple access channels (MAC).\nThis problem has attracted a growing interest in distributed sensing systems,\nand more recently in federated learning, known as over-the-air computation. In\nthis paper, we develop a novel Accelerated Gradient-descent Multiple Access\n(AGMA) algorithm that uses momentum-based gradient signals over noisy fading\nMAC to improve the convergence rate as compared to existing methods.\nFurthermore, AGMA does not require power control or beamforming to cancel the\nfading effect, which simplifies the implementation complexity. We analyze AGMA\ntheoretically, and establish a finite-sample bound of the error for both convex\nand strongly convex loss functions with Lipschitz gradient. For the strongly\nconvex case, we show that AGMA approaches the best-known linear convergence\nrate as the network increases. For the convex case, we show that AGMA\nsignificantly improves the sub-linear convergence rate as compared to existing\nmethods. Finally, we present simulation results using real datasets that\ndemonstrate better performance by AGMA.",
          "link": "http://arxiv.org/abs/2107.12452",
          "publishedOn": "2021-07-28T02:02:32.929Z",
          "wordCount": 638,
          "title": "Accelerated Gradient Descent Learning over Multiple Access Fading Channels. (arXiv:2107.12452v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12395",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Kahlhoefer_F/0/1/0/all/0/1\">Felix Kahlhoefer</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Korsmeier_M/0/1/0/all/0/1\">Michael Korsmeier</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kramer_M/0/1/0/all/0/1\">Michael Kr&#xe4;mer</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Manconi_S/0/1/0/all/0/1\">Silvia Manconi</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Nippel_K/0/1/0/all/0/1\">Kathrin Nippel</a>",
          "description": "The interpretation of data from indirect detection experiments searching for\ndark matter annihilations requires computationally expensive simulations of\ncosmic-ray propagation. In this work we present a new method based on Recurrent\nNeural Networks that significantly accelerates simulations of secondary and\ndark matter Galactic cosmic ray antiprotons while achieving excellent accuracy.\nThis approach allows for an efficient profiling or marginalisation over the\nnuisance parameters of a cosmic ray propagation model in order to perform\nparameter scans for a wide range of dark matter models. We identify importance\nsampling as particularly suitable for ensuring that the network is only\nevaluated in well-trained parameter regions. We present resulting constraints\nusing the most recent AMS-02 antiproton data on several models of Weakly\nInteracting Massive Particles. The fully trained networks are released as\nDarkRayNet together with this work and achieve a speed-up of the runtime by at\nleast two orders of magnitude compared to conventional approaches.",
          "link": "http://arxiv.org/abs/2107.12395",
          "publishedOn": "2021-07-28T02:02:32.922Z",
          "wordCount": 608,
          "title": "Constraining dark matter annihilation with cosmic ray antiprotons using neural networks. (arXiv:2107.12395v1 [astro-ph.HE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12698",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Moreno_E/0/1/0/all/0/1\">Eric A. Moreno</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Vlimant_J/0/1/0/all/0/1\">Jean-Roch Vlimant</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Spiropulu_M/0/1/0/all/0/1\">Maria Spiropulu</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Borzyszkowski_B/0/1/0/all/0/1\">Bartlomiej Borzyszkowski</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>",
          "description": "We present an application of anomaly detection techniques based on deep\nrecurrent autoencoders to the problem of detecting gravitational wave signals\nin laser interferometers. Trained on noise data, this class of algorithms could\ndetect signals using an unsupervised strategy, i.e., without targeting a\nspecific kind of source. We develop a custom architecture to analyze the data\nfrom two interferometers. We compare the obtained performance to that obtained\nwith other autoencoder architectures and with a convolutional classifier. The\nunsupervised nature of the proposed strategy comes with a cost in terms of\naccuracy, when compared to more traditional supervised techniques. On the other\nhand, there is a qualitative gain in generalizing the experimental sensitivity\nbeyond the ensemble of pre-computed signal templates. The recurrent autoencoder\noutperforms other autoencoders based on different architectures. The class of\nrecurrent autoencoders presented in this paper could complement the search\nstrategy employed for gravitational wave detection and extend the reach of the\nongoing detection campaigns.",
          "link": "http://arxiv.org/abs/2107.12698",
          "publishedOn": "2021-07-28T02:02:32.904Z",
          "wordCount": 621,
          "title": "Source-Agnostic Gravitational-Wave Detection with Recurrent Autoencoders. (arXiv:2107.12698v1 [gr-qc])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12375",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Atz_K/0/1/0/all/0/1\">Kenneth Atz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Grisoni_F/0/1/0/all/0/1\">Francesca Grisoni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schneider_G/0/1/0/all/0/1\">Gisbert Schneider</a>",
          "description": "Geometric deep learning (GDL), which is based on neural network architectures\nthat incorporate and process symmetry information, has emerged as a recent\nparadigm in artificial intelligence. GDL bears particular promise in molecular\nmodeling applications, in which various molecular representations with\ndifferent symmetry properties and levels of abstraction exist. This review\nprovides a structured and harmonized overview of molecular GDL, highlighting\nits applications in drug discovery, chemical synthesis prediction, and quantum\nchemistry. Emphasis is placed on the relevance of the learned molecular\nfeatures and their complementarity to well-established molecular descriptors.\nThis review provides an overview of current challenges and opportunities, and\npresents a forecast of the future of GDL for molecular sciences.",
          "link": "http://arxiv.org/abs/2107.12375",
          "publishedOn": "2021-07-28T02:02:32.889Z",
          "wordCount": 546,
          "title": "Geometric Deep Learning on Molecular Representations. (arXiv:2107.12375v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12915",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Cho_I/0/1/0/all/0/1\">In Ho Cho</a>",
          "description": "Although researchers accumulated knowledge about seismogenesis and\ndecades-long earthquake data, predicting imminent individual earthquakes at a\nspecific time and location remains a long-standing enigma. This study\nhypothesizes that the observed data conceal the hidden rules which may be\nunraveled by a novel glass-box (as opposed to black-box) physics rule learner\n(GPRL) framework. Without any predefined earthquake-related mechanisms or\nstatistical laws, GPRL's two essentials, convolved information index and\ntransparent link function, seek generic expressions of rules directly from\ndata. GPRL's training with 10-years data appears to identify plausible rules,\nsuggesting a combination of the pseudo power and the pseudo vorticity of\nreleased energy in the lithosphere. Independent feasibility test supports the\npromising role of the unraveled rules in predicting earthquakes' magnitudes and\ntheir specific locations. The identified rules and GPRL are in their infancy\nrequiring substantial improvement. Still, this study hints at the existence of\nthe data-guided hidden pathway to imminent individual earthquake prediction.",
          "link": "http://arxiv.org/abs/2107.12915",
          "publishedOn": "2021-07-28T02:02:32.864Z",
          "wordCount": 600,
          "title": "Initial Foundation for Predicting Individual Earthquake's Location and Magnitude by Using Glass-Box Physics Rule Learner. (arXiv:2107.12915v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12723",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Richards_D/0/1/0/all/0/1\">Dominic Richards</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kuzborskij_I/0/1/0/all/0/1\">Ilja Kuzborskij</a>",
          "description": "We revisit on-average algorithmic stability of Gradient Descent (GD) for\ntraining overparameterised shallow neural networks and prove new generalisation\nand excess risk bounds without the Neural Tangent Kernel (NTK) or\nPolyak-{\\L}ojasiewicz (PL) assumptions. In particular, we show oracle type\nbounds which reveal that the generalisation and excess risk of GD is controlled\nby an interpolating network with the shortest GD path from initialisation (in a\nsense, an interpolating network with the smallest relative norm). While this\nwas known for kernelised interpolants, our proof applies directly to networks\ntrained by GD without intermediate kernelisation. At the same time, by relaxing\noracle inequalities developed here we recover existing NTK-based risk bounds in\na straightforward way, which demonstrates that our analysis is tighter.\nFinally, unlike most of the NTK-based analyses we focus on regression with\nlabel noise and show that GD with early stopping is consistent.",
          "link": "http://arxiv.org/abs/2107.12723",
          "publishedOn": "2021-07-28T02:02:32.857Z",
          "wordCount": 593,
          "title": "Stability & Generalisation of Gradient Descent for Shallow Neural Networks without the Neural Tangent Kernel. (arXiv:2107.12723v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bobadilla_J/0/1/0/all/0/1\">Jes&#xfa;s Bobadilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_F/0/1/0/all/0/1\">Fernando Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_A/0/1/0/all/0/1\">Abraham Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1\">&#xc1;ngel Gonz&#xe1;lez-Prieto</a>",
          "description": "Deep learning provides accurate collaborative filtering models to improve\nrecommender system results. Deep matrix factorization and their related\ncollaborative neural networks are the state-of-art in the field; nevertheless,\nboth models lack the necessary stochasticity to create the robust, continuous,\nand structured latent spaces that variational autoencoders exhibit. On the\nother hand, data augmentation through variational autoencoder does not provide\naccurate results in the collaborative filtering field due to the high sparsity\nof recommender systems. Our proposed models apply the variational concept to\ninject stochasticity in the latent space of the deep architecture, introducing\nthe variational technique in the neural collaborative filtering field. This\nmethod does not depend on the particular model used to generate the latent\nrepresentation. In this way, this approach can be applied as a plugin to any\ncurrent and future specific models. The proposed models have been tested using\nfour representative open datasets, three different quality measures, and\nstate-of-art baselines. The results show the superiority of the proposed\napproach in scenarios where the variational enrichment exceeds the injected\nnoise effect. Additionally, a framework is provided to enable the\nreproducibility of the conducted experiments.",
          "link": "http://arxiv.org/abs/2107.12677",
          "publishedOn": "2021-07-28T02:02:32.794Z",
          "wordCount": 637,
          "title": "Deep Variational Models for Collaborative Filtering-based Recommender Systems. (arXiv:2107.12677v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2001.07620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isufi_E/0/1/0/all/0/1\">Elvin Isufi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1\">Fernando Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>",
          "description": "Driven by the outstanding performance of neural networks in the structured\nEuclidean domain, recent years have seen a surge of interest in developing\nneural networks for graphs and data supported on graphs. The graph is leveraged\nat each layer of the neural network as a parameterization to capture detail at\nthe node level with a reduced number of parameters and computational\ncomplexity. Following this rationale, this paper puts forth a general framework\nthat unifies state-of-the-art graph neural networks (GNNs) through the concept\nof EdgeNet. An EdgeNet is a GNN architecture that allows different nodes to use\ndifferent parameters to weigh the information of different neighbors. By\nextrapolating this strategy to more iterations between neighboring nodes, the\nEdgeNet learns edge- and neighbor-dependent weights to capture local detail.\nThis is a general linear and local operation that a node can perform and\nencompasses under one formulation all existing graph convolutional neural\nnetworks (GCNNs) as well as graph attention networks (GATs). In writing\ndifferent GNN architectures with a common language, EdgeNets highlight specific\narchitecture advantages and limitations, while providing guidelines to improve\ntheir capacity without compromising their local implementation. An interesting\nconclusion is the unification of GCNNs and GATs -- approaches that have been so\nfar perceived as separate. In particular, we show that GATs are GCNNs on a\ngraph that is learned from the features. This particularization opens the doors\nto develop alternative attention mechanisms for improving discriminatory power.",
          "link": "http://arxiv.org/abs/2001.07620",
          "publishedOn": "2021-07-28T02:02:32.769Z",
          "wordCount": 712,
          "title": "EdgeNets:Edge Varying Graph Neural Networks. (arXiv:2001.07620v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" When viewing\nmice on the shelf, the number of buttons or presence of a wire may not be\nvisible from certain angles or positions. Flat images of candidate mice may not\nprovide the discriminative information needed for \"wireless\". The world, and\nobjects in it, are not flat images but complex 3D shapes. If a human requests\nan object based on any of its basic properties, such as color, shape, or\ntexture, robots should perform the necessary exploration to accomplish the\ntask. In particular, while substantial effort and progress has been made on\nunderstanding explicitly visual attributes like color and category,\ncomparatively little progress has been made on understanding language about\nshapes and contours. In this work, we introduce a novel reasoning task that\ntargets both visual and non-visual language about 3D objects. Our new\nbenchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a\nmodel to choose which of two objects is being referenced by a natural language\ndescription. We introduce several CLIP-based models for distinguishing objects\nand demonstrate that while recent advances in jointly modeling vision and\nlanguage are useful for robotic language understanding, it is still the case\nthat these models are weaker at understanding the 3D nature of objects --\nproperties which play a key role in manipulation. In particular, we find that\nadding view estimation to language grounding models improves accuracy on both\nSNARE and when identifying objects referred to in language on a robot platform.",
          "link": "http://arxiv.org/abs/2107.12514",
          "publishedOn": "2021-07-28T02:02:32.743Z",
          "wordCount": 709,
          "title": "Language Grounding with 3D Objects. (arXiv:2107.12514v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stier_J/0/1/0/all/0/1\">Julian Stier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darji_H/0/1/0/all/0/1\">Harshil Darji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granitzer_M/0/1/0/all/0/1\">Michael Granitzer</a>",
          "description": "Sparsity in the structure of Neural Networks can lead to less energy\nconsumption, less memory usage, faster computation times on convenient\nhardware, and automated machine learning. If sparsity gives rise to certain\nkinds of structure, it can explain automatically obtained features during\nlearning.\n\nWe provide insights into experiments in which we show how sparsity can be\nachieved through prior initialization, pruning, and during learning, and answer\nquestions on the relationship between the structure of Neural Networks and\ntheir performance. This includes the first work of inducing priors from network\ntheory into Recurrent Neural Networks and an architectural performance\nprediction during a Neural Architecture Search. Within our experiments, we show\nhow magnitude class blinded pruning achieves 97.5% on MNIST with 80%\ncompression and re-training, which is 0.5 points more than without compression,\nthat magnitude class uniform pruning is significantly inferior to it and how a\ngenetic search enhanced with performance prediction achieves 82.4% on CIFAR10.\nFurther, performance prediction for Recurrent Networks learning the Reber\ngrammar shows an $R^2$ of up to 0.81 given only structural information.",
          "link": "http://arxiv.org/abs/2107.12917",
          "publishedOn": "2021-07-28T02:02:32.718Z",
          "wordCount": 617,
          "title": "Experiments on Properties of Hidden Structures of Sparse Neural Networks. (arXiv:2107.12917v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1905.05976",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Matsuda_T/0/1/0/all/0/1\">Takeru Matsuda</a>, <a href=\"http://arxiv.org/find/math/1/au:+Uehara_M/0/1/0/all/0/1\">Masatoshi Uehara</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hyvarinen_A/0/1/0/all/0/1\">Aapo Hyvarinen</a>",
          "description": "Many statistical models are given in the form of non-normalized densities\nwith an intractable normalization constant. Since maximum likelihood estimation\nis computationally intensive for these models, several estimation methods have\nbeen developed which do not require explicit computation of the normalization\nconstant, such as noise contrastive estimation (NCE) and score matching.\nHowever, model selection methods for general non-normalized models have not\nbeen proposed so far. In this study, we develop information criteria for\nnon-normalized models estimated by NCE or score matching. They are\napproximately unbiased estimators of discrepancy measures for non-normalized\nmodels. Simulation results and applications to real data demonstrate that the\nproposed criteria enable selection of the appropriate non-normalized model in a\ndata-driven manner.",
          "link": "http://arxiv.org/abs/1905.05976",
          "publishedOn": "2021-07-28T02:02:32.710Z",
          "wordCount": 592,
          "title": "Information criteria for non-normalized models. (arXiv:1905.05976v5 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhenfei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haitao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_G/0/1/0/all/0/1\">Guangchun Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Haiwang Zhong</a>",
          "description": "In electricity markets, locational marginal price (LMP) forecasting is\nparticularly important for market participants in making reasonable bidding\nstrategies, managing potential trading risks, and supporting efficient system\nplanning and operation. Unlike existing methods that only consider LMPs'\ntemporal features, this paper tailors a spectral graph convolutional network\n(GCN) to greatly improve the accuracy of short-term LMP forecasting. A\nthree-branch network structure is then designed to match the structure of LMPs'\ncompositions. Such kind of network can extract the spatial-temporal features of\nLMPs, and provide fast and high-quality predictions for all nodes\nsimultaneously. The attention mechanism is also implemented to assign varying\nimportance weights between different nodes and time slots. Case studies based\non the IEEE-118 test system and real-world data from the PJM validate that the\nproposed model outperforms existing forecasting models in accuracy, and\nmaintains a robust performance by avoiding extreme errors.",
          "link": "http://arxiv.org/abs/2107.12794",
          "publishedOn": "2021-07-28T02:02:32.702Z",
          "wordCount": 601,
          "title": "Short-Term Electricity Price Forecasting based on Graph Convolution Network and Attention Mechanism. (arXiv:2107.12794v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "We consider the problem of estimating frame-level full human body meshes\ngiven a video of a person with natural motion dynamics. While much progress in\nthis field has been in single image-based mesh estimation, there has been a\nrecent uptick in efforts to infer mesh dynamics from video given its role in\nalleviating issues such as depth ambiguity and occlusions. However, a key\nlimitation of existing work is the assumption that all the observed motion\ndynamics can be modeled using one dynamical/recurrent model. While this may\nwork well in cases with relatively simplistic dynamics, inference with\nin-the-wild videos presents many challenges. In particular, it is typically the\ncase that different body parts of a person undergo different dynamics in the\nvideo, e.g., legs may move in a way that may be dynamically different from\nhands (e.g., a person dancing). To address these issues, we present a new\nmethod for video mesh recovery that divides the human mesh into several local\nparts following the standard skeletal model. We then model the dynamics of each\nlocal part with separate recurrent models, with each model conditioned\nappropriately based on the known kinematic structure of the human body. This\nresults in a structure-informed local recurrent learning architecture that can\nbe trained in an end-to-end fashion with available annotations. We conduct a\nvariety of experiments on standard video mesh recovery benchmark datasets such\nas Human3.6M, MPI-INF-3DHP, and 3DPW, demonstrating the efficacy of our design\nof modeling local dynamics as well as establishing state-of-the-art results\nbased on standard evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.12847",
          "publishedOn": "2021-07-28T02:02:32.695Z",
          "wordCount": 708,
          "title": "Learning Local Recurrent Models for Human Mesh Recovery. (arXiv:2107.12847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Constantinou_A/0/1/0/all/0/1\">Anthony C. Constantinou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhigao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitson_N/0/1/0/all/0/1\">Neville K. Kitson</a>",
          "description": "Bayesian Networks (BNs) have become a powerful technology for reasoning under\nuncertainty, particularly in areas that require causal assumptions that enable\nus to simulate the effect of intervention. The graphical structure of these\nmodels can be determined by causal knowledge, learnt from data, or a\ncombination of both. While it seems plausible that the best approach in\nconstructing a causal graph involves combining knowledge with machine learning,\nthis approach remains underused in practice. We implement and evaluate 10\nknowledge approaches with application to different case studies and BN\nstructure learning algorithms available in the open-source Bayesys structure\nlearning system. The approaches enable us to specify pre-existing knowledge\nthat can be obtained from heterogeneous sources, to constrain or guide\nstructure learning. Each approach is assessed in terms of structure learning\neffectiveness and efficiency, including graphical accuracy, model fitting,\ncomplexity, and runtime; making this the first paper that provides a\ncomparative evaluation of a wide range of knowledge approaches for BN structure\nlearning. Because the value of knowledge depends on what data are available, we\nillustrate the results both with limited and big data. While the overall\nresults show that knowledge becomes less important with big data due to higher\nlearning accuracy rendering knowledge less important, some of the knowledge\napproaches are actually found to be more important with big data. Amongst the\nmain conclusions is the observation that reduced search space obtained from\nknowledge does not always imply reduced computational complexity, perhaps\nbecause the relationships implied by the data and knowledge are in tension.",
          "link": "http://arxiv.org/abs/2102.00473",
          "publishedOn": "2021-07-28T02:02:32.688Z",
          "wordCount": 719,
          "title": "Information fusion between knowledge and data in Bayesian network structure learning. (arXiv:2102.00473v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Squires_E/0/1/0/all/0/1\">Eric Squires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konda_R/0/1/0/all/0/1\">Rohit Konda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coogan_S/0/1/0/all/0/1\">Samuel Coogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egerstedt_M/0/1/0/all/0/1\">Magnus Egerstedt</a>",
          "description": "This paper demonstrates that in some cases the safety override arising from\nthe use of a barrier function can be needlessly restrictive. In particular, we\nexamine the case of fixed wing collision avoidance and show that when using a\nbarrier function, there are cases where two fixed wing aircraft can come closer\nto colliding than if there were no barrier function at all. In addition, we\nconstruct cases where the barrier function labels the system as unsafe even\nwhen the vehicles start arbitrarily far apart. In other words, the barrier\nfunction ensures safety but with unnecessary costs to performance. We therefore\nintroduce model free barrier functions which take a data driven approach to\ncreating a barrier function. We demonstrate the effectiveness of model free\nbarrier functions in a collision avoidance simulation of two fixed-wing\naircraft.",
          "link": "http://arxiv.org/abs/2107.12871",
          "publishedOn": "2021-07-28T02:02:32.679Z",
          "wordCount": 565,
          "title": "Model Free Barrier Functions via Implicit Evading Maneuvers. (arXiv:2107.12871v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wimmer_P/0/1/0/all/0/1\">Paul Wimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehnert_J/0/1/0/all/0/1\">Jens Mehnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Condurache</a>",
          "description": "State-of-the-art deep neural network (DNN) pruning techniques, applied\none-shot before training starts, evaluate sparse architectures with the help of\na single criterion -- called pruning score. Pruning weights based on a solitary\nscore works well for some architectures and pruning rates but may also fail for\nother ones. As a common baseline for pruning scores, we introduce the notion of\na generalized synaptic score (GSS). In this work we do not concentrate on a\nsingle pruning criterion, but provide a framework for combining arbitrary GSSs\nto create more powerful pruning strategies. These COmbined Pruning Scores\n(COPS) are obtained by solving a constrained optimization problem. Optimizing\nfor more than one score prevents the sparse network to overly specialize on an\nindividual task, thus COntrols Pruning before training Starts. The\ncombinatorial optimization problem given by COPS is relaxed on a linear program\n(LP). This LP is solved analytically and determines a solution for COPS.\nFurthermore, an algorithm to compute it for two scores numerically is proposed\nand evaluated. Solving COPS in such a way has lower complexity than the best\ngeneral LP solver. In our experiments we compared pruning with COPS against\nstate-of-the-art methods for different network architectures and image\nclassification tasks and obtained improved results.",
          "link": "http://arxiv.org/abs/2107.12673",
          "publishedOn": "2021-07-28T02:02:32.672Z",
          "wordCount": 647,
          "title": "COPS: Controlled Pruning Before Training Starts. (arXiv:2107.12673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12033",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai V. Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bui_Thanh_T/0/1/0/all/0/1\">Tan Bui-Thanh</a>",
          "description": "Deep Learning (DL), in particular deep neural networks (DNN), by design is\npurely data-driven and in general does not require physics. This is the\nstrength of DL but also one of its key limitations when applied to science and\nengineering problems in which underlying physical properties (such as\nstability, conservation, and positivity) and desired accuracy need to be\nachieved. DL methods in their original forms are not capable of respecting the\nunderlying mathematical models or achieving desired accuracy even in big-data\nregimes. On the other hand, many data-driven science and engineering problems,\nsuch as inverse problems, typically have limited experimental or observational\ndata, and DL would overfit the data in this case. Leveraging information\nencoded in the underlying mathematical models, we argue, not only compensates\nmissing information in low data regimes but also provides opportunities to\nequip DL methods with the underlying physics and hence obtaining higher\naccuracy. This short communication introduces several model-constrained DL\napproaches (including both feed-forward DNN and autoencoders) that are capable\nof learning not only information hidden in the training data but also in the\nunderlying mathematical models to solve inverse problems. We present and\nprovide intuitions for our formulations for general nonlinear problems. For\nlinear inverse problems and linear networks, the first order optimality\nconditions show that our model-constrained DL approaches can learn information\nencoded in the underlying mathematical models, and thus can produce consistent\nor equivalent inverse solutions, while naive purely data-based counterparts\ncannot.",
          "link": "http://arxiv.org/abs/2105.12033",
          "publishedOn": "2021-07-28T02:02:32.631Z",
          "wordCount": 694,
          "title": "Model-Constrained Deep Learning Approaches for Inverse Problems. (arXiv:2105.12033v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varma_K/0/1/0/all/0/1\">Kamala Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1\">Nathalie Baracaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1\">Ali Anwar</a>",
          "description": "Federated learning has arisen as a mechanism to allow multiple participants\nto collaboratively train a model without sharing their data. In these settings,\nparticipants (workers) may not trust each other fully; for instance, a set of\ncompetitors may collaboratively train a machine learning model to detect fraud.\nThe workers provide local gradients that a central server uses to update a\nglobal model. This global model can be corrupted when Byzantine workers send\nmalicious gradients, which necessitates robust methods for aggregating\ngradients that mitigate the adverse effects of Byzantine inputs. Existing\nrobust aggregation algorithms are often computationally expensive and only\neffective under strict assumptions. In this paper, we introduce LayerwisE\nGradient AggregatTiOn (LEGATO), an aggregation algorithm that is, by contrast,\nscalable and generalizable. Informed by a study of layer-specific responses of\ngradients to Byzantine attacks, LEGATO employs a dynamic gradient reweighing\nscheme that is novel in its treatment of gradients based on layer-specific\nrobustness. We show that LEGATO is more computationally efficient than multiple\nstate-of-the-art techniques and more generally robust across a variety of\nattack settings in practice. We also demonstrate LEGATO's benefits for gradient\ndescent convergence in the absence of an attack.",
          "link": "http://arxiv.org/abs/2107.12490",
          "publishedOn": "2021-07-28T02:02:32.510Z",
          "wordCount": 644,
          "title": "LEGATO: A LayerwisE Gradient AggregaTiOn Algorithm for Mitigating Byzantine Attacks in Federated Learning. (arXiv:2107.12490v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maurer_T/0/1/0/all/0/1\">Thomas Maurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Autonomous game design, generating games algorithmically, has been a longtime\ngoal within the technical games research field. However, existing autonomous\ngame design systems have relied in large part on human-authoring for game\ndesign knowledge, such as fitness functions in search-based methods. In this\npaper, we describe an experiment to attempt to learn a human-like fitness\nfunction for autonomous game design in an adversarial manner. While our\nexperimental work did not meet our expectations, we present an analysis of our\nsystem and results that we hope will be informative to future autonomous game\ndesign research.",
          "link": "http://arxiv.org/abs/2107.12501",
          "publishedOn": "2021-07-28T02:02:32.394Z",
          "wordCount": 558,
          "title": "Adversarial Random Forest Classifier for Automated Game Design. (arXiv:2107.12501v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rothermel_D/0/1/0/all/0/1\">Danielle Rothermel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Margaret Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "Self-supervised pre-training of large-scale transformer models on text\ncorpora followed by finetuning has achieved state-of-the-art on a number of\nnatural language processing tasks. Recently, Lu et al. (2021, arXiv:2103.05247)\nclaimed that frozen pretrained transformers (FPTs) match or outperform training\nfrom scratch as well as unfrozen (fine-tuned) pretrained transformers in a set\nof transfer tasks to other modalities. In our work, we find that this result\nis, in fact, an artifact of not tuning the learning rates. After carefully\nredesigning the empirical setup, we find that when tuning learning rates\nproperly, pretrained transformers do outperform or match training from scratch\nin all of our tasks, but only as long as the entire model is finetuned. Thus,\nwhile transfer from pretrained language models to other modalities does indeed\nprovide gains and hints at exciting possibilities for future work, properly\ntuning hyperparameters is important for arriving at robust findings.",
          "link": "http://arxiv.org/abs/2107.12460",
          "publishedOn": "2021-07-28T02:02:32.353Z",
          "wordCount": 609,
          "title": "Don't Sweep your Learning Rate under the Rug: A Closer Look at Cross-modal Transfer of Pretrained Transformers. (arXiv:2107.12460v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael Ng</a>",
          "description": "Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.",
          "link": "http://arxiv.org/abs/2107.07058",
          "publishedOn": "2021-07-27T02:03:39.955Z",
          "wordCount": null,
          "title": "A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08751",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1\">Marius Memmel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Deep learning for medical imaging suffers from temporal and privacy-related\nrestrictions on data availability. To still obtain viable models, continual\nlearning aims to train in sequential order, as and when data is available. The\nmain challenge that continual learning methods face is to prevent catastrophic\nforgetting, i.e., a decrease in performance on the data encountered earlier.\nThis issue makes continuous training of segmentation models for medical\napplications extremely difficult. Yet, often, data from at least two different\ndomains is available which we can exploit to train the model in a way that it\ndisregards domain-specific information. We propose an architecture that\nleverages the simultaneous availability of two or more datasets to learn a\ndisentanglement between the content and domain in an adversarial fashion. The\ndomain-invariant content representation then lays the base for continual\nsemantic segmentation. Our approach takes inspiration from domain adaptation\nand combines it with continual learning for hippocampal segmentation in brain\nMRI. We showcase that our method reduces catastrophic forgetting and\noutperforms state-of-the-art continual learning methods.",
          "link": "http://arxiv.org/abs/2107.08751",
          "publishedOn": "2021-07-27T02:03:39.753Z",
          "wordCount": null,
          "title": "Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11419",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Komiyama_J/0/1/0/all/0/1\">Junpei Komiyama</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fouche_E/0/1/0/all/0/1\">Edouard Fouch&#xe9;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Honda_J/0/1/0/all/0/1\">Junya Honda</a>",
          "description": "We consider nonstationary multi-armed bandit problems where the model\nparameters of the arms change over time. We introduce the adaptive resetting\nbandit (ADR-bandit), which is a class of bandit algorithms that leverages\nadaptive windowing techniques from the data stream community. We first provide\nnew guarantees on the quality of estimators resulting from adaptive windowing\ntechniques, which are of independent interest in the data mining community.\nFurthermore, we conduct a finite-time analysis of ADR-bandit in two typical\nenvironments: an abrupt environment where changes occur instantaneously and a\ngradual environment where changes occur progressively. We demonstrate that\nADR-bandit has nearly optimal performance when the abrupt or global changes\noccur in a coordinated manner that we call global changes. We demonstrate that\nforced exploration is unnecessary when we restrict the interest to the global\nchanges. Unlike the existing nonstationary bandit algorithms, ADR-bandit has\noptimal performance in stationary environments as well as nonstationary\nenvironments with global changes. Our experiments show that the proposed\nalgorithms outperform the existing approaches in synthetic and real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2107.11419",
          "publishedOn": "2021-07-27T02:03:39.735Z",
          "wordCount": null,
          "title": "Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits. (arXiv:2107.11419v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Triet H. M. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huaming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1\">M. Ali Babar</a>",
          "description": "Software Vulnerabilities (SVs) are increasing in complexity and scale, posing\ngreat security risks to many software systems. Given the limited resources in\npractice, SV assessment and prioritization help practitioners devise optimal SV\nmitigation plans based on various SV characteristics. The surge in SV data\nsources and data-driven techniques such as Machine Learning and Deep Learning\nhave taken SV assessment and prioritization to the next level. Our survey\nprovides a taxonomy of the past research efforts and highlights the best\npractices for data-driven SV assessment and prioritization. We also discuss the\ncurrent limitations and propose potential solutions to address such issues.",
          "link": "http://arxiv.org/abs/2107.08364",
          "publishedOn": "2021-07-27T02:03:39.406Z",
          "wordCount": null,
          "title": "A Survey on Data-driven Software Vulnerability Assessment and Prioritization. (arXiv:2107.08364v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1\">Janntatul Tajrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naira Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>",
          "description": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.",
          "link": "http://arxiv.org/abs/2107.03844",
          "publishedOn": "2021-07-27T02:03:39.382Z",
          "wordCount": null,
          "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We introduce ParaBLEU, a paraphrase representation learning model and\nevaluation metric for text generation. Unlike previous approaches, ParaBLEU\nlearns to understand paraphrasis using generative conditioning as a pretraining\nobjective. ParaBLEU correlates more strongly with human judgements than\nexisting metrics, obtaining new state-of-the-art results on the 2017 WMT\nMetrics Shared Task. We show that our model is robust to data scarcity,\nexceeding previous state-of-the-art performance using only $50\\%$ of the\navailable training data and surpassing BLEU, ROUGE and METEOR with only $40$\nlabelled examples. Finally, we demonstrate that ParaBLEU can be used to\nconditionally generate novel paraphrases from a single demonstration, which we\nuse to confirm our hypothesis that it learns abstract, generalized paraphrase\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.08251",
          "publishedOn": "2021-07-27T02:03:39.376Z",
          "wordCount": null,
          "title": "Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1\">Ian Colbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1\">Ken Kreutz-Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srinjoy Das</a>",
          "description": "A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.",
          "link": "http://arxiv.org/abs/2107.07647",
          "publishedOn": "2021-07-27T02:03:39.375Z",
          "wordCount": null,
          "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08209",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tasche_D/0/1/0/all/0/1\">Dirk Tasche</a>",
          "description": "For the binary prevalence quantification problem under prior probability\nshift, we determine the asymptotic variance of the maximum likelihood\nestimator. We find that it is a function of the Brier score for the regression\nof the class label against the features under the test data set distribution.\nThis observation suggests that optimising the accuracy of a base classifier on\nthe training data set helps to reduce the variance of the related quantifier on\nthe test data set. Therefore, we also point out training criteria for the base\nclassifier that imply optimisation of both of the Brier scores on the training\nand the test data sets.",
          "link": "http://arxiv.org/abs/2107.08209",
          "publishedOn": "2021-07-27T02:03:39.373Z",
          "wordCount": null,
          "title": "Minimising quantifier variance under prior probability shift. (arXiv:2107.08209v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1\">Amal Feriani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mezghani_A/0/1/0/all/0/1\">Amine Mezghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>",
          "description": "We consider an Intelligent Reflecting Surface (IRS)-aided multiple-input\nsingle-output (MISO) system for downlink transmission. We compare the\nperformance of Deep Reinforcement Learning (DRL) and conventional optimization\nmethods in finding optimal phase shifts of the IRS elements to maximize the\nuser signal-to-noise (SNR) ratio. Furthermore, we evaluate the robustness of\nthese methods to channel impairments and changes in the system. We demonstrate\nnumerically that DRL solutions show more robustness to noisy channels and user\nmobility.",
          "link": "http://arxiv.org/abs/2107.08293",
          "publishedOn": "2021-07-27T02:03:39.372Z",
          "wordCount": null,
          "title": "On the Robustness of Deep Reinforcement Learning in IRS-Aided Wireless Communications Systems. (arXiv:2107.08293v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mrabah_N/0/1/0/all/0/1\">Nairouz Mrabah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouguessa_M/0/1/0/all/0/1\">Mohamed Bouguessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touati_M/0/1/0/all/0/1\">Mohamed Fawzi Touati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ksantini_R/0/1/0/all/0/1\">Riadh Ksantini</a>",
          "description": "Most recent graph clustering methods have resorted to Graph Auto-Encoders\n(GAEs) to perform joint clustering and embedding learning. However, two\ncritical issues have been overlooked. First, the accumulative error, inflicted\nby learning with noisy clustering assignments, degrades the effectiveness and\nrobustness of the clustering model. This problem is called Feature Randomness.\nSecond, reconstructing the adjacency matrix sets the model to learn irrelevant\nsimilarities for the clustering task. This problem is called Feature Drift.\nInterestingly, the theoretical relation between the aforementioned problems has\nnot yet been investigated. We study these issues from two aspects: (1) there is\na trade-off between Feature Randomness and Feature Drift when clustering and\nreconstruction are performed at the same level, and (2) the problem of Feature\nDrift is more pronounced for GAE models, compared with vanilla auto-encoder\nmodels, due to the graph convolutional operation and the graph decoding design.\nMotivated by these findings, we reformulate the GAE-based clustering\nmethodology. Our solution is two-fold. First, we propose a sampling operator\n$\\Xi$ that triggers a protection mechanism against the noisy clustering\nassignments. Second, we propose an operator $\\Upsilon$ that triggers a\ncorrection mechanism against Feature Drift by gradually transforming the\nreconstructed graph into a clustering-oriented one. As principal advantages,\nour solution grants a considerable improvement in clustering effectiveness and\nrobustness and can be easily tailored to existing GAE models.",
          "link": "http://arxiv.org/abs/2107.08562",
          "publishedOn": "2021-07-27T02:03:39.370Z",
          "wordCount": null,
          "title": "Rethinking Graph Auto-Encoder Models for Attributed Graph Clustering. (arXiv:2107.08562v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiajun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Changnan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>",
          "description": "Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning\n(DRL) via combining deep learning (DL) with reinforcement learning (RL), which\nhas noticed that the distribution of the acquired data would change during the\ntraining process. DQN found this property might cause instability for training,\nso it proposed effective methods to handle the downside of the property.\nInstead of focusing on the unfavourable aspects, we find it critical for RL to\nease the gap between the estimated data distribution and the ground truth data\ndistribution while supervised learning (SL) fails to do so. From this new\nperspective, we extend the basic paradigm of RL called the Generalized Policy\nIteration (GPI) into a more generalized version, which is called the\nGeneralized Data Distribution Iteration (GDI). We see massive RL algorithms and\ntechniques can be unified into the GDI paradigm, which can be considered as one\nof the special cases of GDI. We provide theoretical proof of why GDI is better\nthan GPI and how it works. Several practical algorithms based on GDI have been\nproposed to verify the effectiveness and extensiveness of it. Empirical\nexperiments prove our state-of-the-art (SOTA) performance on Arcade Learning\nEnvironment (ALE), wherein our algorithm has achieved 9620.98% mean human\nnormalized score (HNS), 1146.39% median HNS and 22 human world record\nbreakthroughs (HWRB) using only 200M training frames. Our work aims to lead the\nRL research to step into the journey of conquering the human world records and\nseek real superhuman agents on both performance and efficiency.",
          "link": "http://arxiv.org/abs/2106.06232",
          "publishedOn": "2021-07-27T02:03:39.369Z",
          "wordCount": null,
          "title": "GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning. (arXiv:2106.06232v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03383",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Prosperi_M/0/1/0/all/0/1\">Mattia Prosperi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Marini_S/0/1/0/all/0/1\">Simone Marini</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Boucher_C/0/1/0/all/0/1\">Christina Boucher</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>",
          "description": "Whole genome sequencing (WGS) is quickly becoming the customary means for\nidentification of antimicrobial resistance (AMR) due to its ability to obtain\nhigh resolution information about the genes and mechanisms that are causing\nresistance and driving pathogen mobility. By contrast, traditional phenotypic\n(antibiogram) testing cannot easily elucidate such information. Yet development\nof AMR prediction tools from genotype-phenotype data can be biased, since\nsampling is non-randomized. Sample provenience, period of collection, and\nspecies representation can confound the association of genetic traits with AMR.\nThus, prediction models can perform poorly on new data with sampling\ndistribution shifts. In this work -- under an explicit set of causal\nassumptions -- we evaluate the effectiveness of propensity-based rebalancing\nand confounding adjustment on AMR prediction using genotype-phenotype AMR data\nfrom the Pathosystems Resource Integration Center (PATRIC). We select bacterial\ngenotypes (encoded as k-mer signatures, i.e. DNA fragments of length k),\ncountry, year, species, and AMR phenotypes for the tetracycline drug class,\npreparing test data with recent genomes coming from a single country. We test\nboosted logistic regression (BLR) and random forests (RF) with/without\nbias-handling. On 10,936 instances, we find evidence of species, location and\nyear imbalance with respect to the AMR phenotype. The crude versus\nbias-adjusted change in effect of genetic signatures on AMR varies but only\nmoderately (selecting the top 20,000 out of 40+ million k-mers). The area under\nthe receiver operating characteristic (AUROC) of the RF (0.95) is comparable to\nthat of BLR (0.94) on both out-of-bag samples from bootstrap and the external\ntest (n=1,085), where AUROCs do not decrease. We observe a 1%-5% gain in AUROC\nwith bias-handling compared to the sole use of genetic signatures. ...",
          "link": "http://arxiv.org/abs/2107.03383",
          "publishedOn": "2021-07-27T02:03:39.361Z",
          "wordCount": null,
          "title": "Assessing putative bias in prediction of anti-microbial resistance from real-world genotyping data under explicit causal assumptions. (arXiv:2107.03383v2 [q-bio.GN] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Igamberdiev_T/0/1/0/all/0/1\">Timour Igamberdiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>",
          "description": "Graph convolutional networks (GCNs) are a powerful architecture for\nrepresentation learning on documents that naturally occur as graphs, e.g.,\ncitation or social networks. However, sensitive personal information, such as\ndocuments with people's profiles or relationships as edges, are prone to\nprivacy leaks, as the trained model might reveal the original input. Although\ndifferential privacy (DP) offers a well-founded privacy-preserving framework,\nGCNs pose theoretical and practical challenges due to their training specifics.\nWe address these challenges by adapting differentially-private gradient-based\ntraining to GCNs and conduct experiments using two optimizers on five NLP\ndatasets in two languages. We propose a simple yet efficient method based on\nrandom graph splits that not only improves the baseline privacy bounds by a\nfactor of 2.7 while retaining competitive F1 scores, but also provides strong\nprivacy guarantees of epsilon = 1.0. We show that, under certain modeling\nchoices, privacy-preserving GCNs perform up to 90% of their non-private\nvariants, while formally guaranteeing strong privacy measures.",
          "link": "http://arxiv.org/abs/2102.09604",
          "publishedOn": "2021-07-27T02:03:39.357Z",
          "wordCount": null,
          "title": "Privacy-Preserving Graph Convolutional Networks for Text Classification. (arXiv:2102.09604v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03308",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Natarovskii_V/0/1/0/all/0/1\">Viacheslav Natarovskii</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rudolf_D/0/1/0/all/0/1\">Daniel Rudolf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sprungk_B/0/1/0/all/0/1\">Bj&#xf6;rn Sprungk</a>",
          "description": "For Bayesian learning, given likelihood function and Gaussian prior, the\nelliptical slice sampler, introduced by Murray, Adams and MacKay 2010, provides\na tool for the construction of a Markov chain for approximate sampling of the\nunderlying posterior distribution. Besides of its wide applicability and\nsimplicity its main feature is that no tuning is necessary. Under weak\nregularity assumptions on the posterior density we show that the corresponding\nMarkov chain is geometrically ergodic and therefore yield qualitative\nconvergence guarantees. We illustrate our result for Gaussian posteriors as\nthey appear in Gaussian process regression, as well as in a setting of a\nmulti-modal distribution. Remarkably, our numerical experiments indicate a\ndimension-independent performance of elliptical slice sampling even in\nsituations where our ergodicity result does not apply.",
          "link": "http://arxiv.org/abs/2105.03308",
          "publishedOn": "2021-07-27T02:03:39.356Z",
          "wordCount": null,
          "title": "Geometric convergence of elliptical slice sampling. (arXiv:2105.03308v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yinjun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weimer_J/0/1/0/all/0/1\">James Weimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_S/0/1/0/all/0/1\">Susan B. Davidson</a>",
          "description": "High-quality labels are expensive to obtain for many machine learning tasks,\nsuch as medical image classification tasks. Therefore, probabilistic (weak)\nlabels produced by weak supervision tools are used to seed a process in which\ninfluential samples with weak labels are identified and cleaned by several\nhuman annotators to improve the model performance. To lower the overall cost\nand computational overhead of this process, we propose a solution called CHEF\n(CHEap and Fast label cleaning), which consists of the following three\ncomponents. First, to reduce the cost of human annotators, we use Infl, which\nprioritizes the most influential training samples for cleaning and provides\ncleaned labels to save the cost of one human annotator. Second, to accelerate\nthe sample selector phase and the model constructor phase, we use Increm-Infl\nto incrementally produce influential samples, and DeltaGrad-L to incrementally\nupdate the model. Third, we redesign the typical label cleaning pipeline so\nthat human annotators iteratively clean smaller batch of samples rather than\none big batch of samples. This yields better over all model performance and\nenables possible early termination when the expected model performance has been\nachieved. Extensive experiments show that our approach gives good model\nprediction performance while achieving significant speed-ups.",
          "link": "http://arxiv.org/abs/2107.08588",
          "publishedOn": "2021-07-27T02:03:39.355Z",
          "wordCount": null,
          "title": "CHEF: A Cheap and Fast Pipeline for Iteratively Cleaning Label Uncertainties (Technical Report). (arXiv:2107.08588v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Renzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakala_P/0/1/0/all/0/1\">Prem Sakala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xu Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yeye He</a>",
          "description": "Entity matching (EM) refers to the problem of identifying tuple pairs in one\nor more relations that refer to the same real world entities. Supervised\nmachine learning (ML) approaches, and deep learning based approaches in\nparticular, typically achieve state-of-the-art matching results. However, these\napproaches require many labeled examples, in the form of matching and\nnon-matching pairs, which are expensive and time-consuming to label. In this\npaper, we introduce Panda, a weakly supervised system specifically designed for\nEM. Panda uses the same labeling function abstraction as Snorkel, where\nlabeling functions (LF) are user-provided programs that can generate large\namounts of (somewhat noisy) labels quickly and cheaply, which can then be\ncombined via a labeling model to generate accurate final predictions. To\nsupport users developing LFs for EM, Panda provides an integrated development\nenvironment (IDE) that lives in a modern browser architecture. Panda's IDE\nfacilitates the development, debugging, and life-cycle management of LFs in the\ncontext of EM tasks, similar to how IDEs such as Visual Studio or Eclipse excel\nin general-purpose programming. Panda's IDE includes many novel features\npurpose-built for EM, such as smart data sampling, a builtin library of EM\nutility functions, automatically generated LFs, visual debugging of LFs, and\nfinally, an EM-specific labeling model. We show in this demo that Panda IDE can\ngreatly accelerate the development of high-quality EM solutions using weak\nsupervision.",
          "link": "http://arxiv.org/abs/2106.10821",
          "publishedOn": "2021-07-27T02:03:39.354Z",
          "wordCount": null,
          "title": "Demonstration of Panda: A Weakly Supervised Entity Matching System. (arXiv:2106.10821v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01863",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Mesuga_R/0/1/0/all/0/1\">Reymond Mesuga</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Bayanay_B/0/1/0/all/0/1\">Brian James Bayanay</a>",
          "description": "LIGO is considered the most sensitive and complicated gravitational\nexperiment ever built. Its main objective is to detect the gravitational wave\nfrom the strongest events in the universe by observing if the length of its\n4-kilometer arms change by a distance 10,000 times smaller than the diameter of\na proton. Due to its sensitivity, LIGO is prone to the disturbance of external\nnoises which affects the data being collected to detect the gravitational wave.\nThese noises are commonly called by the LIGO community as glitches. The\nobjective of this study is to evaluate the effeciency of various deep trasnfer\nlearning models namely VGG19, ResNet50V2, VGG16 and ResNet101 to detect glitch\nwaveform in gravitational wave data. The accuracy achieved by the said models\nare 98.98%, 98.35%, 97.56% and 94.73% respectively. Even though the models\nachieved fairly high accuracy, it is observed that all of the model suffered\nfrom the lack of data for certain classes which is the main concern found in\nthe experiment.",
          "link": "http://arxiv.org/abs/2107.01863",
          "publishedOn": "2021-07-27T02:03:39.352Z",
          "wordCount": null,
          "title": "On the Efficiency of Various Deep Transfer Learning Models in Glitch Waveform Detection in Gravitational-Wave Data. (arXiv:2107.01863v3 [gr-qc] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07283",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tasche_D/0/1/0/all/0/1\">Dirk Tasche</a>",
          "description": "When probabilistic classifiers are trained and calibrated, the so-called\ngrouping loss component of the calibration loss can easily be overlooked.\nGrouping loss refers to the gap between observable information and information\nactually exploited in the calibration exercise. We investigate the relation\nbetween grouping loss and the concept of sufficiency, identifying\ncomonotonicity as a useful criterion for sufficiency. We revisit the probing\nreduction approach of Langford & Zadrozny (2005) and find that it produces an\nestimator of probabilistic classifiers that reduces grouping loss. Finally, we\ndiscuss Brier curves as tools to support training and 'sufficient' calibration\nof probabilistic classifiers.",
          "link": "http://arxiv.org/abs/2105.07283",
          "publishedOn": "2021-07-27T02:03:39.349Z",
          "wordCount": null,
          "title": "Calibrating sufficiently. (arXiv:2105.07283v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14659",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Andrews_M/0/1/0/all/0/1\">Michael Andrews</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Burkle_B/0/1/0/all/0/1\">Bjorn Burkle</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-fan Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+DiCroce_D/0/1/0/all/0/1\">Davide DiCroce</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gleyzer_S/0/1/0/all/0/1\">Sergei Gleyzer</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Heintz_U/0/1/0/all/0/1\">Ulrich Heintz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Narain_M/0/1/0/all/0/1\">Meenakshi Narain</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Paulini_M/0/1/0/all/0/1\">Manfred Paulini</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pervan_N/0/1/0/all/0/1\">Nikolas Pervan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shafi_Y/0/1/0/all/0/1\">Yusef Shafi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Usai_E/0/1/0/all/0/1\">Emanuele Usai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_K/0/1/0/all/0/1\">Kun Yang</a>",
          "description": "We describe a novel application of the end-to-end deep learning technique to\nthe task of discriminating top quark-initiated jets from those originating from\nthe hadronization of a light quark or a gluon. The end-to-end deep learning\ntechnique combines deep learning algorithms and low-level detector\nrepresentation of the high-energy collision event. In this study, we use\nlow-level detector information from the simulated CMS Open Data samples to\nconstruct the top jet classifiers. To optimize classifier performance we\nprogressively add low-level information from the CMS tracking detector,\nincluding pixel detector reconstructed hits and impact parameters, and\ndemonstrate the value of additional tracking information even when no new\nspatial structures are added. Relying only on calorimeter energy deposits and\nreconstructed pixel detector hits, the end-to-end classifier achieves an AUC\nscore of 0.975$\\pm$0.002 for the task of classifying boosted top quark jets.\nAfter adding derived track quantities, the classifier AUC score increases to\n0.9824$\\pm$0.0013, serving as the first performance benchmark for these CMS\nOpen Data samples. We additionally provide a timing performance comparison of\ndifferent processor unit architectures for training the network.",
          "link": "http://arxiv.org/abs/2104.14659",
          "publishedOn": "2021-07-27T02:03:39.346Z",
          "wordCount": null,
          "title": "End-to-End Jet Classification of Boosted Top Quarks with the CMS Open Data. (arXiv:2104.14659v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04362",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Morehead_A/0/1/0/all/0/1\">Alex Morehead</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sedova_A/0/1/0/all/0/1\">Ada Sedova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_J/0/1/0/all/0/1\">Jianlin Cheng</a>",
          "description": "How and where proteins interface with one another can ultimately impact the\nproteins' functions along with a range of other biological processes. As such,\nprecise computational methods for protein interface prediction (PIP) come\nhighly sought after as they could yield significant advances in drug discovery\nand design as well as protein function analysis. However, the traditional\nbenchmark dataset for this task, Docking Benchmark 5 (DB5), contains only a\nmodest 230 complexes for training, validating, and testing different machine\nlearning algorithms. In this work, we expand on a dataset recently introduced\nfor this task, the Database of Interacting Protein Structures (DIPS), to\npresent DIPS-Plus, an enhanced, feature-rich dataset of 42,112 complexes for\ngeometric deep learning of protein interfaces. The previous version of DIPS\ncontains only the Cartesian coordinates and types of the atoms comprising a\ngiven protein complex, whereas DIPS-Plus now includes a plethora of new\nresidue-level features including protrusion indices, half-sphere amino acid\ncompositions, and new profile hidden Markov model (HMM)-based sequence features\nfor each amino acid, giving researchers a large, well-curated feature bank for\ntraining protein interface prediction methods. We demonstrate through rigorous\nbenchmarks that training an existing state-of-the-art (SOTA) model for PIP on\nDIPS-Plus yields SOTA results, surpassing the performance of all other models\ntrained on residue-level and atom-level encodings of protein complexes to date.",
          "link": "http://arxiv.org/abs/2106.04362",
          "publishedOn": "2021-07-27T02:03:39.343Z",
          "wordCount": null,
          "title": "DIPS-Plus: The Enhanced Database of Interacting Protein Structures for Interface Prediction. (arXiv:2106.04362v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11589",
          "author": "<a href=\"http://arxiv.org/find/hep-ex/1/au:+Abbasi_R/0/1/0/all/0/1\">R. Abbasi</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Ackermann_M/0/1/0/all/0/1\">M. Ackermann</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Adams_J/0/1/0/all/0/1\">J. Adams</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Aguilar_J/0/1/0/all/0/1\">J. A. Aguilar</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Ahlers_M/0/1/0/all/0/1\">M. Ahlers</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Ahrens_M/0/1/0/all/0/1\">M. Ahrens</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Alispach_C/0/1/0/all/0/1\">C. Alispach</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Alves_A/0/1/0/all/0/1\">A. A. Alves Jr.</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Amin_N/0/1/0/all/0/1\">N. M. Amin</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+An_R/0/1/0/all/0/1\">R. An</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Andeen_K/0/1/0/all/0/1\">K. Andeen</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Anderson_T/0/1/0/all/0/1\">T. Anderson</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Ansseau_I/0/1/0/all/0/1\">I. Ansseau</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Anton_G/0/1/0/all/0/1\">G. Anton</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Arguelles_C/0/1/0/all/0/1\">C. Arg&#xfc;elles</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Axani_S/0/1/0/all/0/1\">S. Axani</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bai_X/0/1/0/all/0/1\">X. Bai</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+V%2E_A/0/1/0/all/0/1\">A. Balagopal V.</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Barbano_A/0/1/0/all/0/1\">A. Barbano</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Barwick_S/0/1/0/all/0/1\">S. W. Barwick</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bastian_B/0/1/0/all/0/1\">B. Bastian</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Basu_V/0/1/0/all/0/1\">V. Basu</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Baum_V/0/1/0/all/0/1\">V. Baum</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Baur_S/0/1/0/all/0/1\">S. Baur</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bay_R/0/1/0/all/0/1\">R. Bay</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Beatty_J/0/1/0/all/0/1\">J. J. Beatty</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Becker_K/0/1/0/all/0/1\">K.-H. Becker</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Tjus_J/0/1/0/all/0/1\">J. Becker Tjus</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bellenghi_C/0/1/0/all/0/1\">C. Bellenghi</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+BenZvi_S/0/1/0/all/0/1\">S. BenZvi</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Berley_D/0/1/0/all/0/1\">D. Berley</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bernardini_E/0/1/0/all/0/1\">E. Bernardini</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Besson_D/0/1/0/all/0/1\">D. Z. Besson</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Binder_G/0/1/0/all/0/1\">G. Binder</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bindig_D/0/1/0/all/0/1\">D. Bindig</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Blaufuss_E/0/1/0/all/0/1\">E. Blaufuss</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Blot_S/0/1/0/all/0/1\">S. Blot</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Boser_S/0/1/0/all/0/1\">S. B&#xf6;ser</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Botner_O/0/1/0/all/0/1\">O. Botner</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bottcher_J/0/1/0/all/0/1\">J. B&#xf6;ttcher</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bourbeau_E/0/1/0/all/0/1\">E. Bourbeau</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bourbeau_J/0/1/0/all/0/1\">J. Bourbeau</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bradascio_F/0/1/0/all/0/1\">F. Bradascio</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Braun_J/0/1/0/all/0/1\">J. Braun</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bron_S/0/1/0/all/0/1\">S. Bron</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Brostean_Kaiser_J/0/1/0/all/0/1\">J. Brostean-Kaiser</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Burgman_A/0/1/0/all/0/1\">A. Burgman</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Busse_R/0/1/0/all/0/1\">R. S. Busse</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Campana_M/0/1/0/all/0/1\">M. A. Campana</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Chen_C/0/1/0/all/0/1\">C. Chen</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Chirkin_D/0/1/0/all/0/1\">D. Chirkin</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Choi_S/0/1/0/all/0/1\">S. Choi</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Clark_B/0/1/0/all/0/1\">B. A. Clark</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Clark_K/0/1/0/all/0/1\">K. Clark</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Classen_L/0/1/0/all/0/1\">L. Classen</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Coleman_A/0/1/0/all/0/1\">A. Coleman</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Collin_G/0/1/0/all/0/1\">G. H. Collin</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Conrad_J/0/1/0/all/0/1\">J. M. Conrad</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Coppin_P/0/1/0/all/0/1\">P. Coppin</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Correa_P/0/1/0/all/0/1\">P. Correa</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Cowen_D/0/1/0/all/0/1\">D. F. Cowen</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Cross_R/0/1/0/all/0/1\">R. Cross</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Dave_P/0/1/0/all/0/1\">P. Dave</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Clercq_C/0/1/0/all/0/1\">C. De Clercq</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+DeLaunay_J/0/1/0/all/0/1\">J. J. DeLaunay</a>, et al. (303 additional authors not shown)",
          "description": "Continued improvements on existing reconstruction methods are vital to the\nsuccess of high-energy physics experiments, such as the IceCube Neutrino\nObservatory. In IceCube, further challenges arise as the detector is situated\nat the geographic South Pole where computational resources are limited.\nHowever, to perform real-time analyses and to issue alerts to telescopes around\nthe world, powerful and fast reconstruction methods are desired. Deep neural\nnetworks can be extremely powerful, and their usage is computationally\ninexpensive once the networks are trained. These characteristics make a deep\nlearning-based approach an excellent candidate for the application in IceCube.\nA reconstruction method based on convolutional architectures and hexagonally\nshaped kernels is presented. The presented method is robust towards systematic\nuncertainties in the simulation and has been tested on experimental data. In\ncomparison to standard reconstruction methods in IceCube, it can improve upon\nthe reconstruction accuracy, while reducing the time necessary to run the\nreconstruction by two to three orders of magnitude.",
          "link": "http://arxiv.org/abs/2101.11589",
          "publishedOn": "2021-07-27T02:03:37.189Z",
          "wordCount": 1474,
          "title": "A Convolutional Neural Network based Cascade Reconstruction for the IceCube Neutrino Observatory. (arXiv:2101.11589v2 [hep-ex] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13471",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Barmparis_G/0/1/0/all/0/1\">G. D. Barmparis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tsironis_G/0/1/0/all/0/1\">G. P. Tsironis</a>",
          "description": "For an ensemble of nonlinear systems that model, for instance, molecules or\nphotonic systems, we propose a method that finds efficiently the configuration\nthat has prescribed transfer properties. Specifically, we use physics-informed\nmachine-learning (PIML) techniques to find the parameters for the efficient\ntransfer of an electron (or photon) to a targeted state in a non-linear dimer.\nWe create a machine learning model containing two variables, $\\chi_D$, and\n$\\chi_A$, representing the non-linear terms in the donor and acceptor target\nsystem states. We then introduce a data-free physics-informed loss function as\n$1.0 - P_j$, where $P_j$ is the probability, the electron being in the targeted\nstate, $j$. By minimizing the loss function, we maximize the occupation\nprobability to the targeted state. The method recovers known results in the\nTargeted Energy Transfer (TET) model, and it is then applied to a more complex\nsystem with an additional intermediate state. In this trimer configuration, the\nPIML approach discovers desired resonant paths from the donor to acceptor\nunits. The proposed PIML method is general and may be used in the chemical\ndesign of molecular complexes or engineering design of quantum or photonic\nsystems.",
          "link": "http://arxiv.org/abs/2104.13471",
          "publishedOn": "2021-07-27T02:03:37.141Z",
          "wordCount": 636,
          "title": "Discovering nonlinear resonances through physics-informed machine learning. (arXiv:2104.13471v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panerati_J/0/1/0/all/0/1\">Jacopo Panerati</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hehui Zheng</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">SiQi Zhou</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">James Xu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Prorok_A/0/1/0/all/0/1\">Amanda Prorok</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Schoellig_A/0/1/0/all/0/1\">Angela P. Schoellig</a> (1 and 2) ((1) University of Toronto Institute for Aerospace Studies, (2) Vector Institute for Artificial Intelligence, (3) University of Cambridge)",
          "description": "Robotic simulators are crucial for academic research and education as well as\nthe development of safety-critical applications. Reinforcement learning\nenvironments -- simple simulations coupled with a problem specification in the\nform of a reward function -- are also important to standardize the development\n(and benchmarking) of learning algorithms. Yet, full-scale simulators typically\nlack portability and parallelizability. Vice versa, many reinforcement learning\nenvironments trade-off realism for high sample throughputs in toy-like\nproblems. While public data sets have greatly benefited deep learning and\ncomputer vision, we still lack the software tools to simultaneously develop --\nand fairly compare -- control theory and reinforcement learning approaches. In\nthis paper, we propose an open-source OpenAI Gym-like environment for multiple\nquadcopters based on the Bullet physics engine. Its multi-agent and vision\nbased reinforcement learning interfaces, as well as the support of realistic\ncollisions and aerodynamic effects, make it, to the best of our knowledge, a\nfirst of its kind. We demonstrate its use through several examples, either for\ncontrol (trajectory tracking with PID control, multi-robot flight with\ndownwash, etc.) or reinforcement learning (single and multi-agent stabilization\ntasks), hoping to inspire future research that combines control theory and\nmachine learning.",
          "link": "http://arxiv.org/abs/2103.02142",
          "publishedOn": "2021-07-27T02:03:37.081Z",
          "wordCount": 734,
          "title": "Learning to Fly -- a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control. (arXiv:2103.02142v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1\">Hanbaek Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kureh_Y/0/1/0/all/0/1\">Yacoub H. Kureh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vendrow_J/0/1/0/all/0/1\">Joshua Vendrow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porter_M/0/1/0/all/0/1\">Mason A. Porter</a>",
          "description": "It is common to use networks to encode the architecture of interactions\nbetween entities in complex systems in the physical, biological, social, and\ninformation sciences. Moreover, to study the large-scale behavior of complex\nsystems, it is important to study mesoscale structures in networks as building\nblocks that influence such behavior. In this paper, we present a new approach\nfor describing low-rank mesoscale structure in networks, and we illustrate our\napproach using several synthetic network models and empirical friendship,\ncollaboration, and protein--protein interaction (PPI) networks. We find that\nthese networks possess a relatively small number of `latent motifs' that\ntogether can successfully approximate most subnetworks at a fixed mesoscale. We\nuse an algorithm that we call \"network dictionary learning\" (NDL), which\ncombines a network sampling method and nonnegative matrix factorization, to\nlearn the latent motifs of a given network. The ability to encode a network\nusing a set of latent motifs has a wide range of applications to\nnetwork-analysis tasks, such as comparison, denoising, and edge inference.\nAdditionally, using our new network denoising and reconstruction (NDR)\nalgorithm, we demonstrate how to denoise a corrupted network by using only the\nlatent motifs that one learns directly from the corrupted networks.",
          "link": "http://arxiv.org/abs/2102.06984",
          "publishedOn": "2021-07-27T02:03:37.074Z",
          "wordCount": 689,
          "title": "Learning low-rank latent mesoscale structures in networks. (arXiv:2102.06984v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cousins_C/0/1/0/all/0/1\">Cyrus Cousins</a>",
          "description": "We address an inherent difficulty in welfare-theoretic fair machine learning\nby proposing an equivalently axiomatically-justified alternative and studying\nthe resulting computational and statistical learning questions. Welfare metrics\nquantify overall wellbeing across a population of one or more groups, and\nwelfare-based objectives and constraints have recently been proposed to\nincentivize fair machine learning methods to produce satisfactory solutions\nthat consider the diverse needs of multiple groups. Unfortunately, many\nmachine-learning problems are more naturally cast as loss minimization tasks,\nrather than utility maximization, which complicates direct application of\nwelfare-centric methods to fair machine learning. In this work, we define a\ncomplementary measure, termed malfare, measuring overall societal harm (rather\nthan wellbeing), with axiomatic justification via the standard axioms of\ncardinal welfare. We then cast fair machine learning as malfare minimization\nover the risk values (expected losses) of each group. Surprisingly, the axioms\nof cardinal welfare (malfare) dictate that this is not equivalent to simply\ndefining utility as negative loss. Building upon these concepts, we define\nfair-PAC (FPAC) learning, where an FPAC learner is an algorithm that learns an\n$\\varepsilon$-$\\delta$ malfare-optimal model with bounded sample complexity,\nfor any data distribution, and for any (axiomatically justified) malfare\nconcept. Finally, we show broad conditions under which, with appropriate\nmodifications, standard PAC-learners may be converted to FPAC learners. This\nplaces FPAC learning on firm theoretical ground, as it yields statistical and\ncomputational efficiency guarantees for many well-studied machine-learning\nmodels, and is also practically relevant, as it democratizes fair ML by\nproviding concrete training algorithms and rigorous generalization guarantees\nfor these models",
          "link": "http://arxiv.org/abs/2104.14504",
          "publishedOn": "2021-07-27T02:03:37.068Z",
          "wordCount": 705,
          "title": "An Axiomatic Theory of Provably-Fair Welfare-Centric Machine Learning. (arXiv:2104.14504v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yong-Min Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1\">Cong Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Won-Yong Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>",
          "description": "We study the problem of embedding edgeless nodes such as users who newly\nenter the underlying network, while using graph neural networks (GNNs) widely\nstudied for effective representation learning of graphs thanks to its highly\nexpressive capability via message passing. Our study is motivated by the fact\nthat existing GNNs cannot be adopted for our problem since message passing to\nsuch edgeless nodes having no connections is impossible. To tackle this\nchallenge, we propose Edgeless-GNN, a new framework that enables GNNs to\ngenerate node embeddings even for edgeless nodes through unsupervised inductive\nlearning. Specifically, we start by constructing a $k$-nearest neighbor graph\n($k$NNG) based on the similarity of node attributes to replace the GNN's\ncomputation graph defined by the neighborhood-based aggregation of each node.\nAs our main contributions, the known network structure is used to train model\nparameters, while a new loss function is established using energy-based\nlearning in such a way that our model learns the network structure. For the\nedgeless nodes, we inductively infer embeddings for the edgeless nodes by using\nedges via $k$NNG construction as a computation graph. By evaluating the\nperformance of various downstream machine learning (ML) tasks, we empirically\ndemonstrate that Edgeless-GNN consistently outperforms state-of-the-art methods\nof inductive network embedding. Moreover, our findings corroborate the\neffectiveness of Edgeless-GNN in judiciously combining the replaced computation\ngraph with our newly designed loss. Our framework is GNN-model-agnostic; thus,\nGNN models can be appropriately chosen according to ones' needs and ML tasks.",
          "link": "http://arxiv.org/abs/2104.05225",
          "publishedOn": "2021-07-27T02:03:37.061Z",
          "wordCount": 724,
          "title": "Edgeless-GNN: Unsupervised Inductive Edgeless Network Embedding. (arXiv:2104.05225v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1\">Jeff Z. HaoChen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "Recent works in self-supervised learning have advanced the state-of-the-art\nby relying on the contrastive learning paradigm, which learns representations\nby pushing positive pairs, or similar examples from the same class, closer\ntogether while keeping negative pairs far apart. Despite the empirical\nsuccesses, theoretical foundations are limited -- prior analyses assume\nconditional independence of the positive pairs given the same class label, but\nrecent empirical applications use heavily correlated positive pairs (i.e., data\naugmentations of the same image). Our work analyzes contrastive learning\nwithout assuming conditional independence of positive pairs using a novel\nconcept of the augmentation graph on data. Edges in this graph connect\naugmentations of the same data, and ground-truth classes naturally form\nconnected sub-graphs. We propose a loss that performs spectral decomposition on\nthe population augmentation graph and can be succinctly written as a\ncontrastive learning objective on neural net representations. Minimizing this\nobjective leads to features with provable accuracy guarantees under linear\nprobe evaluation. By standard generalization bounds, these accuracy guarantees\nalso hold when minimizing the training contrastive loss. Empirically, the\nfeatures learned by our objective can match or outperform several strong\nbaselines on benchmark vision datasets. In all, this work provides the first\nprovable analysis for contrastive learning where guarantees for linear probe\nevaluation can apply to realistic empirical settings.",
          "link": "http://arxiv.org/abs/2106.04156",
          "publishedOn": "2021-07-27T02:03:37.054Z",
          "wordCount": 692,
          "title": "Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1\">David Simchi-Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yining Wang</a>",
          "description": "The prevalence of e-commerce has made detailed customers' personal\ninformation readily accessible to retailers, and this information has been\nwidely used in pricing decisions. When involving personalized information, how\nto protect the privacy of such information becomes a critical issue in\npractice. In this paper, we consider a dynamic pricing problem over $T$ time\nperiods with an \\emph{unknown} demand function of posted price and personalized\ninformation. At each time $t$, the retailer observes an arriving customer's\npersonal information and offers a price. The customer then makes the purchase\ndecision, which will be utilized by the retailer to learn the underlying demand\nfunction. There is potentially a serious privacy concern during this process: a\nthird party agent might infer the personalized information and purchase\ndecisions from price changes from the pricing system. Using the fundamental\nframework of differential privacy from computer science, we develop a\nprivacy-preserving dynamic pricing policy, which tries to maximize the retailer\nrevenue while avoiding information leakage of individual customer's information\nand purchasing decisions. To this end, we first introduce a notion of\n\\emph{anticipating} $(\\varepsilon, \\delta)$-differential privacy that is\ntailored to dynamic pricing problem. Our policy achieves both the privacy\nguarantee and the performance guarantee in terms of regret. Roughly speaking,\nfor $d$-dimensional personalized information, our algorithm achieves the\nexpected regret at the order of $\\tilde{O}(\\varepsilon^{-1} \\sqrt{d^3 T})$,\nwhen the customers' information is adversarially chosen. For stochastic\npersonalized information, the regret bound can be further improved to\n$\\tilde{O}(\\sqrt{d^2T} + \\varepsilon^{-2} d^2)$",
          "link": "http://arxiv.org/abs/2009.12920",
          "publishedOn": "2021-07-27T02:03:37.028Z",
          "wordCount": 722,
          "title": "Privacy-Preserving Dynamic Personalized Pricing with Demand Learning. (arXiv:2009.12920v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00757",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Vecchio_A/0/1/0/all/0/1\">Alice Del Vecchio</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deac_A/0/1/0/all/0/1\">Andreea Deac</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>",
          "description": "Antibodies are proteins in the immune system which bind to antigens to detect\nand neutralise them. The binding sites in an antibody-antigen interaction are\nknown as the paratope and epitope, respectively, and the prediction of these\nregions is key to vaccine and synthetic antibody development. Contrary to prior\nart, we argue that paratope and epitope predictors require asymmetric\ntreatment, and propose distinct neural message passing architectures that are\ngeared towards the specific aspects of paratope and epitope prediction,\nrespectively. We obtain significant improvements on both tasks, setting the new\nstate-of-the-art and recovering favourable qualitative predictions on antigens\nof relevance to COVID-19.",
          "link": "http://arxiv.org/abs/2106.00757",
          "publishedOn": "2021-07-27T02:03:37.020Z",
          "wordCount": 613,
          "title": "Neural message passing for joint paratope-epitope prediction. (arXiv:2106.00757v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panknin_D/0/1/0/all/0/1\">Danny Panknin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Klaus Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1\">Shinichi Nakajima</a>",
          "description": "We propose a novel active learning strategy for regression, which is\nmodel-agnostic, robust against model mismatch, and interpretable. Assuming that\na small number of initial samples are available, we derive the optimal training\ndensity that minimizes the generalization error of local polynomial smoothing\n(LPS) with its kernel bandwidth tuned locally: We adopt the mean integrated\nsquared error (MISE) as a generalization criterion, and use the asymptotic\nbehavior of the MISE as well as the locally optimal bandwidths (LOB) - the\nbandwidth function that minimizes MISE in the asymptotic limit. The asymptotic\nexpression of our objective then reveals the dependence of the MISE on the\ntraining density, enabling analytic minimization. As a result,we obtain the\noptimal training density in a closed-form. The almost model-free nature of our\napproach thus helps to encode the essential properties of the target problem,\nproviding a robust and model-agnostic active learning strategy. Furthermore,\nthe obtained training density factorizes the influence of local function\ncomplexity, noise level and test density in a transparent and interpretable\nway. We validate our theory in numerical simulations, and show that the\nproposed active learning method outperforms the existing state-of-the-art\nmodel-agnostic approaches.",
          "link": "http://arxiv.org/abs/2105.11990",
          "publishedOn": "2021-07-27T02:03:36.950Z",
          "wordCount": 650,
          "title": "Optimal Sampling Density for Nonparametric Regression. (arXiv:2105.11990v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meltzer_P/0/1/0/all/0/1\">Peter Meltzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayani_H/0/1/0/all/0/1\">Hooman Shayani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1\">Amir Khasahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_P/0/1/0/all/0/1\">Pradeep Kumar Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1\">Aditya Sanghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1\">Joseph Lambourne</a>",
          "description": "Boundary Representations (B-Reps) are the industry standard in 3D Computer\nAided Design/Manufacturing (CAD/CAM) and industrial design due to their\nfidelity in representing stylistic details. However, they have been ignored in\nthe 3D style research. Existing 3D style metrics typically operate on meshes or\npointclouds, and fail to account for end-user subjectivity by adopting fixed\ndefinitions of style, either through crowd-sourcing for style labels or\nhand-crafted features. We propose UVStyle-Net, a style similarity measure for\nB-Reps that leverages the style signals in the second order statistics of the\nactivations in a pre-trained (unsupervised) 3D encoder, and learns their\nrelative importance to a subjective end-user through few-shot learning. Our\napproach differs from all existing data-driven 3D style methods since it may be\nused in completely unsupervised settings, which is desirable given the lack of\npublicly available labelled B-Rep datasets. More importantly, the few-shot\nlearning accounts for the inherent subjectivity associated with style. We show\nquantitatively that our proposed method with B-Reps is able to capture stronger\nstyle signals than alternative methods on meshes and pointclouds despite its\nsignificantly greater computational efficiency. We also show it is able to\ngenerate meaningful style gradients with respect to the input shape, and that\nfew-shot learning with as few as two positive examples selected by an end-user\nis sufficient to significantly improve the style measure. Finally, we\ndemonstrate its efficacy on a large unlabeled public dataset of CAD models.\nSource code and data will be released in the future.",
          "link": "http://arxiv.org/abs/2105.02961",
          "publishedOn": "2021-07-27T02:03:36.928Z",
          "wordCount": 734,
          "title": "UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity Measure for B-Reps. (arXiv:2105.02961v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1\">Tianmin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1\">Abhishek Bhandwaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kevin A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shari Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1\">Dan Gutfreund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spelke_E/0/1/0/all/0/1\">Elizabeth Spelke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1\">Tomer D. Ullman</a>",
          "description": "For machine agents to successfully interact with humans in real-world\nsettings, they will need to develop an understanding of human mental life.\nIntuitive psychology, the ability to reason about hidden mental variables that\ndrive observable actions, comes naturally to people: even pre-verbal infants\ncan tell agents from objects, expecting agents to act efficiently to achieve\ngoals given constraints. Despite recent interest in machine agents that reason\nabout other agents, it is not clear if such agents learn or hold the core\npsychology principles that drive human reasoning. Inspired by cognitive\ndevelopment studies on intuitive psychology, we present a benchmark consisting\nof a large dataset of procedurally generated 3D animations, AGENT (Action,\nGoal, Efficiency, coNstraint, uTility), structured around four scenarios (goal\npreferences, action efficiency, unobserved constraints, and cost-reward\ntrade-offs) that probe key concepts of core intuitive psychology. We validate\nAGENT with human-ratings, propose an evaluation protocol emphasizing\ngeneralization, and compare two strong baselines built on Bayesian inverse\nplanning and a Theory of Mind neural network. Our results suggest that to pass\nthe designed tests of core intuitive psychology at human levels, a model must\nacquire or have built-in representations of how agents plan, combining utility\ncomputations and core knowledge of objects and physics.",
          "link": "http://arxiv.org/abs/2102.12321",
          "publishedOn": "2021-07-27T02:03:36.917Z",
          "wordCount": 712,
          "title": "AGENT: A Benchmark for Core Psychological Reasoning. (arXiv:2102.12321v4 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_R/0/1/0/all/0/1\">Ryuji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishibashi_H/0/1/0/all/0/1\">Hideaki Ishibashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_T/0/1/0/all/0/1\">Tetsuo Furukawa</a>",
          "description": "Visual analytics (VA) is a visually assisted exploratory analysis approach in\nwhich knowledge discovery is executed interactively between the user and system\nin a human-centered manner. The purpose of this study is to develop a method\nfor the VA of set data aimed at supporting knowledge discovery and member\nselection. A typical target application is a visual support system for team\nanalysis and member selection, by which users can analyze past teams and\nexamine candidate lineups for new teams. Because there are several\ndifficulties, such as the combinatorial explosion problem, developing a VA\nsystem of set data is challenging. In this study, we first define the\nrequirements that the target system should satisfy and clarify the accompanying\nchallenges. Then we propose a method for the VA of set data, which satisfies\nthe requirements. The key idea is to model the generation process of sets and\ntheir outputs using a manifold network model. The proposed method visualizes\nthe relevant factors as a set of topographic maps on which various information\nis visualized. Furthermore, using the topographic maps as a bidirectional\ninterface, users can indicate their targets of interest in the system on these\nmaps. We demonstrate the proposed method by applying it to basketball teams,\nand compare with a benchmark system for outcome prediction and lineup\nreconstruction tasks. Because the method can be adapted to individual\napplication cases by extending the network structure, it can be a general\nmethod by which practical systems can be built.",
          "link": "http://arxiv.org/abs/2104.09231",
          "publishedOn": "2021-07-27T02:03:36.876Z",
          "wordCount": 731,
          "title": "Visual analytics of set data for knowledge discovery and member selection support. (arXiv:2104.09231v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1\">Geeticka Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1\">Brian Tse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>",
          "description": "Recent years have seen many breakthroughs in natural language processing\n(NLP), transitioning it from a mostly theoretical field to one with many\nreal-world applications. Noting the rising number of applications of other\nmachine learning and AI techniques with pervasive societal impact, we\nanticipate the rising importance of developing NLP technologies for social\ngood. Inspired by theories in moral philosophy and global priorities research,\nwe aim to promote a guideline for social good in the context of NLP. We lay the\nfoundations via the moral philosophy definition of social good, propose a\nframework to evaluate the direct and indirect real-world impact of NLP tasks,\nand adopt the methodology of global priorities research to identify priority\ncauses for NLP research. Finally, we use our theoretical framework to provide\nsome practical guidelines for future NLP research for social good. Our data and\ncode are available at this http URL In\naddition, we curate a list of papers and resources on NLP for social good at\nhttps://github.com/zhijing-jin/NLP4SocialGood_Papers.",
          "link": "http://arxiv.org/abs/2106.02359",
          "publishedOn": "2021-07-27T02:03:36.838Z",
          "wordCount": 671,
          "title": "How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lowy_A/0/1/0/all/0/1\">Andrew Lowy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavan_R/0/1/0/all/0/1\">Rakesh Pavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baharlouei_S/0/1/0/all/0/1\">Sina Baharlouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1\">Meisam Razaviyayn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>",
          "description": "Despite the success of large-scale empirical risk minimization (ERM) at\nachieving high accuracy across a variety of machine learning tasks, fair ERM is\nhindered by the incompatibility of fairness constraints with stochastic\noptimization. In this paper, we propose the fair empirical risk minimization\nvia exponential R\\'enyi mutual information (FERMI) framework. FERMI is built on\na stochastic estimator for exponential R\\'enyi mutual information (ERMI), an\ninformation divergence measuring the degree of the dependence of predictions on\nsensitive attributes. Theoretically, we show that ERMI upper bounds existing\npopular fairness violation metrics, thus controlling ERMI provides guarantees\non other commonly used violations, such as $L_\\infty$. We derive an unbiased\nestimator for ERMI, which we use to derive the FERMI algorithm. We prove that\nFERMI converges for demographic parity, equalized odds, and equal opportunity\nnotions of fairness in stochastic optimization. Empirically, we show that FERMI\nis amenable to large-scale problems with multiple (non-binary) sensitive\nattributes and non-binary targets. Extensive experiments show that FERMI\nachieves the most favorable tradeoffs between fairness violation and test\naccuracy across all tested setups compared with state-of-the-art baselines for\ndemographic parity, equalized odds, equal opportunity. These benefits are\nespecially significant for non-binary classification with large sensitive sets\nand small batch sizes, showcasing the effectiveness of the FERMI objective and\nthe developed stochastic algorithm for solving it.",
          "link": "http://arxiv.org/abs/2102.12586",
          "publishedOn": "2021-07-27T02:03:36.819Z",
          "wordCount": 688,
          "title": "FERMI: Fair Empirical Risk Minimization via Exponential R\\'enyi Mutual Information. (arXiv:2102.12586v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oskarsdottir_M/0/1/0/all/0/1\">Mar&#xed;a &#xd3;skarsd&#xf3;ttir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1\">Cristi&#xe1;n Bravo</a>",
          "description": "We present a multilayer network model for credit risk assessment. Our model\naccounts for multiple connections between borrowers (such as their geographic\nlocation and their economic activity) and allows for explicitly modelling the\ninteraction between connected borrowers. We develop a multilayer personalized\nPageRank algorithm that allows quantifying the strength of the default exposure\nof any borrower in the network. We test our methodology in an agricultural\nlending framework, where it has been suspected for a long time default\ncorrelates between borrowers when they are subject to the same structural\nrisks. Our results show there are significant predictive gains just by\nincluding centrality multilayer network information in the model, and these\ngains are increased by more complex information such as the multilayer PageRank\nvariables. The results suggest default risk is highest when an individual is\nconnected to many defaulters, but this risk is mitigated by the size of the\nneighbourhood of the individual, showing both default risk and financial\nstability propagate throughout the network.",
          "link": "http://arxiv.org/abs/2010.09559",
          "publishedOn": "2021-07-27T02:03:36.812Z",
          "wordCount": 654,
          "title": "Multilayer Network Analysis for Improved Credit Risk Prediction. (arXiv:2010.09559v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zejin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongji Wang</a>",
          "description": "The increasing concerns about data privacy and security drive an emerging\nfield of studying privacy-preserving machine learning from isolated data\nsources, i.e., federated learning. A class of federated learning, vertical\nfederated learning, where different parties hold different features for common\nusers, has a great potential of driving a more variety of business cooperation\namong enterprises in many fields. In machine learning, decision tree ensembles\nsuch as gradient boosting decision tree (GBDT) and random forest are widely\napplied powerful models with high interpretability and modeling efficiency.\nHowever, the interpretability is compromised in state-of-the-art vertical\nfederated learning frameworks such as SecureBoost with anonymous features to\navoid possible data breaches. To address this issue in the inference process,\nin this paper, we propose Fed-EINI to protect data privacy and allow the\ndisclosure of feature meaning by concealing decision paths with a\ncommunication-efficient secure computation method for inference outputs. The\nadvantages of Fed-EINI will be demonstrated through both theoretical analysis\nand extensive numerical results.",
          "link": "http://arxiv.org/abs/2105.09540",
          "publishedOn": "2021-07-27T02:03:36.805Z",
          "wordCount": 709,
          "title": "Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10162",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Sen_P/0/1/0/all/0/1\">Pinaki Sen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bhatia_A/0/1/0/all/0/1\">Amandeep Singh Bhatia</a>",
          "description": "In quantum computing, the variational quantum algorithms (VQAs) are well\nsuited for finding optimal combinations of things in specific applications\nranging from chemistry all the way to finance. The training of VQAs with\ngradient descent optimization algorithm has shown a good convergence. At an\nearly stage, the simulation of variational quantum circuits on noisy\nintermediate-scale quantum (NISQ) devices suffers from noisy outputs. Just like\nclassical deep learning, it also suffers from vanishing gradient problems. It\nis a realistic goal to study the topology of loss landscape, to visualize the\ncurvature information and trainability of these circuits in the existence of\nvanishing gradients. In this paper, we calculated the Hessian and visualized\nthe loss landscape of variational quantum classifiers at different points in\nparameter space. The curvature information of variational quantum classifiers\n(VQC) is interpreted and the loss function's convergence is shown. It helps us\nbetter understand the behavior of variational quantum circuits to tackle\noptimization problems efficiently. We investigated the variational quantum\nclassifiers via Hessian on quantum computers, started with a simple 4-bit\nparity problem to gain insight into the practical behavior of Hessian, then\nthoroughly analyzed the behavior of Hessian's eigenvalues on training the\nvariational quantum classifier for the Diabetes dataset. Finally, we show that\nhow the adaptive Hessian learning rate can influence the convergence while\ntraining the variational circuits.",
          "link": "http://arxiv.org/abs/2105.10162",
          "publishedOn": "2021-07-27T02:03:36.797Z",
          "wordCount": 680,
          "title": "Variational Quantum Classifiers Through the Lens of the Hessian. (arXiv:2105.10162v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14694",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Farrell_M/0/1/0/all/0/1\">Max H. Farrell</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Liang_T/0/1/0/all/0/1\">Tengyuan Liang</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Misra_S/0/1/0/all/0/1\">Sanjog Misra</a>",
          "description": "We develop methodology for estimation and inference using machine learning to\nenrich economic models. Our framework takes a standard economic model and\nrecasts the parameters as fully flexible nonparametric functions, to capture\nthe rich heterogeneity based on potentially high dimensional or complex\nobservable characteristics. These \"parameter functions\" retain the\ninterpretability, economic meaning, and discipline of classical parameters.\nDeep learning is particularly well-suited to structured modeling of\nheterogeneity in economics. We show how to design the network architecture to\nmatch the structure of the economic model, delivering novel methodology that\nmoves deep learning beyond prediction. We prove convergence rates for the\nestimated parameter functions. These functions are the key inputs into the\nfinite-dimensional parameter of inferential interest. We obtain inference based\non a novel influence function calculation that covers any second-stage\nparameter and any machine-learning-enriched model that uses a smooth\nper-observation loss function. No additional derivations are required. The\nscore can be taken directly to data, using automatic differentiation if needed.\nThe researcher need only define the original model and define the parameter of\ninterest. A key insight is that we need not write down the influence function\nin order to evaluate it on the data. Our framework gives new results for a host\nof contexts, covering such diverse examples as price elasticities,\nwillingness-to-pay, and surplus measures in binary or multinomial choice\nmodels, effects of continuous treatment variables, fractional outcome models,\ncount data, heterogeneous production functions, and more. We apply our\nmethodology to a large scale advertising experiment for short-term loans. We\nshow how economically meaningful estimates and inferences can be made that\nwould be unavailable without our results.",
          "link": "http://arxiv.org/abs/2010.14694",
          "publishedOn": "2021-07-27T02:03:36.768Z",
          "wordCount": 729,
          "title": "Deep Learning for Individual Heterogeneity: An Automatic Inference Framework. (arXiv:2010.14694v2 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thang M. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1\">Long Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Do state-of-the-art natural language understanding models care about word\norder - one of the most important characteristics of a sequence? Not always! We\nfound 75% to 90% of the correct predictions of BERT-based classifiers, trained\non many GLUE tasks, remain constant after input words are randomly shuffled.\nDespite BERT embeddings are famously contextual, the contribution of each\nindividual word to downstream tasks is almost unchanged even after the word's\ncontext is shuffled. BERT-based models are able to exploit superficial cues\n(e.g. the sentiment of keywords in sentiment analysis; or the word-wise\nsimilarity between sequence-pair inputs in natural language inference) to make\ncorrect decisions when tokens are arranged in random orders. Encouraging\nclassifiers to capture word order information improves the performance on most\nGLUE tasks, SQuAD 2.0 and out-of-samples. Our work suggests that many GLUE\ntasks are not challenging machines to understand the meaning of a sentence.",
          "link": "http://arxiv.org/abs/2012.15180",
          "publishedOn": "2021-07-27T02:03:36.756Z",
          "wordCount": 649,
          "title": "Out of Order: How Important Is The Sequential Order of Words in a Sentence in Natural Language Understanding Tasks?. (arXiv:2012.15180v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardaoui_D/0/1/0/all/0/1\">Dina Mardaoui</a>",
          "description": "The performance of modern algorithms on certain computer vision tasks such as\nobject recognition is now close to that of humans. This success was achieved at\nthe price of complicated architectures depending on millions of parameters and\nit has become quite challenging to understand how particular predictions are\nmade. Interpretability methods propose to give us this understanding. In this\npaper, we study LIME, perhaps one of the most popular. On the theoretical side,\nwe show that when the number of generated examples is large, LIME explanations\nare concentrated around a limit explanation for which we give an explicit\nexpression. We further this study for elementary shape detectors and linear\nmodels. As a consequence of this analysis, we uncover a connection between LIME\nand integrated gradients, another explanation method. More precisely, the LIME\nexplanations are similar to the sum of integrated gradients over the\nsuperpixels used in the preprocessing step of LIME.",
          "link": "http://arxiv.org/abs/2102.06307",
          "publishedOn": "2021-07-27T02:03:36.746Z",
          "wordCount": 617,
          "title": "What does LIME really see in images?. (arXiv:2102.06307v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bellet_A/0/1/0/all/0/1\">Aur&#xe9;lien Bellet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kermarrec_A/0/1/0/all/0/1\">Anne-Marie Kermarrec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavoie_E/0/1/0/all/0/1\">Erick Lavoie</a>",
          "description": "The convergence speed of machine learning models trained with Federated\nLearning is significantly affected by non-independent and identically\ndistributed (non-IID) data partitions, even more so in a fully decentralized\nsetting without a central server. In this paper, we show that the impact of\nlocal class bias, an important type of data non-IIDness, can be significantly\nreduced by carefully designing the underlying communication topology. We\npresent D-Cliques, a novel topology that reduces gradient bias by grouping\nnodes in interconnected cliques such that the local joint distribution in a\nclique is representative of the global class distribution. We also show how to\nadapt the updates of decentralized SGD to obtain unbiased gradients and\nimplement an effective momentum with D-Cliques. Our empirical evaluation on\nMNIST and CIFAR10 demonstrates that our approach provides similar convergence\nspeed as a fully-connected topology with a significant reduction in the number\nof edges and messages. In a 1000-node topology, D-Cliques requires 98% less\nedges and 96% less total messages, with further possible gains using a\nsmall-world topology across cliques.",
          "link": "http://arxiv.org/abs/2104.07365",
          "publishedOn": "2021-07-27T02:03:36.736Z",
          "wordCount": 682,
          "title": "D-Cliques: Compensating NonIIDness in Decentralized Federated Learning with Topology. (arXiv:2104.07365v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sudipan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebel_P/0/1/0/all/0/1\">Patrick Ebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Most change detection methods assume that pre-change and post-change images\nare acquired by the same sensor. However, in many real-life scenarios, e.g.,\nnatural disaster, it is more practical to use the latest available images\nbefore and after the occurrence of incidence, which may be acquired using\ndifferent sensors. In particular, we are interested in the combination of the\nimages acquired by optical and Synthetic Aperture Radar (SAR) sensors. SAR\nimages appear vastly different from the optical images even when capturing the\nsame scene. Adding to this, change detection methods are often constrained to\nuse only target image-pair, no labeled data, and no additional unlabeled data.\nSuch constraints limit the scope of traditional supervised machine learning and\nunsupervised generative approaches for multi-sensor change detection. Recent\nrapid development of self-supervised learning methods has shown that some of\nthem can even work with only few images. Motivated by this, in this work we\npropose a method for multi-sensor change detection using only the unlabeled\ntarget bi-temporal images that are used for training a network in\nself-supervised fashion by using deep clustering and contrastive learning. The\nproposed method is evaluated on four multi-modal bi-temporal scenes showing\nchange and the benefits of our self-supervised approach are demonstrated.",
          "link": "http://arxiv.org/abs/2103.05102",
          "publishedOn": "2021-07-27T02:03:36.711Z",
          "wordCount": 667,
          "title": "Self-supervised Multisensor Change Detection. (arXiv:2103.05102v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14224",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gundersen_G/0/1/0/all/0/1\">Gregory W. Gundersen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cai_D/0/1/0/all/0/1\">Diana Cai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_C/0/1/0/all/0/1\">Chuteng Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Engelhardt_B/0/1/0/all/0/1\">Barbara E. Engelhardt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Adams_R/0/1/0/all/0/1\">Ryan P. Adams</a>",
          "description": "Online algorithms for detecting changepoints, or abrupt shifts in the\nbehavior of a time series, are often deployed with limited resources, e.g., to\nedge computing settings such as mobile phones or industrial sensors. In these\nscenarios it may be beneficial to trade the cost of collecting an environmental\nmeasurement against the quality or \"fidelity\" of this measurement and how the\nmeasurement affects changepoint estimation. For instance, one might decide\nbetween inertial measurements or GPS to determine changepoints for motion. A\nBayesian approach to changepoint detection is particularly appealing because we\ncan represent our posterior uncertainty about changepoints and make active,\ncost-sensitive decisions about data fidelity to reduce this posterior\nuncertainty. Moreover, the total cost could be dramatically lowered through\nactive fidelity switching, while remaining robust to changes in data\ndistribution. We propose a multi-fidelity approach that makes cost-sensitive\ndecisions about which data fidelity to collect based on maximizing information\ngain with respect to changepoints. We evaluate this framework on synthetic,\nvideo, and audio data and show that this information-based approach results in\naccurate predictions while reducing total cost.",
          "link": "http://arxiv.org/abs/2103.14224",
          "publishedOn": "2021-07-27T02:03:36.697Z",
          "wordCount": 638,
          "title": "Active multi-fidelity Bayesian online changepoint detection. (arXiv:2103.14224v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08181",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Granziol_D/0/1/0/all/0/1\">Diego Granziol</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1\">Xingchen Wan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "We conjecture that the inherent difference in generalisation between adaptive\nand non-adaptive gradient methods stems from the increased estimation noise in\nthe flattest directions of the true loss surface. We demonstrate that typical\nschedules used for adaptive methods (with low numerical stability or damping\nconstants) serve to bias relative movement towards flat directions relative to\nsharp directions, effectively amplifying the noise-to-signal ratio and harming\ngeneralisation. We further demonstrate that the numerical stability/damping\nconstant used in these methods can be decomposed into a learning rate reduction\nand linear shrinkage of the estimated curvature matrix. We then demonstrate\nsignificant generalisation improvements by increasing the shrinkage\ncoefficient, closing the generalisation gap entirely in both Logistic\nRegression and Deep Neural Network experiments. Finally, we show that other\npopular modifications to adaptive methods, such as decoupled weight decay and\npartial adaptivity can be shown to calibrate parameter updates to make better\nuse of sharper, more reliable directions.",
          "link": "http://arxiv.org/abs/2011.08181",
          "publishedOn": "2021-07-27T02:03:36.679Z",
          "wordCount": 615,
          "title": "Explaining the Adaptive Generalisation Gap. (arXiv:2011.08181v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baram_N/0/1/0/all/0/1\">Nir Baram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennenholtz_G/0/1/0/all/0/1\">Guy Tennenholtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>",
          "description": "Maximum Entropy (MaxEnt) reinforcement learning is a powerful learning\nparadigm which seeks to maximize return under entropy regularization. However,\naction entropy does not necessarily coincide with state entropy, e.g., when\nmultiple actions produce the same transition. Instead, we propose to maximize\nthe transition entropy, i.e., the entropy of next states. We show that\ntransition entropy can be described by two terms; namely, model-dependent\ntransition entropy and action redundancy. Particularly, we explore the latter\nin both deterministic and stochastic settings and develop tractable\napproximation methods in a near model-free setup. We construct algorithms to\nminimize action redundancy and demonstrate their effectiveness on a synthetic\nenvironment with multiple redundant actions as well as contemporary benchmarks\nin Atari and Mujoco. Our results suggest that action redundancy is a\nfundamental problem in reinforcement learning.",
          "link": "http://arxiv.org/abs/2102.11329",
          "publishedOn": "2021-07-27T02:03:36.672Z",
          "wordCount": 591,
          "title": "Action Redundancy in Reinforcement Learning. (arXiv:2102.11329v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jiaxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>",
          "description": "The rapid evolution of Graph Neural Networks (GNNs) has led to a growing\nnumber of new architectures as well as novel applications. However, current\nresearch focuses on proposing and evaluating specific architectural designs of\nGNNs, as opposed to studying the more general design space of GNNs that\nconsists of a Cartesian product of different design dimensions, such as the\nnumber of layers or the type of the aggregation function. Additionally, GNN\ndesigns are often specialized to a single task, yet few efforts have been made\nto understand how to quickly find the best GNN design for a novel task or a\nnovel dataset. Here we define and systematically study the architectural design\nspace for GNNs which consists of 315,000 different designs over 32 different\npredictive tasks. Our approach features three key innovations: (1) A general\nGNN design space; (2) a GNN task space with a similarity metric, so that for a\ngiven novel task/dataset, we can quickly identify/transfer the best performing\narchitecture; (3) an efficient and effective design space evaluation method\nwhich allows insights to be distilled from a huge number of model-task\ncombinations. Our key results include: (1) A comprehensive set of guidelines\nfor designing well-performing GNNs; (2) while best GNN designs for different\ntasks vary significantly, the GNN task space allows for transferring the best\ndesigns across different tasks; (3) models discovered using our design space\nachieve state-of-the-art performance. Overall, our work offers a principled and\nscalable approach to transition from studying individual GNN designs for\nspecific tasks, to systematically studying the GNN design space and the task\nspace. Finally, we release GraphGym, a powerful platform for exploring\ndifferent GNN designs and tasks. GraphGym features modularized GNN\nimplementation, standardized GNN evaluation, and reproducible and scalable\nexperiment management.",
          "link": "http://arxiv.org/abs/2011.08843",
          "publishedOn": "2021-07-27T02:03:36.665Z",
          "wordCount": 757,
          "title": "Design Space for Graph Neural Networks. (arXiv:2011.08843v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1\">Farzan Memarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goo_W/0/1/0/all/0/1\">Wonjoon Goo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lioutikov_R/0/1/0/all/0/1\">Rudolf Lioutikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1\">Scott Niekum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>",
          "description": "We introduce Self-supervised Online Reward Shaping (SORS), which aims to\nimprove the sample efficiency of any RL algorithm in sparse-reward environments\nby automatically densifying rewards. The proposed framework alternates between\nclassification-based reward inference and policy update steps -- the original\nsparse reward provides a self-supervisory signal for reward inference by\nranking trajectories that the agent observes, while the policy update is\nperformed with the newly inferred, typically dense reward function. We\nintroduce theory that shows that, under certain conditions, this alteration of\nthe reward function will not change the optimal policy of the original MDP,\nwhile potentially increasing learning speed significantly. Experimental results\non several sparse-reward environments demonstrate that, across multiple\ndomains, the proposed algorithm is not only significantly more sample efficient\nthan a standard RL baseline using sparse rewards, but, at times, also achieves\nsimilar sample efficiency compared to when hand-designed dense reward functions\nare used.",
          "link": "http://arxiv.org/abs/2103.04529",
          "publishedOn": "2021-07-27T02:03:36.643Z",
          "wordCount": 625,
          "title": "Self-Supervised Online Reward Shaping in Sparse-Reward Environments. (arXiv:2103.04529v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Meiyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stankovic_J/0/1/0/all/0/1\">John Stankovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartocci_E/0/1/0/all/0/1\">Ezio Bartocci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lu Feng</a>",
          "description": "Predictive monitoring -- making predictions about future states and\nmonitoring if the predicted states satisfy requirements -- offers a promising\nparadigm in supporting the decision making of Cyber-Physical Systems (CPS).\nExisting works of predictive monitoring mostly focus on monitoring individual\npredictions rather than sequential predictions. We develop a novel approach for\nmonitoring sequential predictions generated from Bayesian Recurrent Neural\nNetworks (RNNs) that can capture the inherent uncertainty in CPS, drawing on\ninsights from our study of real-world CPS datasets. We propose a new logic\nnamed \\emph{Signal Temporal Logic with Uncertainty} (STL-U) to monitor a\nflowpipe containing an infinite set of uncertain sequences predicted by\nBayesian RNNs. We define STL-U strong and weak satisfaction semantics based on\nif all or some sequences contained in a flowpipe satisfy the requirement. We\nalso develop methods to compute the range of confidence levels under which a\nflowpipe is guaranteed to strongly (weakly) satisfy an STL-U formula.\nFurthermore, we develop novel criteria that leverage STL-U monitoring results\nto calibrate the uncertainty estimation in Bayesian RNNs. Finally, we evaluate\nthe proposed approach via experiments with real-world datasets and a simulated\nsmart city case study, which show very encouraging results of STL-U based\npredictive monitoring approach outperforming baselines.",
          "link": "http://arxiv.org/abs/2011.00384",
          "publishedOn": "2021-07-27T02:03:36.635Z",
          "wordCount": 699,
          "title": "Predictive Monitoring with Logic-Calibrated Uncertainty for Cyber-Physical Systems. (arXiv:2011.00384v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takemori_S/0/1/0/all/0/1\">Sho Takemori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_M/0/1/0/all/0/1\">Masahiro Sato</a>",
          "description": "The RKHS bandit problem (also called kernelized multi-armed bandit problem)\nis an online optimization problem of non-linear functions with noisy feedback.\nAlthough the problem has been extensively studied, there are unsatisfactory\nresults for some problems compared to the well-studied linear bandit case.\nSpecifically, there is no general algorithm for the adversarial RKHS bandit\nproblem. In addition, high computational complexity of existing algorithms\nhinders practical application. We address these issues by considering a novel\namalgamation of approximation theory and the misspecified linear bandit\nproblem. Using an approximation method, we propose efficient algorithms for the\nstochastic RKHS bandit problem and the first general algorithm for the\nadversarial RKHS bandit problem. Furthermore, we empirically show that one of\nour proposed methods has comparable cumulative regret to IGP-UCB and its\nrunning time is much shorter.",
          "link": "http://arxiv.org/abs/2010.12167",
          "publishedOn": "2021-07-27T02:03:36.628Z",
          "wordCount": 720,
          "title": "Approximation Theory Based Methods for RKHS Bandits. (arXiv:2010.12167v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sodhani_S/0/1/0/all/0/1\">Shagun Sodhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delalleau_O/0/1/0/all/0/1\">Olivier Delalleau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1\">Mahmoud Assran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Koustuv Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>",
          "description": "Codistillation has been proposed as a mechanism to share knowledge among\nconcurrently trained models by encouraging them to represent the same function\nthrough an auxiliary loss. This contrasts with the more commonly used\nfully-synchronous data-parallel stochastic gradient descent methods, where\ndifferent model replicas average their gradients (or parameters) at every\niteration and thus maintain identical parameters. We investigate codistillation\nin a distributed training setup, complementing previous work which focused on\nextremely large batch sizes. Surprisingly, we find that even at moderate batch\nsizes, models trained with codistillation can perform as well as models trained\nwith synchronous data-parallel methods, despite using a much weaker\nsynchronization mechanism. These findings hold across a range of batch sizes\nand learning rate schedules, as well as different kinds of models and datasets.\nObtaining this level of accuracy, however, requires properly accounting for the\nregularization effect of codistillation, which we highlight through several\nempirical observations. Overall, this work contributes to a better\nunderstanding of codistillation and how to best take advantage of it in a\ndistributed computing environment.",
          "link": "http://arxiv.org/abs/2010.02838",
          "publishedOn": "2021-07-27T02:03:36.618Z",
          "wordCount": 653,
          "title": "A Closer Look at Codistillation for Distributed Training. (arXiv:2010.02838v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Janisch_J/0/1/0/all/0/1\">Jarom&#xed;r Janisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Pevn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lisy_V/0/1/0/all/0/1\">Viliam Lis&#xfd;</a>",
          "description": "We extend the framework of Classification with Costly Features (CwCF) that\nworks with samples of fixed dimensions to trees of varying depth and breadth\n(similar to a JSON/XML file). In this setting, the sample is a tree - sets of\nsets of features. Individually for each sample, the task is to sequentially\nselect informative features that help the classification. Each feature has a\nreal-valued cost, and the objective is to maximize accuracy while minimizing\nthe total cost. The process is modeled as an MDP where the states represent the\nacquired features, and the actions select unknown features. We present a\nspecialized neural network architecture trained through deep reinforcement\nlearning that naturally fits the data and directly selects features in the\ntree. We demonstrate our method in seven datasets and compare it to two\nbaselines.",
          "link": "http://arxiv.org/abs/1911.08756",
          "publishedOn": "2021-07-27T02:03:36.595Z",
          "wordCount": 625,
          "title": "Hierarchical Multiple-Instance Data Classification with Costly Features. (arXiv:1911.08756v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yue Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>",
          "description": "Health literacy has emerged as a crucial factor in making appropriate health\ndecisions and ensuring treatment outcomes. However, medical jargon and the\ncomplex structure of professional language in this domain make health\ninformation especially hard to interpret. Thus, there is an urgent unmet need\nfor automated methods to enhance the accessibility of the biomedical literature\nto the general population. This problem can be framed as a type of translation\nproblem between the language of healthcare professionals, and that of the\ngeneral public. In this paper, we introduce the novel task of automated\ngeneration of lay language summaries of biomedical scientific reviews, and\nconstruct a dataset to support the development and evaluation of automated\nmethods through which to enhance the accessibility of the biomedical\nliterature. We conduct analyses of the various challenges in solving this task,\nincluding not only summarization of the key points but also explanation of\nbackground knowledge and simplification of professional language. We experiment\nwith state-of-the-art summarization models as well as several data augmentation\ntechniques, and evaluate their performance using both automated metrics and\nhuman assessment. Results indicate that automatically generated summaries\nproduced using contemporary neural architectures can achieve promising quality\nand readability as compared with reference summaries developed for the lay\npublic by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score\nof 13.30). We also discuss the limitations of the current attempt, providing\ninsights and directions for future work.",
          "link": "http://arxiv.org/abs/2012.12573",
          "publishedOn": "2021-07-27T02:03:36.587Z",
          "wordCount": 716,
          "title": "Automated Lay Language Summarization of Biomedical Scientific Reviews. (arXiv:2012.12573v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Holla_N/0/1/0/all/0/1\">Nithin Holla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Pushkar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1\">Helen Yannakoudakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>",
          "description": "Lifelong learning requires models that can continuously learn from sequential\nstreams of data without suffering catastrophic forgetting due to shifts in data\ndistributions. Deep learning models have thrived in the non-sequential learning\nparadigm; however, when used to learn a sequence of tasks, they fail to retain\npast knowledge and learn incrementally. We propose a novel approach to lifelong\nlearning of language tasks based on meta-learning with sparse experience replay\nthat directly optimizes to prevent forgetting. We show that under the realistic\nsetting of performing a single pass on a stream of tasks and without any task\nidentifiers, our method obtains state-of-the-art results on lifelong text\nclassification and relation extraction. We analyze the effectiveness of our\napproach and further demonstrate its low computational and space complexity.",
          "link": "http://arxiv.org/abs/2009.04891",
          "publishedOn": "2021-07-27T02:03:36.580Z",
          "wordCount": 591,
          "title": "Meta-Learning with Sparse Experience Replay for Lifelong Language Learning. (arXiv:2009.04891v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.12618",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Golts_A/0/1/0/all/0/1\">Alex Golts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schechner_Y/0/1/0/all/0/1\">Yoav Y. Schechner</a>",
          "description": "Computer vision tasks are often expected to be executed on compressed images.\nClassical image compression standards like JPEG 2000 are widely used. However,\nthey do not account for the specific end-task at hand. Motivated by works on\nrecurrent neural network (RNN)-based image compression and three-dimensional\n(3D) reconstruction, we propose unified network architectures to solve both\ntasks jointly. These joint models provide image compression tailored for the\nspecific task of 3D reconstruction. Images compressed by our proposed models,\nyield 3D reconstruction performance superior as compared to using JPEG 2000\ncompression. Our models significantly extend the range of compression rates for\nwhich 3D reconstruction is possible. We also show that this can be done highly\nefficiently at almost no additional cost to obtain compression on top of the\ncomputation already required for performing the 3D reconstruction task.",
          "link": "http://arxiv.org/abs/2003.12618",
          "publishedOn": "2021-07-27T02:03:36.574Z",
          "wordCount": 605,
          "title": "Image compression optimized for 3D reconstruction by utilizing deep neural networks. (arXiv:2003.12618v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Allen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1\">Ankur Moitra</a>",
          "description": "This work represents a natural coalescence of two important lines of work:\nlearning mixtures of Gaussians and algorithmic robust statistics. In particular\nwe give the first provably robust algorithm for learning mixtures of any\nconstant number of Gaussians. We require only mild assumptions on the mixing\nweights (bounded fractionality) and that the total variation distance between\ncomponents is bounded away from zero. At the heart of our algorithm is a new\nmethod for proving dimension-independent polynomial identifiability through\napplying a carefully chosen sequence of differential operations to certain\ngenerating functions that not only encode the parameters we would like to learn\nbut also the system of polynomial equations we would like to solve. We show how\nthe symbolic identities we derive can be directly used to analyze a natural\nsum-of-squares relaxation.",
          "link": "http://arxiv.org/abs/2011.03622",
          "publishedOn": "2021-07-27T02:03:36.567Z",
          "wordCount": 613,
          "title": "Settling the Robust Learnability of Mixtures of Gaussians. (arXiv:2011.03622v3 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_D/0/1/0/all/0/1\">Deepak Mishra</a>",
          "description": "Building neural network classifiers with an ability to distinguish between in\nand out-of distribution inputs is an important step towards faithful deep\nlearning systems. Some of the successful approaches for this, resort to\narchitectural novelties, such as ensembles, with increased complexities in\nterms of the number of parameters and training procedures. Whereas some other\napproaches make use of surrogate samples, which are easy to create and work as\nproxies for actual out-of-distribution (OOD) samples, to train the networks for\nOOD detection. In this paper, we propose a very simple approach for enhancing\nthe ability of a pretrained network to detect OOD inputs without even altering\nthe original parameter values. We define a probabilistic trust interval for\neach weight parameter of the network and optimize its size according to the\nin-distribution (ID) inputs. It allows the network to sample additional weight\nvalues along with the original values at the time of inference and use the\nobserved disagreement among the corresponding outputs for OOD detection. In\norder to capture the disagreement effectively, we also propose a measure and\nestablish its suitability using empirical evidence. Our approach outperforms\nthe existing state-of-the-art methods on various OOD datasets by considerable\nmargins without using any real or surrogate OOD samples. We also analyze the\nperformance of our approach on adversarial and corrupted inputs such as\nCIFAR-10-C and demonstrate its ability to clearly distinguish such inputs as\nwell. By using fundamental theorem of calculus on neural networks, we explain\nwhy our technique doesn't need to observe OOD samples during training to\nachieve results better than the previous works.",
          "link": "http://arxiv.org/abs/2102.01336",
          "publishedOn": "2021-07-27T02:03:36.544Z",
          "wordCount": 717,
          "title": "Probabilistic Trust Intervals for Out of Distribution Detection. (arXiv:2102.01336v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12487",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mardaoui_D/0/1/0/all/0/1\">Dina Mardaoui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>",
          "description": "Text data are increasingly handled in an automated fashion by machine\nlearning algorithms. But the models handling these data are not always\nwell-understood due to their complexity and are more and more often referred to\nas \"black-boxes.\" Interpretability methods aim to explain how these models\noperate. Among them, LIME has become one of the most popular in recent years.\nHowever, it comes without theoretical guarantees: even for simple models, we\nare not sure that LIME behaves accurately. In this paper, we provide a first\ntheoretical analysis of LIME for text data. As a consequence of our theoretical\nfindings, we show that LIME indeed provides meaningful explanations for simple\nmodels, namely decision trees and linear models.",
          "link": "http://arxiv.org/abs/2010.12487",
          "publishedOn": "2021-07-27T02:03:36.538Z",
          "wordCount": 574,
          "title": "An Analysis of LIME for Text Data. (arXiv:2010.12487v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05245",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1\">Delong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_S/0/1/0/all/0/1\">Shunhui Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zewen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xinyu Zhou</a>",
          "description": "The pandemic of COVID-19 has caused millions of infections, which has led to\na great loss all over the world, socially and economically. Due to the\nfalse-negative rate and the time-consuming of the conventional Reverse\nTranscription Polymerase Chain Reaction (RT-PCR) tests, diagnosing based on\nX-ray images and Computed Tomography (CT) images has been widely adopted.\nTherefore, researchers of the computer vision area have developed many\nautomatic diagnosing models based on machine learning or deep learning to\nassist the radiologists and improve the diagnosing accuracy. In this paper, we\npresent a review of these recently emerging automatic diagnosing models. 70\nmodels proposed from February 14, 2020, to July 21, 2020, are involved. We\nanalyzed the models from the perspective of preprocessing, feature extraction,\nclassification, and evaluation. Based on the limitation of existing models, we\npointed out that domain adaption in transfer learning and interpretability\npromotion would be the possible future directions.",
          "link": "http://arxiv.org/abs/2006.05245",
          "publishedOn": "2021-07-27T02:03:36.531Z",
          "wordCount": 691,
          "title": "A Review of Automated Diagnosis of COVID-19 Based on Scanning Images. (arXiv:2006.05245v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1\">Ahmed Mohamed Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oligeri_G/0/1/0/all/0/1\">Gabriele Oligeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voigt_T/0/1/0/all/0/1\">Thiemo Voigt</a>",
          "description": "We present a new machine learning-based attack that exploits network patterns\nto detect the presence of smart IoT devices and running services in the WiFi\nradio spectrum. We perform an extensive measurement campaign of data\ncollection, and we build up a model describing the traffic patterns\ncharacterizing three popular IoT smart home devices, i.e., Google Nest Mini,\nAmazon Echo, and Amazon Echo Dot. We prove that it is possible to detect and\nidentify with overwhelming probability their presence and the services running\nby the aforementioned devices in a crowded WiFi scenario. This work proves that\nstandard encryption techniques alone are not sufficient to protect the privacy\nof the end-user, since the network traffic itself exposes the presence of both\nthe device and the associated service. While more work is required to prevent\nnon-trusted third parties to detect and identify the user's devices, we\nintroduce Eclipse, a technique to mitigate these types of attacks, which\nreshapes the traffic making the identification of the devices and the\nassociated services similar to the random classification baseline.",
          "link": "http://arxiv.org/abs/2009.07672",
          "publishedOn": "2021-07-27T02:03:36.525Z",
          "wordCount": 695,
          "title": "The Dark (and Bright) Side of IoT: Attacks and Countermeasures for Identifying Smart Home Devices and Services. (arXiv:2009.07672v4 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aanjaneya_M/0/1/0/all/0/1\">Mridul Aanjaneya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1\">Kostas Bekris</a>",
          "description": "Learning policies in simulation is promising for reducing human effort when\ntraining robot controllers. This is especially true for soft robots that are\nmore adaptive and safe but also more difficult to accurately model and control.\nThe sim2real gap is the main barrier to successfully transfer policies from\nsimulation to a real robot. System identification can be applied to reduce this\ngap but traditional identification methods require a lot of manual tuning.\nData-driven alternatives can tune dynamical models directly from data but are\noften data hungry, which also incorporates human effort in collecting data.\nThis work proposes a data-driven, end-to-end differentiable simulator focused\non the exciting but challenging domain of tensegrity robots. To the best of the\nauthors' knowledge, this is the first differentiable physics engine for\ntensegrity robots that supports cable, contact, and actuation modeling. The aim\nis to develop a reasonably simplified, data-driven simulation, which can learn\napproximate dynamics with limited ground truth data. The dynamics must be\naccurate enough to generate policies that can be transferred back to the\nground-truth system. As a first step in this direction, the current work\ndemonstrates sim2sim transfer, where the unknown physical model of MuJoCo acts\nas a ground truth system. Two different tensegrity robots are used for\nevaluation and learning of locomotion policies, a 6-bar and a 3-bar tensegrity.\nThe results indicate that only 0.25\\% of ground truth data are needed to train\na policy that works on the ground truth system when the differentiable engine\nis used for training against training the policy directly on the ground truth\nsystem.",
          "link": "http://arxiv.org/abs/2011.04929",
          "publishedOn": "2021-07-27T02:03:36.518Z",
          "wordCount": 744,
          "title": "Sim2Sim Evaluation of a Novel Data-Efficient Differentiable Physics Engine for Tensegrity Robots. (arXiv:2011.04929v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1\">Abdelrahman Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadhoud_M/0/1/0/all/0/1\">Mayada Hadhoud</a>",
          "description": "Instance segmentation has gained recently huge attention in various computer\nvision applications. It aims at providing different IDs to different objects of\nthe scene, even if they belong to the same class. Instance segmentation is\nusually performed as a two-stage pipeline. First, an object is detected, then\nsemantic segmentation within the detected box area is performed which involves\ncostly up-sampling. In this paper, we propose Insta-YOLO, a novel one-stage\nend-to-end deep learning model for real-time instance segmentation. Instead of\npixel-wise prediction, our model predicts instances as object contours\nrepresented by 2D points in Cartesian space. We evaluate our model on three\ndatasets, namely, Carvana,Cityscapes and Airbus. We compare our results to the\nstate-of-the-art models for instance segmentation. The results show our model\nachieves competitive accuracy in terms of mAP at twice the speed on GTX-1080\nGPU.",
          "link": "http://arxiv.org/abs/2102.06777",
          "publishedOn": "2021-07-27T02:03:36.494Z",
          "wordCount": 596,
          "title": "INSTA-YOLO: Real-Time Instance Segmentation. (arXiv:2102.06777v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1\">Bart Smets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1\">Jim Portegies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1\">Remco Duits</a>",
          "description": "We present a PDE-based framework that generalizes Group equivariant\nConvolutional Neural Networks (G-CNNs). In this framework, a network layer is\nseen as a set of PDE-solvers where geometrically meaningful PDE-coefficients\nbecome the layer's trainable weights. Formulating our PDEs on homogeneous\nspaces allows these networks to be designed with built-in symmetries such as\nrotation in addition to the standard translation equivariance of CNNs.\n\nHaving all the desired symmetries included in the design obviates the need to\ninclude them by means of costly techniques such as data augmentation. We will\ndiscuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space\nsetting while also going into the specifics of our primary case of interest:\nroto-translation equivariance.\n\nWe solve the PDE of interest by a combination of linear group convolutions\nand non-linear morphological group convolutions with analytic kernel\napproximations that we underpin with formal theorems. Our kernel approximations\nallow for fast GPU-implementation of the PDE-solvers, we release our\nimplementation with this article. Just like for linear convolution a\nmorphological convolution is specified by a kernel that we train in our\nPDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling\nand ReLUs as they are already subsumed by morphological convolutions.\n\nWe present a set of experiments to demonstrate the strength of the proposed\nPDE-G-CNNs in increasing the performance of deep learning based imaging\napplications with far fewer parameters than traditional CNNs.",
          "link": "http://arxiv.org/abs/2001.09046",
          "publishedOn": "2021-07-27T02:03:36.473Z",
          "wordCount": 767,
          "title": "PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milani_F/0/1/0/all/0/1\">Federico Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraternali_P/0/1/0/all/0/1\">Piero Fraternali</a>",
          "description": "Iconography in art is the discipline that studies the visual content of\nartworks to determine their motifs and themes andto characterize the way these\nare represented. It is a subject of active research for a variety of purposes,\nincluding the interpretation of meaning, the investigation of the origin and\ndiffusion in time and space of representations, and the study of influences\nacross artists and art works. With the proliferation of digital archives of art\nimages, the possibility arises of applying Computer Vision techniques to the\nanalysis of art images at an unprecedented scale, which may support iconography\nresearch and education. In this paper we introduce a novel paintings data set\nfor iconography classification and present the quantitativeand qualitative\nresults of applying a Convolutional Neural Network (CNN) classifier to the\nrecognition of the iconography of artworks. The proposed classifier achieves\ngood performances (71.17% Precision, 70.89% Recall, 70.25% F1-Score and 72.73%\nAverage Precision) in the task of identifying saints in Christian religious\npaintings, a task made difficult by the presence of classes with very similar\nvisual features. Qualitative analysis of the results shows that the CNN focuses\non the traditional iconic motifs that characterize the representation of each\nsaint and exploits such hints to attain correct identification. The ultimate\ngoal of our work is to enable the automatic extraction, decomposition, and\ncomparison of iconography elements to support iconographic studies and\nautomatic art work annotation.",
          "link": "http://arxiv.org/abs/2010.11697",
          "publishedOn": "2021-07-27T02:03:36.466Z",
          "wordCount": 739,
          "title": "A Data Set and a Convolutional Model for Iconography Classification in Paintings. (arXiv:2010.11697v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07356",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jin_H/0/1/0/all/0/1\">Hui Jin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Montufar_G/0/1/0/all/0/1\">Guido Mont&#xfa;far</a>",
          "description": "We investigate gradient descent training of wide neural networks and the\ncorresponding implicit bias in function space. For univariate regression, we\nshow that the solution of training a width-$n$ shallow ReLU network is within\n$n^{- 1/2}$ of the function which fits the training data and whose difference\nfrom the initial function has the smallest 2-norm of the second derivative\nweighted by a curvature penalty that depends on the probability distribution\nthat is used to initialize the network parameters. We compute the curvature\npenalty function explicitly for various common initialization procedures. For\ninstance, asymmetric initialization with a uniform distribution yields a\nconstant curvature penalty, and thence the solution function is the natural\ncubic spline interpolation of the training data. We obtain a similar result for\ndifferent activation functions. For multivariate regression we show an\nanalogous result, whereby the second derivative is replaced by the Radon\ntransform of a fractional Laplacian. For initialization schemes that yield a\nconstant penalty function, the solutions are polyharmonic splines. Moreover, we\nshow that the training trajectories are captured by trajectories of smoothing\nsplines with decreasing regularization strength.",
          "link": "http://arxiv.org/abs/2006.07356",
          "publishedOn": "2021-07-27T02:03:36.459Z",
          "wordCount": 659,
          "title": "Implicit bias of gradient descent for mean squared error regression with wide neural networks. (arXiv:2006.07356v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we address the problem of 3D object detection from point cloud\ndata in real time. For autonomous vehicles to work, it is very important for\nthe perception component to detect the real world objects with both high\naccuracy and fast inference. We propose a novel neural network architecture\nalong with the training and optimization details for detecting 3D objects using\npoint cloud data. We present anchor design along with custom loss functions\nused in this work. A combination of spatial and channel wise attention module\nis used in this work. We use the Kitti 3D Birds Eye View dataset for\nbenchmarking and validating our results. Our method surpasses previous state of\nthe art in this domain both in terms of average precision and speed running at\n> 30 FPS. Finally, we present the ablation study to demonstrate that the\nperformance of our network is generalizable. This makes it a feasible option to\nbe deployed in real time applications like self driving cars.",
          "link": "http://arxiv.org/abs/2107.12137",
          "publishedOn": "2021-07-27T02:03:36.453Z",
          "wordCount": 605,
          "title": "AA3DNet: Attention Augmented Real Time 3D Object Detection. (arXiv:2107.12137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1\">Liang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_H/0/1/0/all/0/1\">Hui Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruchen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhonghao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dewei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Ling Wang</a>",
          "description": "Price movement forecasting aims at predicting the future trends of financial\nassets based on the current market conditions and other relevant information.\nRecently, machine learning(ML) methods have become increasingly popular and\nachieved promising results for price movement forecasting in both academia and\nindustry. Most existing ML solutions formulate the forecasting problem as a\nclassification(to predict the direction) or a regression(to predict the return)\nproblem in the entire set of training data. However, due to the extremely low\nsignal-to-noise ratio and stochastic nature of financial data, good trading\nopportunities are extremely scarce. As a result, without careful selection of\npotentially profitable samples, such ML methods are prone to capture the\npatterns of noises instead of real signals. To address the above issues, we\npropose a novel framework-LARA(Locality-Aware Attention and Adaptive Refined\nLabeling), which contains the following three components: 1)Locality-aware\nattention automatically extracts the potentially profitable samples by\nattending to their label information in order to construct a more accurate\nclassifier on these selected samples. 2)Adaptive refined labeling further\niteratively refines the labels, alleviating the noise of samples. 3)Equipped\nwith metric learning techniques, Locality-aware attention enjoys task-specific\ndistance metrics and distributes attention on potentially profitable samples in\na more effective way. To validate our method, we conduct comprehensive\nexperiments on three real-world financial markets: ETFs, the China's A-share\nstock market, and the cryptocurrency market. LARA achieves superior performance\ncompared with the time-series analysis methods and a set of machine learning\nbased competitors on the Qlib platform. Extensive ablation studies and\nexperiments demonstrate that LARA indeed captures more reliable trading\nopportunities.",
          "link": "http://arxiv.org/abs/2107.11972",
          "publishedOn": "2021-07-27T02:03:36.447Z",
          "wordCount": 724,
          "title": "Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Adaptive Refined Labeling. (arXiv:2107.11972v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1905.11488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balghiti_O/0/1/0/all/0/1\">Othman El Balghiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmachtoub_A/0/1/0/all/0/1\">Adam N. Elmachtoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigas_P/0/1/0/all/0/1\">Paul Grigas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ambuj Tewari</a>",
          "description": "The predict-then-optimize framework is fundamental in many practical\nsettings: predict the unknown parameters of an optimization problem, and then\nsolve the problem using the predicted values of the parameters. A natural loss\nfunction in this environment is to consider the cost of the decisions induced\nby the predicted parameters, in contrast to the prediction error of the\nparameters. This loss function was recently introduced in Elmachtoub and Grigas\n(2017) and referred to as the Smart Predict-then-Optimize (SPO) loss. In this\nwork, we seek to provide bounds on how well the performance of a prediction\nmodel fit on training data generalizes out-of-sample, in the context of the SPO\nloss. Since the SPO loss is non-convex and non-Lipschitz, standard results for\nderiving generalization bounds do not apply.\n\nWe first derive bounds based on the Natarajan dimension that, in the case of\na polyhedral feasible region, scale at most logarithmically in the number of\nextreme points, but, in the case of a general convex feasible region, have\nlinear dependence on the decision dimension. By exploiting the structure of the\nSPO loss function and a key property of the feasible region, which we denote as\nthe strength property, we can dramatically improve the dependence on the\ndecision and feature dimensions. Our approach and analysis rely on placing a\nmargin around problematic predictions that do not yield unique optimal\nsolutions, and then providing generalization bounds in the context of a\nmodified margin SPO loss function that is Lipschitz continuous. Finally, we\ncharacterize the strength property and show that the modified SPO loss can be\ncomputed efficiently for both strongly convex bodies and polytopes with an\nexplicit extreme point representation.",
          "link": "http://arxiv.org/abs/1905.11488",
          "publishedOn": "2021-07-27T02:03:36.422Z",
          "wordCount": 745,
          "title": "Generalization Bounds in the Predict-then-Optimize Framework. (arXiv:1905.11488v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11876",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yen-Ju Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>",
          "description": "Diffusion probabilistic models have demonstrated an outstanding capability to\nmodel natural images and raw audio waveforms through a paired diffusion and\nreverse processes. The unique property of the reverse process (namely,\neliminating non-target signals from the Gaussian noise and noisy signals) could\nbe utilized to restore clean signals. Based on this property, we propose a\ndiffusion probabilistic model-based speech enhancement (DiffuSE) model that\naims to recover clean speech signals from noisy signals. The fundamental\narchitecture of the proposed DiffuSE model is similar to that of DiffWave--a\nhigh-quality audio waveform generation model that has a relatively low\ncomputational cost and footprint. To attain better enhancement performance, we\ndesigned an advanced reverse process, termed the supportive reverse process,\nwhich adds noisy speech in each time-step to the predicted speech. The\nexperimental results show that DiffuSE yields performance that is comparable to\nrelated audio generative models on the standardized Voice Bank corpus SE task.\nMoreover, relative to the generally suggested full sampling schedule, the\nproposed supportive reverse process especially improved the fast sampling,\ntaking few steps to yield better enhancement results over the conventional full\nstep inference process.",
          "link": "http://arxiv.org/abs/2107.11876",
          "publishedOn": "2021-07-27T02:03:36.411Z",
          "wordCount": 637,
          "title": "A Study on Speech Enhancement Based on Diffusion Probabilistic Model. (arXiv:2107.11876v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenyi Zhang</a>",
          "description": "Decentralized federated learning (DFL) is a powerful framework of distributed\nmachine learning and decentralized stochastic gradient descent (SGD) is a\ndriving engine for DFL. The performance of decentralized SGD is jointly\ninfluenced by communication-efficiency and convergence rate. In this paper, we\npropose a general decentralized federated learning framework to strike a\nbalance between communication-efficiency and convergence performance. The\nproposed framework performs both multiple local updates and multiple inter-node\ncommunications periodically, unifying traditional decentralized SGD methods. We\nestablish strong convergence guarantees for the proposed DFL algorithm without\nthe assumption of convex objective function. The balance of communication and\ncomputation rounds is essential to optimize decentralized federated learning\nunder constrained communication and computation resources. For further\nimproving communication-efficiency of DFL, compressed communication is applied\nto DFL, named DFL with compressed communication (C-DFL). The proposed C-DFL\nexhibits linear convergence for strongly convex objectives. Experiment results\nbased on MNIST and CIFAR-10 datasets illustrate the superiority of DFL over\ntraditional decentralized SGD methods and show that C-DFL further enhances\ncommunication-efficiency.",
          "link": "http://arxiv.org/abs/2107.12048",
          "publishedOn": "2021-07-27T02:03:36.404Z",
          "wordCount": 600,
          "title": "Decentralized Federated Learning: Balancing Communication and Computing Costs. (arXiv:2107.12048v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_M/0/1/0/all/0/1\">Moming Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Duo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xinyuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Renping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Liang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xianzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yujuan Tan</a>",
          "description": "Federated Learning (FL) enables the multiple participating devices to\ncollaboratively contribute to a global neural network model while keeping the\ntraining data locally. Unlike the centralized training setting, the non-IID and\nimbalanced (statistical heterogeneity) training data of FL is distributed in\nthe federated network, which will increase the divergences between the local\nmodels and global model, further degrading performance. In this paper, we\npropose a novel clustered federated learning (CFL) framework FedGroup, in which\nwe 1) group the training of clients based on the similarities between the\nclients' optimization directions for high training performance; 2) construct a\nnew data-driven distance measure to improve the efficiency of the client\nclustering procedure. 3) implement a newcomer device cold start mechanism based\non the auxiliary global model for framework scalability and practicality.\n\nFedGroup can achieve improvements by dividing joint optimization into groups\nof sub-optimization and can be combined with FL optimizer FedProx. The\nconvergence and complexity are analyzed to demonstrate the efficiency of our\nproposed framework. We also evaluate FedGroup and FedGrouProx (combined with\nFedProx) on several open datasets and made comparisons with related CFL\nframeworks. The results show that FedGroup can significantly improve absolute\ntest accuracy by +14.1% on FEMNIST compared to FedAvg. +3.4% on Sentiment140\ncompared to FedProx, +6.9% on MNIST compared to FeSEM.",
          "link": "http://arxiv.org/abs/2010.06870",
          "publishedOn": "2021-07-27T02:03:36.390Z",
          "wordCount": 740,
          "title": "FedGroup: Efficient Clustered Federated Learning via Decomposed Data-Driven Measure. (arXiv:2010.06870v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1\">Bettina Fazzinga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>",
          "description": "Dialogue systems are widely used in AI to support timely and interactive\ncommunication with users. We propose a general-purpose dialogue system\narchitecture that leverages computational argumentation and state-of-the-art\nlanguage technologies. We illustrate and evaluate the system using a COVID-19\nvaccine information case study.",
          "link": "http://arxiv.org/abs/2107.12079",
          "publishedOn": "2021-07-27T02:03:36.382Z",
          "wordCount": 541,
          "title": "An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Magister_L/0/1/0/all/0/1\">Lucie Charlotte Magister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1\">Dmitry Kazhdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vikash Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>",
          "description": "While graph neural networks (GNNs) have been shown to perform well on\ngraph-based data from a variety of fields, they suffer from a lack of\ntransparency and accountability, which hinders trust and consequently the\ndeployment of such models in high-stake and safety-critical scenarios. Even\nthough recent research has investigated methods for explaining GNNs, these\nmethods are limited to single-instance explanations, also known as local\nexplanations. Motivated by the aim of providing global explanations, we adapt\nthe well-known Automated Concept-based Explanation approach (Ghorbani et al.,\n2019) to GNN node and graph classification, and propose GCExplainer.\nGCExplainer is an unsupervised approach for post-hoc discovery and extraction\nof global concept-based explanations for GNNs, which puts the human in the\nloop. We demonstrate the success of our technique on five node classification\ndatasets and two graph classification datasets, showing that we are able to\ndiscover and extract high-quality concept representations by putting the human\nin the loop. We achieve a maximum completeness score of 1 and an average\ncompleteness score of 0.753 across the datasets. Finally, we show that the\nconcept-based explanations provide an improved insight into the datasets and\nGNN models compared to the state-of-the-art explanations produced by\nGNNExplainer (Ying et al., 2019).",
          "link": "http://arxiv.org/abs/2107.11889",
          "publishedOn": "2021-07-27T02:03:36.357Z",
          "wordCount": 645,
          "title": "GCExplainer: Human-in-the-Loop Concept-based Explanations for Graph Neural Networks. (arXiv:2107.11889v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aftab_A/0/1/0/all/0/1\">Abdul Rafey Aftab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beeck_M/0/1/0/all/0/1\">Michael von der Beeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrhirsch_S/0/1/0/all/0/1\">Steven Rohrhirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diotte_B/0/1/0/all/0/1\">Benoit Diotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feld_M/0/1/0/all/0/1\">Michael Feld</a>",
          "description": "There is a growing interest in more intelligent natural user interaction with\nthe car. Hand gestures and speech are already being applied for driver-car\ninteraction. Moreover, multimodal approaches are also showing promise in the\nautomotive industry. In this paper, we utilize deep learning for a multimodal\nfusion network for referencing objects outside the vehicle. We use features\nfrom gaze, head pose and finger pointing simultaneously to precisely predict\nthe referenced objects in different car poses. We demonstrate the practical\nlimitations of each modality when used for a natural form of referencing,\nspecifically inside the car. As evident from our results, we overcome the\nmodality specific limitations, to a large extent, by the addition of other\nmodalities. This work highlights the importance of multimodal sensing,\nespecially when moving towards natural user interaction. Furthermore, our user\nbased analysis shows noteworthy differences in recognition of user behavior\ndepending upon the vehicle pose.",
          "link": "http://arxiv.org/abs/2107.12167",
          "publishedOn": "2021-07-27T02:03:36.350Z",
          "wordCount": 603,
          "title": "Multimodal Fusion Using Deep Learning Applied to Driver's Referencing of Outside-Vehicle Objects. (arXiv:2107.12167v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tambon_F/0/1/0/all/0/1\">Florian Tambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laberge_G/0/1/0/all/0/1\">Gabriel Laberge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Le An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1\">Amin Nikanjam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mindom_P/0/1/0/all/0/1\">Paulina Stevia Nouwou Mindom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pequignot_Y/0/1/0/all/0/1\">Yann Pequignot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1\">Giulio Antoniol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlo_E/0/1/0/all/0/1\">Ettore Merlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laviolette_F/0/1/0/all/0/1\">Fran&#xe7;ois Laviolette</a>",
          "description": "Context: Machine Learning (ML) has been at the heart of many innovations over\nthe past years. However, including it in so-called 'safety-critical' systems\nsuch as automotive or aeronautic has proven to be very challenging, since the\nshift in paradigm that ML brings completely changes traditional certification\napproaches.\n\nObjective: This paper aims to elucidate challenges related to the\ncertification of ML-based safety-critical systems, as well as the solutions\nthat are proposed in the literature to tackle them, answering the question 'How\nto Certify Machine Learning Based Safety-critical Systems?'.\n\nMethod: We conduct a Systematic Literature Review (SLR) of research papers\npublished between 2015 to 2020, covering topics related to the certification of\nML systems. In total, we identified 229 papers covering topics considered to be\nthe main pillars of ML certification: Robustness, Uncertainty, Explainability,\nVerification, Safe Reinforcement Learning, and Direct Certification. We\nanalyzed the main trends and problems of each sub-field and provided summaries\nof the papers extracted.\n\nResults: The SLR results highlighted the enthusiasm of the community for this\nsubject, as well as the lack of diversity in terms of datasets and type of\nmodels. It also emphasized the need to further develop connections between\nacademia and industries to deepen the domain study. Finally, it also\nillustrated the necessity to build connections between the above mention main\npillars that are for now mainly studied separately.\n\nConclusion: We highlighted current efforts deployed to enable the\ncertification of ML based software systems, and discuss some future research\ndirections.",
          "link": "http://arxiv.org/abs/2107.12045",
          "publishedOn": "2021-07-27T02:03:36.344Z",
          "wordCount": 708,
          "title": "How to Certify Machine Learning Based Safety-critical Systems? A Systematic Literature Review. (arXiv:2107.12045v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md. Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Sumon Kanti Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tanhim Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1\">Sagor Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1\">Mehadi Hasan Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1\">Kabir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Azam Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1\">Stefan Decker</a>",
          "description": "The exponential growths of social media and micro-blogging sites not only\nprovide platforms for empowering freedom of expressions and individual voices,\nbut also enables people to express anti-social behavior like online harassment,\ncyberbullying, and hate speech. Numerous works have been proposed to utilize\ntextual data for social and anti-social behavior analysis, by predicting the\ncontexts mostly for highly-resourced languages like English. However, some\nlanguages are under-resourced, e.g., South Asian languages like Bengali, that\nlack computational resources for accurate natural language processing (NLP). In\nthis paper, we propose an explainable approach for hate speech detection from\nthe under-resourced Bengali language, which we called DeepHateExplainer.\nBengali texts are first comprehensively preprocessed, before classifying them\ninto political, personal, geopolitical, and religious hates using a neural\nensemble method of transformer-based neural architectures (i.e., monolingual\nBangla BERT-base, multilingual BERT-cased/uncased, and XLM-RoBERTa).\nImportant~(most and least) terms are then identified using sensitivity analysis\nand layer-wise relevance propagation~(LRP), before providing\nhuman-interpretable explanations. Finally, we compute comprehensiveness and\nsufficiency scores to measure the quality of explanations w.r.t faithfulness.\nEvaluations against machine learning~(linear and tree-based models) and neural\nnetworks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word embeddings) baselines\nyield F1-scores of 78%, 91%, 89%, and 84%, for political, personal,\ngeopolitical, and religious hates, respectively, outperforming both ML and DNN\nbaselines.",
          "link": "http://arxiv.org/abs/2012.14353",
          "publishedOn": "2021-07-27T02:03:36.331Z",
          "wordCount": 699,
          "title": "DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language. (arXiv:2012.14353v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1\">Yalin E. Sagduyu</a>",
          "description": "An over-the-air membership inference attack (MIA) is presented to leak\nprivate information from a wireless signal classifier. Machine learning (ML)\nprovides powerful means to classify wireless signals, e.g., for PHY-layer\nauthentication. As an adversarial machine learning attack, the MIA infers\nwhether a signal of interest has been used in the training data of a target\nclassifier. This private information incorporates waveform, channel, and device\ncharacteristics, and if leaked, can be exploited by an adversary to identify\nvulnerabilities of the underlying ML model (e.g., to infiltrate the PHY-layer\nauthentication). One challenge for the over-the-air MIA is that the received\nsignals and consequently the RF fingerprints at the adversary and the intended\nreceiver differ due to the discrepancy in channel conditions. Therefore, the\nadversary first builds a surrogate classifier by observing the spectrum and\nthen launches the black-box MIA on this classifier. The MIA results show that\nthe adversary can reliably infer signals (and potentially the radio and channel\ninformation) used to build the target classifier. Therefore, a proactive\ndefense is developed against the MIA by building a shadow MIA model and fooling\nthe adversary. This defense can successfully reduce the MIA accuracy and\nprevent information leakage from the wireless signal classifier.",
          "link": "http://arxiv.org/abs/2107.12173",
          "publishedOn": "2021-07-27T02:03:36.323Z",
          "wordCount": 656,
          "title": "Membership Inference Attack and Defense for Wireless Signal Classifiers with Deep Learning. (arXiv:2107.12173v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2006.08601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lerman_S/0/1/0/all/0/1\">Samuel Lerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venuto_C/0/1/0/all/0/1\">Charles Venuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_H/0/1/0/all/0/1\">Henry Kautz</a>",
          "description": "We present a simple yet highly generalizable method for explaining\ninteracting parts within a neural network's reasoning process. First, we design\nan algorithm based on cross derivatives for computing statistical interaction\neffects between individual features, which is generalized to both 2-way and\nhigher-order (3-way or more) interactions. We present results side by side with\na weight-based attribution technique, corroborating that cross derivatives are\na superior metric for both 2-way and higher-order interaction detection.\nMoreover, we extend the use of cross derivatives as an explanatory device in\nneural networks to the computer vision setting by expanding Grad-CAM, a popular\ngradient-based explanatory tool for CNNs, to the higher order. While Grad-CAM\ncan only explain the importance of individual objects in images, our method,\nwhich we call Taylor-CAM, can explain a neural network's relational reasoning\nacross multiple objects. We show the success of our explanations both\nqualitatively and quantitatively, including with a user study. We will release\nall code as a tool package to facilitate explainable deep learning.",
          "link": "http://arxiv.org/abs/2006.08601",
          "publishedOn": "2021-07-27T02:03:36.297Z",
          "wordCount": 643,
          "title": "Explaining Local, Global, And Higher-Order Interactions In Deep Learning. (arXiv:2006.08601v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12110",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Hunnefeld_M/0/1/0/all/0/1\">Mirco H&#xfc;nnefeld</a> (for the IceCube Collaboration)",
          "description": "The field of deep learning has become increasingly important for particle\nphysics experiments, yielding a multitude of advances, predominantly in event\nclassification and reconstruction tasks. Many of these applications have been\nadopted from other domains. However, data in the field of physics are unique in\nthe context of machine learning, insofar as their generation process and the\nlaws and symmetries they abide by are usually well understood. Most commonly\nused deep learning architectures fail at utilizing this available information.\nIn contrast, more traditional likelihood-based methods are capable of\nexploiting domain knowledge, but they are often limited by computational\ncomplexity. In this contribution, a hybrid approach is presented that utilizes\ngenerative neural networks to approximate the likelihood, which may then be\nused in a traditional maximum-likelihood setting. Domain knowledge, such as\ninvariances and detector characteristics, can easily be incorporated in this\napproach. The hybrid approach is illustrated by the example of event\nreconstruction in IceCube.",
          "link": "http://arxiv.org/abs/2107.12110",
          "publishedOn": "2021-07-27T02:03:36.290Z",
          "wordCount": 618,
          "title": "Combining Maximum-Likelihood with Deep Learning for Event Reconstruction in IceCube. (arXiv:2107.12110v1 [astro-ph.HE])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Adversarial examples contain carefully crafted perturbations that can fool\ndeep neural networks (DNNs) into making wrong predictions. Enhancing the\nadversarial robustness of DNNs has gained considerable interest in recent\nyears. Although image transformation-based defenses were widely considered at\nan earlier time, most of them have been defeated by adaptive attacks. In this\npaper, we propose a new image transformation defense based on error diffusion\nhalftoning, and combine it with adversarial training to defend against\nadversarial examples. Error diffusion halftoning projects an image into a 1-bit\nspace and diffuses quantization error to neighboring pixels. This process can\nremove adversarial perturbations from a given image while maintaining\nacceptable image quality in the meantime in favor of recognition. Experimental\nresults demonstrate that the proposed method is able to improve adversarial\nrobustness even under advanced adaptive attacks, while most of the other image\ntransformation-based defenses do not. We show that a proper image\ntransformation can still be an effective defense approach. Code:\nhttps://github.com/shaoyuanlo/Halftoning-Defense",
          "link": "http://arxiv.org/abs/2101.09451",
          "publishedOn": "2021-07-27T02:03:36.283Z",
          "wordCount": 654,
          "title": "Error Diffusion Halftoning Against Adversarial Examples. (arXiv:2101.09451v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhechun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1\">Theodoros Rekatsinas</a>",
          "description": "Data corruption is an impediment to modern machine learning deployments.\nCorrupted data can severely bias the learned model and can also lead to invalid\ninferences. We present, Picket, a simple framework to safeguard against data\ncorruptions during both training and deployment of machine learning models over\ntabular data. For the training stage, Picket identifies and removes corrupted\ndata points from the training data to avoid obtaining a biased model. For the\ndeployment stage, Picket flags, in an online manner, corrupted query points to\na trained machine learning model that due to noise will result in incorrect\npredictions. To detect corrupted data, Picket uses a self-supervised deep\nlearning model for mixed-type tabular data, which we call PicketNet. To\nminimize the burden of deployment, learning a PicketNet model does not require\nany human-labeled data. Picket is designed as a plugin that can increase the\nrobustness of any machine learning pipeline. We evaluate Picket on a diverse\narray of real-world data considering different corruption models that include\nsystematic and adversarial noise during both training and testing. We show that\nPicket consistently safeguards against corrupted data during both training and\ndeployment of various models ranging from SVMs to neural networks, beating a\ndiverse array of competing methods that span from data quality validation\nmodels to robust outlier-detection models.",
          "link": "http://arxiv.org/abs/2006.04730",
          "publishedOn": "2021-07-27T02:03:36.269Z",
          "wordCount": 704,
          "title": "Picket: Guarding Against Corrupted Data in Tabular Data during Learning and Inference. (arXiv:2006.04730v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1\">Xi Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weiji Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaomei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weiwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xiaobo Lai</a>",
          "description": "Background: Glioma is the most common brain malignant tumor, with a high\nmorbidity rate and a mortality rate of more than three percent, which seriously\nendangers human health. The main method of acquiring brain tumors in the clinic\nis MRI. Segmentation of brain tumor regions from multi-modal MRI scan images is\nhelpful for treatment inspection, post-diagnosis monitoring, and effect\nevaluation of patients. However, the common operation in clinical brain tumor\nsegmentation is still manual segmentation, lead to its time-consuming and large\nperformance difference between different operators, a consistent and accurate\nautomatic segmentation method is urgently needed. Methods: To meet the above\nchallenges, we propose an automatic brain tumor MRI data segmentation framework\nwhich is called AGSE-VNet. In our study, the Squeeze and Excite (SE) module is\nadded to each encoder, the Attention Guide Filter (AG) module is added to each\ndecoder, using the channel relationship to automatically enhance the useful\ninformation in the channel to suppress the useless information, and use the\nattention mechanism to guide the edge information and remove the influence of\nirrelevant information such as noise. Results: We used the BraTS2020 challenge\nonline verification tool to evaluate our approach. The focus of verification is\nthat the Dice scores of the whole tumor (WT), tumor core (TC) and enhanced\ntumor (ET) are 0.68, 0.85 and 0.70, respectively. Conclusion: Although MRI\nimages have different intensities, AGSE-VNet is not affected by the size of the\ntumor, and can more accurately extract the features of the three regions, it\nhas achieved impressive results and made outstanding contributions to the\nclinical diagnosis and treatment of brain tumor patients.",
          "link": "http://arxiv.org/abs/2107.12046",
          "publishedOn": "2021-07-27T02:03:36.259Z",
          "wordCount": 725,
          "title": "3D AGSE-VNet: An Automatic Brain Tumor MRI Data Segmentation Framework. (arXiv:2107.12046v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2008.02275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1\">Collin Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Critch_A/0/1/0/all/0/1\">Andrew Critch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "We show how to assess a language model's knowledge of basic concepts of\nmorality. We introduce the ETHICS dataset, a new benchmark that spans concepts\nin justice, well-being, duties, virtues, and commonsense morality. Models\npredict widespread moral judgments about diverse text scenarios. This requires\nconnecting physical and social world knowledge to value judgements, a\ncapability that may enable us to steer chatbot outputs or eventually regularize\nopen-ended reinforcement learning agents. With the ETHICS dataset, we find that\ncurrent language models have a promising but incomplete ability to predict\nbasic human ethical judgements. Our work shows that progress can be made on\nmachine ethics today, and it provides a steppingstone toward AI that is aligned\nwith human values.",
          "link": "http://arxiv.org/abs/2008.02275",
          "publishedOn": "2021-07-27T02:03:36.231Z",
          "wordCount": 634,
          "title": "Aligning AI With Shared Human Values. (arXiv:2008.02275v5 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.10136",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zatarain_Vera_O/0/1/0/all/0/1\">Oscar Zatarain-Vera</a>",
          "description": "Andreas Maurer in the paper \"A vector-contraction inequality for Rademacher\ncomplexities\" extended the contraction inequality for Rademacher averages to\nLipschitz functions with vector-valued domains; He did it replacing the\nRademacher variables in the bounding expression by arbitrary idd symmetric and\nsub-gaussian variables. We will see how to extend this work when we replace\nsub-gaussian variables by $p$-stable variables for $1<p<2$.",
          "link": "http://arxiv.org/abs/1912.10136",
          "publishedOn": "2021-07-27T02:03:36.225Z",
          "wordCount": 510,
          "title": "A vector-contraction inequality for Rademacher complexities using $p$-stable variables. (arXiv:1912.10136v2 [math.PR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12070",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tastan_A/0/1/0/all/0/1\">Aylin Tastan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muma_M/0/1/0/all/0/1\">Michael Muma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zoubir_A/0/1/0/all/0/1\">Abdelhak M. Zoubir</a>",
          "description": "The Fiedler vector of a connected graph is the eigenvector associated with\nthe algebraic connectivity of the graph Laplacian and it provides substantial\ninformation to learn the latent structure of a graph. In real-world\napplications, however, the data may be subject to heavy-tailed noise and\noutliers which results in deteriorations in the structure of the Fiedler vector\nestimate. We design a Robust Regularized Locality Preserving Indexing (RRLPI)\nmethod for Fiedler vector estimation that aims to approximate the nonlinear\nmanifold structure of the Laplace Beltrami operator while minimizing the\nnegative impact of outliers. First, an analysis of the effects of two\nfundamental outlier types on the eigen-decomposition for block affinity\nmatrices which are essential in cluster analysis is conducted. Then, an error\nmodel is formulated and a robust Fiedler vector estimation algorithm is\ndeveloped. An unsupervised penalty parameter selection algorithm is proposed\nthat leverages the geometric structure of the projection space to perform\nrobust regularized Fiedler estimation. The performance of RRLPI is benchmarked\nagainst existing competitors in terms of detection probability, partitioning\nquality, image segmentation capability, robustness and computation time using a\nlarge variety of synthetic and real data experiments.",
          "link": "http://arxiv.org/abs/2107.12070",
          "publishedOn": "2021-07-27T02:03:36.218Z",
          "wordCount": 628,
          "title": "Robust Regularized Locality Preserving Indexing for Fiedler Vector Estimation. (arXiv:2107.12070v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2009.12462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Janisch_J/0/1/0/all/0/1\">Jarom&#xed;r Janisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Pevn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lisy_V/0/1/0/all/0/1\">Viliam Lis&#xfd;</a>",
          "description": "We focus on reinforcement learning (RL) in relational problems that are\nnaturally defined in terms of objects, their relations, and manipulations.\nThese problems are characterized by variable state and action spaces, and\nfinding a fixed-length representation, required by most existing RL methods, is\ndifficult, if not impossible. We present a deep RL framework based on graph\nneural networks and auto-regressive policy decomposition that naturally works\nwith these problems and is completely domain-independent. We demonstrate the\nframework in three very distinct domains and we report the method's competitive\nperformance and impressive zero-shot generalization over different problem\nsizes. In goal-oriented BlockWorld, we demonstrate multi-parameter actions with\npre-conditions. In SysAdmin, we show how to select multiple objects\nsimultaneously. In the classical planning domain of Sokoban, the method trained\nexclusively on 10x10 problems with three boxes solves 89% of 15x15 problems\nwith five boxes.",
          "link": "http://arxiv.org/abs/2009.12462",
          "publishedOn": "2021-07-27T02:03:36.212Z",
          "wordCount": 623,
          "title": "Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks. (arXiv:2009.12462v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_N/0/1/0/all/0/1\">Norman Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Frank Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorundo_E/0/1/0/all/0/1\">Evan Dorundo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_R/0/1/0/all/0/1\">Rahul Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tyler Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1\">Samyak Parajuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mike Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilmer_J/0/1/0/all/0/1\">Justin Gilmer</a>",
          "description": "We introduce four new real-world distribution shift datasets consisting of\nchanges in image style, image blurriness, geographic location, camera\noperation, and more. With our new datasets, we take stock of previously\nproposed methods for improving out-of-distribution robustness and put them to\nthe test. We find that using larger models and artificial data augmentations\ncan improve robustness on real-world distribution shifts, contrary to claims in\nprior work. We find improvements in artificial robustness benchmarks can\ntransfer to real-world distribution shifts, contrary to claims in prior work.\nMotivated by our observation that data augmentations can help with real-world\ndistribution shifts, we also introduce a new data augmentation method which\nadvances the state-of-the-art and outperforms models pretrained with 1000 times\nmore labeled data. Overall we find that some methods consistently help with\ndistribution shifts in texture and local image statistics, but these methods do\nnot help with some other distribution shifts like geographic changes. Our\nresults show that future research must study multiple distribution shifts\nsimultaneously, as we demonstrate that no evaluated method consistently\nimproves robustness.",
          "link": "http://arxiv.org/abs/2006.16241",
          "publishedOn": "2021-07-27T02:03:36.205Z",
          "wordCount": 694,
          "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. (arXiv:2006.16241v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03983",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Avrachenkov_K/0/1/0/all/0/1\">Konstantin Avrachenkov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Borkar_V/0/1/0/all/0/1\">Vivek S. Borkar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Moharir_S/0/1/0/all/0/1\">Sharayu Moharir</a>, <a href=\"http://arxiv.org/find/math/1/au:+Shah_S/0/1/0/all/0/1\">Suhail M. Shah</a>",
          "description": "We introduce a model of graph-constrained dynamic choice with reinforcement\nmodeled by positively $\\alpha$-homogeneous rewards. We show that its empirical\nprocess, which can be written as a stochastic approximation recursion with\nMarkov noise, has the same probability law as a certain vertex reinforced\nrandom walk. We use this equivalence to show that for $\\alpha > 0$, the\nasymptotic outcome concentrates around the optimum in a certain limiting sense\nwhen `annealed' by letting $\\alpha\\uparrow\\infty$ slowly.",
          "link": "http://arxiv.org/abs/2007.03983",
          "publishedOn": "2021-07-27T02:03:36.198Z",
          "wordCount": 537,
          "title": "Dynamic social learning under graph constraints. (arXiv:2007.03983v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12078",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhemchuzhnikov_D/0/1/0/all/0/1\">Dmitrii Zhemchuzhnikov</a> (DAO), <a href=\"http://arxiv.org/find/q-bio/1/au:+Igashov_I/0/1/0/all/0/1\">Ilia Igashov</a> (DAO), <a href=\"http://arxiv.org/find/q-bio/1/au:+Grudinin_S/0/1/0/all/0/1\">Sergei Grudinin</a> (DAO)",
          "description": "In this work, we introduce 6D Convolutional Neural Network (6DCNN) designed\nto tackle the problem of detecting relative positions and orientations of local\npatterns when processing three-dimensional volumetric data. 6DCNN also includes\nSE(3)-equivariant message-passing and nonlinear activation operations\nconstructed in the Fourier space. Working in the Fourier space allows\nsignificantly reducing the computational complexity of our operations. We\ndemonstrate the properties of the 6D convolution and its efficiency in the\nrecognition of spatial patterns. We also assess the 6DCNN model on several\ndatasets from the recent CASP protein structure prediction challenges. Here,\n6DCNN improves over the baseline architecture and also outperforms the state of\nthe art.",
          "link": "http://arxiv.org/abs/2107.12078",
          "publishedOn": "2021-07-27T02:03:36.173Z",
          "wordCount": 552,
          "title": "6DCNN with roto-translational convolution filters for volumetric data processing. (arXiv:2107.12078v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaming Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shaohui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1\">Qi Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zidong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunji Chen</a>",
          "description": "Policy gradient methods are appealing in deep reinforcement learning but\nsuffer from high variance of gradient estimate. To reduce the variance, the\nstate value function is applied commonly. However, the effect of the state\nvalue function becomes limited in stochastic dynamic environments, where the\nunexpected state dynamics and rewards will increase the variance. In this\npaper, we propose to replace the state value function with a novel hindsight\nvalue function, which leverages the information from the future to reduce the\nvariance of the gradient estimate for stochastic dynamic environments.\n\nParticularly, to obtain an ideally unbiased gradient estimate, we propose an\ninformation-theoretic approach, which optimizes the embeddings of the future to\nbe independent of previous actions. In our experiments, we apply the proposed\nhindsight value function in stochastic dynamic environments, including\ndiscrete-action environments and continuous-action environments. Compared with\nthe standard state value function, the proposed hindsight value function\nconsistently reduces the variance, stabilizes the training, and improves the\neventual policy.",
          "link": "http://arxiv.org/abs/2107.12216",
          "publishedOn": "2021-07-27T02:03:36.167Z",
          "wordCount": 607,
          "title": "Hindsight Value Function for Variance Reduction in Stochastic Dynamic Environment. (arXiv:2107.12216v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fraboni_Y/0/1/0/all/0/1\">Yann Fraboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Richard Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kameni_L/0/1/0/all/0/1\">Laetitia Kameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1\">Marco Lorenzi</a>",
          "description": "While clients' sampling is a central operation of current state-of-the-art\nfederated learning (FL) approaches, the impact of this procedure on the\nconvergence and speed of FL remains to date under-investigated. In this work we\nintroduce a novel decomposition theorem for the convergence of FL, allowing to\nclearly quantify the impact of client sampling on the global model update.\nContrarily to previous convergence analyses, our theorem provides the exact\ndecomposition of a given convergence step, thus enabling accurate\nconsiderations about the role of client sampling and heterogeneity. First, we\nprovide a theoretical ground for previously reported results on the\nrelationship between FL convergence and the variance of the aggregation\nweights. Second, we prove for the first time that the quality of FL convergence\nis also impacted by the resulting covariance between aggregation weights.\nThird, we establish that the sum of the aggregation weights is another source\nof slow-down and should be equal to 1 to improve FL convergence speed. Our\ntheory is general, and is here applied to Multinomial Distribution (MD) and\nUniform sampling, the two default client sampling in FL, and demonstrated\nthrough a series of experiments in non-iid and unbalanced scenarios. Our\nresults suggest that MD sampling should be used as default sampling scheme, due\nto the resilience to the changes in data ratio during the learning process,\nwhile Uniform sampling is superior only in the special case when clients have\nthe same amount of data.",
          "link": "http://arxiv.org/abs/2107.12211",
          "publishedOn": "2021-07-27T02:03:36.161Z",
          "wordCount": 682,
          "title": "On The Impact of Client Sampling on Federated Learning Convergence. (arXiv:2107.12211v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brabec_J/0/1/0/all/0/1\">Jan Brabec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machlica_L/0/1/0/all/0/1\">Lukas Machlica</a>",
          "description": "In this paper, Bayesian based aggregation of decision trees in an ensemble\n(decision forest) is investigated. The focus is laid on multi-class\nclassification with number of samples significantly skewed toward one of the\nclasses. The algorithm leverages out-of-bag datasets to estimate prediction\nerrors of individual trees, which are then used in accordance with the Bayes\nrule to refine the decision of the ensemble. The algorithm takes prevalence of\nindividual classes into account and does not require setting of any additional\nparameters related to class weights or decision-score thresholds. Evaluation is\nbased on publicly available datasets as well as on an proprietary dataset\ncomprising network traffic telemetry from hundreds of enterprise networks with\nover a million of users overall. The aim is to increase the detection\ncapabilities of an operating malware detection system. While we were able to\nkeep precision of the system higher than 94\\%, that is only 6 out of 100\ndetections shown to the network administrator are false alarms, we were able to\nachieve increase of approximately 7\\% in the number of detections. The\nalgorithm effectively handles large amounts of data, and can be used in\nconjunction with most of the state-of-the-art algorithms used to train decision\nforests.",
          "link": "http://arxiv.org/abs/2107.11862",
          "publishedOn": "2021-07-27T02:03:36.131Z",
          "wordCount": 710,
          "title": "Decision-forest voting scheme for classification of rare classes in network intrusion detection. (arXiv:2107.11862v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12156",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Mishra_A/0/1/0/all/0/1\">Akshansh Mishra</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Dixit_D/0/1/0/all/0/1\">Devarrishi Dixit</a>",
          "description": "Advent in machine learning is leaving a deep impact on various sectors\nincluding the material science domain. The present paper highlights the\napplication of various supervised machine learning regression algorithms such\nas polynomial regression, decision tree regression algorithm, random forest\nalgorithm, support vector regression algorithm, and artificial neural network\nalgorithm to determine the thin film thickness of Polystyrene on the glass\nsubstrates. The results showed that the polynomial regression machine learning\nalgorithm outperforms all other machine learning models by yielding the\ncoefficient of determination of 0.96 approximately and mean square error of\n0.04 respectively.",
          "link": "http://arxiv.org/abs/2107.12156",
          "publishedOn": "2021-07-27T02:03:36.125Z",
          "wordCount": 543,
          "title": "Brain Inspired Computing Approach for the Optimization of the Thin Film Thickness of Polystyrene on the Glass Substrates. (arXiv:2107.12156v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ohmichi_Y/0/1/0/all/0/1\">Yuya Ohmichi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugioka_Y/0/1/0/all/0/1\">Yosuke Sugioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakakita_K/0/1/0/all/0/1\">Kazuyuki Nakakita</a>",
          "description": "In this study, we proposed the truncated total least squares dynamic mode\ndecomposition (T-TLS DMD) algorithm, which can perform DMD analysis of noisy\ndata. By adding truncation regularization to the conventional TLS DMD\nalgorithm, T-TLS DMD improves the stability of the computation while\nmaintaining the accuracy of TLS DMD. The effectiveness of the proposed method\nwas evaluated by the analysis of the wake behind a cylinder and\npressure-sensitive paint (PSP) data for the buffet cell phenomenon. The results\nshowed the importance of regularization in the DMD algorithm. With respect to\nthe eigenvalues, T-TLS DMD was less affected by noise, and accurate eigenvalues\ncould be obtained stably, whereas the eigenvalues of TLS and subspace DMD\nvaried greatly due to noise. It was also observed that the eigenvalues of the\nstandard and exact DMD had the problem of shifting to the damping side, as\nreported in previous studies. With respect to eigenvectors, T-TLS and exact DMD\ncaptured the characteristic flow patterns clearly even in the presence of\nnoise, whereas TLS and subspace DMD were not able to capture them clearly due\nto noise.",
          "link": "http://arxiv.org/abs/2107.11999",
          "publishedOn": "2021-07-27T02:03:36.118Z",
          "wordCount": 624,
          "title": "Stable Dynamic Mode Decomposition Algorithm for Noisy Pressure-Sensitive Paint Measurement Data. (arXiv:2107.11999v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jicong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Yiheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingbo Zhao</a>",
          "description": "The performance of spectral clustering heavily relies on the quality of\naffinity matrix. A variety of affinity-matrix-construction methods have been\nproposed but they have hyper-parameters to determine beforehand, which requires\nstrong experience and lead to difficulty in real applications especially when\nthe inter-cluster similarity is high or/and the dataset is large. On the other\nhand, we often have to determine to use a linear model or a nonlinear model,\nwhich still depends on experience. To solve these two problems, in this paper,\nwe present an eigen-gap guided search method for subspace clustering. The main\nidea is to find the most reliable affinity matrix among a set of candidates\nconstructed by linear and kernel regressions, where the reliability is\nquantified by the \\textit{relative-eigen-gap} of graph Laplacian defined in\nthis paper. We show, theoretically and numerically, that the Laplacian matrix\nwith a larger relative-eigen-gap often yields a higher clustering accuracy and\nstability. Our method is able to automatically search the best model and\nhyper-parameters in a pre-defined space. The search space is very easy to\ndetermine and can be arbitrarily large, though a relatively compact search\nspace can reduce the highly unnecessary computation. Our method has high\nflexibility and convenience in real applications, and also has low\ncomputational cost because the affinity matrix is not computed by iterative\noptimization. We extend the method to large-scale datasets such as MNIST, on\nwhich the time cost is less than 90s and the clustering accuracy is\nstate-of-the-art. Extensive experiments of natural image clustering show that\nour method is more stable, accurate, and efficient than baseline methods.",
          "link": "http://arxiv.org/abs/2107.12183",
          "publishedOn": "2021-07-27T02:03:36.111Z",
          "wordCount": 696,
          "title": "EGGS: Eigen-Gap Guided Search\\\\ Making Subspace Clustering Easy. (arXiv:2107.12183v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Molitor_D/0/1/0/all/0/1\">Dirk Alexander Molitor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubik_C/0/1/0/all/0/1\">Christian Kubik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hetfleisch_R/0/1/0/all/0/1\">Ruben Helmut Hetfleisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groche_P/0/1/0/all/0/1\">Peter Groche</a>",
          "description": "Blanking processes belong to the most widely used manufacturing techniques\ndue to their economic efficiency. Their economic viability depends to a large\nextent on the resulting product quality and the associated customer\nsatisfaction as well as on possible downtimes. In particular, the occurrence of\nincreased tool wear reduces the product quality and leads to downtimes, which\nis why considerable research has been carried out in recent years with regard\nto wear detection. While processes have widely been monitored based on force\nand acceleration signals, a new approach is pursued in this paper. Blanked\nworkpieces manufactured by punches with 16 different wear states are\nphotographed and then used as inputs for Deep Convolutional Neural Networks to\nclassify wear states. The results show that wear states can be predicted with\nsurprisingly high accuracy, opening up new possibilities and research\nopportunities for tool wear monitoring of blanking processes.",
          "link": "http://arxiv.org/abs/2107.12034",
          "publishedOn": "2021-07-27T02:03:36.096Z",
          "wordCount": 600,
          "title": "Workpiece Image-based Tool Wear Classification in Blanking Processes Using Deep Convolutional Neural Networks. (arXiv:2107.12034v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gote_C/0/1/0/all/0/1\">Christoph Gote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perri_V/0/1/0/all/0/1\">Vincenzo Perri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholtes_I/0/1/0/all/0/1\">Ingo Scholtes</a>",
          "description": "Networks are frequently used to model complex systems comprised of\ninteracting elements. While links capture the topology of direct interactions,\nthe true complexity of many systems originates from higher-order patterns in\npaths by which nodes can indirectly influence each other. Path data,\nrepresenting ordered sequences of consecutive direct interactions, can be used\nto model these patterns. However, to avoid overfitting, such models should only\nconsider those higher-order patterns for which the data provide sufficient\nstatistical evidence. On the other hand, we hypothesise that network models,\nwhich capture only direct interactions, underfit higher-order patterns present\nin data. Consequently, both approaches are likely to misidentify influential\nnodes in complex networks. We contribute to this issue by proposing eight\ncentrality measures based on MOGen, a multi-order generative model that\naccounts for all paths up to a maximum distance but disregards paths at higher\ndistances. We compare MOGen-based centralities to equivalent measures for\nnetwork models and path data in a prediction experiment where we aim to\nidentify influential nodes in out-of-sample data. Our results show strong\nevidence supporting our hypothesis. MOGen consistently outperforms both the\nnetwork model and path-based prediction. We further show that the performance\ndifference between MOGen and the path-based approach disappears if we have\nsufficient observations, confirming that the error is due to overfitting.",
          "link": "http://arxiv.org/abs/2107.12100",
          "publishedOn": "2021-07-27T02:03:36.090Z",
          "wordCount": 677,
          "title": "Predicting Influential Higher-Order Patterns in Temporal Network Data. (arXiv:2107.12100v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1\">Rujing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_O/0/1/0/all/0/1\">Ou Wu</a>",
          "description": "Weighting strategy prevails in machine learning. For example, a common\napproach in robust machine learning is to exert lower weights on samples which\nare likely to be noisy or hard. This study reveals another undiscovered\nstrategy, namely, compensating, that has also been widely used in machine\nlearning. Learning with compensating is called compensation learning and a\nsystematic taxonomy is constructed for it in this study. In our taxonomy,\ncompensation learning is divided on the basis of the compensation targets,\ninference manners, and granularity levels. Many existing learning algorithms\nincluding some classical ones can be seen as a special case of compensation\nlearning or partially leveraging compensating. Furthermore, a family of new\nlearning algorithms can be obtained by plugging the compensation learning into\nexisting learning algorithms. Specifically, three concrete new learning\nalgorithms are proposed for robust machine learning. Extensive experiments on\ntext sentiment analysis, image classification, and graph classification verify\nthe effectiveness of the three new algorithms. Compensation learning can also\nbe used in various learning scenarios, such as imbalance learning, clustering,\nregression, and so on.",
          "link": "http://arxiv.org/abs/2107.11921",
          "publishedOn": "2021-07-27T02:03:36.083Z",
          "wordCount": 591,
          "title": "Compensation Learning. (arXiv:2107.11921v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenhai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>",
          "description": "We describe an efficient hierarchical method to compute attention in the\nTransformer architecture. The proposed attention mechanism exploits a matrix\nstructure similar to the Hierarchical Matrix (H-Matrix) developed by the\nnumerical analysis community, and has linear run time and memory complexity. We\nperform extensive experiments to show that the inductive bias embodied by our\nhierarchical attention is effective in capturing the hierarchical structure in\nthe sequences typical for natural language and vision tasks. Our method is\nsuperior to alternative sub-quadratic proposals by over +6 points on average on\nthe Long Range Arena benchmark. It also sets a new SOTA test perplexity on\nOne-Billion Word dataset with 5x fewer model parameters than that of the\nprevious-best Transformer-based models.",
          "link": "http://arxiv.org/abs/2107.11906",
          "publishedOn": "2021-07-27T02:03:35.956Z",
          "wordCount": 552,
          "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences. (arXiv:2107.11906v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12013",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lai_M/0/1/0/all/0/1\">Ming-Chih Lai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chang_C/0/1/0/all/0/1\">Che-Chia Chang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lin_W/0/1/0/all/0/1\">Wei-Syuan Lin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hu_W/0/1/0/all/0/1\">Wei-Fan Hu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lin_T/0/1/0/all/0/1\">Te-Sheng Lin</a>",
          "description": "In this paper, a shallow Ritz-type neural network for solving elliptic\nproblems with delta function singular sources on an interface is developed.\nThere are three novel features in the present work; namely, (i) the delta\nfunction singularity is naturally removed, (ii) level set function is\nintroduced as a feather input, (iii) it is completely shallow consisting of\nonly one hidden layer. We first introduce the energy functional of the problem\nand then transform the contribution of singular sources to a regular surface\nintegral along the interface. In such a way the delta function singularity can\nbe naturally removed without the introduction of discrete delta function that\nis commonly used in traditional regularization methods such as the well-known\nimmersed boundary method. The original problem is then reformulated as a\nminimization problem. We propose a shallow Ritz-type neural network with one\nhidden layer to approximate the global minimizer of the energy functional. As a\nresult, the network is trained by minimizing the loss function that is a\ndiscrete version of the energy. In addition, we include the level set function\nof the interface as a feature input and find that it significantly improves the\ntraining efficiency and accuracy. We perform a series of numerical tests to\ndemonstrate the accuracy of the present network as well as its capability for\nproblems in irregular domains and in higher dimensions.",
          "link": "http://arxiv.org/abs/2107.12013",
          "publishedOn": "2021-07-27T02:03:35.948Z",
          "wordCount": 666,
          "title": "A Shallow Ritz Method for elliptic problems with Singular Sources. (arXiv:2107.12013v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mardia_J/0/1/0/all/0/1\">Jay Mardia</a>",
          "description": "The planted clique problem is well-studied in the context of observing,\nexplaining, and predicting interesting computational phenomena associated with\nstatistical problems. When equating computational efficiency with the existence\nof polynomial time algorithms, the computational hardness of (some variant of)\nthe planted clique problem can be used to infer the computational hardness of a\nhost of other statistical problems.\n\nIs this ability to transfer computational hardness from (some variant of) the\nplanted clique problem to other statistical problems robust to changing our\nnotion of computational efficiency to space efficiency?\n\nWe answer this question affirmatively for three different statistical\nproblems, namely Sparse PCA, submatrix detection, and testing almost k-wise\nindependence. The key challenge is that space efficient randomized reductions\nneed to repeatedly access the randomness they use. Known reductions to these\nproblems are all randomized and need polynomially many random bits to\nimplement. Since we can not store polynomially many random bits in memory, it\nis unclear how to implement these existing reductions space efficiently. There\nare two ideas involved in circumventing this issue and implementing known\nreductions to these problems space efficiently.\n\n1. When solving statistical problems, we can use parts of the input itself as\nrandomness.\n\n2. Secret leakage variants of the planted clique problem with appropriate\nsecret leakage can be more useful than the standard planted clique problem when\nwe want to use parts of the input as randomness.\n\n(abstract shortened due to arxiv constraints)",
          "link": "http://arxiv.org/abs/2107.11886",
          "publishedOn": "2021-07-27T02:03:35.926Z",
          "wordCount": 675,
          "title": "Logspace Reducibility From Secret Leakage Planted Clique. (arXiv:2107.11886v1 [cs.CC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12009",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cahan_N/0/1/0/all/0/1\">Noa Cahan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marom_E/0/1/0/all/0/1\">Edith M. Marom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soffer_S/0/1/0/all/0/1\">Shelly Soffer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barash_Y/0/1/0/all/0/1\">Yiftach Barash</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Konen_E/0/1/0/all/0/1\">Eli Konen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klang_E/0/1/0/all/0/1\">Eyal Klang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greenspan_H/0/1/0/all/0/1\">Hayit Greenspan</a>",
          "description": "Pulmonary embolus (PE) refers to obstruction of pulmonary arteries by blood\nclots. PE accounts for approximately 100,000 deaths per year in the United\nStates alone. The clinical presentation of PE is often nonspecific, making the\ndiagnosis challenging. Thus, rapid and accurate risk stratification is of\nparamount importance. High-risk PE is caused by right ventricular (RV)\ndysfunction from acute pressure overload, which in return can help identify\nwhich patients require more aggressive therapy. Reconstructed four-chamber\nviews of the heart on chest CT can detect right ventricular enlargement. CT\npulmonary angiography (CTPA) is the golden standard in the diagnostic workup of\nsuspected PE. Therefore, it can link between diagnosis and risk stratification\nstrategies. We developed a weakly supervised deep learning algorithm, with an\nemphasis on a novel attention mechanism, to automatically classify RV strain on\nCTPA. Our method is a 3D DenseNet model with integrated 3D residual attention\nblocks. We evaluated our model on a dataset of CTPAs of emergency department\n(ED) PE patients. This model achieved an area under the receiver operating\ncharacteristic curve (AUC) of 0.88 for classifying RV strain. The model showed\na sensitivity of 87% and specificity of 83.7%. Our solution outperforms\nstate-of-the-art 3D CNN networks. The proposed design allows for a fully\nautomated network that can be trained easily in an end-to-end manner without\nrequiring computationally intensive and time-consuming preprocessing or\nstrenuous labeling of the data.We infer that unmarked CTPAs can be used for\neffective RV strain classification. This could be used as a second reader,\nalerting for high-risk PE patients. To the best of our knowledge, there are no\nprevious deep learning-based studies that attempted to solve this problem.",
          "link": "http://arxiv.org/abs/2107.12009",
          "publishedOn": "2021-07-27T02:03:35.918Z",
          "wordCount": 751,
          "title": "Weakly Supervised Attention Model for RV StrainClassification from volumetric CTPA Scans. (arXiv:2107.12009v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11856",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Amor_A/0/1/0/all/0/1\">Amine Amor</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Lio&#x27;</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Singh_V/0/1/0/all/0/1\">Vikash Singh</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Torne_R/0/1/0/all/0/1\">Ramon Vi&#xf1;as Torn&#xe9;</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Terre_H/0/1/0/all/0/1\">Helena Andres Terre</a> (1)",
          "description": "Combining different modalities of data from human tissues has been critical\nin advancing biomedical research and personalised medical care. In this study,\nwe leverage a graph embedding model (i.e VGAE) to perform link prediction on\ntissue-specific Gene-Gene Interaction (GGI) networks. Through ablation\nexperiments, we prove that the combination of multiple biological modalities\n(i.e multi-omics) leads to powerful embeddings and better link prediction\nperformances. Our evaluation shows that the integration of gene methylation\nprofiles and RNA-sequencing data significantly improves the link prediction\nperformance. Overall, the combination of RNA-sequencing and gene methylation\ndata leads to a link prediction accuracy of 71% on GGI networks. By harnessing\ngraph representation learning on multi-omics data, our work brings novel\ninsights to the current literature on multi-omics integration in\nbioinformatics.",
          "link": "http://arxiv.org/abs/2107.11856",
          "publishedOn": "2021-07-27T02:03:35.911Z",
          "wordCount": 578,
          "title": "Graph Representation Learning on Tissue-Specific Multi-Omics. (arXiv:2107.11856v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">David D. Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agha_mohammadi_A/0/1/0/all/0/1\">Ali-akbar Agha-mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1\">Evangelos A. Theodorou</a>",
          "description": "One of the main challenges in autonomous robotic exploration and navigation\nin unknown and unstructured environments is determining where the robot can or\ncannot safely move. A significant source of difficulty in this determination\narises from stochasticity and uncertainty, coming from localization error,\nsensor sparsity and noise, difficult-to-model robot-ground interactions, and\ndisturbances to the motion of the vehicle. Classical approaches to this problem\nrely on geometric analysis of the surrounding terrain, which can be prone to\nmodeling errors and can be computationally expensive. Moreover, modeling the\ndistribution of uncertain traversability costs is a difficult task, compounded\nby the various error sources mentioned above. In this work, we take a\nprincipled learning approach to this problem. We introduce a neural network\narchitecture for robustly learning the distribution of traversability costs.\nBecause we are motivated by preserving the life of the robot, we tackle this\nlearning problem from the perspective of learning tail-risks, i.e. the\nConditional Value-at-Risk (CVaR). We show that this approach reliably learns\nthe expected tail risk given a desired probability risk threshold between 0 and\n1, producing a traversability costmap which is more robust to outliers, more\naccurately captures tail risks, and is more computationally efficient, when\ncompared against baselines. We validate our method on data collected a legged\nrobot navigating challenging, unstructured environments including an abandoned\nsubway, limestone caves, and lava tube caves.",
          "link": "http://arxiv.org/abs/2107.11722",
          "publishedOn": "2021-07-27T02:03:35.905Z",
          "wordCount": 662,
          "title": "Learning Risk-aware Costmaps for Traversability in Challenging Environments. (arXiv:2107.11722v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_F/0/1/0/all/0/1\">Frederik Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_C/0/1/0/all/0/1\">Christopher Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabe_M/0/1/0/all/0/1\">Markus N. Rabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finkbeiner_B/0/1/0/all/0/1\">Bernd Finkbeiner</a>",
          "description": "We train hierarchical Transformers on the task of synthesizing hardware\ncircuits directly out of high-level logical specifications in linear-time\ntemporal logic (LTL). The LTL synthesis problem is a well-known algorithmic\nchallenge with a long history and an annual competition is organized to track\nthe improvement of algorithms and tooling over time. New approaches using\nmachine learning might open a lot of possibilities in this area, but suffer\nfrom the lack of sufficient amounts of training data. In this paper, we\nconsider a method to generate large amounts of additional training data, i.e.,\npairs of specifications and circuits implementing them. We ensure that this\nsynthetic data is sufficiently close to human-written specifications by mining\ncommon patterns from the specifications used in the synthesis competitions. We\nshow that hierarchical Transformers trained on this synthetic data solve a\nsignificant portion of problems from the synthesis competitions, and even\nout-of-distribution examples from a recent case study.",
          "link": "http://arxiv.org/abs/2107.11864",
          "publishedOn": "2021-07-27T02:03:35.870Z",
          "wordCount": 585,
          "title": "Neural Circuit Synthesis from Specification Patterns. (arXiv:2107.11864v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toivonen_T/0/1/0/all/0/1\">Tapani Toivonen</a>",
          "description": "Many combinatorial optimization problems are often considered intractable to\nsolve exactly or by approximation. An example of such problem is maximum clique\nwhich -- under standard assumptions in complexity theory -- cannot be solved in\nsub-exponential time or be approximated within polynomial factor efficiently.\nWe show that if a polynomial time algorithm can query informative Gaussian\npriors from an expert $poly(n)$ times, then a class of combinatorial\noptimization problems can be solved efficiently in expectation up to a\nmultiplicative factor $\\epsilon$ where $\\epsilon$ is arbitrary constant. While\nour proposed methods are merely theoretical, they cast new light on how to\napproach solving these problems that have been usually considered intractable.",
          "link": "http://arxiv.org/abs/2107.11784",
          "publishedOn": "2021-07-27T02:03:35.863Z",
          "wordCount": 551,
          "title": "Power of human-algorithm collaboration in solving combinatorial optimization problems. (arXiv:2107.11784v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1\">Arnab Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gayen_S/0/1/0/all/0/1\">Sutanu Gayen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandasamy_S/0/1/0/all/0/1\">Saravanan Kandasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raval_V/0/1/0/all/0/1\">Vedant Raval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinodchandran_N/0/1/0/all/0/1\">N. V. Vinodchandran</a>",
          "description": "We consider the problem of efficiently inferring interventional distributions\nin a causal Bayesian network from a finite number of observations. Let\n$\\mathcal{P}$ be a causal model on a set $\\mathbf{V}$ of observable variables\non a given causal graph $G$. For sets $\\mathbf{X},\\mathbf{Y}\\subseteq\n\\mathbf{V}$, and setting ${\\bf x}$ to $\\mathbf{X}$, let $P_{\\bf x}(\\mathbf{Y})$\ndenote the interventional distribution on $\\mathbf{Y}$ with respect to an\nintervention ${\\bf x}$ to variables ${\\bf x}$. Shpitser and Pearl (AAAI 2006),\nbuilding on the work of Tian and Pearl (AAAI 2001), gave an exact\ncharacterization of the class of causal graphs for which the interventional\ndistribution $P_{\\bf x}({\\mathbf{Y}})$ can be uniquely determined. We give the\nfirst efficient version of the Shpitser-Pearl algorithm. In particular, under\nnatural assumptions, we give a polynomial-time algorithm that on input a causal\ngraph $G$ on observable variables $\\mathbf{V}$, a setting ${\\bf x}$ of a set\n$\\mathbf{X} \\subseteq \\mathbf{V}$ of bounded size, outputs succinct\ndescriptions of both an evaluator and a generator for a distribution $\\hat{P}$\nthat is $\\varepsilon$-close (in total variation distance) to $P_{\\bf\nx}({\\mathbf{Y}})$ where $Y=\\mathbf{V}\\setminus \\mathbf{X}$, if $P_{\\bf\nx}(\\mathbf{Y})$ is identifiable. We also show that when $\\mathbf{Y}$ is an\narbitrary set, there is no efficient algorithm that outputs an evaluator of a\ndistribution that is $\\varepsilon$-close to $P_{\\bf x}({\\mathbf{Y}})$ unless\nall problems that have statistical zero-knowledge proofs, including the Graph\nIsomorphism problem, have efficient randomized algorithms.",
          "link": "http://arxiv.org/abs/2107.11712",
          "publishedOn": "2021-07-27T02:03:35.856Z",
          "wordCount": 671,
          "title": "Efficient inference of interventional distributions. (arXiv:2107.11712v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Preethi_P/0/1/0/all/0/1\">P Preethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1\">Hrishikesh Viswanath</a>",
          "description": "This work presents a comparison of machine learning algorithms that are\nimplemented to segment the characters of text presented as an image. The\nalgorithms are designed to work on degraded documents with text that is not\naligned in an organized fashion. The paper investigates the use of Support\nVector Machines, K-Nearest Neighbor algorithm and an Encoder Network to perform\nthe operation of character spotting. Character Spotting involves extracting\npotential characters from a stream of text by selecting regions bound by white\nspace.",
          "link": "http://arxiv.org/abs/2107.11795",
          "publishedOn": "2021-07-27T02:03:35.850Z",
          "wordCount": 518,
          "title": "Character Spotting Using Machine Learning Techniques. (arXiv:2107.11795v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11911",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Frazier_P/0/1/0/all/0/1\">Peter I. Frazier</a>",
          "description": "We consider finite-horizon restless bandits with multiple pulls per period,\nwhich play an important role in recommender systems, active learning, revenue\nmanagement, and many other areas. While an optimal policy can be computed, in\nprinciple, using dynamic programming, the computation required scales\nexponentially in the number of arms $N$. Thus, there is substantial value in\nunderstanding the performance of index policies and other policies that can be\ncomputed efficiently for large $N$. We study the growth of the optimality gap,\ni.e., the loss in expected performance compared to an optimal policy, for such\npolicies in a classical asymptotic regime proposed by Whittle in which $N$\ngrows while holding constant the fraction of arms that can be pulled per\nperiod. Intuition from the Central Limit Theorem and the tightest previous\ntheoretical bounds suggest that this optimality gap should grow like\n$O(\\sqrt{N})$. Surprisingly, we show that it is possible to outperform this\nbound. We characterize a non-degeneracy condition and a wide class of novel\npractically-computable policies, called fluid-priority policies, in which the\noptimality gap is $O(1)$. These include most widely-used index policies. When\nthis non-degeneracy condition does not hold, we show that fluid-priority\npolicies nevertheless have an optimality gap that is $O(\\sqrt{N})$,\nsignificantly generalizing the class of policies for which convergence rates\nare known. We demonstrate that fluid-priority policies offer state-of-the-art\nperformance on a collection of restless bandit problems in numerical\nexperiments.",
          "link": "http://arxiv.org/abs/2107.11911",
          "publishedOn": "2021-07-27T02:03:35.843Z",
          "wordCount": 669,
          "title": "Restless Bandits with Many Arms: Beating the Central Limit Theorem. (arXiv:2107.11911v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ogishima_R/0/1/0/all/0/1\">Ryoya Ogishima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karino_I/0/1/0/all/0/1\">Izumi Karino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuniyoshi_Y/0/1/0/all/0/1\">Yasuo Kuniyoshi</a>",
          "description": "Reinforcement Learning (RL) requires a large amount of exploration especially\nin sparse-reward settings. Imitation Learning (IL) can learn from expert\ndemonstrations without exploration, but it never exceeds the expert's\nperformance and is also vulnerable to distributional shift between\ndemonstration and execution. In this paper, we radically unify RL and IL based\non Free Energy Principle (FEP). FEP is a unified Bayesian theory of the brain\nthat explains perception, action and model learning by a common fundamental\nprinciple. We present a theoretical extension of FEP and derive an algorithm in\nwhich an agent learns the world model that internalizes expert demonstrations\nand at the same time uses the model to infer the current and future states and\nactions that maximize rewards. The algorithm thus reduces exploration costs by\npartially imitating experts as well as maximizing its return in a seamless way,\nresulting in a higher performance than the suboptimal expert. Our experimental\nresults show that this approach is promising in visual control tasks especially\nin sparse-reward environments.",
          "link": "http://arxiv.org/abs/2107.11811",
          "publishedOn": "2021-07-27T02:03:35.815Z",
          "wordCount": 595,
          "title": "Reinforced Imitation Learning by Free Energy Principle. (arXiv:2107.11811v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11882",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1\">Riqiang Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1\">Kaiwen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Ho Hin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deppen_S/0/1/0/all/0/1\">Steve Deppen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sandler_K/0/1/0/all/0/1\">Kim Sandler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Massion_P/0/1/0/all/0/1\">Pierre Massion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lasko_T/0/1/0/all/0/1\">Thomas A. Lasko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>",
          "description": "Data from multi-modality provide complementary information in clinical\nprediction, but missing data in clinical cohorts limits the number of subjects\nin multi-modal learning context. Multi-modal missing imputation is challenging\nwith existing methods when 1) the missing data span across heterogeneous\nmodalities (e.g., image vs. non-image); or 2) one modality is largely missing.\nIn this paper, we address imputation of missing data by modeling the joint\ndistribution of multi-modal data. Motivated by partial bidirectional generative\nadversarial net (PBiGAN), we propose a new Conditional PBiGAN (C-PBiGAN) method\nthat imputes one modality combining the conditional knowledge from another\nmodality. Specifically, C-PBiGAN introduces a conditional latent space in a\nmissing imputation framework that jointly encodes the available multi-modal\ndata, along with a class regularization loss on imputed data to recover\ndiscriminative information. To our knowledge, it is the first generative\nadversarial model that addresses multi-modal missing imputation by modeling the\njoint distribution of image and non-image data. We validate our model with both\nthe national lung screening trial (NLST) dataset and an external clinical\nvalidation cohort. The proposed C-PBiGAN achieves significant improvements in\nlung cancer risk estimation compared with representative imputation methods\n(e.g., AUC values increase in both NLST (+2.9\\%) and in-house dataset (+4.3\\%)\ncompared with PBiGAN, p$<$0.05).",
          "link": "http://arxiv.org/abs/2107.11882",
          "publishedOn": "2021-07-27T02:03:35.807Z",
          "wordCount": 683,
          "title": "Lung Cancer Risk Estimation with Incomplete Data: A Joint Missing Imputation Perspective. (arXiv:2107.11882v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mengwu Guo</a>",
          "description": "As a generalization of the work in [Lee et al., 2017], this note briefly\ndiscusses when the prior of a neural network output follows a Gaussian process,\nand how a neural-network-induced Gaussian process is formulated. The posterior\nmean functions of such a Gaussian process regression lie in the reproducing\nkernel Hilbert space defined by the neural-network-induced kernel. In the case\nof two-layer neural networks, the induced Gaussian processes provide an\ninterpretation of the reproducing kernel Hilbert spaces whose union forms a\nBarron space.",
          "link": "http://arxiv.org/abs/2107.11892",
          "publishedOn": "2021-07-27T02:03:35.785Z",
          "wordCount": 523,
          "title": "A brief note on understanding neural networks as Gaussian processes. (arXiv:2107.11892v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asperti_A/0/1/0/all/0/1\">Andrea Asperti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evangelista_D/0/1/0/all/0/1\">Davide Evangelista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzolla_M/0/1/0/all/0/1\">Moreno Marzolla</a>",
          "description": "The term GreenAI refers to a novel approach to Deep Learning, that is more\naware of the ecological impact and the computational efficiency of its methods.\nThe promoters of GreenAI suggested the use of Floating Point Operations (FLOPs)\nas a measure of the computational cost of Neural Networks; however, that\nmeasure does not correlate well with the energy consumption of hardware\nequipped with massively parallel processing units like GPUs or TPUs. In this\narticle, we propose a simple refinement of the formula used to compute floating\npoint operations for convolutional layers, called {\\alpha}-FLOPs, explaining\nand correcting the traditional discrepancy with respect to different layers,\nand closer to reality. The notion of {\\alpha}-FLOPs relies on the crucial\ninsight that, in case of inputs with multiple dimensions, there is no reason to\nbelieve that the speedup offered by parallelism will be uniform along all\ndifferent axes.",
          "link": "http://arxiv.org/abs/2107.11949",
          "publishedOn": "2021-07-27T02:03:35.778Z",
          "wordCount": 602,
          "title": "Dissecting FLOPs along input dimensions for GreenAI cost estimations. (arXiv:2107.11949v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yeli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1\">Daniel Jun Xian Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Easwaran_A/0/1/0/all/0/1\">Arvind Easwaran</a>",
          "description": "Uncertainties in machine learning are a significant roadblock for its\napplication in safety-critical cyber-physical systems (CPS). One source of\nuncertainty arises from distribution shifts in the input data between training\nand test scenarios. Detecting such distribution shifts in real-time is an\nemerging approach to address the challenge. The high dimensional input space in\nCPS applications involving imaging adds extra difficulty to the task.\nGenerative learning models are widely adopted for the task, namely\nout-of-distribution (OoD) detection. To improve the state-of-the-art, we\nstudied existing proposals from both machine learning and CPS fields. In the\nlatter, safety monitoring in real-time for autonomous driving agents has been a\nfocus. Exploiting the spatiotemporal correlation of motion in videos, we can\nrobustly detect hazardous motion around autonomous driving agents. Inspired by\nthe latest advances in the Variational Autoencoder (VAE) theory and practice,\nwe tapped into the prior knowledge in data to further boost OoD detection's\nrobustness. Comparison studies over nuScenes and Synthia data sets show our\nmethods significantly improve detection capabilities of OoD factors unique to\ndriving scenarios, 42% better than state-of-the-art approaches. Our model also\ngeneralized near-perfectly, 97% better than the state-of-the-art across the\nreal-world and simulation driving data sets experimented. Finally, we\ncustomized one proposed method into a twin-encoder model that can be deployed\nto resource limited embedded devices for real-time OoD detection. Its execution\ntime was reduced over four times in low-precision 8-bit integer inference,\nwhile detection capability is comparable to its corresponding floating-point\nmodel.",
          "link": "http://arxiv.org/abs/2107.11750",
          "publishedOn": "2021-07-27T02:03:35.751Z",
          "wordCount": 686,
          "title": "Improving Variational Autoencoder based Out-of-Distribution Detection for Embedded Real-time Applications. (arXiv:2107.11750v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11843",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Drgona_J/0/1/0/all/0/1\">Jan Drgona</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tuor_A/0/1/0/all/0/1\">Aaron Tuor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vasisht_S/0/1/0/all/0/1\">Soumya Vasisht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Skomski_E/0/1/0/all/0/1\">Elliott Skomski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vrabie_D/0/1/0/all/0/1\">Draguna Vrabie</a>",
          "description": "We present a differentiable predictive control (DPC) methodology for learning\nconstrained control laws for unknown nonlinear systems. DPC poses an\napproximate solution to multiparametric programming problems emerging from\nexplicit nonlinear model predictive control (MPC). Contrary to approximate MPC,\nDPC does not require supervision by an expert controller. Instead, a system\ndynamics model is learned from the observed system's dynamics, and the neural\ncontrol law is optimized offline by leveraging the differentiable closed-loop\nsystem model. The combination of a differentiable closed-loop system and\npenalty methods for constraint handling of system outputs and inputs allows us\nto optimize the control law's parameters directly by backpropagating economic\nMPC loss through the learned system model. The control performance of the\nproposed DPC method is demonstrated in simulation using learned model of\nmulti-zone building thermal dynamics.",
          "link": "http://arxiv.org/abs/2107.11843",
          "publishedOn": "2021-07-27T02:03:35.744Z",
          "wordCount": 582,
          "title": "Deep Learning Explicit Differentiable Predictive Control Laws for Buildings. (arXiv:2107.11843v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Ping-Chia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "Despite the success of deep learning on supervised point cloud semantic\nsegmentation, obtaining large-scale point-by-point manual annotations is still\na significant challenge. To reduce the huge annotation burden, we propose a\nRegion-based and Diversity-aware Active Learning (ReDAL), a general framework\nfor many deep learning approaches, aiming to automatically select only\ninformative and diverse sub-scene regions for label acquisition. Observing that\nonly a small portion of annotated regions are sufficient for 3D scene\nunderstanding with deep learning, we use softmax entropy, color discontinuity,\nand structural complexity to measure the information of sub-scene regions. A\ndiversity-aware selection algorithm is also developed to avoid redundant\nannotations resulting from selecting informative but similar regions in a\nquerying batch. Extensive experiments show that our method highly outperforms\nprevious active learning strategies, and we achieve the performance of 90%\nfully supervised learning, while less than 15% and 5% annotations are required\non S3DIS and SemanticKITTI datasets, respectively.",
          "link": "http://arxiv.org/abs/2107.11769",
          "publishedOn": "2021-07-27T02:03:35.737Z",
          "wordCount": 608,
          "title": "ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avelar_P/0/1/0/all/0/1\">Pedro H.C. Avelar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audibert_R/0/1/0/all/0/1\">Rafael B. Audibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavares_A/0/1/0/all/0/1\">Anderson R. Tavares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamb_L/0/1/0/all/0/1\">Lu&#xed;s C. Lamb</a>",
          "description": "Recently, the use of sound measures and metrics in Artificial Intelligence\nhas become the subject of interest of academia, government, and industry.\nEfforts towards measuring different phenomena have gained traction in the AI\ncommunity, as illustrated by the publication of several influential field\nreports and policy documents. These metrics are designed to help decision\ntakers to inform themselves about the fast-moving and impacting influences of\nkey advances in Artificial Intelligence in general and Machine Learning in\nparticular. In this paper we propose to use such newfound capabilities of AI\ntechnologies to augment our AI measuring capabilities. We do so by training a\nmodel to classify publications related to ethical issues and concerns. In our\nmethodology we use an expert, manually curated dataset as the training set and\nthen evaluate a large set of research papers. Finally, we highlight the\nimplications of AI metrics, in particular their contribution towards developing\ntrustful and fair AI-based tools and technologies. Keywords: AI Ethics; AI\nFairness; AI Measurement. Ethics in Computer Science.",
          "link": "http://arxiv.org/abs/2107.11913",
          "publishedOn": "2021-07-27T02:03:35.731Z",
          "wordCount": 619,
          "title": "Measuring Ethics in AI with AI: A Methodology and Dataset Construction. (arXiv:2107.11913v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1\">Nasibullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1\">Partha Pratim Mohanta</a>",
          "description": "Video captioning is one of the challenging problems at the intersection of\nvision and language, having many real-life applications in video retrieval,\nvideo surveillance, assisting visually challenged people, Human-machine\ninterface, and many more. Recent deep learning-based methods have shown\npromising results but are still on the lower side than other vision tasks (such\nas image classification, object detection). A significant drawback with\nexisting video captioning methods is that they are optimized over cross-entropy\nloss function, which is uncorrelated to the de facto evaluation metrics (BLEU,\nMETEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate\nof the true loss function for video captioning. This paper addresses the\ndrawback by introducing a dynamic loss network (DLN), which provides an\nadditional feedback signal that directly reflects the evaluation metrics. Our\nresults on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to\nText (MSRVTT) datasets outperform previous methods.",
          "link": "http://arxiv.org/abs/2107.11707",
          "publishedOn": "2021-07-27T02:03:35.716Z",
          "wordCount": 585,
          "title": "Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Ziji Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>",
          "description": "The transformer has recently achieved impressive results on various tasks. To\nfurther improve the effectiveness and efficiency of the transformer, there are\ntwo trains of thought among existing works: (1) going wider by scaling to more\ntrainable parameters; (2) going shallower by parameter sharing or model\ncompressing along with the depth. However, larger models usually do not scale\nwell when fewer tokens are available to train, and advanced parallelisms are\nrequired when the model is extremely large. Smaller models usually achieve\ninferior performance compared to the original transformer model due to the loss\nof representation power. In this paper, to achieve better performance with\nfewer trainable parameters, we propose a framework to deploy trainable\nparameters efficiently, by going wider instead of deeper. Specially, we scale\nalong model width by replacing feed-forward network (FFN) with\nmixture-of-experts (MoE). We then share the MoE layers across transformer\nblocks using individual layer normalization. Such deployment plays the role to\ntransform various semantic representations, which makes the model more\nparameter-efficient and effective. To evaluate our framework, we design WideNet\nand evaluate it on ImageNet-1K. Our best model outperforms Vision Transformer\n(ViT) by $1.46\\%$ with $0.72 \\times$ trainable parameters. Using $0.46 \\times$\nand $0.13 \\times$ parameters, our WideNet can still surpass ViT and ViT-MoE by\n$0.83\\%$ and $2.08\\%$, respectively.",
          "link": "http://arxiv.org/abs/2107.11817",
          "publishedOn": "2021-07-27T02:03:35.681Z",
          "wordCount": 650,
          "title": "Go Wider Instead of Deeper. (arXiv:2107.11817v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sleeman_W/0/1/0/all/0/1\">William C. Sleeman IV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krawczyk_B/0/1/0/all/0/1\">Bartosz Krawczyk</a>",
          "description": "Learning from imbalanced data is among the most challenging areas in\ncontemporary machine learning. This becomes even more difficult when considered\nthe context of big data that calls for dedicated architectures capable of\nhigh-performance processing. Apache Spark is a highly efficient and popular\narchitecture, but it poses specific challenges for algorithms to be implemented\nfor it. While oversampling algorithms are an effective way for handling class\nimbalance, they have not been designed for distributed environments. In this\npaper, we propose a holistic look on oversampling algorithms for imbalanced big\ndata. We discuss the taxonomy of oversampling algorithms and their mechanisms\nused to handle skewed class distributions. We introduce a Spark library with 14\nstate-of-the-art oversampling algorithms implemented and evaluate their\nefficacy via extensive experimental study. Using binary and multi-class massive\ndata sets, we analyze the effectiveness of oversampling algorithms and their\nrelationships with different types of classifiers. We evaluate the trade-off\nbetween accuracy and time complexity of oversampling algorithms, as well as\ntheir scalability when increasing the size of data. This allows us to gain\ninsight into the usefulness of specific components of oversampling algorithms\nfor big data, as well as formulate guidelines and recommendations for designing\nfuture resampling approaches for massive imbalanced data. Our library can be\ndownloaded from https://github.com/fsleeman/spark-class-balancing.git.",
          "link": "http://arxiv.org/abs/2107.11508",
          "publishedOn": "2021-07-27T02:03:35.666Z",
          "wordCount": 665,
          "title": "Imbalanced Big Data Oversampling: Taxonomy, Algorithms, Software, Guidelines and Future Directions. (arXiv:2107.11508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_A/0/1/0/all/0/1\">Ali Rahmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1\">Seyed-Mohsen Moosavi-Dezfooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Huaiyu Dai</a>",
          "description": "Adversarial training has been shown as an effective approach to improve the\nrobustness of image classifiers against white-box attacks. However, its\neffectiveness against black-box attacks is more nuanced. In this work, we\ndemonstrate that some geometric consequences of adversarial training on the\ndecision boundary of deep networks give an edge to certain types of black-box\nattacks. In particular, we define a metric called robustness gain to show that\nwhile adversarial training is an effective method to dramatically improve the\nrobustness in white-box scenarios, it may not provide such a good robustness\ngain against the more realistic decision-based black-box attacks. Moreover, we\nshow that even the minimal perturbation white-box attacks can converge faster\nagainst adversarially-trained neural networks compared to the regular ones.",
          "link": "http://arxiv.org/abs/2107.11671",
          "publishedOn": "2021-07-27T02:03:35.576Z",
          "wordCount": 569,
          "title": "Adversarial training may be a double-edged sword. (arXiv:2107.11671v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11533",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tran_The_H/0/1/0/all/0/1\">Hung Tran-The</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_Tang_T/0/1/0/all/0/1\">Thanh Nguyen-Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rana_S/0/1/0/all/0/1\">Santu Rana</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "We address policy learning with logged data in contextual bandits. Current\noffline-policy learning algorithms are mostly based on inverse propensity score\n(IPS) weighting requiring the logging policy to have \\emph{full support} i.e. a\nnon-zero probability for any context/action of the evaluation policy. However,\nmany real-world systems do not guarantee such logging policies, especially when\nthe action space is large and many actions have poor or missing rewards. With\nsuch \\emph{support deficiency}, the offline learning fails to find optimal\npolicies. We propose a novel approach that uses a hybrid of offline learning\nwith online exploration. The online exploration is used to explore unsupported\nactions in the logged data whilst offline learning is used to exploit supported\nactions from the logged data avoiding unnecessary explorations. Our approach\ndetermines an optimal policy with theoretical guarantees using the minimal\nnumber of online explorations. We demonstrate our algorithms' effectiveness\nempirically on a diverse collection of datasets.",
          "link": "http://arxiv.org/abs/2107.11533",
          "publishedOn": "2021-07-27T02:03:35.542Z",
          "wordCount": 594,
          "title": "Combining Online Learning and Offline Learning for Contextual Bandits with Deficient Support. (arXiv:2107.11533v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bose_R/0/1/0/all/0/1\">Rupak Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pande_S/0/1/0/all/0/1\">Shivam Pande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>",
          "description": "As the field of remote sensing is evolving, we witness the accumulation of\ninformation from several modalities, such as multispectral (MS), hyperspectral\n(HSI), LiDAR etc. Each of these modalities possess its own distinct\ncharacteristics and when combined synergistically, perform very well in the\nrecognition and classification tasks. However, fusing multiple modalities in\nremote sensing is cumbersome due to highly disparate domains. Furthermore, the\nexisting methods do not facilitate cross-modal interactions. To this end, we\npropose a novel transformer based fusion method for HSI and LiDAR modalities.\nThe model is composed of stacked auto encoders that harness the cross key-value\npairs for HSI and LiDAR, thus establishing a communication between the two\nmodalities, while simultaneously using the CNNs to extract the spectral and\nspatial information from HSI and LiDAR. We test our model on Houston (Data\nFusion Contest - 2013) and MUUFL Gulfport datasets and achieve competitive\nresults.",
          "link": "http://arxiv.org/abs/2107.11585",
          "publishedOn": "2021-07-27T02:03:35.534Z",
          "wordCount": 605,
          "title": "Two Headed Dragons: Multimodal Fusion and Cross Modal Transactions. (arXiv:2107.11585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Lin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>",
          "description": "Smart contract vulnerability detection draws extensive attention in recent\nyears due to the substantial losses caused by hacker attacks. Existing efforts\nfor contract security analysis heavily rely on rigid rules defined by experts,\nwhich are labor-intensive and non-scalable. More importantly, expert-defined\nrules tend to be error-prone and suffer the inherent risk of being cheated by\ncrafty attackers. Recent researches focus on the symbolic execution and formal\nanalysis of smart contracts for vulnerability detection, yet to achieve a\nprecise and scalable solution. Although several methods have been proposed to\ndetect vulnerabilities in smart contracts, there is still a lack of effort that\nconsiders combining expert-defined security patterns with deep neural networks.\nIn this paper, we explore using graph neural networks and expert knowledge for\nsmart contract vulnerability detection. Specifically, we cast the rich control-\nand data- flow semantics of the source code into a contract graph. To highlight\nthe critical nodes in the graph, we further design a node elimination phase to\nnormalize the graph. Then, we propose a novel temporal message propagation\nnetwork to extract the graph feature from the normalized graph, and combine the\ngraph feature with designed expert patterns to yield a final detection system.\nExtensive experiments are conducted on all the smart contracts that have source\ncode in Ethereum and VNT Chain platforms. Empirical results show significant\naccuracy improvements over the state-of-the-art methods on three types of\nvulnerabilities, where the detection accuracy of our method reaches 89.15%,\n89.02%, and 83.21% for reentrancy, timestamp dependence, and infinite loop\nvulnerabilities, respectively.",
          "link": "http://arxiv.org/abs/2107.11598",
          "publishedOn": "2021-07-27T02:03:35.527Z",
          "wordCount": 715,
          "title": "Combining Graph Neural Networks with Expert Knowledge for Smart Contract Vulnerability Detection. (arXiv:2107.11598v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leino_K/0/1/0/all/0/1\">Klas Leino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fromherz_A/0/1/0/all/0/1\">Aymeric Fromherz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangal_R/0/1/0/all/0/1\">Ravi Mangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1\">Matt Fredrikson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parno_B/0/1/0/all/0/1\">Bryan Parno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasareanu_C/0/1/0/all/0/1\">Corina P&#x103;s&#x103;reanu</a>",
          "description": "Neural networks are increasingly being deployed in contexts where safety is a\ncritical concern. In this work, we propose a way to construct neural network\nclassifiers that dynamically repair violations of non-relational safety\nconstraints called safe ordering properties. Safe ordering properties relate\nrequirements on the ordering of a network's output indices to conditions on\ntheir input, and are sufficient to express most useful notions of\nnon-relational safety for classifiers. Our approach is based on a novel\nself-repairing layer, which provably yields safe outputs regardless of the\ncharacteristics of its input. We compose this layer with an existing network to\nconstruct a self-repairing network (SR-Net), and show that in addition to\nproviding safe outputs, the SR-Net is guaranteed to preserve the accuracy of\nthe original network. Notably, our approach is independent of the size and\narchitecture of the network being repaired, depending only on the specified\nproperty and the dimension of the network's output; thus it is scalable to\nlarge state-of-the-art networks. We show that our approach can be implemented\nusing vectorized computations that execute efficiently on a GPU, introducing\nrun-time overhead of less than one millisecond on current hardware -- even on\nlarge, widely-used networks containing hundreds of thousands of neurons and\nmillions of parameters.",
          "link": "http://arxiv.org/abs/2107.11445",
          "publishedOn": "2021-07-27T02:03:35.520Z",
          "wordCount": 651,
          "title": "Self-Repairing Neural Networks: Provable Safety for Deep Networks via Dynamic Repair. (arXiv:2107.11445v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Siqi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangjing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Federated learning (FL) is a solution for privacy challenge, which allows\nmultiparty to train a shared model without violating privacy protection\nregulations. Many excellent works of FL have been proposed in recent years. To\nhelp researchers verify their ideas in FL, we designed and developed FedLab, a\nflexible and modular FL framework based on PyTorch. In this paper, we will\nintroduce architecture and features of FedLab. For current popular research\npoints: optimization and communication compression, FedLab provides functional\ninterfaces and a series of baseline implementation are available, making\nresearchers quickly implement ideas. In addition, FedLab is scale-able in both\nclient simulation and distributed communication.",
          "link": "http://arxiv.org/abs/2107.11621",
          "publishedOn": "2021-07-27T02:03:35.485Z",
          "wordCount": 533,
          "title": "FedLab: A Flexible Federated Learning Framework. (arXiv:2107.11621v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kegl_B/0/1/0/all/0/1\">Bal&#xe1;zs K&#xe9;gl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_G/0/1/0/all/0/1\">Gabriel Hurtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1\">Albert Thomas</a>",
          "description": "We contribute to micro-data model-based reinforcement learning (MBRL) by\nrigorously comparing popular generative models using a fixed (random shooting)\ncontrol agent. We find that on an environment that requires multimodal\nposterior predictives, mixture density nets outperform all other models by a\nlarge margin. When multimodality is not required, our surprising finding is\nthat we do not need probabilistic posterior predictives: deterministic models\nare on par, in fact they consistently (although non-significantly) outperform\ntheir probabilistic counterparts. We also found that heteroscedasticity at\ntraining time, perhaps acting as a regularizer, improves predictions at longer\nhorizons. At the methodological side, we design metrics and an experimental\nprotocol which can be used to evaluate the various models, predicting their\nasymptotic performance when using them on the control problem. Using this\nframework, we improve the state-of-the-art sample complexity of MBRL on Acrobot\nby two to four folds, using an aggressive training schedule which is outside of\nthe hyperparameter interval usually considered",
          "link": "http://arxiv.org/abs/2107.11587",
          "publishedOn": "2021-07-27T02:03:35.479Z",
          "wordCount": 609,
          "title": "Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?. (arXiv:2107.11587v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lindt_A/0/1/0/all/0/1\">Alexandra Lindt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1\">Emiel Hoogeboom</a>",
          "description": "Discrete flow-based models are a recently proposed class of generative models\nthat learn invertible transformations for discrete random variables. Since they\ndo not require data dequantization and maximize an exact likelihood objective,\nthey can be used in a straight-forward manner for lossless compression. In this\npaper, we introduce a new discrete flow-based model for categorical random\nvariables: Discrete Denoising Flows (DDFs). In contrast with other discrete\nflow-based models, our model can be locally trained without introducing\ngradient bias. We show that DDFs outperform Discrete Flows on modeling a toy\nexample, binary MNIST and Cityscapes segmentation maps, measured in\nlog-likelihood.",
          "link": "http://arxiv.org/abs/2107.11625",
          "publishedOn": "2021-07-27T02:03:35.472Z",
          "wordCount": 533,
          "title": "Discrete Denoising Flows. (arXiv:2107.11625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Susu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berges_M/0/1/0/all/0/1\">Mario Berg&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_H/0/1/0/all/0/1\">Hae Young Noh</a>",
          "description": "Monitoring bridge health using vibrations of drive-by vehicles has various\nbenefits, such as no need for directly installing and maintaining sensors on\nthe bridge. However, many of the existing drive-by monitoring approaches are\nbased on supervised learning models that require labeled data from every bridge\nof interest, which is expensive and time-consuming, if not impossible, to\nobtain. To this end, we introduce a new framework that transfers the model\nlearned from one bridge to diagnose damage in another bridge without any labels\nfrom the target bridge. Our framework trains a hierarchical neural network\nmodel in an adversarial way to extract task-shared and task-specific features\nthat are informative to multiple diagnostic tasks and invariant across multiple\nbridges. We evaluate our framework on experimental data collected from 2\nbridges and 3 vehicles. We achieve accuracies of 95% for damage detection, 93%\nfor localization, and up to 72% for quantification, which are ~2 times\nimprovements from baseline methods.",
          "link": "http://arxiv.org/abs/2107.11435",
          "publishedOn": "2021-07-27T02:03:35.466Z",
          "wordCount": 597,
          "title": "HierMUD: Hierarchical Multi-task Unsupervised Domain Adaptation between Bridges for Drive-by Damage Diagnosis. (arXiv:2107.11435v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>",
          "description": "Non-Euclidean geometry with constant negative curvature, i.e., hyperbolic\nspace, has attracted sustained attention in the community of machine learning.\nHyperbolic space, owing to its ability to embed hierarchical structures\ncontinuously with low distortion, has been applied for learning data with\ntree-like structures. Hyperbolic Neural Networks (HNNs) that operate directly\nin hyperbolic space have also been proposed recently to further exploit the\npotential of hyperbolic representations. While HNNs have achieved better\nperformance than Euclidean neural networks (ENNs) on datasets with implicit\nhierarchical structure, they still perform poorly on standard classification\nbenchmarks such as CIFAR and ImageNet. The traditional wisdom is that it is\ncritical for the data to respect the hyperbolic geometry when applying HNNs. In\nthis paper, we first conduct an empirical study showing that the inferior\nperformance of HNNs on standard recognition datasets can be attributed to the\nnotorious vanishing gradient problem. We further discovered that this problem\nstems from the hybrid architecture of HNNs. Our analysis leads to a simple yet\neffective solution called Feature Clipping, which regularizes the hyperbolic\nembedding whenever its norm exceeding a given threshold. Our thorough\nexperiments show that the proposed method can successfully avoid the vanishing\ngradient problem when training HNNs with backpropagation. The improved HNNs are\nable to achieve comparable performance with ENNs on standard image recognition\ndatasets including MNIST, CIFAR10, CIFAR100 and ImageNet, while demonstrating\nmore adversarial robustness and stronger out-of-distribution detection\ncapability.",
          "link": "http://arxiv.org/abs/2107.11472",
          "publishedOn": "2021-07-27T02:03:35.460Z",
          "wordCount": 670,
          "title": "Free Hyperbolic Neural Networks with Limited Radii. (arXiv:2107.11472v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1\">Lucas Liebenwein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maalouf_A/0/1/0/all/0/1\">Alaa Maalouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_O/0/1/0/all/0/1\">Oren Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1\">Dan Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "We present a novel global compression framework for deep neural networks that\nautomatically analyzes each layer to identify the optimal per-layer compression\nratio, while simultaneously achieving the desired overall compression. Our\nalgorithm hinges on the idea of compressing each convolutional (or\nfully-connected) layer by slicing its channels into multiple groups and\ndecomposing each group via low-rank decomposition. At the core of our algorithm\nis the derivation of layer-wise error bounds from the Eckart Young Mirsky\ntheorem. We then leverage these bounds to frame the compression problem as an\noptimization problem where we wish to minimize the maximum compression error\nacross layers and propose an efficient algorithm towards a solution. Our\nexperiments indicate that our method outperforms existing low-rank compression\napproaches across a wide range of networks and data sets. We believe that our\nresults open up new avenues for future research into the global\nperformance-size trade-offs of modern neural networks. Our code is available at\nhttps://github.com/lucaslie/torchprune.",
          "link": "http://arxiv.org/abs/2107.11442",
          "publishedOn": "2021-07-27T02:03:35.449Z",
          "wordCount": 602,
          "title": "Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition. (arXiv:2107.11442v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>",
          "description": "Making classifiers robust to adversarial examples is hard. Thus, many\ndefenses tackle the seemingly easier task of detecting perturbed inputs. We\nshow a barrier towards this goal. We prove a general hardness reduction between\ndetection and classification of adversarial examples: given a robust detector\nfor attacks at distance {\\epsilon} (in some metric), we can build a similarly\nrobust (but inefficient) classifier for attacks at distance {\\epsilon}/2. Our\nreduction is computationally inefficient, and thus cannot be used to build\npractical classifiers. Instead, it is a useful sanity check to test whether\nempirical detection results imply something much stronger than the authors\npresumably anticipated. To illustrate, we revisit 13 detector defenses. For\n11/13 cases, we show that the claimed detection results would imply an\ninefficient classifier with robustness far beyond the state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.11630",
          "publishedOn": "2021-07-27T02:03:35.415Z",
          "wordCount": 581,
          "title": "Detecting Adversarial Examples Is (Nearly) As Hard As Classifying Them. (arXiv:2107.11630v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moubayed_A/0/1/0/all/0/1\">Abdallah Moubayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shami_A/0/1/0/all/0/1\">Abdallah Shami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidari_P/0/1/0/all/0/1\">Parisa Heidari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhtouta_A/0/1/0/all/0/1\">Amine Boukhtouta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larabi_A/0/1/0/all/0/1\">Adel Larabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunner_R/0/1/0/all/0/1\">Richard Brunner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preda_S/0/1/0/all/0/1\">Stere Preda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migault_D/0/1/0/all/0/1\">Daniel Migault</a>",
          "description": "Content delivery networks (CDNs) provide efficient content distribution over\nthe Internet. CDNs improve the connectivity and efficiency of global\ncommunications, but their caching mechanisms may be breached by\ncyber-attackers. Among the security mechanisms, effective anomaly detection\nforms an important part of CDN security enhancement. In this work, we propose a\nmulti-perspective unsupervised learning framework for anomaly detection in\nCDNs. In the proposed framework, a multi-perspective feature engineering\napproach, an optimized unsupervised anomaly detection model that utilizes an\nisolation forest and a Gaussian mixture model, and a multi-perspective\nvalidation method, are developed to detect abnormal behaviors in CDNs mainly\nfrom the client Internet Protocol (IP) and node perspectives, therefore to\nidentify the denial of service (DoS) and cache pollution attack (CPA) patterns.\nExperimental results are presented based on the analytics of eight days of\nreal-world CDN log data provided by a major CDN operator. Through experiments,\nthe abnormal contents, compromised nodes, malicious IPs, as well as their\ncorresponding attack types, are identified effectively by the proposed\nframework and validated by multiple cybersecurity experts. This shows the\neffectiveness of the proposed method when applied to real-world CDN data.",
          "link": "http://arxiv.org/abs/2107.11514",
          "publishedOn": "2021-07-27T02:03:35.407Z",
          "wordCount": 670,
          "title": "Multi-Perspective Content Delivery Networks Security Framework Using Optimized Unsupervised Anomaly Detection. (arXiv:2107.11514v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadigurschi_M/0/1/0/all/0/1\">Menachem Sadigurschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1\">Uri Stemmer</a>",
          "description": "We revisit the fundamental problem of learning Axis-Aligned-Rectangles over a\nfinite grid $X^d\\subseteq{\\mathbb{R}}^d$ with differential privacy. Existing\nresults show that the sample complexity of this problem is at most $\\min\\left\\{\nd{\\cdot}\\log|X| \\;,\\; d^{1.5}{\\cdot}\\left(\\log^*|X| \\right)^{1.5}\\right\\}$.\nThat is, existing constructions either require sample complexity that grows\nlinearly with $\\log|X|$, or else it grows super linearly with the dimension\n$d$. We present a novel algorithm that reduces the sample complexity to only\n$\\tilde{O}\\left\\{d{\\cdot}\\left(\\log^*|X|\\right)^{1.5}\\right\\}$, attaining a\ndimensionality optimal dependency without requiring the sample complexity to\ngrow with $\\log|X|$.The technique used in order to attain this improvement\ninvolves the deletion of \"exposed\" data-points on the go, in a fashion designed\nto avoid the cost of the adaptive composition theorems. The core of this\ntechnique may be of individual interest, introducing a new method for\nconstructing statistically-efficient private algorithms.",
          "link": "http://arxiv.org/abs/2107.11526",
          "publishedOn": "2021-07-27T02:03:35.400Z",
          "wordCount": 573,
          "title": "On the Sample Complexity of Privately Learning Axis-Aligned Rectangles. (arXiv:2107.11526v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1\">Biswadeep Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_S/0/1/0/all/0/1\">Saibal Mukhopadhyay</a>",
          "description": "We present a Model Uncertainty-aware Differentiable ARchiTecture Search\n($\\mu$DARTS) that optimizes neural networks to simultaneously achieve high\naccuracy and low uncertainty. We introduce concrete dropout within DARTS cells\nand include a Monte-Carlo regularizer within the training loss to optimize the\nconcrete dropout probabilities. A predictive variance term is introduced in the\nvalidation loss to enable searching for architecture with minimal model\nuncertainty. The experiments on CIFAR10, CIFAR100, SVHN, and ImageNet verify\nthe effectiveness of $\\mu$DARTS in improving accuracy and reducing uncertainty\ncompared to existing DARTS methods. Moreover, the final architecture obtained\nfrom $\\mu$DARTS shows higher robustness to noise at the input image and model\nparameters compared to the architecture obtained from existing DARTS methods.",
          "link": "http://arxiv.org/abs/2107.11500",
          "publishedOn": "2021-07-27T02:03:35.280Z",
          "wordCount": 549,
          "title": "$\\mu$DARTS: Model Uncertainty-Aware Differentiable Architecture Search. (arXiv:2107.11500v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bahmani_B/0/1/0/all/0/1\">Bahador Bahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">WaiChing Sun</a>",
          "description": "This paper presents a PINN training framework that employs (1) pre-training\nsteps that accelerates and improve the robustness of the training of\nphysics-informed neural network with auxiliary data stored in point clouds, (2)\na net-to-net knowledge transfer algorithm that improves the weight\ninitialization of the neural network and (3) a multi-objective optimization\nalgorithm that may improve the performance of a physical-informed neural\nnetwork with competing constraints. We consider the training and transfer and\nmulti-task learning of physics-informed neural network (PINN) as\nmulti-objective problems where the physics constraints such as the governing\nequation, boundary conditions, thermodynamic inequality, symmetry, and\ninvariant properties, as well as point cloud used for pre-training can\nsometimes lead to conflicts and necessitating the seek of the Pareto optimal\nsolution. In these situations, weighted norms commonly used to handle multiple\nconstraints may lead to poor performance, while other multi-objective\nalgorithms may scale poorly with increasing dimensionality. To overcome this\ntechnical barrier, we adopt the concept of vectorized objective function and\nmodify a gradient descent approach to handle the issue of conflicting\ngradients. Numerical experiments are compared the benchmark boundary value\nproblems solved via PINN. The performance of the proposed paradigm is compared\nagainst the classical equal-weighted norm approach. Our numerical experiments\nindicate that the brittleness and lack of robustness demonstrated in some PINN\nimplementations can be overcome with the proposed strategy.",
          "link": "http://arxiv.org/abs/2107.11496",
          "publishedOn": "2021-07-27T02:03:35.261Z",
          "wordCount": 652,
          "title": "Training multi-objective/multi-task collocation physics-informed neural network with student/teachers transfer learnings. (arXiv:2107.11496v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Rui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gower_R/0/1/0/all/0/1\">Robert M. Gower</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1\">Alessandro Lazaric</a>",
          "description": "The policy gradient (PG) is one of the most popular methods for solving\nreinforcement learning (RL) problems. However, a solid theoretical\nunderstanding of even the \"vanilla\" PG has remained elusive for long time. In\nthis paper, we apply recent tools developed for the analysis of SGD in\nnon-convex optimization to obtain convergence guarantees for both REINFORCE and\nGPOMDP under smoothness assumption on the objective function and weak\nconditions on the second moment of the norm of the estimated gradient. When\ninstantiated under common assumptions on the policy space, our general result\nimmediately recovers existing $\\widetilde{\\mathcal{O}}(\\epsilon^{-4})$ sample\ncomplexity guarantees, but for wider ranges of parameters (e.g., step size and\nbatch size $m$) with respect to previous literature. Notably, our result\nincludes the single trajectory case (i.e., $m=1$) and it provides a more\naccurate analysis of the dependency on problem-specific parameters by fixing\nprevious results available in the literature. We believe that the integration\nof state-of-the-art tools from non-convex optimization may lead to identify a\nmuch broader range of problems where PG methods enjoy strong theoretical\nguarantees.",
          "link": "http://arxiv.org/abs/2107.11433",
          "publishedOn": "2021-07-27T02:03:35.252Z",
          "wordCount": 624,
          "title": "A general sample complexity analysis of vanilla policy gradient. (arXiv:2107.11433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blumer_K/0/1/0/all/0/1\">Katy Blumer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1\">Michael P. Brenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1\">Jon Kleinberg</a>",
          "description": "We analyze a dataset of retinal images using linear probes: linear regression\nmodels trained on some \"target\" task, using embeddings from a deep\nconvolutional (CNN) model trained on some \"source\" task as input. We use this\nmethod across all possible pairings of 93 tasks in the UK Biobank dataset of\nretinal images, leading to ~164k different models. We analyze the performance\nof these linear probes by source and target task and by layer depth. We observe\nthat representations from the middle layers of the network are more\ngeneralizable. We find that some target tasks are easily predicted irrespective\nof the source task, and that some other target tasks are more accurately\npredicted from correlated source tasks than from embeddings trained on the same\ntask.",
          "link": "http://arxiv.org/abs/2107.11468",
          "publishedOn": "2021-07-27T02:03:35.239Z",
          "wordCount": 596,
          "title": "Using a Cross-Task Grid of Linear Probes to Interpret CNN Model Predictions On Retinal Images. (arXiv:2107.11468v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Weichao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaiqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ruihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1\">David Simchi-Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basar_T/0/1/0/all/0/1\">Tamer Ba&#x15f;ar</a>",
          "description": "We consider model-free reinforcement learning (RL) in non-stationary Markov\ndecision processes. Both the reward functions and the state transition\nfunctions are allowed to vary arbitrarily over time as long as their cumulative\nvariations do not exceed certain variation budgets. We propose Restarted\nQ-Learning with Upper Confidence Bounds (RestartQ-UCB), the first model-free\nalgorithm for non-stationary RL, and show that it outperforms existing\nsolutions in terms of dynamic regret. Specifically, RestartQ-UCB with\nFreedman-type bonus terms achieves a dynamic regret bound of\n$\\widetilde{O}(S^{\\frac{1}{3}} A^{\\frac{1}{3}} \\Delta^{\\frac{1}{3}} H\nT^{\\frac{2}{3}})$, where $S$ and $A$ are the numbers of states and actions,\nrespectively, $\\Delta>0$ is the variation budget, $H$ is the number of time\nsteps per episode, and $T$ is the total number of time steps. We further\npresent a parameter-free algorithm named Double-Restart Q-UCB that does not\nrequire prior knowledge of the variation budget. We show that our algorithms\nare \\emph{nearly optimal} by establishing an information-theoretical lower\nbound of $\\Omega(S^{\\frac{1}{3}} A^{\\frac{1}{3}} \\Delta^{\\frac{1}{3}}\nH^{\\frac{2}{3}} T^{\\frac{2}{3}})$, the first lower bound in non-stationary RL.\nNumerical experiments validate the advantages of RestartQ-UCB in terms of both\ncumulative rewards and computational efficiency. We demonstrate the power of\nour results in examples of multi-agent RL and inventory control across related\nproducts.",
          "link": "http://arxiv.org/abs/2010.03161",
          "publishedOn": "2021-07-27T02:03:34.879Z",
          "wordCount": 700,
          "title": "Model-Free Non-Stationary RL: Near-Optimal Regret and Applications in Multi-Agent RL and Inventory Control. (arXiv:2010.03161v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uhrenholt_A/0/1/0/all/0/1\">Anders Kirk Uhrenholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charvet_V/0/1/0/all/0/1\">Valentin Charvet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jensen_B/0/1/0/all/0/1\">Bj&#xf8;rn Sand Jensen</a>",
          "description": "Sparse Gaussian processes and various extensions thereof are enabled through\ninducing points, that simultaneously bottleneck the predictive capacity and act\nas the main contributor towards model complexity. However, the number of\ninducing points is generally not associated with uncertainty which prevents us\nfrom applying the apparatus of Bayesian reasoning for identifying an\nappropriate trade-off. In this work we place a point process prior on the\ninducing points and approximate the associated posterior through stochastic\nvariational inference. By letting the prior encourage a moderate number of\ninducing points, we enable the model to learn which and how many points to\nutilise. We experimentally show that fewer inducing points are preferred by the\nmodel as the points become less informative, and further demonstrate how the\nmethod can be employed in deep Gaussian processes and latent variable\nmodelling.",
          "link": "http://arxiv.org/abs/2010.09370",
          "publishedOn": "2021-07-27T02:03:34.843Z",
          "wordCount": 628,
          "title": "Probabilistic selection of inducing points in sparse Gaussian processes. (arXiv:2010.09370v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.02417",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1\">Michael. I. Jordan</a>",
          "description": "This paper resolves a longstanding open question pertaining to the design of\nnear-optimal first-order algorithms for smooth and\nstrongly-convex-strongly-concave minimax problems. Current state-of-the-art\nfirst-order algorithms find an approximate Nash equilibrium using\n$\\tilde{O}(\\kappa_{\\mathbf x}+\\kappa_{\\mathbf y})$ or\n$\\tilde{O}(\\min\\{\\kappa_{\\mathbf x}\\sqrt{\\kappa_{\\mathbf y}},\n\\sqrt{\\kappa_{\\mathbf x}}\\kappa_{\\mathbf y}\\})$ gradient evaluations, where\n$\\kappa_{\\mathbf x}$ and $\\kappa_{\\mathbf y}$ are the condition numbers for the\nstrong-convexity and strong-concavity assumptions. A gap still remains between\nthese results and the best existing lower bound\n$\\tilde{\\Omega}(\\sqrt{\\kappa_{\\mathbf x}\\kappa_{\\mathbf y}})$. This paper\npresents the first algorithm with $\\tilde{O}(\\sqrt{\\kappa_{\\mathbf\nx}\\kappa_{\\mathbf y}})$ gradient complexity, matching the lower bound up to\nlogarithmic factors. Our algorithm is designed based on an accelerated proximal\npoint method and an accelerated solver for minimax proximal steps. It can be\neasily extended to the settings of strongly-convex-concave, convex-concave,\nnonconvex-strongly-concave, and nonconvex-concave functions. This paper also\npresents algorithms that match or outperform all existing methods in these\nsettings in terms of gradient complexity, up to logarithmic factors.",
          "link": "http://arxiv.org/abs/2002.02417",
          "publishedOn": "2021-07-27T02:03:34.817Z",
          "wordCount": 657,
          "title": "Near-Optimal Algorithms for Minimax Optimization. (arXiv:2002.02417v6 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1806.03884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1\">Thomas George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_C/0/1/0/all/0/1\">C&#xe9;sar Laurent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouthillier_X/0/1/0/all/0/1\">Xavier Bouthillier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1\">Pascal Vincent</a>",
          "description": "Optimization algorithms that leverage gradient covariance information, such\nas variants of natural gradient descent (Amari, 1998), offer the prospect of\nyielding more effective descent directions. For models with many parameters,\nthe covariance matrix they are based on becomes gigantic, making them\ninapplicable in their original form. This has motivated research into both\nsimple diagonal approximations and more sophisticated factored approximations\nsuch as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In\nthe present work we draw inspiration from both to propose a novel approximation\nthat is provably better than KFAC and amendable to cheap partial updates. It\nconsists in tracking a diagonal variance, not in parameter coordinates, but in\na Kronecker-factored eigenbasis, in which the diagonal approximation is likely\nto be more effective. Experiments show improvements over KFAC in optimization\nspeed for several deep network architectures.",
          "link": "http://arxiv.org/abs/1806.03884",
          "publishedOn": "2021-07-27T02:03:34.810Z",
          "wordCount": 617,
          "title": "Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis. (arXiv:1806.03884v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeub_L/0/1/0/all/0/1\">Lucas G. S. Jeub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colavizza_G/0/1/0/all/0/1\">Giovanni Colavizza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaowen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazzi_M/0/1/0/all/0/1\">Marya Bazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1\">Mihai Cucuringu</a>",
          "description": "We propose a decentralised \"local2global\" approach to graph representation\nlearning, that one can a-priori use to scale any embedding technique. Our\nlocal2global approach proceeds by first dividing the input graph into\noverlapping subgraphs (or \"patches\") and training local representations for\neach patch independently. In a second step, we combine the local\nrepresentations into a globally consistent representation by estimating the set\nof rigid motions that best align the local representations using information\nfrom the patch overlaps, via group synchronization. A key distinguishing\nfeature of local2global relative to existing work is that patches are trained\nindependently without the need for the often costly parameter synchronisation\nduring distributed training. This allows local2global to scale to large-scale\nindustrial applications, where the input graph may not even fit into memory and\nmay be stored in a distributed manner. Preliminary results on medium-scale data\nsets (up to $\\sim$7K nodes and $\\sim$200K edges) are promising, with a graph\nreconstruction performance for local2global that is comparable to that of\nglobally trained embeddings. A thorough evaluation of local2global on large\nscale data and applications to downstream tasks, such as node classification\nand link prediction, constitutes ongoing work.",
          "link": "http://arxiv.org/abs/2107.12224",
          "publishedOn": "2021-07-27T02:03:34.804Z",
          "wordCount": 638,
          "title": "Local2Global: Scaling global representation learning on graphs via local training. (arXiv:2107.12224v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.06557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilinski_M/0/1/0/all/0/1\">Mateusz Wilinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lokhov_A/0/1/0/all/0/1\">Andrey Y. Lokhov</a>",
          "description": "Spreading processes play an increasingly important role in modeling for\ndiffusion networks, information propagation, marketing and opinion setting. We\naddress the problem of learning of a spreading model such that the predictions\ngenerated from this model are accurate and could be subsequently used for the\noptimization, and control of diffusion dynamics. We focus on a challenging\nsetting where full observations of the dynamics are not available, and standard\napproaches such as maximum likelihood quickly become intractable for large\nnetwork instances. We introduce a computationally efficient algorithm, based on\na scalable dynamic message-passing approach, which is able to learn parameters\nof the effective spreading model given only limited information on the\nactivation times of nodes in the network. The popular Independent Cascade model\nis used to illustrate our approach. We show that tractable inference from the\nlearned model generates a better prediction of marginal probabilities compared\nto the original model. We develop a systematic procedure for learning a mixture\nof models which further improves the prediction quality.",
          "link": "http://arxiv.org/abs/2007.06557",
          "publishedOn": "2021-07-27T02:03:34.797Z",
          "wordCount": 659,
          "title": "Prediction-Centric Learning of Independent Cascade Dynamics from Partial Observations. (arXiv:2007.06557v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12243",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Wei_J/0/1/0/all/0/1\">Junkang Wei</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1\">Siyuan Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zong_L/0/1/0/all/0/1\">Licheng Zong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>",
          "description": "Protein-RNA interactions are of vital importance to a variety of cellular\nactivities. Both experimental and computational techniques have been developed\nto study the interactions. Due to the limitation of the previous database,\nespecially the lack of protein structure data, most of the existing\ncomputational methods rely heavily on the sequence data, with only a small\nportion of the methods utilizing the structural information. Recently,\nAlphaFold has revolutionized the entire protein and biology field. Foreseeably,\nthe protein-RNA interaction prediction will also be promoted significantly in\nthe upcoming years. In this work, we give a thorough review of this field,\nsurveying both the binding site and binding preference prediction problems and\ncovering the commonly used datasets, features, and models. We also point out\nthe potential challenges and opportunities in this field. This survey\nsummarizes the development of the RBP-RNA interaction field in the past and\nforesees its future development in the post-AlphaFold era.",
          "link": "http://arxiv.org/abs/2107.12243",
          "publishedOn": "2021-07-27T02:03:34.772Z",
          "wordCount": 591,
          "title": "Protein-RNA interaction prediction with deep learning: Structure matters. (arXiv:2107.12243v1 [q-bio.BM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Preethi_P/0/1/0/all/0/1\">P Preethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1\">Hrishikesh Viswanath</a>",
          "description": "This paper is a presentation of a new method for denoising images using\nHaralick features and further segmenting the characters using artificial neural\nnetworks. The image is divided into kernels, each of which is converted to a\nGLCM (Gray Level Co-Occurrence Matrix) on which a Haralick Feature generation\nfunction is called, the result of which is an array with fourteen elements\ncorresponding to fourteen features The Haralick values and the corresponding\nnoise/text classification form a dictionary, which is then used to de-noise the\nimage through kernel comparison. Segmentation is the process of extracting\ncharacters from a document and can be used when letters are separated by white\nspace, which is an explicit boundary marker. Segmentation is the first step in\nmany Natural Language Processing problems. This paper explores the process of\nsegmentation using Neural Networks. While there have been numerous methods to\nsegment characters of a document, this paper is only concerned with the\naccuracy of doing so using neural networks. It is imperative that the\ncharacters be segmented correctly, for failing to do so will lead to incorrect\nrecognition by Natural language processing tools. Artificial Neural Networks\nwas used to attain accuracy of upto 89%. This method is suitable for languages\nwhere the characters are delimited by white space. However, this method will\nfail to provide acceptable results when the language heavily uses connected\nletters. An example would be the Devanagari script, which is predominantly used\nin northern India.",
          "link": "http://arxiv.org/abs/2107.11801",
          "publishedOn": "2021-07-27T02:03:34.764Z",
          "wordCount": 677,
          "title": "Denoising and Segmentation of Epigraphical Scripts. (arXiv:2107.11801v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Susheel Kumar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_J/0/1/0/all/0/1\">Jagdish Chand Bansal</a>",
          "description": "In the binary search space, GSA framework encounters the shortcomings of\nstagnation, diversity loss, premature convergence and high time complexity. To\naddress these issues, a novel binary variant of GSA called `A novel\nneighbourhood archives embedded gravitational constant in GSA for binary search\nspace (BNAGGSA)' is proposed in this paper. In BNAGGSA, the novel\nfitness-distance based social interaction strategy produces a self-adaptive\nstep size mechanism through which the agent moves towards the optimal direction\nwith the optimal step size, as per its current search requirement. The\nperformance of the proposed algorithm is compared with the two binary variants\nof GSA over 23 well-known benchmark test problems. The experimental results and\nstatistical analyses prove the supremacy of BNAGGSA over the compared\nalgorithms. Furthermore, to check the applicability of the proposed algorithm\nin solving real-world applications, a windfarm layout optimization problem is\nconsidered. Two case studies with two different wind data sets of two different\nwind sites is considered for experiments.",
          "link": "http://arxiv.org/abs/2107.11844",
          "publishedOn": "2021-07-27T02:03:34.758Z",
          "wordCount": 621,
          "title": "A binary variant of gravitational search algorithm and its application to windfarm layout optimization problem. (arXiv:2107.11844v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nandy_J/0/1/0/all/0/1\">Jay Nandy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wynne Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mong Li Lee</a>",
          "description": "Deep learning-based models are developed to automatically detect if a retina\nimage is `referable' in diabetic retinopathy (DR) screening. However, their\nclassification accuracy degrades as the input images distributionally shift\nfrom their training distribution. Further, even if the input is not a retina\nimage, a standard DR classifier produces a high confident prediction that the\nimage is `referable'. Our paper presents a Dirichlet Prior Network-based\nframework to address this issue. It utilizes an out-of-distribution (OOD)\ndetector model and a DR classification model to improve generalizability by\nidentifying OOD images. Experiments on real-world datasets indicate that the\nproposed framework can eliminate the unknown non-retina images and identify the\ndistributionally shifted retina images for human intervention.",
          "link": "http://arxiv.org/abs/2107.11822",
          "publishedOn": "2021-07-27T02:03:34.751Z",
          "wordCount": 560,
          "title": "Distributional Shifts in Automated Diabetic Retinopathy Screening. (arXiv:2107.11822v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nordby_J/0/1/0/all/0/1\">Jon Nordby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemazi_F/0/1/0/all/0/1\">Fabian Nemazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieber_D/0/1/0/all/0/1\">Dag Rieber</a>",
          "description": "Outdoor shooting ranges are subject to noise regulations from local and\nnational authorities. Restrictions found in these regulations may include\nlimits on times of activities, the overall number of noise events, as well as\nlimits on number of events depending on the class of noise or activity. A noise\nmonitoring system may be used to track overall sound levels, but rarely provide\nthe ability to detect activity or count the number of events, required to\ncompare directly with such regulations. This work investigates the feasibility\nand performance of an automatic detection system to count noise events. An\nempirical evaluation was done by collecting data at a newly constructed\nshooting range and training facility. The data includes tests of multiple\nweapon configurations from small firearms to high caliber rifles and\nexplosives, at multiple source positions, and collected on multiple different\ndays. Several alternative machine learning models are tested, using as inputs\ntime-series of standard acoustic indicators such as A-weighted sound levels and\n1/3 octave spectrogram, and classifiers such as Logistic Regression and\nConvolutional Neural Networks. Performance for the various alternatives are\nreported in terms of the False Positive Rate and False Negative Rate. The\ndetection performance was found to be satisfactory for use in automatic logging\nof time-periods with training activity.",
          "link": "http://arxiv.org/abs/2107.11453",
          "publishedOn": "2021-07-27T02:03:34.745Z",
          "wordCount": 663,
          "title": "Automatic Detection Of Noise Events at Shooting Range Using Machine Learning. (arXiv:2107.11453v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin-Chun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Le Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shaoming Song</a>",
          "description": "Although federated learning (FL) has recently been proposed for efficient\ndistributed training and data privacy protection, it still encounters many\nobstacles. One of these is the naturally existing statistical heterogeneity\namong clients, making local data distributions non independently and\nidentically distributed (i.e., non-iid), which poses challenges for model\naggregation and personalization. For FL with a deep neural network (DNN),\nprivatizing some layers is a simple yet effective solution for non-iid\nproblems. However, which layers should we privatize to facilitate the learning\nprocess? Do different categories of non-iid scenes have preferred privatization\nways? Can we automatically learn the most appropriate privatization way during\nFL? In this paper, we answer these questions via abundant experimental studies\non several FL benchmarks. First, we present the detailed statistics of these\nbenchmarks and categorize them into covariate and label shift non-iid scenes.\nThen, we investigate both coarse-grained and fine-grained network splits and\nexplore whether the preferred privatization ways have any potential relations\nto the specific category of a non-iid scene. Our findings are exciting, e.g.,\nprivatizing the base layers could boost the performances even in label shift\nnon-iid scenes, which are inconsistent with some natural conjectures. We also\nfind that none of these privatization ways could improve the performances on\nthe Shakespeare benchmark, and we guess that Shakespeare may not be a seriously\nnon-iid scene. Finally, we propose several approaches to automatically learn\nwhere to aggregate via cross-stitch, soft attention, and hard selection. We\nadvocate the proposed methods could serve as a preliminary try to explore where\nto privatize for a novel non-iid scene.",
          "link": "http://arxiv.org/abs/2107.11954",
          "publishedOn": "2021-07-27T02:03:34.722Z",
          "wordCount": 709,
          "title": "Aggregate or Not? Exploring Where to Privatize in DNN Based Federated Learning Under Different Non-IID Scenes. (arXiv:2107.11954v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Um_S/0/1/0/all/0/1\">Se-Yun Um</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sangshin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_K/0/1/0/all/0/1\">Kyungguen Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hong-Goo Kang</a>",
          "description": "In this paper, we propose an effective method to synthesize speaker-specific\nspeech waveforms by conditioning on videos of an individual's face. Using a\ngenerative adversarial network (GAN) with linguistic and speaker characteristic\nfeatures as auxiliary conditions, our method directly converts face images into\nspeech waveforms under an end-to-end training framework. The linguistic\nfeatures are extracted from lip movements using a lip-reading model, and the\nspeaker characteristic features are predicted from face images using\ncross-modal learning with a pre-trained acoustic model. Since these two\nfeatures are uncorrelated and controlled independently, we can flexibly\nsynthesize speech waveforms whose speaker characteristics vary depending on the\ninput face images. Therefore, our method can be regarded as a multi-speaker\nface-to-speech waveform model. We show the superiority of our proposed model\nover conventional methods in terms of both objective and subjective evaluation\nresults. Specifically, we evaluate the performances of the linguistic feature\nand the speaker characteristic generation modules by measuring the accuracy of\nautomatic speech recognition and automatic speaker/gender recognition tasks,\nrespectively. We also evaluate the naturalness of the synthesized speech\nwaveforms using a mean opinion score (MOS) test.",
          "link": "http://arxiv.org/abs/2107.12003",
          "publishedOn": "2021-07-27T02:03:34.706Z",
          "wordCount": 646,
          "title": "Facetron: Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations. (arXiv:2107.12003v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11678",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shang_R/0/1/0/all/0/1\">Ruibo Shang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OBrien_M/0/1/0/all/0/1\">Mikaela A. O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luke_G/0/1/0/all/0/1\">Geoffrey P. Luke</a>",
          "description": "Single-pixel imaging (SPI) has the advantages of high-speed acquisition over\na broad wavelength range and system compactness, which are difficult to achieve\nby conventional imaging sensors. However, a common challenge is low image\nquality arising from undersampling. Deep learning (DL) is an emerging and\npowerful tool in computational imaging for many applications and researchers\nhave applied DL in SPI to achieve higher image quality than conventional\nreconstruction approaches. One outstanding challenge, however, is that the\naccuracy of DL predictions in SPI cannot be assessed in practical applications\nwhere the ground truths are unknown. Here, we propose the use of the Bayesian\nconvolutional neural network (BCNN) to approximate the uncertainty (coming from\nfinite training data and network model) of the DL predictions in SPI. Each\npixel in the predicted result from BCNN represents the parameter of a\nprobability distribution rather than the image intensity value. Then, the\nuncertainty can be approximated with BCNN by minimizing a negative\nlog-likelihood loss function in the training stage and Monte Carlo dropout in\nthe prediction stage. The results show that the BCNN can reliably approximate\nthe uncertainty of the DL predictions in SPI with varying compression ratios\nand noise levels. The predicted uncertainty from BCNN in SPI reveals that most\nof the reconstruction errors in deep-learning-based SPI come from the edges of\nthe image features. The results show that the proposed BCNN can provide a\nreliable tool to approximate the uncertainty of DL predictions in SPI and can\nbe widely used in many applications of SPI.",
          "link": "http://arxiv.org/abs/2107.11678",
          "publishedOn": "2021-07-27T02:03:34.669Z",
          "wordCount": 695,
          "title": "Deep-learning-driven Reliable Single-pixel Imaging with Uncertainty Approximation. (arXiv:2107.11678v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>",
          "description": "One-class novelty detection is to identify anomalous instances that do not\nconform to the expected normal instances. In this paper, the Generative\nAdversarial Networks (GANs) based on encoder-decoder-encoder pipeline are used\nfor detection and achieve state-of-the-art performance. However, deep neural\nnetworks are too over-parameterized to deploy on resource-limited devices.\nTherefore, Progressive Knowledge Distillation with GANs (PKDGAN) is proposed to\nlearn compact and fast novelty detection networks. The P-KDGAN is a novel\nattempt to connect two standard GANs by the designed distillation loss for\ntransferring knowledge from the teacher to the student. The progressive\nlearning of knowledge distillation is a two-step approach that continuously\nimproves the performance of the student GAN and achieves better performance\nthan single step methods. In the first step, the student GAN learns the basic\nknowledge totally from the teacher via guiding of the pretrained teacher GAN\nwith fixed weights. In the second step, joint fine-training is adopted for the\nknowledgeable teacher and student GANs to further improve the performance and\nstability. The experimental results on CIFAR-10, MNIST, and FMNIST show that\nour method improves the performance of the student GAN by 2.44%, 1.77%, and\n1.73% when compressing the computation at ratios of 24.45:1, 311.11:1, and\n700:1, respectively.",
          "link": "http://arxiv.org/abs/2007.06963",
          "publishedOn": "2021-07-27T02:03:34.662Z",
          "wordCount": 674,
          "title": "P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection. (arXiv:2007.06963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Maojun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guangxu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiamo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Caijun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "The popular federated edge learning (FEEL) framework allows\nprivacy-preserving collaborative model training via frequent learning-updates\nexchange between edge devices and server. Due to the constrained bandwidth,\nonly a subset of devices can upload their updates at each communication round.\nThis has led to an active research area in FEEL studying the optimal device\nscheduling policy for minimizing communication time. However, owing to the\ndifficulty in quantifying the exact communication time, prior work in this area\ncan only tackle the problem partially by considering either the communication\nrounds or per-round latency, while the total communication time is determined\nby both metrics. To close this gap, we make the first attempt in this paper to\nformulate and solve the communication time minimization problem. We first\nderive a tight bound to approximate the communication time through\ncross-disciplinary effort involving both learning theory for convergence\nanalysis and communication theory for per-round latency analysis. Building on\nthe analytical result, an optimized probabilistic scheduling policy is derived\nin closed-form by solving the approximate communication time minimization\nproblem. It is found that the optimized policy gradually turns its priority\nfrom suppressing the remaining communication rounds to reducing per-round\nlatency as the training process evolves. The effectiveness of the proposed\nscheme is demonstrated via a use case on collaborative 3D objective detection\nin autonomous driving.",
          "link": "http://arxiv.org/abs/2107.11588",
          "publishedOn": "2021-07-27T02:03:34.640Z",
          "wordCount": 684,
          "title": "Accelerating Federated Edge Learning via Optimized Probabilistic Device Scheduling. (arXiv:2107.11588v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuezihan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1\">Zeang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1\">Xupeng Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "Graph neural networks (GNNs) have been widely used in many graph-based tasks\nsuch as node classification, link prediction, and node clustering. However,\nGNNs gain their performance benefits mainly from performing the feature\npropagation and smoothing across the edges of the graph, thus requiring\nsufficient connectivity and label information for effective propagation.\nUnfortunately, many real-world networks are sparse in terms of both edges and\nlabels, leading to sub-optimal performance of GNNs. Recent interest in this\nsparse problem has focused on the self-training approach, which expands\nsupervised signals with pseudo labels. Nevertheless, the self-training approach\ninherently cannot realize the full potential of refining the learning\nperformance on sparse graphs due to the unsatisfactory quality and quantity of\npseudo labels.\n\nIn this paper, we propose ROD, a novel reception-aware online knowledge\ndistillation approach for sparse graph learning. We design three supervision\nsignals for ROD: multi-scale reception-aware graph knowledge, task-based\nsupervision, and rich distilled knowledge, allowing online knowledge transfer\nin a peer-teaching manner. To extract knowledge concealed in the multi-scale\nreception fields, ROD explicitly requires individual student models to preserve\ndifferent levels of locality information. For a given task, each student would\npredict based on its reception-scale knowledge, while simultaneously a strong\nteacher is established on-the-fly by combining multi-scale knowledge. Our\napproach has been extensively evaluated on 9 datasets and a variety of\ngraph-based tasks, including node classification, link prediction, and node\nclustering. The result demonstrates that ROD achieves state-of-art performance\nand is more robust for the graph sparsity.",
          "link": "http://arxiv.org/abs/2107.11789",
          "publishedOn": "2021-07-27T02:03:34.633Z",
          "wordCount": 687,
          "title": "ROD: Reception-aware Online Distillation for Sparse Graphs. (arXiv:2107.11789v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krause_D/0/1/0/all/0/1\">Daniel Aleksander Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Politis_A/0/1/0/all/0/1\">Archontis Politis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesaros_A/0/1/0/all/0/1\">Annamaria Mesaros</a>",
          "description": "Sound source proximity and distance estimation are of great interest in many\npractical applications, since they provide significant information for acoustic\nscene analysis. As both tasks share complementary qualities, ensuring efficient\ninteraction between these two is crucial for a complete picture of an aural\nenvironment. In this paper, we aim to investigate several ways of performing\njoint proximity and direction estimation from binaural recordings, both defined\nas coarse classification problems based on Deep Neural Networks (DNNs).\nConsidering the limitations of binaural audio, we propose two methods of\nsplitting the sphere into angular areas in order to obtain a set of directional\nclasses. For each method we study different model types to acquire information\nabout the direction-of-arrival (DoA). Finally, we propose various ways of\ncombining the proximity and direction estimation problems into a joint task\nproviding temporal information about the onsets and offsets of the appearing\nsources. Experiments are performed for a synthetic reverberant binaural dataset\nconsisting of up to two overlapping sound events.",
          "link": "http://arxiv.org/abs/2107.12033",
          "publishedOn": "2021-07-27T02:03:34.622Z",
          "wordCount": 611,
          "title": "Joint Direction and Proximity Classification of Overlapping Sound Events from Binaural Audio. (arXiv:2107.12033v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dionelis_N/0/1/0/all/0/1\">Nikolaos Dionelis</a>",
          "description": "Generative Adversarial Networks (GAN) are a powerful methodology and can be\nused for unsupervised anomaly detection, where current techniques have\nlimitations such as the accurate detection of anomalies near the tail of a\ndistribution. GANs generally do not guarantee the existence of a probability\ndensity and are susceptible to mode collapse, while few GANs use likelihood to\nreduce mode collapse. In this paper, we create a GAN-based tail formation model\nfor anomaly detection, the Tail of distribution GAN (TailGAN), to generate\nsamples on the tail of the data distribution and detect anomalies near the\nsupport boundary. Using TailGAN, we leverage GANs for anomaly detection and use\nmaximum entropy regularization. Using GANs that learn the probability of the\nunderlying distribution has advantages in improving the anomaly detection\nmethodology by allowing us to devise a generator for boundary samples, and use\nthis model to characterize anomalies. TailGAN addresses supports with disjoint\ncomponents and achieves competitive performance on images. We evaluate TailGAN\nfor identifying Out-of-Distribution (OoD) data and its performance evaluated on\nMNIST, CIFAR-10, Baggage X-Ray, and OoD data shows competitiveness compared to\nmethods from the literature.",
          "link": "http://arxiv.org/abs/2107.11658",
          "publishedOn": "2021-07-27T02:03:34.616Z",
          "wordCount": 635,
          "title": "Tail of Distribution GAN (TailGAN): Generative- Adversarial-Network-Based Boundary Formation. (arXiv:2107.11658v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhuoqing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shi Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>",
          "description": "In this work, we consider the decentralized optimization problem in which a\nnetwork of $n$ agents, each possessing a smooth and convex objective function,\nwish to collaboratively minimize the average of all the objective functions\nthrough peer-to-peer communication in a directed graph. To solve the problem,\nwe propose two accelerated Push-DIGing methods termed APD and APD-SC for\nminimizing non-strongly convex objective functions and strongly convex ones,\nrespectively. We show that APD and APD-SC respectively converge at the rates\n$O\\left(\\frac{1}{k^2}\\right)$ and $O\\left(\\left(1 -\nC\\sqrt{\\frac{\\mu}{L}}\\right)^k\\right)$ up to constant factors depending only on\nthe mixing matrix. To the best of our knowledge, APD and APD-SC are the first\ndecentralized methods to achieve provable acceleration over unbalanced directed\ngraphs. Numerical experiments demonstrate the effectiveness of both methods.",
          "link": "http://arxiv.org/abs/2107.12065",
          "publishedOn": "2021-07-27T02:03:34.607Z",
          "wordCount": 583,
          "title": "Provably Accelerated Decentralized Gradient Method Over Unbalanced Directed Graphs. (arXiv:2107.12065v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michelucci_U/0/1/0/all/0/1\">Umberto Michelucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperti_M/0/1/0/all/0/1\">Michela Sperti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piga_D/0/1/0/all/0/1\">Dario Piga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venturini_F/0/1/0/all/0/1\">Francesca Venturini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deriu_M/0/1/0/all/0/1\">Marco A. Deriu</a>",
          "description": "This paper presents the intrinsic limit determination algorithm (ILD\nAlgorithm), a novel technique to determine the best possible performance,\nmeasured in terms of the AUC (area under the ROC curve) and accuracy, that can\nbe obtained from a specific dataset in a binary classification problem with\ncategorical features {\\sl regardless} of the model used. This limit, namely the\nBayes error, is completely independent of any model used and describes an\nintrinsic property of the dataset. The ILD algorithm thus provides important\ninformation regarding the prediction limits of any binary classification\nalgorithm when applied to the considered dataset. In this paper the algorithm\nis described in detail, its entire mathematical framework is presented and the\npseudocode is given to facilitate its implementation. Finally, an example with\na real dataset is given.",
          "link": "http://arxiv.org/abs/2107.11609",
          "publishedOn": "2021-07-27T02:03:34.590Z",
          "wordCount": 579,
          "title": "A Model-Agnostic Algorithm for Bayes Error Determination in Binary Classification. (arXiv:2107.11609v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kadeethum_T/0/1/0/all/0/1\">T. Kadeethum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballarin_F/0/1/0/all/0/1\">F. Ballarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Y. Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OMalley_D/0/1/0/all/0/1\">D. O&#x27;Malley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">H. Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouklas_N/0/1/0/all/0/1\">N. Bouklas</a>",
          "description": "Natural convection in porous media is a highly nonlinear multiphysical\nproblem relevant to many engineering applications (e.g., the process of\n$\\mathrm{CO_2}$ sequestration). Here, we present a non-intrusive reduced order\nmodel of natural convection in porous media employing deep convolutional\nautoencoders for the compression and reconstruction and either radial basis\nfunction (RBF) interpolation or artificial neural networks (ANNs) for mapping\nparameters of partial differential equations (PDEs) on the corresponding\nnonlinear manifolds. To benchmark our approach, we also describe linear\ncompression and reconstruction processes relying on proper orthogonal\ndecomposition (POD) and ANNs. We present comprehensive comparisons among\ndifferent models through three benchmark problems. The reduced order models,\nlinear and nonlinear approaches, are much faster than the finite element model,\nobtaining a maximum speed-up of $7 \\times 10^{6}$ because our framework is not\nbound by the Courant-Friedrichs-Lewy condition; hence, it could deliver\nquantities of interest at any given time contrary to the finite element model.\nOur model's accuracy still lies within a mean squared error of 0.07 (two-order\nof magnitude lower than the maximum value of the finite element results) in the\nworst-case scenario. We illustrate that, in specific settings, the nonlinear\napproach outperforms its linear counterpart and vice versa. We hypothesize that\na visual comparison between principal component analysis (PCA) or t-Distributed\nStochastic Neighbor Embedding (t-SNE) could indicate which method will perform\nbetter prior to employing any specific compression strategy.",
          "link": "http://arxiv.org/abs/2107.11460",
          "publishedOn": "2021-07-27T02:03:34.584Z",
          "wordCount": 697,
          "title": "Non-intrusive reduced order modeling of natural convection in porous media using convolutional autoencoders: comparison with linear subspace techniques. (arXiv:2107.11460v1 [cs.CE])"
        },
        {
          "id": "http://arxiv.org/abs/2005.02077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Angione_C/0/1/0/all/0/1\">Claudio Angione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silverman_E/0/1/0/all/0/1\">Eric Silverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaneske_E/0/1/0/all/0/1\">Elisabeth Yaneske</a>",
          "description": "In this proof-of-concept work, we evaluate the performance of multiple\nmachine-learning methods as statistical emulators for use in the analysis of\nagent-based models (ABMs). Analysing ABM outputs can be challenging, as the\nrelationships between input parameters can be non-linear or even chaotic even\nin relatively simple models, and each model run can require significant CPU\ntime. Statistical emulation, in which a statistical model of the ABM is\nconstructed to facilitate detailed model analyses, has been proposed as an\nalternative to computationally costly Monte Carlo methods. Here we compare\nmultiple machine-learning methods for ABM emulation in order to determine the\napproaches best suited to emulating the complex behaviour of ABMs. Our results\nsuggest that, in most scenarios, artificial neural networks (ANNs) and\ngradient-boosted trees outperform Gaussian process emulators, currently the\nmost commonly used method for the emulation of complex computational models.\nANNs produced the most accurate model replications in scenarios with high\nnumbers of model runs, although training times were longer than the other\nmethods. We propose that agent-based modelling would benefit from using\nmachine-learning methods for emulation, as this can facilitate more robust\nsensitivity analyses for the models while also reducing CPU time consumption\nwhen calibrating and analysing the simulation.",
          "link": "http://arxiv.org/abs/2005.02077",
          "publishedOn": "2021-07-27T02:03:34.563Z",
          "wordCount": 664,
          "title": "Using Machine Learning to Emulate Agent-Based Simulations. (arXiv:2005.02077v2 [cs.MA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11740",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Li_C/0/1/0/all/0/1\">Chongcan Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cong_Y/0/1/0/all/0/1\">Yong Cong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deng_W/0/1/0/all/0/1\">Weihua Deng</a>",
          "description": "We preprocess the raw NMR spectrum and extract key characteristic features by\nusing two different methodologies, called equidistant sampling and peak\nsampling for subsequent substructure pattern recognition; meanwhile may provide\nthe alternative strategy to address the imbalance issue of the NMR dataset\nfrequently encountered in dataset collection of statistical modeling and\nestablish two conventional SVM and KNN models to assess the capability of two\nfeature selection, respectively. Our results in this study show that the models\nusing the selected features of peak sampling outperform the ones using the\nother. Then we build the Recurrent Neural Network (RNN) model trained by Data B\ncollected from peak sampling. Furthermore, we illustrate the easier\noptimization of hyper parameters and the better generalization ability of the\nRNN deep learning model by comparison with traditional machine learning SVM and\nKNN models in detail.",
          "link": "http://arxiv.org/abs/2107.11740",
          "publishedOn": "2021-07-27T02:03:34.491Z",
          "wordCount": 591,
          "title": "Identifying the fragment structure of the organic compounds by deeply learning the original NMR data. (arXiv:2107.11740v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengjiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bo Ji</a>",
          "description": "In this paper, we study the problem of fair worker selection in Federated\nLearning systems, where fairness serves as an incentive mechanism that\nencourages more workers to participate in the federation. Considering the\nachieved training accuracy of the global model as the utility of the selected\nworkers, which is typically a monotone submodular function, we formulate the\nworker selection problem as a new multi-round monotone submodular maximization\nproblem with cardinality and fairness constraints. The objective is to maximize\nthe time-average utility over multiple rounds subject to an additional fairness\nrequirement that each worker must be selected for a certain fraction of time.\nWhile the traditional submodular maximization with a cardinality constraint is\nalready a well-known NP-Hard problem, the fairness constraint in the\nmulti-round setting adds an extra layer of difficulty. To address this novel\nchallenge, we propose three algorithms: Fair Continuous Greedy (FairCG1 and\nFairCG2) and Fair Discrete Greedy (FairDG), all of which satisfy the fairness\nrequirement whenever feasible. Moreover, we prove nontrivial lower bounds on\nthe achieved time-average utility under FairCG1 and FairCG2. In addition, by\ngiving a higher priority to fairness, FairDG ensures a stronger short-term\nfairness guarantee, which holds in every round. Finally, we perform extensive\nsimulations to verify the effectiveness of the proposed algorithms in terms of\nthe time-average utility and fairness satisfaction.",
          "link": "http://arxiv.org/abs/2107.11728",
          "publishedOn": "2021-07-27T02:03:34.483Z",
          "wordCount": 672,
          "title": "Federated Learning with Fair Worker Selection: A Multi-Round Submodular Maximization Approach. (arXiv:2107.11728v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11662",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Singh_R/0/1/0/all/0/1\">Rahul Singh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yongxin Chen</a>",
          "description": "We consider inference problems for a class of continuous state collective\nhidden Markov models, where the data is recorded in aggregate (collective) form\ngenerated by a large population of individuals following the same dynamics. We\npropose an aggregate inference algorithm called collective Gaussian\nforward-backward algorithm, extending recently proposed Sinkhorn belief\npropagation algorithm to models characterized by Gaussian densities. Our\nalgorithm enjoys convergence guarantee. In addition, it reduces to the standard\nKalman filter when the observations are generated by a single individual. The\nefficacy of the proposed algorithm is demonstrated through multiple\nexperiments.",
          "link": "http://arxiv.org/abs/2107.11662",
          "publishedOn": "2021-07-27T02:03:34.477Z",
          "wordCount": 531,
          "title": "Inference of collective Gaussian hidden Markov models. (arXiv:2107.11662v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_C/0/1/0/all/0/1\">Chandrajit Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Avik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoran Zhang</a>",
          "description": "Variational Autoencoders (VAEs) have been shown to be remarkably effective in\nrecovering model latent spaces for several computer vision tasks. However,\ncurrently trained VAEs, for a number of reasons, seem to fall short in learning\ninvariant and equivariant clusters in latent space. Our work focuses on\nproviding solutions to this problem and presents an approach to disentangle\nequivariance feature maps in a Lie group manifold by enforcing deep,\ngroup-invariant learning. Simultaneously implementing a novel separation of\nsemantic and equivariant variables of the latent space representation, we\nformulate a modified Evidence Lower BOund (ELBO) by using a mixture model pdf\nlike Gaussian mixtures for invariant cluster embeddings that allows superior\nunsupervised variational clustering. Our experiments show that this model\neffectively learns to disentangle the invariant and equivariant representations\nwith significant improvements in the learning rate and an observably superior\nimage recognition and canonical state reconstruction compared to the currently\nbest deep learning models.",
          "link": "http://arxiv.org/abs/2107.11717",
          "publishedOn": "2021-07-27T02:03:34.470Z",
          "wordCount": 598,
          "title": "Invariance-based Multi-Clustering of Latent Space Embeddings for Equivariant Learning. (arXiv:2107.11717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yeli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Easwaran_A/0/1/0/all/0/1\">Arvind Easwaran</a>",
          "description": "Highly complex deep learning models are increasingly integrated into modern\ncyber-physical systems (CPS), many of which have strict safety requirements.\nOne problem arising from this is that deep learning lacks interpretability,\noperating as a black box. The reliability of deep learning is heavily impacted\nby how well the model training data represents runtime test data, especially\nwhen the input space dimension is high as natural images. In response, we\npropose a robust out-of-distribution (OOD) detection framework. Our approach\ndetects unusual movements from driving video in real-time by combining\nclassical optic flow operation with representation learning via variational\nautoencoder (VAE). We also design a method to locate OOD factors in images.\nEvaluation on a driving simulation data set shows that our approach is\nstatistically more robust than related works.",
          "link": "http://arxiv.org/abs/2107.11736",
          "publishedOn": "2021-07-27T02:03:34.463Z",
          "wordCount": 564,
          "title": "WiP Abstract : Robust Out-of-distribution Motion Detection and Localization in Autonomous CPS. (arXiv:2107.11736v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biza_O/0/1/0/all/0/1\">Ondrej Biza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pol_E/0/1/0/all/0/1\">Elise van der Pol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1\">Thomas Kipf</a>",
          "description": "World models trained by contrastive learning are a compelling alternative to\nautoencoder-based world models, which learn by reconstructing pixel states. In\nthis paper, we describe three cases where small changes in how we sample\nnegative states in the contrastive loss lead to drastic changes in model\nperformance. In previously studied Atari datasets, we show that leveraging time\nstep correlations can double the performance of the Contrastive Structured\nWorld Model. We also collect a full version of the datasets to study\ncontrastive learning under a more diverse set of experiences.",
          "link": "http://arxiv.org/abs/2107.11676",
          "publishedOn": "2021-07-27T02:03:34.446Z",
          "wordCount": 538,
          "title": "The Impact of Negative Sampling on Contrastive Structured World Models. (arXiv:2107.11676v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abou_Kreisha_M/0/1/0/all/0/1\">Mohamed Taha Abou-Kreisha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elnashar_H/0/1/0/all/0/1\">Hany Elnashar</a>",
          "description": "Automated Vehicle License Plate (VLP) detection and recognition have ended up\nbeing a significant research issue as of late. VLP localization and recognition\nare some of the most essential techniques for managing traffic using digital\ntechniques. In this paper, four smart systems are developed to recognize\nEgyptian vehicles license plates. Two systems are based on character\nrecognition, which are (System1, Characters Recognition with Classical Machine\nLearning) and (System2, Characters Recognition with Deep Machine Learning). The\nother two systems are based on the whole plate recognition which are (System3,\nWhole License Plate Recognition with Classical Machine Learning) and (System4,\nWhole License Plate Recognition with Deep Machine Learning). We use object\ndetection algorithms, and machine learning based object recognition algorithms.\nThe performance of the developed systems has been tested on real images, and\nthe experimental results demonstrate that the best detection accuracy rate for\nVLP is provided by using the deep learning method. Where the VLP detection\naccuracy rate is better than the classical system by 32%. However, the best\ndetection accuracy rate for Vehicle License Plate Arabic Character (VLPAC) is\nprovided by using the classical method. Where VLPAC detection accuracy rate is\nbetter than the deep learning-based system by 6%. Also, the results show that\ndeep learning is better than the classical technique used in VLP recognition\nprocesses. Where the recognition accuracy rate is better than the classical\nsystem by 8%. Finally, the paper output recommends a robust VLP recognition\nsystem based on both statistical and deep machine learning.",
          "link": "http://arxiv.org/abs/2107.11640",
          "publishedOn": "2021-07-27T02:03:34.436Z",
          "wordCount": 722,
          "title": "Deep Machine Learning Based Egyptian Vehicle License Plate Recognition Systems. (arXiv:2107.11640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1\">Liu Ziyin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Botao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masahito Ueda</a>",
          "description": "Stochastic gradient descent (SGD) has been deployed to solve highly\nnon-linear and non-convex machine learning problems such as the training of\ndeep neural networks. However, previous works on SGD often rely on highly\nrestrictive and unrealistic assumptions about the nature of noise in SGD. In\nthis work, we mathematically construct examples that defy previous\nunderstandings of SGD. For example, our constructions show that: (1) SGD may\nconverge to a local maximum; (2) SGD may escape a saddle point arbitrarily\nslowly; (3) SGD may prefer sharp minima over the flat ones; and (4) AMSGrad may\nconverge to a local maximum. Our result suggests that the noise structure of\nSGD might be more important than the loss landscape in neural network training\nand that future research should focus on deriving the actual noise structure in\ndeep learning.",
          "link": "http://arxiv.org/abs/2107.11774",
          "publishedOn": "2021-07-27T02:03:34.380Z",
          "wordCount": 570,
          "title": "SGD May Never Escape Saddle Points. (arXiv:2107.11774v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>",
          "description": "Although Graph Convolutional Networks (GCNs) have demonstrated their power in\nvarious applications, the graph convolutional layers, as the most important\ncomponent of GCN, are still using linear transformations and a simple pooling\nstep. In this paper, we propose a novel generalization of Factorized Bilinear\n(FB) layer to model the feature interactions in GCNs. FB performs two\nmatrix-vector multiplications, that is, the weight matrix is multiplied with\nthe outer product of the vector of hidden features from both sides. However,\nthe FB layer suffers from the quadratic number of coefficients, overfitting and\nthe spurious correlations due to correlations between channels of hidden\nrepresentations that violate the i.i.d. assumption. Thus, we propose a compact\nFB layer by defining a family of summarizing operators applied over the\nquadratic term. We analyze proposed pooling operators and motivate their use.\nOur experimental results on multiple datasets demonstrate that the GFB-GCN is\ncompetitive with other methods for text classification.",
          "link": "http://arxiv.org/abs/2107.11666",
          "publishedOn": "2021-07-27T02:03:34.352Z",
          "wordCount": 588,
          "title": "Graph Convolutional Network with Generalized Factorized Bilinear Aggregation. (arXiv:2107.11666v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruoxuan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koenecke_A/0/1/0/all/0/1\">Allison Koenecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_M/0/1/0/all/0/1\">Michael Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1\">Susan Athey</a>",
          "description": "Analyzing observational data from multiple sources can be useful for\nincreasing statistical power to detect a treatment effect; however, practical\nconstraints such as privacy considerations may restrict individual-level\ninformation sharing across data sets. This paper develops federated methods\nthat only utilize summary-level information from heterogeneous data sets. Our\nfederated methods provide doubly-robust point estimates of treatment effects as\nwell as variance estimates. We derive the asymptotic distributions of our\nfederated estimators, which are shown to be asymptotically equivalent to the\ncorresponding estimators from the combined, individual-level data. We show that\nto achieve these properties, federated methods should be adjusted based on\nconditions such as whether models are correctly specified and stable across\nheterogeneous data sets.",
          "link": "http://arxiv.org/abs/2107.11732",
          "publishedOn": "2021-07-27T02:03:34.332Z",
          "wordCount": 561,
          "title": "Federated Causal Inference in Heterogeneous Observational Data. (arXiv:2107.11732v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_H/0/1/0/all/0/1\">Haoyi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jianming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zheyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>",
          "description": "How to explore corner cases as efficiently and thoroughly as possible has\nlong been one of the top concerns in the context of deep reinforcement learning\n(DeepRL) autonomous driving. Training with simulated data is less costly and\ndangerous than utilizing real-world data, but the inconsistency of parameter\ndistribution and the incorrect system modeling in simulators always lead to an\ninevitable Sim2real gap, which probably accounts for the underperformance in\nnovel, anomalous and risky cases that simulators can hardly generate. Domain\nRandomization(DR) is a methodology that can bridge this gap with little or no\nreal-world data. Consequently, in this research, an adversarial model is put\nforward to robustify DeepRL-based autonomous vehicles trained in simulation to\ngradually surfacing harder events, so that the models could readily transfer to\nthe real world.",
          "link": "http://arxiv.org/abs/2107.11762",
          "publishedOn": "2021-07-27T02:03:34.325Z",
          "wordCount": 579,
          "title": "DR2L: Surfacing Corner Cases to Robustify Autonomous Driving via Domain Randomization Reinforcement Learning. (arXiv:2107.11762v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07179",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Zhai_H/0/1/0/all/0/1\">Hanfeng Zhai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hu_G/0/1/0/all/0/1\">Guohui Hu</a>",
          "description": "Micro-bubbles and bubbly flows are widely observed and applied in chemical\nengineering, medicine, involves deformation, rupture, and collision of bubbles,\nphase mixture, etc. We study bubble dynamics by setting up two numerical\nsimulation cases: bubbly flow with a single bubble and multiple bubbles, both\nconfined in the microchannel, with parameters corresponding to their medical\nbackgrounds. Both the cases have their medical background applications.\nMultiphase flow simulation requires high computation accuracy due to possible\ncomponent losses that may be caused by sparse meshing during the computation.\nHence, data-driven methods can be adopted as an useful tool. Based on\nphysics-informed neural networks (PINNs), we propose a novel deep learning\nframework BubbleNet, which entails three main parts: deep neural networks (DNN)\nwith sub nets for predicting different physics fields; the\nsemi-physics-informed part, with only the fluid continuum condition and the\npressure Poisson equation $\\mathcal{P}$ encoded within; the time discretized\nnormalizer (TDN), an algorithm to normalize field data per time step before\ntraining. We apply the traditional DNN and our BubbleNet to train the coarsened\nsimulation data and predict the physics fields of both the two bubbly flow\ncases. The BubbleNets are trained for both with and without $\\mathcal{P}$, from\nwhich we conclude that the 'physics-informed' part can serve as inner\nsupervision. Results indicate our framework can predict the physics fields more\naccurately, estimating the prediction absolute errors. Our deep learning\npredictions outperform traditional numerical methods computed with similar data\ndensity meshing. The proposed network can potentially be applied to many other\nengineering fields.",
          "link": "http://arxiv.org/abs/2105.07179",
          "publishedOn": "2021-07-27T02:03:34.109Z",
          "wordCount": 714,
          "title": "BubbleNet: Inferring micro-bubble dynamics with semi-physics-informed deep learning. (arXiv:2105.07179v2 [physics.flu-dyn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shih-Lun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "Transformers and variational autoencoders (VAE) have been extensively\nemployed for symbolic (e.g., MIDI) domain music generation. While the former\nboast an impressive capability in modeling long sequences, the latter allow\nusers to willingly exert control over different parts (e.g., bars) of the music\nto be generated. In this paper, we are interested in bringing the two together\nto construct a single model that exhibits both strengths. The task is split\ninto two steps. First, we equip Transformer decoders with the ability to accept\nsegment-level, time-varying conditions during sequence generation.\nSubsequently, we combine the developed and tested in-attention decoder with a\nTransformer encoder, and train the resulting MuseMorphose model with the VAE\nobjective to achieve style transfer of long musical pieces, in which users can\nspecify musical attributes including rhythmic intensity and polyphony (i.e.,\nharmonic fullness) they desire, down to the bar level. Experiments show that\nMuseMorphose outperforms recurrent neural network (RNN) based baselines on\nnumerous widely-used metrics for style transfer tasks.",
          "link": "http://arxiv.org/abs/2105.04090",
          "publishedOn": "2021-07-27T02:03:34.059Z",
          "wordCount": 639,
          "title": "MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with One Transformer VAE. (arXiv:2105.04090v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "Generalization performance of stochastic optimization stands a central place\nin learning theory. In this paper, we investigate the excess risk performance\nand towards improved learning rates for two popular approaches of stochastic\noptimization: empirical risk minimization (ERM) and stochastic gradient descent\n(SGD). Although there exists plentiful generalization analysis of ERM and SGD\nfor supervised learning, current theoretical understandings of ERM and SGD\neither have stronger assumptions in convex learning, e.g., strong convexity, or\nshow slow rates and less studied in nonconvex learning. Motivated by these\nproblems, we aim to provide improved rates under milder assumptions in convex\nlearning and derive faster rates in nonconvex learning. It is notable that our\nanalysis span two popular theoretical viewpoints: \\emph{stability} and\n\\emph{uniform convergence}. Specifically, in stability regime, we present high\nprobability learning rates of order $\\mathcal{O} (1/n)$ w.r.t. the sample size\n$n$ for ERM and SGD with milder assumptions in convex learning and similar high\nprobability rates of order $\\mathcal{O} (1/n)$ in nonconvex learning, rather\nthan in expectation. Furthermore, this type of learning rate is improved to\nfaster order $\\mathcal{O} (1/n^2)$ in uniform convergence regime. To our best\nknowledge, for ERM and SGD, the learning rates presented in this paper are all\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2107.08686",
          "publishedOn": "2021-07-27T02:03:34.046Z",
          "wordCount": 657,
          "title": "Improved Learning Rates for Stochastic Optimization: Two Theoretical Viewpoints. (arXiv:2107.08686v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Peng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shi Gu</a>",
          "description": "We study the challenging task of neural network quantization without\nend-to-end retraining, called Post-training Quantization (PTQ). PTQ usually\nrequires a small subset of training data but produces less powerful quantized\nmodels than Quantization-Aware Training (QAT). In this work, we propose a novel\nPTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to\nINT2 for the first time. BRECQ leverages the basic building blocks in neural\nnetworks and reconstructs them one-by-one. In a comprehensive theoretical study\nof the second-order error, we show that BRECQ achieves a good balance between\ncross-layer dependency and generalization error. To further employ the power of\nquantization, the mixed precision technique is incorporated in our framework by\napproximating the inter-layer and intra-layer sensitivity. Extensive\nexperiments on various handcrafted and searched neural architectures are\nconducted for both image classification and object detection tasks. And for the\nfirst time we prove that, without bells and whistles, PTQ can attain 4-bit\nResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster\nproduction of quantized models. Codes are available at\nhttps://github.com/yhhhli/BRECQ.",
          "link": "http://arxiv.org/abs/2102.05426",
          "publishedOn": "2021-07-27T02:03:34.040Z",
          "wordCount": 658,
          "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction. (arXiv:2102.05426v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1\">Francesco D&#x27;Angelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>",
          "description": "Deep ensembles have recently gained popularity in the deep learning community\nfor their conceptual simplicity and efficiency. However, maintaining functional\ndiversity between ensemble members that are independently trained with gradient\ndescent is challenging. This can lead to pathologies when adding more ensemble\nmembers, such as a saturation of the ensemble performance, which converges to\nthe performance of a single model. Moreover, this does not only affect the\nquality of its predictions, but even more so the uncertainty estimates of the\nensemble, and thus its performance on out-of-distribution data. We hypothesize\nthat this limitation can be overcome by discouraging different ensemble members\nfrom collapsing to the same function. To this end, we introduce a kernelized\nrepulsive term in the update rule of the deep ensembles. We show that this\nsimple modification not only enforces and maintains diversity among the members\nbut, even more importantly, transforms the maximum a posteriori inference into\nproper Bayesian inference. Namely, we show that the training dynamics of our\nproposed repulsive ensembles follow a Wasserstein gradient flow of the KL\ndivergence with the true posterior. We study repulsive terms in weight and\nfunction space and empirically compare their performance to standard ensembles\nand Bayesian baselines on synthetic and real-world prediction tasks.",
          "link": "http://arxiv.org/abs/2106.11642",
          "publishedOn": "2021-07-27T02:03:34.022Z",
          "wordCount": 655,
          "title": "Repulsive Deep Ensembles are Bayesian. (arXiv:2106.11642v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiaxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "Bi-Level Optimization (BLO) is originated from the area of economic game\ntheory and then introduced into the optimization community. BLO is able to\nhandle problems with a hierarchical structure, involving two levels of\noptimization tasks, where one task is nested inside the other. In machine\nlearning and computer vision fields, despite the different motivations and\nmechanisms, a lot of complex problems, such as hyper-parameter optimization,\nmulti-task and meta-learning, neural architecture search, adversarial learning\nand deep reinforcement learning, actually all contain a series of closely\nrelated subproblms. In this paper, we first uniformly express these complex\nlearning and vision problems from the perspective of BLO. Then we construct a\nbest-response-based single-level reformulation and establish a unified\nalgorithmic framework to understand and formulate mainstream gradient-based BLO\nmethodologies, covering aspects ranging from fundamental automatic\ndifferentiation schemes to various accelerations, simplifications, extensions\nand their convergence and complexity properties. Last but not least, we discuss\nthe potentials of our unified BLO framework for designing new algorithms and\npoint out some promising directions for future research.",
          "link": "http://arxiv.org/abs/2101.11517",
          "publishedOn": "2021-07-27T02:03:34.015Z",
          "wordCount": 664,
          "title": "Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond. (arXiv:2101.11517v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Visca_M/0/1/0/all/0/1\">Marco Visca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1\">Sampo Kuutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1\">Roger Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1\">Saber Fallah</a>",
          "description": "Terrain traversability analysis plays a major role in ensuring safe robotic\nnavigation in unstructured environments. However, real-time constraints\nfrequently limit the accuracy of online tests especially in scenarios where\nrealistic robot-terrain interactions are complex to model. In this context, we\npropose a deep learning framework trained in an end-to-end fashion from\nelevation maps and trajectories to estimate the occurrence of failure events.\nThe network is first trained and tested in simulation over synthetic maps\ngenerated by the OpenSimplex algorithm. The prediction performance of the Deep\nLearning framework is illustrated by being able to retain over 94% recall of\nthe original simulator at 30% of the computational time. Finally, the network\nis transferred and tested on real elevation maps collected by the SEEKER\nconsortium during the Martian rover test trial in the Atacama desert in Chile.\nWe show that transferring and fine-tuning of an application-independent\npre-trained model retains better performance than training uniquely on scarcely\navailable real data.",
          "link": "http://arxiv.org/abs/2105.10937",
          "publishedOn": "2021-07-27T02:03:34.008Z",
          "wordCount": 639,
          "title": "Deep Learning Traversability Estimator for Mobile Robots in Unstructured Environments. (arXiv:2105.10937v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1\">Amal Feriani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>",
          "description": "Multi-Agent Reinforcement Learning (MARL) is a challenging subarea of\nReinforcement Learning due to the non-stationarity of the environments and the\nlarge dimensionality of the combined action space. Deep MARL algorithms have\nbeen applied to solve different task offloading problems. However, in\nreal-world applications, information required by the agents (i.e. rewards and\nstates) are subject to noise and alterations. The stability and the robustness\nof deep MARL to practical challenges is still an open research problem. In this\nwork, we apply state-of-the art MARL algorithms to solve task offloading with\nreward uncertainty. We show that perturbations in the reward signal can induce\ndecrease in the performance compared to learning with perfect rewards. We\nexpect this paper to stimulate more research in studying and addressing the\npractical challenges of deploying deep MARL solutions in wireless\ncommunications systems.",
          "link": "http://arxiv.org/abs/2107.08114",
          "publishedOn": "2021-07-27T02:03:34.002Z",
          "wordCount": 585,
          "title": "Decentralized Multi-Agent Reinforcement Learning for Task Offloading Under Uncertainty. (arXiv:2107.08114v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuowei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijun Zhang</a>",
          "description": "This paper concentrates on the approximation power of deep feed-forward\nneural networks in terms of width and depth. It is proved by construction that\nReLU networks with width $\\mathcal{O}\\big(\\max\\{d\\lfloor N^{1/d}\\rfloor,\\,\nN+2\\}\\big)$ and depth $\\mathcal{O}(L)$ can approximate a H\\\"older continuous\nfunction on $[0,1]^d$ with an approximation rate\n$\\mathcal{O}\\big(\\lambda\\sqrt{d} (N^2L^2\\ln N)^{-\\alpha/d}\\big)$, where\n$\\alpha\\in (0,1]$ and $\\lambda>0$ are H\\\"older order and constant,\nrespectively. Such a rate is optimal up to a constant in terms of width and\ndepth separately, while existing results are only nearly optimal without the\nlogarithmic factor in the approximation rate. More generally, for an arbitrary\ncontinuous function $f$ on $[0,1]^d$, the approximation rate becomes\n$\\mathcal{O}\\big(\\,\\sqrt{d}\\,\\omega_f\\big( (N^2L^2\\ln N)^{-1/d}\\big)\\,\\big)$,\nwhere $\\omega_f(\\cdot)$ is the modulus of continuity. We also extend our\nanalysis to any continuous function $f$ on a bounded set. Particularly, if ReLU\nnetworks with depth $31$ and width $\\mathcal{O}(N)$ are used to approximate\none-dimensional Lipschitz continuous functions on $[0,1]$ with a Lipschitz\nconstant $\\lambda>0$, the approximation rate in terms of the total number of\nparameters, $W=\\mathcal{O}(N^2)$, becomes $\\mathcal{O}(\\tfrac{\\lambda}{W\\ln\nW})$, which has not been discovered in the literature for fixed-depth ReLU\nnetworks.",
          "link": "http://arxiv.org/abs/2103.00502",
          "publishedOn": "2021-07-27T02:03:33.994Z",
          "wordCount": 677,
          "title": "Optimal Approximation Rate of ReLU Networks in terms of Width and Depth. (arXiv:2103.00502v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lixin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhanxing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation,\nCriticism, and Application Trend of Explainable AI. Deep neural networks (DNNs)\nhave undoubtedly brought great success to a wide range of applications in\ncomputer vision, computational linguistics, and AI. However, foundational\nprinciples underlying the DNNs' success and their resilience to adversarial\nattacks are still largely missing. Interpreting and theorizing the internal\nmechanisms of DNNs becomes a compelling yet controversial topic. This workshop\npays a special interest in theoretic foundations, limitations, and new\napplication trends in the scope of XAI. These issues reflect new bottlenecks in\nthe future development of XAI.",
          "link": "http://arxiv.org/abs/2107.08821",
          "publishedOn": "2021-07-27T02:03:33.974Z",
          "wordCount": 584,
          "title": "Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI. (arXiv:2107.08821v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.",
          "link": "http://arxiv.org/abs/2106.11342",
          "publishedOn": "2021-07-27T02:03:33.967Z",
          "wordCount": 596,
          "title": "Dive into Deep Learning. (arXiv:2106.11342v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1\">Larisa Markeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1\">Ilya Tolstikhin</a>",
          "description": "We introduce a generalization to the lottery ticket hypothesis in which the\nnotion of \"sparsity\" is relaxed by choosing an arbitrary basis in the space of\nparameters. We present evidence that the original results reported for the\ncanonical basis continue to hold in this broader setting. We describe how\nstructured pruning methods, including pruning units or factorizing\nfully-connected layers into products of low-rank matrices, can be cast as\nparticular instances of this \"generalized\" lottery ticket hypothesis. The\ninvestigations reported here are preliminary and are provided to encourage\nfurther research along this direction.",
          "link": "http://arxiv.org/abs/2107.06825",
          "publishedOn": "2021-07-27T02:03:33.960Z",
          "wordCount": 571,
          "title": "A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying-Jun Angela Zhang</a>",
          "description": "We study over-the-air model aggregation in federated edge learning (FEEL)\nsystems, where channel state information at the transmitters (CSIT) is assumed\nto be unavailable. We leverage the reconfigurable intelligent surface (RIS)\ntechnology to align the cascaded channel coefficients for CSIT-free model\naggregation. To this end, we jointly optimize the RIS and the receiver by\nminimizing the aggregation error under the channel alignment constraint. We\nthen develop a difference-of-convex algorithm for the resulting non-convex\noptimization. Numerical experiments on image classification show that the\nproposed method is able to achieve a similar learning accuracy as the\nstate-of-the-art CSIT-based solution, demonstrating the efficiency of our\napproach in combating the lack of CSIT.",
          "link": "http://arxiv.org/abs/2102.10749",
          "publishedOn": "2021-07-27T02:03:33.952Z",
          "wordCount": 631,
          "title": "CSIT-Free Model Aggregation for Federated Edge Learning via Reconfigurable Intelligent Surface. (arXiv:2102.10749v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schutz_N/0/1/0/all/0/1\">Narayan Sch&#xfc;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botros_A/0/1/0/all/0/1\">Angela Botros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Single_M/0/1/0/all/0/1\">Michael Single</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naef_A/0/1/0/all/0/1\">Aileen C. Naef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buluschek_P/0/1/0/all/0/1\">Philipp Buluschek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nef_T/0/1/0/all/0/1\">Tobias Nef</a>",
          "description": "Sensor technologies are becoming increasingly prevalent in the biomedical\nfield, with applications ranging from telemonitoring of people at risk, to\nusing sensor derived information as objective endpoints in clinical trials. To\nfully utilize sensor information, signals from distinct sensors often have to\nbe temporally aligned. However, due to imperfect oscillators and significant\nnoise, commonly encountered with biomedical signals, temporal alignment of raw\nsignals is an all but trivial problem, with, to-date, no generally applicable\nsolution. In this work, we present Deep Canonical Correlation Alignment (DCCA),\na novel, generally applicable solution for the temporal alignment of raw\n(biomedical) sensor signals. DCCA allows practitioners to directly align raw\nsignals, from distinct sensors, without requiring deep domain knowledge. On a\nselection of artificial and real datasets, we demonstrate the performance and\nutility of DCCA under a variety of conditions. We compare the DCCA algorithm to\nother warping based methods, DCCA outperforms dynamic time warping and cross\ncorrelation based methods by an order of magnitude in terms of alignment error.\nDCCA performs especially well on almost periodic biomedical signals such as\nheart-beats and breathing patterns. In comparison to existing approaches, that\nare not tailored towards raw sensor data, DCCA is not only fast enough to work\non signals with billions of data points but also provides automatic filtering\nand transformation functionalities, allowing it to deal with very noisy and\neven morphologically distinct signals.",
          "link": "http://arxiv.org/abs/2106.03637",
          "publishedOn": "2021-07-27T02:03:33.945Z",
          "wordCount": 691,
          "title": "Deep Canonical Correlation Alignment for Sensor Signals. (arXiv:2106.03637v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "We consider nonconvex-concave minimax problems, $\\min_{\\mathbf{x}}\n\\max_{\\mathbf{y} \\in \\mathcal{Y}} f(\\mathbf{x}, \\mathbf{y})$, where $f$ is\nnonconvex in $\\mathbf{x}$ but concave in $\\mathbf{y}$ and $\\mathcal{Y}$ is a\nconvex and bounded set. One of the most popular algorithms for solving this\nproblem is the celebrated gradient descent ascent (GDA) algorithm, which has\nbeen widely used in machine learning, control theory and economics. Despite the\nextensive convergence results for the convex-concave setting, GDA with equal\nstepsize can converge to limit cycles or even diverge in a general setting. In\nthis paper, we present the complexity results on two-time-scale GDA for solving\nnonconvex-concave minimax problems, showing that the algorithm can find a\nstationary point of the function $\\Phi(\\cdot) := \\max_{\\mathbf{y} \\in\n\\mathcal{Y}} f(\\cdot, \\mathbf{y})$ efficiently. To the best our knowledge, this\nis the first nonasymptotic analysis for two-time-scale GDA in this setting,\nshedding light on its superior practical performance in training generative\nadversarial networks (GANs) and other real applications.",
          "link": "http://arxiv.org/abs/1906.00331",
          "publishedOn": "2021-07-27T02:03:33.938Z",
          "wordCount": 684,
          "title": "On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems. (arXiv:1906.00331v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guozhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1\">Cheng Kevin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_P/0/1/0/all/0/1\">Pulin Gong</a>",
          "description": "Learning in deep neural networks (DNNs) is implemented through minimizing a\nhighly non-convex loss function, typically by a stochastic gradient descent\n(SGD) method. This learning process can effectively find good wide minima\nwithout being trapped in poor local ones. We present a novel account of how\nsuch effective deep learning emerges through the interactions of the SGD and\nthe geometrical structure of the loss landscape. Rather than being a normal\ndiffusion process (i.e. Brownian motion) as often assumed, we find that the SGD\nexhibits rich, complex dynamics when navigating through the loss landscape;\ninitially, the SGD exhibits anomalous superdiffusion, which attenuates\ngradually and changes to subdiffusion at long times when the solution is\nreached. Such learning dynamics happen ubiquitously in different DNNs such as\nResNet and VGG-like networks and are insensitive to batch size and learning\nrate. The anomalous superdiffusion process during the initial learning phase\nindicates that the motion of SGD along the loss landscape possesses\nintermittent, big jumps; this non-equilibrium property enables the SGD to\nescape from sharp local minima. By adapting the methods developed for studying\nenergy landscapes in complex physical systems, we find that such superdiffusive\nlearning dynamics are due to the interactions of the SGD and the fractal-like\nstructure of the loss landscape. We further develop a simple model to\ndemonstrate the mechanistic role of the fractal loss landscape in enabling the\nSGD to effectively find global minima. Our results thus reveal the\neffectiveness of deep learning from a novel perspective and have implications\nfor designing efficient deep neural networks.",
          "link": "http://arxiv.org/abs/2009.10588",
          "publishedOn": "2021-07-27T02:03:33.919Z",
          "wordCount": 732,
          "title": "Anomalous diffusion dynamics of learning in deep neural networks. (arXiv:2009.10588v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.08260",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zellinger_W/0/1/0/all/0/1\">Werner Zellinger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moser_B/0/1/0/all/0/1\">Bernhard A Moser</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Saminger_Platz_S/0/1/0/all/0/1\">Susanne Saminger-Platz</a>",
          "description": "Domain adaptation algorithms are designed to minimize the misclassification\nrisk of a discriminative model for a target domain with little training data by\nadapting a model from a source domain with a large amount of training data.\nStandard approaches measure the adaptation discrepancy based on distance\nmeasures between the empirical probability distributions in the source and\ntarget domain. In this setting, we address the problem of deriving\ngeneralization bounds under practice-oriented general conditions on the\nunderlying probability distributions. As a result, we obtain generalization\nbounds for domain adaptation based on finitely many moments and smoothness\nconditions.",
          "link": "http://arxiv.org/abs/2002.08260",
          "publishedOn": "2021-07-27T02:03:33.901Z",
          "wordCount": 569,
          "title": "On generalization in moment-based domain adaptation. (arXiv:2002.08260v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.05541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_I/0/1/0/all/0/1\">Icaro O. de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1\">Rayson Laroca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_K/0/1/0/all/0/1\">Keiko V. O. Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1\">Rodrigo Minetto</a>",
          "description": "This work addresses the problem of vehicle identification through\nnon-overlapping cameras. As our main contribution, we introduce a novel dataset\nfor vehicle identification, called Vehicle-Rear, that contains more than three\nhours of high-resolution videos, with accurate information about the make,\nmodel, color and year of nearly 3,000 vehicles, in addition to the position and\nidentification of their license plates. To explore our dataset we design a\ntwo-stream CNN that simultaneously uses two of the most distinctive and\npersistent features available: the vehicle's appearance and its license plate.\nThis is an attempt to tackle a major problem: false alarms caused by vehicles\nwith similar designs or by very close license plate identifiers. In the first\nnetwork stream, shape similarities are identified by a Siamese CNN that uses a\npair of low-resolution vehicle patches recorded by two different cameras. In\nthe second stream, we use a CNN for OCR to extract textual information,\nconfidence scores, and string similarities from a pair of high-resolution\nlicense plate patches. Then, features from both streams are merged by a\nsequence of fully connected layers for decision. In our experiments, we\ncompared the two-stream network against several well-known CNN architectures\nusing single or multiple vehicle features. The architectures, trained models,\nand dataset are publicly available at https://github.com/icarofua/vehicle-rear.",
          "link": "http://arxiv.org/abs/1911.05541",
          "publishedOn": "2021-07-27T02:03:33.894Z",
          "wordCount": 721,
          "title": "Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle Identification Using Convolutional Neural Networks. (arXiv:1911.05541v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>",
          "description": "Generative adversarial networks (GANs) nowadays are capable of producing\nimages of incredible realism. One concern raised is whether the\nstate-of-the-art GAN's learned distribution still suffers from mode collapse,\nand what to do if so. Existing diversity tests of samples from GANs are usually\nconducted qualitatively on a small scale, and/or depends on the access to\noriginal training data as well as the trained model parameters. This paper\nexplores to diagnose GAN intra-mode collapse and calibrate that, in a novel\nblack-box setting: no access to training data, nor the trained model\nparameters, is assumed. The new setting is practically demanded, yet rarely\nexplored and significantly more challenging. As a first stab, we devise a set\nof statistical tools based on sampling, that can visualize, quantify, and\nrectify intra-mode collapse. We demonstrate the effectiveness of our proposed\ndiagnosis and calibration techniques, via extensive simulations and\nexperiments, on unconditional GAN image generation (e.g., face and vehicle).\nOur study reveals that the intra-mode collapse is still a prevailing problem in\nstate-of-the-art GANs and the mode collapse is diagnosable and calibratable in\nblack-box settings. Our codes are available at:\nhttps://github.com/VITA-Group/BlackBoxGANCollapse.",
          "link": "http://arxiv.org/abs/2107.12202",
          "publishedOn": "2021-07-27T02:03:33.750Z",
          "wordCount": 646,
          "title": "Black-Box Diagnosis and Calibration on GAN Intra-Mode Collapse: A Pilot Study. (arXiv:2107.12202v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>",
          "description": "When humans solve complex problems, they rarely come up with a decision\nright-away. Instead, they start with an intuitive decision, reflect upon it,\nspot mistakes, resolve contradictions and jump between different hypotheses.\nThus, they create a sequence of ideas and follow a train of thought that\nultimately reaches a conclusive decision. Contrary to this, today's neural\nclassification models are mostly trained to map an input to one single and\nfixed output. In this paper, we investigate how we can give models the\nopportunity of a second, third and $k$-th thought. We take inspiration from\nHegel's dialectics and propose a method that turns an existing classifier's\nclass prediction (such as the image class forest) into a sequence of\npredictions (such as forest $\\rightarrow$ tree $\\rightarrow$ mushroom).\nConcretely, we propose a correction module that is trained to estimate the\nmodel's correctness as well as an iterative prediction update based on the\nprediction's gradient. Our approach results in a dynamic system over class\nprobability distributions $\\unicode{x2014}$ the thought flow. We evaluate our\nmethod on diverse datasets and tasks from computer vision and natural language\nprocessing. We observe surprisingly complex but intuitive behavior and\ndemonstrate that our method (i) can correct misclassifications, (ii)\nstrengthens model performance, (iii) is robust to high levels of adversarial\nattacks, (iv) can increase accuracy up to 4% in a label-distribution-shift\nsetting and (iv) provides a tool for model interpretability that uncovers model\nknowledge which otherwise remains invisible in a single distribution\nprediction.",
          "link": "http://arxiv.org/abs/2107.12220",
          "publishedOn": "2021-07-27T02:03:33.743Z",
          "wordCount": 694,
          "title": "Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin-Chun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shaoming Song</a>",
          "description": "Automatically mining sentiment tendency contained in natural language is a\nfundamental research to some artificial intelligent applications, where\nsolutions alternate with challenges. Transfer learning and multi-task learning\ntechniques have been leveraged to mitigate the supervision sparsity and\ncollaborate multiple heterogeneous domains correspondingly. Recent years, the\nsensitive nature of users' private data raises another challenge for sentiment\nclassification, i.e., data privacy protection. In this paper, we resort to\nfederated learning for multiple domain sentiment classification under the\nconstraint that the corpora must be stored on decentralized devices. In view of\nthe heterogeneous semantics across multiple parties and the peculiarities of\nword embedding, we pertinently provide corresponding solutions. First, we\npropose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework for\nbetter model aggregation and personalization in federated sentiment\nclassification. Second, we propose KTEPS$^\\star$ with the consideration of the\nrich semantic and huge embedding size properties of word vectors, utilizing\nProjection-based Dimension Reduction (PDR) methods for privacy protection and\nefficient transmission simultaneously. We propose two federated sentiment\nclassification scenes based on public benchmarks, and verify the superiorities\nof our proposed methods with abundant experimental investigations.",
          "link": "http://arxiv.org/abs/2107.11956",
          "publishedOn": "2021-07-27T02:03:33.704Z",
          "wordCount": 615,
          "title": "Preliminary Steps Towards Federated Sentiment Classification. (arXiv:2107.11956v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Arun Kumar Singh</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Priyanka Singh</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Nathwani_K/0/1/0/all/0/1\">Karan Nathwani</a> (1) ((1) Indian Institute of Technology Jammu, (2) Dhirubhai Ambani Institute of Information and Communication Technology)",
          "description": "The recent developments in technology have re-warded us with amazing audio\nsynthesis models like TACOTRON and WAVENETS. On the other side, it poses\ngreater threats such as speech clones and deep fakes, that may go undetected.\nTo tackle these alarming situations, there is an urgent need to propose models\nthat can help discriminate a synthesized speech from an actual human speech and\nalso identify the source of such a synthesis. Here, we propose a model based on\nConvolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network\n(BiRNN) that helps to achieve both the aforementioned objectives. The temporal\ndependencies present in AI synthesized speech are exploited using Bidirectional\nRNN and CNN. The model outperforms the state-of-the-art approaches by\nclassifying the AI synthesized audio from real human speech with an error rate\nof 1.9% and detecting the underlying architecture with an accuracy of 97%.",
          "link": "http://arxiv.org/abs/2107.11412",
          "publishedOn": "2021-07-27T02:03:33.549Z",
          "wordCount": 629,
          "title": "Using Deep Learning Techniques and Inferential Speech Statistics for AI Synthesised Speech Recognition. (arXiv:2107.11412v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chung-Hsuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_E/0/1/0/all/0/1\">Erik G. Larsson</a>",
          "description": "Federated Learning (FL) is a newly emerged decentralized machine learning\n(ML) framework that combines on-device local training with server-based model\nsynchronization to train a centralized ML model over distributed nodes. In this\npaper, we propose an asynchronous FL framework with periodic aggregation to\neliminate the straggler issue in FL systems. For the proposed model, we\ninvestigate several device scheduling and update aggregation policies and\ncompare their performances when the devices have heterogeneous computation\ncapabilities and training data distributions. From the simulation results, we\nconclude that the scheduling and aggregation design for asynchronous FL can be\nrather different from the synchronous case. For example, a norm-based\nsignificance-aware scheduling policy might not be efficient in an asynchronous\nFL setting, and an appropriate \"age-aware\" weighting design for the model\naggregation can greatly improve the learning performance of such systems.",
          "link": "http://arxiv.org/abs/2107.11415",
          "publishedOn": "2021-07-27T02:03:33.455Z",
          "wordCount": 607,
          "title": "Device Scheduling and Update Aggregation Policies for Asynchronous Federated Learning. (arXiv:2107.11415v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_K/0/1/0/all/0/1\">Keren Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masotto_X/0/1/0/all/0/1\">Xander Masotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachani_V/0/1/0/all/0/1\">Vandana Bachani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikodem_J/0/1/0/all/0/1\">Jack Nikodem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dong Yin</a>",
          "description": "We propose a simulation framework for generating realistic instance-dependent\nnoisy labels via a pseudo-labeling paradigm. We show that this framework\ngenerates synthetic noisy labels that exhibit important characteristics of the\nlabel noise in practical settings via comparison with the CIFAR10-H dataset.\nEquipped with controllable label noise, we study the negative impact of noisy\nlabels across a few realistic settings to understand when label noise is more\nproblematic. We also benchmark several existing algorithms for learning with\nnoisy labels and compare their behavior on our synthetic datasets and on the\ndatasets with independent random label noise. Additionally, with the\navailability of annotator information from our simulation framework, we propose\na new technique, Label Quality Model (LQM), that leverages annotator features\nto predict and correct against noisy labels. We show that by adding LQM as a\nlabel correction step before applying existing noisy label techniques, we can\nfurther improve the models' performance.",
          "link": "http://arxiv.org/abs/2107.11413",
          "publishedOn": "2021-07-27T02:03:33.417Z",
          "wordCount": 596,
          "title": "A Realistic Simulation Framework for Learning with Label Noise. (arXiv:2107.11413v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11381",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Min_S/0/1/0/all/0/1\">Seonwoo Min</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lee_B/0/1/0/all/0/1\">Byunghan Lee</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "MicroRNAs (miRNAs) play pivotal roles in gene expression regulation by\nbinding to target sites of messenger RNAs (mRNAs). While identifying functional\ntargets of miRNAs is of utmost importance, their prediction remains a great\nchallenge. Previous computational algorithms have major limitations. They use\nconservative candidate target site (CTS) selection criteria mainly focusing on\ncanonical site types, rely on laborious and time-consuming manual feature\nextraction, and do not fully capitalize on the information underlying miRNA-CTS\ninteractions. In this paper, we introduce TargetNet, a novel deep\nlearning-based algorithm for functional miRNA target prediction. To address the\nlimitations of previous approaches, TargetNet has three key components: (1)\nrelaxed CTS selection criteria accommodating irregularities in the seed region,\n(2) a novel miRNA-CTS sequence encoding scheme incorporating extended seed\nregion alignments, and (3) a deep residual network-based prediction model. The\nproposed model was trained with miRNA-CTS pair datasets and evaluated with\nmiRNA-mRNA pair datasets. TargetNet advances the previous state-of-the-art\nalgorithms used in functional miRNA target classification. Furthermore, it\ndemonstrates great potential for distinguishing high-functional miRNA targets.",
          "link": "http://arxiv.org/abs/2107.11381",
          "publishedOn": "2021-07-27T02:03:33.388Z",
          "wordCount": 612,
          "title": "TargetNet: Functional microRNA Target Prediction with Deep Neural Networks. (arXiv:2107.11381v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_I/0/1/0/all/0/1\">Ian E. Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dera_D/0/1/0/all/0/1\">Dimah Dera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal Bouaynaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_R/0/1/0/all/0/1\">Ravi P. Ramachandran</a>",
          "description": "With the rise of deep neural networks, the challenge of explaining the\npredictions of these networks has become increasingly recognized. While many\nmethods for explaining the decisions of deep neural networks exist, there is\ncurrently no consensus on how to evaluate them. On the other hand, robustness\nis a popular topic for deep learning research; however, it is hardly talked\nabout in explainability until very recently. In this tutorial paper, we start\nby presenting gradient-based interpretability methods. These techniques use\ngradient signals to assign the burden of the decision on the input features.\nLater, we discuss how gradient-based methods can be evaluated for their\nrobustness and the role that adversarial robustness plays in having meaningful\nexplanations. We also discuss the limitations of gradient-based methods.\nFinally, we present the best practices and attributes that should be examined\nbefore choosing an explainability method. We conclude with the future\ndirections for research in the area at the convergence of robustness and\nexplainability.",
          "link": "http://arxiv.org/abs/2107.11400",
          "publishedOn": "2021-07-27T02:03:33.329Z",
          "wordCount": 617,
          "title": "Robust Explainability: A Tutorial on Gradient-Based Attribution Methods for Deep Neural Networks. (arXiv:2107.11400v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}