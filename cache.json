{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.16038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zijun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>",
          "description": "Recent pretraining models in Chinese neglect two important aspects specific\nto the Chinese language: glyph and pinyin, which carry significant syntax and\nsemantic information for language understanding. In this work, we propose\nChineseBERT, which incorporates both the {\\it glyph} and {\\it pinyin}\ninformation of Chinese characters into language model pretraining. The glyph\nembedding is obtained based on different fonts of a Chinese character, being\nable to capture character semantics from the visual features, and the pinyin\nembedding characterizes the pronunciation of Chinese characters, which handles\nthe highly prevalent heteronym phenomenon in Chinese (the same character has\ndifferent pronunciations with different meanings). Pretrained on large-scale\nunlabeled Chinese corpus, the proposed ChineseBERT model yields significant\nperformance boost over baseline models with fewer training steps. The porpsoed\nmodel achieves new SOTA performances on a wide range of Chinese NLP tasks,\nincluding machine reading comprehension, natural language inference, text\nclassification, sentence pair matching, and competitive performances in named\nentity recognition. Code and pretrained models are publicly available at\nhttps://github.com/ShannonAI/ChineseBert.",
          "link": "http://arxiv.org/abs/2106.16038",
          "publishedOn": "2021-07-01T01:59:34.663Z",
          "wordCount": 613,
          "title": "ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information. (arXiv:2106.16038v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahari_R/0/1/0/all/0/1\">Robert Zev Mahari</a>",
          "description": "This paper demonstrate how NLP can be used to address an unmet need of the\nlegal community and increase access to justice. The paper introduces Legal\nPrecedent Prediction (LPP), the task of predicting relevant passages from\nprecedential court decisions given the context of a legal argument. To this\nend, the paper showcases a BERT model, trained on 530,000 examples of legal\narguments made by U.S. federal judges, to predict relevant passages from\nprecedential court decisions given the context of a legal argument. In 96% of\nunseen test examples the correct target passage is among the top-10 predicted\npassages. The same model is able to predict relevant precedent given a short\nsummary of a complex and unseen legal brief, predicting the precedent that was\nactually cited by the brief's co-author, former U.S. Solicitor General and\ncurrent U.S. Supreme Court Justice Elena Kagan.",
          "link": "http://arxiv.org/abs/2106.16034",
          "publishedOn": "2021-07-01T01:59:34.535Z",
          "wordCount": 568,
          "title": "AutoLAW: Augmented Legal Reasoning through Legal Precedent Prediction. (arXiv:2106.16034v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "Commonsense inference to understand and explain human language is a\nfundamental research problem in natural language processing. Explaining human\nconversations poses a great challenge as it requires contextual understanding,\nplanning, inference, and several aspects of reasoning including causal,\ntemporal, and commonsense reasoning. In this work, we introduce CIDER -- a\nmanually curated dataset that contains dyadic dialogue explanations in the form\nof implicit and explicit knowledge triplets inferred using contextual\ncommonsense inference. Extracting such rich explanations from conversations can\nbe conducive to improving several downstream applications. The annotated\ntriplets are categorized by the type of commonsense knowledge present (e.g.,\ncausal, conditional, temporal). We set up three different tasks conditioned on\nthe annotated dataset: Dialogue-level Natural Language Inference, Span\nExtraction, and Multi-choice Span Selection. Baseline results obtained with\ntransformer-based models reveal that the tasks are difficult, paving the way\nfor promising future research. The dataset and the baseline implementations are\npublicly available at https://cider-task.github.io/cider/.",
          "link": "http://arxiv.org/abs/2106.00510",
          "publishedOn": "2021-07-01T01:59:34.123Z",
          "wordCount": 618,
          "title": "CIDER: Commonsense Inference for Dialogue Explanation and Reasoning. (arXiv:2106.00510v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1\">Pavel Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mager_M/0/1/0/all/0/1\">Manuel Mager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>",
          "description": "This paper describes the submission to the IWSLT 2021 Low-Resource Speech\nTranslation Shared Task by IMS team. We utilize state-of-the-art models\ncombined with several data augmentation, multi-task and transfer learning\napproaches for the automatic speech recognition (ASR) and machine translation\n(MT) steps of our cascaded system. Moreover, we also explore the feasibility of\na full end-to-end speech translation (ST) model in the case of very constrained\namount of ground truth labeled data. Our best system achieves the best\nperformance among all submitted systems for Congolese Swahili to English and\nFrench with BLEU scores 7.7 and 13.7 respectively, and the second best result\nfor Coastal Swahili to English with BLEU score 14.9.",
          "link": "http://arxiv.org/abs/2106.16055",
          "publishedOn": "2021-07-01T01:59:34.116Z",
          "wordCount": 559,
          "title": "IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task. (arXiv:2106.16055v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Egetenmeyer_J/0/1/0/all/0/1\">Jakob Egetenmeyer</a>",
          "description": "German and French football language display tense-aspect-mood (TAM) forms\nwhich differ from the TAM use in other genres. In German football talk, the\npresent indicative may replace the pluperfect subjunctive. In French reports of\nfootball matches, the imperfective past may occur instead of a perfective past\ntense-aspect form. We argue that the two phenomena share a functional core and\nare licensed in the same way, which is a direct result of the genre they occur\nin. More precisely, football match reports adhere to a precise script and\nspecific events are temporally determined in terms of objective time. This\nallows speakers to exploit a secondary function of TAM forms, namely, they\nshift the temporal perspective. We argue that it is on the grounds of the genre\nthat comprehenders predict the deviating forms and are also able to decode\nthem. We present various corpus studies where we explore the functioning of\nthese phenomena in order to gain insights into their distribution,\ngrammaticalization and their functioning in discourse. Relevant factors are\nAktionsart properties, rhetorical relations and their interaction with other\nTAM forms. This allows us to discuss coping mechanisms on the part of the\ncomprehender. We broaden our understanding of the phenomena, which have only\nbeen partly covered for French and up to now seem to have been ignored in\nGerman.",
          "link": "http://arxiv.org/abs/2106.15872",
          "publishedOn": "2021-07-01T01:59:34.081Z",
          "wordCount": 652,
          "title": "Genre determining prediction: Non-standard TAM marking in football language. (arXiv:2106.15872v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_S/0/1/0/all/0/1\">Sohail Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_V/0/1/0/all/0/1\">Valerio Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patti_V/0/1/0/all/0/1\">Viviana Patti</a>",
          "description": "Social media platforms provide users the freedom of expression and a medium\nto exchange information and express diverse opinions. Unfortunately, this has\nalso resulted in the growth of abusive content with the purpose of\ndiscriminating people and targeting the most vulnerable communities such as\nimmigrants, LGBT, Muslims, Jews and women. Because abusive language is\nsubjective in nature, there might be highly polarizing topics or events\ninvolved in the annotation of abusive contents such as hate speech (HS).\nTherefore, we need novel approaches to model conflicting perspectives and\nopinions coming from people with different personal and demographic\nbackgrounds. In this paper, we present an in-depth study to model polarized\nopinions coming from different communities under the hypothesis that similar\ncharacteristics (ethnicity, social background, culture etc.) can influence the\nperspectives of annotators on a certain phenomenon. We believe that by relying\non this information, we can divide the annotators into groups sharing similar\nperspectives. We can create separate gold standards, one for each group, to\ntrain state-of-the-art deep learning models. We can employ an ensemble approach\nto combine the perspective-aware classifiers from different groups to an\ninclusive model. We also propose a novel resource, a multi-perspective English\nlanguage dataset annotated according to different sub-categories relevant for\ncharacterising online abuse: hate speech, aggressiveness, offensiveness and\nstereotype. By training state-of-the-art deep learning models on this novel\nresource, we show how our approach improves the prediction performance of a\nstate-of-the-art supervised classifier.",
          "link": "http://arxiv.org/abs/2106.15896",
          "publishedOn": "2021-07-01T01:59:33.016Z",
          "wordCount": 686,
          "title": "Whose Opinions Matter? Perspective-aware Models to Identify Opinions of Hate Speech Victims in Abusive Language Detection. (arXiv:2106.15896v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2007.03834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bradley_T/0/1/0/all/0/1\">Tai-Danae Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlassopoulos_Y/0/1/0/all/0/1\">Yiannis Vlassopoulos</a>",
          "description": "This work originates from the observation that today's state of the art\nstatistical language models are impressive not only for their performance, but\nalso - and quite crucially - because they are built entirely from correlations\nin unstructured text data. The latter observation prompts a fundamental\nquestion that lies at the heart of this paper: What mathematical structure\nexists in unstructured text data? We put forth enriched category theory as a\nnatural answer. We show that sequences of symbols from a finite alphabet, such\nas those found in a corpus of text, form a category enriched over\nprobabilities. We then address a second fundamental question: How can this\ninformation be stored and modeled in a way that preserves the categorical\nstructure? We answer this by constructing a functor from our enriched category\nof text to a particular enriched category of reduced density operators. The\nlatter leverages the Loewner order on positive semidefinite operators, which\ncan further be interpreted as a toy example of entailment.",
          "link": "http://arxiv.org/abs/2007.03834",
          "publishedOn": "2021-07-01T01:59:32.858Z",
          "wordCount": 650,
          "title": "Language Modeling with Reduced Densities. (arXiv:2007.03834v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1\">Payal Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "In this paper, we introduce ELECTRA-style tasks to cross-lingual language\nmodel pre-training. Specifically, we present two pre-training tasks, namely\nmultilingual replaced token detection, and translation replaced token\ndetection. Besides, we pretrain the model, named as XLM-E, on both multilingual\nand parallel corpora. Our model outperforms the baseline models on various\ncross-lingual understanding tasks with much less computation cost. Moreover,\nanalysis shows that XLM-E tends to obtain better cross-lingual transferability.",
          "link": "http://arxiv.org/abs/2106.16138",
          "publishedOn": "2021-07-01T01:59:32.852Z",
          "wordCount": 507,
          "title": "XLM-E: Cross-lingual Language Model Pre-training via ELECTRA. (arXiv:2106.16138v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boenninghoff_B/0/1/0/all/0/1\">Benedikt Boenninghoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickel_R/0/1/0/all/0/1\">Robert M. Nickel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolossa_D/0/1/0/all/0/1\">Dorothea Kolossa</a>",
          "description": "The PAN 2021 authorship verification (AV) challenge is part of a three-year\nstrategy, moving from a cross-topic/closed-set to a cross-topic/open-set AV\ntask over a collection of fanfiction texts. In this work, we present our\nmodified hybrid neural-probabilistic framework. It is based on our 2020 winning\nsubmission, with updates to significantly reduce sensitivities to topical\nvariations and to further improve the system's calibration by means of an\nuncertainty-adaptation layer. Our framework additionally includes an\nOut-Of-Distribution Detector (O2D2) for defining non-responses, outperforming\nall other systems that participated in the PAN 2021 AV task.",
          "link": "http://arxiv.org/abs/2106.15825",
          "publishedOn": "2021-07-01T01:59:32.847Z",
          "wordCount": 528,
          "title": "O2D2: Out-Of-Distribution Detector to Capture Undecidable Trials in Authorship Verification. (arXiv:2106.15825v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rogoz_A/0/1/0/all/0/1\">Ana-Cristina Rogoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaman_M/0/1/0/all/0/1\">Mihaela Gaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "In this work, we introduce a corpus for satire detection in Romanian news. We\ngathered 55,608 public news articles from multiple real and satirical news\nsources, composing one of the largest corpora for satire detection regardless\nof language and the only one for the Romanian language. We provide an official\nsplit of the text samples, such that training news articles belong to different\nsources than test news articles, thus ensuring that models do not achieve high\nperformance simply due to overfitting. We conduct experiments with two\nstate-of-the-art deep neural models, resulting in a set of strong baselines for\nour novel corpus. Our results show that the machine-level accuracy for satire\ndetection in Romanian is quite low (under 73% on the test set) compared to the\nhuman-level accuracy (87%), leaving enough room for improvement in future\nresearch.",
          "link": "http://arxiv.org/abs/2105.06456",
          "publishedOn": "2021-07-01T01:59:32.824Z",
          "wordCount": 621,
          "title": "SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles. (arXiv:2105.06456v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1\">Maxine Eskenazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1\">Shikib Mehri</a>",
          "description": "Automatic evaluation metrics are a crucial component of dialog systems\nresearch. Standard language evaluation metrics are known to be ineffective for\nevaluating dialog. As such, recent research has proposed a number of novel,\ndialog-specific metrics that correlate better with human judgements. Due to the\nfast pace of research, many of these metrics have been assessed on different\ndatasets and there has as yet been no time for a systematic comparison between\nthem. To this end, this paper provides a comprehensive assessment of recently\nproposed dialog evaluation metrics on a number of datasets. In this paper, 17\ndifferent automatic evaluation metrics are evaluated on 10 different datasets.\nFurthermore, the metrics are assessed in different settings, to better qualify\ntheir respective strengths and weaknesses. Metrics are assessed (1) on both the\nturn level and the dialog level, (2) for different dialog lengths, (3) for\ndifferent dialog qualities (e.g., coherence, engaging), (4) for different types\nof response generation models (i.e., generative, retrieval, simple models and\nstate-of-the-art models), (5) taking into account the similarity of different\nmetrics and (6) exploring combinations of different metrics. This comprehensive\nassessment offers several takeaways pertaining to dialog evaluation metrics in\ngeneral. It also suggests how to best assess evaluation metrics and indicates\npromising directions for future work.",
          "link": "http://arxiv.org/abs/2106.03706",
          "publishedOn": "2021-07-01T01:59:32.792Z",
          "wordCount": 658,
          "title": "A Comprehensive Assessment of Dialog Evaluation Metrics. (arXiv:2106.03706v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Voskarides_N/0/1/0/all/0/1\">Nikos Voskarides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meij_E/0/1/0/all/0/1\">Edgar Meij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauer_S/0/1/0/all/0/1\">Sabrina Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "Writers such as journalists often use automatic tools to find relevant\ncontent to include in their narratives. In this paper, we focus on supporting\nwriters in the news domain to develop event-centric narratives. Given an\nincomplete narrative that specifies a main event and a context, we aim to\nretrieve news articles that discuss relevant events that would enable the\ncontinuation of the narrative. We formally define this task and propose a\nretrieval dataset construction procedure that relies on existing news articles\nto simulate incomplete narratives and relevant articles. Experiments on two\ndatasets derived from this procedure show that state-of-the-art lexical and\nsemantic rankers are not sufficient for this task. We show that combining those\nwith a ranker that ranks articles by reverse chronological order outperforms\nthose rankers alone. We also perform an in-depth quantitative and qualitative\nanalysis of the results that sheds light on the characteristics of this task.",
          "link": "http://arxiv.org/abs/2106.16053",
          "publishedOn": "2021-07-01T01:59:32.765Z",
          "wordCount": 592,
          "title": "News Article Retrieval in Context for Event-centric Narrative Creation. (arXiv:2106.16053v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh-Tung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan-Phi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>",
          "description": "We introduce a generic seq2seq parsing framework that casts constituency\nparsing problems (syntactic and discourse parsing) into a series of conditional\nsplitting decisions. Our parsing model estimates the conditional probability\ndistribution of possible splitting points in a given text span and supports\nefficient top-down decoding, which is linear in number of nodes. The\nconditional splitting formulation together with efficient beam search inference\nfacilitate structural consistency without relying on expensive structured\ninference. Crucially, for discourse analysis we show that in our formulation,\ndiscourse segmentation can be framed as a special case of parsing which allows\nus to perform discourse parsing without requiring segmentation as a\npre-requisite. Experiments show that our model achieves good results on the\nstandard syntactic parsing tasks under settings with/without pre-trained\nrepresentations and rivals state-of-the-art (SoTA) methods that are more\ncomputationally expensive than ours. In discourse parsing, our method\noutperforms SoTA by a good margin.",
          "link": "http://arxiv.org/abs/2106.15760",
          "publishedOn": "2021-07-01T01:59:32.756Z",
          "wordCount": 588,
          "title": "A Conditional Splitting Framework for Efficient Constituency Parsing. (arXiv:2106.15760v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1\">Dhiraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1\">Kartik Audhkhasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>",
          "description": "Current methods for learning visually grounded language from videos often\nrely on text annotation, such as human generated captions or machine generated\nautomatic speech recognition (ASR) transcripts. In this work, we introduce the\nAudio-Video Language Network (AVLnet), a self-supervised network that learns a\nshared audio-visual embedding space directly from raw video inputs. To\ncircumvent the need for text annotation, we learn audio-visual representations\nfrom randomly segmented video clips and their raw audio waveforms. We train\nAVLnet on HowTo100M, a large corpus of publicly available instructional videos,\nand evaluate on image retrieval and video retrieval tasks, achieving\nstate-of-the-art performance. We perform analysis of AVLnet's learned\nrepresentations, showing our model utilizes speech and natural sounds to learn\naudio-visual concepts. Further, we propose a tri-modal model that jointly\nprocesses raw audio, video, and text captions from videos to learn a\nmulti-modal semantic embedding space useful for text-video retrieval. Our code,\ndata, and trained models will be released at avlnet.csail.mit.edu",
          "link": "http://arxiv.org/abs/2006.09199",
          "publishedOn": "2021-07-01T01:59:32.577Z",
          "wordCount": 675,
          "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos. (arXiv:2006.09199v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmoudi_S/0/1/0/all/0/1\">Seyyed Ehsan Mahmoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>",
          "description": "In recent years there has been a special interest in word embeddings as a new\napproach to convert words to vectors. It has been a focal point to understand\nhow much of the semantics of the the words has been transferred into embedding\nvectors. This is important as the embedding is going to be used as the basis\nfor downstream NLP applications and it will be costly to evaluate the\napplication end-to-end in order to identify quality of the used embedding\nmodel. Generally the word embeddings are evaluated through a number of tests,\nincluding analogy test. In this paper we propose a test framework for Persian\nembedding models. Persian is a low resource language and there is no rich\nsemantic benchmark to evaluate word embedding models for this language. In this\npaper we introduce an evaluation framework including a hand crafted Persian SAT\nbased analogy dataset, a colliquial test set (specific to Persian) and a\nbenchmark to study the impact of various parameters on the semantic evaluation\ntask.",
          "link": "http://arxiv.org/abs/2106.15674",
          "publishedOn": "2021-07-01T01:59:32.518Z",
          "wordCount": 607,
          "title": "SAT Based Analogy Evaluation Framework for Persian Word Embeddings. (arXiv:2106.15674v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongkun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>",
          "description": "Conversational Question Simplification (CQS) aims to simplify self-contained\nquestions into conversational ones by incorporating some conversational\ncharacteristics, e.g., anaphora and ellipsis. Existing maximum likelihood\nestimation (MLE) based methods often get trapped in easily learned tokens as\nall tokens are treated equally during training. In this work, we introduce a\nReinforcement Iterative Sequence Editing (RISE) framework that optimizes the\nminimum Levenshtein distance (MLD) through explicit editing actions. RISE is\nable to pay attention to tokens that are related to conversational\ncharacteristics. To train RISE, we devise an Iterative Reinforce Training (IRT)\nalgorithm with a Dynamic Programming based Sampling (DPS) process to improve\nexploration. Experimental results on two benchmark datasets show that RISE\nsignificantly outperforms state-of-the-art methods and generalizes well on\nunseen data.",
          "link": "http://arxiv.org/abs/2106.15903",
          "publishedOn": "2021-07-01T01:59:32.475Z",
          "wordCount": 573,
          "title": "Learning to Ask Conversational Questions by Optimizing Levenshtein Distance. (arXiv:2106.15903v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>",
          "description": "Early risk detection of mental illnesses has a massive positive impact upon\nthe well-being of people. The eRisk workshop has been at the forefront of\nenabling interdisciplinary research in developing computational methods to\nautomatically estimate early risk factors for mental issues such as depression,\nself-harm, anorexia and pathological gambling. In this paper, we present the\ncontributions of the BLUE team in the 2021 edition of the workshop, in which we\ntackle the problems of early detection of gambling addiction, self-harm and\nestimating depression severity from social media posts. We employ pre-trained\nBERT transformers and data crawled automatically from mental health subreddits\nand obtain reasonable results on all three tasks.",
          "link": "http://arxiv.org/abs/2106.16175",
          "publishedOn": "2021-07-01T01:59:32.469Z",
          "wordCount": 564,
          "title": "Early Risk Detection of Pathological Gambling, Self-Harm and Depression Using BERT. (arXiv:2106.16175v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miao_S/0/1/0/all/0/1\">Shen-Yun Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chao-Chun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1\">Keh-Yih Su</a>",
          "description": "We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms\nof both language patterns and problem types) English math word problem (MWP)\ncorpus for evaluating the capability of various MWP solvers. Existing MWP\ncorpora for studying AI progress remain limited either in language usage\npatterns or in problem types. We thus present a new English MWP corpus with\n2,305 MWPs that cover more text patterns and most problem types taught in\nelementary school. Each MWP is annotated with its problem type and grade level\n(for indicating the level of difficulty). Furthermore, we propose a metric to\nmeasure the lexicon usage diversity of a given MWP corpus, and demonstrate that\nASDiv is more diverse than existing corpora. Experiments show that our proposed\ncorpus reflects the true capability of MWP solvers more faithfully.",
          "link": "http://arxiv.org/abs/2106.15772",
          "publishedOn": "2021-07-01T01:59:32.440Z",
          "wordCount": 574,
          "title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers. (arXiv:2106.15772v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>",
          "description": "In neural machine translation, cross entropy (CE) is the standard loss\nfunction in two training methods of auto-regressive models, i.e., teacher\nforcing and scheduled sampling. In this paper, we propose mixed cross entropy\nloss (mixed CE) as a substitute for CE in both training approaches. In teacher\nforcing, the model trained with CE regards the translation problem as a\none-to-one mapping process, while in mixed CE this process can be relaxed to\none-to-many. In scheduled sampling, we show that mixed CE has the potential to\nencourage the training and testing behaviours to be similar to each other, more\neffectively mitigating the exposure bias problem. We demonstrate the\nsuperiority of mixed CE over CE on several machine translation datasets, WMT'16\nRo-En, WMT'16 Ru-En, and WMT'14 En-De in both teacher forcing and scheduled\nsampling setups. Furthermore, in WMT'14 En-De, we also find mixed CE\nconsistently outperforms CE on a multi-reference set as well as a challenging\nparaphrased reference set. We also found the model trained with mixed CE is\nable to provide a better probability distribution defined over the translation\noutput space. Our code is available at https://github.com/haorannlp/mix.",
          "link": "http://arxiv.org/abs/2106.15880",
          "publishedOn": "2021-07-01T01:59:32.425Z",
          "wordCount": 619,
          "title": "Mixed Cross Entropy Loss for Neural Machine Translation. (arXiv:2106.15880v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1\">Casey Kennington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steenson_M/0/1/0/all/0/1\">McKenzie Steenson</a>",
          "description": "Automated speech and text interfaces are continuing to improve, resulting in\nincreased research in the area of dialogue systems. Moreover, conferences and\nworkshops from various fields are focusing more on language through speech and\ntext mediums as candidates for interaction with applications such as search\ninterfaces and robots. In this paper, we explore how visible the SigDial\nconference is to outside conferences by analysing papers from top Natural\nLangauge Processing conferences since 2015 to determine the popularity of\ncertain SigDial-related topics, as well as analysing what SigDial papers are\nbeing cited by others outside of SigDial. We find that despite a dramatic\nincrease in dialogue-related research, SigDial visibility has not increased. We\nconclude by offering some suggestions.",
          "link": "http://arxiv.org/abs/2106.16196",
          "publishedOn": "2021-07-01T01:59:32.007Z",
          "wordCount": 548,
          "title": "An Analysis of the Recent Visibility of the SigDial Conference. (arXiv:2106.16196v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Transformers have become a standard architecture for many NLP problems. This\nhas motivated theoretically analyzing their capabilities as models of language,\nin order to understand what makes them successful, and what their potential\nweaknesses might be. Recent work has shown that transformers with hard\nattention are quite limited in capacity, and in fact can be simulated by\nconstant-depth circuits. However, hard attention is a restrictive assumption,\nwhich may complicate the relevance of these results for practical transformers.\nIn this work, we analyze the circuit complexity of transformers with saturated\nattention: a generalization of hard attention that more closely captures the\nattention patterns learnable in practical transformers. We show that saturated\ntransformers transcend the limitations of hard-attention transformers. With\nsome minor assumptions, we prove that the number of bits needed to represent a\nsaturated transformer memory vector is $O(\\log n)$, which implies saturated\ntransformers can be simulated by log-depth circuits. Thus, the jump from hard\nto saturated attention can be understood as increasing the transformer's\neffective circuit depth by a factor of $O(\\log n)$.",
          "link": "http://arxiv.org/abs/2106.16213",
          "publishedOn": "2021-07-01T01:59:31.930Z",
          "wordCount": 622,
          "title": "On the Power of Saturated Transformers: A View from Circuit Complexity. (arXiv:2106.16213v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bilal_I/0/1/0/all/0/1\">Iman Munire Bilal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>",
          "description": "Collecting together microblogs representing opinions about the same topics\nwithin the same timeframe is useful to a number of different tasks and\npractitioners. A major question is how to evaluate the quality of such thematic\nclusters. Here we create a corpus of microblog clusters from three different\ndomains and time windows and define the task of evaluating thematic coherence.\nWe provide annotation guidelines and human annotations of thematic coherence by\njournalist experts. We subsequently investigate the efficacy of different\nautomated evaluation metrics for the task. We consider a range of metrics\nincluding surface level metrics, ones for topic model coherence and text\ngeneration metrics (TGMs). While surface level metrics perform well,\noutperforming topic coherence metrics, they are not as consistent as TGMs. TGMs\nare more reliable than all other metrics considered for capturing thematic\ncoherence in microblog clusters due to being less sensitive to the effect of\ntime windows.",
          "link": "http://arxiv.org/abs/2106.15971",
          "publishedOn": "2021-07-01T01:59:31.923Z",
          "wordCount": 592,
          "title": "Evaluation of Thematic Coherence in Microblogs. (arXiv:2106.15971v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_V/0/1/0/all/0/1\">Vincent Gao</a>",
          "description": "E-commerce stores collect customer feedback to let sellers learn about\ncustomer concerns and enhance customer order experience. Because customer\nfeedback often contains redundant information, a concise summary of the\nfeedback can be generated to help sellers better understand the issues causing\ncustomer dissatisfaction. Previous state-of-the-art abstractive text\nsummarization models make two major types of factual errors when producing\nsummaries from customer feedback, which are wrong entity detection (WED) and\nincorrect product-defect description (IPD). In this work, we introduce a set of\nmethods to enhance the factual consistency of abstractive summarization on\ncustomer feedback. We augment the training data with artificially corrupted\nsummaries, and use them as counterparts of the target summaries. We add a\ncontrastive loss term into the training objective so that the model learns to\navoid certain factual errors. Evaluation results show that a large portion of\nWED and IPD errors are alleviated for BART and T5. Furthermore, our approaches\ndo not depend on the structure of the summarization model and thus are\ngeneralizable to any abstractive summarization systems.",
          "link": "http://arxiv.org/abs/2106.16188",
          "publishedOn": "2021-07-01T01:59:31.768Z",
          "wordCount": 606,
          "title": "Improving Factual Consistency of Abstractive Summarization on Customer Feedback. (arXiv:2106.16188v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Josiah Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_J/0/1/0/all/0/1\">Josiel Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lala_C/0/1/0/all/0/1\">Chiraag Lala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>",
          "description": "This paper introduces a large-scale multimodal and multilingual dataset that\naims to facilitate research on grounding words to images in their contextual\nusage in language. The dataset consists of images selected to unambiguously\nillustrate concepts expressed in sentences from movie subtitles. The dataset is\na valuable resource as (i) the images are aligned to text fragments rather than\nwhole sentences; (ii) multiple images are possible for a text fragment and a\nsentence; (iii) the sentences are free-form and real-world like; (iv) the\nparallel texts are multilingual. We set up a fill-in-the-blank game for humans\nto evaluate the quality of the automatic image selection process of our\ndataset. We show the utility of the dataset on two automatic tasks: (i)\nfill-in-the blank; (ii) lexical translation. Results of the human evaluation\nand automatic models demonstrate that images can be a useful complement to the\ntextual context. The dataset will benefit research on visual grounding of words\nespecially in the context of free-form sentences, and can be obtained from\nhttps://doi.org/10.5281/zenodo.5034604 under a Creative Commons licence.",
          "link": "http://arxiv.org/abs/2103.01910",
          "publishedOn": "2021-07-01T01:59:31.758Z",
          "wordCount": 695,
          "title": "MultiSubs: A Large-scale Multimodal and Multilingual Dataset. (arXiv:2103.01910v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhiyuan_W/0/1/0/all/0/1\">Wen Zhiyuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiannong_C/0/1/0/all/0/1\">Cao Jiannong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruosong_Y/0/1/0/all/0/1\">Yang Ruosong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuaiqi_L/0/1/0/all/0/1\">Liu Shuaiqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiaxing_S/0/1/0/all/0/1\">Shen Jiaxing</a>",
          "description": "To provide consistent emotional interaction with users, dialog systems should\nbe capable to automatically select appropriate emotions for responses like\nhumans. However, most existing works focus on rendering specified emotions in\nresponses or empathetically respond to the emotion of users, yet the individual\ndifference in emotion expression is overlooked. This may lead to inconsistent\nemotional expressions and disinterest users. To tackle this issue, we propose\nto equip the dialog system with personality and enable it to automatically\nselect emotions in responses by simulating the emotion transition of humans in\nconversation. In detail, the emotion of the dialog system is transitioned from\nits preceding emotion in context. The transition is triggered by the preceding\ndialog context and affected by the specified personality trait. To achieve\nthis, we first model the emotion transition in the dialog system as the\nvariation between the preceding emotion and the response emotion in the\nValence-Arousal-Dominance (VAD) emotion space. Then, we design neural networks\nto encode the preceding dialog context and the specified personality traits to\ncompose the variation. Finally, the emotion for response is selected from the\nsum of the preceding emotion and the variation. We construct a dialog dataset\nwith emotion and personality labels and conduct emotion prediction tasks for\nevaluation. Experimental results validate the effectiveness of the\npersonality-affected emotion transition.",
          "link": "http://arxiv.org/abs/2106.15846",
          "publishedOn": "2021-07-01T01:59:31.750Z",
          "wordCount": 663,
          "title": "Automatically Select Emotion for Response via Personality-affected Emotion Transition. (arXiv:2106.15846v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_J/0/1/0/all/0/1\">Jian Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qianqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "This paper describes the systems submitted to IWSLT 2021 by the Volctrans\nteam. We participate in the offline speech translation and text-to-text\nsimultaneous translation tracks. For offline speech translation, our best\nend-to-end model achieves 8.1 BLEU improvements over the benchmark on the\nMuST-C test set and is even approaching the results of a strong cascade\nsolution. For text-to-text simultaneous translation, we explore the best\npractice to optimize the wait-k model. As a result, our final submitted systems\nexceed the benchmark at around 7 BLEU on the same latency regime. We will\npublish our code and model to facilitate both future research works and\nindustrial applications.\n\nThis paper describes the systems submitted to IWSLT 2021 by the Volctrans\nteam. We participate in the offline speech translation and text-to-text\nsimultaneous translation tracks. For offline speech translation, our best\nend-to-end model achieves 7.9 BLEU improvements over the benchmark on the\nMuST-C test set and is even approaching the results of a strong cascade\nsolution. For text-to-text simultaneous translation, we explore the best\npractice to optimize the wait-k model. As a result, our final submitted systems\nexceed the benchmark at around 7 BLEU on the same latency regime. We release\nour code and model at\n\\url{https://github.com/bytedance/neurst/tree/master/examples/iwslt21} to\nfacilitate both future research works and industrial applications.",
          "link": "http://arxiv.org/abs/2105.07319",
          "publishedOn": "2021-07-01T01:59:31.741Z",
          "wordCount": 701,
          "title": "The Volctrans Neural Speech Translation System for IWSLT 2021. (arXiv:2105.07319v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1\">David Grangier</a>",
          "description": "Whereas existing literature on unsupervised machine translation (MT) focuses\non exploiting unsupervised techniques for low-resource language pairs where\nbilingual training data is scare or unavailable, we investigate whether\nunsupervised MT can also improve translation quality of high-resource language\npairs where sufficient bitext does exist. We compare the style of correct\ntranslations generated by either supervised or unsupervised MT and find that\nthe unsupervised output is less monotonic and more natural than supervised\noutput. We demonstrate a way to combine the benefits of unsupervised and\nsupervised MT into a single system, resulting in better human evaluation of\nquality and fluency. Our results open the door to discussions about the\npotential contributions of unsupervised MT in high-resource settings, and how\nsupervised and unsupervised systems might be mutually-beneficial.",
          "link": "http://arxiv.org/abs/2106.15818",
          "publishedOn": "2021-07-01T01:59:31.718Z",
          "wordCount": 559,
          "title": "What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?. (arXiv:2106.15818v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Paheli Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1\">Soham Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudra_K/0/1/0/all/0/1\">Koustav Rudra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1\">Kripabandhu Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>",
          "description": "Automatic summarization of legal case documents is an important and practical\nchallenge. Apart from many domain-independent text summarization algorithms\nthat can be used for this purpose, several algorithms have been developed\nspecifically for summarizing legal case documents. However, most of the\nexisting algorithms do not systematically incorporate domain knowledge that\nspecifies what information should ideally be present in a legal case document\nsummary. To address this gap, we propose an unsupervised summarization\nalgorithm DELSumm which is designed to systematically incorporate guidelines\nfrom legal experts into an optimization setup. We conduct detailed experiments\nover case documents from the Indian Supreme Court. The experiments show that\nour proposed unsupervised method outperforms several strong baselines in terms\nof ROUGE scores, including both general summarization algorithms and\nlegal-specific ones. In fact, though our proposed algorithm is unsupervised, it\noutperforms several supervised summarization models that are trained over\nthousands of document-summary pairs.",
          "link": "http://arxiv.org/abs/2106.15876",
          "publishedOn": "2021-07-01T01:59:31.712Z",
          "wordCount": 602,
          "title": "Incorporating Domain Knowledge for Extractive Summarization of Legal Case Documents. (arXiv:2106.15876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baradaran_R/0/1/0/all/0/1\">Razieh Baradaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>",
          "description": "One of the main challenges of the machine reading comprehension (MRC) models\nis their fragile out-of-domain generalization, which makes these models not\nproperly applicable to real-world general-purpose question answering problems.\nIn this paper, we leverage a zero-shot weighted ensemble method for improving\nthe robustness of out-of-domain generalization in MRC models. In the proposed\nmethod, a weight estimation module is used to estimate out-of-domain weights,\nand an ensemble module aggregate several base models' predictions based on\ntheir weights. The experiments indicate that the proposed method not only\nimproves the final accuracy, but also is robust against domain changes.",
          "link": "http://arxiv.org/abs/2106.16013",
          "publishedOn": "2021-07-01T01:59:31.705Z",
          "wordCount": 541,
          "title": "Zero-Shot Estimation of Base Models' Weights in Ensemble of Machine Reading Comprehension Systems for Robust Generalization. (arXiv:2106.16013v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1\">Matej Ul&#x10d;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>",
          "description": "Building machine learning prediction models for a specific NLP task requires\nsufficient training data, which can be difficult to obtain for low-resource\nlanguages. Cross-lingual embeddings map word embeddings from a low-resource\nlanguage to a high-resource language so that a prediction model trained on data\nfrom the high-resource language can also be used in the low-resource language.\nTo produce cross-lingual mappings of recent contextual embeddings, anchor\npoints between the embedding spaces have to be words in the same context. We\naddress this issue with a new method for creating datasets for cross-lingual\ncontextual alignments. Based on that, we propose novel cross-lingual mapping\nmethods for ELMo embeddings. Our linear mapping methods use existing vecmap and\nMUSE alignments on contextual ELMo embeddings. Our new nonlinear ELMoGAN\nmapping method is based on GANs and does not assume isomorphic embedding\nspaces. We evaluate the proposed mapping methods on nine languages, using two\ndownstream tasks, NER and dependency parsing. The ELMoGAN method performs well\non the NER task, with low cross-lingual loss compared to direct training on\nsome languages. In the dependency parsing, linear alignment variants are more\nsuccessful.",
          "link": "http://arxiv.org/abs/2106.15986",
          "publishedOn": "2021-07-01T01:59:31.698Z",
          "wordCount": 611,
          "title": "Cross-lingual alignments of ELMo contextual embeddings. (arXiv:2106.15986v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenkai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hockenmaier_J/0/1/0/all/0/1\">Julia Hockenmaier</a>",
          "description": "Text-to-Graph extraction aims to automatically extract information graphs\nconsisting of mentions and types from natural language texts. Existing\napproaches, such as table filling and pairwise scoring, have shown impressive\nperformance on various information extraction tasks, but they are difficult to\nscale to datasets with longer input texts because of their second-order\nspace/time complexities with respect to the input length. In this work, we\npropose a Hybrid Span Generator (HySPA) that invertibly maps the information\ngraph to an alternating sequence of nodes and edge types, and directly\ngenerates such sequences via a hybrid span decoder which can decode both the\nspans and the types recurrently in linear time and space complexities.\nExtensive experiments on the ACE05 dataset show that our approach also\nsignificantly outperforms state-of-the-art on the joint entity and relation\nextraction task.",
          "link": "http://arxiv.org/abs/2106.15838",
          "publishedOn": "2021-07-01T01:59:31.691Z",
          "wordCount": 569,
          "title": "HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction. (arXiv:2106.15838v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1\">Nikolay Bogoychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pinzhen Chen</a>",
          "description": "Machine translation systems are vulnerable to domain mismatch, especially\nwhen the task is low-resource. In this setting, out of domain translations are\noften of poor quality and prone to hallucinations, due to the translation model\npreferring to predict common words it has seen during training, as opposed to\nthe more uncommon ones from a different domain. We present two simple methods\nfor improving translation quality in this particular setting: First, we use\nlexical shortlisting in order to restrict the neural network predictions by IBM\nmodel computed alignments. Second, we perform $n$-best list reordering by\nreranking all translations based on the amount they overlap with each other.\nOur methods are computationally simpler and faster than alternative approaches,\nand show a moderate success on low-resource settings with explicit out of\ndomain test sets. However, our methods lose their effectiveness when the domain\nmismatch is too great, or in high resource setting.",
          "link": "http://arxiv.org/abs/2101.00421",
          "publishedOn": "2021-07-01T01:59:31.657Z",
          "wordCount": 606,
          "title": "Decoding Time Lexical Domain Adaptation for Neural Machine Translation. (arXiv:2101.00421v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1\">Thibault Sellam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadlowsky_S/0/1/0/all/0/1\">Steve Yadlowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmour_A/0/1/0/all/0/1\">Alexander D&#x27;Amour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1\">Jasmijn Bastings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turc_I/0/1/0/all/0/1\">Iulia Turc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenney_I/0/1/0/all/0/1\">Ian Tenney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>",
          "description": "Experiments with pretrained models such as BERT are often based on a single\ncheckpoint. While the conclusions drawn apply to the artifact (i.e., the\nparticular instance of the model), it is not always clear whether they hold for\nthe more general procedure (which includes the model architecture, training\ndata, initialization scheme, and loss function). Recent work has shown that\nre-running pretraining can lead to substantially different conclusions about\nperformance, suggesting that alternative evaluations are needed to make\nprincipled statements about procedures. To address this question, we introduce\nMultiBERTs: a set of 25 BERT-base checkpoints, trained with similar\nhyper-parameters as the original BERT model but differing in random\ninitialization and data shuffling. The aim is to enable researchers to draw\nrobust and statistically justified conclusions about pretraining procedures.\nThe full release includes 25 fully trained checkpoints, as well as statistical\nguidelines and a code library implementing our recommended hypothesis testing\nmethods. Finally, for five of these models we release a set of 28 intermediate\ncheckpoints in order to support research on learning dynamics.",
          "link": "http://arxiv.org/abs/2106.16163",
          "publishedOn": "2021-07-01T01:59:31.644Z",
          "wordCount": 624,
          "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis. (arXiv:2106.16163v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1\">Gautam Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1\">Milind Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dheram_P/0/1/0/all/0/1\">Pranav Dheram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Bryan Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_B/0/1/0/all/0/1\">Bach Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>",
          "description": "We propose an end-to-end trained spoken language understanding (SLU) system\nthat extracts transcripts, intents and slots from an input speech utterance. It\nconsists of a streaming recurrent neural network transducer (RNNT) based\nautomatic speech recognition (ASR) model connected to a neural natural language\nunderstanding (NLU) model through a neural interface. This interface allows for\nend-to-end training using multi-task RNNT and NLU losses. Additionally, we\nintroduce semantic sequence loss training for the joint RNNT-NLU system that\nallows direct optimization of non-differentiable SLU metrics. This end-to-end\nSLU model paradigm can leverage state-of-the-art advancements and pretrained\nmodels in both ASR and NLU research communities, outperforming recently\nproposed direct speech-to-semantics models, and conventional pipelined ASR and\nNLU systems. We show that this method improves both ASR and NLU metrics on both\npublic SLU datasets and large proprietary datasets.",
          "link": "http://arxiv.org/abs/2106.15919",
          "publishedOn": "2021-07-01T01:59:31.560Z",
          "wordCount": 582,
          "title": "End-to-End Spoken Language Understanding using RNN-Transducer ASR. (arXiv:2106.15919v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rohanian_M/0/1/0/all/0/1\">Morteza Rohanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hough_J/0/1/0/all/0/1\">Julian Hough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>",
          "description": "We present two multimodal fusion-based deep learning models that consume ASR\ntranscribed speech and acoustic data simultaneously to classify whether a\nspeaker in a structured diagnostic task has Alzheimer's Disease and to what\ndegree, evaluating the ADReSSo challenge 2021 data. Our best model, a BiLSTM\nwith highway layers using words, word probabilities, disfluency features, pause\ninformation, and a variety of acoustic features, achieves an accuracy of 84%\nand RSME error prediction of 4.26 on MMSE cognitive scores. While predicting\ncognitive decline is more challenging, our models show improvement using the\nmultimodal approach and word probabilities, disfluency and pause information\nover word-only models. We show considerable gains for AD classification using\nmultimodal fusion and gating, which can effectively deal with noisy inputs from\nacoustic features and ASR hypotheses.",
          "link": "http://arxiv.org/abs/2106.15684",
          "publishedOn": "2021-07-01T01:59:31.528Z",
          "wordCount": 592,
          "title": "Alzheimer's Dementia Recognition Using Acoustic, Lexical, Disfluency and Speech Pause Features Robust to Noisy Inputs. (arXiv:2106.15684v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turc_I/0/1/0/all/0/1\">Iulia Turc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>",
          "description": "Despite their success, large pre-trained multilingual models have not\ncompletely alleviated the need for labeled data, which is cumbersome to collect\nfor all target languages. Zero-shot cross-lingual transfer is emerging as a\npractical solution: pre-trained models later fine-tuned on one transfer\nlanguage exhibit surprising performance when tested on many target languages.\nEnglish is the dominant source language for transfer, as reinforced by popular\nzero-shot benchmarks. However, this default choice has not been systematically\nvetted. In our study, we compare English against other transfer languages for\nfine-tuning, on two pre-trained multilingual models (mBERT and mT5) and\nmultiple classification and question answering tasks. We find that other\nhigh-resource languages such as German and Russian often transfer more\neffectively, especially when the set of target languages is diverse or unknown\na priori. Unexpectedly, this can be true even when the training sets were\nautomatically translated from English. This finding can have immediate impact\non multilingual zero-shot systems, and should inform future benchmark designs.",
          "link": "http://arxiv.org/abs/2106.16171",
          "publishedOn": "2021-07-01T01:59:31.491Z",
          "wordCount": 596,
          "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer. (arXiv:2106.16171v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>",
          "description": "Medical coding translates professionally written medical reports into\nstandardized codes, which is an essential part of medical information systems\nand health insurance reimbursement. Manual coding by trained human coders is\ntime-consuming and error-prone. Thus, automated coding algorithms have been\ndeveloped, building especially on the recent advances in machine learning and\ndeep neural networks. To solve the challenges of encoding lengthy and noisy\nclinical documents and capturing code associations, we propose a multitask\nrecalibrated aggregation network. In particular, multitask learning shares\ninformation across different coding schemes and captures the dependencies\nbetween different medical codes. Feature recalibration and aggregation in\nshared modules enhance representation learning for lengthy notes. Experiments\nwith a real-world MIMIC-III dataset show significantly improved predictive\nperformance.",
          "link": "http://arxiv.org/abs/2104.00952",
          "publishedOn": "2021-06-30T02:01:00.504Z",
          "wordCount": 589,
          "title": "Multitask Recalibrated Aggregation Network for Medical Code Prediction. (arXiv:2104.00952v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.07755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eberts_M/0/1/0/all/0/1\">Markus Eberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulges_A/0/1/0/all/0/1\">Adrian Ulges</a>",
          "description": "We introduce SpERT, an attention model for span-based joint entity and\nrelation extraction. Our key contribution is a light-weight reasoning on BERT\nembeddings, which features entity recognition and filtering, as well as\nrelation classification with a localized, marker-free context representation.\nThe model is trained using strong within-sentence negative samples, which are\nefficiently extracted in a single BERT pass. These aspects facilitate a search\nover all spans in the sentence.\n\nIn ablation studies, we demonstrate the benefits of pre-training, strong\nnegative sampling and localized context. Our model outperforms prior work by up\nto 2.6% F1 score on several datasets for joint entity and relation extraction.",
          "link": "http://arxiv.org/abs/1909.07755",
          "publishedOn": "2021-06-30T02:01:00.463Z",
          "wordCount": 618,
          "title": "Span-based Joint Entity and Relation Extraction with Transformer Pre-training. (arXiv:1909.07755v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conklin_H/0/1/0/all/0/1\">Henry Conklin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kenny Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>",
          "description": "Natural language is compositional; the meaning of a sentence is a function of\nthe meaning of its parts. This property allows humans to create and interpret\nnovel sentences, generalizing robustly outside their prior experience. Neural\nnetworks have been shown to struggle with this kind of generalization, in\nparticular performing poorly on tasks designed to assess compositional\ngeneralization (i.e. where training and testing distributions differ in ways\nthat would be trivial for a compositional strategy to resolve). Their poor\nperformance on these tasks may in part be due to the nature of supervised\nlearning which assumes training and testing data to be drawn from the same\ndistribution. We implement a meta-learning augmented version of supervised\nlearning whose objective directly optimizes for out-of-distribution\ngeneralization. We construct pairs of tasks for meta-learning by sub-sampling\nexisting training data. Each pair of tasks is constructed to contain relevant\nexamples, as determined by a similarity metric, in an effort to inhibit models\nfrom memorizing their input. Experimental results on the COGS and SCAN datasets\nshow that our similarity-driven meta-learning can improve generalization\nperformance.",
          "link": "http://arxiv.org/abs/2106.04252",
          "publishedOn": "2021-06-30T02:01:00.446Z",
          "wordCount": 625,
          "title": "Meta-Learning to Compositionally Generalize. (arXiv:2106.04252v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>",
          "description": "Recently generating natural language explanations has shown very promising\nresults in not only offering interpretable explanations but also providing\nadditional information and supervision for prediction. However, existing\napproaches usually require a large set of human annotated explanations for\ntraining while collecting a large set of explanations is not only time\nconsuming but also expensive. In this paper, we develop a general framework for\ninterpretable natural language understanding that requires only a small set of\nhuman annotated explanations for training. Our framework treats natural\nlanguage explanations as latent variables that model the underlying reasoning\nprocess of a neural model. We develop a variational EM framework for\noptimization where an explanation generation module and an\nexplanation-augmented prediction module are alternatively optimized and\nmutually enhance each other. Moreover, we further propose an explanation-based\nself-training method under this framework for semi-supervised learning. It\nalternates between assigning pseudo-labels to unlabeled data and generating new\nexplanations to iteratively improve each other. Experiments on two natural\nlanguage understanding tasks demonstrate that our framework can not only make\neffective predictions in both supervised and semi-supervised settings, but also\ngenerate good natural language explanation.",
          "link": "http://arxiv.org/abs/2011.05268",
          "publishedOn": "2021-06-30T02:01:00.441Z",
          "wordCount": 670,
          "title": "Towards Interpretable Natural Language Understanding with Explanations as Latent Variables. (arXiv:2011.05268v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08454",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>",
          "description": "Recent advances in automatic speech recognition (ASR) have achieved accuracy\nlevels comparable to human transcribers, which led researchers to debate if the\nmachine has reached human performance. Previous work focused on the English\nlanguage and modular hidden Markov model-deep neural network (HMM-DNN) systems.\nIn this paper, we perform a comprehensive benchmarking for end-to-end\ntransformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the\nArabic language and its dialects. For the HSR, we evaluate linguist performance\nand lay-native speaker performance on a new dataset collected as a part of this\nstudy. For ASR the end-to-end work led to 12.5%, 27.5%, 33.8% WER; a new\nperformance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our\nresults suggest that human performance in the Arabic language is still\nconsiderably better than the machine with an absolute WER gap of 3.5% on\naverage.",
          "link": "http://arxiv.org/abs/2101.08454",
          "publishedOn": "2021-06-30T02:01:00.419Z",
          "wordCount": 608,
          "title": "Arabic Speech Recognition by End-to-End, Modular Systems and Human. (arXiv:2101.08454v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajput_G/0/1/0/all/0/1\">Gaurav Rajput</a>, <a href=\"http://arxiv.org/find/cs/1/au:+punn_N/0/1/0/all/0/1\">Narinder Singh punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "With increasing popularity of social media platforms hate speech is emerging\nas a major concern, where it expresses abusive speech that targets specific\ngroup characteristics, such as gender, religion or ethnicity to spread\nviolence. Earlier people use to verbally deliver hate speeches but now with the\nexpansion of technology, some people are deliberately using social media\nplatforms to spread hate by posting, sharing, commenting, etc. Whether it is\nChristchurch mosque shootings or hate crimes against Asians in west, it has\nbeen observed that the convicts are very much influenced from hate text present\nonline. Even though AI systems are in place to flag such text but one of the\nkey challenges is to reduce the false positive rate (marking non hate as hate),\nso that these systems can detect hate speech without undermining the freedom of\nexpression. In this paper, we use ETHOS hate speech detection dataset and\nanalyze the performance of hate speech detection classifier by replacing or\nintegrating the word embeddings (fastText (FT), GloVe (GV) or FT + GV) with\nstatic BERT embeddings (BE). With the extensive experimental trails it is\nobserved that the neural network performed better with static BE compared to\nusing FT, GV or FT + GV as word embeddings. In comparison to fine-tuned BERT,\none metric that significantly improved is specificity.",
          "link": "http://arxiv.org/abs/2106.15537",
          "publishedOn": "2021-06-30T02:01:00.389Z",
          "wordCount": 655,
          "title": "Hate speech detection using static BERT embeddings. (arXiv:2106.15537v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1\">Evangelos Kanoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "Electronic health record (EHR) coding is the task of assigning ICD codes to\neach EHR. Most previous studies either only focus on the frequent ICD codes or\ntreat rare and frequent ICD codes in the same way. These methods perform well\non frequent ICD codes but due to the extremely unbalanced distribution of ICD\ncodes, the performance on rare ones is far from satisfactory. We seek to\nimprove the performance for both frequent and rare ICD codes by using a\ncontrastive graph-based EHR coding framework, CoGraph, which re-casts EHR\ncoding as a few-shot learning task. First, we construct a heterogeneous EHR\nword-entity (HEWE) graph for each EHR, where the words and entities extracted\nfrom an EHR serve as nodes and the relations between them serve as edges. Then,\nCoGraph learns similarities and dissimilarities between HEWE graphs from\ndifferent ICD codes so that information can be transferred among them. In a\nfew-shot learning scenario, the model only has access to frequent ICD codes\nduring training, which might force it to encode features that are useful for\nfrequent ICD codes only. To mitigate this risk, CoGraph devises two graph\ncontrastive learning schemes, GSCL and GECL, that exploit the HEWE graph\nstructures so as to encode transferable features. GSCL utilizes the\nintra-correlation of different sub-graphs sampled from HEWE graphs while GECL\nexploits the inter-correlation among HEWE graphs at different clinical stages.\nExperiments on the MIMIC-III benchmark dataset show that CoGraph significantly\noutperforms state-of-the-art methods on EHR coding, not only on frequent ICD\ncodes, but also on rare codes, in terms of several evaluation indicators. On\nfrequent ICD codes, GSCL and GECL improve the classification accuracy and F1 by\n1.31% and 0.61%, respectively, and on rare ICD codes CoGraph has more obvious\nimprovements by 2.12% and 2.95%.",
          "link": "http://arxiv.org/abs/2106.15467",
          "publishedOn": "2021-06-30T02:01:00.382Z",
          "wordCount": 738,
          "title": "Few-Shot Electronic Health Record Coding through Graph Contrastive Learning. (arXiv:2106.15467v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
          "link": "http://arxiv.org/abs/2106.15561",
          "publishedOn": "2021-06-30T02:01:00.372Z",
          "wordCount": 607,
          "title": "A Survey on Neural Speech Synthesis. (arXiv:2106.15561v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maniar_T/0/1/0/all/0/1\">Tabish Maniar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akkinepally_A/0/1/0/all/0/1\">Alekhya Akkinepally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anantha Sharma</a>",
          "description": "The use of machine learning algorithms to model user behavior and drive\nbusiness decisions has become increasingly commonplace, specifically providing\nintelligent recommendations to automated decision making. This has led to an\nincrease in the use of customers personal data to analyze customer behavior and\npredict their interests in a companys products. Increased use of this customer\npersonal data can lead to better models but also to the potential of customer\ndata being leaked, reverse engineered, and mishandled. In this paper, we assess\ndifferential privacy as a solution to address these privacy problems by\nbuilding privacy protections into the data engineering and model training\nstages of predictive model development. Our interest is a pragmatic\nimplementation in an operational environment, which necessitates a general\npurpose differentially private modeling framework, and we evaluate one such\ntool from LeapYear as applied to the Credit Risk modeling domain. Credit Risk\nModel is a major modeling methodology in banking and finance where user data is\nanalyzed to determine the total Expected Loss to the bank. We examine the\napplication of differential privacy on the credit risk model and evaluate the\nperformance of a Differentially Private Model with a Non Differentially Private\nModel. Credit Risk Model is a major modeling methodology in banking and finance\nwhere users data is analyzed to determine the total Expected Loss to the bank.\nIn this paper, we explore the application of differential privacy on the credit\nrisk model and evaluate the performance of a Non Differentially Private Model\nwith Differentially Private Model.",
          "link": "http://arxiv.org/abs/2106.15343",
          "publishedOn": "2021-06-30T02:01:00.362Z",
          "wordCount": 697,
          "title": "Differential Privacy for Credit Risk Model. (arXiv:2106.15343v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marie_B/0/1/0/all/0/1\">Benjamin Marie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_A/0/1/0/all/0/1\">Atsushi Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubino_R/0/1/0/all/0/1\">Raphael Rubino</a>",
          "description": "This paper presents the first large-scale meta-evaluation of machine\ntranslation (MT). We annotated MT evaluations conducted in 769 research papers\npublished from 2010 to 2020. Our study shows that practices for automatic MT\nevaluation have dramatically changed during the past decade and follow\nconcerning trends. An increasing number of MT evaluations exclusively rely on\ndifferences between BLEU scores to draw conclusions, without performing any\nkind of statistical significance testing nor human evaluation, while at least\n108 metrics claiming to be better than BLEU have been proposed. MT evaluations\nin recent papers tend to copy and compare automatic metric scores from previous\nwork to claim the superiority of a method or an algorithm without confirming\nneither exactly the same training, validating, and testing data have been used\nnor the metric scores are comparable. Furthermore, tools for reporting\nstandardized metric scores are still far from being widely adopted by the MT\ncommunity. After showing how the accumulation of these pitfalls leads to\ndubious evaluation, we propose a guideline to encourage better automatic MT\nevaluation along with a simple meta-evaluation scoring method to assess its\ncredibility.",
          "link": "http://arxiv.org/abs/2106.15195",
          "publishedOn": "2021-06-30T02:01:00.339Z",
          "wordCount": 622,
          "title": "Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers. (arXiv:2106.15195v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Radstok_W/0/1/0/all/0/1\">Wessel Radstok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chekol_M/0/1/0/all/0/1\">Mel Chekol</a>",
          "description": "The inclusion of temporal scopes of facts in knowledge graph embedding (KGE)\npresents significant opportunities for improving the resulting embeddings, and\nconsequently for increased performance in downstream applications. Yet, little\nresearch effort has focussed on this area and much of the carried out research\nreports only marginally improved results compared to models trained without\ntemporal scopes (static models). Furthermore, rather than leveraging existing\nwork on static models, they introduce new models specific to temporal knowledge\ngraphs. We propose a novel perspective that takes advantage of the power of\nexisting static embedding models by focussing effort on manipulating the data\ninstead. Our method, SpliMe, draws inspiration from the field of signal\nprocessing and early work in graph embedding. We show that SpliMe competes with\nor outperforms the current state of the art in temporal KGE. Additionally, we\nuncover issues with the procedure currently used to assess the performance of\nstatic models on temporal graphs and introduce two ways to counteract them.",
          "link": "http://arxiv.org/abs/2106.15223",
          "publishedOn": "2021-06-30T02:01:00.320Z",
          "wordCount": 598,
          "title": "Leveraging Static Models for Link Prediction in Temporal Knowledge Graphs. (arXiv:2106.15223v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_W/0/1/0/all/0/1\">Wenbin Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1\">Hannah Danner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>",
          "description": "Social media offer plenty of information to perform market research in order\nto meet the requirements of customers. One way how this research is conducted\nis that a domain expert gathers and categorizes user-generated content into a\ncomplex and fine-grained class structure. In many of such cases, little data\nmeets complex annotations. It is not yet fully understood how this can be\nleveraged successfully for classification. We examine the classification\naccuracy of expert labels when used with a) many fine-grained classes and b)\nfew abstract classes. For scenario b) we compare abstract class labels given by\nthe domain expert as baseline and by automatic hierarchical clustering. We\ncompare this to another baseline where the entire class structure is given by a\ncompletely unsupervised clustering approach. By doing so, this work can serve\nas an example of how complex expert annotations are potentially beneficial and\ncan be utilized in the most optimal way for opinion mining in highly specific\ndomains. By exploring across a range of techniques and experiments, we find\nthat automated class abstraction approaches in particular the unsupervised\napproach performs remarkably well against domain expert baseline on text\nclassification tasks. This has the potential to inspire opinion mining\napplications in order to support market researchers in practice and to inspire\nfine-grained automated content analysis on a large scale.",
          "link": "http://arxiv.org/abs/2106.15498",
          "publishedOn": "2021-06-30T02:01:00.311Z",
          "wordCount": 657,
          "title": "Classification of Consumer Belief Statements From Social Media. (arXiv:2106.15498v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Issam_K/0/1/0/all/0/1\">Kalliath Abdul Rasheed Issam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shivam Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1\">Subalalitha C. N</a>",
          "description": "Text summarization is an approach for identifying important information\npresent within text documents. This computational technique aims to generate\nshorter versions of the source text, by including only the relevant and salient\ninformation present within the source text. In this paper, we propose a novel\nmethod to summarize a text document by clustering its contents based on latent\ntopics produced using topic modeling techniques and by generating extractive\nsummaries for each of the identified text clusters. All extractive\nsub-summaries are later combined to generate a summary for any given source\ndocument. We utilize the lesser used and challenging WikiHow dataset in our\napproach to text summarization. This dataset is unlike the commonly used news\ndatasets which are available for text summarization. The well-known news\ndatasets present their most important information in the first few lines of\ntheir source texts, which make their summarization a lesser challenging task\nwhen compared to summarizing the WikiHow dataset. Contrary to these news\ndatasets, the documents in the WikiHow dataset are written using a generalized\napproach and have lesser abstractedness and higher compression ratio, thus\nproposing a greater challenge to generate summaries. A lot of the current\nstate-of-the-art text summarization techniques tend to eliminate important\ninformation present in source documents in the favor of brevity. Our proposed\ntechnique aims to capture all the varied information present in source\ndocuments. Although the dataset proved challenging, after performing extensive\ntests within our experimental setup, we have discovered that our model produces\nencouraging ROUGE results and summaries when compared to the other published\nextractive and abstractive text summarization models.",
          "link": "http://arxiv.org/abs/2106.15313",
          "publishedOn": "2021-06-30T02:01:00.304Z",
          "wordCount": 718,
          "title": "Topic Modeling Based Extractive Text Summarization. (arXiv:2106.15313v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Linyi_Y/0/1/0/all/0/1\">Yang Linyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiazheng_L/0/1/0/all/0/1\">Li Jiazheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padraig_C/0/1/0/all/0/1\">Cunningham P&#xe1;draig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barry_S/0/1/0/all/0/1\">Smyth Barry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruihai_D/0/1/0/all/0/1\">Dong Ruihai</a>",
          "description": "While state-of-the-art NLP models have been achieving the excellent\nperformance of a wide range of tasks in recent years, important questions are\nbeing raised about their robustness and their underlying sensitivity to\nsystematic biases that may exist in their training and test data. Such issues\ncome to be manifest in performance problems when faced with out-of-distribution\ndata in the field. One recent solution has been to use counterfactually\naugmented datasets in order to reduce any reliance on spurious patterns that\nmay exist in the original data. Producing high-quality augmented data can be\ncostly and time-consuming as it usually needs to involve human feedback and\ncrowdsourcing efforts. In this work, we propose an alternative by describing\nand evaluating an approach to automatically generating counterfactual data for\ndata augmentation and explanation. A comprehensive evaluation on several\ndifferent datasets and using a variety of state-of-the-art benchmarks\ndemonstrate how our approach can achieve significant improvements in model\nperformance when compared to models training on the original data and even when\ncompared to models trained with the benefit of human-generated augmented data.",
          "link": "http://arxiv.org/abs/2106.15231",
          "publishedOn": "2021-06-30T02:01:00.295Z",
          "wordCount": 632,
          "title": "Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis. (arXiv:2106.15231v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1\">Ana Valeria Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>",
          "description": "A myriad of explainability methods have been proposed in recent years, but\nthere is little consensus on how to evaluate them. While automatic metrics\nallow for quick benchmarking, it isn't clear how such metrics reflect human\ninteraction with explanations. Human evaluation is of paramount importance, but\nprevious protocols fail to account for belief biases affecting human\nperformance, which may lead to misleading conclusions. We provide an overview\nof belief bias, its role in human evaluation, and ideas for NLP practitioners\non how to account for it. For two experimental paradigms, we present a case\nstudy of gradient-based explainability introducing simple ways to account for\nhumans' prior beliefs: models of varying quality and adversarial examples. We\nshow that conclusions about the highest performing methods change when\nintroducing such controls, pointing to the importance of accounting for belief\nbias in evaluation.",
          "link": "http://arxiv.org/abs/2106.15355",
          "publishedOn": "2021-06-30T02:01:00.279Z",
          "wordCount": 581,
          "title": "On the Interaction of Belief Bias and Explanations. (arXiv:2106.15355v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15153",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jinhyeok Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Jae-Sung Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bak_T/0/1/0/all/0/1\">Taejun Bak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1\">Youngik Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1\">Hoon-Young Cho</a>",
          "description": "Recent advances in neural multi-speaker text-to-speech (TTS) models have\nenabled the generation of reasonably good speech quality with a single model\nand made it possible to synthesize the speech of a speaker with limited\ntraining data. Fine-tuning to the target speaker data with the multi-speaker\nmodel can achieve better quality, however, there still exists a gap compared to\nthe real speech sample and the model depends on the speaker. In this work, we\npropose GANSpeech, which is a high-fidelity multi-speaker TTS model that adopts\nthe adversarial training method to a non-autoregressive multi-speaker TTS\nmodel. In addition, we propose simple but efficient automatic scaling methods\nfor feature matching loss used in adversarial training. In the subjective\nlistening tests, GANSpeech significantly outperformed the baseline\nmulti-speaker FastSpeech and FastSpeech2 models, and showed a better MOS score\nthan the speaker-specific fine-tuned FastSpeech2.",
          "link": "http://arxiv.org/abs/2106.15153",
          "publishedOn": "2021-06-30T02:01:00.260Z",
          "wordCount": 595,
          "title": "GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis. (arXiv:2106.15153v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ochieng_P/0/1/0/all/0/1\">Peter Ochieng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mugambi_D/0/1/0/all/0/1\">Dennis Mugambi</a>",
          "description": "Conversational machine reading (CMR) tools have seen a rapid progress in the\nrecent past. The current existing tools rely on the supervised learning\ntechnique which require labeled dataset for their training. The supervised\ntechnique necessitates that for every new rule text, a manually labeled dataset\nmust be created. This is tedious and error prone. This paper introduces and\ndemonstrates how unsupervised learning technique can be applied in the\ndevelopment of CMR. Specifically, we demonstrate how unsupervised learning can\nbe used in rule extraction and entailment modules of CMR. Compared to the\ncurrent best CMR tool, our developed framework reports 3.3% improvement in\nmicro averaged accuracy and 1.4 % improvement in macro averaged accuracy.",
          "link": "http://arxiv.org/abs/2106.15247",
          "publishedOn": "2021-06-30T02:01:00.252Z",
          "wordCount": 540,
          "title": "Unsupervised Technique To Conversational Machine Reading. (arXiv:2106.15247v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammoud_J/0/1/0/all/0/1\">Jaafar Hammoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatian_A/0/1/0/all/0/1\">Aleksandra Vatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobrenko_N/0/1/0/all/0/1\">Natalia Dobrenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedernikov_N/0/1/0/all/0/1\">Nikolai Vedernikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalyto_A/0/1/0/all/0/1\">Anatoly Shalyto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusarova_N/0/1/0/all/0/1\">Natalia Gusarova</a>",
          "description": "The Arabic language suffers from a great shortage of datasets suitable for\ntraining deep learning models, and the existing ones include general\nnon-specialized classifications. In this work, we introduce a new Arab medical\ndataset, which includes two thousand medical documents collected from several\nArabic medical websites, in addition to the Arab Medical Encyclopedia. The\ndataset was built for the task of classifying texts and includes 10 classes\n(Blood, Bone, Cardiovascular, Ear, Endocrine, Eye, Gastrointestinal, Immune,\nLiver and Nephrological) diseases. Experiments on the dataset were performed by\nfine-tuning three pre-trained models: BERT from Google, Arabert that based on\nBERT with large Arabic corpus, and AraBioNER that based on Arabert with Arabic\nmedical corpus.",
          "link": "http://arxiv.org/abs/2106.15236",
          "publishedOn": "2021-06-30T02:01:00.239Z",
          "wordCount": 544,
          "title": "New Arabic Medical Dataset for Diseases Classification. (arXiv:2106.15236v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1\">Meihan Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>",
          "description": "Few-shot Named Entity Recognition (NER) exploits only a handful of\nannotations to identify and classify named entity mentions. Prototypical\nnetwork shows superior performance on few-shot NER. However, existing\nprototypical methods fail to differentiate rich semantics in other-class words,\nwhich will aggravate overfitting under few shot scenario. To address the issue,\nwe propose a novel model, Mining Undefined Classes from Other-class (MUCO),\nthat can automatically induce different undefined classes from the other class\nto improve few-shot NER. With these extra-labeled undefined classes, our method\nwill improve the discriminative ability of NER classifier and enhance the\nunderstanding of predefined classes with stand-by semantic knowledge.\nExperimental results demonstrate that our model outperforms five\nstate-of-the-art models in both 1-shot and 5-shots settings on four NER\nbenchmarks. We will release the code upon acceptance. The source code is\nreleased on https: //github.com/shuaiwa16/OtherClassNER.git.",
          "link": "http://arxiv.org/abs/2106.15167",
          "publishedOn": "2021-06-30T02:01:00.224Z",
          "wordCount": 578,
          "title": "Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition. (arXiv:2106.15167v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "The evaluation of neural machine translation systems is usually built upon\ngenerated translation of a certain decoding method (e.g., beam search) with\nevaluation metrics over the generated translation (e.g., BLEU). However, this\nevaluation framework suffers from high search errors brought by heuristic\nsearch algorithms and is limited by its nature of evaluation over one best\ncandidate. In this paper, we propose a novel evaluation protocol, which not\nonly avoids the effect of search errors but provides a system-level evaluation\nin the perspective of model ranking. In particular, our method is based on our\nnewly proposed exact top-$k$ decoding instead of beam search. Our approach\nevaluates model errors by the distance between the candidate spaces scored by\nthe references and the model respectively. Extensive experiments on WMT'14\nEnglish-German demonstrate that bad ranking ability is connected to the\nwell-known beam search curse, and state-of-the-art Transformer models are\nfacing serious ranking errors. By evaluating various model architectures and\ntechniques, we provide several interesting findings. Finally, to effectively\napproximate the exact search algorithm with same time cost as original beam\nsearch, we present a minimum heap augmented beam search algorithm.",
          "link": "http://arxiv.org/abs/2106.15217",
          "publishedOn": "2021-06-30T02:01:00.215Z",
          "wordCount": 624,
          "title": "Rethinking the Evaluation of Neural Machine Translation. (arXiv:2106.15217v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Ashish Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Samarth Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khare_S/0/1/0/all/0/1\">Shreya Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1\">Saneem Chemmengath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_K/0/1/0/all/0/1\">Karthik Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>",
          "description": "Spoken intent detection has become a popular approach to interface with\nvarious smart devices with ease. However, such systems are limited to the\npreset list of intents-terms or commands, which restricts the quick\ncustomization of personal devices to new intents. This paper presents a\nfew-shot spoken intent classification approach with task-agnostic\nrepresentations via meta-learning paradigm. Specifically, we leverage the\npopular representation-based meta-learning learning to build a task-agnostic\nrepresentation of utterances, that then use a linear classifier for prediction.\nWe evaluate three such approaches on our novel experimental protocol developed\non two popular spoken intent classification datasets: Google Commands and the\nFluent Speech Commands dataset. For a 5-shot (1-shot) classification of novel\nclasses, the proposed framework provides an average classification accuracy of\n88.6% (76.3%) on the Google Commands dataset, and 78.5% (64.2%) on the Fluent\nSpeech Commands dataset. The performance is comparable to traditionally\nsupervised classification models with abundant training samples.",
          "link": "http://arxiv.org/abs/2106.15238",
          "publishedOn": "2021-06-30T02:01:00.164Z",
          "wordCount": 609,
          "title": "Representation based meta-learning for few-shot spoken intent recognition. (arXiv:2106.15238v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">M Zeeshan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Tanvir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1\">M M Sufyan Beg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikram_A/0/1/0/all/0/1\">Asma Ikram</a>",
          "description": "The conventional natural language processing approaches are not accustomed to\nthe social media text due to colloquial discourse and non-homogeneous\ncharacteristics. Significantly, the language identification in a multilingual\ndocument is ascertained to be a preceding subtask in several information\nextraction applications such as information retrieval, named entity\nrecognition, relation extraction, etc. The problem is often more challenging in\ncode-mixed documents wherein foreign languages words are drawn into base\nlanguage while framing the text. The word embeddings are powerful language\nmodeling tools for representation of text documents useful in obtaining\nsimilarity between words or documents. We present a simple probabilistic\napproach for building efficient word embedding for code-mixed text and\nexemplifying it over language identification of Hindi-English short test\nmessages scrapped from Twitter. We examine its efficacy for the classification\ntask using bidirectional LSTMs and SVMs and observe its improved scores over\nvarious existing code-mixed embeddings",
          "link": "http://arxiv.org/abs/2106.15102",
          "publishedOn": "2021-06-30T02:01:00.130Z",
          "wordCount": 583,
          "title": "A Simple and Efficient Probabilistic Language model for Code-Mixed Text. (arXiv:2106.15102v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">En-Shiun Annie Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skenduli_M/0/1/0/all/0/1\">Marjana Prifti Skenduli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_R/0/1/0/all/0/1\">Ravi Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehreen Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Rishemjit Kaur</a>",
          "description": "Neural Machine Translation (NMT) has seen a tremendous spurt of growth in\nless than ten years, and has already entered a mature phase. While considered\nas the most widely used solution for Machine Translation, its performance on\nlow-resource language pairs still remains sub-optimal compared to the\nhigh-resource counterparts, due to the unavailability of large parallel\ncorpora. Therefore, the implementation of NMT techniques for low-resource\nlanguage pairs has been receiving the spotlight in the recent NMT research\narena, thus leading to a substantial amount of research reported on this topic.\nThis paper presents a detailed survey of research advancements in low-resource\nlanguage NMT (LRL-NMT), along with a quantitative analysis aimed at identifying\nthe most popular solutions. Based on our findings from reviewing previous work,\nthis survey paper provides a set of guidelines to select the possible NMT\ntechnique for a given LRL data setting. It also presents a holistic view of the\nLRL-NMT research landscape and provides a list of recommendations to further\nenhance the research efforts on LRL-NMT.",
          "link": "http://arxiv.org/abs/2106.15115",
          "publishedOn": "2021-06-30T02:01:00.108Z",
          "wordCount": 618,
          "title": "Neural Machine Translation for Low-Resource Languages: A Survey. (arXiv:2106.15115v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>",
          "description": "Neural text generation models are typically trained by maximizing\nlog-likelihood with the sequence cross entropy loss, which encourages an exact\ntoken-by-token match between a target sequence with a generated sequence. Such\ntraining objective is sub-optimal when the target sequence not perfect, e.g.,\nwhen the target sequence is corrupted with noises, or when only weak sequence\nsupervision is available. To address this challenge, we propose a novel\nEdit-Invariant Sequence Loss (EISL), which computes the matching loss of a\ntarget n-gram with all n-grams in the generated sequence. EISL draws\ninspirations from convolutional networks (ConvNets) which are shift-invariant\nto images, hence is robust to the shift of n-grams to tolerate edits in the\ntarget sequences. Moreover, the computation of EISL is essentially a\nconvolution operation with target n-grams as kernels, which is easy to\nimplement with existing libraries. To demonstrate the effectiveness of EISL, we\nconduct experiments on three tasks: machine translation with noisy target\nsequences, unsupervised text style transfer, and non-autoregressive machine\ntranslation. Experimental results show our method significantly outperforms\ncross entropy loss on these three tasks.",
          "link": "http://arxiv.org/abs/2106.15078",
          "publishedOn": "2021-06-30T02:01:00.100Z",
          "wordCount": 631,
          "title": "Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostapenko_A/0/1/0/all/0/1\">Alissa Ostapenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1\">Vijay Viswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>",
          "description": "Decomposable tasks are complex and comprise of a hierarchy of sub-tasks.\nSpoken intent prediction, for example, combines automatic speech recognition\nand natural language understanding. Existing benchmarks, however, typically\nhold out examples for only the surface-level sub-task. As a result, models with\nsimilar performance on these benchmarks may have unobserved performance\ndifferences on the other sub-tasks. To allow insightful comparisons between\ncompetitive end-to-end architectures, we propose a framework to construct\nrobust test sets using coordinate ascent over sub-task specific utility\nfunctions. Given a dataset for a decomposable task, our method optimally\ncreates a test set for each sub-task to individually assess sub-components of\nthe end-to-end model. Using spoken language understanding as a case study, we\ngenerate new splits for the Fluent Speech Commands and Snips SmartLights\ndatasets. Each split has two test sets: one with held-out utterances assessing\nnatural language understanding abilities, and one with held-out speakers to\ntest speech processing skills. Our splits identify performance gaps up to 10%\nbetween end-to-end systems that were within 1% of each other on the original\ntest sets. These performance gaps allow more realistic and actionable\ncomparisons between different architectures, driving future model development.\nWe release our splits and tools for the community.",
          "link": "http://arxiv.org/abs/2106.15065",
          "publishedOn": "2021-06-30T02:01:00.084Z",
          "wordCount": 662,
          "title": "Rethinking End-to-End Evaluation of Decomposable Tasks: A Case Study on Spoken Language Understanding. (arXiv:2106.15065v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Mohd Zeeshan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Tanvir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_N/0/1/0/all/0/1\">Noaima Bari</a>",
          "description": "Language Identification in textual documents is the process of automatically\ndetecting the language contained in a document based on its content. The\npresent Language Identification techniques presume that a document contains\ntext in one of the fixed set of languages, however, this presumption is\nincorrect when dealing with multilingual document which includes content in\nmore than one possible language. Due to the unavailability of large standard\ncorpora for Hindi-English mixed lingual language processing tasks we propose\nthe language lexicons, a novel kind of lexical database that supports several\nmultilingual language processing tasks. These lexicons are built by learning\nclassifiers over transliterated Hindi and English vocabulary. The designed\nlexicons possess richer quantitative characteristic than its primary source of\ncollection which is revealed using the visualization techniques.",
          "link": "http://arxiv.org/abs/2106.15105",
          "publishedOn": "2021-06-30T02:01:00.044Z",
          "wordCount": 553,
          "title": "Language Lexicons for Hindi-English Multilingual Text Processing. (arXiv:2106.15105v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1\">Anastasios Nentidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsimpras_G/0/1/0/all/0/1\">Georgios Katsimpras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandorou_E/0/1/0/all/0/1\">Eirini Vandorou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1\">Anastasia Krithara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasco_L/0/1/0/all/0/1\">Luis Gasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krallinger_M/0/1/0/all/0/1\">Martin Krallinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>",
          "description": "Advancing the state-of-the-art in large-scale biomedical semantic indexing\nand question answering is the main focus of the BioASQ challenge. BioASQ\norganizes respective tasks where different teams develop systems that are\nevaluated on the same benchmark datasets that represent the real information\nneeds of experts in the biomedical domain. This paper presents an overview of\nthe ninth edition of the BioASQ challenge in the context of the Conference and\nLabs of the Evaluation Forum (CLEF) 2021. In this year, a new question\nanswering task, named Synergy, is introduced to support researchers studying\nthe COVID-19 disease and measure the ability of the participating teams to\ndiscern information while the problem is still developing. In total, 42 teams\nwith more than 170 systems were registered to participate in the four tasks of\nthe challenge. The evaluation results, similarly to previous years, show a\nperformance gain against the baselines which indicates the continuous\nimprovement of the state-of-the-art in this field.",
          "link": "http://arxiv.org/abs/2106.14885",
          "publishedOn": "2021-06-30T02:01:00.037Z",
          "wordCount": 677,
          "title": "Overview of BioASQ 2021: The ninth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering. (arXiv:2106.14885v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1\">Shangqing Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1\">Tong Cui</a>",
          "description": "Wikipedia abstract generation aims to distill a Wikipedia abstract from web\nsources and has met significant success by adopting multi-document\nsummarization techniques. However, previous works generally view the abstract\nas plain text, ignoring the fact that it is a description of a certain entity\nand can be decomposed into different topics. In this paper, we propose a\ntwo-stage model TWAG that guides the abstract generation with topical\ninformation. First, we detect the topic of each input paragraph with a\nclassifier trained on existing Wikipedia articles to divide input documents\ninto different topics. Then, we predict the topic distribution of each abstract\nsentence, and decode the sentence from topic-aware representations with a\nPointer-Generator network. We evaluate our model on the WikiCatSum dataset, and\nthe results show that \\modelnames outperforms various existing baselines and is\ncapable of generating comprehensive abstracts. Our code and dataset can be\naccessed at \\url{https://github.com/THU-KEG/TWAG}",
          "link": "http://arxiv.org/abs/2106.15135",
          "publishedOn": "2021-06-30T02:00:59.970Z",
          "wordCount": 585,
          "title": "TWAG: A Topic-Guided Wikipedia Abstract Generator. (arXiv:2106.15135v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gillis_N/0/1/0/all/0/1\">Noa Baker Gillis</a>",
          "description": "We analyze 6.7 million case law documents to determine the presence of gender\nbias within our judicial system. We find that current bias detectino methods in\nNLP are insufficient to determine gender bias in our case law database and\npropose an alternative approach. We show that existing algorithms' inconsistent\nresults are consequences of prior research's definition of biases themselves.\nBias detection algorithms rely on groups of words to represent bias (e.g.,\n'salary,' 'job,' and 'boss' to represent employment as a potentially biased\ntheme against women in text). However, the methods to build these groups of\nwords have several weaknesses, primarily that the word lists are based on the\nresearchers' own intuitions. We suggest two new methods of automating the\ncreation of word lists to represent biases. We find that our methods outperform\ncurrent NLP bias detection methods. Our research improves the capabilities of\nNLP technology to detect bias and highlights gender biases present in\ninfluential case law. In order test our NLP bias detection method's\nperformance, we regress our results of bias in case law against U.S census data\nof women's participation in the workforce in the last 100 years.",
          "link": "http://arxiv.org/abs/2106.15103",
          "publishedOn": "2021-06-30T02:00:59.960Z",
          "wordCount": 617,
          "title": "Sexism in the Judiciary. (arXiv:2106.15103v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">Jeremy R. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillick_D/0/1/0/all/0/1\">Daniel Gillick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>",
          "description": "Many facts come with an expiration date, from the name of the President to\nthe basketball team Lebron James plays for. But language models (LMs) are\ntrained on snapshots of data collected at a specific moment in time, and this\ncan limit their utility, especially in the closed-book setting where the\npretraining corpus must contain the facts the model should memorize. We\nintroduce a diagnostic dataset aimed at probing LMs for factual knowledge that\nchanges over time and highlight problems with LMs at either end of the spectrum\n-- those trained on specific slices of temporal data, as well as those trained\non a wide range of temporal data. To mitigate these problems, we propose a\nsimple technique for jointly modeling text with its timestamp. This improves\nmemorization of seen facts from the training time period, as well as\ncalibration on predictions about unseen facts from future time periods. We also\nshow that models trained with temporal context can be efficiently ``refreshed''\nas new data arrives, without the need for retraining from scratch.",
          "link": "http://arxiv.org/abs/2106.15110",
          "publishedOn": "2021-06-30T02:00:59.929Z",
          "wordCount": 610,
          "title": "Time-Aware Language Models as Temporal Knowledge Bases. (arXiv:2106.15110v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghong Li</a>",
          "description": "Generating high-quality and diverse essays with a set of topics is a\nchallenging task in natural language generation. Since several given topics\nonly provide limited source information, utilizing various topic-related\nknowledge is essential for improving essay generation performance. However,\nprevious works cannot sufficiently use that knowledge to facilitate the\ngeneration procedure. This paper aims to improve essay generation by extracting\ninformation from both internal and external knowledge. Thus, a topic-to-essay\ngeneration model with comprehensive knowledge enhancement, named TEGKE, is\nproposed. For internal knowledge enhancement, both topics and related essays\nare fed to a teacher network as source information. Then, informative features\nwould be obtained from the teacher network and transferred to a student network\nwhich only takes topics as input but provides comparable information compared\nwith the teacher network. For external knowledge enhancement, a topic knowledge\ngraph encoder is proposed. Unlike the previous works only using the nearest\nneighbors of topics in the commonsense base, our topic knowledge graph encoder\ncould exploit more structural and semantic information of the commonsense\nknowledge graph to facilitate essay generation. Moreover, the adversarial\ntraining based on the Wasserstein distance is proposed to improve generation\nquality. Experimental results demonstrate that TEGKE could achieve\nstate-of-the-art performance on both automatic and human evaluation.",
          "link": "http://arxiv.org/abs/2106.15142",
          "publishedOn": "2021-06-30T02:00:59.920Z",
          "wordCount": 635,
          "title": "Topic-to-Essay Generation with Comprehensive Knowledge Enhancement. (arXiv:2106.15142v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Junyi Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yujie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_H/0/1/0/all/0/1\">Homa Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parveen_D/0/1/0/all/0/1\">Daraksha Parveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondapally_R/0/1/0/all/0/1\">Ranganath Kondapally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjin Xu</a>",
          "description": "In this paper, we present an automatic knowledge base construction system\nfrom large scale enterprise documents with minimal efforts of human\nintervention. In the design and deployment of such a knowledge mining system\nfor enterprise, we faced several challenges including data distributional\nshift, performance evaluation, compliance requirements and other practical\nissues. We leveraged state-of-the-art deep learning models to extract\ninformation (named entities and definitions) at per document level, then\nfurther applied classical machine learning techniques to process global\nstatistical information to improve the knowledge base. Experimental results are\nreported on actual enterprise documents. This system is currently serving as\npart of a Microsoft 365 service.",
          "link": "http://arxiv.org/abs/2106.15085",
          "publishedOn": "2021-06-30T02:00:59.898Z",
          "wordCount": 539,
          "title": "Automatic Construction of Enterprise Knowledge Base. (arXiv:2106.15085v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianxing Yu</a>",
          "description": "Dialogue policy learning, a subtask that determines the content of system\nresponse generation and then the degree of task completion, is essential for\ntask-oriented dialogue systems. However, the unbalanced distribution of system\nactions in dialogue datasets often causes difficulty in learning to generate\ndesired actions and responses. In this paper, we propose a\nretrieve-and-memorize framework to enhance the learning of system actions.\nSpecially, we first design a neural context-aware retrieval module to retrieve\nmultiple candidate system actions from the training set given a dialogue\ncontext. Then, we propose a memory-augmented multi-decoder network to generate\nthe system actions conditioned on the candidate actions, which allows the\nnetwork to adaptively select key information in the candidate actions and\nignore noises. We conduct experiments on the large-scale multi-domain\ntask-oriented dialogue dataset MultiWOZ 2.0 and MultiWOZ 2.1. Experimental\nresults show that our method achieves competitive performance among several\nstate-of-the-art models in the context-to-response generation task.",
          "link": "http://arxiv.org/abs/2106.02317",
          "publishedOn": "2021-06-29T01:55:14.378Z",
          "wordCount": 607,
          "title": "Retrieve & Memorize: Dialog Policy Learning with Multi-Action Memory. (arXiv:2106.02317v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>",
          "description": "Recent developments in Text Style Transfer have led this field to be more\nhighlighted than ever. The task of transferring an input's style to another is\naccompanied by plenty of challenges (e.g., fluency and content preservation)\nthat need to be taken care of. In this research, we introduce PGST, a novel\npolyglot text style transfer approach in the gender domain, composed of\ndifferent constitutive elements. In contrast to prior studies, it is feasible\nto apply a style transfer method in multiple languages by fulfilling our\nmethod's predefined elements. We have proceeded with a pre-trained word\nembedding for token replacement purposes, a character-based token classifier\nfor gender exchange purposes, and a beam search algorithm for extracting the\nmost fluent combination. Since different approaches are introduced in our\nresearch, we determine a trade-off value for evaluating different models'\nsuccess in faking our gender identification model with transferred text. To\ndemonstrate our method's multilingual applicability, we applied our method on\nboth English and Persian corpora and ended up defeating our proposed gender\nidentification model by 45.6% and 39.2%, respectively. While this research's\nfocus is not limited to a specific language, our obtained evaluation results\nare highly competitive in an analogy among English state of the art methods.",
          "link": "http://arxiv.org/abs/2009.01040",
          "publishedOn": "2021-06-29T01:55:14.265Z",
          "wordCount": 673,
          "title": "PGST: a Polyglot Gender Style Transfer method. (arXiv:2009.01040v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhichao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>",
          "description": "Research on overlapped and discontinuous named entity recognition (NER) has\nreceived increasing attention. The majority of previous work focuses on either\noverlapped or discontinuous entities. In this paper, we propose a novel\nspan-based model that can recognize both overlapped and discontinuous entities\njointly. The model includes two major steps. First, entity fragments are\nrecognized by traversing over all possible text spans, thus, overlapped\nentities can be recognized. Second, we perform relation classification to judge\nwhether a given pair of entity fragments to be overlapping or succession. In\nthis way, we can recognize not only discontinuous entities, and meanwhile\ndoubly check the overlapped entities. As a whole, our model can be regarded as\na relation extraction paradigm essentially. Experimental results on multiple\nbenchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly\ncompetitive for overlapped and discontinuous NER.",
          "link": "http://arxiv.org/abs/2106.14373",
          "publishedOn": "2021-06-29T01:55:14.249Z",
          "wordCount": 586,
          "title": "A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition. (arXiv:2106.14373v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haipang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1\">Sanhui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongfeng Li</a>",
          "description": "We present a knowledge-grounded dialog system developed for the ninth Dialog\nSystem Technology Challenge (DSTC9) Track 1 - Beyond Domain APIs: Task-oriented\nConversational Modeling with Unstructured Knowledge Access. We leverage\ntransfer learning with existing language models to accomplish the tasks in this\nchallenge track. Specifically, we divided the task into four sub-tasks and\nfine-tuned several Transformer models on each of the sub-tasks. We made\nadditional changes that yielded gains in both performance and efficiency,\nincluding the combination of the model with traditional entity-matching\ntechniques, and the addition of a pointer network to the output layer of the\nlanguage model.",
          "link": "http://arxiv.org/abs/2106.14444",
          "publishedOn": "2021-06-29T01:55:14.236Z",
          "wordCount": 539,
          "title": "A Knowledge-Grounded Dialog System Based on Pre-Trained Language Models. (arXiv:2106.14444v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1\">Yash Bhartia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suthaharan_S/0/1/0/all/0/1\">Shan Suthaharan</a>",
          "description": "Toxicity detection of text has been a popular NLP task in the recent years.\nIn SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic\nspans within passages. Most state-of-the-art span detection approaches employ\nvarious techniques, each of which can be broadly classified into Token\nClassification or Span Prediction approaches. In our paper, we explore simple\nversions of both of these approaches and their performance on the task.\nSpecifically, we use BERT-based models -- BERT, RoBERTa, and SpanBERT for both\napproaches. We also combine these approaches and modify them to bring\nimprovements for Toxic Spans prediction. To this end, we investigate results on\nfour hybrid approaches -- Multi-Span, Span+Token, LSTM-CRF, and a combination\nof predicted offsets using union/intersection. Additionally, we perform a\nthorough ablative analysis and analyze our observed results. Our best\nsubmission -- a combination of SpanBERT Span Predictor and RoBERTa Token\nClassifier predictions -- achieves an F1 score of 0.6753 on the test set. Our\nbest post-eval F1 score is 0.6895 on intersection of predicted offsets from\ntop-3 RoBERTa Token Classification checkpoints. These approaches improve the\nperformance by 3% on average than those of the shared baseline models -- RNNSL\nand SpaCy NER.",
          "link": "http://arxiv.org/abs/2102.12254",
          "publishedOn": "2021-06-29T01:55:14.161Z",
          "wordCount": 685,
          "title": "NLRG at SemEval-2021 Task 5: Toxic Spans Detection Leveraging BERT-based Token Classification and Span Prediction Techniques. (arXiv:2102.12254v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Automatically generating radiology reports can improve current clinical\npractice in diagnostic radiology. On one hand, it can relieve radiologists from\nthe heavy burden of report writing; On the other hand, it can remind\nradiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.\nYet, this task remains a challenging job for data-driven neural networks, due\nto the serious visual and textual data biases. To this end, we propose a\nPosterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to\nimitate the working patterns of radiologists, who will first examine the\nabnormal regions and assign the disease topic tags to the abnormal regions, and\nthen rely on the years of prior medical knowledge and prior working experience\naccumulations to write reports. Thus, the PPKED includes three modules:\nPosterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and\nMulti-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior\nknowledge, which provides explicit abnormal visual regions to alleviate visual\ndata bias; PrKE explores the prior knowledge from the prior medical knowledge\ngraph (medical knowledge) and prior radiology reports (working experience) to\nalleviate textual data bias. The explored knowledge is distilled by the MKD to\ngenerate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our\nmethod is able to outperform previous state-of-the-art models on these two\ndatasets.",
          "link": "http://arxiv.org/abs/2106.06963",
          "publishedOn": "2021-06-29T01:55:14.155Z",
          "wordCount": 689,
          "title": "Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation. (arXiv:2106.06963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingtao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuejiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "It is common for people to create different types of charts to explore a\nmulti-dimensional dataset (table). However, to recommend commonly composed\ncharts in real world, one should take the challenges of efficiency, imbalanced\ndata and table context into consideration. In this paper, we propose\nTable2Charts framework which learns common patterns from a large corpus of\n(table, charts) pairs. Based on deep Q-learning with copying mechanism and\nheuristic searching, Table2Charts does table-to-sequence generation, where each\nsequence follows a chart template. On a large spreadsheet corpus with 165k\ntables and 266k charts, we show that Table2Charts could learn a shared\nrepresentation of table fields so that recommendation tasks on different chart\ntypes could mutually enhance each other. Table2Charts outperforms other chart\nrecommendation systems in both multi-type task (with doubled recall numbers\nR@3=0.61 and R@1=0.43) and human evaluations.",
          "link": "http://arxiv.org/abs/2008.11015",
          "publishedOn": "2021-06-29T01:55:14.104Z",
          "wordCount": 641,
          "title": "Table2Charts: Recommending Charts by Learning Shared Table Representations. (arXiv:2008.11015v4 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>",
          "description": "Given the prevalence of pre-trained contextualized representations in today's\nNLP, there have been several efforts to understand what information such\nrepresentations contain. A common strategy to use such representations is to\nfine-tune them for an end task. However, how fine-tuning for a task changes the\nunderlying space is less studied. In this work, we study the English BERT\nfamily and use two probing techniques to analyze how fine-tuning changes the\nspace. Our experiments reveal that fine-tuning improves performance because it\npushes points associated with a label away from other labels. By comparing the\nrepresentations before and after fine-tuning, we also discover that fine-tuning\ndoes not change the representations arbitrarily; instead, it adjusts the\nrepresentations to downstream tasks while preserving the original structure.\nFinally, using carefully constructed experiments, we show that fine-tuning can\nencode training sets in a representation, suggesting an overfitting problem of\na new kind.",
          "link": "http://arxiv.org/abs/2106.14282",
          "publishedOn": "2021-06-29T01:55:14.079Z",
          "wordCount": 574,
          "title": "A Closer Look at How Fine-tuning Changes BERT. (arXiv:2106.14282v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arseniev_Koehler_A/0/1/0/all/0/1\">Alina Arseniev-Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochran_S/0/1/0/all/0/1\">Susan D. Cochran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mays_V/0/1/0/all/0/1\">Vickie M. Mays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jacob Gates Foster</a>",
          "description": "There is an escalating need for methods to identify latent patterns in text\ndata from many domains. We introduce a new method to identify topics in a\ncorpus and represent documents as topic sequences. Discourse Atom Topic\nModeling draws on advances in theoretical machine learning to integrate topic\nmodeling and word embedding, capitalizing on the distinct capabilities of each.\nWe first identify a set of vectors (\"discourse atoms\") that provide a sparse\nrepresentation of an embedding space. Atom vectors can be interpreted as latent\ntopics: Through a generative model, atoms map onto distributions over words;\none can also infer the topic that generated a sequence of words. We illustrate\nour method with a prominent example of underutilized text: the U.S. National\nViolent Death Reporting System (NVDRS). The NVDRS summarizes violent death\nincidents with structured variables and unstructured narratives. We identify\n225 latent topics in the narratives (e.g., preparation for death and physical\naggression); many of these topics are not captured by existing structured\nvariables. Motivated by known patterns in suicide and homicide by gender, and\nrecent research on gender biases in semantic space, we identify the gender bias\nof our topics (e.g., a topic about pain medication is feminine). We then\ncompare the gender bias of topics to their prevalence in narratives of female\nversus male victims. Results provide a detailed quantitative picture of\nreporting about lethal violence and its gendered nature. Our method offers a\nflexible and broadly applicable approach to model topics in text data.",
          "link": "http://arxiv.org/abs/2106.14365",
          "publishedOn": "2021-06-29T01:55:14.072Z",
          "wordCount": 698,
          "title": "Integrating topic modeling and word embedding to characterize violent deaths. (arXiv:2106.14365v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Devansh Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>",
          "description": "Multimodal Machine Translation (MMT) enriches the source text with visual\ninformation for translation. It has gained popularity in recent years, and\nseveral pipelines have been proposed in the same direction. Yet, the task lacks\nquality datasets to illustrate the contribution of visual modality in the\ntranslation systems. In this paper, we propose our system under the team name\nVolta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We\nalso participate in the textual-only subtask of the same language pair for\nwhich we use mBART, a pretrained multilingual sequence-to-sequence model. For\nmultimodal translation, we propose to enhance the textual input by bringing the\nvisual information to a textual domain by extracting object tags from the\nimage. We also explore the robustness of our system by systematically degrading\nthe source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test\nset and challenge set of the multimodal task.",
          "link": "http://arxiv.org/abs/2106.00250",
          "publishedOn": "2021-06-29T01:55:13.966Z",
          "wordCount": 625,
          "title": "ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Arkadipta De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1\">Tanik Saikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "In this article, we present a description of our systems as a part of our\nparticipation in the shared task namely Artificial Intelligence for Legal\nAssistance (AILA 2019). This is an integral event of Forum for Information\nRetrieval Evaluation-2019. The outcomes of this track would be helpful for the\nautomation of the working process of the Indian Judiciary System. The manual\nworking procedures and documentation at any level (from lower to higher court)\nof the judiciary system are very complex in nature. The systems produced as a\npart of this track would assist the law practitioners. It would be helpful for\ncommon men too. This kind of track also opens the path of research of Natural\nLanguage Processing (NLP) in the judicial domain. This track defined two\nproblems such as Task 1: Identifying relevant prior cases for a given situation\nand Task 2: Identifying the most relevant statutes for a given situation. We\ntackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As\nper the results declared by the task organizers, we are in 3rd and a modest\nposition in Task 1 and Task 2 respectively.",
          "link": "http://arxiv.org/abs/2105.11347",
          "publishedOn": "2021-06-29T01:55:13.954Z",
          "wordCount": 669,
          "title": "IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task. (arXiv:2105.11347v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1\">Yash Bhartia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>",
          "description": "In this article, we present our methodologies for SemEval-2021 Task-4:\nReading Comprehension of Abstract Meaning. Given a fill-in-the-blank-type\nquestion and a corresponding context, the task is to predict the most suitable\nword from a list of 5 options. There are three sub-tasks within this task:\nImperceptibility (subtask-I), Non-Specificity (subtask-II), and Intersection\n(subtask-III). We use encoders of transformers-based models pre-trained on the\nmasked language modelling (MLM) task to build our Fill-in-the-blank (FitB)\nmodels. Moreover, to model imperceptibility, we define certain linguistic\nfeatures, and to model non-specificity, we leverage information from hypernyms\nand hyponyms provided by a lexical database. Specifically, for non-specificity,\nwe try out augmentation techniques, and other statistical techniques. We also\npropose variants, namely Chunk Voting and Max Context, to take care of input\nlength restrictions for BERT, etc. Additionally, we perform a thorough ablation\nstudy, and use Integrated Gradients to explain our predictions on a few\nsamples. Our best submissions achieve accuracies of 75.31% and 77.84%, on the\ntest sets for subtask-I and subtask-II, respectively. For subtask-III, we\nachieve accuracies of 65.64% and 62.27%.",
          "link": "http://arxiv.org/abs/2102.12255",
          "publishedOn": "2021-06-29T01:55:13.893Z",
          "wordCount": 666,
          "title": "LRG at SemEval-2021 Task 4: Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting. (arXiv:2102.12255v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingjiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chengli Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chunlin Chen</a>",
          "description": "Intelligent robots designed to interact with humans in real scenarios need to\nbe able to refer to entities actively by natural language. In spatial referring\nexpression generation, the ambiguity is unavoidable due to the diversity of\nreference frames, which will lead to an understanding gap between humans and\nrobots. To narrow this gap, in this paper, we propose a novel\nperspective-corrected spatial referring expression generation (PcSREG) approach\nfor human-robot interaction by considering the selection of reference frames.\nThe task of referring expression generation is simplified into the process of\ngenerating diverse spatial relation units. First, we pick out all landmarks in\nthese spatial relation units according to the entropy of preference and allow\nits updating through a stack model. Then all possible referring expressions are\ngenerated according to different reference frame strategies. Finally, we\nevaluate every expression using a probabilistic referring expression resolution\nmodel and find the best expression that satisfies both of the appropriateness\nand effectiveness. We implement the proposed approach on a robot system and\nempirical experiments show that our approach can generate more effective\nspatial referring expressions for practical applications.",
          "link": "http://arxiv.org/abs/2104.01558",
          "publishedOn": "2021-06-29T01:55:13.859Z",
          "wordCount": 651,
          "title": "Perspective-corrected Spatial Referring Expression Generation for Human-Robot Interaction. (arXiv:2104.01558v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiamas_I/0/1/0/all/0/1\">Ioannis Tsiamas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escolano_C/0/1/0/all/0/1\">Carlos Escolano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1\">Jos&#xe9; A. R. Fonollosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>",
          "description": "This paper describes the submission to the IWSLT 2021 offline speech\ntranslation task by the UPC Machine Translation group. The task consists of\nbuilding a system capable of translating English audio recordings extracted\nfrom TED talks into German text. Submitted systems can be either cascade or\nend-to-end and use a custom or given segmentation. Our submission is an\nend-to-end speech translation system, which combines pre-trained models\n(Wav2Vec 2.0 and mBART) with coupling modules between the encoder and decoder,\nand uses an efficient fine-tuning technique, which trains only 20% of its total\nparameters. We show that adding an Adapter to the system and pre-training it,\ncan increase the convergence speed and the final result, with which we achieve\na BLEU score of 27.3 on the MuST-C test set. Our final model is an ensemble\nthat obtains 28.22 BLEU score on the same set. Our submission also uses a\ncustom segmentation algorithm that employs pre-trained Wav2Vec 2.0 for\nidentifying periods of untranscribable text and can bring improvements of 2.5\nto 3 BLEU score on the IWSLT 2019 test set, as compared to the result with the\ngiven segmentation.",
          "link": "http://arxiv.org/abs/2105.04512",
          "publishedOn": "2021-06-29T01:55:13.784Z",
          "wordCount": 676,
          "title": "End-to-End Speech Translation with Pre-trained Models and Adapters: UPC at IWSLT 2021. (arXiv:2105.04512v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Avik Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>",
          "description": "Intent classification is a major task in spoken language understanding (SLU).\nSince most models are built with pre-collected in-domain (IND) training\nutterances, their ability to detect unsupported out-of-domain (OOD) utterances\nhas a critical effect in practical use. Recent works have shown that using\nextra data and labels can improve the OOD detection performance, yet it could\nbe costly to collect such data. This paper proposes to train a model with only\nIND data while supporting both IND intent classification and OOD detection. Our\nmethod designs a novel domain-regularized module (DRM) to reduce the\noverconfident phenomenon of a vanilla classifier, achieving a better\ngeneralization in both cases. Besides, DRM can be used as a drop-in replacement\nfor the last layer in any neural network-based intent classifier, providing a\nlow-cost strategy for a significant improvement. The evaluation on four\ndatasets shows that our method built on BERT and RoBERTa models achieves\nstate-of-the-art performance against existing approaches and the strong\nbaselines we created for the comparisons.",
          "link": "http://arxiv.org/abs/2106.14464",
          "publishedOn": "2021-06-29T01:55:13.709Z",
          "wordCount": 604,
          "title": "Enhancing the Generalization for Intent Classification and Out-of-Domain Detection in SLU. (arXiv:2106.14464v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qingjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Luyuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>",
          "description": "Target speech separation is the process of filtering a certain speaker's\nvoice out of speech mixtures according to the additional speaker identity\ninformation provided. Recent works have made considerable improvement by\nprocessing signals in the time domain directly. The majority of them take fully\noverlapped speech mixtures for training. However, since most real-life\nconversations occur randomly and are sparsely overlapped, we argue that\ntraining with different overlap ratio data benefits. To do so, an unavoidable\nproblem is that the popularly used SI-SNR loss has no definition for silent\nsources. This paper proposes the weighted SI-SNR loss, together with the joint\nlearning of target speech separation and personal VAD. The weighted SI-SNR loss\nimposes a weight factor that is proportional to the target speaker's duration\nand returns zero when the target speaker is absent. Meanwhile, the personal VAD\ngenerates masks and sets non-target speech to silence. Experiments show that\nour proposed method outperforms the baseline by 1.73 dB in terms of SDR on\nfully overlapped speech, as well as by 4.17 dB and 0.9 dB on sparsely\noverlapped speech of clean and noisy conditions. Besides, with slight\ndegradation in performance, our model could reduce the time costs in inference.",
          "link": "http://arxiv.org/abs/2106.14371",
          "publishedOn": "2021-06-29T01:55:13.536Z",
          "wordCount": 672,
          "title": "Sparsely Overlapped Speech Training in the Time Domain: Joint Learning of Target Speech Separation and Personal VAD Benefits. (arXiv:2106.14371v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.",
          "link": "http://arxiv.org/abs/2106.14463",
          "publishedOn": "2021-06-29T01:55:13.456Z",
          "wordCount": 674,
          "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "We generalize deep self-attention distillation in MiniLM (Wang et al., 2020)\nby only using self-attention relation distillation for task-agnostic\ncompression of pretrained Transformers. In particular, we define multi-head\nself-attention relations as scaled dot-product between the pairs of query, key,\nand value vectors within each self-attention module. Then we employ the above\nrelational knowledge to train the student model. Besides its simplicity and\nunified principle, more favorably, there is no restriction in terms of the\nnumber of student's attention heads, while most previous work has to guarantee\nthe same head number between teacher and student. Moreover, the fine-grained\nself-attention relations tend to fully exploit the interaction knowledge\nlearned by Transformer. In addition, we thoroughly examine the layer selection\nstrategy for teacher models, rather than just relying on the last layer as in\nMiniLM. We conduct extensive experiments on compressing both monolingual and\nmultilingual pretrained models. Experimental results demonstrate that our\nmodels distilled from base-size and large-size teachers (BERT, RoBERTa and\nXLM-R) outperform the state-of-the-art.",
          "link": "http://arxiv.org/abs/2012.15828",
          "publishedOn": "2021-06-29T01:55:13.441Z",
          "wordCount": 638,
          "title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers. (arXiv:2012.15828v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Shib Sankar Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1\">Michael Boratko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atmakuri_S/0/1/0/all/0/1\">Shriya Atmakuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Dhruvesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>",
          "description": "Learning vector representations for words is one of the most fundamental\ntopics in NLP, capable of capturing syntactic and semantic relationships useful\nin a variety of downstream NLP tasks. Vector representations can be limiting,\nhowever, in that typical scoring such as dot product similarity intertwines\nposition and magnitude of the vector in space. Exciting innovations in the\nspace of representation learning have proposed alternative fundamental\nrepresentations, such as distributions, hyperbolic vectors, or regions. Our\nmodel, Word2Box, takes a region-based approach to the problem of word\nrepresentation, representing words as $n$-dimensional rectangles. These\nrepresentations encode position and breadth independently and provide\nadditional geometric operations such as intersection and containment which\nallow them to model co-occurrence patterns vectors struggle with. We\ndemonstrate improved performance on various word similarity tasks, particularly\non less common words, and perform a qualitative analysis exploring the\nadditional unique expressivity provided by Word2Box.",
          "link": "http://arxiv.org/abs/2106.14361",
          "publishedOn": "2021-06-29T01:55:13.434Z",
          "wordCount": 589,
          "title": "Word2Box: Learning Word Representation Using Box Embeddings. (arXiv:2106.14361v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valipour_M/0/1/0/all/0/1\">Mojtaba Valipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_B/0/1/0/all/0/1\">Bowen You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panju_M/0/1/0/all/0/1\">Maysum Panju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>",
          "description": "Symbolic regression is the task of identifying a mathematical expression that\nbest fits a provided dataset of input and output values. Due to the richness of\nthe space of mathematical expressions, symbolic regression is generally a\nchallenging problem. While conventional approaches based on genetic evolution\nalgorithms have been used for decades, deep learning-based methods are\nrelatively new and an active research area. In this work, we present\nSymbolicGPT, a novel transformer-based language model for symbolic regression.\nThis model exploits the advantages of probabilistic language models like GPT,\nincluding strength in performance and flexibility. Through comprehensive\nexperiments, we show that our model performs strongly compared to competing\nmodels with respect to the accuracy, running time, and data efficiency.",
          "link": "http://arxiv.org/abs/2106.14131",
          "publishedOn": "2021-06-29T01:55:13.427Z",
          "wordCount": 560,
          "title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression. (arXiv:2106.14131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huber_J/0/1/0/all/0/1\">Joseph Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Weile Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgakoudis_G/0/1/0/all/0/1\">Giorgis Georgakoudis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doerfert_J/0/1/0/all/0/1\">Johannes Doerfert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_O/0/1/0/all/0/1\">Oscar Hernandez</a>",
          "description": "This paper presents a methodology for using LLVM-based tools to tune the\nDCA++ (dynamical clusterapproximation) application that targets the new ARM\nA64FX processor. The goal is to describethe changes required for the new\narchitecture and generate efficient single instruction/multiple data(SIMD)\ninstructions that target the new Scalable Vector Extension instruction set.\nDuring manualtuning, the authors used the LLVM tools to improve code\nparallelization by using OpenMP SIMD,refactored the code and applied\ntransformation that enabled SIMD optimizations, and ensured thatthe correct\nlibraries were used to achieve optimal performance. By applying these code\nchanges, codespeed was increased by 1.98X and 78 GFlops were achieved on the\nA64FX processor. The authorsaim to automatize parts of the efforts in the\nOpenMP Advisor tool, which is built on top of existingand newly introduced LLVM\ntooling.",
          "link": "http://arxiv.org/abs/2106.14332",
          "publishedOn": "2021-06-29T01:55:13.415Z",
          "wordCount": 590,
          "title": "A Case Study of LLVM-Based Analysis for Optimizing SIMD Code Generation. (arXiv:2106.14332v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotelnikov_E/0/1/0/all/0/1\">Evgeny Kotelnikov</a>",
          "description": "Currently, there are more than a dozen Russian-language corpora for sentiment\nanalysis, differing in the source of the texts, domain, size, number and ratio\nof sentiment classes, and annotation method. This work examines publicly\navailable Russian-language corpora, presents their qualitative and quantitative\ncharacteristics, which make it possible to get an idea of the current landscape\nof the corpora for sentiment analysis. The ranking of corpora by annotation\nquality is proposed, which can be useful when choosing corpora for training and\ntesting. The influence of the training dataset on the performance of sentiment\nanalysis is investigated based on the use of the deep neural network model\nBERT. The experiments with review corpora allow us to conclude that on average\nthe quality of models increases with an increase in the number of training\ncorpora. For the first time, quality scores were obtained for the corpus of\nreviews of ROMIP seminars based on the BERT model. Also, the study proposes the\ntask of the building a universal model for sentiment analysis.",
          "link": "http://arxiv.org/abs/2106.14434",
          "publishedOn": "2021-06-29T01:55:13.399Z",
          "wordCount": 601,
          "title": "Current Landscape of the Russian Sentiment Corpora. (arXiv:2106.14434v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S.M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-06-29T01:55:13.393Z",
          "wordCount": 608,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ayush Singh</a>",
          "description": "Natural interface to database (NLIDB) has been researched a lot during the\npast decades. In the core of NLIDB, is a semantic parser used to convert\nnatural language into SQL. Solutions from traditional NLP methodology focuses\non grammar rule pattern learning and pairing via intermediate logic forms.\nAlthough those methods give an acceptable performance on certain specific\ndatabase and parsing tasks, they are hard to generalize and scale. On the other\nhand, recent progress in neural deep learning seems to provide a promising\ndirection towards building a general NLIDB system. Unlike the traditional\napproach, those neural methodologies treat the parsing problem as a\nsequence-to-sequence learning problem. In this paper, we experimented on\nseveral sequence-to-sequence learning models and evaluate their performance on\ngeneral database parsing task.",
          "link": "http://arxiv.org/abs/2106.13858",
          "publishedOn": "2021-06-29T01:55:13.386Z",
          "wordCount": 560,
          "title": "Semantic Parsing Natural Language into Relational Algebra. (arXiv:2106.13858v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Tahmid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samin_K/0/1/0/all/0/1\">Kazi Samin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yong-Bin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Sohel Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>",
          "description": "Contemporary works on abstractive text summarization have focused primarily\non high-resource languages like English, mostly due to the limited availability\nof datasets for low/mid-resource ones. In this work, we present XL-Sum, a\ncomprehensive and diverse dataset comprising 1 million professionally annotated\narticle-summary pairs from BBC, extracted using a set of carefully designed\nheuristics. The dataset covers 44 languages ranging from low to high-resource,\nfor many of which no public dataset is currently available. XL-Sum is highly\nabstractive, concise, and of high quality, as indicated by human and intrinsic\nevaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model,\nwith XL-Sum and experiment on multilingual and low-resource summarization\ntasks. XL-Sum induces competitive results compared to the ones obtained using\nsimilar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10\nlanguages we benchmark on, with some of them exceeding 15, as obtained by\nmultilingual training. Additionally, training on low-resource languages\nindividually also provides competitive performance. To the best of our\nknowledge, XL-Sum is the largest abstractive summarization dataset in terms of\nthe number of samples collected from a single source and the number of\nlanguages covered. We are releasing our dataset and models to encourage future\nresearch on multilingual abstractive summarization. The resources can be found\nat \\url{https://github.com/csebuetnlp/xl-sum}.",
          "link": "http://arxiv.org/abs/2106.13822",
          "publishedOn": "2021-06-29T01:55:13.378Z",
          "wordCount": 662,
          "title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages. (arXiv:2106.13822v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_Z/0/1/0/all/0/1\">Zeinab Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ShamsFard_M/0/1/0/all/0/1\">Mehrnoush ShamsFard</a>",
          "description": "Recognizing causal elements and causal relations in text is one of the\nchallenging issues in natural language processing; specifically, in low\nresource languages such as Persian. In this research we prepare a causality\nhuman annotated corpus for the Persian language which consists of 4446\nsentences and 5128 causal relations and three labels of cause, effect and\ncausal mark -- if possibl -- are specified for each relation. We have used this\ncorpus to train a system for detecting causal elements boundaries. Also, we\npresent a causality detection benchmark for three machine learning methods and\ntwo deep learning systems based on this corpus. Performance evaluations\nindicate that our best total result is obtained through CRF classifier which\nhas F-measure of 0.76 and the best accuracy obtained through Bi-LSTM-CRF deep\nlearning method with Accuracy equal to %91.4.",
          "link": "http://arxiv.org/abs/2106.14165",
          "publishedOn": "2021-06-29T01:55:13.370Z",
          "wordCount": 575,
          "title": "Persian Causality Corpus (PerCause) and the Causality Detection Benchmark. (arXiv:2106.14165v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fishcheva_I/0/1/0/all/0/1\">Irina Fishcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goloviznina_V/0/1/0/all/0/1\">Valeriya Goloviznina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotelnikov_E/0/1/0/all/0/1\">Evgeny Kotelnikov</a>",
          "description": "Argumentation mining is a field of computational linguistics that is devoted\nto extracting from texts and classifying arguments and relations between them,\nas well as constructing an argumentative structure. A significant obstacle to\nresearch in this area for the Russian language is the lack of annotated\nRussian-language text corpora. This article explores the possibility of\nimproving the quality of argumentation mining using the extension of the\nRussian-language version of the Argumentative Microtext Corpus (ArgMicro) based\non the machine translation of the Persuasive Essays Corpus (PersEssays). To\nmake it possible to use these two corpora combined, we propose a Joint Argument\nAnnotation Scheme based on the schemes used in ArgMicro and PersEssays. We\nsolve the problem of classifying argumentative discourse units (ADUs) into two\nclasses - \"pro\" (\"for\") and \"opp\" (\"against\") using traditional machine\nlearning techniques (SVM, Bagging and XGBoost) and a deep neural network (BERT\nmodel). An ensemble of XGBoost and BERT models was proposed, which showed the\nhighest performance of ADUs classification for both corpora.",
          "link": "http://arxiv.org/abs/2106.14438",
          "publishedOn": "2021-06-29T01:55:13.355Z",
          "wordCount": 615,
          "title": "Traditional Machine Learning and Deep Learning Models for Argumentation Mining in Russian Texts. (arXiv:2106.14438v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "In recent years, reference-based and supervised summarization evaluation\nmetrics have been widely explored. However, collecting human-annotated\nreferences and ratings are costly and time-consuming. To avoid these\nlimitations, we propose a training-free and reference-free summarization\nevaluation metric. Our metric consists of a centrality-weighted relevance score\nand a self-referenced redundancy score. The relevance score is computed between\nthe pseudo reference built from the source document and the given summary,\nwhere the pseudo reference content is weighted by the sentence centrality to\nprovide importance guidance. Besides an $F_1$-based relevance score, we also\ndesign an $F_\\beta$-based variant that pays more attention to the recall score.\nAs for the redundancy score of the summary, we compute a self-masked similarity\nscore with the summary itself to evaluate the redundant information in the\nsummary. Finally, we combine the relevance and redundancy scores to produce the\nfinal evaluation score of the given summary. Extensive experiments show that\nour methods can significantly outperform existing methods on both\nmulti-document and single-document summarization evaluation.",
          "link": "http://arxiv.org/abs/2106.13945",
          "publishedOn": "2021-06-29T01:55:13.349Z",
          "wordCount": 607,
          "title": "A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy. (arXiv:2106.13945v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeedizade_M/0/1/0/all/0/1\">Mohammad Javad Saeedizade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torabian_N/0/1/0/all/0/1\">Najmeh Torabian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minaei_Bidgoli_B/0/1/0/all/0/1\">Behrouz Minaei-Bidgoli</a>",
          "description": "Link prediction is the task of predicting missing relations between entities\nof the knowledge graph by inferring from the facts contained in it. Recent work\nin link prediction has attempted to provide a model for increasing link\nprediction accuracy by using more layers in neural network architecture or\nmethods that add to the computational complexity of models. This paper we\nproposed a method for refining the knowledge graph, which makes the knowledge\ngraph more informative, and link prediction operations can be performed more\naccurately using relatively fast translational models. Translational link\nprediction models, such as TransE, TransH, TransD, etc., have much less\ncomplexity than deep learning approaches. This method uses the hierarchy of\nrelationships and also the hierarchy of entities in the knowledge graph to add\nthe entity information as a new entity to the graph and connect it to the nodes\nwhich contain this information in their hierarchy. Our experiments show that\nour method can significantly increase the performance of translational link\nprediction methods in H@10, MR, MRR.",
          "link": "http://arxiv.org/abs/2106.14233",
          "publishedOn": "2021-06-29T01:55:13.343Z",
          "wordCount": 614,
          "title": "KGRefiner: Knowledge Graph Refinement for Improving Accuracy of Translational Link Prediction Methods. (arXiv:2106.14233v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>",
          "description": "We ask the question: to what extent can recent large-scale language and image\ngeneration models blend visual concepts? Given an arbitrary object, we identify\na relevant object and generate a single-sentence description of the blend of\nthe two using a language model. We then generate a visual depiction of the\nblend using a text-based image generation model. Quantitative and qualitative\nevaluations demonstrate the superiority of language models over classical\nmethods for conceptual blending, and of recent large-scale image generation\nmodels over prior models for the visual depiction.",
          "link": "http://arxiv.org/abs/2106.14127",
          "publishedOn": "2021-06-29T01:55:13.334Z",
          "wordCount": 527,
          "title": "Visual Conceptual Blending with Large-scale Language and Vision Models. (arXiv:2106.14127v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1\">Priyam Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1\">Tiasa Singha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muftuoglu_Z/0/1/0/all/0/1\">Zumrut Muftuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>",
          "description": "Natural Language Processing (NLP) techniques can be applied to help with the\ndiagnosis of medical conditions such as depression, using a collection of a\nperson's utterances. Depression is a serious medical illness that can have\nadverse effects on how one feels, thinks, and acts, which can lead to emotional\nand physical problems. Due to the sensitive nature of such data, privacy\nmeasures need to be taken for handling and training models with such data. In\nthis work, we study the effects that the application of Differential Privacy\n(DP) has, in both a centralized and a Federated Learning (FL) setup, on\ntraining contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).\nWe offer insights on how to privately train NLP models and what architectures\nand setups provide more desirable privacy utility trade-offs. We envisage this\nwork to be used in future healthcare and mental health studies to keep medical\nhistory private. Therefore, we provide an open-source implementation of this\nwork.",
          "link": "http://arxiv.org/abs/2106.13973",
          "publishedOn": "2021-06-29T01:55:13.326Z",
          "wordCount": 612,
          "title": "Benchmarking Differential Privacy and Federated Learning for BERT Models. (arXiv:2106.13973v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuniyoshi_F/0/1/0/all/0/1\">Fusataka Kuniyoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozawa_J/0/1/0/all/0/1\">Jun Ozawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miwa_M/0/1/0/all/0/1\">Makoto Miwa</a>",
          "description": "In the field of inorganic materials science, there is a growing demand to\nextract knowledge such as physical properties and synthesis processes of\nmaterials by machine-reading a large number of papers. This is because\nmaterials researchers refer to many papers in order to come up with promising\nterms of experiments for material synthesis. However, there are only a few\nsystems that can extract material names and their properties. This study\nproposes a large-scale natural language processing (NLP) pipeline for\nextracting material names and properties from materials science literature to\nenable the search and retrieval of results in materials science. Therefore, we\npropose a label definition for extracting material names and properties and\naccordingly build a corpus containing 836 annotated paragraphs extracted from\n301 papers for training a named entity recognition (NER) model. Experimental\nresults demonstrate the utility of this NER model; it achieves successful\nextraction with a micro-F1 score of 78.1%. To demonstrate the efficacy of our\napproach, we present a thorough evaluation on a real-world automatically\nannotated corpus by applying our trained NER model to 12,895 materials science\npapers. We analyze the trend in materials science by visualizing the outputs of\nthe NLP pipeline. For example, the country-by-year analysis indicates that in\nrecent years, the number of papers on \"MoS2,\" a material used in perovskite\nsolar cells, has been increasing rapidly in China but decreasing in the United\nStates. Further, according to the conditions-by-year analysis, the processing\ntemperature of the catalyst material \"PEDOT:PSS\" is shifting below 200 degree,\nand the number of reports with a processing time exceeding 5 h is increasing\nslightly.",
          "link": "http://arxiv.org/abs/2106.14157",
          "publishedOn": "2021-06-29T01:55:13.309Z",
          "wordCount": 700,
          "title": "Analyzing Research Trends in Inorganic Materials Literature Using NLP. (arXiv:2106.14157v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahmohammadi_S/0/1/0/all/0/1\">Sara Shahmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veisi_H/0/1/0/all/0/1\">Hadi Veisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darzi_A/0/1/0/all/0/1\">Ali Darzi</a>",
          "description": "Over the past years, interest in discourse analysis and discourse parsing has\nsteadily grown, and many discourse-annotated corpora and, as a result,\ndiscourse parsers have been built. In this paper, we present a\ndiscourse-annotated corpus for the Persian language built in the framework of\nRhetorical Structure Theory as well as a discourse parser built upon the DPLP\nparser, an open-source discourse parser. Our corpus consists of 150\njournalistic texts, each text having an average of around 400 words. Corpus\ntexts were annotated using 18 discourse relations and based on the annotation\nguideline of the English RST Discourse Treebank corpus. Our text-level\ndiscourse parser is trained using gold segmentation and is built upon the DPLP\ndiscourse parser, which uses a large-margin transition-based approach to solve\nthe problem of discourse parsing. The performance of our discourse parser in\nspan (S), nuclearity (N) and relation (R) detection is around 78%, 64%, 44%\nrespectively, in terms of F1 measure.",
          "link": "http://arxiv.org/abs/2106.13833",
          "publishedOn": "2021-06-29T01:55:13.303Z",
          "wordCount": 580,
          "title": "Persian Rhetorical Structure Theory. (arXiv:2106.13833v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sinno_B/0/1/0/all/0/1\">Barea Sinno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oviedo_B/0/1/0/all/0/1\">Bernardo Oviedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atwell_K/0/1/0/all/0/1\">Katherine Atwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>",
          "description": "Analyzing political ideology and polarization is of critical importance in\nadvancing our understanding of the political context in society. Recent\nresearch has made great strides towards understanding the ideological bias\n(i.e., stance) of news media along a left-right spectrum. In this work, we take\na novel approach and study the ideology of the policy under discussion teasing\napart the nuanced co-existence of stance and ideology. Aligned with the\ntheoretical accounts in political science, we treat ideology as a\nmulti-dimensional construct, and introduce the first diachronic dataset of news\narticles whose political ideology under discussion is annotated by trained\npolitical scientists and linguists at the paragraph-level. We showcase that\nthis framework enables quantitative analysis of polarization, a temporal,\nmultifaceted measure of ideological distance. We further present baseline\nmodels for ideology prediction.",
          "link": "http://arxiv.org/abs/2106.14387",
          "publishedOn": "2021-06-29T01:55:13.297Z",
          "wordCount": 574,
          "title": "Political Ideology and Polarization of Policy Positions: A Multi-dimensional Approach. (arXiv:2106.14387v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lianbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Huimin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiliang Zhang</a>",
          "description": "Extracting relational triples from texts is a fundamental task in knowledge\ngraph construction. The popular way of existing methods is to jointly extract\nentities and relations using a single model, which often suffers from the\noverlapping triple problem. That is, there are multiple relational triples that\nshare the same entities within one sentence. In this work, we propose an\neffective cascade dual-decoder approach to extract overlapping relational\ntriples, which includes a text-specific relation decoder and a\nrelation-corresponded entity decoder. Our approach is straightforward: the\ntext-specific relation decoder detects relations from a sentence according to\nits text semantics and treats them as extra features to guide the entity\nextraction; for each extracted relation, which is with trainable embedding, the\nrelation-corresponded entity decoder detects the corresponding head and tail\nentities using a span-based tagging scheme. In this way, the overlapping triple\nproblem is tackled naturally. Experiments on two public datasets demonstrate\nthat our proposed approach outperforms state-of-the-art methods and achieves\nbetter F1 scores under the strict evaluation metric. Our implementation is\navailable at https://github.com/prastunlp/DualDec.",
          "link": "http://arxiv.org/abs/2106.14163",
          "publishedOn": "2021-06-29T01:55:13.290Z",
          "wordCount": 612,
          "title": "Effective Cascade Dual-Decoder Model for Joint Entity and Relation Extraction. (arXiv:2106.14163v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_C/0/1/0/all/0/1\">Chengping Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianxun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>",
          "description": "Partial differential equations (PDEs) play a fundamental role in modeling and\nsimulating problems across a wide range of disciplines. Recent advances in deep\nlearning have shown the great potential of physics-informed neural networks\n(PINNs) to solve PDEs as a basis for data-driven modeling and inverse analysis.\nHowever, the majority of existing PINN methods, based on fully-connected NNs,\npose intrinsic limitations to low-dimensional spatiotemporal parameterizations.\nMoreover, since the initial/boundary conditions (I/BCs) are softly imposed via\npenalty, the solution quality heavily relies on hyperparameter tuning. To this\nend, we propose the novel physics-informed convolutional-recurrent learning\narchitectures (PhyCRNet and PhyCRNet-s) for solving PDEs without any labeled\ndata. Specifically, an encoder-decoder convolutional long short-term memory\nnetwork is proposed for low-dimensional spatial feature extraction and temporal\nevolution learning. The loss function is defined as the aggregated discretized\nPDE residuals, while the I/BCs are hard-encoded in the network to ensure\nforcible satisfaction (e.g., periodic boundary padding). The networks are\nfurther enhanced by autoregressive and residual connections that explicitly\nsimulate time marching. The performance of our proposed methods has been\nassessed by solving three nonlinear PDEs (e.g., 2D Burgers' equations, the\n$\\lambda$-$\\omega$ and FitzHugh Nagumo reaction-diffusion equations), and\ncompared against the start-of-the-art baseline algorithms. The numerical\nresults demonstrate the superiority of our proposed methodology in the context\nof solution accuracy, extrapolability and generalizability.",
          "link": "http://arxiv.org/abs/2106.14103",
          "publishedOn": "2021-06-29T01:55:13.282Z",
          "wordCount": 662,
          "title": "PhyCRNet: Physics-informed Convolutional-Recurrent Network for Solving Spatiotemporal PDEs. (arXiv:2106.14103v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Min Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiasheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haipang Wu</a>",
          "description": "This paper describes our approach to DSTC 9 Track 2: Cross-lingual\nMulti-domain Dialog State Tracking, the task goal is to build a Cross-lingual\ndialog state tracker with a training set in rich resource language and a\ntesting set in low resource language. We formulate a method for joint learning\nof slot operation classification task and state tracking task respectively.\nFurthermore, we design a novel mask mechanism for fusing contextual information\nabout dialogue, the results show the proposed model achieves excellent\nperformance on DSTC Challenge II with a joint accuracy of 62.37% and 23.96% in\nMultiWOZ(en - zh) dataset and CrossWOZ(zh - en) dataset, respectively.",
          "link": "http://arxiv.org/abs/2106.14433",
          "publishedOn": "2021-06-29T01:55:13.250Z",
          "wordCount": 544,
          "title": "Efficient Dialogue State Tracking by Masked Hierarchical Transformer. (arXiv:2106.14433v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-06-29T01:55:13.244Z",
          "wordCount": 628,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>",
          "description": "Explainable machine learning models primarily justify predicted labels using\neither extractive rationales (i.e., subsets of input features) or free-text\nnatural language explanations (NLEs) as abstractive justifications. While NLEs\ncan be more comprehensive than extractive rationales, machine-generated NLEs\nhave been shown to sometimes lack commonsense knowledge. Here, we show that\ncommonsense knowledge can act as a bridge between extractive rationales and\nNLEs, rendering both types of explanations better. More precisely, we introduce\na unified framework, called RExC (Rationale-Inspired Explanations with\nCommonsense), that (1) extracts rationales as a set of features responsible for\nmachine predictions, (2) expands the extractive rationales using available\ncommonsense resources, and (3) uses the expanded knowledge to generate natural\nlanguage explanations. Our framework surpasses by a large margin the previous\nstate-of-the-art in generating NLEs across five tasks in both natural language\nprocessing and vision-language understanding, with human annotators\nconsistently rating the explanations generated by RExC to be more\ncomprehensive, grounded in commonsense, and overall preferred compared to\nprevious state-of-the-art models. Moreover, our work shows that\ncommonsense-grounded explanations can enhance both task performance and\nrationales extraction capabilities.",
          "link": "http://arxiv.org/abs/2106.13876",
          "publishedOn": "2021-06-29T01:55:13.210Z",
          "wordCount": 615,
          "title": "Rationale-Inspired Natural Language Explanations with Commonsense. (arXiv:2106.13876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>",
          "description": "Despite the success of various text generation metrics such as BERTScore, it\nis still difficult to evaluate the image captions without enough reference\ncaptions due to the diversity of the descriptions. In this paper, we introduce\na new metric UMIC, an Unreferenced Metric for Image Captioning which does not\nrequire reference captions to evaluate image captions. Based on\nVision-and-Language BERT, we train UMIC to discriminate negative captions via\ncontrastive learning. Also, we observe critical problems of the previous\nbenchmark dataset (i.e., human annotations) on image captioning metric, and\nintroduce a new collection of human annotations on the generated captions. We\nvalidate UMIC on four datasets, including our new dataset, and show that UMIC\nhas a higher correlation than all previous metrics that require multiple\nreferences. We release the benchmark dataset and pre-trained models to compute\nthe UMIC.",
          "link": "http://arxiv.org/abs/2106.14019",
          "publishedOn": "2021-06-29T01:55:13.194Z",
          "wordCount": 585,
          "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning. (arXiv:2106.14019v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Etezadi_R/0/1/0/all/0/1\">Romina Etezadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>",
          "description": "Question answering systems may find the answers to users' questions from\neither unstructured texts or structured data such as knowledge graphs.\nAnswering questions using supervised learning approaches including deep\nlearning models need large training datasets. In recent years, some datasets\nhave been presented for the task of Question answering over knowledge graphs,\nwhich is the focus of this paper. Although many datasets in English were\nproposed, there have been a few question-answering datasets in Persian. This\npaper introduces \\textit{PeCoQ}, a dataset for Persian question answering. This\ndataset contains 10,000 complex questions and answers extracted from the\nPersian knowledge graph, FarsBase. For each question, the SPARQL query and two\nparaphrases that were written by linguists are provided as well. There are\ndifferent types of complexities in the dataset, such as multi-relation,\nmulti-entity, ordinal, and temporal constraints. In this paper, we discuss the\ndataset's characteristics and describe our methodology for building it.",
          "link": "http://arxiv.org/abs/2106.14167",
          "publishedOn": "2021-06-29T01:55:13.181Z",
          "wordCount": 593,
          "title": "PeCoQ: A Dataset for Persian Complex Question Answering over Knowledge Graph. (arXiv:2106.14167v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lachmy_R/0/1/0/all/0/1\">Royi Lachmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>",
          "description": "Forming and interpreting abstraction is a core process in human\ncommunication. In particular, when giving and performing complex instructions\nstated in natural language (NL), people may naturally evoke abstract constructs\nsuch as objects, loops, conditions and functions to convey their intentions in\nan efficient and precise way. Yet, interpreting and grounding abstraction\nstated in NL has not been systematically studied in NLP/AI. To elicit\nnaturally-occurring abstractions in NL we develop the Hexagons referential\ngame, where players describe increasingly complex images on a two-dimensional\nHexagons board, and other players need to follow these instructions to recreate\nthe images. Using this game we collected the Hexagons dataset, which consists\nof 164 images and over 3000 naturally-occurring instructions, rich with diverse\nabstractions. Results of our baseline models on an instruction-to-execution\ntask derived from the Hexagons dataset confirm that higher-level abstractions\nin NL are indeed more challenging for current systems to process. Thus, this\ndataset exposes a new and challenging dimension for grounded semantic parsing,\nand we propose it for the community as a future benchmark to explore more\nsophisticated and high-level communication within NLP applications.",
          "link": "http://arxiv.org/abs/2106.14321",
          "publishedOn": "2021-06-29T01:55:13.172Z",
          "wordCount": 622,
          "title": "Draw Me a Flower: Grounding Formal Abstract Structures Stated in Informal Natural Language. (arXiv:2106.14321v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lujun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhuoren Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>",
          "description": "Automatic chat summarization can help people quickly grasp important\ninformation from numerous chat messages. Unlike conventional documents, chat\nlogs usually have fragmented and evolving topics. In addition, these logs\ncontain a quantity of elliptical and interrogative sentences, which make the\nchat summarization highly context dependent. In this work, we propose a novel\nunsupervised framework called RankAE to perform chat summarization without\nemploying manually labeled data. RankAE consists of a topic-oriented ranking\nstrategy that selects topic utterances according to centrality and diversity\nsimultaneously, as well as a denoising auto-encoder that is carefully designed\nto generate succinct but context-informative summaries based on the selected\nutterances. To evaluate the proposed method, we collect a large-scale dataset\nof chat logs from a customer service environment and build an annotated set\nonly for model evaluation. Experimental results show that RankAE significantly\noutperforms other unsupervised methods and is able to generate high-quality\nsummaries in terms of relevance and topic coverage.",
          "link": "http://arxiv.org/abs/2012.07300",
          "publishedOn": "2021-06-28T01:57:54.755Z",
          "wordCount": 640,
          "title": "Unsupervised Summarization for Chat Logs with Topic-Oriented Ranking and Context-Aware Auto-Encoders. (arXiv:2012.07300v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>",
          "description": "In this paper, we analyze the interplay between the use of offensive language\nand mental health. We acquired publicly available datasets created for\noffensive language identification and depression detection and we train\ncomputational models to compare the use of offensive language in social media\nposts written by groups of individuals with and without self-reported\ndepression diagnosis. We also look at samples written by groups of individuals\nwhose posts show signs of depression according to recent related studies. Our\nanalysis indicates that offensive language is more frequently used in the\nsamples written by individuals with self-reported depression as well as\nindividuals showing signs of depression. The results discussed here open new\navenues in research in politeness/offensiveness and mental health.",
          "link": "http://arxiv.org/abs/2105.14888",
          "publishedOn": "2021-06-28T01:57:54.735Z",
          "wordCount": 585,
          "title": "An Exploratory Analysis of the Relation Between Offensive Language and Mental Health. (arXiv:2105.14888v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09474",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Keon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>",
          "description": "Previous works on neural text-to-speech (TTS) have been addressed on limited\nspeed in training and inference time, robustness for difficult synthesis\nconditions, expressiveness, and controllability. Although several approaches\nresolve some limitations, there has been no attempt to solve all weaknesses at\nonce. In this paper, we propose STYLER, an expressive and controllable TTS\nframework with high-speed and robust synthesis. Our novel audio-text aligning\nmethod called Mel Calibrator and excluding autoregressive decoding enable rapid\ntraining and inference and robust synthesis on unseen data. Also, disentangled\nstyle factor modeling under supervision enlarges the controllability in\nsynthesizing process leading to expressive TTS. On top of it, a novel noise\nmodeling pipeline using domain adversarial training and Residual Decoding\nempowers noise-robust style transfer, decomposing the noise without any\nadditional label. Various experiments demonstrate that STYLER is more effective\nin speed and robustness than expressive TTS with autoregressive decoding and\nmore expressive and controllable than reading style non-autoregressive TTS.\nSynthesis samples and experiment results are provided via our demo page, and\ncode is available publicly.",
          "link": "http://arxiv.org/abs/2103.09474",
          "publishedOn": "2021-06-28T01:57:54.702Z",
          "wordCount": 684,
          "title": "STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text to Speech. (arXiv:2103.09474v4 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_K/0/1/0/all/0/1\">Kishlay Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>",
          "description": "Fake news travels at unprecedented speeds, reaches global audiences and puts\nusers and communities at great risk via social media platforms. Deep learning\nbased models show good performance when trained on large amounts of labeled\ndata on events of interest, whereas the performance of models tends to degrade\non other events due to domain shift. Therefore, significant challenges are\nposed for existing detection approaches to detect fake news on emergent events,\nwhere large-scale labeled datasets are difficult to obtain. Moreover, adding\nthe knowledge from newly emergent events requires to build a new model from\nscratch or continue to fine-tune the model, which can be challenging,\nexpensive, and unrealistic for real-world settings. In order to address those\nchallenges, we propose an end-to-end fake news detection framework named\nMetaFEND, which is able to learn quickly to detect fake news on emergent events\nwith a few verified posts. Specifically, the proposed model integrates\nmeta-learning and neural process methods together to enjoy the benefits of\nthese approaches. In particular, a label embedding module and a hard attention\nmechanism are proposed to enhance the effectiveness by handling categorical\ninformation and trimming irrelevant posts. Extensive experiments are conducted\non multimedia datasets collected from Twitter and Weibo. The experimental\nresults show our proposed MetaFEND model can detect fake news on never-seen\nevents effectively and outperform the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.13711",
          "publishedOn": "2021-06-28T01:57:54.687Z",
          "wordCount": 668,
          "title": "Multimodal Emergent Fake News Detection via Meta Neural Process Networks. (arXiv:2106.13711v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kates_B/0/1/0/all/0/1\">Brandon Kates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mentch_J/0/1/0/all/0/1\">Jeff Mentch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharkar_A/0/1/0/all/0/1\">Anant Kharkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "This work improves the quality of automated machine learning (AutoML) systems\nby using dataset and function descriptions while significantly decreasing\ncomputation time from minutes to milliseconds by using a zero-shot approach.\nGiven a new dataset and a well-defined machine learning task, humans begin by\nreading a description of the dataset and documentation for the algorithms to be\nused. This work is the first to use these textual descriptions, which we call\nprivileged information, for AutoML. We use a pre-trained Transformer model to\nprocess the privileged text and demonstrate that using this information\nimproves AutoML performance. Thus, our approach leverages the progress of\nunsupervised representation learning in natural language processing to provide\na significant boost to AutoML. We demonstrate that using only textual\ndescriptions of the data and functions achieves reasonable classification\nperformance, and adding textual descriptions to data meta-features improves\nclassification across tabular datasets. To achieve zero-shot AutoML we train a\ngraph neural network with these description embeddings and the data\nmeta-features. Each node represents a training dataset, which we use to predict\nthe best machine learning pipeline for a new test dataset in a zero-shot\nfashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a\nsupervised learning task and dataset. In contrast, most AutoML systems require\ntens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces\nrunning and prediction times from minutes to milliseconds, consistently across\ndatasets. By speeding up AutoML by orders of magnitude this work demonstrates\nreal-time AutoML.",
          "link": "http://arxiv.org/abs/2106.13743",
          "publishedOn": "2021-06-28T01:57:54.647Z",
          "wordCount": 677,
          "title": "Privileged Zero-Shot AutoML. (arXiv:2106.13743v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gezmu_A/0/1/0/all/0/1\">Andargachew Mekonnen Gezmu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bati_T/0/1/0/all/0/1\">Tesfaye Bayu Bati</a>",
          "description": "This paper describes the acquisition, preprocessing, segmentation, and\nalignment of an Amharic-English parallel corpus. It will be useful for machine\ntranslation of an under-resourced language, Amharic. The corpus is larger than\npreviously compiled corpora; it is released for research purposes. We trained\nneural machine translation and phrase-based statistical machine translation\nmodels using the corpus. In the automatic evaluation, neural machine\ntranslation models outperform phrase-based statistical machine translation\nmodels.",
          "link": "http://arxiv.org/abs/2104.03543",
          "publishedOn": "2021-06-28T01:57:54.598Z",
          "wordCount": 535,
          "title": "Extended Parallel Corpus for Amharic-English Machine Translation. (arXiv:2104.03543v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05144",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shulby_C/0/1/0/all/0/1\">Christopher Shulby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_F/0/1/0/all/0/1\">Frederico Santos de Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teixeira_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Teixeira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Maria Aluisio</a>",
          "description": "Speech provides a natural way for human-computer interaction. In particular,\nspeech synthesis systems are popular in different applications, such as\npersonal assistants, GPS applications, screen readers and accessibility tools.\nHowever, not all languages are on the same level when in terms of resources and\nsystems for speech synthesis. This work consists of creating publicly available\nresources for Brazilian Portuguese in the form of a novel dataset along with\ndeep learning models for end-to-end speech synthesis. Such dataset has 10.5\nhours from a single speaker, from which a Tacotron 2 model with the RTISI-LA\nvocoder presented the best performance, achieving a 4.03 MOS value. The\nobtained results are comparable to related works covering English language and\nthe state-of-the-art in Portuguese.",
          "link": "http://arxiv.org/abs/2005.05144",
          "publishedOn": "2021-06-28T01:57:54.581Z",
          "wordCount": 620,
          "title": "TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese. (arXiv:2005.05144v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1\">Michael Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>",
          "description": "Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding & Reasoning\nBenchmark) at https://aka.ms/BLURB.",
          "link": "http://arxiv.org/abs/2007.15779",
          "publishedOn": "2021-06-28T01:57:54.572Z",
          "wordCount": 708,
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_M/0/1/0/all/0/1\">Marcos Garcia</a>",
          "description": "This paper presents a multilingual study of word meaning representations in\ncontext. We assess the ability of both static and contextualized models to\nadequately represent different lexical-semantic relations, such as homonymy and\nsynonymy. To do so, we created a new multilingual dataset that allows us to\nperform a controlled evaluation of several factors such as the impact of the\nsurrounding context or the overlap between words, conveying the same or\ndifferent senses. A systematic assessment on four scenarios shows that the best\nmonolingual models based on Transformers can adequately disambiguate homonyms\nin context. However, as they rely heavily on context, these models fail at\nrepresenting words with different senses when occurring in similar sentences.\nExperiments are performed in Galician, Portuguese, English, and Spanish, and\nboth the dataset (with more than 3,000 evaluation items) and new models are\nfreely released with this study.",
          "link": "http://arxiv.org/abs/2106.13553",
          "publishedOn": "2021-06-28T01:57:54.559Z",
          "wordCount": 590,
          "title": "Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy. (arXiv:2106.13553v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Much recent effort has been invested in non-autoregressive neural machine\ntranslation, which appears to be an efficient alternative to state-of-the-art\nautoregressive machine translation on modern GPUs. In contrast to the latter,\nwhere generation is sequential, the former allows generation to be parallelized\nacross target token positions. Some of the latest non-autoregressive models\nhave achieved impressive translation quality-speed tradeoffs compared to\nautoregressive baselines. In this work, we reexamine this tradeoff and argue\nthat autoregressive baselines can be substantially sped up without loss in\naccuracy. Specifically, we study autoregressive models with encoders and\ndecoders of varied depths. Our extensive experiments show that given a\nsufficiently deep encoder, a single-layer autoregressive decoder can\nsubstantially outperform strong non-autoregressive models with comparable\ninference speed. We show that the speed disadvantage for autoregressive\nbaselines compared to non-autoregressive methods has been overestimated in\nthree aspects: suboptimal layer allocation, insufficient speed measurement, and\nlack of knowledge distillation. Our results establish a new protocol for future\nresearch toward fast, accurate machine translation. Our code is available at\nhttps://github.com/jungokasai/deep-shallow.",
          "link": "http://arxiv.org/abs/2006.10369",
          "publishedOn": "2021-06-28T01:57:54.517Z",
          "wordCount": 666,
          "title": "Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation. (arXiv:2006.10369v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1\">Jiachen Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aiswarya Vinod Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamyal_H/0/1/0/all/0/1\">Hira Dhamyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>",
          "description": "Open-set speaker recognition can be regarded as a metric learning problem,\nwhich is to maximize inter-class variance and minimize intra-class variance.\nSupervised metric learning can be categorized into entity-based learning and\nproxy-based learning. Most of the existing metric learning objectives like\nContrastive, Triplet, Prototypical, GE2E, etc all belong to the former\ndivision, the performance of which is either highly dependent on sample mining\nstrategy or restricted by insufficient label information in the mini-batch.\nProxy-based losses mitigate both shortcomings, however, fine-grained\nconnections among entities are either not or indirectly leveraged. This paper\nproposes a Masked Proxy (MP) loss which directly incorporates both proxy-based\nrelationships and pair-based relationships. We further propose Multinomial\nMasked Proxy (MMP) loss to leverage the hardness of speaker pairs. These\nmethods have been applied to evaluate on VoxCeleb test set and reach\nstate-of-the-art Equal Error Rate(EER).",
          "link": "http://arxiv.org/abs/2011.04491",
          "publishedOn": "2021-06-28T01:57:54.488Z",
          "wordCount": 612,
          "title": "Masked Proxy Loss For Text-Independent Speaker Verification. (arXiv:2011.04491v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yaru Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "ELECTRA pretrains a discriminator to detect replaced tokens, where the\nreplacements are sampled from a generator trained with masked language\nmodeling. Despite the compelling performance, ELECTRA suffers from the\nfollowing two issues. First, there is no direct feedback loop from\ndiscriminator to generator, which renders replacement sampling inefficient.\nSecond, the generator's prediction tends to be over-confident along with\ntraining, making replacements biased to correct tokens. In this paper, we\npropose two methods to improve replacement sampling for ELECTRA pre-training.\nSpecifically, we augment sampling with a hardness prediction mechanism, so that\nthe generator can encourage the discriminator to learn what it has not\nacquired. We also prove that efficient sampling reduces the training variance\nof the discriminator. Moreover, we propose to use a focal loss for the\ngenerator in order to relieve oversampling of correct tokens as replacements.\nExperimental results show that our method improves ELECTRA pre-training on\nvarious downstream tasks.",
          "link": "http://arxiv.org/abs/2106.13715",
          "publishedOn": "2021-06-28T01:57:54.421Z",
          "wordCount": 589,
          "title": "Learning to Sample Replacements for ELECTRA Pre-Training. (arXiv:2106.13715v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_H/0/1/0/all/0/1\">Hrishikesh Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alicea_B/0/1/0/all/0/1\">Bradly Alicea</a>",
          "description": "Literary artefacts are generally indexed and searched based on titles, meta\ndata and keywords over the years. This searching and indexing works well when\nuser/reader already knows about that particular creative textual artefact or\ndocument. This indexing and search hardly takes into account interest and\nemotional makeup of readers and its mapping to books. When a person is looking\nfor a literary textual artefact, he/she might be looking for not only\ninformation but also to seek the joy of reading. In case of literary artefacts,\nprogression of emotions across the key events could prove to be the key for\nindexing and searching. In this paper, we establish clusters among literary\nartefacts based on computational relationships among sentiment progressions\nusing intelligent text analysis. We have created a database of 1076 English\ntitles + 20 Marathi titles and also used database\nthis http URL with 16559 titles and their\nsummaries. We have proposed Sentiment Progression based Indexing for searching\nand recommending books. This can be used to create personalized clusters of\nbook titles of interest to readers. The analysis clearly suggests better\nsearching and indexing when we are targeting book lovers looking for a\nparticular type of book or creative artefact. This indexing and searching can\nfind many real-life applications for recommending books.",
          "link": "http://arxiv.org/abs/2106.13767",
          "publishedOn": "2021-06-28T01:57:54.388Z",
          "wordCount": 654,
          "title": "Sentiment Progression based Searching and Indexing of Literary Textual Artefacts. (arXiv:2106.13767v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.15082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiamang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>",
          "description": "Mixture-of-Experts (MoE) models can achieve promising results with outrageous\nlarge amount of parameters but constant computation cost, and thus it has\nbecome a trend in model scaling. Still it is a mystery how MoE layers bring\nquality gains by leveraging the parameters with sparse activation. In this\nwork, we investigate several key factors in sparse expert models. We observe\nthat load imbalance may not be a significant problem affecting model quality,\ncontrary to the perspectives of recent studies, while the number of sparsely\nactivated experts $k$ and expert capacity $C$ in top-$k$ routing can\nsignificantly make a difference in this context. Furthermore, we take a step\nforward to propose a simple method called expert prototyping that splits\nexperts into different prototypes and applies $k$ top-$1$ routing. This\nstrategy improves the model quality but maintains constant computational costs,\nand our further exploration on extremely large-scale models reflects that it is\nmore effective in training larger models. We push the model scale to over $1$\ntrillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in\ncomparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model\nachieves substantial speedup in convergence over the same-size baseline.",
          "link": "http://arxiv.org/abs/2105.15082",
          "publishedOn": "2021-06-28T01:57:54.318Z",
          "wordCount": 689,
          "title": "Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "Recent years have witnessed the rapid advance in neural machine translation\n(NMT), the core of which lies in the encoder-decoder architecture. Inspired by\nthe recent progress of large-scale pre-trained language models on machine\ntranslation in a limited scenario, we firstly demonstrate that a single\nlanguage model (LM4MT) can achieve comparable performance with strong\nencoder-decoder NMT models on standard machine translation benchmarks, using\nthe same training data and similar amount of model parameters. LM4MT can also\neasily utilize source-side texts as additional supervision. Though modeling the\nsource- and target-language texts with the same mechanism, LM4MT can provide\nunified representations for both source and target sentences, which can better\ntransfer knowledge across languages. Extensive experiments on pivot-based and\nzero-shot translation tasks show that LM4MT can outperform the encoder-decoder\nNMT model by a large margin.",
          "link": "http://arxiv.org/abs/2106.13627",
          "publishedOn": "2021-06-28T01:57:54.292Z",
          "wordCount": 576,
          "title": "Language Models are Good Translators. (arXiv:2106.13627v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lujun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Minlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhuoren Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>",
          "description": "In a customer service system, dialogue summarization can boost service\nefficiency by automatically creating summaries for long spoken dialogues in\nwhich customers and agents try to address issues about specific topics. In this\nwork, we focus on topic-oriented dialogue summarization, which generates highly\nabstractive summaries that preserve the main ideas from dialogues. In spoken\ndialogues, abundant dialogue noise and common semantics could obscure the\nunderlying informative content, making the general topic modeling approaches\ndifficult to apply. In addition, for customer service, role-specific\ninformation matters and is an indispensable part of a summary. To effectively\nperform topic modeling on dialogues and capture multi-role information, in this\nwork we propose a novel topic-augmented two-stage dialogue summarizer (TDS)\njointly with a saliency-aware neural topic model (SATM) for topic-oriented\nsummarization of customer service dialogues. Comprehensive studies on a\nreal-world Chinese customer service dataset demonstrated the superiority of our\nmethod against several strong baselines.",
          "link": "http://arxiv.org/abs/2012.07311",
          "publishedOn": "2021-06-28T01:57:54.260Z",
          "wordCount": 637,
          "title": "Topic-Oriented Spoken Dialogue Summarization for Customer Service with Saliency-Aware Topic Modeling. (arXiv:2012.07311v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "While pretrained encoders have achieved success in various natural language\nunderstanding (NLU) tasks, there is a gap between these pretrained encoders and\nnatural language generation (NLG). NLG tasks are often based on the\nencoder-decoder framework, where the pretrained encoders can only benefit part\nof it. To reduce this gap, we introduce DeltaLM, a pretrained multilingual\nencoder-decoder model that regards the decoder as the task layer of\noff-the-shelf pretrained encoders. Specifically, we augment the pretrained\nmultilingual encoder with a decoder and pre-train it in a self-supervised way.\nTo take advantage of both the large-scale monolingual data and bilingual data,\nwe adopt the span corruption and translation span corruption as the\npre-training tasks. Experiments show that DeltaLM outperforms various strong\nbaselines on both natural language generation and translation tasks, including\nmachine translation, abstractive text summarization, data-to-text, and question\ngeneration.",
          "link": "http://arxiv.org/abs/2106.13736",
          "publishedOn": "2021-06-28T01:57:54.252Z",
          "wordCount": 594,
          "title": "DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders. (arXiv:2106.13736v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.07936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erk_K/0/1/0/all/0/1\">Katrin Erk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbelot_A/0/1/0/all/0/1\">Aurelie Herbelot</a>",
          "description": "In this paper, we derive a notion of 'word meaning in context' which accounts\nfor the wide range of lexical shifts and ambiguities observed in utterance\ninterpretation. We characterize the lexical comprehension process as a\ncombination of cognitive semantics and Discourse Representation Theory,\nformalized as a 'situation description system': a probabilistic model which\ntakes utterance understanding to be the mental process of describing one or\nmore situations that would account for an observed utterance. Our model uses\ninsights from different types of generative models to capture the interplay of\nlocal and global contexts and their joint influence upon the lexical\nrepresentation of sentence constituents. We implement the system using a\ndirected graphical model, and apply it to examples containing various\ncontextualisation phenomena.",
          "link": "http://arxiv.org/abs/2009.07936",
          "publishedOn": "2021-06-28T01:57:54.245Z",
          "wordCount": 582,
          "title": "How to marry a star: probabilistic constraints for meaning in context. (arXiv:2009.07936v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Voita_E/0/1/0/all/0/1\">Elena Voita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>",
          "description": "In Neural Machine Translation (and, more generally, conditional language\nmodeling), the generation of a target token is influenced by two types of\ncontext: the source and the prefix of the target sequence. While many attempts\nto understand the internal workings of NMT models have been made, none of them\nexplicitly evaluates relative source and target contributions to a generation\ndecision. We argue that this relative contribution can be evaluated by adopting\na variant of Layerwise Relevance Propagation (LRP). Its underlying\n'conservation principle' makes relevance propagation unique: differently from\nother methods, it evaluates not an abstract quantity reflecting token\nimportance, but the proportion of each token's influence. We extend LRP to the\nTransformer and conduct an analysis of NMT models which explicitly evaluates\nthe source and target relative contributions to the generation process. We\nanalyze changes in these contributions when conditioning on different types of\nprefixes, when varying the training objective or the amount of training data,\nand during the training process. We find that models trained with more data\ntend to rely on source information more and to have more sharp token\ncontributions; the training process is non-monotonic with several stages of\ndifferent nature.",
          "link": "http://arxiv.org/abs/2010.10907",
          "publishedOn": "2021-06-28T01:57:54.237Z",
          "wordCount": 680,
          "title": "Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation. (arXiv:2010.10907v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1808.00054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1\">Michael Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>",
          "description": "Humans read by making a sequence of fixations and saccades. They often skip\nwords, without apparent detriment to understanding. We offer a novel\nexplanation for skipping: readers optimize a tradeoff between performing a\nlanguage-related task and fixating as few words as possible. We propose a\nneural architecture that combines an attention module (deciding whether to skip\nwords) and a task module (memorizing the input). We show that our model\npredicts human skipping behavior, while also modeling reading times well, even\nthough it skips 40% of the input. A key prediction of our model is that\ndifferent reading tasks should result in different skipping behaviors. We\nconfirm this prediction in an eye-tracking experiment in which participants\nanswers questions about a text. We are able to capture these experimental\nresults using the our model, replacing the memorization module with a task\nmodule that performs neural question answering.",
          "link": "http://arxiv.org/abs/1808.00054",
          "publishedOn": "2021-06-28T01:57:54.194Z",
          "wordCount": 611,
          "title": "Modeling Task Effects in Human Reading with Neural Attention. (arXiv:1808.00054v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutt_F/0/1/0/all/0/1\">Florina Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Subhajit Das</a>",
          "description": "Twitter is a useful resource to analyze peoples' opinions on various topics.\nOften these topics are correlated or associated with locations from where these\nTweet posts are made. For example, restaurant owners may need to know where\ntheir target customers eat with respect to the sentiment of the posts made\nrelated to food, policy planners may need to analyze citizens' opinion on\nrelevant issues such as crime, safety, congestion, etc. with respect to\nspecific parts of the city, or county or state. As promising as this is, less\nthan $1\\%$ of the crawled Tweet posts come with geolocation tags. That makes\naccurate prediction of Tweet posts for the non geo-tagged tweets very critical\nto analyze data in various domains. In this research, we utilized millions of\nTwitter posts and end-users domain expertise to build a set of deep neural\nnetwork models using natural language processing (NLP) techniques, that\npredicts the geolocation of non geo-tagged Tweet posts at various level of\ngranularities such as neighborhood, zipcode, and longitude with latitudes. With\nmultiple neural architecture experiments, and a collaborative human-machine\nworkflow design, our ongoing work on geolocation detection shows promising\nresults that empower end-users to correlate relationship between variables of\nchoice with the location information.",
          "link": "http://arxiv.org/abs/2106.13411",
          "publishedOn": "2021-06-28T01:57:54.127Z",
          "wordCount": 646,
          "title": "Fine-grained Geolocation Prediction of Tweets with Human Machine Collaboration. (arXiv:2106.13411v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gezmu_A/0/1/0/all/0/1\">Andargachew Mekonnen Gezmu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lema_T/0/1/0/all/0/1\">Tirufat Tesifaye Lema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyoum_B/0/1/0/all/0/1\">Binyam Ephrem Seyoum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>",
          "description": "This paper presents a manually annotated spelling error corpus for Amharic,\nlingua franca in Ethiopia. The corpus is designed to be used for the evaluation\nof spelling error detection and correction. The misspellings are tagged as\nnon-word and real-word errors. In addition, the contextual information\navailable in the corpus makes it useful in dealing with both types of spelling\nerrors.",
          "link": "http://arxiv.org/abs/2106.13521",
          "publishedOn": "2021-06-28T01:57:54.098Z",
          "wordCount": 503,
          "title": "Manually Annotated Spelling Error Corpus for Amharic. (arXiv:2106.13521v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luong_H/0/1/0/all/0/1\">Hieu-Thi Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>",
          "description": "Generally speaking, the main objective when training a neural speech\nsynthesis system is to synthesize natural and expressive speech from the output\nlayer of the neural network without much attention given to the hidden layers.\nHowever, by learning useful latent representation, the system can be used for\nmany more practical scenarios. In this paper, we investigate the use of\nquantized vectors to model the latent linguistic embedding and compare it with\nthe continuous counterpart. By enforcing different policies over the latent\nspaces in the training, we are able to obtain a latent linguistic embedding\nthat takes on different properties while having a similar performance in terms\nof quality and speaker similarity. Our experiments show that the voice cloning\nsystem built with vector quantization has only a small degradation in terms of\nperceptive evaluations, but has a discrete latent space that is useful for\nreducing the representation bit-rate, which is desirable for data transferring,\nor limiting the information leaking, which is important for speaker\nanonymization and other tasks of that nature.",
          "link": "http://arxiv.org/abs/2106.13479",
          "publishedOn": "2021-06-28T01:57:54.043Z",
          "wordCount": 624,
          "title": "Preliminary study on using vector quantization latent spaces for TTS/VC systems with consistent performance. (arXiv:2106.13479v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13403",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1\">Thi-Hai-Yen Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1\">Quan Minh Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chau Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1\">Binh Tran Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Le Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1\">Ken Satoh</a>",
          "description": "Ambiguity is a characteristic of natural language, which makes expression\nideas flexible. However, in a domain that requires accurate statements, it\nbecomes a barrier. Specifically, a single word can have many meanings and\nmultiple words can have the same meaning. When translating a text into a\nforeign language, the translator needs to determine the exact meaning of each\nelement in the original sentence to produce the correct translation sentence.\nFrom that observation, in this paper, we propose ParaLaw Nets, a pretrained\nmodel family using sentence-level cross-lingual information to reduce ambiguity\nand increase the performance in legal text processing. This approach achieved\nthe best result in the Question Answering task of COLIEE-2021.",
          "link": "http://arxiv.org/abs/2106.13403",
          "publishedOn": "2021-06-28T01:57:54.010Z",
          "wordCount": 574,
          "title": "ParaLaw Nets -- Cross-lingual Sentence-level Pretraining for Legal Text Processing. (arXiv:2106.13403v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1\">Robert L. Logan IV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balazevic_I/0/1/0/all/0/1\">Ivana Bala&#x17e;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>",
          "description": "Prompting language models (LMs) with training examples and task descriptions\nhas been seen as critical to recent successes in few-shot learning. In this\nwork, we show that finetuning LMs in the few-shot setting can considerably\nreduce the need for prompt engineering. In fact, one can use null prompts,\nprompts that contain neither task-specific templates nor training examples, and\nachieve competitive accuracy to manually-tuned prompts across a wide range of\ntasks. While finetuning LMs does introduce new parameters for each downstream\ntask, we show that this memory overhead can be substantially reduced:\nfinetuning only the bias terms can achieve comparable or better accuracy than\nstandard finetuning while only updating 0.1% of the parameters. All in all, we\nrecommend finetuning LMs for few-shot learning as it is more accurate, robust\nto different prompts, and can be made nearly as efficient as using frozen LMs.",
          "link": "http://arxiv.org/abs/2106.13353",
          "publishedOn": "2021-06-28T01:57:54.004Z",
          "wordCount": 592,
          "title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models. (arXiv:2106.13353v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "Large pre-trained models have achieved great success in many natural language\nprocessing tasks. However, when they are applied in specific domains, these\nmodels suffer from domain shift and bring challenges in fine-tuning and online\nserving for latency and capacity constraints. In this paper, we present a\ngeneral approach to developing small, fast and effective pre-trained models for\nspecific domains. This is achieved by adapting the off-the-shelf general\npre-trained models and performing task-agnostic knowledge distillation in\ntarget domains. Specifically, we propose domain-specific vocabulary expansion\nin the adaptation stage and employ corpus level occurrence probability to\nchoose the size of incremental vocabulary automatically. Then we systematically\nexplore different strategies to compress the large pre-trained models for\nspecific domains. We conduct our experiments in the biomedical and computer\nscience domain. The experimental results demonstrate that our approach achieves\nbetter performance over the BERT BASE model in domain-specific tasks while 3.3x\nsmaller and 5.1x faster than BERT BASE. The code and pre-trained models are\navailable at https://aka.ms/adalm.",
          "link": "http://arxiv.org/abs/2106.13474",
          "publishedOn": "2021-06-28T01:57:53.783Z",
          "wordCount": 604,
          "title": "Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains. (arXiv:2106.13474v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1\">Thi-Hai-Yen Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1\">Quan Minh Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chau Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1\">Binh Tran Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Le Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1\">Ken Satoh</a>",
          "description": "COLIEE is an annual competition in automatic computerized legal text\nprocessing. Automatic legal document processing is an ambitious goal, and the\nstructure and semantics of the law are often far more complex than everyday\nlanguage. In this article, we survey and report our methods and experimental\nresults in using deep learning in legal document processing. The results show\nthe difficulties as well as potentials in this family of approaches.",
          "link": "http://arxiv.org/abs/2106.13405",
          "publishedOn": "2021-06-28T01:57:53.734Z",
          "wordCount": 532,
          "title": "JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE 2021. (arXiv:2106.13405v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1\">Alexandre Drouin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>",
          "description": "This article introduces byteSteady -- a fast model for classification using\nbyte-level n-gram embeddings. byteSteady assumes that each input comes as a\nsequence of bytes. A representation vector is produced using the averaged\nembedding vectors of byte-level n-grams, with a pre-defined set of n. The\nhashing trick is used to reduce the number of embedding vectors. This input\nrepresentation vector is then fed into a linear classifier. A straightforward\napplication of byteSteady is text classification. We also apply byteSteady to\none type of non-language data -- DNA sequences for gene classification. For\nboth problems we achieved competitive classification results against strong\nbaselines, suggesting that byteSteady can be applied to both language and\nnon-language data. Furthermore, we find that simple compression using Huffman\ncoding does not significantly impact the results, which offers an\naccuracy-speed trade-off previously unexplored in machine learning.",
          "link": "http://arxiv.org/abs/2106.13302",
          "publishedOn": "2021-06-28T01:57:53.715Z",
          "wordCount": 571,
          "title": "byteSteady: Fast Classification Using Byte-Level n-Gram Embeddings. (arXiv:2106.13302v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogahn_R/0/1/0/all/0/1\">Richard Rogahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul N. Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>",
          "description": "Information overload is a prevalent challenge in many high-value domains. A\nprominent case in point is the explosion of the biomedical literature on\nCOVID-19, which swelled to hundreds of thousands of papers in a matter of\nmonths. In general, biomedical literature expands by two papers every minute,\ntotalling over a million new papers every year. Search in the biomedical realm,\nand many other vertical domains is challenging due to the scarcity of direct\nsupervision from click logs. Self-supervised learning has emerged as a\npromising direction to overcome the annotation bottleneck. We propose a general\napproach for vertical search based on domain-specific pretraining and present a\ncase study for the biomedical domain. Despite being substantially simpler and\nnot using any relevance labels for training or development, our method performs\ncomparably or better than the best systems in the official TREC-COVID\nevaluation, a COVID-related biomedical search competition. Using distributed\ncomputing in modern cloud infrastructure, our system can scale to tens of\nmillions of articles on PubMed and has been deployed as Microsoft Biomedical\nSearch, a new search experience for biomedical literature:\nhttps://aka.ms/biomedsearch.",
          "link": "http://arxiv.org/abs/2106.13375",
          "publishedOn": "2021-06-28T01:57:53.704Z",
          "wordCount": 693,
          "title": "Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McGovern_H/0/1/0/all/0/1\">Hope McGovern</a>",
          "description": "It is well-documented that word embeddings trained on large public corpora\nconsistently exhibit known human social biases. Although many methods for\ndebiasing exist, almost all fixate on completely eliminating biased information\nfrom the embeddings and often diminish training set size in the process. In\nthis paper, we present a simple yet effective method for debiasing GloVe word\nembeddings (Pennington et al., 2014) which works by incorporating explicit\ninformation about training set bias rather than removing biased data outright.\nOur method runs quickly and efficiently with the help of a fast bias gradient\napproximation method from Brunet et al. (2019). As our approach is akin to the\nnotion of 'source criticism' in the humanities, we term our method\nSource-Critical GloVe (SC-GloVe). We show that SC-GloVe reduces the effect size\non Word Embedding Association Test (WEAT) sets without sacrificing training\ndata or TOP-1 performance.",
          "link": "http://arxiv.org/abs/2106.13382",
          "publishedOn": "2021-06-28T01:57:53.661Z",
          "wordCount": 569,
          "title": "A Source-Criticism Debiasing Method for GloVe Embeddings. (arXiv:2106.13382v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kacupaj_E/0/1/0/all/0/1\">Endri Kacupaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premnadh_S/0/1/0/all/0/1\">Shyamnath Premnadh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kuldeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maleshkova_M/0/1/0/all/0/1\">Maria Maleshkova</a>",
          "description": "In recent years, there have been significant developments in Question\nAnswering over Knowledge Graphs (KGQA). Despite all the notable advancements,\ncurrent KGQA systems only focus on answer generation techniques and not on\nanswer verbalization. However, in real-world scenarios (e.g., voice assistants\nsuch as Alexa, Siri, etc.), users prefer verbalized answers instead of a\ngenerated response. This paper addresses the task of answer verbalization for\n(complex) question answering over knowledge graphs. In this context, we propose\na multi-task-based answer verbalization framework: VOGUE (Verbalization thrOuGh\nmUlti-task lEarning). The VOGUE framework attempts to generate a verbalized\nanswer using a hybrid approach through a multi-task learning paradigm. Our\nframework can generate results based on using questions and queries as inputs\nconcurrently. VOGUE comprises four modules that are trained simultaneously\nthrough multi-task learning. We evaluate our framework on existing datasets for\nanswer verbalization, and it outperforms all current baselines on both BLEU and\nMETEOR scores.",
          "link": "http://arxiv.org/abs/2106.13316",
          "publishedOn": "2021-06-28T01:57:53.600Z",
          "wordCount": 593,
          "title": "VOGUE: Answer Verbalization through Multi-Task Learning. (arXiv:2106.13316v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kacupaj_E/0/1/0/all/0/1\">Endri Kacupaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1\">Joan Plepi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kuldeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_H/0/1/0/all/0/1\">Harsh Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maleshkova_M/0/1/0/all/0/1\">Maria Maleshkova</a>",
          "description": "This paper addresses the task of (complex) conversational question answering\nover a knowledge graph. For this task, we propose LASAGNE (muLti-task semAntic\nparSing with trAnsformer and Graph atteNtion nEtworks). It is the first\napproach, which employs a transformer architecture extended with Graph\nAttention Networks for multi-task neural semantic parsing. LASAGNE uses a\ntransformer model for generating the base logical forms, while the Graph\nAttention model is used to exploit correlations between (entity) types and\npredicates to produce node representations. LASAGNE also includes a novel\nentity recognition module which detects, links, and ranks all relevant entities\nin the question context. We evaluate LASAGNE on a standard dataset for complex\nsequential question answering, on which it outperforms existing baseline\naverages on all question types. Specifically, we show that LASAGNE improves the\nF1-score on eight out of ten question types; in some cases, the increase in\nF1-score is more than 20% compared to the state of the art.",
          "link": "http://arxiv.org/abs/2104.01569",
          "publishedOn": "2021-06-25T02:00:46.117Z",
          "wordCount": 642,
          "title": "Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks. (arXiv:2104.01569v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1\">Mark Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez Rodr&#xed;guez</a>",
          "description": "We present the system submission from the FASTPARSE team for the EUD Shared\nTask at IWPT 2021. We engaged in the task last year by focusing on efficiency.\nThis year we have focused on experimenting with new ideas on a limited time\nbudget. Our system is based on splitting the EUD graph into several trees,\nbased on linguistic criteria. We predict these trees using a sequence-labelling\nparser and combine them into an EUD graph. The results were relatively poor,\nalthough not a total disaster and could probably be improved with some\npolishing of the system's rough edges.",
          "link": "http://arxiv.org/abs/2106.13155",
          "publishedOn": "2021-06-25T02:00:46.058Z",
          "wordCount": 549,
          "title": "Splitting EUD graphs into trees: A quick and clatty approach. (arXiv:2106.13155v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "In spoken conversational question answering (SCQA), the answer to the\ncorresponding question is generated by retrieving and then analyzing a fixed\nspoken document, including multi-part conversations. Most SCQA systems have\nconsidered only retrieving information from ordered utterances. However, the\nsequential order of dialogue is important to build a robust spoken\nconversational question answering system, and the changes of utterances order\nmay severely result in low-quality and incoherent corpora. To this end, we\nintroduce a self-supervised learning approach, including incoherence\ndiscrimination, insertion detection, and question prediction, to explicitly\ncapture the coreference resolution and dialogue coherence among spoken\ndocuments. Specifically, we design a joint learning framework where the\nauxiliary self-supervised tasks can enable the pre-trained SCQA systems towards\nmore coherent and meaningful spoken dialogue learning. We also utilize the\nproposed self-supervised learning tasks to capture intra-sentence coherence.\nExperimental results demonstrate that our proposed method provides more\ncoherent, meaningful, and appropriate responses, yielding superior performance\ngains compared to the original pre-trained language models. Our method achieves\nstate-of-the-art results on the Spoken-CoQA dataset.",
          "link": "http://arxiv.org/abs/2106.02182",
          "publishedOn": "2021-06-25T02:00:44.993Z",
          "wordCount": 645,
          "title": "Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seanie Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minki Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Juho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "QA models based on pretrained language mod-els have achieved remarkable\nperformance on various benchmark datasets.However, QA models do not generalize\nwell to unseen data that falls outside the training distribution, due to\ndistributional shifts.Data augmentation (DA) techniques which drop/replace\nwords have shown to be effective in regularizing the model from overfitting to\nthe training data.Yet, they may adversely affect the QA tasks since they incur\nsemantic changes that may lead to wrong answers for the QA task. To tackle this\nproblem, we propose a simple yet effective DA method based on a stochastic\nnoise generator, which learns to perturb the word embedding of the input\nquestions and context without changing their semantics. We validate the\nperformance of the QA models trained with our word embedding perturbation on a\nsingle source dataset, on five different target domains.The results show that\nour method significantly outperforms the baselineDA methods. Notably, the model\ntrained with ours outperforms the model trained with more than 240K\nartificially generated QA pairs.",
          "link": "http://arxiv.org/abs/2105.02692",
          "publishedOn": "2021-06-25T02:00:44.973Z",
          "wordCount": 637,
          "title": "Learning to Perturb Word Embeddings for Out-of-distribution QA. (arXiv:2105.02692v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pogrebnyakov_N/0/1/0/all/0/1\">Nicolai Pogrebnyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaghaghian_S/0/1/0/all/0/1\">Shohreh Shaghaghian</a>",
          "description": "Transfer learning methods, and in particular domain adaptation, help exploit\nlabeled data in one domain to improve the performance of a certain task in\nanother domain. However, it is still not clear what factors affect the success\nof domain adaptation. This paper models adaptation success and selection of the\nmost suitable source domains among several candidates in text similarity. We\nuse descriptive domain information and cross-domain similarity metrics as\npredictive features. While mostly positive, the results also point to some\ndomains where adaptation success was difficult to predict.",
          "link": "http://arxiv.org/abs/2106.04641",
          "publishedOn": "2021-06-25T02:00:44.963Z",
          "wordCount": 543,
          "title": "Predicting the Success of Domain Adaptation in Text Similarity. (arXiv:2106.04641v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1\">Joan Plepi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacupaj_E/0/1/0/all/0/1\">Endri Kacupaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kuldeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_H/0/1/0/all/0/1\">Harsh Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>",
          "description": "Neural semantic parsing approaches have been widely used for Question\nAnswering (QA) systems over knowledge graphs. Such methods provide the\nflexibility to handle QA datasets with complex queries and a large number of\nentities. In this work, we propose a novel framework named CARTON, which\nperforms multi-task semantic parsing for handling the problem of conversational\nquestion answering over a large-scale knowledge graph. Our framework consists\nof a stack of pointer networks as an extension of a context transformer model\nfor parsing the input question and the dialog history. The framework generates\na sequence of actions that can be executed on the knowledge graph. We evaluate\nCARTON on a standard dataset for complex sequential question answering on which\nCARTON outperforms all baselines. Specifically, we observe performance\nimprovements in F1-score on eight out of ten question types compared to the\nprevious state of the art. For logical reasoning questions, an improvement of\n11 absolute points is reached.",
          "link": "http://arxiv.org/abs/2103.07766",
          "publishedOn": "2021-06-25T02:00:44.946Z",
          "wordCount": 637,
          "title": "Context Transformer with Stacked Pointer Networks for Conversational Question Answering over Knowledge Graphs. (arXiv:2103.07766v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>",
          "description": "Inferring social relations from dialogues is vital for building emotionally\nintelligent robots to interpret human language better and act accordingly. We\nmodel the social network as an And-or Graph, named SocAoG, for the consistency\nof relations among a group and leveraging attributes as inference cues.\nMoreover, we formulate a sequential structure prediction task, and propose an\n$\\alpha$-$\\beta$-$\\gamma$ strategy to incrementally parse SocAoG for the\ndynamic inference upon any incoming utterance: (i) an $\\alpha$ process\npredicting attributes and relations conditioned on the semantics of dialogues,\n(ii) a $\\beta$ process updating the social relations based on related\nattributes, and (iii) a $\\gamma$ process updating individual's attributes based\non interpersonal social relations. Empirical results on DialogRE and MovieGraph\nshow that our model infers social relations more accurately than the\nstate-of-the-art methods. Moreover, the ablation study shows the three\nprocesses complement each other, and the case study demonstrates the dynamic\nrelational inference.",
          "link": "http://arxiv.org/abs/2106.01006",
          "publishedOn": "2021-06-25T02:00:44.940Z",
          "wordCount": 622,
          "title": "SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues. (arXiv:2106.01006v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chiyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>",
          "description": "As machine learning methods are deployed in real-world settings such as\nhealthcare, legal systems, and social science, it is crucial to recognize how\nthey shape social biases and stereotypes in these sensitive decision-making\nprocesses. Among such real-world deployments are large-scale pretrained\nlanguage models (LMs) that can be potentially dangerous in manifesting\nundesirable representational biases - harmful biases resulting from\nstereotyping that propagate negative generalizations involving gender, race,\nreligion, and other social constructs. As a step towards improving the fairness\nof LMs, we carefully define several sources of representational biases before\nproposing new benchmarks and metrics to measure them. With these tools, we\npropose steps towards mitigating social biases during text generation. Our\nempirical results and human evaluation demonstrate effectiveness in mitigating\nbias while retaining crucial contextual information for high-fidelity text\ngeneration, thereby pushing forward the performance-fairness Pareto frontier.",
          "link": "http://arxiv.org/abs/2106.13219",
          "publishedOn": "2021-06-25T02:00:44.927Z",
          "wordCount": 596,
          "title": "Towards Understanding and Mitigating Social Biases in Language Models. (arXiv:2106.13219v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Spoken conversational question answering (SCQA) requires machines to model\ncomplex dialogue flow given the speech utterances and text corpora. Different\nfrom traditional text question answering (QA) tasks, SCQA involves audio signal\nprocessing, passage comprehension, and contextual understanding. However, ASR\nsystems introduce unexpected noisy signals to the transcriptions, which result\nin performance degradation on SCQA. To overcome the problem, we propose CADNet,\na novel contextualized attention-based distillation approach, which applies\nboth cross-attention and self-attention to obtain ASR-robust contextualized\nembedding representations of the passage and dialogue history for performance\nimprovements. We also introduce the spoken conventional knowledge distillation\nframework to distill the ASR-robust knowledge from the estimated probabilities\nof the teacher model to the student. We conduct extensive experiments on the\nSpoken-CoQA dataset and demonstrate that our approach achieves remarkable\nperformance in this task.",
          "link": "http://arxiv.org/abs/2010.11066",
          "publishedOn": "2021-06-25T02:00:44.921Z",
          "wordCount": 631,
          "title": "Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1\">Tomasz Limisiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marecek_D/0/1/0/all/0/1\">David Mare&#x10d;ek</a>",
          "description": "With the recent success of pre-trained models in NLP, a significant focus was\nput on interpreting their representations. One of the most prominent approaches\nis structural probing (Hewitt and Manning, 2019), where a linear projection of\nword embeddings is performed in order to approximate the topology of dependency\nstructures. In this work, we introduce a new type of structural probing, where\nthe linear projection is decomposed into 1. isomorphic space rotation; 2.\nlinear scaling that identifies and scales the most relevant dimensions. In\naddition to syntactic dependency, we evaluate our method on novel tasks\n(lexical hypernymy and position in a sentence). We jointly train the probes for\nmultiple tasks and experimentally show that lexical and syntactic information\nis separated in the representations. Moreover, the orthogonal constraint makes\nthe Structural Probes less vulnerable to memorization.",
          "link": "http://arxiv.org/abs/2012.15228",
          "publishedOn": "2021-06-25T02:00:44.903Z",
          "wordCount": 583,
          "title": "Introducing Orthogonal Constraint in Structural Probes. (arXiv:2012.15228v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tammewar_A/0/1/0/all/0/1\">Aniruddha Tammewar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cervone_A/0/1/0/all/0/1\">Alessandra Cervone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riccardi_G/0/1/0/all/0/1\">Giuseppe Riccardi</a>",
          "description": "Personal Narratives (PN) - recollections of facts, events, and thoughts from\none's own experience - are often used in everyday conversations. So far, PNs\nhave mainly been explored for tasks such as valence prediction or emotion\nclassification (e.g. happy, sad). However, these tasks might overlook more\nfine-grained information that could prove to be relevant for understanding PNs.\nIn this work, we propose a novel task for Narrative Understanding: Emotion\nCarrier Recognition (ECR). Emotion carriers, the text fragments that carry the\nemotions of the narrator (e.g. loss of a grandpa, high school reunion), provide\na fine-grained description of the emotion state. We explore the task of ECR in\na corpus of PNs manually annotated with emotion carriers and investigate\ndifferent machine learning models for the task. We propose evaluation\nstrategies for ECR including metrics that can be appropriate for different\ntasks.",
          "link": "http://arxiv.org/abs/2008.07481",
          "publishedOn": "2021-06-25T02:00:44.896Z",
          "wordCount": 604,
          "title": "Emotion Carrier Recognition from Personal Narratives. (arXiv:2008.07481v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Terrance Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_A/0/1/0/all/0/1\">Anna Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muszynski_M/0/1/0/all/0/1\">Michal Muszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_R/0/1/0/all/0/1\">Ryo Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1\">Nicholas Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1\">Randy Auerbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brent_D/0/1/0/all/0/1\">David Brent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Mental health conditions remain underdiagnosed even in countries with common\naccess to advanced medical care. The ability to accurately and efficiently\npredict mood from easily collectible data has several important implications\nfor the early detection, intervention, and treatment of mental health\ndisorders. One promising data source to help monitor human behavior is daily\nsmartphone usage. However, care must be taken to summarize behaviors without\nidentifying the user through personal (e.g., personally identifiable\ninformation) or protected (e.g., race, gender) attributes. In this paper, we\nstudy behavioral markers of daily mood using a recent dataset of mobile\nbehaviors from adolescent populations at high risk of suicidal behaviors. Using\ncomputational models, we find that language and multimodal representations of\nmobile typed text (spanning typed characters, words, keystroke timings, and app\nusage) are predictive of daily mood. However, we find that models trained to\npredict mood often also capture private user identities in their intermediate\nrepresentations. To tackle this problem, we evaluate approaches that obfuscate\nuser identity while remaining predictive. By combining multimodal\nrepresentations with privacy-preserving learning, we are able to push forward\nthe performance-privacy frontier.",
          "link": "http://arxiv.org/abs/2106.13213",
          "publishedOn": "2021-06-25T02:00:44.881Z",
          "wordCount": 656,
          "title": "Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data. (arXiv:2106.13213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Ke-Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Bo-Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuan-Yu Chen</a>",
          "description": "In this paper, inspired by the successes of visionlanguage pre-trained models\nand the benefits from training with adversarial attacks, we present a novel\ntransformerbased cross-modal fusion modeling by incorporating the both notions\nfor VQA challenge 2021. Specifically, the proposed model is on top of the\narchitecture of VinVL model [19], and the adversarial training strategy [4] is\napplied to make the model robust and generalized. Moreover, two implementation\ntricks are also used in our system to obtain better results. The experiments\ndemonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.",
          "link": "http://arxiv.org/abs/2106.13033",
          "publishedOn": "2021-06-25T02:00:44.853Z",
          "wordCount": 548,
          "title": "A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021. (arXiv:2106.13033v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.07987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troshin_S/0/1/0/all/0/1\">Sergey Troshin</a>",
          "description": "Initially developed for natural language processing (NLP), Transformers are\nnow widely used for source code processing, due to the format similarity\nbetween source code and text. In contrast to natural language, source code is\nstrictly structured, i.e., it follows the syntax of the programming language.\nSeveral recent works develop Transformer modifications for capturing syntactic\ninformation in source code. The drawback of these works is that they do not\ncompare to each other and consider different tasks. In this work, we conduct a\nthorough empirical study of the capabilities of Transformers to utilize\nsyntactic information in different tasks. We consider three tasks (code\ncompletion, function naming and bug fixing) and re-implement different\nsyntax-capturing modifications in a unified framework. We show that\nTransformers are able to make meaningful predictions based purely on syntactic\ninformation and underline the best practices of taking the syntactic\ninformation into account for improving the performance of the model.",
          "link": "http://arxiv.org/abs/2010.07987",
          "publishedOn": "2021-06-25T02:00:44.846Z",
          "wordCount": 632,
          "title": "Empirical Study of Transformers for Source Code. (arXiv:2010.07987v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohiuddin_T/0/1/0/all/0/1\">Tasnim Mohiuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>",
          "description": "Transfer learning has yielded state-of-the-art (SoTA) results in many\nsupervised NLP tasks. However, annotated data for every target task in every\ntarget language is rare, especially for low-resource languages. We propose\nUXLA, a novel unsupervised data augmentation framework for zero-resource\ntransfer learning scenarios. In particular, UXLA aims to solve cross-lingual\nadaptation problems from a source language task distribution to an unknown\ntarget language task distribution, assuming no training label in the target\nlanguage. At its core, UXLA performs simultaneous self-training with data\naugmentation and unsupervised sample selection. To show its effectiveness, we\nconduct extensive experiments on three diverse zero-resource cross-lingual\ntransfer tasks. UXLA achieves SoTA results in all the tasks, outperforming the\nbaselines by a good margin. With an in-depth framework dissection, we\ndemonstrate the cumulative contributions of different components to its\nsuccess.",
          "link": "http://arxiv.org/abs/2004.13240",
          "publishedOn": "2021-06-25T02:00:44.829Z",
          "wordCount": 611,
          "title": "UXLA: A Robust Unsupervised Data Augmentation Framework for {Zero-Resource} Cross-Lingual NLP. (arXiv:2004.13240v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>",
          "description": "We introduce the largest transcribed Arabic speech corpus, QASR, collected\nfrom the broadcast domain. This multi-dialect speech dataset contains 2,000\nhours of speech sampled at 16kHz crawled from Aljazeera news channel. The\ndataset is released with lightly supervised transcriptions, aligned with the\naudio segments. Unlike previous datasets, QASR contains linguistically\nmotivated segmentation, punctuation, speaker information among others. QASR is\nsuitable for training and evaluating speech recognition systems, acoustics-\nand/or linguistics- based Arabic dialect identification, punctuation\nrestoration, speaker identification, speaker linking, and potentially other NLP\nmodules for spoken data. In addition to QASR transcription, we release a\ndataset of 130M words to aid in designing and training a better language model.\nWe show that end-to-end automatic speech recognition trained on QASR reports a\ncompetitive word error rate compared to the previous MGB-2 corpus. We report\nbaseline results for downstream natural language processing tasks such as named\nentity recognition using speech transcript. We also report the first baseline\nfor Arabic punctuation restoration. We make the corpus available for the\nresearch community.",
          "link": "http://arxiv.org/abs/2106.13000",
          "publishedOn": "2021-06-25T02:00:44.817Z",
          "wordCount": 645,
          "title": "QASR: QCRI Aljazeera Speech Resource -- A Large Scale Annotated Arabic Speech Corpus. (arXiv:2106.13000v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.13985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Alexis Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>",
          "description": "Humans have been shown to give contrastive explanations, which explain why an\nobserved event happened rather than some other counterfactual event (the\ncontrast case). Despite the influential role that contrastivity plays in how\nhumans explain, this property is largely missing from current methods for\nexplaining NLP models. We present Minimal Contrastive Editing (MiCE), a method\nfor producing contrastive explanations of model predictions in the form of\nedits to inputs that change model outputs to the contrast case. Our experiments\nacross three tasks--binary sentiment classification, topic classification, and\nmultiple-choice question answering--show that MiCE is able to produce edits\nthat are not only contrastive, but also minimal and fluent, consistent with\nhuman contrastive edits. We demonstrate how MiCE edits can be used for two use\ncases in NLP system development--debugging incorrect model outputs and\nuncovering dataset artifacts--and thereby illustrate that producing contrastive\nexplanations is a promising research direction for model interpretability.",
          "link": "http://arxiv.org/abs/2012.13985",
          "publishedOn": "2021-06-25T02:00:44.796Z",
          "wordCount": 612,
          "title": "Explaining NLP Models via Minimal Contrastive Editing (MiCE). (arXiv:2012.13985v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghannay_S/0/1/0/all/0/1\">Sahar Ghannay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caubriere_A/0/1/0/all/0/1\">Antoine Caubri&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mdhaffar_S/0/1/0/all/0/1\">Salima Mdhaffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laperriere_G/0/1/0/all/0/1\">Ga&#xeb;lle Laperri&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabaian_B/0/1/0/all/0/1\">Bassam Jabaian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>",
          "description": "Spoken language understanding (SLU) topic has seen a lot of progress these\nlast three years, with the emergence of end-to-end neural approaches. Spoken\nlanguage understanding refers to natural language processing tasks related to\nsemantic extraction from speech signal, like named entity recognition from\nspeech or slot filling task in a context of human-machine dialogue.\nClassically, SLU tasks were processed through a cascade approach that consists\nin applying, firstly, an automatic speech recognition process, followed by a\nnatural language processing module applied to the automatic transcriptions.\nThese three last years, end-to-end neural approaches, based on deep neural\nnetworks, have been proposed in order to directly extract the semantics from\nspeech signal, by using a single neural model. More recent works on\nself-supervised training with unlabeled data open new perspectives in term of\nperformance for automatic speech recognition and natural language processing.\nIn this paper, we present a brief overview of the recent advances on the French\nMEDIA benchmark dataset for SLU, with or without the use of additional data. We\nalso present our last results that significantly outperform the current\nstate-of-the-art with a Concept Error Rate (CER) of 11.2%, instead of 13.6% for\nthe last state-of-the-art system presented this year.",
          "link": "http://arxiv.org/abs/2106.13045",
          "publishedOn": "2021-06-25T02:00:44.789Z",
          "wordCount": 658,
          "title": "Where are we in semantic concept extraction for Spoken Language Understanding?. (arXiv:2106.13045v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Katsis_Y/0/1/0/all/0/1\">Yannis Katsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1\">Saneem Chemmengath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Samarth Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canim_M/0/1/0/all/0/1\">Mustafa Canim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1\">Michael Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feifei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_J/0/1/0/all/0/1\">Jaydeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_K/0/1/0/all/0/1\">Karthik Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>",
          "description": "Recent advances in transformers have enabled Table Question Answering (Table\nQA) systems to achieve high accuracy and SOTA results on open domain datasets\nlike WikiTableQuestions and WikiSQL. Such transformers are frequently\npre-trained on open-domain content such as Wikipedia, where they effectively\nencode questions and corresponding tables from Wikipedia as seen in Table QA\ndataset. However, web tables in Wikipedia are notably flat in their layout,\nwith the first row as the sole column header. The layout lends to a relational\nview of tables where each row is a tuple. Whereas, tables in domain-specific\nbusiness or scientific documents often have a much more complex layout,\nincluding hierarchical row and column headers, in addition to having\nspecialized vocabulary terms from that domain.\n\nTo address this problem, we introduce the domain-specific Table QA dataset\nAIT-QA (Airline Industry Table QA). The dataset consists of 515 questions\nauthored by human annotators on 116 tables extracted from public U.S. SEC\nfilings (publicly available at: https://www.sec.gov/edgar.shtml) of major\nairline companies for the fiscal years 2017-2019. We also provide annotations\npertaining to the nature of questions, marking those that require hierarchical\nheaders, domain-specific terminology, and paraphrased forms. Our zero-shot\nbaseline evaluation of three transformer-based SOTA Table QA methods - TaPAS\n(end-to-end), TaBERT (semantic parsing-based), and RCI (row-column\nencoding-based) - clearly exposes the limitation of these methods in this\npractical setting, with the best accuracy at just 51.8\\% (RCI). We also present\npragmatic table preprocessing steps used to pivot and project these complex\ntables into a layout suitable for the SOTA Table QA models.",
          "link": "http://arxiv.org/abs/2106.12944",
          "publishedOn": "2021-06-25T02:00:44.771Z",
          "wordCount": 714,
          "title": "AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry. (arXiv:2106.12944v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahnala_A/0/1/0/all/0/1\">Allison Lahnala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuntian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_C/0/1/0/all/0/1\">Charles Welch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1\">Jonathan K. Kummerfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Lawrence An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resnicow_K/0/1/0/all/0/1\">Kenneth Resnicow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Rosas_V/0/1/0/all/0/1\">Ver&#xf3;nica P&#xe9;rez-Rosas</a>",
          "description": "A growing number of people engage in online health forums, making it\nimportant to understand the quality of the advice they receive. In this paper,\nwe explore the role of expertise in responses provided to help-seeking posts\nregarding mental health. We study the differences between (1) interactions with\npeers; and (2) interactions with self-identified mental health professionals.\nFirst, we show that a classifier can distinguish between these two groups,\nindicating that their language use does in fact differ. To understand this\ndifference, we perform several analyses addressing engagement aspects,\nincluding whether their comments engage the support-seeker further as well as\nlinguistic aspects, such as dominant language and linguistic style matching.\nOur work contributes toward the developing efforts of understanding how health\nexperts engage with health information- and support-seekers in social networks.\nMore broadly, it is a step toward a deeper understanding of the styles of\ninteractions that cultivate supportive engagement in online communities.",
          "link": "http://arxiv.org/abs/2106.12976",
          "publishedOn": "2021-06-25T02:00:44.764Z",
          "wordCount": 601,
          "title": "Exploring Self-Identified Counseling Expertise in Online Support Forums. (arXiv:2106.12976v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Solbiati_A/0/1/0/all/0/1\">Alessandro Solbiati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1\">Kevin Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damaskinos_G/0/1/0/all/0/1\">Georgios Damaskinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1\">Shivani Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_S/0/1/0/all/0/1\">Shubham Modi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cali_J/0/1/0/all/0/1\">Jacques Cali</a>",
          "description": "Topic segmentation of meetings is the task of dividing multi-person meeting\ntranscripts into topic blocks. Supervised approaches to the problem have proven\nintractable due to the difficulties in collecting and accurately annotating\nlarge datasets. In this paper we show how previous unsupervised topic\nsegmentation methods can be improved using pre-trained neural architectures. We\nintroduce an unsupervised approach based on BERT embeddings that achieves a\n15.5% reduction in error rate over existing unsupervised approaches applied to\ntwo popular datasets for meeting transcripts.",
          "link": "http://arxiv.org/abs/2106.12978",
          "publishedOn": "2021-06-25T02:00:44.700Z",
          "wordCount": 519,
          "title": "Unsupervised Topic Segmentation of Meetings with BERT Embeddings. (arXiv:2106.12978v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandrahas/0/1/0/all/0/1\">Chandrahas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Pratim Talukdar</a>",
          "description": "Open Knowledge Graphs (OpenKG) refer to a set of (head noun phrase, relation\nphrase, tail noun phrase) triples such as (tesla, return to, new york)\nextracted from a corpus using OpenIE tools. While OpenKGs are easy to bootstrap\nfor a domain, they are very sparse and far from being directly usable in an end\ntask. Therefore, the task of predicting new facts, i.e., link prediction,\nbecomes an important step while using these graphs in downstream tasks such as\ntext comprehension, question answering, and web search query recommendation.\nLearning embeddings for OpenKGs is one approach for link prediction that has\nreceived some attention lately. However, on careful examination, we found that\ncurrent OpenKG link prediction algorithms often predict noun phrases (NPs) with\nincompatible types for given noun and relation phrases. We address this problem\nin this work and propose OKGIT that improves OpenKG link prediction using novel\ntype compatibility score and type regularization. With extensive experiments on\nmultiple datasets, we show that the proposed method achieves state-of-the-art\nperformance while producing type compatible NPs in the link prediction task.",
          "link": "http://arxiv.org/abs/2106.12806",
          "publishedOn": "2021-06-25T02:00:44.690Z",
          "wordCount": 613,
          "title": "OKGIT: Open Knowledge Graph Link Prediction with Implicit Types. (arXiv:2106.12806v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_C/0/1/0/all/0/1\">Christiaan Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>",
          "description": "Acoustic word embedding models map variable duration speech segments to fixed\ndimensional vectors, enabling efficient speech search and discovery. Previous\nwork explored how embeddings can be obtained in zero-resource settings where no\nlabelled data is available in the target language. The current best approach\nuses transfer learning: a single supervised multilingual model is trained using\nlabelled data from multiple well-resourced languages and then applied to a\ntarget zero-resource language (without fine-tuning). However, it is still\nunclear how the specific choice of training languages affect downstream\nperformance. Concretely, here we ask whether it is beneficial to use training\nlanguages related to the target. Using data from eleven languages spoken in\nSouthern Africa, we experiment with adding data from different language\nfamilies while controlling for the amount of data per language. In word\ndiscrimination and query-by-example search evaluations, we show that training\non languages from the same family gives large improvements. Through\nfiner-grained analysis, we show that training on even just a single related\nlanguage gives the largest gain. We also find that adding data from unrelated\nlanguages generally doesn't hurt performance.",
          "link": "http://arxiv.org/abs/2106.12834",
          "publishedOn": "2021-06-25T02:00:44.681Z",
          "wordCount": 641,
          "title": "Multilingual transfer of acoustic word embeddings improves when training on languages related to the target zero-resource language. (arXiv:2106.12834v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murauer_B/0/1/0/all/0/1\">Benjamin Murauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschuggnall_M/0/1/0/all/0/1\">Michael Tschuggnall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specht_G/0/1/0/all/0/1\">G&#xfc;nther Specht</a>",
          "description": "In the last decade, machine translation has become a popular means to deal\nwith multilingual digital content. By providing higher quality translations,\nobfuscating the source language of a text becomes more attractive. In this\npaper, we analyze the ability to detect the source language from the translated\noutput of two widely used commercial machine translation systems by utilizing\nmachine-learning algorithms with basic textual features like n-grams.\nEvaluations show that the source language can be reconstructed with high\naccuracy for documents that contain a sufficient amount of translated text. In\naddition, we analyze how the document size influences the performance of the\nprediction, as well as how limiting the set of possible source languages\nimproves the classification accuracy.",
          "link": "http://arxiv.org/abs/2106.12830",
          "publishedOn": "2021-06-25T02:00:44.648Z",
          "wordCount": 571,
          "title": "On the Influence of Machine Translation on Language Origin Obfuscation. (arXiv:2106.12830v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandle_S/0/1/0/all/0/1\">Sebastian Br&#xe4;ndle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanussek_M/0/1/0/all/0/1\">Marc Hanussek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blohm_M/0/1/0/all/0/1\">Matthias Blohm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1\">Maximilien Kintz</a>",
          "description": "Automated Machine Learning (AutoML) has gained increasing success on tabular\ndata in recent years. However, processing unstructured data like text is a\nchallenge and not widely supported by open-source AutoML tools. This work\ncompares three manually created text representations and text embeddings\nautomatically created by AutoML tools. Our benchmark includes four popular\nopen-source AutoML tools and eight datasets for text classification purposes.\nThe results show that straightforward text representations perform better than\nAutoML tools with automatically created text embeddings.",
          "link": "http://arxiv.org/abs/2106.12798",
          "publishedOn": "2021-06-25T02:00:44.639Z",
          "wordCount": 526,
          "title": "Evaluation of Representation Models for Text Classification with AutoML Tools. (arXiv:2106.12798v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schutte_D/0/1/0/all/0/1\">Dalton Schutte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1\">Jake Vasilakes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bompelli_A/0/1/0/all/0/1\">Anu Bompelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiszman_M/0/1/0/all/0/1\">Marcelo Fiszman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilicoglu_H/0/1/0/all/0/1\">Halil Kilicoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bishop_J/0/1/0/all/0/1\">Jeffrey R. Bishop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_T/0/1/0/all/0/1\">Terrence Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "OBJECTIVE: Leverage existing biomedical NLP tools and DS domain terminology\nto produce a novel and comprehensive knowledge graph containing dietary\nsupplement (DS) information for discovering interactions between DS and drugs,\nor Drug-Supplement Interactions (DSI). MATERIALS AND METHODS: We created\nSemRepDS (an extension of SemRep), capable of extracting semantic relations\nfrom abstracts by leveraging a DS-specific terminology (iDISK) containing\n28,884 DS terms not found in the UMLS. PubMed abstracts were processed using\nSemRepDS to generate semantic relations, which were then filtered using a\nPubMedBERT-based model to remove incorrect relations before generating our\nknowledge graph (SuppKG). Two pathways are used to identify potential DS-Drug\ninteractions which are then evaluated by medical professionals for mechanistic\nplausibility. RESULTS: Comparison analysis found that SemRepDS returned 206.9%\nmore DS relations and 158.5% more DS entities than SemRep. The fine-tuned BERT\nmodel obtained an F1 score of 0.8605 and removed 43.86% of the relations,\nimproving the precision of the relations by 26.4% compared to pre-filtering.\nSuppKG consists of 2,928 DS-specific nodes. Manual review of findings\nidentified 44 (88%) proposed DS-Gene-Drug and 32 (64%) proposed\nDS-Gene1-Function-Gene2-Drug pathways to be mechanistically plausible.\nDISCUSSION: The additional relations extracted using SemRepDS generated SuppKG\nthat was used to find plausible DSI not found in the current literature. By the\nnature of the SuppKG, these interactions are unlikely to have been found using\nSemRep without the expanded DS terminology. CONCLUSION: We successfully extend\nSemRep to include DS information and produce SuppKG which can be used to find\npotential DS-Drug interactions.",
          "link": "http://arxiv.org/abs/2106.12741",
          "publishedOn": "2021-06-25T02:00:44.619Z",
          "wordCount": 714,
          "title": "Discovering novel drug-supplement interactions using a dietary supplements knowledge graph generated from the biomedical literature. (arXiv:2106.12741v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1\">Shang-Chi Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao-Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>",
          "description": "Given the clinical notes written in electronic health records (EHRs), it is\nchallenging to predict the diagnostic codes which is formulated as a\nmulti-label classification task. The large set of labels, the hierarchical\ndependency, and the imbalanced data make this prediction task extremely hard.\nMost existing work built a binary prediction for each label independently,\nignoring the dependencies between labels. To address this problem, we propose a\ntwo-stage framework to improve automatic ICD coding by capturing the label\ncorrelation. Specifically, we train a label set distribution estimator to\nrescore the probability of each label set candidate generated by a base\npredictor. This paper is the first attempt at learning the label set\ndistribution as a reranking module for medical code prediction. In the\nexperiments, our proposed framework is able to improve upon best-performing\npredictors on the benchmark MIMIC datasets. The source code of this project is\navailable at https://github.com/MiuLab/ICD-Correlation.",
          "link": "http://arxiv.org/abs/2106.12800",
          "publishedOn": "2021-06-25T02:00:44.598Z",
          "wordCount": 592,
          "title": "Modeling Diagnostic Label Correlation for Automatic ICD Coding. (arXiv:2106.12800v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>",
          "description": "This paper describes FBK's system submission to the IWSLT 2021 Offline Speech\nTranslation task. We participated with a direct model, which is a\nTransformer-based architecture trained to translate English speech audio data\ninto German texts. The training pipeline is characterized by knowledge\ndistillation and a two-step fine-tuning procedure. Both knowledge distillation\nand the first fine-tuning step are carried out on manually segmented real and\nsynthetic data, the latter being generated with an MT system trained on the\navailable corpora. Differently, the second fine-tuning step is carried out on a\nrandom segmentation of the MuST-C v2 En-De dataset. Its main goal is to reduce\nthe performance drops occurring when a speech translation model trained on\nmanually segmented data (i.e. an ideal, sentence-like segmentation) is\nevaluated on automatically segmented audio (i.e. actual, more realistic testing\nconditions). For the same purpose, a custom hybrid segmentation procedure that\naccounts for both audio content (pauses) and for the length of the produced\nsegments is applied to the test data before passing them to the system. At\ninference time, we compared this procedure with a baseline segmentation method\nbased on Voice Activity Detection (VAD). Our results indicate the effectiveness\nof the proposed hybrid approach, shown by a reduction of the gap with manual\nsegmentation from 8.3 to 1.4 BLEU points.",
          "link": "http://arxiv.org/abs/2106.12607",
          "publishedOn": "2021-06-25T02:00:44.574Z",
          "wordCount": 660,
          "title": "Dealing with training and test segmentation mismatch: FBK@IWSLT2021. (arXiv:2106.12607v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dongjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evensen_S/0/1/0/all/0/1\">Sara Evensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1\">&#xc7;a&#x11f;atay Demiralp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1\">Estevam Hruschka</a>",
          "description": "Despite rapid developments in the field of machine learning research,\ncollecting high-quality labels for supervised learning remains a bottleneck for\nmany applications. This difficulty is exacerbated by the fact that\nstate-of-the-art models for NLP tasks are becoming deeper and more complex,\noften increasing the amount of training data required even for fine-tuning.\nWeak supervision methods, including data programming, address this problem and\nreduce the cost of label collection by using noisy label sources for\nsupervision. However, until recently, data programming was only accessible to\nusers who knew how to program. To bridge this gap, the Data Programming by\nDemonstration framework was proposed to facilitate the automatic creation of\nlabeling functions based on a few examples labeled by a domain expert. This\nframework has proven successful for generating high-accuracy labeling models\nfor document classification. In this work, we extend the DPBD framework to\nspan-level annotation tasks, arguably one of the most time-consuming NLP\nlabeling tasks. We built a novel tool, TagRuler, that makes it easy for\nannotators to build span-level labeling functions without programming and\nencourages them to explore trade-offs between different labeling models and\nactive learning strategies. We empirically demonstrated that an annotator could\nachieve a higher F1 score using the proposed tool compared to manual labeling\nfor different span-level annotation tasks.",
          "link": "http://arxiv.org/abs/2106.12767",
          "publishedOn": "2021-06-25T02:00:44.536Z",
          "wordCount": 660,
          "title": "TagRuler: Interactive Tool for Span-Level Data Programming by Demonstration. (arXiv:2106.12767v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chong_J/0/1/0/all/0/1\">Jia Wei Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Mei Shin Oh</a>",
          "description": "Service manual documents are crucial to the engineering company as they\nprovide guidelines and knowledge to service engineers. However, it has become\ninconvenient and inefficient for service engineers to retrieve specific\nknowledge from documents due to the complexity of resources. In this research,\nwe propose an automated knowledge mining and document classification system\nwith novel multi-model transfer learning approaches. Particularly, the\nclassification performance of the system has been improved with three effective\ntechniques: fine-tuning, pruning, and multi-model method. The fine-tuning\ntechnique optimizes a pre-trained BERT model by adding a feed-forward neural\nnetwork layer and the pruning technique is used to retrain the BERT model with\nnew data. The multi-model method initializes and trains multiple BERT models to\novercome the randomness of data ordering during the fine-tuning process. In the\nfirst iteration of the training process, multiple BERT models are being trained\nsimultaneously. The best model is then selected for the next phase of the\ntraining process with another two iterations and the training processes for\nother BERT models will be terminated. The performance of the proposed system\nhas been evaluated by comparing with two robust baseline methods, BERT and\nBERT-CNN. Experimental results on a widely used Corpus of Linguistic\nAcceptability (CoLA) dataset have shown that the proposed techniques perform\nbetter than these baseline methods in terms of accuracy and MCC score.",
          "link": "http://arxiv.org/abs/2106.12744",
          "publishedOn": "2021-06-25T02:00:44.512Z",
          "wordCount": 677,
          "title": "An Automated Knowledge Mining and Document Classification System with Multi-model Transfer Learning. (arXiv:2106.12744v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>",
          "description": "We analyze the process of creating word embedding feature representations\ndesigned for a learning task when annotated data is scarce, for example, in\ndepressive language detection from Tweets. We start with a rich word embedding\npre-trained from a large general dataset, which is then augmented with\nembeddings learned from a much smaller and more specific domain dataset through\na simple non-linear mapping mechanism. We also experimented with several other\nmore sophisticated methods of such mapping including, several auto-encoder\nbased and custom loss-function based methods that learn embedding\nrepresentations through gradually learning to be close to the words of similar\nsemantics and distant to dissimilar semantics. Our strengthened representations\nbetter capture the semantics of the depression domain, as it combines the\nsemantics learned from the specific domain coupled with word coverage from the\ngeneral language. We also present a comparative performance analyses of our\nword embedding representations with a simple bag-of-words model, well known\nsentiment and psycholinguistic lexicons, and a general pre-trained word\nembedding. When used as feature representations for several different machine\nlearning methods, including deep learning models in a depressive Tweets\nidentification task, we show that our augmented word embedding representations\nachieve a significantly better F1 score than the others, specially when applied\nto a high quality dataset. Also, we present several data ablation tests which\nconfirm the efficacy of our augmentation techniques.",
          "link": "http://arxiv.org/abs/2106.12797",
          "publishedOn": "2021-06-25T02:00:44.500Z",
          "wordCount": 707,
          "title": "A comprehensive empirical analysis on cross-domain semantic enrichment for detection of depressive language. (arXiv:2106.12797v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ryskina_M/0/1/0/all/0/1\">Maria Ryskina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>",
          "description": "Traditionally, character-level transduction problems have been solved with\nfinite-state models designed to encode structural and linguistic knowledge of\nthe underlying process, whereas recent approaches rely on the power and\nflexibility of sequence-to-sequence models with attention. Focusing on the less\nexplored unsupervised learning scenario, we compare the two model classes side\nby side and find that they tend to make different types of errors even when\nachieving comparable performance. We analyze the distributions of different\nerror classes using two unsupervised tasks as testbeds: converting informally\nromanized text into the native script of its language (for Russian, Arabic, and\nKannada) and translating between a pair of closely related languages (Serbian\nand Bosnian). Finally, we investigate how combining finite-state and\nsequence-to-sequence models at decoding time affects the output quantitatively\nand qualitatively.",
          "link": "http://arxiv.org/abs/2106.12698",
          "publishedOn": "2021-06-25T02:00:44.478Z",
          "wordCount": 574,
          "title": "Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction. (arXiv:2106.12698v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1\">Cheng Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zigeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>",
          "description": "With the increasing scale of search engine marketing, designing an efficient\nbidding system is becoming paramount for the success of e-commerce companies.\nThe critical challenges faced by a modern industrial-level bidding system\ninclude: 1. the catalog is enormous, and the relevant bidding features are of\nhigh sparsity; 2. the large volume of bidding requests induces significant\ncomputation burden to both the offline and online serving. Leveraging\nextraneous user-item information proves essential to mitigate the sparsity\nissue, for which we exploit the natural language signals from the users' query\nand the contextual knowledge from the products. In particular, we extract the\nvector representations of ads via the Transformer model and leverage their\ngeometric relation to building collaborative bidding predictions via\nclustering. The two-step procedure also significantly reduces the computation\nstress of bid evaluation and optimization. In this paper, we introduce the\nend-to-end structure of the bidding system for search engine marketing for\nWalmart e-commerce, which successfully handles tens of millions of bids each\nday. We analyze the online and offline performances of our approach and discuss\nhow we find it as a production-efficient solution.",
          "link": "http://arxiv.org/abs/2106.12700",
          "publishedOn": "2021-06-25T02:00:44.455Z",
          "wordCount": 626,
          "title": "Bidding via Clustering Ads Intentions: an Efficient Search Engine Marketing System for E-commerce. (arXiv:2106.12700v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1\">Simon Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>",
          "description": "State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.",
          "link": "http://arxiv.org/abs/2106.12672",
          "publishedOn": "2021-06-25T02:00:44.409Z",
          "wordCount": 628,
          "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Chelsea Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caufield_J/0/1/0/all/0/1\">J. Harry Caufield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_K/0/1/0/all/0/1\">Kevin Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Calvin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_P/0/1/0/all/0/1\">Peipei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "The clinical named entity recognition (CNER) task seeks to locate and\nclassify clinical terminologies into predefined categories, such as diagnostic\nprocedure, disease disorder, severity, medication, medication dosage, and sign\nsymptom. CNER facilitates the study of side-effect on medications including\nidentification of novel phenomena and human-focused information extraction.\nExisting approaches in extracting the entities of interests focus on using\nstatic word embeddings to represent each word. However, one word can have\ndifferent interpretations that depend on the context of the sentences.\nEvidently, static word embeddings are insufficient to integrate the diverse\ninterpretation of a word. To overcome this challenge, the technique of\ncontextualized word embedding has been introduced to better capture the\nsemantic meaning of each word based on its context. Two of these language\nmodels, ELMo and Flair, have been widely used in the field of Natural Language\nProcessing to generate the contextualized word embeddings on domain-generic\ndocuments. However, these embeddings are usually too general to capture the\nproximity among vocabularies of specific domains. To facilitate various\ndownstream applications using clinical case reports (CCRs), we pre-train two\ndeep contextualized language models, Clinical Embeddings from Language Model\n(C-ELMo) and Clinical Contextual String Embeddings (C-Flair) using the\nclinical-related corpus from the PubMed Central. Explicit experiments show that\nour models gain dramatic improvements compared to both static word embeddings\nand domain-generic language models.",
          "link": "http://arxiv.org/abs/2106.12608",
          "publishedOn": "2021-06-25T02:00:44.401Z",
          "wordCount": 672,
          "title": "Clinical Named Entity Recognition using Contextualized Token Representations. (arXiv:2106.12608v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.16102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Victor Zitian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montano_Campos_F/0/1/0/all/0/1\">Felipe Montano-Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_W/0/1/0/all/0/1\">Wlodek Zadrozny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canfield_E/0/1/0/all/0/1\">Evan Canfield</a>",
          "description": "The volume of scientific publications in organizational research becomes\nexceedingly overwhelming for human researchers who seek to timely extract and\nreview knowledge. This paper introduces natural language processing (NLP)\nmodels to accelerate the discovery, extraction, and organization of theoretical\ndevelopments (i.e., hypotheses) from social science publications. We illustrate\nand evaluate NLP models in the context of a systematic review of stakeholder\nvalue constructs and hypotheses. Specifically, we develop NLP models to\nautomatically 1) detect sentences in scholarly documents as hypotheses or not\n(Hypothesis Detection), 2) deconstruct the hypotheses into nodes (constructs)\nand links (causal/associative relationships) (Relationship Deconstruction ),\nand 3) classify the features of links in terms causality (versus association)\nand direction (positive, negative, versus nonlinear) (Feature Classification).\nOur models have reported high performance metrics for all three tasks. While\nour models are built in Python, we have made the pre-trained models fully\naccessible for non-programmers. We have provided instructions on installing and\nusing our pre-trained models via an R Shiny app graphic user interface (GUI).\nFinally, we suggest the next paths to extend our methodology for\ncomputer-assisted knowledge synthesis.",
          "link": "http://arxiv.org/abs/2106.16102",
          "publishedOn": "2021-07-01T01:59:31.579Z",
          "wordCount": 632,
          "title": "Machine Reading of Hypotheses for Organizational Research Reviews and Pre-trained Models via R Shiny App for Non-Programmers. (arXiv:2106.16102v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Nan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1\">Kyle Kai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1\">Arian Prabowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Mohammad Saiedur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>",
          "description": "Generative Adversarial Networks (GANs) have shown remarkable success in\nproducing realistic-looking images in the computer vision area. Recently,\nGAN-based techniques are shown to be promising for spatio-temporal-based\napplications such as trajectory prediction, events generation and time-series\ndata imputation. While several reviews for GANs in computer vision have been\npresented, no one has considered addressing the practical applications and\nchallenges relevant to spatio-temporal data. In this paper, we have conducted a\ncomprehensive review of the recent developments of GANs for spatio-temporal\ndata. We summarise the application of popular GAN architectures for\nspatio-temporal data and the common practices for evaluating the performance of\nspatio-temporal applications with GANs. Finally, we point out future research\ndirections to benefit researchers in this area.",
          "link": "http://arxiv.org/abs/2008.08903",
          "publishedOn": "2021-07-01T01:59:31.548Z",
          "wordCount": 625,
          "title": "Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1\">Binbin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingsheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>",
          "description": "We discuss a novel task, Chorus Recognition, which could potentially benefit\ndownstream tasks such as song search and music summarization. Different from\nthe existing tasks such as music summarization or lyrics summarization relying\non single-modal information, this paper models chorus recognition as a\nmulti-modal one by utilizing both the lyrics and the tune information of songs.\nWe propose a multi-modal Chorus Recognition model that considers diverse\nfeatures. Besides, we also create and publish the first Chorus Recognition\ndataset containing 627 songs for public use. Our empirical study performed on\nthe dataset demonstrates that our approach outperforms several baselines in\nchorus recognition. In addition, our approach also helps to improve the\naccuracy of its downstream task - song search by more than 10.6%.",
          "link": "http://arxiv.org/abs/2106.16153",
          "publishedOn": "2021-07-01T01:59:31.536Z",
          "wordCount": 577,
          "title": "Multi-Modal Chorus Recognition for Improving Song Search. (arXiv:2106.16153v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehpanah_A/0/1/0/all/0/1\">Arman Dehpanah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghori_M/0/1/0/all/0/1\">Muheeb Faizan Ghori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemmell_J/0/1/0/all/0/1\">Jonathan Gemmell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobasher_B/0/1/0/all/0/1\">Bamshad Mobasher</a>",
          "description": "Online competitive games have become a mainstream entertainment platform. To\ncreate a fair and exciting experience, these games use rating systems to match\nplayers with similar skills. While there has been an increasing amount of\nresearch on improving the performance of these systems, less attention has been\npaid to how their performance is evaluated. In this paper, we explore the\nutility of several metrics for evaluating three popular rating systems on a\nreal-world dataset of over 25,000 team battle royale matches. Our results\nsuggest considerable differences in their evaluation patterns. Some metrics\nwere highly impacted by the inclusion of new players. Many could not capture\nthe real differences between certain groups of players. Among all metrics\nstudied, normalized discounted cumulative gain (NDCG) demonstrated more\nreliable performance and more flexibility. It alleviated most of the challenges\nfaced by the other metrics while adding the freedom to adjust the focus of the\nevaluations on different groups of players.",
          "link": "http://arxiv.org/abs/2105.14069",
          "publishedOn": "2021-07-01T01:59:31.521Z",
          "wordCount": 633,
          "title": "The Evaluation of Rating Systems in Team-based Battle Royale Games. (arXiv:2105.14069v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongkun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>",
          "description": "Conversational Question Simplification (CQS) aims to simplify self-contained\nquestions into conversational ones by incorporating some conversational\ncharacteristics, e.g., anaphora and ellipsis. Existing maximum likelihood\nestimation (MLE) based methods often get trapped in easily learned tokens as\nall tokens are treated equally during training. In this work, we introduce a\nReinforcement Iterative Sequence Editing (RISE) framework that optimizes the\nminimum Levenshtein distance (MLD) through explicit editing actions. RISE is\nable to pay attention to tokens that are related to conversational\ncharacteristics. To train RISE, we devise an Iterative Reinforce Training (IRT)\nalgorithm with a Dynamic Programming based Sampling (DPS) process to improve\nexploration. Experimental results on two benchmark datasets show that RISE\nsignificantly outperforms state-of-the-art methods and generalizes well on\nunseen data.",
          "link": "http://arxiv.org/abs/2106.15903",
          "publishedOn": "2021-07-01T01:59:31.503Z",
          "wordCount": 573,
          "title": "Learning to Ask Conversational Questions by Optimizing Levenshtein Distance. (arXiv:2106.15903v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Voskarides_N/0/1/0/all/0/1\">Nikos Voskarides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meij_E/0/1/0/all/0/1\">Edgar Meij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauer_S/0/1/0/all/0/1\">Sabrina Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "Writers such as journalists often use automatic tools to find relevant\ncontent to include in their narratives. In this paper, we focus on supporting\nwriters in the news domain to develop event-centric narratives. Given an\nincomplete narrative that specifies a main event and a context, we aim to\nretrieve news articles that discuss relevant events that would enable the\ncontinuation of the narrative. We formally define this task and propose a\nretrieval dataset construction procedure that relies on existing news articles\nto simulate incomplete narratives and relevant articles. Experiments on two\ndatasets derived from this procedure show that state-of-the-art lexical and\nsemantic rankers are not sufficient for this task. We show that combining those\nwith a ranker that ranks articles by reverse chronological order outperforms\nthose rankers alone. We also perform an in-depth quantitative and qualitative\nanalysis of the results that sheds light on the characteristics of this task.",
          "link": "http://arxiv.org/abs/2106.16053",
          "publishedOn": "2021-07-01T01:59:31.458Z",
          "wordCount": 592,
          "title": "News Article Retrieval in Context for Event-centric Narrative Creation. (arXiv:2106.16053v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zeyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>",
          "description": "Factorization machine (FM) is a prevalent approach to modeling pairwise\n(second-order) feature interactions when dealing with high-dimensional sparse\ndata. However, on the one hand, FM fails to capture higher-order feature\ninteractions suffering from combinatorial expansion, on the other hand, taking\ninto account interaction between every pair of features may introduce noise and\ndegrade prediction accuracy. To solve the problems, we propose a novel approach\nGraph Factorization Machine (GraphFM) by naturally representing features in the\ngraph structure. In particular, a novel mechanism is designed to select the\nbeneficial feature interactions and formulate them as edges between features.\nThen our proposed model which integrates the interaction function of FM into\nthe feature aggregation strategy of Graph Neural Network (GNN), can model\narbitrary-order feature interactions on the graph-structured features by\nstacking layers. Experimental results on several real-world datasets has\ndemonstrated the rationality and effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2105.11866",
          "publishedOn": "2021-07-01T01:59:31.292Z",
          "wordCount": 615,
          "title": "GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadiq_S/0/1/0/all/0/1\">Shazia W. Sadiq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>",
          "description": "With the rapid growth of location-based social networks (LBSNs),\nPoint-Of-Interest (POI) recommendation has been broadly studied in this decade.\nRecently, the next POI recommendation, a natural extension of POI\nrecommendation, has attracted much attention. It aims at suggesting the next\nPOI to a user in spatial and temporal context, which is a practical yet\nchallenging task in various applications. Existing approaches mainly model the\nspatial and temporal information, and memorize historical patterns through\nuser's trajectories for recommendation. However, they suffer from the negative\nimpact of missing and irregular check-in data, which significantly influences\nthe model performance. In this paper, we propose an attention-based\nsequence-to-sequence generative model, namely POI-Augmentation Seq2Seq\n(PA-Seq2Seq), to address the sparsity of training set by making check-in\nrecords to be evenly-spaced. Specifically, the encoder summarises each check-in\nsequence and the decoder predicts the possible missing check-ins based on the\nencoded information. In order to learn time-aware correlation among user\nhistory, we employ local attention mechanism to help the decoder focus on a\nspecific range of context information when predicting a certain missing\ncheck-in point. Extensive experiments have been conducted on two real-world\ncheck-in datasets, Gowalla and Brightkite, for performance and effectiveness\nevaluation.",
          "link": "http://arxiv.org/abs/2106.15984",
          "publishedOn": "2021-07-01T01:59:31.223Z",
          "wordCount": 646,
          "title": "Context-Aware Attention-Based Data Augmentation for POI Recommendation. (arXiv:2106.15984v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Paheli Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1\">Soham Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudra_K/0/1/0/all/0/1\">Koustav Rudra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1\">Kripabandhu Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>",
          "description": "Automatic summarization of legal case documents is an important and practical\nchallenge. Apart from many domain-independent text summarization algorithms\nthat can be used for this purpose, several algorithms have been developed\nspecifically for summarizing legal case documents. However, most of the\nexisting algorithms do not systematically incorporate domain knowledge that\nspecifies what information should ideally be present in a legal case document\nsummary. To address this gap, we propose an unsupervised summarization\nalgorithm DELSumm which is designed to systematically incorporate guidelines\nfrom legal experts into an optimization setup. We conduct detailed experiments\nover case documents from the Indian Supreme Court. The experiments show that\nour proposed unsupervised method outperforms several strong baselines in terms\nof ROUGE scores, including both general summarization algorithms and\nlegal-specific ones. In fact, though our proposed algorithm is unsupervised, it\noutperforms several supervised summarization models that are trained over\nthousands of document-summary pairs.",
          "link": "http://arxiv.org/abs/2106.15876",
          "publishedOn": "2021-07-01T01:59:31.127Z",
          "wordCount": 602,
          "title": "Incorporating Domain Knowledge for Extractive Summarization of Legal Case Documents. (arXiv:2106.15876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1\">Qiaomin Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Ning Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "Robust recommendation aims at capturing true preference of users from noisy\ndata, for which there are two lines of methods have been proposed. One is based\non noise injection, and the other is to adopt the generative model Variational\nAuto-encoder (VAE). However, the existing works still face two challenges.\nFirst, the noise injection based methods often draw the noise from a fixed\nnoise distribution given in advance, while in real world, the noise\ndistributions of different users and items may differ from each other due to\npersonal behaviors and item usage patterns. Second, the VAE based models are\nnot expressive enough to capture the true preference since VAE often yields an\nembedding space of a single modal, while in real world, user-item interactions\nusually exhibit multi-modality on user preference distribution. In this paper,\nwe propose a novel model called Dual Adversarial Variational Embedding (DAVE)\nfor robust recommendation, which can provide personalized noise reduction for\ndifferent users and items, and capture the multi-modality of the embedding\nspace, by combining the advantages of VAE and adversarial training between the\nintroduced auxiliary discriminators and the variational inference networks. The\nextensive experiments conducted on real datasets verify the effectiveness of\nDAVE on robust recommendation.",
          "link": "http://arxiv.org/abs/2106.15779",
          "publishedOn": "2021-07-01T01:59:31.084Z",
          "wordCount": 626,
          "title": "Dual Adversarial Variational Embedding for Robust Recommendation. (arXiv:2106.15779v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>",
          "description": "Being an indispensable component in location-based social networks, next\npoint-of-interest (POI) recommendation recommends users unexplored POIs based\non their recent visiting histories. However, existing work mainly models\ncheck-in data as isolated POI sequences, neglecting the crucial collaborative\nsignals from cross-sequence check-in information. Furthermore, the sparse\nPOI-POI transitions restrict the ability of a model to learn effective\nsequential patterns for recommendation. In this paper, we propose\nSequence-to-Graph (Seq2Graph) augmentation for each POI sequence, allowing\ncollaborative signals to be propagated from correlated POIs belonging to other\nsequences. We then devise a novel Sequence-to-Graph POI Recommender (SGRec),\nwhich jointly learns POI embeddings and infers a user's temporal preferences\nfrom the graph-augmented POI sequence. To overcome the sparsity of POI-level\ninteractions, we further infuse category-awareness into SGRec with a multi-task\nlearning scheme that captures the denser category-wise transitions. As such,\nSGRec makes full use of the collaborative signals for learning expressive POI\nrepresentations, and also comprehensively uncovers multi-level sequential\npatterns for user preference modelling. Extensive experiments on two real-world\ndatasets demonstrate the superiority of SGRec against state-of-the-art methods\nin next POI recommendation.",
          "link": "http://arxiv.org/abs/2106.15814",
          "publishedOn": "2021-07-01T01:59:31.058Z",
          "wordCount": 614,
          "title": "Discovering Collaborative Signals for Next POI Recommendation with Iterative Seq2Graph Augmentation. (arXiv:2106.15814v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Issam_K/0/1/0/all/0/1\">Kalliath Abdul Rasheed Issam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shivam Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1\">Subalalitha C. N</a>",
          "description": "Text summarization is an approach for identifying important information\npresent within text documents. This computational technique aims to generate\nshorter versions of the source text, by including only the relevant and salient\ninformation present within the source text. In this paper, we propose a novel\nmethod to summarize a text document by clustering its contents based on latent\ntopics produced using topic modeling techniques and by generating extractive\nsummaries for each of the identified text clusters. All extractive\nsub-summaries are later combined to generate a summary for any given source\ndocument. We utilize the lesser used and challenging WikiHow dataset in our\napproach to text summarization. This dataset is unlike the commonly used news\ndatasets which are available for text summarization. The well-known news\ndatasets present their most important information in the first few lines of\ntheir source texts, which make their summarization a lesser challenging task\nwhen compared to summarizing the WikiHow dataset. Contrary to these news\ndatasets, the documents in the WikiHow dataset are written using a generalized\napproach and have lesser abstractedness and higher compression ratio, thus\nproposing a greater challenge to generate summaries. A lot of the current\nstate-of-the-art text summarization techniques tend to eliminate important\ninformation present in source documents in the favor of brevity. Our proposed\ntechnique aims to capture all the varied information present in source\ndocuments. Although the dataset proved challenging, after performing extensive\ntests within our experimental setup, we have discovered that our model produces\nencouraging ROUGE results and summaries when compared to the other published\nextractive and abstractive text summarization models.",
          "link": "http://arxiv.org/abs/2106.15313",
          "publishedOn": "2021-06-30T02:01:00.059Z",
          "wordCount": 718,
          "title": "Topic Modeling Based Extractive Text Summarization. (arXiv:2106.15313v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peiyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_Z/0/1/0/all/0/1\">Zisen Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Aiquan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guodong Cao</a>",
          "description": "As a new type of e-commerce platform developed in recent years, local\nconsumer service platform provides users with software to consume service to\nthe nearby store or to the home, such as Groupon and Koubei. Different from\nother common e-commerce platforms, the behavior of users on the local consumer\nservice platform is closely related to their real-time local context\ninformation. Therefore, building a context-aware user behavior prediction\nsystem is able to provide both merchants and users better service in local\nconsumer service platforms. However, most of the previous work just treats the\ncontextual information as an ordinary feature into the prediction model to\nobtain the prediction list under a specific context, which ignores the fact\nthat the interest of a user in different contexts is often significantly\ndifferent. Hence, in this paper, we propose a context-aware heterogeneous graph\nattention network (CHGAT) to dynamically generate the representation of the\nuser and to estimate the probability for future behavior. Specifically, we\nfirst construct the meta-path based heterogeneous graphs with the historical\nbehaviors from multiple sources and comprehend heterogeneous vertices in the\ngraph with a novel unified knowledge representing approach. Next, a multi-level\nattention mechanism is introduced for context-aware aggregation with graph\nvertices, which contains the vertex-level attention network and the path-level\nattention network. Both of them aim to capture the semantic correlation between\ninformation contained in the graph and the outside real-time contextual\ninformation in the search system. Then the model proposed in this paper\naggregates specific graphs with their corresponding context features and\nobtains the representation of user interest under a specific context and input\nit into the prediction network to finally obtain the predicted probability of\nuser behavior.",
          "link": "http://arxiv.org/abs/2106.14652",
          "publishedOn": "2021-06-30T02:01:00.050Z",
          "wordCount": 742,
          "title": "Context-aware Heterogeneous Graph Attention Network for User Behavior Prediction in Local Consumer Service Platform. (arXiv:2106.14652v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_W/0/1/0/all/0/1\">Wenbin Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1\">Hannah Danner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>",
          "description": "Social media offer plenty of information to perform market research in order\nto meet the requirements of customers. One way how this research is conducted\nis that a domain expert gathers and categorizes user-generated content into a\ncomplex and fine-grained class structure. In many of such cases, little data\nmeets complex annotations. It is not yet fully understood how this can be\nleveraged successfully for classification. We examine the classification\naccuracy of expert labels when used with a) many fine-grained classes and b)\nfew abstract classes. For scenario b) we compare abstract class labels given by\nthe domain expert as baseline and by automatic hierarchical clustering. We\ncompare this to another baseline where the entire class structure is given by a\ncompletely unsupervised clustering approach. By doing so, this work can serve\nas an example of how complex expert annotations are potentially beneficial and\ncan be utilized in the most optimal way for opinion mining in highly specific\ndomains. By exploring across a range of techniques and experiments, we find\nthat automated class abstraction approaches in particular the unsupervised\napproach performs remarkably well against domain expert baseline on text\nclassification tasks. This has the potential to inspire opinion mining\napplications in order to support market researchers in practice and to inspire\nfine-grained automated content analysis on a large scale.",
          "link": "http://arxiv.org/abs/2106.15498",
          "publishedOn": "2021-06-30T02:01:00.011Z",
          "wordCount": 657,
          "title": "Classification of Consumer Belief Statements From Social Media. (arXiv:2106.15498v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chaochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doss_R/0/1/0/all/0/1\">Robin Ram Mohan Doss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiangshan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_K/0/1/0/all/0/1\">Keshav Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Longxiang Gao</a>",
          "description": "With the development of blockchain technologies, the number of smart\ncontracts deployed on blockchain platforms is growing exponentially, which\nmakes it difficult for users to find desired services by manual screening. The\nautomatic classification of smart contracts can provide blockchain users with\nkeyword-based contract searching and helps to manage smart contracts\neffectively. Current research on smart contract classification focuses on\nNatural Language Processing (NLP) solutions which are based on contract source\ncode. However, more than 94% of smart contracts are not open-source, so the\napplication scenarios of NLP methods are very limited. Meanwhile, NLP models\nare vulnerable to adversarial attacks. This paper proposes a classification\nmodel based on features from contract bytecode instead of source code to solve\nthese problems. We also use feature selection and ensemble learning to optimize\nthe model. Our experimental studies on over 3,300 real-world Ethereum smart\ncontracts show that our model can classify smart contracts without source code\nand has better performance than baseline models. Our model also has good\nresistance to adversarial attacks compared with NLP-based models. In addition,\nour analysis reveals that account features used in many smart contract\nclassification models have little effect on classification and can be excluded.",
          "link": "http://arxiv.org/abs/2106.15497",
          "publishedOn": "2021-06-30T02:00:59.992Z",
          "wordCount": 645,
          "title": "A Bytecode-based Approach for Smart Contract Classification. (arXiv:2106.15497v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/1705.10351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tellez_E/0/1/0/all/0/1\">Eric S. Tellez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_G/0/1/0/all/0/1\">Guillermo Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavez_E/0/1/0/all/0/1\">Edgar Chavez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graff_M/0/1/0/all/0/1\">Mario Graff</a>",
          "description": "Near neighbor search (NNS) is a powerful abstraction for data access;\nhowever, data indexing is troublesome even for approximate indexes. For\nintrinsically high-dimensional data, high-quality fast searches demand either\nindexes with impractically large memory usage or preprocessing time.\n\nIn this paper, we introduce an algorithm to solve a nearest-neighbor query\n$q$ by minimizing a kernel function defined by the distance from $q$ to each\nobject in the database. The minimization is performed using metaheuristics to\nsolve the problem rapidly; even when some methods in the literature use this\nstrategy behind the scenes, our approach is the first one using it explicitly.\nWe also provide two approaches to select edges in the graph's construction\nstage that limit memory footprint and reduce the number of free parameters\nsimultaneously.\n\nWe carry out a thorough experimental comparison with state-of-the-art indexes\nthrough synthetic and real-world datasets; we found out that our contributions\nachieve competitive performances regarding speed, accuracy, and memory in\nalmost any of our benchmarks.",
          "link": "http://arxiv.org/abs/1705.10351",
          "publishedOn": "2021-06-30T02:00:59.984Z",
          "wordCount": 674,
          "title": "A scalable solution to the nearest neighbor search problem through local-search methods on neighbor graphs. (arXiv:1705.10351v4 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vaccario_G/0/1/0/all/0/1\">Giacomo Vaccario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verginer_L/0/1/0/all/0/1\">Luca Verginer</a>",
          "description": "Journal rankings are widely used and are often based on citation data in\ncombination with a network perspective. We argue that some of these\nnetwork-based rankings can produce misleading results. From a theoretical point\nof view, we show that the standard network modelling approach of citation data\nat the journal level (i.e., the projection of paper citations onto journals)\nintroduces fictitious relations among journals. To overcome this problem, we\npropose a citation path perspective, and empirically show that rankings based\non the network and the citation path perspective are very different. Based on\nour theoretical and empirical analysis, we highlight the limitations of\nstandard network metrics, and propose a method to overcome these limitations\nand compute journal rankings.",
          "link": "http://arxiv.org/abs/2106.15541",
          "publishedOn": "2021-06-30T02:00:59.978Z",
          "wordCount": 578,
          "title": "When standard network measures fail to rank journals: A theoretical and empirical analysis. (arXiv:2106.15541v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allaix_M/0/1/0/all/0/1\">Matteo Allaix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Seunghoan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzbaur_L/0/1/0/all/0/1\">Lukas Holzbaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pllaha_T/0/1/0/all/0/1\">Tefjol Pllaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_M/0/1/0/all/0/1\">Masahito Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollanti_C/0/1/0/all/0/1\">Camilla Hollanti</a>",
          "description": "In quantum private information retrieval (QPIR), a user retrieves a classical\nfile from multiple servers by downloading quantum systems without revealing the\nidentity of the file. The QPIR capacity is the maximal achievable ratio of the\nretrieved file size to the total download size. In this paper, the capacity of\nQPIR from MDS-coded and colluding servers is studied. Two classes of QPIR,\ncalled stabilizer QPIR and dimension squared QPIR induced from classical\nstrongly linear PIR are defined, and the related QPIR capacities are derived.\nFor the non-colluding case, the general QPIR capacity is derived when the\nnumber of files goes to infinity. The capacities of symmetric and non-symmetric\nQPIR with coded and colluding servers are proved to coincide, being double to\ntheir classical counterparts. A general statement on the converse bound for\nQPIR with coded and colluding servers is derived showing that the capacities of\nstabilizer QPIR and dimension squared QPIR induced from any class of PIR are\nupper bounded by twice the classical capacity of the respective PIR class. The\nproposed capacity-achieving scheme combines the star-product scheme by\nFreij-Hollanti et al. and the stabilizer QPIR scheme by Song et al. by\nemploying (weakly) self-dual Reed--Solomon codes.",
          "link": "http://arxiv.org/abs/2106.14719",
          "publishedOn": "2021-06-30T02:00:59.939Z",
          "wordCount": 676,
          "title": "On the Capacity of Quantum Private Information Retrieval from MDS-Coded and Colluding Servers. (arXiv:2106.14719v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1\">Anastasios Nentidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsimpras_G/0/1/0/all/0/1\">Georgios Katsimpras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandorou_E/0/1/0/all/0/1\">Eirini Vandorou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1\">Anastasia Krithara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasco_L/0/1/0/all/0/1\">Luis Gasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krallinger_M/0/1/0/all/0/1\">Martin Krallinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>",
          "description": "Advancing the state-of-the-art in large-scale biomedical semantic indexing\nand question answering is the main focus of the BioASQ challenge. BioASQ\norganizes respective tasks where different teams develop systems that are\nevaluated on the same benchmark datasets that represent the real information\nneeds of experts in the biomedical domain. This paper presents an overview of\nthe ninth edition of the BioASQ challenge in the context of the Conference and\nLabs of the Evaluation Forum (CLEF) 2021. In this year, a new question\nanswering task, named Synergy, is introduced to support researchers studying\nthe COVID-19 disease and measure the ability of the participating teams to\ndiscern information while the problem is still developing. In total, 42 teams\nwith more than 170 systems were registered to participate in the four tasks of\nthe challenge. The evaluation results, similarly to previous years, show a\nperformance gain against the baselines which indicates the continuous\nimprovement of the state-of-the-art in this field.",
          "link": "http://arxiv.org/abs/2106.14885",
          "publishedOn": "2021-06-30T02:00:59.350Z",
          "wordCount": 677,
          "title": "Overview of BioASQ 2021: The ninth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering. (arXiv:2106.14885v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hron_J/0/1/0/all/0/1\">Jiri Hron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1\">Karl Krauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1\">Niki Kilbertus</a>",
          "description": "Thanks to their scalability, two-stage recommenders are used by many of\ntoday's largest online platforms, including YouTube, LinkedIn, and Pinterest.\nThese systems produce recommendations in two steps: (i) multiple nominators --\ntuned for low prediction latency -- preselect a small subset of candidates from\nthe whole item pool; (ii)~a slower but more accurate ranker further narrows\ndown the nominated items, and serves to the user. Despite their popularity, the\nliterature on two-stage recommenders is relatively scarce, and the algorithms\nare often treated as the sum of their parts. Such treatment presupposes that\nthe two-stage performance is explained by the behavior of individual components\nif they were deployed independently. This is not the case: using synthetic and\nreal-world data, we demonstrate that interactions between the ranker and the\nnominators substantially affect the overall performance. Motivated by these\nfindings, we derive a generalization lower bound which shows that careful\nchoice of each nominator's training set is sometimes the only difference\nbetween a poor and an optimal two-stage recommender. Since searching for a good\nchoice manually is difficult, we learn one instead. In particular, using a\nMixture-of-Experts approach, we train the nominators (experts) to specialize on\ndifferent subsets of the item pool. This significantly improves performance.",
          "link": "http://arxiv.org/abs/2106.14979",
          "publishedOn": "2021-06-30T02:00:59.314Z",
          "wordCount": 641,
          "title": "On component interactions in two-stage recommender systems. (arXiv:2106.14979v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1\">Caglar Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussallem_D/0/1/0/all/0/1\">Diego Moussallem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heindorf_S/0/1/0/all/0/1\">Stefan Heindorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>",
          "description": "Knowledge graph embedding research has mainly focused on the two smallest\nnormed division algebras, $\\mathbb{R}$ and $\\mathbb{C}$. Recent results suggest\nthat trilinear products of quaternion-valued embeddings can be a more effective\nmeans to tackle link prediction. In addition, models based on convolutions on\nreal-valued embeddings often yield state-of-the-art results for link\nprediction. In this paper, we investigate a composition of convolution\noperations with hypercomplex multiplications. We propose the four approaches\nQMult, OMult, ConvQ and ConvO to tackle the link prediction problem. QMult and\nOMult can be considered as quaternion and octonion extensions of previous\nstate-of-the-art approaches, including DistMult and ComplEx. ConvQ and ConvO\nbuild upon QMult and OMult by including convolution operations in a way\ninspired by the residual learning framework. We evaluated our approaches on\nseven link prediction datasets including WN18RR, FB15K-237 and YAGO3-10.\nExperimental results suggest that the benefits of learning hypercomplex-valued\nvector representations become more apparent as the size and complexity of the\nknowledge graph grows. ConvO outperforms state-of-the-art approaches on\nFB15K-237 in MRR, Hit@1 and Hit@3, while QMult, OMult, ConvQ and ConvO\noutperform state-of-the-approaches on YAGO3-10 in all metrics. Results also\nsuggest that link prediction performances can be further improved via\nprediction averaging. To foster reproducible research, we provide an\nopen-source implementation of approaches, including training and evaluation\nscripts as well as pretrained models.",
          "link": "http://arxiv.org/abs/2106.15230",
          "publishedOn": "2021-06-30T02:00:59.290Z",
          "wordCount": 648,
          "title": "Convolutional Hypercomplex Embeddings for Link Prediction. (arXiv:2106.15230v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hansi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>",
          "description": "Modern E-commerce websites contain heterogeneous sources of information, such\nas numerical ratings, textual reviews and images. These information can be\nutilized to assist recommendation. Through textual reviews, a user explicitly\nexpress her affinity towards the item. Previous researchers found that by using\nthe information extracted from these reviews, we can better profile the users'\nexplicit preferences as well as the item features, leading to the improvement\nof recommendation performance. However, most of the previous algorithms were\nonly utilizing the review information for explicit-feedback problem i.e. rating\nprediction, and when it comes to implicit-feedback ranking problem such as\ntop-N recommendation, the usage of review information has not been fully\nexplored. Seeing this gap, in this work, we investigate the effectiveness of\ntextual review information for top-N recommendation under E-commerce settings.\nWe adapt several SOTA review-based rating prediction models for top-N\nrecommendation tasks and compare them to existing top-N recommendation models\nfrom both performance and efficiency. We find that models utilizing only review\ninformation can not achieve better performances than vanilla implicit-feedback\nmatrix factorization method. When utilizing review information as a regularizer\nor auxiliary information, the performance of implicit-feedback matrix\nfactorization method can be further improved. However, the optimal model\nstructure to utilize textual reviews for E-commerce top-N recommendation is yet\nto be determined.",
          "link": "http://arxiv.org/abs/2106.09665",
          "publishedOn": "2021-06-29T01:55:13.421Z",
          "wordCount": 667,
          "title": "Understanding the Effectiveness of Reviews in E-commerce Top-N Recommendation. (arXiv:2106.09665v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianxin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_Y/0/1/0/all/0/1\">Yiqun Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yanan Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Depeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Sequential recommendation aims to leverage users' historical behaviors to\npredict their next interaction. Existing works have not yet addressed two main\nchallenges in sequential recommendation. First, user behaviors in their rich\nhistorical sequences are often implicit and noisy preference signals, they\ncannot sufficiently reflect users' actual preferences. In addition, users'\ndynamic preferences often change rapidly over time, and hence it is difficult\nto capture user patterns in their historical sequences. In this work, we\npropose a graph neural network model called SURGE (short for SeqUential\nRecommendation with Graph neural nEtworks) to address these two issues.\nSpecifically, SURGE integrates different types of preferences in long-term user\nbehaviors into clusters in the graph by re-constructing loose item sequences\ninto tight item-item interest graphs based on metric learning. This helps\nexplicitly distinguish users' core interests, by forming dense clusters in the\ninterest graph. Then, we perform cluster-aware and query-aware graph\nconvolutional propagation and graph pooling on the constructed graph. It\ndynamically fuses and extracts users' current activated core interests from\nnoisy user behavior sequences. We conduct extensive experiments on both public\nand proprietary industrial datasets. Experimental results demonstrate\nsignificant performance gains of our proposed method compared to\nstate-of-the-art methods. Further studies on sequence length confirm that our\nmethod can model long behavioral sequences effectively and efficiently.",
          "link": "http://arxiv.org/abs/2106.14226",
          "publishedOn": "2021-06-29T01:55:13.218Z",
          "wordCount": 652,
          "title": "Sequential Recommendation with Graph Neural Networks. (arXiv:2106.14226v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.13392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gominski_D/0/1/0/all/0/1\">Dimitri Gominski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouet_Brunet_V/0/1/0/all/0/1\">Val&#xe9;rie Gouet-Brunet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>",
          "description": "Advances in high resolution remote sensing image analysis are currently\nhampered by the difficulty of gathering enough annotated data for training deep\nlearning methods, giving rise to a variety of small datasets and associated\ndataset-specific methods. Moreover, typical tasks such as classification and\nretrieval lack a systematic evaluation on standard benchmarks and training\ndatasets, which make it hard to identify durable and generalizable scientific\ncontributions. We aim at unifying remote sensing image retrieval and\nclassification with a new large-scale training and testing dataset, SF300,\nincluding both vertical and oblique aerial images and made available to the\nresearch community, and an associated fine-tuning method. We additionally\npropose a new adversarial fine-tuning method for global descriptors. We show\nthat our framework systematically achieves a boost of retrieval and\nclassification performance on nine different datasets compared to an ImageNet\npretrained baseline, with currently no other method to compare to.",
          "link": "http://arxiv.org/abs/2102.13392",
          "publishedOn": "2021-06-29T01:55:13.161Z",
          "wordCount": 646,
          "title": "Unifying Remote Sensing Image Retrieval and Classification with Robust Fine-tuning. (arXiv:2102.13392v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yonghao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>",
          "description": "Most sequential recommendation models capture the features of consecutive\nitems in a user-item interaction history. Though effective, their\nrepresentation expressiveness is still hindered by the sparse learning signals.\nAs a result, the sequential recommender is prone to make inconsistent\npredictions. In this paper, we propose a model, \\textbf{SSI}, to improve\nsequential recommendation consistency with Self-Supervised Imitation.\nPrecisely, we extract the consistency knowledge by utilizing three\nself-supervised pre-training tasks, where temporal consistency and persona\nconsistency capture user-interaction dynamics in terms of the chronological\norder and persona sensitivities, respectively. Furthermore, to provide the\nmodel with a global perspective, global session consistency is introduced by\nmaximizing the mutual information among global and local interaction sequences.\nFinally, to comprehensively take advantage of all three independent aspects of\nconsistency-enhanced knowledge, we establish an integrated imitation learning\nframework. The consistency knowledge is effectively internalized and\ntransferred to the student model by imitating the conventional prediction logit\nas well as the consistency-enhanced item representations. In addition, the\nflexible self-supervised imitation framework can also benefit other student\nrecommenders. Experiments on four real-world datasets show that SSI effectively\noutperforms the state-of-the-art sequential recommendation methods.",
          "link": "http://arxiv.org/abs/2106.14031",
          "publishedOn": "2021-06-29T01:55:13.150Z",
          "wordCount": 623,
          "title": "Improving Sequential Recommendation Consistency with Self-Supervised Imitation. (arXiv:2106.14031v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_M/0/1/0/all/0/1\">Muvazima Mansoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Srikanth Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinath_R/0/1/0/all/0/1\">Ramamoorthy Srinath</a>",
          "description": "In this paper, we propose an architecture to solve a novel problem statement\nthat has stemmed more so in recent times with an increase in demand for virtual\ncontent delivery due to the COVID-19 pandemic. All educational institutions,\nworkplaces, research centers, etc. are trying to bridge the gap of\ncommunication during these socially distanced times with the use of online\ncontent delivery. The trend now is to create presentations, and then\nsubsequently deliver the same using various virtual meeting platforms. The time\nbeing spent in such creation of presentations and delivering is what we try to\nreduce and eliminate through this paper which aims to use Machine Learning (ML)\nalgorithms and Natural Language Processing (NLP) modules to automate the\nprocess of creating a slides-based presentation from a document, and then use\nstate-of-the-art voice cloning models to deliver the content in the desired\nauthor's voice. We consider a structured document such as a research paper to\nbe the content that has to be presented. The research paper is first summarized\nusing BERT summarization techniques and condensed into bullet points that go\ninto the slides. Tacotron inspired architecture with Encoder, Synthesizer, and\na Generative Adversarial Network (GAN) based vocoder, is used to convey the\ncontents of the slides in the author's voice (or any customized voice). Almost\nall learning has now been shifted to online mode, and professionals are now\nworking from the comfort of their homes. Due to the current situation, teachers\nand professionals have shifted to presentations to help them in imparting\ninformation. In this paper, we aim to reduce the considerable amount of time\nthat is taken in creating a presentation by automating this process and\nsubsequently delivering this presentation in a customized voice, using a\ncontent delivery mechanism that can clone any voice using a short audio clip.",
          "link": "http://arxiv.org/abs/2106.14213",
          "publishedOn": "2021-06-29T01:55:13.143Z",
          "wordCount": 783,
          "title": "AI based Presentation Creator With Customized Audio Content Delivery. (arXiv:2106.14213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Arkadipta De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1\">Tanik Saikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "In this article, we present a description of our systems as a part of our\nparticipation in the shared task namely Artificial Intelligence for Legal\nAssistance (AILA 2019). This is an integral event of Forum for Information\nRetrieval Evaluation-2019. The outcomes of this track would be helpful for the\nautomation of the working process of the Indian Judiciary System. The manual\nworking procedures and documentation at any level (from lower to higher court)\nof the judiciary system are very complex in nature. The systems produced as a\npart of this track would assist the law practitioners. It would be helpful for\ncommon men too. This kind of track also opens the path of research of Natural\nLanguage Processing (NLP) in the judicial domain. This track defined two\nproblems such as Task 1: Identifying relevant prior cases for a given situation\nand Task 2: Identifying the most relevant statutes for a given situation. We\ntackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As\nper the results declared by the task organizers, we are in 3rd and a modest\nposition in Task 1 and Task 2 respectively.",
          "link": "http://arxiv.org/abs/2105.11347",
          "publishedOn": "2021-06-29T01:55:13.121Z",
          "wordCount": 669,
          "title": "IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task. (arXiv:2105.11347v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1\">Sana Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Saeid Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1\">Raziyeh Zall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1\">Mohammad Reza Kangavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1\">Sara Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>",
          "description": "Multimodal sentiment analysis benefits various applications such as\nhuman-computer interaction and recommendation systems. It aims to infer the\nusers' bipolar ideas using visual, textual, and acoustic signals. Although\nresearchers affirm the association between cognitive cues and emotional\nmanifestations, most of the current multimodal approaches in sentiment analysis\ndisregard user-specific aspects. To tackle this issue, we devise a novel method\nto perform multimodal sentiment prediction using cognitive cues, such as\npersonality. Our framework constructs an adaptive tree by hierarchically\ndividing users and trains the LSTM-based submodels, utilizing an\nattention-based fusion to transfer cognitive-oriented knowledge within the\ntree. Subsequently, the framework consumes the conclusive agglomerative\nknowledge from the adaptive tree to predict final sentiments. We also devise a\ndynamic dropout method to facilitate data sharing between neighboring nodes,\nreducing data sparsity. The empirical results on real-world datasets determine\nthat our proposed model for sentiment prediction can surpass trending rivals.\nMoreover, compared to other ensemble approaches, the proposed transfer-based\nalgorithm can better utilize the latent cognitive cues and foster the\nprediction outcomes. Based on the given extrinsic and intrinsic analysis\nresults, we note that compared to other theoretical-based techniques, the\nproposed hierarchical clustering approach can better group the users within the\nadaptive tree.",
          "link": "http://arxiv.org/abs/2106.14174",
          "publishedOn": "2021-06-29T01:55:13.112Z",
          "wordCount": 664,
          "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. (arXiv:2106.14174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>",
          "description": "In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have primarily focused on processing text for\nclassification and small-scale databases. This paper describes a novel\nmultimodal deep learning architecture, called TechDoc, for technical document\nclassification, which utilizes both natural language and descriptive images to\ntrain hierarchical classifiers. The architecture synthesizes convolutional\nneural networks and recurrent neural networks through an integrated training\nprocess. We applied the architecture to a large multimodal technical document\ndatabase and trained the model for classifying documents based on the\nhierarchical International Patent Classification system. Our results show that\nthe trained neural network presents a greater classification accuracy than\nthose using a single modality and several earlier text classification methods.\nThe trained model can potentially be scaled to millions of real-world technical\ndocuments with both text and figures, which is useful for data and knowledge\nmanagement in large technology companies and organizations.",
          "link": "http://arxiv.org/abs/2106.14269",
          "publishedOn": "2021-06-29T01:55:13.104Z",
          "wordCount": 625,
          "title": "Deep Learning for Technical Document Classification. (arXiv:2106.14269v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tieyun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yile Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Ke Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiyong Peng</a>",
          "description": "One key property in recommender systems is the long-tail distribution in\nuser-item interactions where most items only have few user feedback. Improving\nthe recommendation of tail items can promote novelty and bring positive effects\nto both users and providers, and thus is a desirable property of recommender\nsystems. Current novel recommendation studies over-emphasize the importance of\ntail items without differentiating the degree of users' intent on popularity\nand often incur a sharp decline of accuracy. Moreover, none of existing methods\nhas ever taken the extreme case of tail items, i.e., cold-start items without\nany interaction, into consideration.\n\nIn this work, we first disclose the mechanism that drives a user's\ninteraction towards popular or niche items by disentangling her intent into\nconformity influence (popularity) and personal interests (preference). We then\npresent a unified end-to-end framework to simultaneously optimize accuracy and\nnovelty targets based on the disentangled intent of popularity and that of\npreference. We further develop a new paradigm for novel recommendation of\ncold-start items which exploits the self-supervised learning technique to model\nthe correlation between collaborative features and content features. We conduct\nextensive experimental results on three real-world datasets. The results\ndemonstrate that our proposed model yields significant improvements over the\nstate-of-the-art baselines in terms of accuracy, novelty, coverage, and\ntrade-off.",
          "link": "http://arxiv.org/abs/2106.14388",
          "publishedOn": "2021-06-29T01:55:13.094Z",
          "wordCount": 644,
          "title": "Intent Disentanglement and Feature Self-supervision for Novel Recommendation. (arXiv:2106.14388v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2006.04279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boratto_L/0/1/0/all/0/1\">Ludovico Boratto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fenu_G/0/1/0/all/0/1\">Gianni Fenu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marras_M/0/1/0/all/0/1\">Mirko Marras</a>",
          "description": "Considering the impact of recommendations on item providers is one of the\nduties of multi-sided recommender systems. Item providers are key stakeholders\nin online platforms, and their earnings and plans are influenced by the\nexposure their items receive in recommended lists. Prior work showed that\ncertain minority groups of providers, characterized by a common sensitive\nattribute (e.g., gender or race), are being disproportionately affected by\nindirect and unintentional discrimination. Our study in this paper handles a\nsituation where ($i$) the same provider is associated with multiple items of a\nlist suggested to a user, ($ii$) an item is created by more than one provider\njointly, and ($iii$) predicted user-item relevance scores are biasedly\nestimated for items of provider groups. Under this scenario, we assess\ndisparities in relevance, visibility, and exposure, by simulating diverse\nrepresentations of the minority group in the catalog and the interactions.\nBased on emerged unfair outcomes, we devise a treatment that combines\nobservation upsampling and loss regularization, while learning user-item\nrelevance scores. Experiments on real-world data demonstrate that our treatment\nleads to lower disparate relevance. The resulting recommended lists show fairer\nvisibility and exposure, higher minority item coverage, and negligible loss in\nrecommendation utility.",
          "link": "http://arxiv.org/abs/2006.04279",
          "publishedOn": "2021-06-29T01:55:13.076Z",
          "wordCount": 683,
          "title": "Interplay between Upsampling and Regularization for Provider Fairness in Recommender Systems. (arXiv:2006.04279v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makhortykh_M/0/1/0/all/0/1\">Mykola Makhortykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urman_A/0/1/0/all/0/1\">Aleksandra Urman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulloa_R/0/1/0/all/0/1\">Roberto Ulloa</a>",
          "description": "Web search engines influence perception of social reality by filtering and\nranking information. However, their outputs are often subjected to bias that\ncan lead to skewed representation of subjects such as professional occupations\nor gender. In our paper, we use a mixed-method approach to investigate presence\nof race and gender bias in representation of artificial intelligence (AI) in\nimage search results coming from six different search engines. Our findings\nshow that search engines prioritize anthropomorphic images of AI that portray\nit as white, whereas non-white images of AI are present only in non-Western\nsearch engines. By contrast, gender representation of AI is more diverse and\nless skewed towards a specific gender that can be attributed to higher\nawareness about gender bias in search outputs. Our observations indicate both\nthe the need and the possibility for addressing bias in representation of\nsocietally relevant subjects, such as technological innovation, and emphasize\nthe importance of designing new approaches for detecting bias in information\nretrieval systems.",
          "link": "http://arxiv.org/abs/2106.14072",
          "publishedOn": "2021-06-29T01:55:13.044Z",
          "wordCount": 627,
          "title": "Detecting race and gender bias in visual representation of AI on web search engines. (arXiv:2106.14072v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.",
          "link": "http://arxiv.org/abs/2106.14463",
          "publishedOn": "2021-06-29T01:55:13.015Z",
          "wordCount": 674,
          "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiding Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weixue Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Suqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yukun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Daiting Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhicong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>",
          "description": "Retrieval is a crucial stage in web search that identifies a small set of\nquery-relevant candidates from a billion-scale corpus. Discovering more\nsemantically-related candidates in the retrieval stage is very promising to\nexpose more high-quality results to the end users. However, it still remains\nnon-trivial challenges of building and deploying effective retrieval models for\nsemantic matching in real search engine. In this paper, we describe the\nretrieval system that we developed and deployed in Baidu Search. The system\nexploits the recent state-of-the-art Chinese pretrained language model, namely\nEnhanced Representation through kNowledge IntEgration (ERNIE), which\nfacilitates the system with expressive semantic matching. In particular, we\ndeveloped an ERNIE-based retrieval model, which is equipped with 1) expressive\nTransformer-based semantic encoders, and 2) a comprehensive multi-stage\ntraining paradigm. More importantly, we present a practical system workflow for\ndeploying the model in web-scale retrieval. Eventually, the system is fully\ndeployed into production, where rigorous offline and online experiments were\nconducted. The results show that the system can perform high-quality candidate\nretrieval, especially for those tail queries with uncommon demands. Overall,\nthe new retrieval system facilitated by pretrained language model (i.e., ERNIE)\ncan largely improve the usability and applicability of our search engine.",
          "link": "http://arxiv.org/abs/2106.03373",
          "publishedOn": "2021-06-28T01:57:53.900Z",
          "wordCount": 664,
          "title": "Pre-trained Language Model for Web-scale Retrieval in Baidu Search. (arXiv:2106.03373v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1\">Minjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_j/0/1/0/all/0/1\">jinhong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joonseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1\">Hyunjung Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongwuk Lee</a>",
          "description": "Session-based recommendation aims at predicting the next item given a\nsequence of previous items consumed in the session, e.g., on e-commerce or\nmultimedia streaming services. Specifically, session data exhibits some unique\ncharacteristics, i.e., session consistency and sequential dependency over items\nwithin the session, repeated item consumption, and session timeliness. In this\npaper, we propose simple-yet-effective linear models for considering the\nholistic aspects of the sessions. The comprehensive nature of our models helps\nimprove the quality of session-based recommendation. More importantly, it\nprovides a generalized framework for reflecting different perspectives of\nsession data. Furthermore, since our models can be solved by closed-form\nsolutions, they are highly scalable. Experimental results demonstrate that the\nproposed linear models show competitive or state-of-the-art performance in\nvarious metrics on several real-world datasets.",
          "link": "http://arxiv.org/abs/2103.16104",
          "publishedOn": "2021-06-28T01:57:53.803Z",
          "wordCount": 599,
          "title": "Session-aware Linear Item-Item Models for Session-based Recommendation. (arXiv:2103.16104v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_H/0/1/0/all/0/1\">Hrishikesh Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alicea_B/0/1/0/all/0/1\">Bradly Alicea</a>",
          "description": "Literary artefacts are generally indexed and searched based on titles, meta\ndata and keywords over the years. This searching and indexing works well when\nuser/reader already knows about that particular creative textual artefact or\ndocument. This indexing and search hardly takes into account interest and\nemotional makeup of readers and its mapping to books. When a person is looking\nfor a literary textual artefact, he/she might be looking for not only\ninformation but also to seek the joy of reading. In case of literary artefacts,\nprogression of emotions across the key events could prove to be the key for\nindexing and searching. In this paper, we establish clusters among literary\nartefacts based on computational relationships among sentiment progressions\nusing intelligent text analysis. We have created a database of 1076 English\ntitles + 20 Marathi titles and also used database\nthis http URL with 16559 titles and their\nsummaries. We have proposed Sentiment Progression based Indexing for searching\nand recommending books. This can be used to create personalized clusters of\nbook titles of interest to readers. The analysis clearly suggests better\nsearching and indexing when we are targeting book lovers looking for a\nparticular type of book or creative artefact. This indexing and searching can\nfind many real-life applications for recommending books.",
          "link": "http://arxiv.org/abs/2106.13767",
          "publishedOn": "2021-06-28T01:57:53.746Z",
          "wordCount": 654,
          "title": "Sentiment Progression based Searching and Indexing of Literary Textual Artefacts. (arXiv:2106.13767v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lesota_O/0/1/0/all/0/1\">Oleg Lesota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1\">Daniel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasserbauer_K/0/1/0/all/0/1\">Klaus Antonius Grasserbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>",
          "description": "Existing neural ranking models follow the text matching paradigm, where\ndocument-to-query relevance is estimated through predicting the matching score.\nDrawing from the rich literature of classical generative retrieval models, we\nintroduce and formalize the paradigm of deep generative retrieval models\ndefined via the cumulative probabilities of generating query terms. This\nparadigm offers a grounded probabilistic view on relevance estimation while\nstill enabling the use of modern neural architectures. In contrast to the\nmatching paradigm, the probabilistic nature of generative rankers readily\noffers a fine-grained measure of uncertainty. We adopt several current neural\ngenerative models in our framework and introduce a novel generative ranker\n(T-PGN), which combines the encoding capacity of Transformers with the Pointer\nGenerator Network model. We conduct an extensive set of evaluation experiments\non passage retrieval, leveraging the MS MARCO Passage Re-ranking and TREC Deep\nLearning 2019 Passage Re-ranking collections. Our results show the\nsignificantly higher performance of the T-PGN model when compared with other\ngenerative models. Lastly, we demonstrate that exploiting the uncertainty\ninformation of deep generative rankers opens new perspectives to\nquery/collection understanding, and significantly improves the cut-off\nprediction task.",
          "link": "http://arxiv.org/abs/2106.13618",
          "publishedOn": "2021-06-28T01:57:53.674Z",
          "wordCount": 629,
          "title": "A Modern Perspective on Query Likelihood with Deep Generative Retrieval Models. (arXiv:2106.13618v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_K/0/1/0/all/0/1\">Kishlay Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>",
          "description": "Fake news travels at unprecedented speeds, reaches global audiences and puts\nusers and communities at great risk via social media platforms. Deep learning\nbased models show good performance when trained on large amounts of labeled\ndata on events of interest, whereas the performance of models tends to degrade\non other events due to domain shift. Therefore, significant challenges are\nposed for existing detection approaches to detect fake news on emergent events,\nwhere large-scale labeled datasets are difficult to obtain. Moreover, adding\nthe knowledge from newly emergent events requires to build a new model from\nscratch or continue to fine-tune the model, which can be challenging,\nexpensive, and unrealistic for real-world settings. In order to address those\nchallenges, we propose an end-to-end fake news detection framework named\nMetaFEND, which is able to learn quickly to detect fake news on emergent events\nwith a few verified posts. Specifically, the proposed model integrates\nmeta-learning and neural process methods together to enjoy the benefits of\nthese approaches. In particular, a label embedding module and a hard attention\nmechanism are proposed to enhance the effectiveness by handling categorical\ninformation and trimming irrelevant posts. Extensive experiments are conducted\non multimedia datasets collected from Twitter and Weibo. The experimental\nresults show our proposed MetaFEND model can detect fake news on never-seen\nevents effectively and outperform the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.13711",
          "publishedOn": "2021-06-28T01:57:53.608Z",
          "wordCount": 668,
          "title": "Multimodal Emergent Fake News Detection via Meta Neural Process Networks. (arXiv:2106.13711v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Lixin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hengyi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Dehong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Suqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Daiting Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhifan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weiyue Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhicong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>",
          "description": "As the heart of a search engine, the ranking system plays a crucial role in\nsatisfying users' information demands. More recently, neural rankers fine-tuned\nfrom pre-trained language models (PLMs) establish state-of-the-art ranking\neffectiveness. However, it is nontrivial to directly apply these PLM-based\nrankers to the large-scale web search system due to the following challenging\nissues:(1) the prohibitively expensive computations of massive neural PLMs,\nespecially for long texts in the web-document, prohibit their deployments in an\nonline ranking system that demands extremely low latency;(2) the discrepancy\nbetween existing ranking-agnostic pre-training objectives and the ad-hoc\nretrieval scenarios that demand comprehensive relevance modeling is another\nmain barrier for improving the online ranking system;(3) a real-world search\nengine typically involves a committee of ranking components, and thus the\ncompatibility of the individually fine-tuned ranking model is critical for a\ncooperative ranking system. In this work, we contribute a series of\nsuccessfully applied techniques in tackling these exposed issues when deploying\nthe state-of-the-art Chinese pre-trained language model, i.e., ERNIE, in the\nonline search engine system. We first articulate a novel practice to\ncost-efficiently summarize the web document and contextualize the resultant\nsummary content with the query using a cheap yet powerful Pyramid-ERNIE\narchitecture. Then we endow an innovative paradigm to finely exploit the\nlarge-scale noisy and biased post-click behavioral data for relevance-oriented\npre-training. We also propose a human-anchored fine-tuning strategy tailored\nfor the online ranking system, aiming to stabilize the ranking signals across\nvarious online components. Extensive offline and online experimental results\nshow that the proposed techniques significantly boost the search engine's\nperformance.",
          "link": "http://arxiv.org/abs/2105.11108",
          "publishedOn": "2021-06-28T01:57:53.587Z",
          "wordCount": 751,
          "title": "Pre-trained Language Model based Ranking in Baidu Search. (arXiv:2105.11108v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruiming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Ben Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng Ann Heng</a>",
          "description": "Fairness in recommendation has attracted increasing attention due to bias and\ndiscrimination possibly caused by traditional recommenders. In Interactive\nRecommender Systems (IRS), user preferences and the system's fairness status\nare constantly changing over time. Existing fairness-aware recommenders mainly\nconsider fairness in static settings. Directly applying existing methods to IRS\nwill result in poor recommendation. To resolve this problem, we propose a\nreinforcement learning based framework, FairRec, to dynamically maintain a\nlong-term balance between accuracy and fairness in IRS. User preferences and\nthe system's fairness status are jointly compressed into the state\nrepresentation to generate recommendations. FairRec aims at maximizing our\ndesigned cumulative reward that combines accuracy and fairness. Extensive\nexperiments validate that FairRec can improve fairness, while preserving good\nrecommendation quality.",
          "link": "http://arxiv.org/abs/2106.13386",
          "publishedOn": "2021-06-28T01:57:53.503Z",
          "wordCount": 564,
          "title": "Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning. (arXiv:2106.13386v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Russell_Rose_T/0/1/0/all/0/1\">Tony Russell-Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gooch_P/0/1/0/all/0/1\">Philip Gooch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruschwitz_U/0/1/0/all/0/1\">Udo Kruschwitz</a>",
          "description": "Knowledge workers (such as healthcare information professionals, patent\nagents and recruitment professionals) undertake work tasks where search forms a\ncore part of their duties. In these instances, the search task is often complex\nand time-consuming and requires specialist expert knowledge to formulate\naccurate search strategies. Interactive features such as query expansion can\nplay a key role in supporting these tasks. However, generating query\nsuggestions within a professional search context requires that consideration be\ngiven to the specialist, structured nature of the search strategies they\nemploy. In this paper, we investigate a variety of query expansion methods\napplied to a collection of Boolean search strategies used in a variety of\nreal-world professional search tasks. The results demonstrate the utility of\ncontext-free distributional language models and the value of using linguistic\ncues such as ngram order to optimise the balance between precision and recall.",
          "link": "http://arxiv.org/abs/2106.13528",
          "publishedOn": "2021-06-28T01:57:53.495Z",
          "wordCount": 576,
          "title": "Interactive query expansion for professional search applications. (arXiv:2106.13528v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jinjin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiguo Gong</a>",
          "description": "The abundant sequential documents such as online archival, social media and\nnews feeds are streamingly updated, where each chunk of documents is\nincorporated with smoothly evolving yet dependent topics. Such digital texts\nhave attracted extensive research on dynamic topic modeling to infer hidden\nevolving topics and their temporal dependencies. However, most of the existing\napproaches focus on single-topic-thread evolution and ignore the fact that a\ncurrent topic may be coupled with multiple relevant prior topics. In addition,\nthese approaches also incur the intractable inference problem when inferring\nlatent parameters, resulting in a high computational cost and performance\ndegradation. In this work, we assume that a current topic evolves from all\nprior topics with corresponding coupling weights, forming the\nmulti-topic-thread evolution. Our method models the dependencies between\nevolving topics and thoroughly encodes their complex multi-couplings across\ntime steps. To conquer the intractable inference challenge, a new solution with\na set of novel data augmentation techniques is proposed, which successfully\ndiscomposes the multi-couplings between evolving topics. A fully conjugate\nmodel is thus obtained to guarantee the effectiveness and efficiency of the\ninference technique. A novel Gibbs sampler with a backward-forward filter\nalgorithm efficiently learns latent timeevolving parameters in a closed-form.\nIn addition, the latent Indian Buffet Process (IBP) compound distribution is\nexploited to automatically infer the overall topic number and customize the\nsparse topic proportions for each sequential document without bias. The\nproposed method is evaluated on both synthetic and real-world datasets against\nthe competitive baselines, demonstrating its superiority over the baselines in\nterms of the low per-word perplexity, high coherent topics, and better document\ntime prediction.",
          "link": "http://arxiv.org/abs/2106.13732",
          "publishedOn": "2021-06-28T01:57:53.232Z",
          "wordCount": 695,
          "title": "Recurrent Coupled Topic Modeling over Sequential Documents. (arXiv:2106.13732v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhouyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Spreadsheet table detection is the task of detecting all tables on a given\nsheet and locating their respective ranges. Automatic table detection is a key\nenabling technique and an initial step in spreadsheet data intelligence.\nHowever, the detection task is challenged by the diversity of table structures\nand table layouts on the spreadsheet. Considering the analogy between a cell\nmatrix as spreadsheet and a pixel matrix as image, and encouraged by the\nsuccessful application of Convolutional Neural Networks (CNN) in computer\nvision, we have developed TableSense, a novel end-to-end framework for\nspreadsheet table detection. First, we devise an effective cell featurization\nscheme to better leverage the rich information in each cell; second, we develop\nan enhanced convolutional neural network model for table detection to meet the\ndomain-specific requirement on precise table boundary detection; third, we\npropose an effective uncertainty metric to guide an active learning based smart\nsampling algorithm, which enables the efficient build-up of a training dataset\nwith 22,176 tables on 10,220 sheets with broad coverage of diverse table\nstructures and layouts. Our evaluation shows that TableSense is highly\neffective with 91.3\\% recall and 86.5\\% precision in EoB-2 metric, a\nsignificant improvement over both the current detection algorithm that are used\nin commodity spreadsheet tools and state-of-the-art convolutional neural\nnetworks in computer vision.",
          "link": "http://arxiv.org/abs/2106.13500",
          "publishedOn": "2021-06-28T01:57:53.189Z",
          "wordCount": 648,
          "title": "TableSense: Spreadsheet Table Detection with Convolutional Neural Networks. (arXiv:2106.13500v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogahn_R/0/1/0/all/0/1\">Richard Rogahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul N. Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>",
          "description": "Information overload is a prevalent challenge in many high-value domains. A\nprominent case in point is the explosion of the biomedical literature on\nCOVID-19, which swelled to hundreds of thousands of papers in a matter of\nmonths. In general, biomedical literature expands by two papers every minute,\ntotalling over a million new papers every year. Search in the biomedical realm,\nand many other vertical domains is challenging due to the scarcity of direct\nsupervision from click logs. Self-supervised learning has emerged as a\npromising direction to overcome the annotation bottleneck. We propose a general\napproach for vertical search based on domain-specific pretraining and present a\ncase study for the biomedical domain. Despite being substantially simpler and\nnot using any relevance labels for training or development, our method performs\ncomparably or better than the best systems in the official TREC-COVID\nevaluation, a COVID-related biomedical search competition. Using distributed\ncomputing in modern cloud infrastructure, our system can scale to tens of\nmillions of articles on PubMed and has been deployed as Microsoft Biomedical\nSearch, a new search experience for biomedical literature:\nhttps://aka.ms/biomedsearch.",
          "link": "http://arxiv.org/abs/2106.13375",
          "publishedOn": "2021-06-28T01:57:53.178Z",
          "wordCount": 693,
          "title": "Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1\">Giorgos Kordopatis-Zilos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1\">Christos Tzelepis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>",
          "description": "In this paper, we address the problem of high performance and computationally\nefficient content-based video retrieval in large-scale datasets. Current\nmethods typically propose either: (i) fine-grained approaches employing\nspatio-temporal representations and similarity calculations, achieving high\nperformance at a high computational cost or (ii) coarse-grained approaches\nrepresenting/indexing videos as global vectors, where the spatio-temporal\nstructure is lost, providing low performance but also having low computational\ncost. In this work, we propose a Knowledge Distillation framework, which we\ncall Distill-and-Select (DnS), that starting from a well-performing\nfine-grained Teacher Network learns: a) Student Networks at different retrieval\nperformance and computational efficiency trade-offs and b) a Selection Network\nthat at test time rapidly directs samples to the appropriate student to\nmaintain both high retrieval performance and high computational efficiency. We\ntrain several students with different architectures and arrive at different\ntrade-offs of performance and efficiency, i.e., speed and storage requirements,\nincluding fine-grained students that store index videos using binary\nrepresentations. Importantly, the proposed scheme allows Knowledge Distillation\nin large, unlabelled datasets -- this leads to good students. We evaluate DnS\non five public datasets on three different video retrieval tasks and\ndemonstrate a) that our students achieve state-of-the-art performance in\nseveral cases and b) that our DnS framework provides an excellent trade-off\nbetween retrieval performance, computational speed, and storage space. In\nspecific configurations, our method achieves similar mAP with the teacher but\nis 20 times faster and requires 240 times less storage space. Our collected\ndataset and implementation are publicly available:\nhttps://github.com/mever-team/distill-and-select.",
          "link": "http://arxiv.org/abs/2106.13266",
          "publishedOn": "2021-06-28T01:57:53.164Z",
          "wordCount": 699,
          "title": "DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soni_B/0/1/0/all/0/1\">Badal Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakuria_D/0/1/0/all/0/1\">Debangan Thakuria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nath_N/0/1/0/all/0/1\">Nilutpal Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1\">Navarun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boro_B/0/1/0/all/0/1\">Bhaskarananda Boro</a>",
          "description": "Anime is quite well-received today, especially among the younger generations.\nWith many genres of available shows, more and more people are increasingly\ngetting attracted to this niche section of the entertainment industry. As anime\nhas recently garnered mainstream attention, we have insufficient information\nregarding users' penchant and watching habits. Therefore, it is an uphill task\nto build a recommendation engine for this relatively obscure entertainment\nmedium. In this attempt, we have built a novel hybrid recommendation system\nthat could act both as a recommendation system and as a means of exploring new\nanime genres and titles. We have analyzed the general trends in this field and\nthe users' watching habits for coming up with our efficacious solution. Our\nsolution employs deep autoencoders for the tasks of predicting ratings and\ngenerating embeddings. Following this, we formed clusters using the embeddings\nof the anime titles. These clusters form the search space for anime with\nsimilarities and are used to find anime similar to the ones liked and disliked\nby the user. This method, combined with the predicted ratings, forms the novel\nhybrid filter. In this article, we have demonstrated this idea and compared the\nperformance of our implemented model with the existing state-of-the-art\ntechniques.",
          "link": "http://arxiv.org/abs/2106.12970",
          "publishedOn": "2021-06-25T02:00:44.393Z",
          "wordCount": 644,
          "title": "RikoNet: A Novel Anime Recommendation Engine. (arXiv:2106.12970v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Spoken conversational question answering (SCQA) requires machines to model\ncomplex dialogue flow given the speech utterances and text corpora. Different\nfrom traditional text question answering (QA) tasks, SCQA involves audio signal\nprocessing, passage comprehension, and contextual understanding. However, ASR\nsystems introduce unexpected noisy signals to the transcriptions, which result\nin performance degradation on SCQA. To overcome the problem, we propose CADNet,\na novel contextualized attention-based distillation approach, which applies\nboth cross-attention and self-attention to obtain ASR-robust contextualized\nembedding representations of the passage and dialogue history for performance\nimprovements. We also introduce the spoken conventional knowledge distillation\nframework to distill the ASR-robust knowledge from the estimated probabilities\nof the teacher model to the student. We conduct extensive experiments on the\nSpoken-CoQA dataset and demonstrate that our approach achieves remarkable\nperformance in this task.",
          "link": "http://arxiv.org/abs/2010.11066",
          "publishedOn": "2021-06-25T02:00:44.372Z",
          "wordCount": 631,
          "title": "Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salatino_A/0/1/0/all/0/1\">Angelo Salatino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannocci_A/0/1/0/all/0/1\">Andrea Mannocci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osborne_F/0/1/0/all/0/1\">Francesco Osborne</a>",
          "description": "Analysing research trends and predicting their impact on academia and\nindustry is crucial to gain a deeper understanding of the advances in a\nresearch field and to inform critical decisions about research funding and\ntechnology adoption. In the last years, we saw the emergence of several\npublicly-available and large-scale Scientific Knowledge Graphs fostering the\ndevelopment of many data-driven approaches for performing quantitative analyses\nof research trends. This chapter presents an innovative framework for\ndetecting, analysing, and forecasting research topics based on a large-scale\nknowledge graph characterising research articles according to the research\ntopics from the Computer Science Ontology. We discuss the advantages of a\nsolution based on a formal representation of topics and describe how it was\napplied to produce bibliometric studies and innovative tools for analysing and\npredicting research dynamics.",
          "link": "http://arxiv.org/abs/2106.12875",
          "publishedOn": "2021-06-25T02:00:44.353Z",
          "wordCount": 569,
          "title": "Detection, Analysis, and Prediction of Research Topics with Scientific Knowledge Graphs. (arXiv:2106.12875v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_S/0/1/0/all/0/1\">Simon Poon</a>",
          "description": "Process mining is a relatively new subject which builds a bridge between\nprocess modelling and data mining. An exclusive choice in a process model\nusually splits the process into different branches. However, in some processes,\nit is possible to switch from one branch to another. The inductive miner\nguarantees to return sound process models, but fails to return a precise model\nwhen there are switch behaviours between different exclusive choice branches\ndue to the limitation of process trees. In this paper, we present a novel\nextension to the process tree model to support switch behaviours between\ndifferent branches of the exclusive choice operator and propose a novel\nextension to the inductive miner to discover sound process models with switch\nbehaviours. The proposed discovery technique utilizes the theory of a previous\nstudy to detect possible switch behaviours. We apply both artificial and\npublicly-available datasets to evaluate our approach. Our results show that our\napproach can improve the precision of discovered models by 36% while\nmaintaining high fitness values compared to the original inductive miner.",
          "link": "http://arxiv.org/abs/2106.12765",
          "publishedOn": "2021-06-25T02:00:44.307Z",
          "wordCount": 629,
          "title": "A Novel Approach to Discover Switch Behaviours in Process Mining. (arXiv:2106.12765v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asprino_L/0/1/0/all/0/1\">Luigi Asprino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colonna_C/0/1/0/all/0/1\">Christian Colonna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mongiovi_M/0/1/0/all/0/1\">Misael Mongiov&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porena_M/0/1/0/all/0/1\">Margherita Porena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Presutti_V/0/1/0/all/0/1\">Valentina Presutti</a>",
          "description": "We present a novel approach to knowledge graph visualization based on\nontology design patterns. This approach relies on OPLa (Ontology Pattern\nLanguage) annotations and on a catalogue of visual frames, which are associated\nwith foundational ontology design patterns. We demonstrate that this approach\nsignificantly reduces the cognitive load required to users for visualizing and\ninterpreting a knowledge graph and guides the user in exploring it through\nmeaningful thematic paths provided by ontology patterns.",
          "link": "http://arxiv.org/abs/2106.12857",
          "publishedOn": "2021-06-25T02:00:44.277Z",
          "wordCount": 510,
          "title": "Pattern-based Visualization of Knowledge Graphs. (arXiv:2106.12857v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chong_J/0/1/0/all/0/1\">Jia Wei Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Mei Shin Oh</a>",
          "description": "Service manual documents are crucial to the engineering company as they\nprovide guidelines and knowledge to service engineers. However, it has become\ninconvenient and inefficient for service engineers to retrieve specific\nknowledge from documents due to the complexity of resources. In this research,\nwe propose an automated knowledge mining and document classification system\nwith novel multi-model transfer learning approaches. Particularly, the\nclassification performance of the system has been improved with three effective\ntechniques: fine-tuning, pruning, and multi-model method. The fine-tuning\ntechnique optimizes a pre-trained BERT model by adding a feed-forward neural\nnetwork layer and the pruning technique is used to retrain the BERT model with\nnew data. The multi-model method initializes and trains multiple BERT models to\novercome the randomness of data ordering during the fine-tuning process. In the\nfirst iteration of the training process, multiple BERT models are being trained\nsimultaneously. The best model is then selected for the next phase of the\ntraining process with another two iterations and the training processes for\nother BERT models will be terminated. The performance of the proposed system\nhas been evaluated by comparing with two robust baseline methods, BERT and\nBERT-CNN. Experimental results on a widely used Corpus of Linguistic\nAcceptability (CoLA) dataset have shown that the proposed techniques perform\nbetter than these baseline methods in terms of accuracy and MCC score.",
          "link": "http://arxiv.org/abs/2106.12744",
          "publishedOn": "2021-06-25T02:00:44.245Z",
          "wordCount": 677,
          "title": "An Automated Knowledge Mining and Document Classification System with Multi-model Transfer Learning. (arXiv:2106.12744v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schutte_D/0/1/0/all/0/1\">Dalton Schutte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1\">Jake Vasilakes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bompelli_A/0/1/0/all/0/1\">Anu Bompelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiszman_M/0/1/0/all/0/1\">Marcelo Fiszman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilicoglu_H/0/1/0/all/0/1\">Halil Kilicoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bishop_J/0/1/0/all/0/1\">Jeffrey R. Bishop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_T/0/1/0/all/0/1\">Terrence Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "OBJECTIVE: Leverage existing biomedical NLP tools and DS domain terminology\nto produce a novel and comprehensive knowledge graph containing dietary\nsupplement (DS) information for discovering interactions between DS and drugs,\nor Drug-Supplement Interactions (DSI). MATERIALS AND METHODS: We created\nSemRepDS (an extension of SemRep), capable of extracting semantic relations\nfrom abstracts by leveraging a DS-specific terminology (iDISK) containing\n28,884 DS terms not found in the UMLS. PubMed abstracts were processed using\nSemRepDS to generate semantic relations, which were then filtered using a\nPubMedBERT-based model to remove incorrect relations before generating our\nknowledge graph (SuppKG). Two pathways are used to identify potential DS-Drug\ninteractions which are then evaluated by medical professionals for mechanistic\nplausibility. RESULTS: Comparison analysis found that SemRepDS returned 206.9%\nmore DS relations and 158.5% more DS entities than SemRep. The fine-tuned BERT\nmodel obtained an F1 score of 0.8605 and removed 43.86% of the relations,\nimproving the precision of the relations by 26.4% compared to pre-filtering.\nSuppKG consists of 2,928 DS-specific nodes. Manual review of findings\nidentified 44 (88%) proposed DS-Gene-Drug and 32 (64%) proposed\nDS-Gene1-Function-Gene2-Drug pathways to be mechanistically plausible.\nDISCUSSION: The additional relations extracted using SemRepDS generated SuppKG\nthat was used to find plausible DSI not found in the current literature. By the\nnature of the SuppKG, these interactions are unlikely to have been found using\nSemRep without the expanded DS terminology. CONCLUSION: We successfully extend\nSemRep to include DS information and produce SuppKG which can be used to find\npotential DS-Drug interactions.",
          "link": "http://arxiv.org/abs/2106.12741",
          "publishedOn": "2021-06-25T02:00:44.227Z",
          "wordCount": 714,
          "title": "Discovering novel drug-supplement interactions using a dietary supplements knowledge graph generated from the biomedical literature. (arXiv:2106.12741v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Wei-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daniel Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hsiang-Fu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1\">Choon-Hui Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1\">Kai Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolluri_K/0/1/0/all/0/1\">Kedarnath Kolluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shandilya_N/0/1/0/all/0/1\">Nikhil Shandilya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ievgrafov_V/0/1/0/all/0/1\">Vyacheslav Ievgrafov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Japinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit S. Dhillon</a>",
          "description": "We consider the problem of semantic matching in product search: given a\ncustomer query, retrieve all semantically related products from a huge catalog\nof size 100 million, or more. Because of large catalog spaces and real-time\nlatency constraints, semantic matching algorithms not only desire high recall\nbut also need to have low latency. Conventional lexical matching approaches\n(e.g., Okapi-BM25) exploit inverted indices to achieve fast inference time, but\nfail to capture behavioral signals between queries and products. In contrast,\nembedding-based models learn semantic representations from customer behavior\ndata, but the performance is often limited by shallow neural encoders due to\nlatency constraints. Semantic product search can be viewed as an eXtreme\nMulti-label Classification (XMC) problem, where customer queries are input\ninstances and products are output labels. In this paper, we aim to improve\nsemantic product search by using tree-based XMC models where inference time\ncomplexity is logarithmic in the number of products. We consider hierarchical\nlinear models with n-gram features for fast real-time inference.\nQuantitatively, our method maintains a low latency of 1.25 milliseconds per\nquery and achieves a 65% improvement of Recall@100 (60.9% v.s. 36.8%) over a\ncompeting embedding-based DSSM model. Our model is robust to weight pruning\nwith varying thresholds, which can flexibly meet different system requirements\nfor online deployments. Qualitatively, our method can retrieve products that\nare complementary to existing product search system and add diversity to the\nmatch set.",
          "link": "http://arxiv.org/abs/2106.12657",
          "publishedOn": "2021-06-25T02:00:44.209Z",
          "wordCount": 695,
          "title": "Extreme Multi-label Learning for Semantic Matching in Product Search. (arXiv:2106.12657v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenshuo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1\">Karl Krauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1\">Nikhil Garg</a>",
          "description": "Recommender systems -- and especially matrix factorization-based\ncollaborative filtering algorithms -- play a crucial role in mediating our\naccess to online information. We show that such algorithms induce a particular\nkind of stereotyping: if preferences for a \\textit{set} of items are\nanti-correlated in the general user population, then those items may not be\nrecommended together to a user, regardless of that user's preferences and\nratings history. First, we introduce a notion of \\textit{joint accessibility},\nwhich measures the extent to which a set of items can jointly be accessed by\nusers. We then study joint accessibility under the standard factorization-based\ncollaborative filtering framework, and provide theoretical necessary and\nsufficient conditions when joint accessibility is violated. Moreover, we show\nthat these conditions can easily be violated when the users are represented by\na single feature vector. To improve joint accessibility, we further propose an\nalternative modelling fix, which is designed to capture the diverse multiple\ninterests of each user using a multi-vector representation. We conduct\nextensive experiments on real and simulated datasets, demonstrating the\nstereotyping problem with standard single-vector matrix factorization models.",
          "link": "http://arxiv.org/abs/2106.12622",
          "publishedOn": "2021-06-25T02:00:44.164Z",
          "wordCount": 615,
          "title": "The Stereotyping Problem in Collaboratively Filtered Recommender Systems. (arXiv:2106.12622v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1\">Hayden S. Helm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdin_M/0/1/0/all/0/1\">Marah Abdin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedigo_B/0/1/0/all/0/1\">Benjamin D. Pedigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1\">Shweti Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyzinski_V/0/1/0/all/0/1\">Vince Lyzinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Youngser Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Amitabh Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_P/0/1/0/all/0/1\">Piali~Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Christopher M. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>",
          "description": "In modern ranking problems, different and disparate representations of the\nitems to be ranked are often available. It is sensible, then, to try to combine\nthese representations to improve ranking. Indeed, learning to rank via\ncombining representations is both principled and practical for learning a\nranking function for a particular query. In extremely data-scarce settings,\nhowever, the amount of labeled data available for a particular query can lead\nto a highly variable and ineffective ranking function. One way to mitigate the\neffect of the small amount of data is to leverage information from semantically\nsimilar queries. Indeed, as we demonstrate in simulation settings and real data\nexamples, when semantically similar queries are available it is possible to\ngainfully use them when ranking with respect to a particular query. We describe\nand explore this phenomenon in the context of the bias-variance trade off and\napply it to the data-scarce settings of a Bing navigational graph and the\nDrosophila larva connectome.",
          "link": "http://arxiv.org/abs/2106.12621",
          "publishedOn": "2021-06-25T02:00:44.150Z",
          "wordCount": 616,
          "title": "Leveraging semantically similar queries for ranking via combining representations. (arXiv:2106.12621v1 [cs.LG])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2008.09883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Ahmed Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moy_F/0/1/0/all/0/1\">Foong Ming Moy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_N/0/1/0/all/0/1\">Nadine Kong</a>",
          "description": "Obesity is known to lower the quality of life substantially. It is often\nassociated with increased chances of non-communicable diseases such as\ndiabetes, cardiovascular problems, different types of cancers, etc. Evidence\nsuggests that diet-related mobile applications play a vital role in assisting\nan individual in making healthier choices and keeping track of food intake.\nHowever, due to an abundance of similar applications, it becomes pertinent to\nevaluate each of them in terms of functionality, usability, and possible design\nissues to truly determine state-of-the-art solutions for the future. Since\nthese applications involve implementing multiple user requirements and\nrecommendations from different dietitians, the evaluation becomes quite\ncomplex. Therefore, this study aims to review existing dietary applications at\nlength to highlight key features and problems that enhance or undermine an\napplication's usability. For this purpose, we have examined the published\nliterature from various scientific databases of the CINAHL, Science Direct, and\nPUBMED. Out of our findings, fifty-six primary studies met our inclusion\ncriteria after filtering out titles, abstracts, and full text. A total of 35\napps are analyzed from the selected studies. Our detailed analysis concluded\nthe comprehensiveness of freely available mHealth applications from users and\ndietitians' frames of reference. Furthermore, we have also specified potential\nfuture challenges and stated recommendations to help develop clinically\naccurate diet-related applications.",
          "link": "http://arxiv.org/abs/2008.09883",
          "publishedOn": "2021-07-01T01:59:31.601Z",
          "wordCount": 712,
          "title": "A Systematic Literature Review of Critical Features and General Issues of Freely Available mHealth Apps For Dietary Assessment. (arXiv:2008.09883v3 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhenqiu Shu</a>",
          "description": "Hashing plays an important role in information retrieval, due to its low\nstorage and high speed of processing. Among the techniques available in the\nliterature, multi-modal hashing, which can encode heterogeneous multi-modal\nfeatures into compact hash codes, has received particular attention. Most of\nthe existing multi-modal hashing methods adopt the fixed weighting factors to\nfuse multiple modalities for any query data, which cannot capture the variation\nof different queries. Besides, many methods introduce hyper-parameters to\nbalance many regularization terms that make the optimization harder. Meanwhile,\nit is time-consuming and labor-intensive to set proper parameter values. The\nlimitations may significantly hinder their promotion in real applications. In\nthis paper, we propose a simple, yet effective method that is inspired by the\nHadamard matrix. The proposed method captures the multi-modal feature\ninformation in an adaptive manner and preserves the discriminative semantic\ninformation in the hash codes. Our framework is flexible and involves a very\nfew hyper-parameters. Extensive experimental results show the method is\neffective and achieves superior performance compared to state-of-the-art\nalgorithms.",
          "link": "http://arxiv.org/abs/2009.12148",
          "publishedOn": "2021-07-01T01:59:31.469Z",
          "wordCount": 641,
          "title": "Adaptive Multi-modal Fusion Hashing via Hadamard Matrix. (arXiv:2009.12148v3 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1\">Dhiraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1\">Kartik Audhkhasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>",
          "description": "Current methods for learning visually grounded language from videos often\nrely on text annotation, such as human generated captions or machine generated\nautomatic speech recognition (ASR) transcripts. In this work, we introduce the\nAudio-Video Language Network (AVLnet), a self-supervised network that learns a\nshared audio-visual embedding space directly from raw video inputs. To\ncircumvent the need for text annotation, we learn audio-visual representations\nfrom randomly segmented video clips and their raw audio waveforms. We train\nAVLnet on HowTo100M, a large corpus of publicly available instructional videos,\nand evaluate on image retrieval and video retrieval tasks, achieving\nstate-of-the-art performance. We perform analysis of AVLnet's learned\nrepresentations, showing our model utilizes speech and natural sounds to learn\naudio-visual concepts. Further, we propose a tri-modal model that jointly\nprocesses raw audio, video, and text captions from videos to learn a\nmulti-modal semantic embedding space useful for text-video retrieval. Our code,\ndata, and trained models will be released at avlnet.csail.mit.edu",
          "link": "http://arxiv.org/abs/2006.09199",
          "publishedOn": "2021-07-01T01:59:31.361Z",
          "wordCount": 675,
          "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos. (arXiv:2006.09199v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Prasanta Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raj Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinping Yang</a>",
          "description": "Emotional expressions form a key part of user behavior on today's digital\nplatforms. While multimodal emotion recognition techniques are gaining research\nattention, there is a lack of deeper understanding on how visual and non-visual\nfeatures can be used to better recognize emotions in certain contexts, but not\nothers. This study analyzes the interplay between the effects of multimodal\nemotion features derived from facial expressions, tone and text in conjunction\nwith two key contextual factors: i) gender of the speaker, and ii) duration of\nthe emotional episode. Using a large public dataset of 2,176 manually annotated\nYouTube videos, we found that while multimodal features consistently\noutperformed bimodal and unimodal features, their performance varied\nsignificantly across different emotions, gender and duration contexts.\nMultimodal features performed particularly better for male speakers in\nrecognizing most emotions. Furthermore, multimodal features performed\nparticularly better for shorter than for longer videos in recognizing neutral\nand happiness, but not sadness and anger. These findings offer new insights\ntowards the development of more context-aware emotion recognition and\nempathetic systems.",
          "link": "http://arxiv.org/abs/2004.13274",
          "publishedOn": "2021-07-01T01:59:31.319Z",
          "wordCount": 668,
          "title": "Exploring the contextual factors affecting multimodal emotion recognition in videos. (arXiv:2004.13274v5 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1\">Chris Chafe</a>",
          "description": "This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet \\cite{oord2016wavenet}. This is fully\nprobabilistic, auto-regressive, and causal, i.e. each sample generated depends\nonly on the previously observed samples. Our approach outperforms a widely used\nwavenet architecture by up to 9\\% on a similar dataset for predicting the next\nstep. Using the attention mechanism, we enable the architecture to learn which\naudio samples are important for the prediction of the future sample. We show\nhow causal transformer generative models can be used for raw waveform\nsynthesis. We also show that this performance can be improved by another 2\\% by\nconditioning samples over a wider context. The flexibility of the current model\nto synthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.",
          "link": "http://arxiv.org/abs/2106.16036",
          "publishedOn": "2021-07-01T01:59:31.307Z",
          "wordCount": 620,
          "title": "A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maruyama_M/0/1/0/all/0/1\">Mizuki Maruyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Shuvozit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1\">Katsufumi Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Partha Pratim Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamura_M/0/1/0/all/0/1\">Masakazu Iwamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_M/0/1/0/all/0/1\">Michifumi Yoshioka</a>",
          "description": "In recent years, Word-level Sign Language Recognition (WSLR) research has\ngained popularity in the computer vision community, and thus various approaches\nhave been proposed. Among these approaches, the method using I3D network\nachieves the highest recognition accuracy on large public datasets for WSLR.\nHowever, the method with I3D only utilizes appearance information of the upper\nbody of the signers to recognize sign language words. On the other hand, in\nWSLR, the information of local regions, such as the hand shape and facial\nexpression, and the positional relationship among the body and both hands are\nimportant. Thus in this work, we utilized local region images of both hands and\nface, along with skeletal information to capture local information and the\npositions of both hands relative to the body, respectively. In other words, we\npropose a novel multi-stream WSLR framework, in which a stream with local\nregion images and a stream with skeletal information are introduced by\nextending I3D network to improve the recognition accuracy of WSLR. From the\nexperimental results on WLASL dataset, it is evident that the proposed method\nhas achieved about 15% improvement in the Top-1 accuracy than the existing\nconventional methods.",
          "link": "http://arxiv.org/abs/2106.15989",
          "publishedOn": "2021-07-01T01:59:31.271Z",
          "wordCount": 645,
          "title": "Word-level Sign Language Recognition with Multi-stream Neural Networks Focusing on Local Regions. (arXiv:2106.15989v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Guoli Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Images can convey rich semantics and induce various emotions in viewers.\nRecently, with the rapid advancement of emotional intelligence and the\nexplosive growth of visual data, extensive research efforts have been dedicated\nto affective image content analysis (AICA). In this survey, we will\ncomprehensively review the development of AICA in the recent two decades,\nespecially focusing on the state-of-the-art methods with respect to three main\nchallenges -- the affective gap, perception subjectivity, and label noise and\nabsence. We begin with an introduction to the key emotion representation models\nthat have been widely employed in AICA and description of available datasets\nfor performing evaluation with quantitative comparison of label noise and\ndataset bias. We then summarize and compare the representative approaches on\n(1) emotion feature extraction, including both handcrafted and deep features,\n(2) learning methods on dominant emotion recognition, personalized emotion\nprediction, emotion distribution learning, and learning from noisy data or few\nlabels, and (3) AICA based applications. Finally, we discuss some challenges\nand promising research directions in the future, such as image content and\ncontext understanding, group emotion clustering, and viewer-image interaction.",
          "link": "http://arxiv.org/abs/2106.16125",
          "publishedOn": "2021-07-01T01:59:31.259Z",
          "wordCount": 643,
          "title": "Affective Image Content Analysis: Two Decades Review and New Perspectives. (arXiv:2106.16125v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
          "link": "http://arxiv.org/abs/2106.15561",
          "publishedOn": "2021-06-30T02:00:59.384Z",
          "wordCount": 607,
          "title": "A Survey on Neural Speech Synthesis. (arXiv:2106.15561v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pranay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Divyanshu Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "We introduce SynSE, a novel syntactically guided generative approach for\nZero-Shot Learning (ZSL). Our end-to-end approach learns progressively refined\ngenerative embedding spaces constrained within and across the involved\nmodalities (visual, language). The inter-modal constraints are defined between\naction sequence embedding and embeddings of Parts of Speech (PoS) tagged words\nin the corresponding action description. We deploy SynSE for the task of\nskeleton-based action sequence recognition. Our design choices enable SynSE to\ngeneralize compositionally, i.e., recognize sequences whose action descriptions\ncontain words not encountered during training. We also extend our approach to\nthe more challenging Generalized Zero-Shot Learning (GZSL) problem via a\nconfidence-based gating mechanism. We are the first to present zero-shot\nskeleton action recognition results on the large-scale NTU-60 and NTU-120\nskeleton action datasets with multiple splits. Our results demonstrate SynSE's\nstate of the art performance in both ZSL and GZSL settings compared to strong\nbaselines on the NTU-60 and NTU-120 datasets. The code and pretrained models\nare available at https://github.com/skelemoa/synse-zsl",
          "link": "http://arxiv.org/abs/2101.11530",
          "publishedOn": "2021-06-30T02:00:59.373Z",
          "wordCount": 646,
          "title": "Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action Recognition. (arXiv:2101.11530v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perov_I/0/1/0/all/0/1\">Ivan Perov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Daiheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chervoniy_N/0/1/0/all/0/1\">Nikolay Chervoniy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marangonda_S/0/1/0/all/0/1\">Sugasa Marangonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ume_C/0/1/0/all/0/1\">Chris Um&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dpfks_M/0/1/0/all/0/1\">Mr. Dpfks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facenheim_C/0/1/0/all/0/1\">Carl Shift Facenheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+RP_L/0/1/0/all/0/1\">Luis RP</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Pingyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>",
          "description": "Deepfake defense not only requires the research of detection but also\nrequires the efforts of generation methods. However, current deepfake methods\nsuffer the effects of obscure workflow and poor performance. To solve this\nproblem, we present DeepFaceLab, the current dominant deepfake framework for\nface-swapping. It provides the necessary tools as well as an easy-to-use way to\nconduct high-quality face-swapping. It also offers a flexible and loose\ncoupling structure for people who need to strengthen their pipeline with other\nfeatures without writing complicated boilerplate code. We detail the principles\nthat drive the implementation of DeepFaceLab and introduce its pipeline,\nthrough which every aspect of the pipeline can be modified painlessly by users\nto achieve their customization purpose. It is noteworthy that DeepFaceLab could\nachieve cinema-quality results with high fidelity. We demonstrate the advantage\nof our system by comparing our approach with other face-swapping methods.For\nmore information, please visit:https://github.com/iperov/DeepFaceLab/.",
          "link": "http://arxiv.org/abs/2005.05535",
          "publishedOn": "2021-06-30T02:00:59.142Z",
          "wordCount": 674,
          "title": "DeepFaceLab: Integrated, flexible and extensible face-swapping framework. (arXiv:2005.05535v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1\">Hannaneh Barahouei Pasandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1\">Tamer Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1\">Hadi Amirpour</a>",
          "description": "Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency\nhave promised to increase data throughput by allowing concurrent communication\nbetween one Access Point and multiple users. However, we are still a long way\nfrom enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry\napplications such as video streaming in practical WiFi network settings due to\nheterogeneous channel conditions and devices, unreliable transmissions, and\nlack of useful feedback exchange among the lower and upper layers'\nrequirements. This paper introduces MuViS, a novel dual-phase optimization\nframework that proposes a Quality of Experience (QoE) aware MU-MIMO\noptimization for multi-user video streaming over IEEE 802.11ac. MuViS first\nemploys reinforcement learning to optimize the MU-MIMO user group and mode\nselection for users based on their PHY/MAC layer characteristics. The video\nbitrate is then optimized based on the user's mode (Multi-User (MU) or\nSingle-User (SU)). We present our design and its evaluation on smartphones and\nlaptops using 802.11ac WiFi. Our experimental results in various indoor\nenvironments and configurations show a scalable framework that can support a\nlarge number of users with streaming at high video rates and satisfying QoE\nrequirements.",
          "link": "http://arxiv.org/abs/2106.15262",
          "publishedOn": "2021-06-30T02:00:59.126Z",
          "wordCount": 632,
          "title": "MuViS: Online MU-MIMO Grouping for Multi-User Applications Over Commodity WiFi. (arXiv:2106.15262v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1\">Simone Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1\">Ehsan Nowroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orazi_G/0/1/0/all/0/1\">Gabriele Orazi</a>",
          "description": "The last-generation video conferencing software allows users to utilize a\nvirtual background to conceal their personal environment due to privacy\nconcerns, especially in official meetings with other employers. On the other\nhand, users maybe want to fool people in the meeting by considering the virtual\nbackground to conceal where they are. In this case, developing tools to\nunderstand the virtual background utilize for fooling people in meeting plays\nan important role. Besides, such detectors must prove robust against different\nkinds of attacks since a malicious user can fool the detector by applying a set\nof adversarial editing steps on the video to conceal any revealing footprint.\nIn this paper, we study the feasibility of an efficient tool to detect whether\na videoconferencing user background is real. In particular, we provide the\nfirst tool which computes pixel co-occurrences matrices and uses them to search\nfor inconsistencies among spectral and spatial bands. Our experiments confirm\nthat cross co-occurrences matrices improve the robustness of the detector\nagainst different kinds of attacks. This work's performance is especially\nnoteworthy with regard to color SPAM features. Moreover, the performance\nespecially is significant with regard to robustness versus post-processing,\nlike geometric transformations, filtering, contrast enhancement, and JPEG\ncompression with different quality factors.",
          "link": "http://arxiv.org/abs/2106.15130",
          "publishedOn": "2021-06-30T02:00:59.106Z",
          "wordCount": 669,
          "title": "Do Not Deceive Your Employer with a Virtual Background: A Video Conferencing Manipulation-Detection System. (arXiv:2106.15130v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Samira Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1\">Mojtaba Mahdavi</a>",
          "description": "Content-independent watermarks and block-wise independency can be considered\nas vulnerabilities in semi-fragile watermarking methods. In this paper to\nachieve the objectives of semi-fragile watermarking techniques, a method is\nproposed to not have the mentioned shortcomings. In the proposed method, the\nwatermark is generated by relying on image content and a key. Furthermore, the\nembedding scheme causes the watermarked blocks to become dependent on each\nother, using a key. In the embedding phase, the image is partitioned into\nnon-overlapping blocks. In order to detect and separate the different types of\nattacks more precisely, the proposed method embeds three copies of each\nwatermark bit into LWT coefficients of each 4x4 block. In the authentication\nphase, by voting between the extracted bits the error maps are created; these\nmaps indicate image authenticity and reveal the modified regions. Also, in\norder to automate the authentication, the images are classified into four\ncategories using seven features. Classification accuracy in the experiments is\n97.97 percent. It is noted that our experiments demonstrate that the proposed\nmethod is robust against JPEG compression and is competitive with a\nstate-of-the-art semi-fragile watermarking method, in terms of robustness and\nsemi-fragility.",
          "link": "http://arxiv.org/abs/2106.14150",
          "publishedOn": "2021-06-29T01:55:13.227Z",
          "wordCount": 633,
          "title": "Image content dependent semi-fragile watermarking with localized tamper detection. (arXiv:2106.14150v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1\">Sana Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Saeid Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1\">Raziyeh Zall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1\">Mohammad Reza Kangavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1\">Sara Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>",
          "description": "Multimodal sentiment analysis benefits various applications such as\nhuman-computer interaction and recommendation systems. It aims to infer the\nusers' bipolar ideas using visual, textual, and acoustic signals. Although\nresearchers affirm the association between cognitive cues and emotional\nmanifestations, most of the current multimodal approaches in sentiment analysis\ndisregard user-specific aspects. To tackle this issue, we devise a novel method\nto perform multimodal sentiment prediction using cognitive cues, such as\npersonality. Our framework constructs an adaptive tree by hierarchically\ndividing users and trains the LSTM-based submodels, utilizing an\nattention-based fusion to transfer cognitive-oriented knowledge within the\ntree. Subsequently, the framework consumes the conclusive agglomerative\nknowledge from the adaptive tree to predict final sentiments. We also devise a\ndynamic dropout method to facilitate data sharing between neighboring nodes,\nreducing data sparsity. The empirical results on real-world datasets determine\nthat our proposed model for sentiment prediction can surpass trending rivals.\nMoreover, compared to other ensemble approaches, the proposed transfer-based\nalgorithm can better utilize the latent cognitive cues and foster the\nprediction outcomes. Based on the given extrinsic and intrinsic analysis\nresults, we note that compared to other theoretical-based techniques, the\nproposed hierarchical clustering approach can better group the users within the\nadaptive tree.",
          "link": "http://arxiv.org/abs/2106.14174",
          "publishedOn": "2021-06-29T01:55:13.055Z",
          "wordCount": 664,
          "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. (arXiv:2106.14174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (52.73 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data will be made available.",
          "link": "http://arxiv.org/abs/2106.14118",
          "publishedOn": "2021-06-29T01:55:13.025Z",
          "wordCount": 593,
          "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>",
          "description": "In this paper, we address the text-to-audio grounding issue, namely,\ngrounding the segments of the sound event described by a natural language query\nin the untrimmed audio. This is a newly proposed but challenging audio-language\ntask, since it requires to not only precisely localize all the on- and off-sets\nof the desired segments in the audio, but to perform comprehensive acoustic and\nlinguistic understandings and reason the multimodal interactions between the\naudio and query. To tackle those problems, the existing method treats the query\nholistically as a single unit by a global query representation, which fails to\nhighlight the keywords that contain rich semantics. Besides, this method has\nnot fully exploited interactions between the query and audio. Moreover, since\nthe audio and queries are arbitrary and variable in length, many meaningless\nparts of them are not filtered out in this method, which hinders the grounding\nof the desired segments.\n\nTo this end, we propose a novel Query Graph with Cross-gating Attention\n(QGCA) model, which models the comprehensive relations between the words in\nquery through a novel query graph. Besides, to capture the fine-grained\ninteractions between audio and query, a cross-modal attention module that\nassigns higher weights to the keywords is introduced to generate the\nsnippet-specific query representations. Finally, we also design a cross-gating\nmodule to emphasize the crucial parts as well as weaken the irrelevant ones in\nthe audio and query. We extensively evaluate the proposed QGCA model on the\npublic Audiogrounding dataset with significant improvements over several\nstate-of-the-art methods. Moreover, further ablation study shows the consistent\neffectiveness of different modules in the proposed QGCA model.",
          "link": "http://arxiv.org/abs/2106.14136",
          "publishedOn": "2021-06-29T01:55:12.995Z",
          "wordCount": 707,
          "title": "Query-graph with Cross-gating Attention Model for Text-to-Audio Grounding. (arXiv:2106.14136v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiri Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zekuang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>",
          "description": "Nowadays, most existing blind image quality assessment (BIQA) models 1) are\ndeveloped for synthetically-distorted images and often generalize poorly to\nauthentic ones; 2) heavily rely on human ratings, which are prohibitively\nlabor-expensive to collect. Here, we propose an $opinion$-$free$ BIQA method\nthat learns from synthetically-distorted images and multiple agents to assess\nthe perceptual quality of authentically-distorted ones captured in the wild\nwithout relying on human labels. Specifically, we first assemble a large number\nof image pairs from synthetically-distorted images and use a set of\nfull-reference image quality assessment (FR-IQA) models to assign pseudo-binary\nlabels of each pair indicating which image has higher quality as the\nsupervisory signal. We then train a convolutional neural network (CNN)-based\nBIQA model to rank the perceptual quality, optimized for consistency with the\nbinary labels. Since there exists domain shift between the synthetically- and\nauthentically-distorted images, an unsupervised domain adaptation (UDA) module\nis introduced to alleviate this issue. Extensive experiments demonstrate the\neffectiveness of our proposed $opinion$-$free$ BIQA model, yielding\nstate-of-the-art performance in terms of correlation with human opinion scores,\nas well as gMAD competition. Codes will be made publicly available upon\nacceptance.",
          "link": "http://arxiv.org/abs/2106.14076",
          "publishedOn": "2021-06-29T01:55:12.531Z",
          "wordCount": 633,
          "title": "Learning from Synthetic Data for Opinion-free Blind Image Quality Assessment in the Wild. (arXiv:2106.14076v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zytko_D/0/1/0/all/0/1\">Douglas Zytko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1\">Jacob Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundquist_N/0/1/0/all/0/1\">Nathaniel Lundquist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1\">Medina Taylor</a>",
          "description": "Immersive stories for health are 360-degree videos that intend to alter\nviewer perceptions about behaviors detrimental to health. They have potential\nto inform public health at scale, however, immersive story design is still in\nearly stages and largely devoid of best practices. This paper presents a focus\ngroup study with 147 viewers of an immersive story about binge drinking\nexperienced through VR headsets and mobile phones. The objective of the study\nis to identify aspects of immersive story design that influence attitudes\ntowards the health issue exhibited, and to understand how health information is\nconsumed in immersive stories. Findings emphasize the need for an immersive\nstory to provide reasoning behind character engagement in the focal health\nbehavior, to show the main character clearly engaging in the behavior, and to\nenable viewers to experience escalating symptoms of the behavior before the\npenultimate health consequence. Findings also show how the design of supporting\ncharacters can inadvertently distract viewers and lead them to justify the\ndetrimental behavior being exhibited. The paper concludes with design\nconsiderations for enabling immersive stories to better inform public\nperception of health issues.",
          "link": "http://arxiv.org/abs/2106.13921",
          "publishedOn": "2021-06-29T01:55:12.509Z",
          "wordCount": 652,
          "title": "Immersive Stories for Health Information: Design Considerations from Binge Drinking in VR. (arXiv:2106.13921v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14014",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tandon_P/0/1/0/all/0/1\">Pulkit Tandon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandak_S/0/1/0/all/0/1\">Shubham Chandak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pataranutaporn_P/0/1/0/all/0/1\">Pat Pataranutaporn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yimeng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mapuranga_A/0/1/0/all/0/1\">Anesu M. Mapuranga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maes_P/0/1/0/all/0/1\">Pattie Maes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weissman_T/0/1/0/all/0/1\">Tsachy Weissman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sra_M/0/1/0/all/0/1\">Misha Sra</a>",
          "description": "Video represents the majority of internet traffic today leading to a\ncontinuous technological arms race between generating higher quality content,\ntransmitting larger file sizes and supporting network infrastructure. Adding to\nthis is the recent COVID-19 pandemic fueled surge in the use of video\nconferencing tools. Since videos take up substantial bandwidth (~100 Kbps to\nfew Mbps), improved video compression can have a substantial impact on network\nperformance for live and pre-recorded content, providing broader access to\nmultimedia content worldwide. In this work, we present a novel video\ncompression pipeline, called Txt2Vid, which substantially reduces data\ntransmission rates by compressing webcam videos (\"talking-head videos\") to a\ntext transcript. The text is transmitted and decoded into a realistic\nreconstruction of the original video using recent advances in deep learning\nbased voice cloning and lip syncing models. Our generative pipeline achieves\ntwo to three orders of magnitude reduction in the bitrate as compared to the\nstandard audio-video codecs (encoders-decoders), while maintaining equivalent\nQuality-of-Experience based on a subjective evaluation by users (n=242) in an\nonline study. The code for this work is available at\nhttps://github.com/tpulkit/txt2vid.git.",
          "link": "http://arxiv.org/abs/2106.14014",
          "publishedOn": "2021-06-29T01:55:12.484Z",
          "wordCount": 686,
          "title": "Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text. (arXiv:2106.14014v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianrong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Nan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuewei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>",
          "description": "Cued Speech (CS) is a communication system for deaf people or hearing\nimpaired people, in which a speaker uses it to aid a lipreader in phonetic\nlevel by clarifying potentially ambiguous mouth movements with hand shape and\npositions. Feature extraction of multi-modal CS is a key step in CS\nrecognition. Recent supervised deep learning based methods suffer from noisy CS\ndata annotations especially for hand shape modality. In this work, we first\npropose a self-supervised contrastive learning method to learn the feature\nrepresentation of image without using labels. Secondly, a small amount of\nmanually annotated CS data are used to fine-tune the first module. Thirdly, we\npresent a module, which combines Bi-LSTM and self-attention networks to further\nlearn sequential features with temporal and contextual information. Besides, to\nenlarge the volume and the diversity of the current limited CS datasets, we\nbuild a new British English dataset containing 5 native CS speakers. Evaluation\nresults on both French and British English datasets show that our model\nachieves over 90% accuracy in hand shape recognition. Significant improvements\nof 8.75% (for French) and 10.09% (for British English) are achieved in CS\nphoneme recognition correctness compared with the state-of-the-art.",
          "link": "http://arxiv.org/abs/2106.14016",
          "publishedOn": "2021-06-29T01:55:12.438Z",
          "wordCount": 641,
          "title": "An Attention Self-supervised Contrastive Learning based Three-stage Model for Hand Shape Feature Representation in Cued Speech. (arXiv:2106.14016v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianrong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Ziyue Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuewei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>",
          "description": "Cued Speech (CS) is a visual communication system for the deaf or hearing\nimpaired people. It combines lip movements with hand cues to obtain a complete\nphonetic repertoire. Current deep learning based methods on automatic CS\nrecognition suffer from a common problem, which is the data scarcity. Until\nnow, there are only two public single speaker datasets for French (238\nsentences) and British English (97 sentences). In this work, we propose a\ncross-modal knowledge distillation method with teacher-student structure, which\ntransfers audio speech information to CS to overcome the limited data problem.\nFirstly, we pretrain a teacher model for CS recognition with a large amount of\nopen source audio speech data, and simultaneously pretrain the feature\nextractors for lips and hands using CS data. Then, we distill the knowledge\nfrom teacher model to the student model with frame-level and sequence-level\ndistillation strategies. Importantly, for frame-level, we exploit multi-task\nlearning to weigh losses automatically, to obtain the balance coefficient.\nBesides, we establish a five-speaker British English CS dataset for the first\ntime. The proposed method is evaluated on French and British English CS\ndatasets, showing superior CS recognition performance to the state-of-the-art\n(SOTA) by a large margin.",
          "link": "http://arxiv.org/abs/2106.13686",
          "publishedOn": "2021-06-28T01:57:53.477Z",
          "wordCount": 645,
          "title": "Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition. (arXiv:2106.13686v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1\">Giorgos Kordopatis-Zilos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1\">Christos Tzelepis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>",
          "description": "In this paper, we address the problem of high performance and computationally\nefficient content-based video retrieval in large-scale datasets. Current\nmethods typically propose either: (i) fine-grained approaches employing\nspatio-temporal representations and similarity calculations, achieving high\nperformance at a high computational cost or (ii) coarse-grained approaches\nrepresenting/indexing videos as global vectors, where the spatio-temporal\nstructure is lost, providing low performance but also having low computational\ncost. In this work, we propose a Knowledge Distillation framework, which we\ncall Distill-and-Select (DnS), that starting from a well-performing\nfine-grained Teacher Network learns: a) Student Networks at different retrieval\nperformance and computational efficiency trade-offs and b) a Selection Network\nthat at test time rapidly directs samples to the appropriate student to\nmaintain both high retrieval performance and high computational efficiency. We\ntrain several students with different architectures and arrive at different\ntrade-offs of performance and efficiency, i.e., speed and storage requirements,\nincluding fine-grained students that store index videos using binary\nrepresentations. Importantly, the proposed scheme allows Knowledge Distillation\nin large, unlabelled datasets -- this leads to good students. We evaluate DnS\non five public datasets on three different video retrieval tasks and\ndemonstrate a) that our students achieve state-of-the-art performance in\nseveral cases and b) that our DnS framework provides an excellent trade-off\nbetween retrieval performance, computational speed, and storage space. In\nspecific configurations, our method achieves similar mAP with the teacher but\nis 20 times faster and requires 240 times less storage space. Our collected\ndataset and implementation are publicly available:\nhttps://github.com/mever-team/distill-and-select.",
          "link": "http://arxiv.org/abs/2106.13266",
          "publishedOn": "2021-06-28T01:57:53.203Z",
          "wordCount": 699,
          "title": "DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wanqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lizhong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>",
          "description": "Self-Rating Depression Scale (SDS) questionnaire has frequently been used for\nefficient depression preliminary screening. However, the uncontrollable\nself-administered measure can be easily affected by insouciantly or deceptively\nanswering, and producing the different results with the clinician-administered\nHamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,\nfacial expression (FE) and actions play a vital role in clinician-administered\nevaluation, while FE and action are underexplored for self-administered\nevaluations. In this work, we collect a novel dataset of 200 subjects to\nevidence the validity of self-rating questionnaires with their corresponding\nquestion-wise video recording. To automatically interpret depression from the\nSDS evaluation and the paired video, we propose an end-to-end hierarchical\nframework for the long-term variable-length video, which is also conditioned on\nthe questionnaire results and the answering time. Specifically, we resort to a\nhierarchical model which utilizes a 3D CNN for local temporal pattern\nexploration and a redundancy-aware self-attention (RAS) scheme for\nquestion-wise global feature aggregation. Targeting for the redundant long-term\nFE video processing, our RAS is able to effectively exploit the correlations of\neach video clip within a question set to emphasize the discriminative\ninformation and eliminate the redundancy based on feature pair-wise affinity.\nThen, the question-wise video feature is concatenated with the questionnaire\nscores for final depression detection. Our thorough evaluations also show the\nvalidity of fusing SDS evaluation and its video recording, and the superiority\nof our framework to the conventional state-of-the-art temporal modeling\nmethods.",
          "link": "http://arxiv.org/abs/2106.13393",
          "publishedOn": "2021-06-28T01:57:53.133Z",
          "wordCount": 702,
          "title": "Interpreting Depression From Question-wise Long-term Video Recording of SDS Evaluation. (arXiv:2106.13393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyowon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scriney_M/0/1/0/all/0/1\">Michael Scriney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1\">Alan F. Smeaton</a>",
          "description": "Much of the delivery of University education is now by synchronous or\nasynchronous video. For students, one of the challenges is managing the sheer\nvolume of such video material as video presentations of taught material are\ndifficult to abbreviate and summarise because they do not have highlights which\nstand out. Apart from video bookmarks there are no tools available to determine\nwhich parts of video content should be replayed at revision time or just before\nexaminations. We have developed and deployed a digital library for managing\nvideo learning material which has many dozens of hours of short-form video\ncontent from a range of taught courses for hundreds of students at\nundergraduate level. Through a web browser we allow students to access and play\nthese videos and we log their anonymised playback usage. From these logs we\nscore to each segment of each video based on the amount of playback it receives\nfrom across all students, whether the segment has been re-wound and re-played\nin the same student session, whether the on-screen window is the window in\nfocus on the student's desktop/laptop, and speed of playback. We also\nincorporate negative scoring if a video segment is skipped or fast-forward, and\noverarching all this we include a decay function based on recency of playback,\nso the most recent days of playback contribute more to the video segment\nscores. For each video in the library we present a usage-based graph which\nallows students to see which parts of each video attract the most playback from\ntheir peers, which helps them select material at revision time. Usage of the\nsystem is fully anonymised and GDPR-compliant.",
          "link": "http://arxiv.org/abs/2106.13504",
          "publishedOn": "2021-06-28T01:57:53.067Z",
          "wordCount": 710,
          "title": "Usage-based Summaries of Learning Videos. (arXiv:2106.13504v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samelak_J/0/1/0/all/0/1\">Jaros&#x142;aw Samelak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domanski_M/0/1/0/all/0/1\">Marek Doma&#x144;ski</a>",
          "description": "The paper presents a new approach to multiview video coding using Screen\nContent Coding. It is assumed that for a time instant the frames corresponding\nto all views are packed into a single frame, i.e. the frame-compatible approach\nto multiview coding is applied. For such coding scenario, the paper\ndemonstrates that Screen Content Coding can be efficiently used for multiview\nvideo coding. Two approaches are considered: the first using standard HEVC\nScreen Content Coding, and the second using Advanced Screen Content Coding. The\nlatter is the original proposal of the authors that exploits quarter-pel motion\nvectors and other nonstandard extensions of HEVC Screen Content Coding. The\nexperimental results demonstrate that multiview video coding even using\nstandard HEVC Screen Content Coding is much more efficient than simulcast HEVC\ncoding. The proposed Advanced Screen Content Coding provides virtually the same\ncoding efficiency as MV-HEVC, which is the state-of-the-art multiview video\ncompression technique. The authors suggest that Advanced Screen Content Coding\ncan be efficiently used within the new Versatile Video Coding (VVC) technology.\nNevertheless a reference multiview extension of VVC does not exist yet,\ntherefore, for VVC-based coding, the experimental comparisons are left for\nfuture work.",
          "link": "http://arxiv.org/abs/2106.13574",
          "publishedOn": "2021-06-28T01:57:53.036Z",
          "wordCount": 627,
          "title": "Multiview Video Compression Using Advanced HEVC Screen Content Coding. (arXiv:2106.13574v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cetinkaya_E/0/1/0/all/0/1\">Ekrem Cetinkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1\">Hadi Amirpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanbari_M/0/1/0/all/0/1\">Mohammad Ghanbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timmerer_C/0/1/0/all/0/1\">Christian Timmerer</a>",
          "description": "High-Efficiency Video Coding (HEVC) surpasses its predecessors in encoding\nefficiency by introducing new coding tools at the cost of an increased encoding\ntime-complexity. The Coding Tree Unit (CTU) is the main building block used in\nHEVC. In the HEVC standard, frames are divided into CTUs with the predetermined\nsize of up to 64x64 pixels. Each CTU is then divided recursively into a number\nof equally sized square areas, known as Coding Units (CUs). Although this\ndiversity of frame partitioning increases encoding efficiency, it also causes\nan increase in the time complexity due to the increased number of ways to find\nthe optimal partitioning. To address this complexity, numerous algorithms have\nbeen proposed to eliminate unnecessary searches during partitioning CTUs by\nexploiting the correlation in the video. In this paper, existing CTU depth\ndecision algorithms for HEVC are surveyed. These algorithms are categorized\ninto two groups, namely statistics and machine learning approaches. Statistics\napproaches are further subdivided into neighboring and inherent approaches.\nNeighboring approaches exploit the similarity between adjacent CTUs to limit\nthe depth range of the current CTU, while inherent approaches use only the\navailable information within the current CTU. Machine learning approaches try\nto extract and exploit similarities implicitly. Traditional methods like\nsupport vector machines or random forests use manually selected features, while\nrecently proposed deep learning methods extract features during training.\nFinally, this paper discusses extending these methods to more recent video\ncoding formats such as Versatile Video Coding (VVC) and AOMedia Video 1(AV1).",
          "link": "http://arxiv.org/abs/2104.08328",
          "publishedOn": "2021-06-25T02:00:43.994Z",
          "wordCount": 712,
          "title": "CTU Depth Decision Algorithms for HEVC: A Survey. (arXiv:2104.08328v2 [cs.MM] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.15793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>",
          "description": "To reduce annotation labor associated with object detection, an increasing\nnumber of studies focus on transferring the learned knowledge from a labeled\nsource domain to another unlabeled target domain. However, existing methods\nassume that the labeled data are sampled from a single source domain, which\nignores a more generalized scenario, where labeled data are from multiple\nsource domains. For the more challenging task, we propose a unified Faster\nR-CNN based framework, termed Divide-and-Merge Spindle Network (DMSN), which\ncan simultaneously enhance domain invariance and preserve discriminative power.\nSpecifically, the framework contains multiple source subnets and a pseudo\ntarget subnet. First, we propose a hierarchical feature alignment strategy to\nconduct strong and weak alignments for low- and high-level features,\nrespectively, considering their different effects for object detection. Second,\nwe develop a novel pseudo subnet learning algorithm to approximate optimal\nparameters of pseudo target subset by weighted combination of parameters in\ndifferent source subnets. Finally, a consistency regularization for region\nproposal network is proposed to facilitate each subnet to learn more abstract\ninvariances. Extensive experiments on different adaptation scenarios\ndemonstrate the effectiveness of the proposed model.",
          "link": "http://arxiv.org/abs/2106.15793",
          "publishedOn": "2021-07-01T01:59:34.067Z",
          "wordCount": 623,
          "title": "Multi-Source Domain Adaptation for Object Detection. (arXiv:2106.15793v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15765",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_C/0/1/0/all/0/1\">Chao Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suo_J/0/1/0/all/0/1\">Jinli Suo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>",
          "description": "High resolution images are widely used in our daily life, whereas high-speed\nvideo capture is challenging due to the low frame rate of cameras working at\nthe high resolution mode. Digging deeper, the main bottleneck lies in the low\nthroughput of existing imaging systems. Towards this end, snapshot compressive\nimaging (SCI) was proposed as a promising solution to improve the throughput of\nimaging systems by compressive sampling and computational reconstruction.\nDuring acquisition, multiple high-speed images are encoded and collapsed to a\nsingle measurement. After this, algorithms are employed to retrieve the video\nframes from the coded snapshot. Recently developed Plug-and-Play (PnP)\nalgorithms make it possible for SCI reconstruction in large-scale problems.\nHowever, the lack of high-resolution encoding systems still precludes SCI's\nwide application. In this paper, we build a novel hybrid coded aperture\nsnapshot compressive imaging (HCA-SCI) system by incorporating a dynamic liquid\ncrystal on silicon and a high-resolution lithography mask. We further implement\na PnP reconstruction algorithm with cascaded denoisers for high quality\nreconstruction. Based on the proposed HCA-SCI system and algorithm, we achieve\na 10-mega pixel SCI system to capture high-speed scenes, leading to a high\nthroughput of 4.6G voxels per second. Both simulation and real data experiments\nverify the feasibility and performance of our proposed HCA-SCI scheme.",
          "link": "http://arxiv.org/abs/2106.15765",
          "publishedOn": "2021-07-01T01:59:34.045Z",
          "wordCount": 666,
          "title": "10-mega pixel snapshot compressive imaging with a hybrid coded aperture. (arXiv:2106.15765v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Momin_R/0/1/0/all/0/1\">Rauf Momin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momin_A/0/1/0/all/0/1\">Ali Shan Momin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_K/0/1/0/all/0/1\">Khalid Rasheed</a>",
          "description": "Facial expressions are the most universal forms of body language and\nautomatic facial expression recognition is one of the challenging tasks due to\ndifferent uncertainties. However, it has been an active field of research for\nmany years. Nevertheless, efficiency and performance are yet essential aspects\nfor building robust systems. We proposed two models, EmoXNet which is an\nensemble learning technique for learning convoluted facial representations, and\nEmoXNetLite which is a distillation technique that is useful for transferring\nthe knowledge from our ensemble model to an efficient deep neural network using\nlabel-smoothen soft labels for able to effectively detect expressions in\nreal-time. Both of the techniques performed quite well, where the ensemble\nmodel (EmoXNet) helped to achieve 85.07% test accuracy on FER2013 with FER+\nannotations and 86.25% test accuracy on RAF-DB. Moreover, the distilled model\n(EmoXNetLite) showed 82.07% test accuracy on FER2013 with FER+ annotations and\n81.78% test accuracy on RAF-DB.",
          "link": "http://arxiv.org/abs/2106.16126",
          "publishedOn": "2021-07-01T01:59:33.706Z",
          "wordCount": 603,
          "title": "Recognizing Facial Expressions in the Wild using Multi-Architectural Representations based Ensemble Learning with Distillation. (arXiv:2106.16126v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spurr_A/0/1/0/all/0/1\">Adrian Spurr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_A/0/1/0/all/0/1\">Aneesh Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xucong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>",
          "description": "Acquiring accurate 3D annotated data for hand pose estimation is a\nnotoriously difficult problem. This typically requires complex multi-camera\nsetups and controlled conditions, which in turn creates a domain gap that is\nhard to bridge to fully unconstrained settings. Encouraged by the success of\ncontrastive learning on image classification tasks, we propose a new\nself-supervised method for the structured regression task of 3D hand pose\nestimation. Contrastive learning makes use of unlabeled data for the purpose of\nrepresentation learning via a loss formulation that encourages the learned\nfeature representations to be invariant under any image transformation. For 3D\nhand pose estimation, it too is desirable to have invariance to appearance\ntransformation such as color jitter. However, the task requires equivariance\nunder affine transformations, such as rotation and translation. To address this\nissue, we propose an equivariant contrastive objective and demonstrate its\neffectiveness in the context of 3D hand pose estimation. We experimentally\ninvestigate the impact of invariant and equivariant contrastive objectives and\nshow that learning equivariant features leads to better representations for the\ntask of 3D hand pose estimation. Furthermore, we show that a standard\nResNet-152, trained on additional unlabeled data, attains an improvement of\n$7.6\\%$ in PA-EPE on FreiHAND and thus achieves state-of-the-art performance\nwithout any task specific, specialized architectures.",
          "link": "http://arxiv.org/abs/2106.05953",
          "publishedOn": "2021-07-01T01:59:33.603Z",
          "wordCount": 671,
          "title": "Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. (arXiv:2106.05953v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1\">Zelin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "Self-supervised contrastive learning has demonstrated great potential in\nlearning visual representations. Despite their success on various downstream\ntasks such as image classification and object detection, self-supervised\npre-training for fine-grained scenarios is not fully explored. In this paper,\nwe first point out that current contrastive methods are prone to memorizing\nbackground/foreground texture and therefore have a limitation in localizing the\nforeground object. Analysis suggests that learning to extract discriminative\ntexture information and localization are equally crucial for self-supervised\npre-training under fine-grained scenarios. Based on our findings, we introduce\nCross-view Saliency Alignment (CVSA), a contrastive learning framework that\nfirst crops and swaps saliency regions of images as a novel view generation and\nthen guides the model to localize on the foreground object via a cross-view\nalignment loss. Extensive experiments on four popular fine-grained\nclassification benchmarks show that CVSA significantly improves the learned\nrepresentation.",
          "link": "http://arxiv.org/abs/2106.15788",
          "publishedOn": "2021-07-01T01:59:33.560Z",
          "wordCount": 590,
          "title": "Align Yourself: Self-supervised Pre-training for Fine-grained Recognition via Saliency Alignment. (arXiv:2106.15788v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1\">Dhiraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1\">Kartik Audhkhasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>",
          "description": "Current methods for learning visually grounded language from videos often\nrely on text annotation, such as human generated captions or machine generated\nautomatic speech recognition (ASR) transcripts. In this work, we introduce the\nAudio-Video Language Network (AVLnet), a self-supervised network that learns a\nshared audio-visual embedding space directly from raw video inputs. To\ncircumvent the need for text annotation, we learn audio-visual representations\nfrom randomly segmented video clips and their raw audio waveforms. We train\nAVLnet on HowTo100M, a large corpus of publicly available instructional videos,\nand evaluate on image retrieval and video retrieval tasks, achieving\nstate-of-the-art performance. We perform analysis of AVLnet's learned\nrepresentations, showing our model utilizes speech and natural sounds to learn\naudio-visual concepts. Further, we propose a tri-modal model that jointly\nprocesses raw audio, video, and text captions from videos to learn a\nmulti-modal semantic embedding space useful for text-video retrieval. Our code,\ndata, and trained models will be released at avlnet.csail.mit.edu",
          "link": "http://arxiv.org/abs/2006.09199",
          "publishedOn": "2021-07-01T01:59:33.537Z",
          "wordCount": 675,
          "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos. (arXiv:2006.09199v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishi_S/0/1/0/all/0/1\">Shintaro Nishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadota_T/0/1/0/all/0/1\">Takeaki Kadota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "This paper analyzes a large number of logo images from the LLD-logo dataset,\nby recent deep learning-based techniques, to understand not only design trends\nof logo images and but also the correlation to their owner company. Especially,\nwe focus on three correlations between logo images and their text areas,\nbetween the text areas and the number of followers on Twitter, and between the\nlogo images and the number of followers. Various findings include the weak\npositive correlation between the text area ratio and the number of followers of\nthe company. In addition, deep regression and deep ranking methods can catch\ncorrelations between the logo images and the number of followers.",
          "link": "http://arxiv.org/abs/2104.00327",
          "publishedOn": "2021-07-01T01:59:33.518Z",
          "wordCount": 592,
          "title": "Famous Companies Use More Letters in Logo:A Large-Scale Analysis of Text Area in Logo. (arXiv:2104.00327v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12407",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Junshen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turk_E/0/1/0/all/0/1\">Esra Abaci Turk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grant_P/0/1/0/all/0/1\">P. Ellen Grant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a>",
          "description": "Fetal motion is unpredictable and rapid on the scale of conventional MR scan\ntimes. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and\ndynamics of fetal function, is limited to fast imaging techniques with\ncompromises in image quality and resolution. Super-resolution for dynamic fetal\nMRI is still a challenge, especially when multi-oriented stacks of image slices\nfor oversampling are not available and high temporal resolution for recording\nthe dynamics of the fetus or placenta is desired. Further, fetal motion makes\nit difficult to acquire high-resolution images for supervised learning methods.\nTo address this problem, in this work, we propose STRESS (Spatio-Temporal\nResolution Enhancement with Simulated Scans), a self-supervised\nsuper-resolution framework for dynamic fetal MRI with interleaved slice\nacquisitions. Our proposed method simulates an interleaved slice acquisition\nalong the high-resolution axis on the originally acquired data to generate\npairs of low- and high-resolution images. Then, it trains a super-resolution\nnetwork by exploiting both spatial and temporal correlations in the MR time\nseries, which is used to enhance the resolution of the original data.\nEvaluations on both simulated and in utero data show that our proposed method\noutperforms other self-supervised super-resolution methods and improves image\nquality, which is beneficial to other downstream tasks and evaluations.",
          "link": "http://arxiv.org/abs/2106.12407",
          "publishedOn": "2021-07-01T01:59:33.459Z",
          "wordCount": 673,
          "title": "STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning. (arXiv:2106.12407v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11396",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Bilevel optimization recently has attracted increased interest in machine\nlearning due to its many applications such as hyper-parameter optimization and\npolicy optimization. Although some methods recently have been proposed to solve\nthe bilevel problems, these methods do not consider using adaptive learning\nrates. To fill this gap, in the paper, we propose a class of fast and effective\nadaptive methods for solving bilevel optimization problems that the outer\nproblem is possibly nonconvex and the inner problem is strongly-convex.\nSpecifically, we propose a fast single-loop BiAdam algorithm based on the basic\nmomentum technique, which achieves a sample complexity of\n$\\tilde{O}(\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point. At the\nsame time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by\nusing variance reduced technique, which reaches the best known sample\ncomplexity of $\\tilde{O}(\\epsilon^{-3})$. To further reduce computation in\nestimating derivatives, we propose a fast single-loop stochastic approximated\nBiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still\nachieves a sample complexity of $\\tilde{O}(\\epsilon^{-4})$ without large\nbatches. We further present an accelerated version of saBiAdam algorithm\n(VR-saBiAdam), which also reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$. We apply the unified adaptive matrices to our\nmethods as the SUPER-ADAM \\citep{huang2021super}, which including many types of\nadaptive learning rates. Moreover, our framework can flexibly use the momentum\nand variance reduced techniques. In particular, we provide a useful convergence\nanalysis framework for both the constrained and unconstrained bilevel\noptimization. To the best of our knowledge, we first study the adaptive bilevel\noptimization methods with adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.11396",
          "publishedOn": "2021-07-01T01:59:33.376Z",
          "wordCount": 714,
          "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.07192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Ling Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>",
          "description": "Hash coding has been widely used in approximate nearest neighbor search for\nlarge-scale image retrieval. Given semantic annotations such as class labels\nand pairwise similarities of the training data, hashing methods can learn and\ngenerate effective and compact binary codes. While some newly introduced images\nmay contain undefined semantic labels, which we call unseen images, zeor-shot\nhashing techniques have been studied. However, existing zeor-shot hashing\nmethods focus on the retrieval of single-label images, and cannot handle\nmulti-label images. In this paper, for the first time, a novel transductive\nzero-shot hashing method is proposed for multi-label unseen image retrieval. In\norder to predict the labels of the unseen/target data, a visual-semantic bridge\nis built via instance-concept coherence ranking on the seen/source data. Then,\npairwise similarity loss and focal quantization loss are constructed for\ntraining a hashing model using both the seen/source and unseen/target data.\nExtensive evaluations on three popular multi-label datasets demonstrate that,\nthe proposed hashing method achieves significantly better results than the\ncompeting methods.",
          "link": "http://arxiv.org/abs/1911.07192",
          "publishedOn": "2021-07-01T01:59:33.183Z",
          "wordCount": 648,
          "title": "Transductive Zero-Shot Hashing for Multilabel Image Retrieval. (arXiv:1911.07192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zheheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feixiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1\">Aite Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Ling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>",
          "description": "Home-cage social behaviour analysis of mice is an invaluable tool to assess\ntherapeutic efficacy of neurodegenerative diseases. Despite tremendous efforts\nmade within the research community, single-camera video recordings are mainly\nused for such analysis. Because of the potential to create rich descriptions of\nmouse social behaviors, the use of multi-view video recordings for rodent\nobservations is increasingly receiving much attention. However, identifying\nsocial behaviours from various views is still challenging due to the lack of\ncorrespondence across data sources. To address this problem, we here propose a\nnovel multiview latent-attention and dynamic discriminative model that jointly\nlearns view-specific and view-shared sub-structures, where the former captures\nunique dynamics of each view whilst the latter encodes the interaction between\nthe views. Furthermore, a novel multi-view latent-attention variational\nautoencoder model is introduced in learning the acquired features, enabling us\nto learn discriminative features in each view. Experimental results on the\nstandard CRMI13 and our multi-view Parkinson's Disease Mouse Behaviour (PDMB)\ndatasets demonstrate that our model outperforms the other state of the arts\ntechnologies and effectively deals with the imbalanced data problem.",
          "link": "http://arxiv.org/abs/2011.02451",
          "publishedOn": "2021-07-01T01:59:33.160Z",
          "wordCount": 660,
          "title": "Muti-view Mouse Social Behaviour Recognition with Deep Graphical Model. (arXiv:2011.02451v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martini_M/0/1/0/all/0/1\">Mauro Martini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1\">Vittorio Mazzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaliq_A/0/1/0/all/0/1\">Aleem Khaliq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>",
          "description": "The increasing availability of large-scale remote sensing labeled data has\nprompted researchers to develop increasingly precise and accurate data-driven\nmodels for land cover and crop classification (LC&CC). Moreover, with the\nintroduction of self-attention and introspection mechanisms, deep learning\napproaches have shown promising results in processing long temporal sequences\nin the multi-spectral domain with a contained computational request.\nNevertheless, most practical applications cannot rely on labeled data, and in\nthe field, surveys are a time consuming solution that poses strict limitations\nto the number of collected samples. Moreover, atmospheric conditions and\nspecific geographical region characteristics constitute a relevant domain gap\nthat does not allow direct applicability of a trained model on the available\ndataset to the area of interest. In this paper, we investigate adversarial\ntraining of deep neural networks to bridge the domain discrepancy between\ndistinct geographical zones. In particular, we perform a thorough analysis of\ndomain adaptation applied to challenging multi-spectral, multi-temporal data,\naccurately highlighting the advantages of adapting state-of-the-art\nself-attention based models for LC&CC to different target zones where labeled\ndata are not available. Extensive experimentation demonstrated significant\nperformance and generalization gain in applying domain-adversarial training to\nsource and target regions with marked dissimilarities between the distribution\nof extracted features.",
          "link": "http://arxiv.org/abs/2104.00564",
          "publishedOn": "2021-07-01T01:59:33.154Z",
          "wordCount": 679,
          "title": "Domain-Adversarial Training of Self-Attention Based Networks for Land Cover Classification using Multi-temporal Sentinel-2 Satellite Imagery. (arXiv:2104.00564v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vats_A/0/1/0/all/0/1\">Anuja Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersen_M/0/1/0/all/0/1\">Marius Pedersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_A/0/1/0/all/0/1\">Ahmed Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovde_O/0/1/0/all/0/1\">&#xd8;istein Hovde</a>",
          "description": "The progress in Computer Aided Diagnosis (CADx) of Wireless Capsule Endoscopy\n(WCE) is thwarted by the lack of data. The inadequacy in richly representative\nhealthy and abnormal conditions results in isolated analyses of pathologies,\nthat can not handle realistic multi-pathology scenarios. In this work, we\nexplore how to learn more for free, from limited data through solving a WCE\nmulticentric, multi-pathology classification problem. Learning more implies to\nlearning more than full supervision would allow with the same data. This is\ndone by combining self supervision with full supervision, under multi task\nlearning. Additionally, we draw inspiration from the Human Visual System (HVS)\nin designing self supervision tasks and investigate if seemingly ineffectual\nsignals within the data itself can be exploited to gain performance, if so,\nwhich signals would be better than others. Further, we present our analysis of\nthe high level features as a stepping stone towards more robust multi-pathology\nCADx in WCE.",
          "link": "http://arxiv.org/abs/2106.16162",
          "publishedOn": "2021-07-01T01:59:33.142Z",
          "wordCount": 612,
          "title": "Learning More for Free - A Multi Task Learning Approach for Improved Pathology Classification in Capsule Endoscopy. (arXiv:2106.16162v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tautkute_I/0/1/0/all/0/1\">Ivona Tautkute</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzcinski</a>",
          "description": "This paper addresses the problem of media retrieval using a multimodal query\n(a query which combines visual input with additional semantic information in\nnatural language feedback). We propose a SynthTriplet GAN framework which\nresolves this task by expanding the multimodal query with a synthetically\ngenerated image that captures semantic information from both image and text\ninput. We introduce a novel triplet mining method that uses a synthetic image\nas an anchor to directly optimize for embedding distances of generated and\ntarget images. We demonstrate that apart from the added value of retrieval\nillustration with synthetic image with the focus on customization and user\nfeedback, the proposed method greatly surpasses other multimodal generation\nmethods and achieves state of the art results in the multimodal retrieval task.\nWe also show that in contrast to other retrieval methods, our method provides\nexplainable embeddings.",
          "link": "http://arxiv.org/abs/2102.08871",
          "publishedOn": "2021-07-01T01:59:33.136Z",
          "wordCount": 619,
          "title": "I Want This Product but Different : Multimodal Retrieval with Synthetic Query Expansion. (arXiv:2102.08871v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunsong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongzi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinhong Jiang</a>",
          "description": "Monocular 3D object detection is an important task in autonomous driving. It\ncan be easily intractable where there exists ego-car pose change w.r.t. ground\nplane. This is common due to the slight fluctuation of road smoothness and\nslope. Due to the lack of insight in industrial application, existing methods\non open datasets neglect the camera pose information, which inevitably results\nin the detector being susceptible to camera extrinsic parameters. The\nperturbation of objects is very popular in most autonomous driving cases for\nindustrial products. To this end, we propose a novel method to capture camera\npose to formulate the detector free from extrinsic perturbation. Specifically,\nthe proposed framework predicts camera extrinsic parameters by detecting\nvanishing point and horizon change. A converter is designed to rectify\nperturbative features in the latent space. By doing so, our 3D detector works\nindependent of the extrinsic parameter variations and produces accurate results\nin realistic cases, e.g., potholed and uneven roads, where almost all existing\nmonocular detectors fail to handle. Experiments demonstrate our method yields\nthe best performance compared with the other state-of-the-arts by a large\nmargin on both KITTI 3D and nuScenes datasets.",
          "link": "http://arxiv.org/abs/2106.15796",
          "publishedOn": "2021-07-01T01:59:33.129Z",
          "wordCount": 631,
          "title": "Monocular 3D Object Detection: An Extrinsic Parameter Free Approach. (arXiv:2106.15796v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scalbert_M/0/1/0/all/0/1\">Marin Scalbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couzinie_Devy_F/0/1/0/all/0/1\">Florent Couzini&#xe9;-Devy</a>",
          "description": "Multi-Source Unsupervised Domain Adaptation (multi-source UDA) aims to learn\na model from several labeled source domains while performing well on a\ndifferent target domain where only unlabeled data are available at training\ntime. To align source and target features distributions, several recent works\nuse source and target explicit statistics matching such as features moments or\nclass centroids. Yet, these approaches do not guarantee class conditional\ndistributions alignment across domains. In this work, we propose a new\nframework called Contrastive Multi-Source Domain Adaptation (CMSDA) for\nmulti-source UDA that addresses this limitation. Discriminative features are\nlearned from interpolated source examples via cross entropy minimization and\nfrom target examples via consistency regularization and hard pseudo-labeling.\nSimultaneously, interpolated source examples are leveraged to align source\nclass conditional distributions through an interpolated version of the\nsupervised contrastive loss. This alignment leads to more general and\ntransferable features which further improve the generalization on the target\ndomain. Extensive experiments have been carried out on three standard\nmulti-source UDA datasets where our method reports state-of-the-art results.",
          "link": "http://arxiv.org/abs/2106.16093",
          "publishedOn": "2021-07-01T01:59:33.106Z",
          "wordCount": 609,
          "title": "Multi-Source domain adaptation via supervised contrastive learning and confident consistency regularization. (arXiv:2106.16093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15944",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jingang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_R/0/1/0/all/0/1\">Runmu Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nie_Y/0/1/0/all/0/1\">Yunfeng Nie</a>",
          "description": "Hyperspectral imaging enables versatile applications due to its competence in\ncapturing abundant spatial and spectral information, which are crucial for\nidentifying substances. However, the devices for acquiring hyperspectral images\nare expensive and complicated. Therefore, many alternative spectral imaging\nmethods have been proposed by directly reconstructing the hyperspectral\ninformation from lower-cost, more available RGB images. We present a thorough\ninvestigation of these state-of-the-art spectral reconstruction methods from\nthe widespread RGB images. A systematic study and comparison of more than 25\nmethods has revealed that most of the data-driven deep learning methods are\nsuperior to prior-based methods in terms of reconstruction accuracy and quality\ndespite lower speeds. This comprehensive review can serve as a fruitful\nreference source for peer researchers, thus further inspiring future\ndevelopment directions in related domains.",
          "link": "http://arxiv.org/abs/2106.15944",
          "publishedOn": "2021-07-01T01:59:33.073Z",
          "wordCount": 580,
          "title": "Learnable Reconstruction Methods from RGB Images to Hyperspectral Imaging: A Survey. (arXiv:2106.15944v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shubao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke-Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "Face anti-spoofing approaches based on domain generalization (DG) have drawn\ngrowing attention due to their robustness for unseen scenarios. Previous\nmethods treat each sample from multiple domains indiscriminately during the\ntraining process, and endeavor to extract a common feature space to improve the\ngeneralization. However, due to complex and biased data distribution, directly\ntreating them equally will corrupt the generalization ability. To settle the\nissue, we propose a novel Dual Reweighting Domain Generalization (DRDG)\nframework which iteratively reweights the relative importance between samples\nto further improve the generalization. Concretely, Sample Reweighting Module is\nfirst proposed to identify samples with relatively large domain bias, and\nreduce their impact on the overall optimization. Afterwards, Feature\nReweighting Module is introduced to focus on these samples and extract more\ndomain-irrelevant features via a self-distilling mechanism. Combined with the\ndomain discriminator, the iteration of the two modules promotes the extraction\nof generalized features. Extensive experiments and visualizations are presented\nto demonstrate the effectiveness and interpretability of our method against the\nstate-of-the-art competitors.",
          "link": "http://arxiv.org/abs/2106.16128",
          "publishedOn": "2021-07-01T01:59:33.066Z",
          "wordCount": 620,
          "title": "Dual Reweighting Domain Generalization for Face Presentation Attack Detection. (arXiv:2106.16128v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_W/0/1/0/all/0/1\">William Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_G/0/1/0/all/0/1\">Glen Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1\">Robert Leer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricardo_F/0/1/0/all/0/1\">Frederick Ricardo</a>",
          "description": "Generative Adversarial Networks (GANs) have been extremely successful in\nvarious application domains. Adversarial image synthesis has drawn increasing\nattention and made tremendous progress in recent years because of its wide\nrange of applications in many computer vision and image processing problems.\nAmong the many applications of GAN, image synthesis is the most well-studied\none, and research in this area has already demonstrated the great potential of\nusing GAN in image synthesis. In this paper, we provide a taxonomy of methods\nused in image synthesis, review different models for text-to-image synthesis\nand image-to-image translation, and discuss some evaluation metrics as well as\npossible future research directions in image synthesis with GAN.",
          "link": "http://arxiv.org/abs/2106.16056",
          "publishedOn": "2021-07-01T01:59:33.054Z",
          "wordCount": 556,
          "title": "A Survey on Adversarial Image Synthesis. (arXiv:2106.16056v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15753",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_L/0/1/0/all/0/1\">Liming Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1\">Shuo Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1\">Alain Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salama_P/0/1/0/all/0/1\">Paul Salama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dunn_K/0/1/0/all/0/1\">Kenneth W. Dunn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>",
          "description": "Robust and accurate nuclei centroid detection is important for the\nunderstanding of biological structures in fluorescence microscopy images.\nExisting automated nuclei localization methods face three main challenges: (1)\nMost of object detection methods work only on 2D images and are difficult to\nextend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes\nbut it is computational expensive for large microscopy volumes and they have\ndifficulty distinguishing different instances of objects; (3) Hand annotated\nground truth is limited for 3D microscopy volumes. To address these issues, we\npresent a scalable approach for nuclei centroid detection of 3D microscopy\nvolumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each\nslice of the volume from different directions and 3D agglomerative hierarchical\nclustering (AHC) is used to estimate the 3D centroids of nuclei in a volume.\nThe model was trained with the synthetic microscopy data generated using\nSpatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and\ntested on different types of real 3D microscopy data. Extensive experimental\nresults demonstrate that our proposed method can accurately count and detect\nthe nuclei centroids in a 3D microscopy volume.",
          "link": "http://arxiv.org/abs/2106.15753",
          "publishedOn": "2021-07-01T01:59:33.047Z",
          "wordCount": 657,
          "title": "RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid Detection in Three-Dimensional Fluorescence Microscopy Images. (arXiv:2106.15753v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Diaz_N/0/1/0/all/0/1\">Nuria Rodriguez-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aspandi_D/0/1/0/all/0/1\">Decky Aspandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukno_F/0/1/0/all/0/1\">Federico Sukno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binefa_X/0/1/0/all/0/1\">Xavier Binefa</a>",
          "description": "Lie detection is considered a concern for everyone in their day to day life\ngiven its impact on human interactions. Thus, people normally pay attention to\nboth what their interlocutors are saying and also to their visual appearances,\nincluding faces, to try to find any signs that indicate whether the person is\ntelling the truth or not. While automatic lie detection may help us to\nunderstand this lying characteristics, current systems are still fairly\nlimited, partly due to lack of adequate datasets to evaluate their performance\nin realistic scenarios. In this work, we have collected an annotated dataset of\nfacial images, comprising both 2D and 3D information of several participants\nduring a card game that encourages players to lie. Using our collected dataset,\nWe evaluated several types of machine learning-based lie detectors in terms of\ntheir generalization, person-specific and cross-domain experiments. Our results\nshow that models based on deep learning achieve the best accuracy, reaching up\nto 57\\% for the generalization task and 63\\% when dealing with a single\nparticipant. Finally, we also highlight the limitation of the deep learning\nbased lie detector when dealing with cross-domain lie detection tasks.",
          "link": "http://arxiv.org/abs/2104.12345",
          "publishedOn": "2021-07-01T01:59:33.040Z",
          "wordCount": 661,
          "title": "Machine Learning-based Lie Detector applied to a Novel Annotated Game Dataset. (arXiv:2104.12345v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1\">Thomas Kollar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskey_M/0/1/0/all/0/1\">Michael Laskey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_K/0/1/0/all/0/1\">Kevin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1\">Brijen Thananjeyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjersland_M/0/1/0/all/0/1\">Mark Tjersland</a>",
          "description": "Robot manipulation of unknown objects in unstructured environments is a\nchallenging problem due to the variety of shapes, materials, arrangements and\nlighting conditions. Even with large-scale real-world data collection, robust\nperception and manipulation of transparent and reflective objects across\nvarious lighting conditions remain challenging. To address these challenges we\npropose an approach to performing sim-to-real transfer of robotic perception.\nThe underlying model, SimNet, is trained as a single multi-headed neural\nnetwork using simulated stereo data as input and simulated object segmentation\nmasks, 3D oriented bounding boxes (OBBs), object keypoints, and disparity as\noutput. A key component of SimNet is the incorporation of a learned stereo\nsub-network that predicts disparity. SimNet is evaluated on 2D car detection,\nunknown object detection, and deformable object keypoint detection and\nsignificantly outperforms a baseline that uses a structured light RGB-D sensor.\nBy inferring grasp positions using the OBB and keypoint predictions, SimNet can\nbe used to perform end-to-end manipulation of unknown objects in both easy and\nhard scenarios using our fleet of Toyota HSR robots in four home environments.\nIn unknown object grasping experiments, the predictions from the baseline RGB-D\nnetwork and SimNet enable successful grasps of most of the easy objects.\nHowever, the RGB-D baseline only grasps 35% of the hard (e.g., transparent)\nobjects, while SimNet grasps 95%, suggesting that SimNet can enable robust\nmanipulation of unknown objects, including transparent objects, in unknown\nenvironments.",
          "link": "http://arxiv.org/abs/2106.16118",
          "publishedOn": "2021-07-01T01:59:32.988Z",
          "wordCount": 683,
          "title": "SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo. (arXiv:2106.16118v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yiping Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Transformer models have achieved great progress on computer vision tasks\nrecently. The rapid development of vision transformers is mainly contributed by\ntheir high representation ability for extracting informative features from\ninput images. However, the mainstream transformer models are designed with deep\narchitectures, and the feature diversity will be continuously reduced as the\ndepth increases, i.e., feature collapse. In this paper, we theoretically\nanalyze the feature collapse phenomenon and study the relationship between\nshortcuts and feature diversity in these transformer models. Then, we present\nan augmented shortcut scheme, which inserts additional paths with learnable\nparameters in parallel on the original shortcuts. To save the computational\ncosts, we further explore an efficient approach that uses the block-circulant\nprojection to implement augmented shortcuts. Extensive experiments conducted on\nbenchmark datasets demonstrate the effectiveness of the proposed method, which\nbrings about 1% accuracy increase of the state-of-the-art visual transformers\nwithout obviously increasing their parameters and FLOPs.",
          "link": "http://arxiv.org/abs/2106.15941",
          "publishedOn": "2021-07-01T01:59:32.909Z",
          "wordCount": 591,
          "title": "Augmented Shortcuts for Vision Transformers. (arXiv:2106.15941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sansone_E/0/1/0/all/0/1\">Emanuele Sansone</a>",
          "description": "This work considers the problem of learning structured representations from\nraw images using self-supervised learning. We propose a principled framework\nbased on a mutual information objective, which integrates self-supervised and\nstructure learning. Furthermore, we devise a post-hoc procedure to interpret\nthe meaning of the learnt representations. Preliminary experiments on CIFAR-10\nshow that the proposed framework achieves higher generalization performance in\ndownstream classification tasks and provides more interpretable representations\ncompared to the ones learnt through traditional self-supervised learning.",
          "link": "http://arxiv.org/abs/2106.16060",
          "publishedOn": "2021-07-01T01:59:32.900Z",
          "wordCount": 503,
          "title": "Leveraging Hidden Structure in Self-Supervised Learning. (arXiv:2106.16060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaofu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaojie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuebin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>",
          "description": "Long-range context information is crucial for the semantic segmentation of\nHigh-Resolution (HR) Remote Sensing Images (RSIs). The image cropping\noperations, commonly used for training neural networks, limit the perception of\nlong-range context information in large RSIs. To break this limitation, we\npropose a Wider-Context Network (WiCNet) for the semantic segmentation of HR\nRSIs. In the WiCNet, apart from a conventional feature extraction network to\naggregate the local information, an extra context branch is designed to\nexplicitly model the context information in a larger image area. The\ninformation between the two branches is communicated through a Context\nTransformer, which is a novel design derived from the Vision Transformer to\nmodel the long-range context correlations. Ablation studies and comparative\nexperiments conducted on several benchmark datasets prove the effectiveness of\nthe proposed method. Additionally, we present a new Beijing Land-Use (BLU)\ndataset. This is a large-scale HR satellite dataset provided with high-quality\nand fine-grained reference labels, which we hope will boost future studies in\nthis field.",
          "link": "http://arxiv.org/abs/2106.15754",
          "publishedOn": "2021-07-01T01:59:32.830Z",
          "wordCount": 621,
          "title": "Looking Outside the Window: Wider-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2106.15754v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maruyama_M/0/1/0/all/0/1\">Mizuki Maruyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Shuvozit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1\">Katsufumi Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Partha Pratim Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamura_M/0/1/0/all/0/1\">Masakazu Iwamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_M/0/1/0/all/0/1\">Michifumi Yoshioka</a>",
          "description": "In recent years, Word-level Sign Language Recognition (WSLR) research has\ngained popularity in the computer vision community, and thus various approaches\nhave been proposed. Among these approaches, the method using I3D network\nachieves the highest recognition accuracy on large public datasets for WSLR.\nHowever, the method with I3D only utilizes appearance information of the upper\nbody of the signers to recognize sign language words. On the other hand, in\nWSLR, the information of local regions, such as the hand shape and facial\nexpression, and the positional relationship among the body and both hands are\nimportant. Thus in this work, we utilized local region images of both hands and\nface, along with skeletal information to capture local information and the\npositions of both hands relative to the body, respectively. In other words, we\npropose a novel multi-stream WSLR framework, in which a stream with local\nregion images and a stream with skeletal information are introduced by\nextending I3D network to improve the recognition accuracy of WSLR. From the\nexperimental results on WLASL dataset, it is evident that the proposed method\nhas achieved about 15% improvement in the Top-1 accuracy than the existing\nconventional methods.",
          "link": "http://arxiv.org/abs/2106.15989",
          "publishedOn": "2021-07-01T01:59:32.811Z",
          "wordCount": 645,
          "title": "Word-level Sign Language Recognition with Multi-stream Neural Networks Focusing on Local Regions. (arXiv:2106.15989v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Mingyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Honghui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shumin Han</a>",
          "description": "Transformers with remarkable global representation capacities achieve\ncompetitive results for visual tasks, but fail to consider high-level local\npattern information in input images. In this paper, we present a generic\nDual-stream Network (DS-Net) to fully explore the representation capacity of\nlocal and global pattern features for image classification. Our DS-Net can\nsimultaneously calculate fine-grained and integrated features and efficiently\nfuse them. Specifically, we propose an Intra-scale Propagation module to\nprocess two different resolutions in each block and an Inter-Scale Alignment\nmodule to perform information interaction across features at dual scales.\nBesides, we also design a Dual-stream FPN (DS-FPN) to further enhance\ncontextual information for downstream dense predictions. Without bells and\nwhistles, the propsed DS-Net outperforms Deit-Small by 2.4% in terms of top-1\naccuracy on ImageNet-1k and achieves state-of-the-art performance over other\nVision Transformers and ResNets. For object detection and instance\nsegmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5 %\nin terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art\nscheme, which significantly demonstrates its potential to be a general backbone\nin vision tasks. The code will be released soon.",
          "link": "http://arxiv.org/abs/2105.14734",
          "publishedOn": "2021-07-01T01:59:32.694Z",
          "wordCount": 639,
          "title": "Dual-stream Network for Visual Recognition. (arXiv:2105.14734v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>",
          "description": "Transferability estimation is an essential problem in transfer learning to\npredict how good the performance is when transferring a source model (or source\ntask) to a target task. Recent analytical transferability metrics have been\nwidely used for source model selection and multi-task learning. A major\nchallenge is how to make transfereability estimation robust under the\ncross-domain cross-task settings. The recently proposed OTCE score solves this\nproblem by considering both domain and task differences, with the help of\ntransfer experiences on auxiliary tasks, which causes an efficiency overhead.\nIn this work, we propose a practical transferability metric called JC-NCE score\nthat dramatically improves the robustness of the task difference estimation in\nOTCE, thus removing the need for auxiliary tasks. Specifically, we build the\njoint correspondences between source and target data via solving an optimal\ntransport problem with a ground cost considering both the sample distance and\nlabel distance, and then compute the transferability score as the negative\nconditional entropy of the matched labels. Extensive validations under the\nintra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE\nscore outperforms the auxiliary-task free version of OTCE for 7% and 12%,\nrespectively, and is also more robust than other existing transferability\nmetrics on average.",
          "link": "http://arxiv.org/abs/2106.10479",
          "publishedOn": "2021-07-01T01:59:32.668Z",
          "wordCount": 660,
          "title": "Practical Transferability Estimation for Image Classification Tasks. (arXiv:2106.10479v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1\">Monty Santarossa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1\">Simon-Martin Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelenka_C/0/1/0/all/0/1\">Claudius Zelenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiko_R/0/1/0/all/0/1\">Rainer Kiko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stracke_J/0/1/0/all/0/1\">Jenny Stracke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkmann_N/0/1/0/all/0/1\">Nina Volkmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>",
          "description": "Semi-Supervised Learning (SSL) can decrease the amount of required labeled\nimage data and thus the cost for deep learning. Most SSL methods only consider\na clear distinction between classes but in many real-world datasets, this clear\ndistinction is not given due to intra- or interobserver variability. This\nvariability can lead to different annotations per image. Thus many images have\nambiguous annotations and their label needs to be considered \"fuzzy\". This\nfuzziness of labels must be addressed as it will limit the performance of\nSemi-Supervised Learning (SSL) and deep learning in general. We propose\nSemi-Supervised Classification & Clustering (S2C2) which can extend many deep\nSSL algorithms. S2C2 can estimate the fuzziness of a label and applies SSL as a\nclassification to certainly labeled data while creating distinct clusters for\nimages with similar but fuzzy labels. We show that S2C2 results in median 7.4%\nbetter F1-score for classifications and 5.4% lower inner distance of clusters\nacross multiple SSL algorithms and datasets while being more interpretable due\nto the fuzziness estimation of our method. Overall, a combination of\nSemi-Supervised Learning with our method S2C2 leads to better handling of the\nfuzziness of labels and thus real-world datasets.",
          "link": "http://arxiv.org/abs/2106.16209",
          "publishedOn": "2021-07-01T01:59:32.652Z",
          "wordCount": 644,
          "title": "S2C2 - An orthogonal method for Semi-Supervised Learning on fuzzy labels. (arXiv:2106.16209v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Abdurrahim Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_R/0/1/0/all/0/1\">Rahmetullah Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goktay_F/0/1/0/all/0/1\">Fatih Goktay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gencoglan_G/0/1/0/all/0/1\">Gulsum Gencoglan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demircali_A/0/1/0/all/0/1\">Ali Anil Demircali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilsizoglu_B/0/1/0/all/0/1\">Berk Dilsizoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uvet_H/0/1/0/all/0/1\">Huseyin Uvet</a>",
          "description": "Clinical dermatology, still relies heavily on manual introspection of fungi\nwithin a Potassium Hydroxide (KOH) solution using a brightfield microscope.\nHowever, this method takes a long time, is based on the experience of the\nclinician, and has a low accuracy. With the increase of neural network\napplications in the field of clinical microscopy it is now possible to automate\nsuch manual processes increasing both efficiency and accuracy. This study\npresents a deep neural network structure that enables the rapid solutions for\nthese problems and can perform automatic fungi detection in grayscale images\nwithout colorants. Microscopic images of 81 fungi and 235 ceratine were\ncollected. Then, smaller patches were extracted containing 2062 fungi and 2142\nceratine. In order to detect fungus and ceratine, two models were created one\nof which was a custom neural network and the other was based on the VGG16\narchitecture. The developed custom model had 99.84% accuracy, and an area under\nthe curve (AUC) value of 1.00, while the VGG16 model had 98.89% accuracy and an\nAUC value of 0.99. However, average accuracy and AUC value of clinicians is\n72.8% and 0.87 respectively. This deep learning model allows the development of\nan automated system that can detect fungi within microscopic images.",
          "link": "http://arxiv.org/abs/2106.16139",
          "publishedOn": "2021-07-01T01:59:32.643Z",
          "wordCount": 649,
          "title": "Automated Onychomycosis Detection Using Deep Neural Networks. (arXiv:2106.16139v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuechen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "Temporal language grounding (TLG) is a fundamental and challenging problem\nfor vision and language understanding. Existing methods mainly focus on fully\nsupervised setting with temporal boundary labels for training, which, however,\nsuffers expensive cost of annotation. In this work, we are dedicated to weakly\nsupervised TLG, where multiple description sentences are given to an untrimmed\nvideo without temporal boundary labels. In this task, it is critical to learn a\nstrong cross-modal semantic alignment between sentence semantics and visual\ncontent. To this end, we introduce a novel weakly supervised temporal adjacent\nnetwork (WSTAN) for temporal language grounding. Specifically, WSTAN learns\ncross-modal semantic alignment by exploiting temporal adjacent network in a\nmultiple instance learning (MIL) paradigm, with a whole description paragraph\nas input. Moreover, we integrate a complementary branch into the framework,\nwhich explicitly refines the predictions with pseudo supervision from the MIL\nstage. An additional self-discriminating loss is devised on both the MIL branch\nand the complementary branch, aiming to enhance semantic discrimination by\nself-supervising. Extensive experiments are conducted on three widely used\nbenchmark datasets, \\emph{i.e.}, ActivityNet-Captions, Charades-STA, and\nDiDeMo, and the results demonstrate the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2106.16136",
          "publishedOn": "2021-07-01T01:59:32.625Z",
          "wordCount": 632,
          "title": "Weakly Supervised Temporal Adjacent Network for Language Grounding. (arXiv:2106.16136v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanbin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shihao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zibo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "With the rapid development of social media, tremendous videos with new\nclasses are generated daily, which raise an urgent demand for video\nclassification methods that can continuously update new classes while\nmaintaining the knowledge of old videos with limited storage and computing\nresources. In this paper, we summarize this task as \\textit{Class-Incremental\nVideo Classification (CIVC)} and propose a novel framework to address it. As a\nsubarea of incremental learning tasks, the challenge of \\textit{catastrophic\nforgetting} is unavoidable in CIVC. To better alleviate it, we utilize some\ncharacteristics of videos. First, we decompose the spatio-temporal knowledge\nbefore distillation rather than treating it as a whole in the knowledge\ntransfer process; trajectory is also used to refine the decomposition. Second,\nwe propose a dual granularity exemplar selection method to select and store\nrepresentative video instances of old classes and key-frames inside videos\nunder a tight storage budget. We benchmark our method and previous SOTA\nclass-incremental learning methods on Something-Something V2 and Kinetics\ndatasets, and our method outperforms previous methods significantly.",
          "link": "http://arxiv.org/abs/2106.15827",
          "publishedOn": "2021-07-01T01:59:32.619Z",
          "wordCount": 602,
          "title": "When Video Classification Meets Incremental Classes. (arXiv:2106.15827v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xingxing Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrill_N/0/1/0/all/0/1\">Nathaniel Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoquan Huang</a>",
          "description": "In this work, we present a lightweight, tightly-coupled deep depth network\nand visual-inertial odometry (VIO) system, which can provide accurate state\nestimates and dense depth maps of the immediate surroundings. Leveraging the\nproposed lightweight Conditional Variational Autoencoder (CVAE) for depth\ninference and encoding, we provide the network with previously marginalized\nsparse features from VIO to increase the accuracy of initial depth prediction\nand generalization capability. The compact encoded depth maps are then updated\njointly with navigation states in a sliding window estimator in order to\nprovide the dense local scene geometry. We additionally propose a novel method\nto obtain the CVAE's Jacobian which is shown to be more than an order of\nmagnitude faster than previous works, and we additionally leverage\nFirst-Estimate Jacobian (FEJ) to avoid recalculation. As opposed to previous\nworks relying on completely dense residuals, we propose to only provide sparse\nmeasurements to update the depth code and show through careful experimentation\nthat our choice of sparse measurements and FEJs can still significantly improve\nthe estimated depth maps. Our full system also exhibits state-of-the-art pose\nestimation accuracy, and we show that it can run in real-time with\nsingle-thread execution while utilizing GPU acceleration only for the network\nand code Jacobian.",
          "link": "http://arxiv.org/abs/2012.10133",
          "publishedOn": "2021-07-01T01:59:32.614Z",
          "wordCount": 675,
          "title": "CodeVIO: Visual-Inertial Odometry with Learned Optimizable Dense Depth. (arXiv:2012.10133v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pintea_S/0/1/0/all/0/1\">Silvia L.Pintea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1\">Nergis Tomen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_S/0/1/0/all/0/1\">Stanley F. Goes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1\">Marco Loog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>",
          "description": "Resolution in deep convolutional neural networks (CNNs) is typically bounded\nby the receptive field size through filter sizes, and subsampling layers or\nstrided convolutions on feature maps. The optimal resolution may vary\nsignificantly depending on the dataset. Modern CNNs hard-code their resolution\nhyper-parameters in the network architecture which makes tuning such\nhyper-parameters cumbersome. We propose to do away with hard-coded resolution\nhyper-parameters and aim to learn the appropriate resolution from data. We use\nscale-space theory to obtain a self-similar parametrization of filters and make\nuse of the N-Jet: a truncated Taylor series to approximate a filter by a\nlearned combination of Gaussian derivative filters. The parameter sigma of the\nGaussian basis controls both the amount of detail the filter encodes and the\nspatial extent of the filter. Since sigma is a continuous parameter, we can\noptimize it with respect to the loss. The proposed N-Jet layer achieves\ncomparable performance when used in state-of-the art architectures, while\nlearning the correct resolution in each layer automatically. We evaluate our\nN-Jet layer on both classification and segmentation, and we show that learning\nsigma is especially beneficial for inputs at multiple sizes.",
          "link": "http://arxiv.org/abs/2106.03412",
          "publishedOn": "2021-07-01T01:59:32.606Z",
          "wordCount": 647,
          "title": "Resolution learning in deep convolutional networks using scale-space theory. (arXiv:2106.03412v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhihang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Ye Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Imari Sato</a>",
          "description": "Real-time video deblurring still remains a challenging task due to the\ncomplexity of spatially and temporally varying blur itself and the requirement\nof low computational cost. To improve the network efficiency, we adopt residual\ndense blocks into RNN cells, so as to efficiently extract the spatial features\nof the current frame. Furthermore, a global spatio-temporal attention module is\nproposed to fuse the effective hierarchical features from past and future\nframes to help better deblur the current frame. Another issue needs to be\naddressed urgently is the lack of a real-world benchmark dataset. Thus, we\ncontribute a novel dataset (BSD) to the community, by collecting paired\nblurry/sharp video clips using a co-axis beam splitter acquisition system.\nExperimental results show that the proposed method (ESTRNN) can achieve better\ndeblurring performance both quantitatively and qualitatively with less\ncomputational cost against state-of-the-art video deblurring methods. In\naddition, cross-validation experiments between datasets illustrate the high\ngenerality of BSD over the synthetic datasets. The code and dataset are\nreleased at https://github.com/zzh-tech/ESTRNN.",
          "link": "http://arxiv.org/abs/2106.16028",
          "publishedOn": "2021-07-01T01:59:32.594Z",
          "wordCount": 615,
          "title": "Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring. (arXiv:2106.16028v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stoian_M/0/1/0/all/0/1\">Mihaela C&#x103;t&#x103;lina Stoian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallari_T/0/1/0/all/0/1\">Tommaso Cavallari</a>",
          "description": "Many man-made objects are characterised by a shape that is symmetric along\none or more planar directions. Estimating the location and orientation of such\nsymmetry planes can aid many tasks such as estimating the overall orientation\nof an object of interest or performing shape completion, where a partial scan\nof an object is reflected across the estimated symmetry plane in order to\nobtain a more detailed shape. Many methods processing 3D data rely on expensive\n3D convolutions. In this paper we present an alternative novel encoding that\ninstead slices the data along the height dimension and passes it sequentially\nto a 2D convolutional recurrent regression scheme. The method also comprises a\ndifferentiable least squares step, allowing for end-to-end accurate and fast\nprocessing of both full and partial scans of symmetric objects. We use this\napproach to efficiently handle 3D inputs to design a method to estimate planar\nreflective symmetries. We show that our approach has an accuracy comparable to\nstate-of-the-art techniques on the task of planar reflective symmetry\nestimation on full synthetic objects. Additionally, we show that it can be\ndeployed on partial scans of objects in a real-world pipeline to improve the\noutputs of a 3D object detector.",
          "link": "http://arxiv.org/abs/2106.16129",
          "publishedOn": "2021-07-01T01:59:32.570Z",
          "wordCount": 644,
          "title": "Recurrently Estimating Reflective Symmetry Planes from Partial Pointclouds. (arXiv:2106.16129v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1\">Cory Braker Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mjolsness_E/0/1/0/all/0/1\">Eric Mjolsness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyen_D/0/1/0/all/0/1\">Diane Oyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodera_C/0/1/0/all/0/1\">Chie Kodera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchez_D/0/1/0/all/0/1\">David Bouchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uyttewaal_M/0/1/0/all/0/1\">Magalie Uyttewaal</a>",
          "description": "We present a method for learning \"spectrally descriptive\" edge weights for\ngraphs. We generalize a previously known distance measure on graphs (Graph\nDiffusion Distance), thereby allowing it to be tuned to minimize an arbitrary\nloss function. Because all steps involved in calculating this modified GDD are\ndifferentiable, we demonstrate that it is possible for a small neural network\nmodel to learn edge weights which minimize loss. GDD alone does not effectively\ndiscriminate between graphs constructed from shoot apical meristem images of\nwild-type vs. mutant \\emph{Arabidopsis thaliana} specimens. However, training\nedge weights and kernel parameters with contrastive loss produces a learned\ndistance metric with large margins between these graph categories. We\ndemonstrate this by showing improved performance of a simple\nk-nearest-neighbors classifier on the learned distance matrix. We also\ndemonstrate a further application of this method to biological image analysis:\nonce trained, we use our model to compute the distance between the biological\ngraphs and a set of graphs output by a cell division simulator. This allows us\nto identify simulation parameter regimes which are similar to each class of\ngraph in our original dataset.",
          "link": "http://arxiv.org/abs/2106.15716",
          "publishedOn": "2021-07-01T01:59:32.560Z",
          "wordCount": 638,
          "title": "Diff2Dist: Learning Spectrally Distinct Edge Functions, with Applications to Cell Morphology Analysis. (arXiv:2106.15716v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_T/0/1/0/all/0/1\">Tiago de C. G. Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_T/0/1/0/all/0/1\">Teofilo E. de Campos</a>",
          "description": "In the world where big data reigns and there is plenty of hardware prepared\nto gather a huge amount of non structured data, data acquisition is no longer a\nproblem. Surveillance cameras are ubiquitous and they capture huge numbers of\npeople walking across different scenes. However, extracting value from this\ndata is challenging, specially for tasks that involve human images, such as\nface recognition and person re-identification. Annotation of this kind of data\nis a challenging and expensive task. In this work we propose a domain\nadaptation workflow to allow CNNs that were trained in one domain to be applied\nto another domain without the need for new annotation of the target data. Our\nmethod uses AlignedReID++ as the baseline, trained using a Triplet loss with\nbatch hard. Domain adaptation is done by using pseudo-labels generated using an\nunsupervised learning strategy. Our results show that domain adaptation\ntechniques really improve the performance of the CNN when applied in the target\ndomain.",
          "link": "http://arxiv.org/abs/2106.15693",
          "publishedOn": "2021-07-01T01:59:32.530Z",
          "wordCount": 633,
          "title": "Domain adaptation for person re-identification on new unlabeled data using AlignedReID++. (arXiv:2106.15693v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1\">Stefan Zernetsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trupp_O/0/1/0/all/0/1\">Oliver Trupp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1\">Viktor Kress</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1\">Konrad Doll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1\">Bernhard Sick</a>",
          "description": "This article presents a novel approach to incorporate visual cues from\nvideo-data from a wide-angle stereo camera system mounted at an urban\nintersection into the forecast of cyclist trajectories. We extract features\nfrom image and optical flow (OF) sequences using 3D convolutional neural\nnetworks (3D-ConvNet) and combine them with features extracted from the\ncyclist's past trajectory to forecast future cyclist positions. By the use of\nadditional information, we are able to improve positional accuracy by about 7.5\n% for our test dataset and by up to 22 % for specific motion types compared to\na method solely based on past trajectories. Furthermore, we compare the use of\nimage sequences to the use of OF sequences as additional information, showing\nthat OF alone leads to significant improvements in positional accuracy. By\ntraining and testing our methods using a real-world dataset recorded at a\nheavily frequented public intersection and evaluating the methods' runtimes, we\ndemonstrate the applicability in real traffic scenarios. Our code and parts of\nour dataset are made publicly available.",
          "link": "http://arxiv.org/abs/2106.15991",
          "publishedOn": "2021-07-01T01:59:32.512Z",
          "wordCount": 613,
          "title": "Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information. (arXiv:2106.15991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Georgakis_G/0/1/0/all/0/1\">Georgios Georgakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_B/0/1/0/all/0/1\">Bernadette Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1\">Karl Schmeckpeper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siddharth Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>",
          "description": "We consider the problem of object goal navigation in unseen environments. In\nour view, solving this problem requires learning of contextual semantic priors,\na challenging endeavour given the spatial and semantic variability of indoor\nenvironments. Current methods learn to implicitly encode these priors through\ngoal-oriented navigation policy functions operating on spatial representations\nthat are limited to the agent's observable areas. In this work, we propose a\nnovel framework that actively learns to generate semantic maps outside the\nfield of view of the agent and leverages the uncertainty over the semantic\nclasses in the unobserved areas to decide on long term goals. We demonstrate\nthat through this spatial prediction strategy, we are able to learn semantic\npriors in scenes that can be leveraged in unknown environments. Additionally,\nwe show how different objectives can be defined by balancing exploration with\nexploitation during searching for semantic targets. Our method is validated in\nthe visually realistic environments offered by the Matterport3D dataset and\nshow state of the art results on the object goal navigation task.",
          "link": "http://arxiv.org/abs/2106.15648",
          "publishedOn": "2021-07-01T01:59:32.462Z",
          "wordCount": 612,
          "title": "Learning to Map for Active Semantic Goal Navigation. (arXiv:2106.15648v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08208",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Adaptive gradient methods have shown excellent performance for solving many\nmachine learning problems. Although multiple adaptive methods were recently\nstudied, they mainly focus on either empirical or theoretical aspects and also\nonly work for specific problems by using specific adaptive learning rates. It\nis desired to design a universal framework for practical algorithms of adaptive\ngradients with theoretical guarantee to solve general problems. To fill this\ngap, we propose a faster and universal framework of adaptive gradients (i.e.,\nSUPER-ADAM) by introducing a universal adaptive matrix that includes most\nexisting adaptive gradient forms. Moreover, our framework can flexibly\nintegrates the momentum and variance reduced techniques. In particular, our\nnovel framework provides the convergence analysis support for adaptive gradient\nmethods under the nonconvex setting. In theoretical analysis, we prove that our\nnew algorithm can achieve the best known complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms.",
          "link": "http://arxiv.org/abs/2106.08208",
          "publishedOn": "2021-07-01T01:59:32.418Z",
          "wordCount": 648,
          "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1\">Kerem Turgutlu</a>",
          "description": "Existing computer vision research in categorization struggles with\nfine-grained attributes recognition due to the inherently high intra-class\nvariances and low inter-class variances. SOTA methods tackle this challenge by\nlocating the most informative image regions and rely on them to classify the\ncomplete image. The most recent work, Vision Transformer (ViT), shows its\nstrong performance in both traditional and fine-grained classification tasks.\nIn this work, we propose a multi-stage ViT framework for fine-grained image\nclassification tasks, which localizes the informative image regions without\nrequiring architectural changes using the inherent multi-head self-attention\nmechanism. We also introduce attention-guided augmentations for improving the\nmodel's capabilities. We demonstrate the value of our approach by experimenting\nwith four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,\nStanford Dogs, and FGVC7 Plant Pathology. We also prove our model's\ninterpretability via qualitative results.",
          "link": "http://arxiv.org/abs/2106.10587",
          "publishedOn": "2021-07-01T01:59:32.401Z",
          "wordCount": 621,
          "title": "Exploring Vision Transformers for Fine-grained Classification. (arXiv:2106.10587v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Recently, different researchers have found that the gallery composition of a\nface database can induce performance differentials to facial identification\nsystems in which a probe image is compared against up to all stored reference\nimages to reach a biometric decision. This negative effect is referred to as\n\"watchlist imbalance effect\". In this work, we present a method to\ntheoretically estimate said effect for a biometric identification system given\nits verification performance across demographic groups and the composition of\nthe used gallery. Further, we report results for identification experiments on\ndifferently composed demographic subsets, i.e. females and males, of the public\nacademic MORPH database using the open-source ArcFace face recognition system.\nIt is shown that the database composition has a huge impact on performance\ndifferentials in biometric identification systems, even if performance\ndifferentials are less pronounced in the verification scenario. This study\nrepresents the first detailed analysis of the watchlist imbalance effect which\nis expected to be of high interest for future research in the field of facial\nrecognition.",
          "link": "http://arxiv.org/abs/2106.08049",
          "publishedOn": "2021-07-01T01:59:32.394Z",
          "wordCount": 646,
          "title": "Demographic Fairness in Face Identification: The Watchlist Imbalance Effect. (arXiv:2106.08049v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1\">Marcos Vendramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1\">Alexei Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identify\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.",
          "link": "http://arxiv.org/abs/2105.10013",
          "publishedOn": "2021-07-01T01:59:32.387Z",
          "wordCount": 626,
          "title": "Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10159",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Nguyen_D/0/1/0/all/0/1\">Du Nguyen</a>",
          "description": "We provide an explicit formula for the Levi-Civita connection and Riemannian\nHessian for a Riemannian manifold that is a quotient of a manifold embedded in\nan inner product space with a non-constant metric function. Together with a\nclassical formula for projection, this allows us to evaluate Riemannian\ngradient and Hessian for several families of metrics on classical manifolds,\nincluding a family of metrics on Stiefel manifolds connecting both the constant\nand canonical ambient metrics with closed-form geodesics. Using these formulas,\nwe derive Riemannian optimization frameworks on quotients of Stiefel manifolds,\nincluding flag manifolds, and a new family of complete quotient metrics on the\nmanifold of positive-semidefinite matrices of fixed rank, considered as a\nquotient of a product of Stiefel and positive-definite matrix manifold with\naffine-invariant metrics. The method is procedural, and in many instances, the\nRiemannian gradient and Hessian formulas could be derived by symbolic calculus.\nThe method extends the list of potential metrics that could be used in manifold\noptimization and machine learning.",
          "link": "http://arxiv.org/abs/2009.10159",
          "publishedOn": "2021-07-01T01:59:32.375Z",
          "wordCount": 639,
          "title": "Operator-valued formulas for Riemannian Gradient and Hessian and families of tractable metrics. (arXiv:2009.10159v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1\">Rufin VanRullen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1\">Andrea Alamia</a>",
          "description": "Transformer attention architectures, similar to those developed for natural\nlanguage processing, have recently proved efficient also in vision, either in\nconjunction with or as a replacement for convolutional layers. Typically,\nvisual attention is inserted in the network architecture as a (series of)\nfeedforward self-attention module(s), with mutual key-query agreement as the\nmain selection and routing operation. However efficient, this strategy is only\nvaguely compatible with the way that attention is implemented in biological\nbrains: as a separate and unified network of attentional selection regions,\nreceiving inputs from and exerting modulatory influence on the entire hierarchy\nof visual regions. Here, we report experiments with a simple such attention\nsystem that can improve the performance of standard convolutional networks,\nwith relatively few additional parameters. Each spatial position in each layer\nof the network produces a key-query vector pair; all queries are then pooled\ninto a global attention query. On the next iteration, the match between each\nkey and the global attention query modulates the network's activations --\nemphasizing or silencing the locations that agree or disagree (respectively)\nwith the global attention system. We demonstrate the usefulness of this\nbrain-inspired Global Attention Agreement network (GAttANet) for various\nconvolutional backbones (from a simple 5-layer toy model to a standard ResNet50\narchitecture) and datasets (CIFAR10, CIFAR100, Imagenet-1k). Each time, our\nglobal attention system improves accuracy over the corresponding baseline.",
          "link": "http://arxiv.org/abs/2104.05575",
          "publishedOn": "2021-07-01T01:59:32.355Z",
          "wordCount": 704,
          "title": "GAttANet: Global attention agreement for convolutional neural networks. (arXiv:2104.05575v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>",
          "description": "Last ten years have witnessed the growth of many computer vision applications\nfor food recognition. Dietary studies showed that dietary-related problem such\nas obesity is associated with other chronic diseases like hypertension,\nirregular blood sugar levels, and increased risk of heart attacks. The primary\ncause of these problems is poor lifestyle choices and unhealthy dietary habits,\nwhich are manageable by using interactive mHealth apps that use automatic\nvisual-based methods to assess dietary intake. This review discusses the most\nperforming methodologies that have been developed so far for automatic food\nrecognition. First, we will present the rationale of visual-based methods for\nfood recognition. The core of the paper is the presentation, discussion and\nevaluation of these methods on popular food image databases. We also discussed\nthe mobile applications that are implementing these methods. The review ends\nwith a discussion of research gaps and future challenges in this area.",
          "link": "http://arxiv.org/abs/2106.11776",
          "publishedOn": "2021-07-01T01:59:32.345Z",
          "wordCount": 605,
          "title": "A Review of the Vision-based Approaches for Dietary Assessment. (arXiv:2106.11776v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kousha_S/0/1/0/all/0/1\">Shayan Kousha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>",
          "description": "The purpose of generative Zero-shot learning (ZSL) is to learning from seen\nclasses, transfer the learned knowledge, and create samples of unseen classes\nfrom the description of these unseen categories. To achieve better ZSL\naccuracies, models need to better understand the descriptions of unseen\nclasses. We introduce a novel form of regularization that encourages generative\nZSL models to pay more attention to the description of each category. Our\nempirical results demonstrate improvements over the performance of multiple\nstate-of-the-art models on the task of generalized zero-shot recognition and\nclassification when trained on textual description-based datasets like CUB and\nNABirds and attribute-based datasets like AWA2, aPY and SUN.",
          "link": "http://arxiv.org/abs/2106.16108",
          "publishedOn": "2021-07-01T01:59:32.310Z",
          "wordCount": 534,
          "title": "Zero-shot Learning with Class Description Regularization. (arXiv:2106.16108v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07636",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>",
          "description": "We present SR3, an approach to image Super-Resolution via Repeated\nRefinement. SR3 adapts denoising diffusion probabilistic models to conditional\nimage generation and performs super-resolution through a stochastic denoising\nprocess. Inference starts with pure Gaussian noise and iteratively refines the\nnoisy output using a U-Net model trained on denoising at various noise levels.\nSR3 exhibits strong performance on super-resolution tasks at different\nmagnification factors, on faces and natural images. We conduct human evaluation\non a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA\nGAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic\noutputs, while GANs do not exceed a fool rate of 34%. We further show the\neffectiveness of SR3 in cascaded image generation, where generative models are\nchained with super-resolution models, yielding a competitive FID score of 11.3\non ImageNet.",
          "link": "http://arxiv.org/abs/2104.07636",
          "publishedOn": "2021-07-01T01:59:32.295Z",
          "wordCount": 600,
          "title": "Image Super-Resolution via Iterative Refinement. (arXiv:2104.07636v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>",
          "description": "This work is an update of a previous paper on the same topic published a few\nyears ago. With the dramatic progress in generative modeling, a suite of new\nquantitative and qualitative techniques to evaluate models has emerged.\nAlthough some measures such as Inception Score, Frechet Inception Distance,\nPrecision-Recall, and Perceptual Path Length are relatively more popular, GAN\nevaluation is not a settled issue and there is still room for improvement.\nHere, I describe new dimensions that are becoming important in assessing models\n(e.g. bias and fairness) and discuss the connection between GAN evaluation and\ndeepfakes. These are important areas of concern in the machine learning\ncommunity today and progress in GAN evaluation can help mitigate them.",
          "link": "http://arxiv.org/abs/2103.09396",
          "publishedOn": "2021-07-01T01:59:32.288Z",
          "wordCount": 581,
          "title": "Pros and Cons of GAN Evaluation Measures: New Developments. (arXiv:2103.09396v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1\">Felix Leeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "The encoders and decoders of autoencoders effectively project the input onto\nlearned manifolds in the latent space and data space respectively. We propose a\nframework, called latent responses, for probing the learned data manifold using\ninterventions in the latent space. Using this framework, we investigate \"holes\"\nin the representation to quantitatively ascertain to what extent the latent\nspace of a trained VAE is consistent with the chosen prior. Furthermore, we use\nthe identified structure to improve interpolation between latent vectors. We\nevaluate how our analyses improve the quality of the generated samples using\nthe VAE on a variety of benchmark datasets.",
          "link": "http://arxiv.org/abs/2106.16091",
          "publishedOn": "2021-07-01T01:59:32.268Z",
          "wordCount": 541,
          "title": "Interventional Assays for the Latent Space of Autoencoders. (arXiv:2106.16091v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_z/0/1/0/all/0/1\">zhenyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">Dandan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhan Ma</a>",
          "description": "This paper proposes a decoder-side Cross Resolution Synthesis (CRS) module to\npursue better compression efficiency beyond the latest Versatile Video Coding\n(VVC), where we encode intra frames at original high resolution (HR), compress\ninter frames at a lower resolution (LR), and then super-resolve decoded LR\ninter frames with the help from preceding HR intra and neighboring LR inter\nframes. For a LR inter frame, a motion alignment and aggregation network (MAN)\nis devised to produce temporally aggregated motion representation (AMR) for the\nguarantee of temporal smoothness; Another texture compensation network (TCN)\ninputs decoded HR intra frame, re-sampled HR intra frame, and this LR inter\nframe to generate multiscale affinity map (MAM) and multiscale texture\nrepresentation (MTR) for better augmenting spatial details; Finally,\nsimilarity-driven fusion synthesizes AMR, MTR, MAM to upscale LR inter frame\nfor the removal of compression and resolution re-sampling noises. We enhance\nthe VVC using proposed CRS, showing averaged 8.76% and 11.93% Bj{\\o}ntegaard\nDelta Rate (BD-Rate) gains against the latest VVC anchor in Random Access (RA)\nand Low-delay P (LDP) settings respectively. In addition, experimental\ncomparisons to the state-of-the-art super-resolution (SR) based VVC enhancement\nmethods, and ablation studies are conducted to further report superior\nefficiency and generalization of proposed algorithm. All materials will be made\nto public at https://njuvision.github.io/CRS for reproducible research.",
          "link": "http://arxiv.org/abs/2012.00650",
          "publishedOn": "2021-07-01T01:59:32.263Z",
          "wordCount": 705,
          "title": "Decoder-side Cross Resolution Synthesis for Video Compression Enhancement. (arXiv:2012.00650v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1\">Ronny Hug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1\">Wolfgang H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1\">Brendan T. Morris</a>",
          "description": "In applications such as object tracking, time-series data inevitably carry\nmissing observations. Following the success of deep learning-based models for\nvarious sequence learning tasks, these models increasingly replace classic\napproaches in object tracking applications for inferring the object motions\nstate. While traditional tracking approaches can deal with missing\nobservations, most of their deep counterparts are, by default, not suited for\nthis.\n\nTowards this end, this paper introduces a transformer-based approach for\nhandling missing observations in variable input length trajectory data. The\nmodel is formed indirectly by successively increasing the complexity of the\ndemanded inference tasks. Starting from reproducing noise-free trajectories,\nthe model then learns to infer trajectories from noisy inputs. By providing\nmissing tokens, binary-encoded missing events, the model learns to in-attend to\nmissing data and infers a complete trajectory conditioned on the remaining\ninputs. In the case of a sequence of successive missing events, the model then\nacts as a pure prediction model. The model's abilities are demonstrated on\nsynthetic data and real-world data reflecting prototypical object tracking\nscenarios.",
          "link": "http://arxiv.org/abs/2106.16009",
          "publishedOn": "2021-07-01T01:59:32.256Z",
          "wordCount": 615,
          "title": "MissFormer: (In-)attention-based handling of missing observations for trajectory filtering and prediction. (arXiv:2106.16009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Convolutional neural network (CNN) have proven its success for semantic\nsegmentation, which is a core task of emerging industrial applications such as\nautonomous driving. However, most progress in semantic segmentation of urban\nscenes is reported on standard scenarios, i.e., daytime scenes with favorable\nillumination conditions. In practical applications, the outdoor weather and\nillumination are changeable, e.g., cloudy and nighttime, which results in a\nsignificant drop of semantic segmentation accuracy of CNN only trained with\ndaytime data. In this paper, we propose a novel generative adversarial network\n(namely Mutual-GAN) to alleviate the accuracy decline when daytime-trained\nneural network is applied to videos captured under adverse weather conditions.\nThe proposed Mutual-GAN adopts mutual information constraint to preserve\nimage-objects during cross-weather adaptation, which is an unsolved problem for\nmost unsupervised image-to-image translation approaches (e.g., CycleGAN). The\nproposed Mutual-GAN is evaluated on two publicly available driving video\ndatasets (i.e., CamVid and SYNTHIA). The experimental results demonstrate that\nour Mutual-GAN can yield visually plausible translated images and significantly\nimprove the semantic segmentation accuracy of daytime-trained deep learning\nnetwork while processing videos under challenging weathers.",
          "link": "http://arxiv.org/abs/2106.16000",
          "publishedOn": "2021-07-01T01:59:32.248Z",
          "wordCount": 627,
          "title": "Mutual-GAN: Towards Unsupervised Cross-Weather Adaptation with Mutual Information Constraint. (arXiv:2106.16000v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15893",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wilm_F/0/1/0/all/0/1\">Frauke Wilm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benz_M/0/1/0/all/0/1\">Michaela Benz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bruns_V/0/1/0/all/0/1\">Volker Bruns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baghdadlian_S/0/1/0/all/0/1\">Serop Baghdadlian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dexl_J/0/1/0/all/0/1\">Jakob Dexl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartmann_D/0/1/0/all/0/1\">David Hartmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuritcyn_P/0/1/0/all/0/1\">Petr Kuritcyn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weidenfeller_M/0/1/0/all/0/1\">Martin Weidenfeller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wittenberg_T/0/1/0/all/0/1\">Thomas Wittenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Merkel_S/0/1/0/all/0/1\">Susanne Merkel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartmann_A/0/1/0/all/0/1\">Arndt Hartmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eckstein_M/0/1/0/all/0/1\">Markus Eckstein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geppert_C/0/1/0/all/0/1\">Carol I. Geppert</a>",
          "description": "Whole-slide-image cartography is the process of automatically detecting and\noutlining different tissue types in digitized histological specimen. This\nsemantic segmentation provides a basis for many follow-up analyses and can\npotentially guide subsequent medical decisions. Due to their large size,\nwhole-slide-images typically have to be divided into smaller patches which are\nthen analyzed individually using machine learning-based approaches. Thereby,\nlocal dependencies of image regions get lost and since a whole-slide-image\ncomprises many thousands of such patches this process is inherently slow. We\npropose to subdivide the image into coherent regions prior to classification by\ngrouping visually similar adjacent image pixels into larger segments, i.e.\nsuperpixels. Afterwards, only a random subset of patches per superpixel is\nclassified and patch labels are combined into a single superpixel label. The\nalgorithm has been developed and validated on a dataset of 159 hand-annotated\nwhole-slide-images of colon resections and its performance has been compared to\na standard patch-based approach. The algorithm shows an average speed-up of 41%\non the test data and the overall accuracy is increased from 93.8% to 95.7%. We\nadditionally propose a metric for identifying superpixels with an uncertain\nclassification so they can be excluded from further analysis. Finally, we\nevaluate two potential medical applications, namely tumor area estimation\nincluding tumor invasive margin generation and tumor composition analysis.",
          "link": "http://arxiv.org/abs/2106.15893",
          "publishedOn": "2021-07-01T01:59:32.242Z",
          "wordCount": 700,
          "title": "Fast whole-slide cartography in colon cancer histology using superpixels and CNN classification. (arXiv:2106.15893v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1\">Christian Berger</a>",
          "description": "ML-enabled software systems have been incorporated in many public\ndemonstrations for automated driving (AD) systems. Such solutions have also\nbeen considered as a crucial approach to aim at SAE Level 5 systems, where the\npassengers in such vehicles do not have to interact with the system at all\nanymore. Already in 2016, Nvidia demonstrated a complete end-to-end approach\nfor training the complete software stack covering perception, planning and\ndecision making, and the actual vehicle control. While such approaches show the\ngreat potential of such ML-enabled systems, there have also been demonstrations\nwhere already changes to single pixels in a video frame can potentially lead to\ncompletely different decisions with dangerous consequences. In this paper, a\nstructured analysis has been conducted to explore video degradation effects on\nthe performance of an ML-enabled pedestrian detector. Firstly, a baseline of\napplying YOLO to 1,026 frames with pedestrian annotations in the KITTI Vision\nBenchmark Suite has been established. Next, video degradation candidates for\neach of these frames were generated using the leading video codecs libx264,\nlibx265, Nvidia HEVC, and AV1: 52 frames for the various compression presets\nfor color and gray-scale frames resulting in 104 degradation candidates per\noriginal KITTI frame and 426,816 images in total. YOLO was applied to each\nimage to compute the intersection-over-union (IoU) metric to compare the\nperformance with the original baseline. While aggressively lossy compression\nsettings result in significant performance drops as expected, it was also\nobserved that some configurations actually result in slightly better IoU\nresults compared to the baseline. The findings show that carefully chosen lossy\nvideo configurations preserve a decent performance of particular ML-enabled\nsystems while allowing for substantial savings when storing or transmitting\ndata.",
          "link": "http://arxiv.org/abs/2106.15889",
          "publishedOn": "2021-07-01T01:59:32.233Z",
          "wordCount": 726,
          "title": "A Structured Analysis of the Video Degradation Effects on the Performance of a Machine Learning-enabled Pedestrian Detector. (arXiv:2106.15889v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Christopher Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1\">Arsalan Mousavian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "Segmenting unseen object instances in cluttered environments is an important\ncapability that robots need when functioning in unstructured environments.\nWhile previous methods have exhibited promising results, they still tend to\nprovide incorrect results in highly cluttered scenes. We postulate that a\nnetwork architecture that encodes relations between objects at a high-level can\nbe beneficial. Thus, in this work, we propose a novel framework that refines\nthe output of such methods by utilizing a graph-based representation of\ninstance masks. We train deep networks capable of sampling smart perturbations\nto the segmentations, and a graph neural network, which can encode relations\nbetween objects, to evaluate the perturbed segmentations. Our proposed method\nis orthogonal to previous works and achieves state-of-the-art performance when\ncombined with them. We demonstrate an application that uses uncertainty\nestimates generated by our method to guide a manipulator, leading to efficient\nunderstanding of cluttered scenes. Code, models, and video can be found at\nhttps://github.com/chrisdxie/rice .",
          "link": "http://arxiv.org/abs/2106.15711",
          "publishedOn": "2021-07-01T01:59:32.203Z",
          "wordCount": 601,
          "title": "RICE: Refining Instance Masks in Cluttered Environments with Graph Neural Networks. (arXiv:2106.15711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+chen_Y/0/1/0/all/0/1\">Yuanhong chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seon Ho Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verjans_J/0/1/0/all/0/1\">Johan W. Verjans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajvinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>",
          "description": "Unsupervised anomaly detection (UAD) learns one-class classifiers exclusively\nwith normal (i.e., healthy) images to detect any abnormal (i.e., unhealthy)\nsamples that do not conform to the expected normal patterns. UAD has two main\nadvantages over its fully supervised counterpart. Firstly, it is able to\ndirectly leverage large datasets available from health screening programs that\ncontain mostly normal image samples, avoiding the costly manual labelling of\nabnormal samples and the subsequent issues involved in training with extremely\nclass-imbalanced data. Further, UAD approaches can potentially detect and\nlocalise any type of lesions that deviate from the normal patterns. One\nsignificant challenge faced by UAD methods is how to learn effective\nlow-dimensional image representations to detect and localise subtle\nabnormalities, generally consisting of small lesions. To address this\nchallenge, we propose a novel self-supervised representation learning method,\ncalled Constrained Contrastive Distribution learning for anomaly detection\n(CCD), which learns fine-grained feature representations by simultaneously\npredicting the distribution of augmented data and image contexts using\ncontrastive learning with pretext constraints. The learned representations can\nbe leveraged to train more anomaly-sensitive detection models. Extensive\nexperiment results show that our method outperforms current state-of-the-art\nUAD approaches on three different colonoscopy and fundus screening datasets.\nOur code is available at https://github.com/tianyu0207/CCD.",
          "link": "http://arxiv.org/abs/2103.03423",
          "publishedOn": "2021-07-01T01:59:32.187Z",
          "wordCount": 694,
          "title": "Constrained Contrastive Distribution Learning for Unsupervised Anomaly Detection and Localisation in Medical Images. (arXiv:2103.03423v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhat_P/0/1/0/all/0/1\">Prashant Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Self-supervised learning solves pretext prediction tasks that do not require\nannotations to learn feature representations. For vision tasks, pretext tasks\nsuch as predicting rotation, solving jigsaw are solely created from the input\ndata. Yet, predicting this known information helps in learning representations\nuseful for downstream tasks. However, recent works have shown that wider and\ndeeper models benefit more from self-supervised learning than smaller models.\nTo address the issue of self-supervised pre-training of smaller models, we\npropose Distill-on-the-Go (DoGo), a self-supervised learning paradigm using\nsingle-stage online knowledge distillation to improve the representation\nquality of the smaller models. We employ deep mutual learning strategy in which\ntwo models collaboratively learn from each other to improve one another.\nSpecifically, each model is trained using self-supervised learning along with\ndistillation that aligns each model's softmax probabilities of similarity\nscores with that of the peer model. We conduct extensive experiments on\nmultiple benchmark datasets, learning objectives, and architectures to\ndemonstrate the potential of our proposed method. Our results show significant\nperformance gain in the presence of noisy and limited labels and generalization\nto out-of-distribution data.",
          "link": "http://arxiv.org/abs/2104.09866",
          "publishedOn": "2021-07-01T01:59:32.169Z",
          "wordCount": 663,
          "title": "Distill on the Go: Online knowledge distillation in self-supervised learning. (arXiv:2104.09866v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tzu-Mao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>",
          "description": "Neural networks are susceptible to small transformations including 2D\nrotations and shifts, image crops, and even changes in object colors. This is\noften attributed to biases in the training dataset, and the lack of 2D\nshift-invariance due to not respecting the sampling theorem. In this paper, we\nchallenge this hypothesis by training and testing on unbiased datasets, and\nshowing that networks are brittle to both small 3D perspective changes and\nlighting variations which cannot be explained by dataset bias or lack of\nshift-invariance. To find these in-distribution errors, we introduce an\nevolution strategies (ES) based approach, which we call CMA-Search. Despite\ntraining with a large-scale (0.5 million images), unbiased dataset of camera\nand light variations, in over 71% cases CMA-Search can find camera parameters\nin the vicinity of a correctly classified image which lead to in-distribution\nmisclassifications with < 3.6% change in parameters. With lighting changes,\nCMA-Search finds misclassifications in 33% cases with < 11.6% change in\nparameters. Finally, we extend this method to find misclassifications in the\nvicinity of ImageNet images for both ResNet and OpenAI's CLIP model.",
          "link": "http://arxiv.org/abs/2106.16198",
          "publishedOn": "2021-07-01T01:59:32.146Z",
          "wordCount": 630,
          "title": "Small in-distribution changes in 3D perspective and lighting fool both CNNs and Transformers. (arXiv:2106.16198v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nikhil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, along with their unprecedented size and\namounts of training data, many have come to believe that they are not suitable\nfor small sets of data. This trend leads to great concerns, including but not\nlimited to: limited availability of data in certain scientific domains and the\nexclusion of those with limited resource from research in the field. In this\npaper, we dispel the myth that transformers are \"data hungry\" and therefore can\nonly be applied to large sets of data. We show for the first time that with the\nright size and tokenization, transformers can perform head-to-head with\nstate-of-the-art CNNs on small datasets. Our model eliminates the requirement\nfor class token and positional embeddings through a novel sequence pooling\nstrategy and the use of convolutions. We show that compared to CNNs, our\ncompact transformers have fewer parameters and MACs, while obtaining similar\naccuracies. Our method is flexible in terms of model size, and can have as\nlittle as 0.28M parameters and achieve reasonable results. It can reach an\naccuracy of 95.29 % when training from scratch on CIFAR-10, which is comparable\nwith modern CNN based approaches, and a significant improvement over previous\nTransformer based models. Our simple and compact design democratizes\ntransformers by making them accessible to those equipped with basic computing\nresources and/or dealing with important small datasets. Our method works on\nlarger datasets, such as ImageNet (80.28% accuracy with 29% parameters of ViT),\nand NLP tasks as well. Our code and pre-trained models are publicly available\nat https://github.com/SHI-Labs/Compact-Transformers.",
          "link": "http://arxiv.org/abs/2104.05704",
          "publishedOn": "2021-07-01T01:59:32.140Z",
          "wordCount": 748,
          "title": "Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Model-agnostic meta-learning (MAML) is arguably the most popular\nmeta-learning algorithm nowadays, given its flexibility to incorporate various\nmodel architectures and to be applied to different problems. Nevertheless, its\nperformance on few-shot classification is far behind many recent algorithms\ndedicated to the problem. In this paper, we point out several key facets of how\nto train MAML to excel in few-shot classification. First, we find that a large\nnumber of gradient steps are needed for the inner loop update, which\ncontradicts the common usage of MAML for few-shot classification. Second, we\nfind that MAML is sensitive to the permutation of class assignments in\nmeta-testing: for a few-shot task of $N$ classes, there are exponentially many\nways to assign the learned initialization of the $N$-way classifier to the $N$\nclasses, leading to an unavoidably huge variance. Third, we investigate several\nways for permutation invariance and find that learning a shared classifier\ninitialization for all the classes performs the best. On benchmark datasets\nsuch as MiniImageNet and TieredImageNet, our approach, which we name\nUNICORN-MAML, performs on a par with or even outperforms state-of-the-art\nalgorithms, while keeping the simplicity of MAML without adding any extra\nsub-networks.",
          "link": "http://arxiv.org/abs/2106.16245",
          "publishedOn": "2021-07-01T01:59:32.133Z",
          "wordCount": 632,
          "title": "How to Train Your MAML to Excel in Few-Shot Classification. (arXiv:2106.16245v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>",
          "description": "Efficient long-short temporal modeling is key for enhancing the performance\nof action recognition task. In this paper, we propose a new two-stream action\nrecognition network, termed as MENet, consisting of a Motion Enhancement (ME)\nmodule and a Video-level Aggregation (VLA) module to achieve long-short\ntemporal modeling. Specifically, motion representations have been proved\neffective in capturing short-term and high-frequency action. However, current\nmotion representations are calculated from adjacent frames, which may have poor\ninterpretation and bring useless information (noisy or blank). Thus, for\nshort-term motions, we design an efficient ME module to enhance the short-term\nmotions by mingling the motion saliency among neighboring segments. As for\nlong-term aggregations, VLA is adopted at the top of the appearance branch to\nintegrate the long-term dependencies across all segments. The two components of\nMENet are complementary in temporal modeling. Extensive experiments are\nconducted on UCF101 and HMDB51 benchmarks, which verify the effectiveness and\nefficiency of our proposed MENet.",
          "link": "http://arxiv.org/abs/2106.15787",
          "publishedOn": "2021-07-01T01:59:32.113Z",
          "wordCount": 591,
          "title": "Long-Short Temporal Modeling for Efficient Action Recognition. (arXiv:2106.15787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1\">Juan Tapia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droguett_E/0/1/0/all/0/1\">Enrique Lopez Droguett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenzuela_A/0/1/0/all/0/1\">Andres Valenzuela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benalcazar_D/0/1/0/all/0/1\">Daniel Benalcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Causa_L/0/1/0/all/0/1\">Leonardo Causa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "This paper proposes a new framework to detect, segment, and estimate the\nlocalization of the eyes from a periocular Near-Infra-Red iris image under\nalcohol consumption. The purpose of the system is to measure the fitness for\nduty. Fitness systems allow us to determine whether a person is physically or\npsychologically able to perform their tasks. Our framework is based on an\nobject detector trained from scratch to detect both eyes from a single image.\nThen, two efficient networks were used for semantic segmentation; a Criss-Cross\nattention network and DenseNet10, with only 122,514 and 210,732 parameters,\nrespectively. These networks can find the pupil, iris, and sclera. In the end,\nthe binary output eye mask is used for pupil and iris diameter estimation with\nhigh precision. Five state-of-the-art algorithms were used for this purpose. A\nmixed proposal reached the best results. A second contribution is establishing\nan alcohol behavior curve to detect the alcohol presence utilizing a stream of\nimages captured from an iris instance. Also, a manually labeled database with\nmore than 20k images was created. Our best method obtains a mean\nIntersection-over-Union of 94.54% with DenseNet10 with only 210,732 parameters\nand an error of only 1-pixel on average.",
          "link": "http://arxiv.org/abs/2106.15828",
          "publishedOn": "2021-07-01T01:59:32.106Z",
          "wordCount": 643,
          "title": "Semantic Segmentation of Periocular Near-Infra-Red Eye Images Under Alcohol Effects. (arXiv:2106.15828v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rufeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Compared to many other dense prediction tasks, e.g., semantic segmentation,\nit is the arbitrary number of instances that has made instance segmentation\nmuch more challenging. In order to predict a mask for each instance, mainstream\napproaches either follow the 'detect-then-segment' strategy (e.g., Mask R-CNN),\nor predict embedding vectors first then cluster pixels into individual\ninstances. In this paper, we view the task of instance segmentation from a\ncompletely new perspective by introducing the notion of \"instance categories\",\nwhich assigns categories to each pixel within an instance according to the\ninstance's location. With this notion, we propose segmenting objects by\nlocations (SOLO), a simple, direct, and fast framework for instance\nsegmentation with strong performance. We derive a few SOLO variants (e.g.,\nVanilla SOLO, Decoupled SOLO, Dynamic SOLO) following the basic principle. Our\nmethod directly maps a raw input image to the desired object categories and\ninstance masks, eliminating the need for the grouping post-processing or the\nbounding box detection. Our approach achieves state-of-the-art results for\ninstance segmentation in terms of both speed and accuracy, while being\nconsiderably simpler than the existing methods. Besides instance segmentation,\nour method yields state-of-the-art results in object detection (from our mask\nbyproduct) and panoptic segmentation. We further demonstrate the flexibility\nand high-quality segmentation of SOLO by extending it to perform one-stage\ninstance-level image matting. Code is available at: https://git.io/AdelaiDet",
          "link": "http://arxiv.org/abs/2106.15947",
          "publishedOn": "2021-07-01T01:59:32.089Z",
          "wordCount": 672,
          "title": "SOLO: A Simple Framework for Instance Segmentation. (arXiv:2106.15947v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zipei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_F/0/1/0/all/0/1\">Fengqian Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chuyang Ye</a>",
          "description": "Cell detection in histopathology images is of great value in clinical\npractice. \\textit{Convolutional neural networks} (CNNs) have been applied to\ncell detection to improve the detection accuracy, where cell annotations are\nrequired for network training. However, due to the variety and large number of\ncells, complete annotations that include every cell of interest in the training\nimages can be challenging. Usually, incomplete annotations can be achieved,\nwhere positive labeling results are carefully examined to ensure their\nreliability but there can be other positive instances, i.e., cells of interest,\nthat are not included in the annotations. This annotation strategy leads to a\nlack of knowledge about true negative samples. Most existing methods simply\ntreat instances that are not labeled as positive as truly negative during\nnetwork training, which can adversely affect the network performance. In this\nwork, to address the problem of incomplete annotations, we formulate the\ntraining of detection networks as a positive-unlabeled learning problem.\nSpecifically, the classification loss in network training is revised to take\ninto account incomplete annotations, where the terms corresponding to negative\nsamples are approximated with the true positive samples and the other samples\nof which the labels are unknown. To evaluate the proposed method, experiments\nwere performed on a publicly available dataset for mitosis detection in breast\ncancer cells, and the experimental results show that our method improves the\nperformance of cell detection given incomplete annotations for training.",
          "link": "http://arxiv.org/abs/2106.15918",
          "publishedOn": "2021-07-01T01:59:32.068Z",
          "wordCount": 680,
          "title": "Positive-unlabeled Learning for Cell Detection in Histopathology Images with Incomplete Annotations. (arXiv:2106.15918v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1\">Himanshu Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Saurabh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shichong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>",
          "description": "Shape completion is the problem of completing partial input shapes such as\npartial scans. This problem finds important applications in computer vision and\nrobotics due to issues such as occlusion or sparsity in real-world data.\nHowever, most of the existing research related to shape completion has been\nfocused on completing shapes by learning a one-to-one mapping which limits the\ndiversity and creativity of the produced results. We propose a novel multimodal\nshape completion technique that is effectively able to learn a one-to-many\nmapping and generates diverse complete shapes. Our approach is based on the\nconditional Implicit MaximumLikelihood Estimation (IMLE) technique wherein we\ncondition our inputs on partial 3D point clouds. We extensively evaluate our\napproach by comparing it to various baselines both quantitatively and\nqualitatively. We show that our method is superior to alternatives in terms of\ncompleteness and diversity of shapes",
          "link": "http://arxiv.org/abs/2106.16237",
          "publishedOn": "2021-07-01T01:59:32.031Z",
          "wordCount": 571,
          "title": "Shape Completion via IMLE. (arXiv:2106.16237v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16174",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_P/0/1/0/all/0/1\">Pingjun Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Aminu_M/0/1/0/all/0/1\">Muhammad Aminu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hussein_S/0/1/0/all/0/1\">Siba El Hussein</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khoury_J/0/1/0/all/0/1\">Joseph Khoury</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>",
          "description": "The cells and their spatial patterns in the tumor microenvironment (TME) play\na key role in tumor evolution, and yet remains an understudied topic in\ncomputational pathology. This study, to the best of our knowledge, is among the\nfirst to hybrid local and global graph methods to profile orchestration and\ninteraction of cellular components. To address the challenge in hematolymphoid\ncancers where the cell classes in TME are unclear, we first implemented cell\nlevel unsupervised learning and identified two new cell subtypes. Local cell\ngraphs or supercells were built for each image by considering the individual\ncell's geospatial location and classes. Then, we applied supercell level\nclustering and identified two new cell communities. In the end, we built global\ngraphs to abstract spatial interaction patterns and extract features for\ndisease diagnosis. We evaluate the proposed algorithm on H\\&E slides of 60\nhematolymphoid neoplasm patients and further compared it with three cell level\ngraph-based algorithms, including the global cell graph, cluster cell graph,\nand FLocK. The proposed algorithm achieves a mean diagnosis accuracy of 0.703\nwith the repeated 5-fold cross-validation scheme. In conclusion, our algorithm\nshows superior performance over the existing methods and can be potentially\napplied to other cancer types.",
          "link": "http://arxiv.org/abs/2106.16174",
          "publishedOn": "2021-07-01T01:59:31.999Z",
          "wordCount": 661,
          "title": "Hierarchical Phenotyping and Graph Modeling of Spatial Architecture in Lymphoid Neoplasms. (arXiv:2106.16174v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16031",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dalmaz_O/0/1/0/all/0/1\">Onat Dalmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1\">Mahmut Yurt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>",
          "description": "Multi-modal imaging is a key healthcare technology in the diagnosis and\nmanagement of disease, but it is often underutilized due to costs associated\nwith multiple separate scans. This limitation yields the need for synthesis of\nunacquired modalities from the subset of available modalities. In recent years,\ngenerative adversarial network (GAN) models with superior depiction of\nstructural details have been established as state-of-the-art in numerous\nmedical image synthesis tasks. However, GANs are characteristically based on\nconvolutional neural network (CNN) backbones that perform local processing with\ncompact filters. This inductive bias, in turn, compromises learning of\nlong-range spatial dependencies. While attention maps incorporated in GANs can\nmultiplicatively modulate CNN features to emphasize critical image regions,\ntheir capture of global context is mostly implicit. Here, we propose a novel\ngenerative adversarial approach for medical image synthesis, ResViT, to combine\nlocal precision of convolution operators with contextual sensitivity of vision\ntransformers. Based on an encoder-decoder architecture, ResViT employs a\ncentral bottleneck comprising novel aggregated residual transformer (ART)\nblocks that synergistically combine convolutional and transformer modules.\nComprehensive demonstrations are performed for synthesizing missing sequences\nin multi-contrast MRI and CT images from MRI. Our results indicate the\nsuperiority of ResViT against competing methods in terms of qualitative\nobservations and quantitative metrics.",
          "link": "http://arxiv.org/abs/2106.16031",
          "publishedOn": "2021-07-01T01:59:31.980Z",
          "wordCount": 650,
          "title": "ResViT: Residual vision transformers for multi-modal medical image synthesis. (arXiv:2106.16031v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaofo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>",
          "description": "Convolutional Neural Networks (CNNs) have achieved great success due to the\npowerful feature learning ability of convolution layers. Specifically, the\nstandard convolution traverses the input images/features using a sliding window\nscheme to extract features. However, not all the windows contribute equally to\nthe prediction results of CNNs. In practice, the convolutional operation on\nsome of the windows (e.g., smooth windows that contain very similar pixels) can\nbe very redundant and may introduce noises into the computation. Such\nredundancy may not only deteriorate the performance but also incur the\nunnecessary computational cost. Thus, it is important to reduce the\ncomputational redundancy of convolution to improve the performance. To this\nend, we propose a Content-aware Convolution (CAC) that automatically detects\nthe smooth windows and applies a 1x1 convolutional kernel to replace the\noriginal large kernel. In this sense, we are able to effectively avoid the\nredundant computation on similar pixels. By replacing the standard convolution\nin CNNs with our CAC, the resultant models yield significantly better\nperformance and lower computational cost than the baseline models with the\nstandard convolution. More critically, we are able to dynamically allocate\nsuitable computation resources according to the data smoothness of different\nimages, making it possible for content-aware computation. Extensive experiments\non various computer vision tasks demonstrate the superiority of our method over\nexisting methods.",
          "link": "http://arxiv.org/abs/2106.15797",
          "publishedOn": "2021-07-01T01:59:31.970Z",
          "wordCount": 656,
          "title": "Content-Aware Convolutional Neural Networks. (arXiv:2106.15797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongdao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiangxin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Association, aiming to link bounding boxes of the same identity in a video\nsequence, is a central component in multi-object tracking (MOT). To train\nassociation modules, e.g., parametric networks, real video data are usually\nused. However, annotating person tracks in consecutive video frames is\nexpensive, and such real data, due to its inflexibility, offer us limited\nopportunities to evaluate the system performance w.r.t changing tracking\nscenarios. In this paper, we study whether 3D synthetic data can replace\nreal-world videos for association training. Specifically, we introduce a\nlarge-scale synthetic data engine named MOTX, where the motion characteristics\nof cameras and objects are manually configured to be similar to those in\nreal-world datasets. We show that compared with real data, association\nknowledge obtained from synthetic data can achieve very similar performance on\nreal-world test sets without domain adaption techniques. Our intriguing\nobservation is credited to two factors. First and foremost, 3D engines can well\nsimulate motion factors such as camera movement, camera view and object\nmovement, so that the simulated videos can provide association modules with\neffective motion features. Second, experimental results show that the\nappearance domain gap hardly harms the learning of association knowledge. In\naddition, the strong customization ability of MOTX allows us to quantitatively\nassess the impact of motion factors on MOT, which brings new insights to the\ncommunity.",
          "link": "http://arxiv.org/abs/2106.16100",
          "publishedOn": "2021-07-01T01:59:31.963Z",
          "wordCount": 670,
          "title": "Synthetic Data Are as Good as the Real for Association Knowledge Learning in Multi-object Tracking. (arXiv:2106.16100v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Guoli Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Images can convey rich semantics and induce various emotions in viewers.\nRecently, with the rapid advancement of emotional intelligence and the\nexplosive growth of visual data, extensive research efforts have been dedicated\nto affective image content analysis (AICA). In this survey, we will\ncomprehensively review the development of AICA in the recent two decades,\nespecially focusing on the state-of-the-art methods with respect to three main\nchallenges -- the affective gap, perception subjectivity, and label noise and\nabsence. We begin with an introduction to the key emotion representation models\nthat have been widely employed in AICA and description of available datasets\nfor performing evaluation with quantitative comparison of label noise and\ndataset bias. We then summarize and compare the representative approaches on\n(1) emotion feature extraction, including both handcrafted and deep features,\n(2) learning methods on dominant emotion recognition, personalized emotion\nprediction, emotion distribution learning, and learning from noisy data or few\nlabels, and (3) AICA based applications. Finally, we discuss some challenges\nand promising research directions in the future, such as image content and\ncontext understanding, group emotion clustering, and viewer-image interaction.",
          "link": "http://arxiv.org/abs/2106.16125",
          "publishedOn": "2021-07-01T01:59:31.957Z",
          "wordCount": 643,
          "title": "Affective Image Content Analysis: Two Decades Review and New Perspectives. (arXiv:2106.16125v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wiens_D/0/1/0/all/0/1\">Daniel Wiens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Even though deep neural networks succeed on many different tasks including\nsemantic segmentation, they lack on robustness against adversarial examples. To\ncounteract this exploit, often adversarial training is used. However, it is\nknown that adversarial training with weak adversarial attacks (e.g. using the\nFast Gradient Method) does not improve the robustness against stronger attacks.\nRecent research shows that it is possible to increase the robustness of such\nsingle-step methods by choosing an appropriate step size during the training.\nFinding such a step size, without increasing the computational effort of\nsingle-step adversarial training, is still an open challenge. In this work we\naddress the computationally particularly demanding task of semantic\nsegmentation and propose a new step size control algorithm that increases the\nrobustness of single-step adversarial training. The proposed algorithm does not\nincrease the computational effort of single-step adversarial training\nconsiderably and also simplifies training, because it is free of\nmeta-parameter. We show that the robustness of our approach can compete with\nmulti-step adversarial training on two popular benchmarks for semantic\nsegmentation.",
          "link": "http://arxiv.org/abs/2106.15998",
          "publishedOn": "2021-07-01T01:59:31.936Z",
          "wordCount": 609,
          "title": "Single-Step Adversarial Training for Semantic Segmentation. (arXiv:2106.15998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15953",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wei_X/0/1/0/all/0/1\">Xinxu Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xianshi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shisen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1\">Cheng Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yanlin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_K/0/1/0/all/0/1\">Kaifu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yongjie Li</a>",
          "description": "Images obtained in real-world low-light conditions are not only low in\nbrightness, but they also suffer from many other types of degradation, such as\ncolor bias, unknown noise, detail loss and halo artifacts. In this paper, we\npropose a very fast deep learning framework called Bringing the Lightness\n(denoted as BLNet) that consists of two U-Nets with a series of well-designed\nloss functions to tackle all of the above degradations. Based on Retinex\nTheory, the decomposition net in our model can decompose low-light images into\nreflectance and illumination and remove noise in the reflectance during the\ndecomposition phase. We propose a Noise and Color Bias Control module (NCBC\nModule) that contains a convolutional neural network and two loss functions\n(noise loss and color loss). This module is only used to calculate the loss\nfunctions during the training phase, so our method is very fast during the test\nphase. This module can smooth the reflectance to achieve the purpose of noise\nremoval while preserving details and edge information and controlling color\nbias. We propose a network that can be trained to learn the mapping between\nlow-light and normal-light illumination and enhance the brightness of images\ntaken in low-light illumination. We train and evaluate the performance of our\nproposed model over the real-world Low-Light (LOL) dataset), and we also test\nour model over several other frequently used datasets (LIME, DICM and MEF\ndatasets). We conduct extensive experiments to demonstrate that our approach\nachieves a promising effect with good rubustness and generalization and\noutperforms many other state-of-the-art methods qualitatively and\nquantitatively. Our method achieves high speed because we use loss functions\ninstead of introducing additional denoisers for noise removal and color\ncorrection. The code and model are available at\nhttps://github.com/weixinxu666/BLNet.",
          "link": "http://arxiv.org/abs/2106.15953",
          "publishedOn": "2021-07-01T01:59:31.910Z",
          "wordCount": 765,
          "title": "BLNet: A Fast Deep Learning Framework for Low-Light Image Enhancement with Noise Removal and Color Restoration. (arXiv:2106.15953v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bohao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradbury_K/0/1/0/all/0/1\">Kyle Bradbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malof_J/0/1/0/all/0/1\">Jordan M. Malof</a>",
          "description": "Recently deep neural networks (DNNs) have achieved tremendous success for\nobject detection in overhead (e.g., satellite) imagery. One ongoing challenge\nhowever is the acquisition of training data, due to high costs of obtaining\nsatellite imagery and annotating objects in it. In this work we present a\nsimple approach - termed Synthetic object IMPLantation (SIMPL) - to easily and\nrapidly generate large quantities of synthetic overhead training data for\ncustom target objects. We demonstrate the effectiveness of using SIMPL\nsynthetic imagery for training DNNs in zero-shot scenarios where no real\nimagery is available; and few-shot learning scenarios, where limited real-world\nimagery is available. We also conduct experiments to study the sensitivity of\nSIMPL's effectiveness to some key design parameters, providing users for\ninsights when designing synthetic imagery for custom objects. We release a\nsoftware implementation of our SIMPL approach so that others can build upon it,\nor use it for their own custom problems.",
          "link": "http://arxiv.org/abs/2106.15681",
          "publishedOn": "2021-07-01T01:59:31.903Z",
          "wordCount": 599,
          "title": "SIMPL: Generating Synthetic Overhead Imagery to Address Zero-shot and Few-Shot Detection Problems. (arXiv:2106.15681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabani_H/0/1/0/all/0/1\">Hamid Tabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramaniam_A/0/1/0/all/0/1\">Ajay Balasubramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1\">Shabbir Marzban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Transformers provide promising accuracy and have become popular and used in\nvarious domains such as natural language processing and computer vision.\nHowever, due to their massive number of model parameters, memory and\ncomputation requirements, they are not suitable for resource-constrained\nlow-power devices. Even with high-performance and specialized devices, the\nmemory bandwidth can become a performance-limiting bottleneck. In this paper,\nwe present a performance analysis of state-of-the-art vision transformers on\nseveral devices. We propose to reduce the overall memory footprint and memory\ntransfers by clustering the model parameters. We show that by using only 64\nclusters to represent model parameters, it is possible to reduce the data\ntransfer from the main memory by more than 4x, achieve up to 22% speedup and\n39% energy savings on mobile devices with less than 0.1% accuracy loss.",
          "link": "http://arxiv.org/abs/2106.16006",
          "publishedOn": "2021-07-01T01:59:31.887Z",
          "wordCount": 589,
          "title": "Improving the Efficiency of Transformers for Resource-Constrained Devices. (arXiv:2106.16006v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wenming Tang Guoping Qiu</a>",
          "description": "This paper presents new designs of graph convolutional neural networks (GCNs)\non 3D meshes for 3D object segmentation and classification. We use the faces of\nthe mesh as basic processing units and represent a 3D mesh as a graph where\neach node corresponds to a face. To enhance the descriptive power of the graph,\nwe introduce a 1-ring face neighbourhood structure to derive novel\nmulti-dimensional spatial and structure features to represent the graph nodes.\nBased on this new graph representation, we then design a densely connected\ngraph convolutional block which aggregates local and regional features as the\nkey construction component to build effective and efficient practical GCN\nmodels for 3D object classification and segmentation. We will present\nexperimental results to show that our new technique outperforms state of the\nart where our models are shown to have the smallest number of parameters and\nconsietently achieve the highest accuracies across a number of benchmark\ndatasets. We will also present ablation studies to demonstrate the soundness of\nour design principles and the effectiveness of our practical models.",
          "link": "http://arxiv.org/abs/2106.15778",
          "publishedOn": "2021-07-01T01:59:31.848Z",
          "wordCount": 621,
          "title": "Dense Graph Convolutional Neural Networks on 3D Meshes for 3D Object Segmentation and Classification. (arXiv:2106.15778v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1\">Anders Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_Y/0/1/0/all/0/1\">Yasaman Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>",
          "description": "Although machine learning models typically experience a drop in performance\non out-of-distribution data, accuracies on in- versus out-of-distribution data\nare widely observed to follow a single linear trend when evaluated across a\ntestbed of models. Models that are more accurate on the out-of-distribution\ndata relative to this baseline exhibit \"effective robustness\" and are\nexceedingly rare. Identifying such models, and understanding their properties,\nis key to improving out-of-distribution performance. We conduct a thorough\nempirical investigation of effective robustness during fine-tuning and\nsurprisingly find that models pre-trained on larger datasets exhibit effective\nrobustness during training that vanishes at convergence. We study how\nproperties of the data influence effective robustness, and we show that it\nincreases with the larger size, more diversity, and higher example difficulty\nof the dataset. We also find that models that display effective robustness are\nable to correctly classify 10% of the examples that no other current testbed\nmodel gets correct. Finally, we discuss several strategies for scaling\neffective robustness to the high-accuracy regime to improve the\nout-of-distribution accuracy of state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.15831",
          "publishedOn": "2021-07-01T01:59:31.828Z",
          "wordCount": 617,
          "title": "The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning. (arXiv:2106.15831v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1\">Poorya Aghdaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1\">Baaria Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "Morphed images have exploited loopholes in the face recognition checkpoints,\ne.g., Credential Authentication Technology (CAT), used by Transportation\nSecurity Administration (TSA), which is a non-trivial security concern. To\novercome the risks incurred due to morphed presentations, we propose a\nwavelet-based morph detection methodology which adopts an end-to-end trainable\nsoft attention mechanism . Our attention-based deep neural network (DNN)\nfocuses on the salient Regions of Interest (ROI) which have the most spatial\nsupport for morph detector decision function, i.e, morph class binary softmax\noutput. A retrospective of morph synthesizing procedure aids us to speculate\nthe ROI as regions around facial landmarks , particularly for the case of\nlandmark-based morphing techniques. Moreover, our attention-based DNN is\nadapted to the wavelet space, where inputs of the network are coarse-to-fine\nspectral representations, 48 stacked wavelet sub-bands to be exact. We evaluate\nperformance of the proposed framework using three datasets, VISAPP17, LMA, and\nMorGAN. In addition, as attention maps can be a robust indicator whether a\nprobe image under investigation is genuine or counterfeit, we analyze the\nestimated attention maps for both a bona fide image and its corresponding\nmorphed image. Finally, we present an ablation study on the efficacy of\nutilizing attention mechanism for the sake of morph detection.",
          "link": "http://arxiv.org/abs/2106.15686",
          "publishedOn": "2021-07-01T01:59:31.806Z",
          "wordCount": 647,
          "title": "Attention Aware Wavelet-based Detection of Morphed Face Images. (arXiv:2106.15686v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15707",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yinzhe Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zeyu Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Binghuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Firmin_D/0/1/0/all/0/1\">David Firmin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>",
          "description": "Segmentation of cardiac fibrosis and scar are essential for clinical\ndiagnosis and can provide invaluable guidance for the treatment of cardiac\ndiseases. Late Gadolinium enhancement (LGE) cardiovascular magnetic resonance\n(CMR) has been successful for its efficacy in guiding the clinical diagnosis\nand treatment reliably. For LGE CMR, many methods have demonstrated success in\naccurately segmenting scarring regions. Co-registration with other\nnon-contrast-agent (non-CA) modalities, balanced steady-state free precession\n(bSSFP) and cine magnetic resonance imaging (MRI) for example, can further\nenhance the efficacy of automated segmentation of cardiac anatomies. Many\nconventional methods have been proposed to provide automated or semi-automated\nsegmentation of scars. With the development of deep learning in recent years,\nwe can also see more advanced methods that are more efficient in providing more\naccurate segmentations. This paper conducts a state-of-the-art review of\nconventional and current state-of-the-art approaches utilising different\nmodalities for accurate cardiac fibrosis and scar segmentation.",
          "link": "http://arxiv.org/abs/2106.15707",
          "publishedOn": "2021-07-01T01:59:31.800Z",
          "wordCount": 622,
          "title": "Recent Advances in Fibrosis and Scar Segmentation from Cardiac MRI: A State-of-the-Art Review and Future Perspectives. (arXiv:2106.15707v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10796",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yue_Z/0/1/0/all/0/1\">Zongsheng Yue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yong_H/0/1/0/all/0/1\">Hongwei Yong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "Deep neural networks (DNNs) have achieved significant success in image\nrestoration tasks by directly learning a powerful non-linear mapping from\ncorrupted images to their latent clean ones. However, there still exist two\nmajor limitations for these deep learning (DL)-based methods. Firstly, the\nnoises contained in real corrupted images are very complex, usually neglected\nand largely under-estimated in most current methods. Secondly, existing DL\nmethods are mostly trained on one pre-assumed degradation process for all of\nthe training image pairs, such as the widely used bicubic downsampling\nassumption in the image super-resolution task, inevitably leading to poor\ngeneralization performance when the true degradation does not match with such\nassumed one. To address these issues, we propose a unified generative model for\nthe image restoration, which elaborately configures the degradation process\nfrom the latent clean image to the observed corrupted one. Specifically,\ndifferent from most of current methods, the pixel-wisely non-i.i.d. Gaussian\ndistribution, being with more flexibility, is adopted in our method to fit the\ncomplex real noises. Furthermore, the method is built on the general image\ndegradation process, making it capable of adapting diverse degradations under\none single model. Besides, we design a variational inference algorithm to learn\nall parameters involved in the proposed model with explicit form of objective\nloss. Specifically, beyond traditional variational methodology, two DNNs are\nemployed to parameterize the posteriori distributions, one to infer the\ndistribution of the latent clean image, and another to infer the distribution\nof the image noise. Extensive experiments demonstrate the superiority of the\nproposed method on three classical image restoration tasks, including image\ndenoising, image super-resolution and JPEG image deblocking.",
          "link": "http://arxiv.org/abs/2008.10796",
          "publishedOn": "2021-06-30T02:01:02.194Z",
          "wordCount": 733,
          "title": "Variational Image Restoration Network. (arXiv:2008.10796v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iyer_C/0/1/0/all/0/1\">C.V.Krishnakumar Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Feili Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Henry Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yonghong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1\">Kay Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Swetava Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1\">Vipul Pandey</a>",
          "description": "We present a no-code Artificial Intelligence (AI) platform called Trinity\nwith the main design goal of enabling both machine learning researchers and\nnon-technical geospatial domain experts to experiment with domain-specific\nsignals and datasets for solving a variety of complex problems on their own.\nThis versatility to solve diverse problems is achieved by transforming complex\nSpatio-temporal datasets to make them consumable by standard deep learning\nmodels, in this case, Convolutional Neural Networks (CNNs), and giving the\nability to formulate disparate problems in a standard way, eg. semantic\nsegmentation. With an intuitive user interface, a feature store that hosts\nderivatives of complex feature engineering, a deep learning kernel, and a\nscalable data processing mechanism, Trinity provides a powerful platform for\ndomain experts to share the stage with scientists and engineers in solving\nbusiness-critical problems. It enables quick prototyping, rapid experimentation\nand reduces the time to production by standardizing model building and\ndeployment. In this paper, we present our motivation behind Trinity and its\ndesign along with showcasing sample applications to motivate the idea of\nlowering the bar to using AI.",
          "link": "http://arxiv.org/abs/2106.11756",
          "publishedOn": "2021-06-30T02:01:02.188Z",
          "wordCount": 666,
          "title": "Trinity: A No-Code AI platform for complex spatial datasets. (arXiv:2106.11756v4 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>",
          "description": "Compared with cheap addition operation, multiplication operation is of much\nhigher computation complexity. The widely-used convolutions in deep neural\nnetworks are exactly cross-correlation to measure the similarity between input\nfeature and convolution filters, which involves massive multiplications between\nfloat values. In this paper, we present adder networks (AdderNets) to trade\nthese massive multiplications in deep neural networks, especially convolutional\nneural networks (CNNs), for much cheaper additions to reduce computation costs.\nIn AdderNets, we take the $\\ell_1$-norm distance between filters and input\nfeature as the output response. We first develop a theoretical foundation for\nAdderNets, by showing that both the single hidden layer AdderNet and the\nwidth-bounded deep AdderNet with ReLU activation functions are universal\nfunction approximators. An approximation bound for AdderNets with a single\nhidden layer is also presented. We further analyze the influence of this new\nsimilarity measure on the optimization of neural network and develop a special\ntraining scheme for AdderNets. Based on the gradient magnitude, an adaptive\nlearning rate strategy is proposed to enhance the training procedure of\nAdderNets. AdderNets can achieve a 75.7% Top-1 accuracy and a 92.3% Top-5\naccuracy using ResNet-50 on the ImageNet dataset without any multiplication in\nthe convolutional layer.",
          "link": "http://arxiv.org/abs/2105.14202",
          "publishedOn": "2021-06-30T02:01:02.169Z",
          "wordCount": 682,
          "title": "Universal Adder Neural Networks. (arXiv:2105.14202v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidoni_S/0/1/0/all/0/1\">Stefano Ghidoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahnam_S/0/1/0/all/0/1\">Sheryl Brahnam</a>",
          "description": "Features play a crucial role in computer vision. Initially designed to detect\nsalient elements by means of handcrafted algorithms, features are now often\nlearned by different layers in Convolutional Neural Networks (CNNs). This paper\ndevelops a generic computer vision system based on features extracted from\ntrained CNNs. Multiple learned features are combined into a single structure to\nwork on different image classification tasks. The proposed system was\nexperimentally derived by testing several approaches for extracting features\nfrom the inner layers of CNNs and using them as inputs to SVMs that are then\ncombined by sum rule. Dimensionality reduction techniques are used to reduce\nthe high dimensionality of inner layers. The resulting vision system is shown\nto significantly boost the performance of standard CNNs across a large and\ndiverse collection of image data sets. An ensemble of different topologies\nusing the same approach obtains state-of-the-art results on a virus data set.",
          "link": "http://arxiv.org/abs/2104.03488",
          "publishedOn": "2021-06-30T02:01:02.086Z",
          "wordCount": 612,
          "title": "Deep Features for training Support Vector Machine. (arXiv:2104.03488v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14591",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1\">Zihao Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_J/0/1/0/all/0/1\">Jiang Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_C/0/1/0/all/0/1\">Cheng Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongchao Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1\">Jianping Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>",
          "description": "Accurate segmentation of brain tumors from magnetic resonance imaging (MRI)\nis clinically relevant in diagnoses, prognoses and surgery treatment, which\nrequires multiple modalities to provide complementary morphological and\nphysiopathologic information. However, missing modality commonly occurs due to\nimage corruption, artifacts, different acquisition protocols or allergies to\ncertain contrast agents in clinical practice. Though existing efforts\ndemonstrate the possibility of a unified model for all missing situations, most\nof them perform poorly when more than one modality is missing. In this paper,\nwe propose a novel Adversarial Co-training Network (ACN) to solve this issue,\nin which a series of independent yet related models are trained dedicated to\neach missing situation with significantly better results. Specifically, ACN\nadopts a novel co-training network, which enables a coupled learning process\nfor both full modality and missing modality to supplement each other's domain\nand feature representations, and more importantly, to recover the `missing'\ninformation of absent modalities. Then, two unsupervised modules, i.e., entropy\nand knowledge adversarial learning modules are proposed to minimize the domain\ngap while enhancing prediction reliability and encouraging the alignment of\nlatent representations, respectively. We also adapt modality-mutual information\nknowledge transfer learning to ACN to retain the rich mutual information among\nmodalities. Extensive experiments on BraTS2018 dataset show that our proposed\nmethod significantly outperforms all state-of-the-art methods under any missing\nsituation.",
          "link": "http://arxiv.org/abs/2106.14591",
          "publishedOn": "2021-06-30T02:01:02.045Z",
          "wordCount": 700,
          "title": "ACN: Adversarial Co-training Network for Brain Tumor Segmentation with Missing Modalities. (arXiv:2106.14591v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huan_L/0/1/0/all/0/1\">Linxi Huan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xianwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1\">Jianya Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>",
          "description": "This paper presents a context-aware tracing strategy (CATS) for crisp edge\ndetection with deep edge detectors, based on an observation that the\nlocalization ambiguity of deep edge detectors is mainly caused by the mixing\nphenomenon of convolutional neural networks: feature mixing in edge\nclassification and side mixing during fusing side predictions. The CATS\nconsists of two modules: a novel tracing loss that performs feature unmixing by\ntracing boundaries for better side edge learning, and a context-aware fusion\nblock that tackles the side mixing by aggregating the complementary merits of\nlearned side edges. Experiments demonstrate that the proposed CATS can be\nintegrated into modern deep edge detectors to improve localization accuracy.\nWith the vanilla VGG16 backbone, in terms of BSDS500 dataset, our CATS improves\nthe F-measure (ODS) of the RCF and BDCN deep edge detectors by 12% and 6%\nrespectively when evaluating without using the morphological non-maximal\nsuppression scheme for edge detection.",
          "link": "http://arxiv.org/abs/2011.09808",
          "publishedOn": "2021-06-30T02:01:02.040Z",
          "wordCount": 620,
          "title": "Unmixing Convolutional Features for Crisp Edge Detection. (arXiv:2011.09808v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perov_I/0/1/0/all/0/1\">Ivan Perov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Daiheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chervoniy_N/0/1/0/all/0/1\">Nikolay Chervoniy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marangonda_S/0/1/0/all/0/1\">Sugasa Marangonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ume_C/0/1/0/all/0/1\">Chris Um&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dpfks_M/0/1/0/all/0/1\">Mr. Dpfks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facenheim_C/0/1/0/all/0/1\">Carl Shift Facenheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+RP_L/0/1/0/all/0/1\">Luis RP</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Pingyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>",
          "description": "Deepfake defense not only requires the research of detection but also\nrequires the efforts of generation methods. However, current deepfake methods\nsuffer the effects of obscure workflow and poor performance. To solve this\nproblem, we present DeepFaceLab, the current dominant deepfake framework for\nface-swapping. It provides the necessary tools as well as an easy-to-use way to\nconduct high-quality face-swapping. It also offers a flexible and loose\ncoupling structure for people who need to strengthen their pipeline with other\nfeatures without writing complicated boilerplate code. We detail the principles\nthat drive the implementation of DeepFaceLab and introduce its pipeline,\nthrough which every aspect of the pipeline can be modified painlessly by users\nto achieve their customization purpose. It is noteworthy that DeepFaceLab could\nachieve cinema-quality results with high fidelity. We demonstrate the advantage\nof our system by comparing our approach with other face-swapping methods.For\nmore information, please visit:https://github.com/iperov/DeepFaceLab/.",
          "link": "http://arxiv.org/abs/2005.05535",
          "publishedOn": "2021-06-30T02:01:02.035Z",
          "wordCount": 674,
          "title": "DeepFaceLab: Integrated, flexible and extensible face-swapping framework. (arXiv:2005.05535v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huangjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "Contrastive learning (CL) is effective in learning data representations\nwithout label supervision, where the encoder needs to contrast each positive\nsample over multiple negative samples via a one-vs-many softmax cross-entropy\nloss. However, conventional CL is sensitive to how many negative samples are\nincluded and how they are selected. Proposed in this paper is a doubly CL\nstrategy that contrasts positive samples and negative ones within themselves\nseparately. We realize this strategy with contrastive attraction and\ncontrastive repulsion (CACR) makes the query not only exert a greater force to\nattract more distant positive samples but also do so to repel closer negative\nsamples. Theoretical analysis reveals the connection between CACR and CL from\nthe perspectives of both positive attraction and negative repulsion and shows\nthe benefits in both efficiency and robustness brought by separately\ncontrasting within the sampled positive and negative pairs. Extensive\nlarge-scale experiments on standard vision tasks show that CACR not only\nconsistently outperforms existing CL methods on benchmark datasets in\nrepresentation learning, but also provides interpretable contrastive weights,\ndemonstrating the efficacy of the proposed doubly contrastive strategy.",
          "link": "http://arxiv.org/abs/2105.03746",
          "publishedOn": "2021-06-30T02:01:02.024Z",
          "wordCount": 664,
          "title": "Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kelu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>",
          "description": "Adversarial training is one of the most effective approaches to improve model\nrobustness against adversarial examples. However, previous works mainly focus\non the overall robustness of the model, and the in-depth analysis on the role\nof each class involved in adversarial training is still missing. In this paper,\nwe propose to analyze the class-wise robustness in adversarial training. First,\nwe provide a detailed diagnosis of adversarial training on six benchmark\ndatasets, i.e., MNIST, CIFAR-10, CIFAR-100, SVHN, STL-10 and ImageNet.\nSurprisingly, we find that there are remarkable robustness discrepancies among\nclasses, leading to unbalance/unfair class-wise robustness in the robust\nmodels. Furthermore, we keep investigating the relations between classes and\nfind that the unbalanced class-wise robustness is pretty consistent among\ndifferent attack and defense methods. Moreover, we observe that the stronger\nattack methods in adversarial learning achieve performance improvement mainly\nfrom a more successful attack on the vulnerable classes (i.e., classes with\nless robustness). Inspired by these interesting findings, we design a simple\nbut effective attack method based on the traditional PGD attack, named\nTemperature-PGD attack, which proposes to enlarge the robustness disparity\namong classes with a temperature factor on the confidence distribution of each\nimage. Experiments demonstrate our method can achieve a higher attack rate than\nthe PGD attack. Furthermore, from the defense perspective, we also make some\nmodifications in the training and inference phase to improve the robustness of\nthe most vulnerable class, so as to mitigate the large difference in class-wise\nrobustness. We believe our work can contribute to a more comprehensive\nunderstanding of adversarial training as well as rethinking the class-wise\nproperties in robust models.",
          "link": "http://arxiv.org/abs/2105.14240",
          "publishedOn": "2021-06-30T02:01:02.009Z",
          "wordCount": 754,
          "title": "Analysis and Applications of Class-wise Robustness in Adversarial Training. (arXiv:2105.14240v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barenboim_G/0/1/0/all/0/1\">Gabriela Barenboim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirn_J/0/1/0/all/0/1\">Johannes Hirn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanz_V/0/1/0/all/0/1\">Veronica Sanz</a>",
          "description": "We explore whether Neural Networks (NNs) can {\\it discover} the presence of\nsymmetries as they learn to perform a task. For this, we train hundreds of NNs\non a {\\it decoy task} based on well-controlled Physics templates, where no\ninformation on symmetry is provided. We use the output from the last hidden\nlayer of all these NNs, projected to fewer dimensions, as the input for a\nsymmetry classification task, and show that information on symmetry had indeed\nbeen identified by the original NN without guidance. As an interdisciplinary\napplication of this procedure, we identify the presence and level of symmetry\nin artistic paintings from different styles such as those of Picasso, Pollock\nand Van Gogh.",
          "link": "http://arxiv.org/abs/2103.06115",
          "publishedOn": "2021-06-30T02:01:01.980Z",
          "wordCount": 579,
          "title": "Symmetry meets AI. (arXiv:2103.06115v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Numair Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Min H. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1\">James Tompkin</a>",
          "description": "We present a method to estimate dense depth by optimizing a sparse set of\npoints such that their diffusion into a depth map minimizes a multi-view\nreprojection error from RGB supervision. We optimize point positions, depths,\nand weights with respect to the loss by differential splatting that models\npoints as Gaussians with analytic transmittance. Further, we develop an\nefficient optimization routine that can simultaneously optimize the 50k+ points\nrequired for complex scene reconstruction. We validate our routine using ground\ntruth data and show high reconstruction quality. Then, we apply this to light\nfield and wider baseline images via self supervision, and show improvements in\nboth average and outlier error for depth maps diffused from inaccurate sparse\npoints. Finally, we compare qualitative and quantitative results to image\nprocessing and deep learning methods. this http URL",
          "link": "http://arxiv.org/abs/2106.08917",
          "publishedOn": "2021-06-30T02:01:01.953Z",
          "wordCount": 592,
          "title": "Differentiable Diffusion for Dense Depth Estimation from Multi-view Images. (arXiv:2106.08917v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiangkai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yajing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoxian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haozhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyou Zhang</a>",
          "description": "We present a fully automatic system that can produce high-fidelity,\nphoto-realistic 3D digital human heads with a consumer RGB-D selfie camera. The\nsystem only needs the user to take a short selfie RGB-D video while rotating\nhis/her head, and can produce a high quality head reconstruction in less than\n30 seconds. Our main contribution is a new facial geometry modeling and\nreflectance synthesis procedure that significantly improves the\nstate-of-the-art. Specifically, given the input video a two-stage frame\nselection procedure is first employed to select a few high-quality frames for\nreconstruction. Then a differentiable renderer based 3D Morphable Model (3DMM)\nfitting algorithm is applied to recover facial geometries from multiview RGB-D\ndata, which takes advantages of a powerful 3DMM basis constructed with\nextensive data generation and perturbation. Our 3DMM has much larger expressive\ncapacities than conventional 3DMM, allowing us to recover more accurate facial\ngeometry using merely linear basis. For reflectance synthesis, we present a\nhybrid approach that combines parametric fitting and CNNs to synthesize\nhigh-resolution albedo/normal maps with realistic hair/pore/wrinkle details.\nResults show that our system can produce faithful 3D digital human faces with\nextremely realistic details. The main code and the newly constructed 3DMM basis\nis publicly available.",
          "link": "http://arxiv.org/abs/2010.05562",
          "publishedOn": "2021-06-30T02:01:01.935Z",
          "wordCount": 692,
          "title": "High-Fidelity 3D Digital Human Head Creation from RGB-D Selfies. (arXiv:2010.05562v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yuhu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingying Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingzhi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Ning Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Li Shang</a>",
          "description": "This work presents MemX: a biologically-inspired attention-aware eyewear\nsystem developed with the goal of pursuing the long-awaited vision of a\npersonalized visual Memex. MemX captures human visual attention on the fly,\nanalyzes the salient visual content, and records moments of personal interest\nin the form of compact video snippets. Accurate attentive scene detection and\nanalysis on resource-constrained platforms is challenging because these tasks\nare computation and energy intensive. We propose a new temporal visual\nattention network that unifies human visual attention tracking and salient\nvisual content analysis. Attention tracking focuses computation-intensive video\nanalysis on salient regions, while video analysis makes human attention\ndetection and tracking more accurate. Using the YouTube-VIS dataset and 30\nparticipants, we experimentally show that MemX significantly improves the\nattention tracking accuracy over the eye-tracking-alone method, while\nmaintaining high system energy efficiency. We have also conducted 11 in-field\npilot studies across a range of daily usage scenarios, which demonstrate the\nfeasibility and potential benefits of MemX.",
          "link": "http://arxiv.org/abs/2105.00916",
          "publishedOn": "2021-06-30T02:01:01.913Z",
          "wordCount": 680,
          "title": "MemX: An Attention-Aware Smart Eyewear System for Personalized Moment Auto-capture. (arXiv:2105.00916v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gabbay_A/0/1/0/all/0/1\">Aviv Gabbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Niv Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>",
          "description": "Unsupervised disentanglement has been shown to be theoretically impossible\nwithout inductive biases on the models and the data. As an alternative\napproach, recent methods rely on limited supervision to disentangle the factors\nof variation and allow their identifiability. While annotating the true\ngenerative factors is only required for a limited number of observations, we\nargue that it is infeasible to enumerate all the factors of variation that\ndescribe a real-world image distribution. To this end, we propose a method for\ndisentangling a set of factors which are only partially labeled, as well as\nseparating the complementary set of residual factors that are never explicitly\nspecified. Our success in this challenging setting, demonstrated on synthetic\nbenchmarks, gives rise to leveraging off-the-shelf image descriptors to\npartially annotate a subset of attributes in real image domains (e.g. of human\nfaces) with minimal manual effort. Specifically, we use a recent language-image\nembedding model (CLIP) to annotate a set of attributes of interest in a\nzero-shot manner and demonstrate state-of-the-art disentangled image\nmanipulation results.",
          "link": "http://arxiv.org/abs/2106.15610",
          "publishedOn": "2021-06-30T02:01:01.891Z",
          "wordCount": 625,
          "title": "An Image is Worth More Than a Thousand Words: Towards Disentanglement in the Wild. (arXiv:2106.15610v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.13826",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Raffy_P/0/1/0/all/0/1\">Philippe Raffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pambrun_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Pambrun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Ashish Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubois_D/0/1/0/all/0/1\">David Dubois</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patti_J/0/1/0/all/0/1\">Jay Waldron Patti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cairns_R/0/1/0/all/0/1\">Robyn Alexandra Cairns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Young_R/0/1/0/all/0/1\">Ryan Young</a>",
          "description": "Standardized body region labelling of individual images provides data that\ncan improve human and computer use of medical images. A CNN-based classifier\nwas developed to identify body regions in CT and MRI. 17 CT (18 MRI) body\nregions covering the entire human body were defined for the classification\ntask. Three retrospective databases were built for the AI model training,\nvalidation, and testing, with a balanced distribution of studies per body\nregion. The test databases originated from a different healthcare network.\nAccuracy, recall and precision of the classifier was evaluated for patient age,\npatient gender, institution, scanner manufacturer, contrast, slice thickness,\nMRI sequence, and CT kernel. The data included a retrospective cohort of 2,934\nanonymized CT cases (training: 1,804 studies, validation: 602 studies, test:\n528 studies) and 3,185 anonymized MRI cases (training: 1,911 studies,\nvalidation: 636 studies, test: 638 studies). 27 institutions from primary care\nhospitals, community hospitals and imaging centers contributed to the test\ndatasets. The data included cases of all genders in equal proportions and\nsubjects aged from a few months old to +90 years old. An image-level prediction\naccuracy of 91.9% (90.2 - 92.1) for CT, and 94.2% (92.0 - 95.6) for MRI was\nachieved. The classification results were robust across all body regions and\nconfounding factors. Due to limited data, performance results for subjects\nunder 10 years-old could not be reliably evaluated. We show that deep learning\nmodels can classify CT and MRI images by body region including lower and upper\nextremities with high accuracy.",
          "link": "http://arxiv.org/abs/2104.13826",
          "publishedOn": "2021-06-30T02:01:01.885Z",
          "wordCount": 732,
          "title": "Deep Learning Body Region Classification of MRI and CT examinations. (arXiv:2104.13826v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Saem Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Donghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>",
          "description": "Video frame interpolation is the task of creating an interframe between two\nadjacent frames along the time axis. So, instead of simply averaging two\nadjacent frames to create an intermediate image, this operation should maintain\nsemantic continuity with the adjacent frames. Most conventional methods use\noptical flow, and various tools such as occlusion handling and object smoothing\nare indispensable. Since the use of these various tools leads to complex\nproblems, we tried to tackle the video interframe generation problem without\nusing problematic optical flow . To enable this , we have tried to use a deep\nneural network with an invertible structure, and developed an U-Net based\nGenerative Flow which is a modified normalizing flow. In addition, we propose a\nlearning method with a new consistency loss in the latent space to maintain\nsemantic temporal consistency between frames. The resolution of the generated\nimage is guaranteed to be identical to that of the original images by using an\ninvertible network. Furthermore, as it is not a random image like the ones by\ngenerative models, our network guarantees stable outputs without flicker.\nThrough experiments, we \\sam {confirmed the feasibility of the proposed\nalgorithm and would like to suggest the U-Net based Generative Flow as a new\npossibility for baseline in video frame interpolation. This paper is meaningful\nin that it is the world's first attempt to use invertible networks instead of\noptical flows for video interpolation.",
          "link": "http://arxiv.org/abs/2103.09576",
          "publishedOn": "2021-06-30T02:01:01.879Z",
          "wordCount": 718,
          "title": "The U-Net based GLOW for Optical-Flow-free Video Interframe Generation. (arXiv:2103.09576v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenchi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>",
          "description": "Most deep learning models are data-driven and the excellent performance is\nhighly dependent on the abundant and diverse datasets. However, it is very hard\nto obtain and label the datasets of some specific scenes or applications. If we\ntrain the detector using the data from one domain, it cannot perform well on\nthe data from another domain due to domain shift, which is one of the big\nchallenges of most object detection models. To address this issue, some\nimage-to-image translation techniques have been employed to generate some fake\ndata of some specific scenes to train the models. With the advent of Generative\nAdversarial Networks (GANs), we could realize unsupervised image-to-image\ntranslation in both directions from a source to a target domain and from the\ntarget to the source domain. In this study, we report a new approach to making\nuse of the generated images. We propose to concatenate the original 3-channel\nimages and their corresponding GAN-generated fake images to form 6-channel\nrepresentations of the dataset, hoping to address the domain shift problem\nwhile exploiting the success of available detection models. The idea of\naugmented data representation may inspire further study on object detection and\nother applications.",
          "link": "http://arxiv.org/abs/2101.00561",
          "publishedOn": "2021-06-30T02:01:01.863Z",
          "wordCount": 655,
          "title": "Six-channel Image Representation for Cross-domain Object Detection. (arXiv:2101.00561v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shyh_Yaw_J/0/1/0/all/0/1\">Jou Shyh-Yaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Yen_S/0/1/0/all/0/1\">Su Chung-Yen</a>",
          "description": "In the paper of ExquisiteNetV1, the ability of classification of\nExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and\nbetter model ExquisiteNetV2. We conduct many experiments to evaluate its\nperformance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known\nmodels on 15 credible datasets under the same condition. According to the\nexperimental results, ExquisiteNetV2 gets the highest classification accuracy\nover half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts\nof parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing\nspeed.",
          "link": "http://arxiv.org/abs/2105.09008",
          "publishedOn": "2021-06-30T02:01:01.841Z",
          "wordCount": 561,
          "title": "A Novel lightweight Convolutional Neural Network, ExquisiteNetV2. (arXiv:2105.09008v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pranay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Divyanshu Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "We introduce SynSE, a novel syntactically guided generative approach for\nZero-Shot Learning (ZSL). Our end-to-end approach learns progressively refined\ngenerative embedding spaces constrained within and across the involved\nmodalities (visual, language). The inter-modal constraints are defined between\naction sequence embedding and embeddings of Parts of Speech (PoS) tagged words\nin the corresponding action description. We deploy SynSE for the task of\nskeleton-based action sequence recognition. Our design choices enable SynSE to\ngeneralize compositionally, i.e., recognize sequences whose action descriptions\ncontain words not encountered during training. We also extend our approach to\nthe more challenging Generalized Zero-Shot Learning (GZSL) problem via a\nconfidence-based gating mechanism. We are the first to present zero-shot\nskeleton action recognition results on the large-scale NTU-60 and NTU-120\nskeleton action datasets with multiple splits. Our results demonstrate SynSE's\nstate of the art performance in both ZSL and GZSL settings compared to strong\nbaselines on the NTU-60 and NTU-120 datasets. The code and pretrained models\nare available at https://github.com/skelemoa/synse-zsl",
          "link": "http://arxiv.org/abs/2101.11530",
          "publishedOn": "2021-06-30T02:01:01.808Z",
          "wordCount": 646,
          "title": "Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action Recognition. (arXiv:2101.11530v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14844",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yucheng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1\">Seung-Won Jung</a>",
          "description": "Low-light imaging on mobile devices is typically challenging due to\ninsufficient incident light coming through the relatively small aperture,\nresulting in a low signal-to-noise ratio. Most of the previous works on\nlow-light image processing focus either only on a single task such as\nillumination adjustment, color enhancement, or noise removal; or on a joint\nillumination adjustment and denoising task that heavily relies on short-long\nexposure image pairs collected from specific camera models, and thus these\napproaches are less practical and generalizable in real-world settings where\ncamera-specific joint enhancement and restoration is required. To tackle this\nproblem, in this paper, we propose a low-light image processing framework that\nperforms joint illumination adjustment, color enhancement, and denoising.\nConsidering the difficulty in model-specific data collection and the ultra-high\ndefinition of the captured images, we design two branches: a coefficient\nestimation branch as well as a joint enhancement and denoising branch. The\ncoefficient estimation branch works in a low-resolution space and predicts the\ncoefficients for enhancement via bilateral learning, whereas the joint\nenhancement and denoising branch works in a full-resolution space and performs\njoint enhancement and denoising in a progressive manner. In contrast to\nexisting methods, our framework does not need to recollect massive data when\nbeing adapted to another camera model, which significantly reduces the efforts\nrequired to fine-tune our approach for practical usage. Through extensive\nexperiments, we demonstrate its great potential in real-world low-light imaging\napplications when compared with current state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.14844",
          "publishedOn": "2021-06-30T02:01:01.801Z",
          "wordCount": 702,
          "title": "Progressive Joint Low-light Enhancement and Noise Removal for Raw Images. (arXiv:2106.14844v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Savarese_P/0/1/0/all/0/1\">Pedro Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunnie S. Y. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maire_M/0/1/0/all/0/1\">Michael Maire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1\">Greg Shakhnarovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAllester_D/0/1/0/all/0/1\">David McAllester</a>",
          "description": "We study image segmentation from an information-theoretic perspective,\nproposing a novel adversarial method that performs unsupervised segmentation by\npartitioning images into maximally independent sets. More specifically, we\ngroup image pixels into foreground and background, with the goal of minimizing\npredictability of one set from the other. An easily computed loss drives a\ngreedy search process to maximize inpainting error over these partitions. Our\nmethod does not involve training deep networks, is computationally cheap,\nclass-agnostic, and even applicable in isolation to a single unlabeled image.\nExperiments demonstrate that it achieves a new state-of-the-art in unsupervised\nsegmentation quality, while being substantially faster and more general than\ncompeting approaches.",
          "link": "http://arxiv.org/abs/2012.07287",
          "publishedOn": "2021-06-30T02:01:01.783Z",
          "wordCount": 589,
          "title": "Information-Theoretic Segmentation by Inpainting Error Maximization. (arXiv:2012.07287v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yonghao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Ying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun-Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng Ann Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>",
          "description": "Automatic surgical gesture recognition is fundamentally important to enable\nintelligent cognitive assistance in robotic surgery. With recent advancement in\nrobot-assisted minimally invasive surgery, rich information including surgical\nvideos and robotic kinematics can be recorded, which provide complementary\nknowledge for understanding surgical gestures. However, existing methods either\nsolely adopt uni-modal data or directly concatenate multi-modal\nrepresentations, which can not sufficiently exploit the informative\ncorrelations inherent in visual and kinematics data to boost gesture\nrecognition accuracies. In this regard, we propose a novel online approach of\nmulti-modal relational graph network (i.e., MRG-Net) to dynamically integrate\nvisual and kinematics information through interactive message propagation in\nthe latent feature space. In specific, we first extract embeddings from video\nand kinematics sequences with temporal convolutional networks and LSTM units.\nNext, we identify multi-relations in these multi-modal embeddings and leverage\nthem through a hierarchical relational graph learning module. The effectiveness\nof our method is demonstrated with state-of-the-art results on the public\nJIGSAWS dataset, outperforming current uni-modal and multi-modal methods on\nboth suturing and knot typing tasks. Furthermore, we validated our method on\nin-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK)\nplatforms in two centers, with consistent promising performance achieved.",
          "link": "http://arxiv.org/abs/2011.01619",
          "publishedOn": "2021-06-30T02:01:01.777Z",
          "wordCount": 701,
          "title": "Relational Graph Learning on Visual and Kinematics Embeddings for Accurate Gesture Recognition in Robotic Surgery. (arXiv:2011.01619v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.13200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Boxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Compared with cheap addition operation, multiplication operation is of much\nhigher computation complexity. The widely-used convolutions in deep neural\nnetworks are exactly cross-correlation to measure the similarity between input\nfeature and convolution filters, which involves massive multiplications between\nfloat values. In this paper, we present adder networks (AdderNets) to trade\nthese massive multiplications in deep neural networks, especially convolutional\nneural networks (CNNs), for much cheaper additions to reduce computation costs.\nIn AdderNets, we take the $\\ell_1$-norm distance between filters and input\nfeature as the output response. The influence of this new similarity measure on\nthe optimization of neural network have been thoroughly analyzed. To achieve a\nbetter performance, we develop a special back-propagation approach for\nAdderNets by investigating the full-precision gradient. We then propose an\nadaptive learning rate strategy to enhance the training procedure of AdderNets\naccording to the magnitude of each neuron's gradient. As a result, the proposed\nAdderNets can achieve 74.9% Top-1 accuracy 91.7% Top-5 accuracy using ResNet-50\non the ImageNet dataset without any multiplication in convolution layer. The\ncodes are publicly available at: https://github.com/huaweinoah/AdderNet.",
          "link": "http://arxiv.org/abs/1912.13200",
          "publishedOn": "2021-06-30T02:01:01.771Z",
          "wordCount": 685,
          "title": "AdderNet: Do We Really Need Multiplications in Deep Learning?. (arXiv:1912.13200v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajput_G/0/1/0/all/0/1\">Gaurav Rajput</a>, <a href=\"http://arxiv.org/find/cs/1/au:+punn_N/0/1/0/all/0/1\">Narinder Singh punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "With increasing popularity of social media platforms hate speech is emerging\nas a major concern, where it expresses abusive speech that targets specific\ngroup characteristics, such as gender, religion or ethnicity to spread\nviolence. Earlier people use to verbally deliver hate speeches but now with the\nexpansion of technology, some people are deliberately using social media\nplatforms to spread hate by posting, sharing, commenting, etc. Whether it is\nChristchurch mosque shootings or hate crimes against Asians in west, it has\nbeen observed that the convicts are very much influenced from hate text present\nonline. Even though AI systems are in place to flag such text but one of the\nkey challenges is to reduce the false positive rate (marking non hate as hate),\nso that these systems can detect hate speech without undermining the freedom of\nexpression. In this paper, we use ETHOS hate speech detection dataset and\nanalyze the performance of hate speech detection classifier by replacing or\nintegrating the word embeddings (fastText (FT), GloVe (GV) or FT + GV) with\nstatic BERT embeddings (BE). With the extensive experimental trails it is\nobserved that the neural network performed better with static BE compared to\nusing FT, GV or FT + GV as word embeddings. In comparison to fine-tuned BERT,\none metric that significantly improved is specificity.",
          "link": "http://arxiv.org/abs/2106.15537",
          "publishedOn": "2021-06-30T02:01:01.765Z",
          "wordCount": 655,
          "title": "Hate speech detection using static BERT embeddings. (arXiv:2106.15537v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weis_M/0/1/0/all/0/1\">Marissa A. Weis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1\">Kashyap Chitta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ecker_A/0/1/0/all/0/1\">Alexander S. Ecker</a>",
          "description": "Perceiving the world in terms of objects and tracking them through time is a\ncrucial prerequisite for reasoning and scene understanding. Recently, several\nmethods have been proposed for unsupervised learning of object-centric\nrepresentations. However, since these models were evaluated on different\ndownstream tasks, it remains unclear how they compare in terms of basic\nperceptual abilities such as detection, figure-ground segmentation and tracking\nof objects. To close this gap, we design a benchmark with four data sets of\nvarying complexity and seven additional test sets featuring challenging\ntracking scenarios relevant for natural videos. Using this benchmark, we\ncompare the perceptual abilities of four object-centric approaches: ViMON, a\nvideo-extension of MONet, based on recurrent spatial attention, OP3, which\nexploits clustering via spatial mixture models, as well as TBA and SCALOR,\nwhich use explicit factorization via spatial transformers. Our results suggest\nthat the architectures with unconstrained latent representations learn more\npowerful representations in terms of object detection, segmentation and\ntracking than the spatial transformer based architectures. We also observe that\nnone of the methods are able to gracefully handle the most challenging tracking\nscenarios despite their synthetic nature, suggesting that our benchmark may\nprovide fruitful guidance towards learning more robust object-centric video\nrepresentations.",
          "link": "http://arxiv.org/abs/2006.07034",
          "publishedOn": "2021-06-30T02:01:01.759Z",
          "wordCount": 674,
          "title": "Benchmarking Unsupervised Object Representations for Video Sequences. (arXiv:2006.07034v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maanpaa_J/0/1/0/all/0/1\">Jyri Maanp&#xe4;&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taher_J/0/1/0/all/0/1\">Josef Taher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manninen_P/0/1/0/all/0/1\">Petri Manninen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakola_L/0/1/0/all/0/1\">Leo Pakola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melekhov_I/0/1/0/all/0/1\">Iaroslav Melekhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyyppa_J/0/1/0/all/0/1\">Juha Hyypp&#xe4;</a>",
          "description": "Autonomous driving is challenging in adverse road and weather conditions in\nwhich there might not be lane lines, the road might be covered in snow and the\nvisibility might be poor. We extend the previous work on end-to-end learning\nfor autonomous steering to operate in these adverse real-life conditions with\nmultimodal data. We collected 28 hours of driving data in several road and\nweather conditions and trained convolutional neural networks to predict the car\nsteering wheel angle from front-facing color camera images and lidar range and\nreflectance data. We compared the CNN model performances based on the different\nmodalities and our results show that the lidar modality improves the\nperformances of different multimodal sensor-fusion models. We also performed\non-road tests with different models and they support this observation.",
          "link": "http://arxiv.org/abs/2010.14924",
          "publishedOn": "2021-06-30T02:01:01.743Z",
          "wordCount": 638,
          "title": "Multimodal End-to-End Learning for Autonomous Steering in Adverse Road and Weather Conditions. (arXiv:2010.14924v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "The population of elderly people has been increasing at a rapid rate over the\nlast few decades and their population is expected to further increase in the\nupcoming future. Their increasing population is associated with their\nincreasing needs due to problems like physical disabilities, cognitive issues,\nweakened memory and disorganized behavior, that elderly people face with\nincreasing age. To reduce their financial burden on the world economy and to\nenhance their quality of life, it is essential to develop technology-based\nsolutions that are adaptive, assistive and intelligent in nature. Intelligent\nAffect Aware Systems that can not only analyze but also predict the behavior of\nelderly people in the context of their day to day interactions with technology\nin an IoT-based environment, holds immense potential for serving as a long-term\nsolution for improving the user experience of elderly in smart homes. This work\ntherefore proposes the framework for an Intelligent Affect Aware environment\nfor elderly people that can not only analyze the affective components of their\ninteractions but also predict their likely user experience even before they\nstart engaging in any activity in the given smart home environment. This\nforecasting of user experience would provide scope for enhancing the same,\nthereby increasing the assistive and adaptive nature of such intelligent\nsystems. To uphold the efficacy of this proposed framework for improving the\nquality of life of elderly people in smart homes, it has been tested on three\ndatasets and the results are presented and discussed.",
          "link": "http://arxiv.org/abs/2106.15599",
          "publishedOn": "2021-06-30T02:01:01.737Z",
          "wordCount": 723,
          "title": "Framework for an Intelligent Affect Aware Smart Home Environment for Elderly People. (arXiv:2106.15599v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Symeonidis_C/0/1/0/all/0/1\">C. Symeonidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nousi_P/0/1/0/all/0/1\">P. Nousi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tosidis_P/0/1/0/all/0/1\">P. Tosidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsampazis_K/0/1/0/all/0/1\">K. Tsampazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1\">N. Passalis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1\">A. Tefas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaidis_N/0/1/0/all/0/1\">N. Nikolaidis</a>",
          "description": "The performance of supervised deep learning algorithms depends significantly\non the scale, quality and diversity of the data used for their training.\nCollecting and manually annotating large amount of data can be both\ntime-consuming and costly tasks to perform. In the case of tasks related to\nvisual human-centric perception, the collection and distribution of such data\nmay also face restrictions due to legislation regarding privacy. In addition,\nthe design and testing of complex systems, e.g., robots, which often employ\ndeep learning-based perception models, may face severe difficulties as even\nstate-of-the-art methods trained on real and large-scale datasets cannot always\nperform adequately as they have not adapted to the visual differences between\nthe virtual and the real world data. As an attempt to tackle and mitigate the\neffect of these issues, we present a method that automatically generates\nrealistic synthetic data with annotations for a) person detection, b) face\nrecognition, and c) human pose estimation. The proposed method takes as input\nreal background images and populates them with human figures in various poses.\nInstead of using hand-made 3D human models, we propose the use of models\ngenerated through deep learning methods, further reducing the dataset creation\ncosts, while maintaining a high level of realism. In addition, we provide\nopen-source and easy to use tools that implement the proposed pipeline,\nallowing for generating highly-realistic synthetic datasets for a variety of\ntasks. A benchmarking and evaluation in the corresponding tasks shows that\nsynthetic data can be effectively used as a supplement to real data.",
          "link": "http://arxiv.org/abs/2106.15409",
          "publishedOn": "2021-06-30T02:01:01.585Z",
          "wordCount": 700,
          "title": "Efficient Realistic Data Generation Framework leveraging Deep Learning-based Human Digitization. (arXiv:2106.15409v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoppe_A/0/1/0/all/0/1\">Anett Hoppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1\">David Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>",
          "description": "Illustrations are widely used in education, and sometimes, alternatives are\nnot available for visually impaired students. Therefore, those students would\nbenefit greatly from an automatic illustration description system, but only if\nthose descriptions were complete, correct, and easily understandable using a\nscreenreader. In this paper, we report on a study for the assessment of\nautomated image descriptions. We interviewed experts to establish evaluation\ncriteria, which we then used to create an evaluation questionnaire for sighted\nnon-expert raters, and description templates. We used this questionnaire to\nevaluate the quality of descriptions which could be generated with a\ntemplate-based automatic image describer. We present evidence that these\ntemplates have the potential to generate useful descriptions, and that the\nquestionnaire identifies problems with description templates.",
          "link": "http://arxiv.org/abs/2106.15553",
          "publishedOn": "2021-06-30T02:01:01.578Z",
          "wordCount": 635,
          "title": "Evaluation of Automated Image Descriptions for Visually Impaired Students. (arXiv:2106.15553v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/1908.01931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuhua Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinyan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Y/0/1/0/all/0/1\">Yanhong She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiye Liang</a>",
          "description": "Logic reasoning is a significant ability of human intelligence and also an\nimportant task in artificial intelligence. The existing logic reasoning\nmethods, quite often, need to design some reasoning patterns beforehand. This\nhas led to an interesting question: can logic reasoning patterns be directly\nlearned from given data? The problem is termed as a data concept logic. In this\nstudy, a learning logic task from images, called a LiLi task, first is\nproposed. This task is to learn and reason the logic relation from images,\nwithout presetting any reasoning patterns. As a preliminary exploration, we\ndesign six LiLi data sets (Bitwise And, Bitwise Or, Bitwise Xor, Addition,\nSubtraction and Multiplication), in which each image is embedded with a n-digit\nnumber. It is worth noting that a learning model beforehand does not know the\nmeaning of the n-digit numbers embedded in images and the relation between the\ninput images and the output image. In order to tackle the task, in this work we\nuse many typical neural network models and produce fruitful results. However,\nthese models have the poor performances on the difficult logic task. For\nfurthermore addressing this task, a novel network framework called a divide and\nconquer model by adding some label information is designed, achieving a high\ntesting accuracy.",
          "link": "http://arxiv.org/abs/1908.01931",
          "publishedOn": "2021-06-30T02:01:01.546Z",
          "wordCount": 676,
          "title": "Logic could be learned from images. (arXiv:1908.01931v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1\">Anthony Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan M. Lavista Ferres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Brandon Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>",
          "description": "Longitudinal studies are vital to understanding dynamic changes of the\nplanet, but labels (e.g., buildings, facilities, roads) are often available\nonly for a single point in time. We propose a general model, Temporal Cluster\nMatching (TCM), for detecting building changes in time series of remotely\nsensed imagery when footprint labels are observed only once. The intuition\nbehind the model is that the relationship between spectral values inside and\noutside of building's footprint will change when a building is constructed (or\ndemolished). For instance, in rural settings, the pre-construction area may\nlook similar to the surrounding environment until the building is constructed.\nSimilarly, in urban settings, the pre-construction areas will look different\nfrom the surrounding environment until construction. We further propose a\nheuristic method for selecting the parameters of our model which allows it to\nbe applied in novel settings without requiring data labeling efforts (to fit\nthe parameters). We apply our model over a dataset of poultry barns from\n2016/2017 high-resolution aerial imagery in the Delmarva Peninsula and a\ndataset of solar farms from a 2020 mosaic of Sentinel 2 imagery in India. Our\nresults show that our model performs as well when fit using the proposed\nheuristic as it does when fit with labeled data, and further, that supervised\nversions of our model perform the best among all the baselines we test against.\nFinally, we show that our proposed approach can act as an effective data\naugmentation strategy -- it enables researchers to augment existing structure\nfootprint labels along the time dimension and thus use imagery from multiple\npoints in time to train deep learning models. We show that this improves the\nspatial generalization of such models when evaluated on the same change\ndetection task.",
          "link": "http://arxiv.org/abs/2103.09787",
          "publishedOn": "2021-06-30T02:01:01.521Z",
          "wordCount": 765,
          "title": "Temporal Cluster Matching for Change Detection of Structures from Satellite Imagery. (arXiv:2103.09787v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingtian Zhu</a>",
          "description": "3D reconstruction has lately attracted increasing attention due to its wide\napplication in many areas, such as autonomous driving, robotics and virtual\nreality. As a dominant technique in artificial intelligence, deep learning has\nbeen successfully adopted to solve various computer vision problems. However,\ndeep learning for 3D reconstruction is still at its infancy due to its unique\nchallenges and varying pipelines. To stimulate future research, this paper\npresents a review of recent progress in deep learning methods for Multi-view\nStereo (MVS), which is considered as a crucial task of image-based 3D\nreconstruction. It also presents comparative results on several publicly\navailable datasets, with insightful observations and inspiring future research\ndirections.",
          "link": "http://arxiv.org/abs/2106.15328",
          "publishedOn": "2021-06-30T02:01:01.499Z",
          "wordCount": 543,
          "title": "Deep Learning for Multi-View Stereo via Plane Sweep: A Survey. (arXiv:2106.15328v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yan San Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_V/0/1/0/all/0/1\">Varsha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soh_J/0/1/0/all/0/1\">Jonathan Soh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_D/0/1/0/all/0/1\">Desmond C. Ong</a>",
          "description": "Facial Expression Recognition is a commercially important application, but\none common limitation is that applications often require making predictions on\nout-of-sample distributions, where target images may have very different\nproperties from the images that the model was trained on. How well, or badly,\ndo these models do on unseen target domains? In this paper, we provide a\nsystematic evaluation of domain adaptation in facial expression recognition.\nUsing state-of-the-art transfer learning techniques and six commonly-used\nfacial expression datasets (three collected in the lab and three\n\"in-the-wild\"), we conduct extensive round-robin experiments to examine the\nclassification accuracies for a state-of-the-art CNN model. We also perform\nmulti-source experiments where we examine a model's ability to transfer from\nmultiple source datasets, including (i) within-setting (e.g., lab to lab), (ii)\ncross-setting (e.g., in-the-wild to lab), (iii) mixed-setting (e.g., lab and\nwild to lab) transfer learning experiments. We find sobering results that the\naccuracy of transfer learning is not high, and varies idiosyncratically with\nthe target dataset, and to a lesser extent the source dataset. Generally, the\nbest settings for transfer include fine-tuning the weights of a pre-trained\nmodel, and we find that training with more datasets, regardless of setting,\nimproves transfer performance. We end with a discussion of the need for more --\nand regular -- systematic investigations into the generalizability of FER\nmodels, especially for deployed applications.",
          "link": "http://arxiv.org/abs/2106.15453",
          "publishedOn": "2021-06-30T02:01:01.475Z",
          "wordCount": 668,
          "title": "A Systematic Evaluation of Domain Adaptation in Facial Expression Recognition. (arXiv:2106.15453v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juan Liu</a>",
          "description": "Recently, deep learning methods have been proposed for quantitative\nsusceptibility mapping (QSM) data processing: background field removal,\nfield-to-source inversion, and single-step QSM reconstruction. However, the\nconventional padding mechanism used in convolutional neural networks (CNNs) can\nintroduce spatial artifacts, especially in QSM background field removal and\nsingle-step QSM which requires inference from total fields with extreme large\nvalues at the edge boundaries of volume of interest. To address this issue, we\npropose an improved padding technique which utilizes the neighboring valid\nvoxels to estimate the invalid voxels of feature maps at volume boundaries in\nthe neural networks. Studies using simulated and in-vivo data show that the\nproposed padding greatly improves estimation accuracy and reduces artifacts in\nthe results in the tasks of background field removal, field-to-source\ninversion, and single-step QSM reconstruction.",
          "link": "http://arxiv.org/abs/2106.15331",
          "publishedOn": "2021-06-30T02:01:01.470Z",
          "wordCount": 566,
          "title": "Improved Padding in CNNs for Quantitative Susceptibility Mapping. (arXiv:2106.15331v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dewen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yukun Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qiu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruixue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hongwen Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Meiping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jian Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>",
          "description": "Most existing deep learning-based frameworks for image segmentation assume\nthat a unique ground truth is known and can be used for performance evaluation.\nThis is true for many applications, but not all. Myocardial segmentation of\nMyocardial Contrast Echocardiography (MCE), a critical task in automatic\nmyocardial perfusion analysis, is an example. Due to the low resolution and\nserious artifacts in MCE data, annotations from different cardiologists can\nvary significantly, and it is hard to tell which one is the best. In this case,\nhow can we find a good way to evaluate segmentation performance and how do we\ntrain the neural network? In this paper, we address the first problem by\nproposing a new extended Dice to effectively evaluate the segmentation\nperformance when multiple accepted ground truth is available. Then based on our\nproposed metric, we solve the second problem by further incorporating the new\nmetric into a loss function that enables neural networks to flexibly learn\ngeneral features of myocardium. Experiment results on our clinical MCE data set\ndemonstrate that the neural network trained with the proposed loss function\noutperforms those existing ones that try to obtain a unique ground truth from\nmultiple annotations, both quantitatively and qualitatively. Finally, our\ngrading study shows that using extended Dice as an evaluation metric can better\nidentify segmentation results that need manual correction compared with using\nDice.",
          "link": "http://arxiv.org/abs/2106.15597",
          "publishedOn": "2021-06-30T02:01:01.464Z",
          "wordCount": 687,
          "title": "Segmentation with Multiple Acceptable Annotations: A Case Study of Myocardial Segmentation in Contrast Echocardiography. (arXiv:2106.15597v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Taofeng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Beihong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Beibei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinzhou Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">He Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Wenhai Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuejian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuo Zhou</a>",
          "description": "Interactions between users and videos are the major data source of performing\nvideo recommendation. Despite lots of existing recommendation methods, user\nbehaviors on videos, which imply the complex relations between users and\nvideos, are still far from being fully explored. In the paper, we present a\nmodel named Sagittarius. Sagittarius adopts a graph convolutional neural\nnetwork to capture the influence between users and videos. In particular,\nSagittarius differentiates between different user behaviors by weighting and\nfuses the semantics of user behaviors into the embeddings of users and videos.\nMoreover, Sagittarius combines multiple optimization objectives to learn user\nand video embeddings and then achieves the video recommendation by the learned\nuser and video embeddings. The experimental results on multiple datasets show\nthat Sagittarius outperforms several state-of-the-art models in terms of\nrecall, unique recall and NDCG.",
          "link": "http://arxiv.org/abs/2106.15402",
          "publishedOn": "2021-06-30T02:01:01.459Z",
          "wordCount": 584,
          "title": "A Behavior-aware Graph Convolution Network Model for Video Recommendation. (arXiv:2106.15402v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1\">Anthony Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughey_L/0/1/0/all/0/1\">Lacey Hughey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stabach_J/0/1/0/all/0/1\">Jared A. Stabach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan M. Lavista Ferres</a>",
          "description": "Localizing and counting large ungulates -- hoofed mammals like cows and elk\n-- in very high-resolution satellite imagery is an important task for\nsupporting ecological studies. Prior work has shown that this is feasible with\ndeep learning based methods and sub-meter multi-spectral satellite imagery. We\nextend this line of work by proposing a baseline method, CowNet, that\nsimultaneously estimates the number of animals in an image (counts), as well as\npredicts their location at a pixel level (localizes). We also propose an\nmethodology for evaluating such models on counting and localization tasks\nacross large scenes that takes the uncertainty of noisy labels and the\ninformation needed by stakeholders in ecological monitoring tasks into account.\nFinally, we benchmark our baseline method with state of the art vision methods\nfor counting objects in scenes. We specifically test the temporal\ngeneralization of the resulting models over a large landscape in Point Reyes\nSeashore, CA. We find that the LC-FCN model performs the best and achieves an\naverage precision between 0.56 and 0.61 and an average recall between 0.78 and\n0.92 over three held out test scenes.",
          "link": "http://arxiv.org/abs/2106.15448",
          "publishedOn": "2021-06-30T02:01:01.454Z",
          "wordCount": 638,
          "title": "Detecting Cattle and Elk in the Wild from Space. (arXiv:2106.15448v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Laiyan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>",
          "description": "3D semantic scene completion and 2D semantic segmentation are two tightly\ncorrelated tasks that are both essential for indoor scene understanding,\nbecause they predict the same semantic classes, using positively correlated\nhigh-level features. Current methods use 2D features extracted from early-fused\nRGB-D images for 2D segmentation to improve 3D scene completion. We argue that\nthis sequential scheme does not ensure these two tasks fully benefit each\nother, and present an Iterative Mutual Enhancement Network (IMENet) to solve\nthem jointly, which interactively refines the two tasks at the late prediction\nstage. Specifically, two refinement modules are developed under a unified\nframework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP)\nmodule, which receives the projection from the current 3D predictions to refine\nthe 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is\nproposed to leverage the reprojected results from 2D predictions to update the\ncoarse 3D predictions. This iterative fusion happens to the stable high-level\nfeatures of both tasks at a late stage. Extensive experiments on NYU and NYUCAD\ndatasets verify the effectiveness of the proposed iterative late fusion scheme,\nand our approach outperforms the state of the art on both 3D semantic scene\ncompletion and 2D semantic segmentation.",
          "link": "http://arxiv.org/abs/2106.15413",
          "publishedOn": "2021-06-30T02:01:01.449Z",
          "wordCount": 656,
          "title": "IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement. (arXiv:2106.15413v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matsumori_S/0/1/0/all/0/1\">Shoya Matsumori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shingyouchi_K/0/1/0/all/0/1\">Kosuke Shingyouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_Y/0/1/0/all/0/1\">Yuki Abe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuchi_Y/0/1/0/all/0/1\">Yosuke Fukuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imai_M/0/1/0/all/0/1\">Michita Imai</a>",
          "description": "Building an interactive artificial intelligence that can ask questions about\nthe real world is one of the biggest challenges for vision and language\nproblems. In particular, goal-oriented visual dialogue, where the aim of the\nagent is to seek information by asking questions during a turn-taking dialogue,\nhas been gaining scholarly attention recently. While several existing models\nbased on the GuessWhat?! dataset have been proposed, the Questioner typically\nasks simple category-based questions or absolute spatial questions. This might\nbe problematic for complex scenes where the objects share attributes or in\ncases where descriptive questions are required to distinguish objects. In this\npaper, we propose a novel Questioner architecture, called Unified Questioner\nTransformer (UniQer), for descriptive question generation with referring\nexpressions. In addition, we build a goal-oriented visual dialogue task called\nCLEVR Ask. It synthesizes complex scenes that require the Questioner to\ngenerate descriptive questions. We train our model with two variants of CLEVR\nAsk datasets. The results of the quantitative and qualitative evaluations show\nthat UniQer outperforms the baseline.",
          "link": "http://arxiv.org/abs/2106.15550",
          "publishedOn": "2021-06-30T02:01:01.444Z",
          "wordCount": 614,
          "title": "Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue. (arXiv:2106.15550v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingjie Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhiquan Qi</a>",
          "description": "Numerous detection problems in computer vision, including road crack\ndetection, suffer from exceedingly foreground-background imbalance.\nFortunately, modification of loss function appears to solve this puzzle once\nand for all. In this paper, we propose a pixel-based adaptive weighted\ncross-entropy loss in conjunction with Jaccard distance to facilitate\nhigh-quality pixel-level road crack detection. Our work profoundly demonstrates\nthe influence of loss functions on detection outcomes, and sheds light on the\nsophisticated consecutive improvements in the realm of crack detection.\nSpecifically, to verify the effectiveness of the proposed loss, we conduct\nextensive experiments on four public databases, i.e., CrackForest, AigleRN,\nCrack360, and BJN260. Compared with the vanilla weighted cross-entropy, the\nproposed loss significantly speeds up the training process while retaining the\ntest accuracy.",
          "link": "http://arxiv.org/abs/2106.15510",
          "publishedOn": "2021-06-30T02:01:01.426Z",
          "wordCount": 564,
          "title": "Fast and Accurate Road Crack Detection Based on Adaptive Cost-Sensitive Loss Function. (arXiv:2106.15510v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanal_B/0/1/0/all/0/1\">Bidur Khanal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>",
          "description": "Incorrectly labeled examples, or label noise, is common in real-world\ncomputer vision datasets. While the impact of label noise on learning in deep\nneural networks has been studied in prior work, these studies have exclusively\nfocused on homogeneous label noise, i.e., the degree of label noise is the same\nacross all categories. However, in the real-world, label noise is often\nheterogeneous, with some categories being affected to a greater extent than\nothers. Here, we address this gap in the literature. We hypothesized that\nheterogeneous label noise would only affect the classes that had label noise\nunless there was transfer from those classes to the classes without label\nnoise. To test this hypothesis, we designed a series of computer vision studies\nusing MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous\nlabel noise during the training of multi-class, multi-task, and multi-label\nsystems. Our results provide evidence in support of our hypothesis: label noise\nonly affects the class affected by it unless there is transfer.",
          "link": "http://arxiv.org/abs/2106.15475",
          "publishedOn": "2021-06-30T02:01:01.407Z",
          "wordCount": 598,
          "title": "How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?. (arXiv:2106.15475v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1903.04855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gou_C/0/1/0/all/0/1\">Chao Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenbo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Huadan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1\">Qiang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhengyu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei-Yue Wang</a>",
          "description": "There has been much progress in data-driven artificial intelligence\ntechnology for medical image analysis in the last decades. However, it still\nremains challenging due to its distinctive complexity of acquiring and\nannotating image data, extracting medical domain knowledge, and explaining the\ndiagnostic decision for medical image analysis. In this paper, we propose a\ndata-knowledge-driven framework termed as Parallel Medical Imaging (PMI) for\nintelligent medical image analysis based on the methodology of interactive\nACP-based parallel intelligence. In the PMI framework, computational\nexperiments with predictive learning in a data-driven way are conducted to\nextract medical knowledge for diagnostic decision support. Artificial imaging\nsystems are introduced to select and prescriptively generate medical image data\nin a knowledge-driven way to utilize medical domain knowledge. Through the\nclosed-loop optimization based on parallel execution, our proposed PMI\nframework can boost the generalization ability and alleviate the limitation of\nmedical interpretation for diagnostic decisions. Furthermore, we illustrate the\npreliminary implementation of PMI method through the case studies of mammogram\nanalysis and skin lesion image analysis. Experimental results on several public\nmedical image datasets demonstrate the effectiveness of proposed PMI.",
          "link": "http://arxiv.org/abs/1903.04855",
          "publishedOn": "2021-06-30T02:01:01.396Z",
          "wordCount": 672,
          "title": "Parallel Medical Imaging for Intelligent Medical Image Analysis: Concepts, Methods, and Applications. (arXiv:1903.04855v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15575",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Awate_S/0/1/0/all/0/1\">Suyash Awate</a>",
          "description": "Deep neural networks for image quality enhancement typically need large\nquantities of highly-curated training data comprising pairs of low-quality\nimages and their corresponding high-quality images. While high-quality image\nacquisition is typically expensive and time-consuming, medium-quality images\nare faster to acquire, at lower equipment costs, and available in larger\nquantities. Thus, we propose a novel generative adversarial network (GAN) that\ncan leverage training data at multiple levels of quality (e.g., high and medium\nquality) to improve performance while limiting costs of data curation. We apply\nour mixed-supervision GAN to (i) super-resolve histopathology images and (ii)\nenhance laparoscopy images by combining super-resolution and surgical smoke\nremoval. Results on large clinical and pre-clinical datasets show the benefits\nof our mixed-supervision GAN over the state of the art.",
          "link": "http://arxiv.org/abs/2106.15575",
          "publishedOn": "2021-06-30T02:01:01.380Z",
          "wordCount": 573,
          "title": "A Mixed-Supervision Multilevel GAN Framework for Image Quality Enhancement. (arXiv:2106.15575v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15379",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1\">Benyamin Ghojogh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1\">Mark Crowley</a>",
          "description": "This is a tutorial and survey paper on unification of spectral dimensionality\nreduction methods, kernel learning by Semidefinite Programming (SDP), Maximum\nVariance Unfolding (MVU) or Semidefinite Embedding (SDE), and its variants. We\nfirst explain how the spectral dimensionality reduction methods can be unified\nas kernel Principal Component Analysis (PCA) with different kernels. This\nunification can be interpreted as eigenfunction learning or representation of\nkernel in terms of distance matrix. Then, since the spectral methods are\nunified as kernel PCA, we say let us learn the best kernel for unfolding the\nmanifold of data to its maximum variance. We first briefly introduce kernel\nlearning by SDP for the transduction task. Then, we explain MVU in detail.\nVarious versions of supervised MVU using nearest neighbors graph, by class-wise\nunfolding, by Fisher criterion, and by colored MVU are explained. We also\nexplain out-of-sample extension of MVU using eigenfunctions and kernel mapping.\nFinally, we introduce other variants of MVU including action respecting\nembedding, relaxed MVU, and landmark MVU for big data.",
          "link": "http://arxiv.org/abs/2106.15379",
          "publishedOn": "2021-06-30T02:01:01.367Z",
          "wordCount": 644,
          "title": "Unified Framework for Spectral Dimensionality Reduction, Maximum Variance Unfolding, and Kernel Learning By Semidefinite Programming: Tutorial and Survey. (arXiv:2106.15379v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanbei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hepp_T/0/1/0/all/0/1\">Tobias Hepp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>",
          "description": "Image-to-image translation plays a vital role in tackling various medical\nimaging tasks such as attenuation correction, motion correction, undersampled\nreconstruction, and denoising. Generative adversarial networks have been shown\nto achieve the state-of-the-art in generating high fidelity images for these\ntasks. However, the state-of-the-art GAN-based frameworks do not estimate the\nuncertainty in the predictions made by the network that is essential for making\ninformed medical decisions and subsequent revision by medical experts and has\nrecently been shown to improve the performance and interpretability of the\nmodel. In this work, we propose an uncertainty-guided progressive learning\nscheme for image-to-image translation. By incorporating aleatoric uncertainty\nas attention maps for GANs trained in a progressive manner, we generate images\nof increasing fidelity progressively. We demonstrate the efficacy of our model\non three challenging medical image translation tasks, including PET to CT\ntranslation, undersampled MRI reconstruction, and MRI motion artefact\ncorrection. Our model generalizes well in three different tasks and improves\nperformance over state of the art under full-supervision and weak-supervision\nwith limited data. Code is released here:\nhttps://github.com/ExplainableML/UncerGuidedI2I",
          "link": "http://arxiv.org/abs/2106.15542",
          "publishedOn": "2021-06-30T02:01:01.361Z",
          "wordCount": 634,
          "title": "Uncertainty-Guided Progressive GANs for Medical Image Translation. (arXiv:2106.15542v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1904.03936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fatras_K/0/1/0/all/0/1\">Kilian Fatras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1\">Bharath Bhushan Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobry_S/0/1/0/all/0/1\">Sylvain Lobry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1\">R&#xe9;mi Flamary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1\">Devis Tuia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1\">Nicolas Courty</a>",
          "description": "Noisy labels often occur in vision datasets, especially when they are\nobtained from crowdsourcing or Web scraping. We propose a new regularization\nmethod, which enables learning robust classifiers in presence of noisy data. To\nachieve this goal, we propose a new adversarial regularization scheme based on\nthe Wasserstein distance. Using this distance allows taking into account\nspecific relations between classes by leveraging the geometric properties of\nthe labels space. Our Wasserstein Adversarial Regularization (WAR) encodes a\nselective regularization, which promotes smoothness of the classifier between\nsome classes, while preserving sufficient complexity of the decision boundary\nbetween others. We first discuss how and why adversarial regularization can be\nused in the context of label noise and then show the effectiveness of our\nmethod on five datasets corrupted with noisy labels: in both benchmarks and\nreal datasets, WAR outperforms the state-of-the-art competitors.",
          "link": "http://arxiv.org/abs/1904.03936",
          "publishedOn": "2021-06-30T02:01:01.347Z",
          "wordCount": 638,
          "title": "Wasserstein Adversarial Regularization (WAR) on label noise. (arXiv:1904.03936v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yixuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xianbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xianbiao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>",
          "description": "TextVQA requires models to read and reason about text in images to answer\nquestions about them. Specifically, models need to incorporate a new modality\nof text present in the images and reason over it to answer TextVQA questions.\nIn this challenge, we use generative model T5 for TextVQA task. Based on\npre-trained checkpoint T5-3B from HuggingFace repository, two other\npre-training tasks including masked language modeling(MLM) and relative\nposition prediction(RPP) are designed to better align object feature and scene\ntext. In the stage of pre-training, encoder is dedicate to handle the fusion\namong multiple modalities: question text, object text labels, scene text\nlabels, object visual features, scene visual features. After that decoder\ngenerates the text sequence step-by-step, cross entropy loss is required by\ndefault. We use a large-scale scene text dataset in pre-training and then\nfine-tune the T5-3B with the TextVQA dataset only.",
          "link": "http://arxiv.org/abs/2106.15332",
          "publishedOn": "2021-06-30T02:01:01.341Z",
          "wordCount": 604,
          "title": "Winner Team Mia at TextVQA Challenge 2021: Vision-and-Language Representation Learning with Pre-trained Sequence-to-Sequence Model. (arXiv:2106.15332v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15395",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyang Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+shi_J/0/1/0/all/0/1\">Jun shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>",
          "description": "The thick-slice magnetic resonance (MR) images are often structurally blurred\nin coronal and sagittal views, which causes harm to diagnosis and image\npost-processing. Deep learning (DL) has shown great potential to re-construct\nthe high-resolution (HR) thin-slice MR images from those low-resolution (LR)\ncases, which we refer to as the slice interpolation task in this work. However,\nsince it is generally difficult to sample abundant paired LR-HR MR images, the\nclassical fully supervised DL-based models cannot be effectively trained to get\nrobust performance. To this end, we propose a novel Two-stage Self-supervised\nCycle-consistency Network (TSCNet) for MR slice interpolation, in which a\ntwo-stage self-supervised learning (SSL) strategy is developed for unsupervised\nDL network training. The paired LR-HR images are synthesized along the sagittal\nand coronal directions of input LR images for network pretraining in the\nfirst-stage SSL, and then a cyclic in-terpolation procedure based on triplet\naxial slices is designed in the second-stage SSL for further refinement. More\ntraining samples with rich contexts along all directions are exploited as\nguidance to guarantee the improved in-terpolation performance. Moreover, a new\ncycle-consistency constraint is proposed to supervise this cyclic procedure,\nwhich encourages the network to reconstruct more realistic HR images. The\nexperimental results on a real MRI dataset indicate that TSCNet achieves\nsuperior performance over the conventional and other SSL-based algorithms, and\nobtains competitive quali-tative and quantitative results compared with the\nfully supervised algorithm.",
          "link": "http://arxiv.org/abs/2106.15395",
          "publishedOn": "2021-06-30T02:01:01.336Z",
          "wordCount": 681,
          "title": "Two-Stage Self-Supervised Cycle-Consistency Network for Reconstruction of Thin-Slice MR Images. (arXiv:2106.15395v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Quanxue Gao</a>",
          "description": "Graph-based multi-view clustering has become an active topic due to the\nefficiency in characterizing both the complex structure and relationship\nbetween multimedia data. However, existing methods have the following\nshortcomings: (1) They are inefficient or even fail for graph learning in large\nscale due to the graph construction and eigen-decomposition. (2) They cannot\nwell exploit both the complementary information and spatial structure embedded\nin graphs of different views. To well exploit complementary information and\ntackle the scalability issue plaguing graph-based multi-view clustering, we\npropose an efficient multiple graph learning model via a small number of anchor\npoints and tensor Schatten p-norm minimization. Specifically, we construct a\nhidden and tractable large graph by anchor graph for each view and well exploit\ncomplementary information embedded in anchor graphs of different views by\ntensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm,\nwhich scales linearly with the data size, to solve our proposed model.\nExtensive experimental results on several datasets indicate that our proposed\nmethod outperforms some state-of-the-art multi-view clustering algorithms.",
          "link": "http://arxiv.org/abs/2106.15382",
          "publishedOn": "2021-06-30T02:01:01.328Z",
          "wordCount": 603,
          "title": "Multiple Graph Learning for Scalable Multi-view Clustering. (arXiv:2106.15382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cong Ma</a>",
          "description": "TANet is one of state-of-the-art 3D object detection method on KITTI and JRDB\nbenchmark, the network contains a Triple Attention module and Coarse-to-Fine\nRegression module to improve the robustness and accuracy of 3D Detection.\nHowever, since the original input data (point clouds) contains a lot of noise\nduring collecting the data, which will further affect the training of the\nmodel. For example, the object is far from the robot, the sensor is difficult\nto obtain enough pointcloud. If the objects only contains few point clouds, and\nthe samples are fed into model with the normal samples together during\ntraining, the detector will be difficult to distinguish the individual with few\npointcloud belong to object or background. In this paper, we propose TANet++ to\nimprove the performance on 3D Detection, which adopt a novel training strategy\non training the TANet. In order to reduce the negative impact by the weak\nsamples, the training strategy previously filtered the training data, and then\nthe TANet++ is trained by the rest of data. The experimental results shows that\nAP score of TANet++ is 8.98\\% higher than TANet on JRDB benchmark.",
          "link": "http://arxiv.org/abs/2106.15366",
          "publishedOn": "2021-06-30T02:01:01.313Z",
          "wordCount": 623,
          "title": "TANet++: Triple Attention Network with Filtered Pointcloud on 3D Detection. (arXiv:2106.15366v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuanqi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1\">Quan Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>",
          "description": "Pseudo-normality synthesis, which computationally generates a pseudo-normal\nimage from an abnormal one (e.g., with lesions), is critical in many\nperspectives, from lesion detection, data augmentation to clinical surgery\nsuggestion. However, it is challenging to generate high-quality pseudo-normal\nimages in the absence of the lesion information. Thus, expensive lesion\nsegmentation data have been introduced to provide lesion information for the\ngenerative models and improve the quality of the synthetic images. In this\npaper, we aim to alleviate the need of a large amount of lesion segmentation\ndata when generating pseudo-normal images. We propose a Semi-supervised Medical\nImage generative LEarning network (SMILE) which not only utilizes limited\nmedical images with segmentation masks, but also leverages massive medical\nimages without segmentation masks to generate realistic pseudo-normal images.\nExtensive experiments show that our model outperforms the best state-of-the-art\nmodel by up to 6% for data augmentation task and 3% in generating high-quality\nimages. Moreover, the proposed semi-supervised learning achieves comparable\nmedical image synthesis quality with supervised learning model, using only 50\nof segmentation data.",
          "link": "http://arxiv.org/abs/2106.15345",
          "publishedOn": "2021-06-30T02:01:01.307Z",
          "wordCount": 623,
          "title": "Where is the disease? Semi-supervised pseudo-normality synthesis from an abnormal image. (arXiv:2106.15345v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotariya_V/0/1/0/all/0/1\">Vineet Kotariya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_U/0/1/0/all/0/1\">Udayan Ganguly</a>",
          "description": "Spiking Neural Networks (SNNs) have shown great potential in solving deep\nlearning problems in an energy-efficient manner. However, they are still\nlimited to simple classification tasks. In this paper, we propose Spiking-GAN,\nthe first spike-based Generative Adversarial Network (GAN). It employs a kind\nof temporal coding scheme called time-to-first-spike coding. We train it using\napproximate backpropagation in the temporal domain. We use simple\nintegrate-and-fire (IF) neurons with very high refractory period for our\nnetwork which ensures a maximum of one spike per neuron. This makes the model\nmuch sparser than a spike rate-based system. Our modified temporal loss\nfunction called 'Aggressive TTFS' improves the inference time of the network by\nover 33% and reduces the number of spikes in the network by more than 11%\ncompared to previous works. Our experiments show that on training the network\non the MNIST dataset using this approach, we can generate high quality samples.\nThereby demonstrating the potential of this framework for solving such problems\nin the spiking domain.",
          "link": "http://arxiv.org/abs/2106.15420",
          "publishedOn": "2021-06-30T02:01:01.302Z",
          "wordCount": 610,
          "title": "Spiking-GAN: A Spiking Generative Adversarial Network Using Time-To-First-Spike Coding. (arXiv:2106.15420v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chuanbiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>",
          "description": "In the field of adversarial robustness, there is a common practice that\nadopts the single-step adversarial training for quickly developing\nadversarially robust models. However, the single-step adversarial training is\nmost likely to cause catastrophic overfitting, as after a few training epochs\nit will be hard to generate strong adversarial examples to continuously boost\nthe adversarial robustness. In this work, we aim to avoid the catastrophic\noverfitting by introducing multi-step adversarial examples during the\nsingle-step adversarial training. Then, to balance the large training overhead\nof generating multi-step adversarial examples, we propose a Multi-stage\nOptimization based Adversarial Training (MOAT) method that periodically trains\nthe model on mixed benign examples, single-step adversarial examples, and\nmulti-step adversarial examples stage by stage. In this way, the overall\ntraining overhead is reduced significantly, meanwhile, the model could avoid\ncatastrophic overfitting. Extensive experiments on CIFAR-10 and CIFAR-100\ndatasets demonstrate that under similar amount of training overhead, the\nproposed MOAT exhibits better robustness than either single-step or multi-step\nadversarial training methods.",
          "link": "http://arxiv.org/abs/2106.15357",
          "publishedOn": "2021-06-30T02:01:01.286Z",
          "wordCount": 597,
          "title": "Multi-stage Optimization based Adversarial Training. (arXiv:2106.15357v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dogaru_R/0/1/0/all/0/1\">Radu Dogaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogaru_I/0/1/0/all/0/1\">Ioana Dogaru</a>",
          "description": "Light binary convolutional neural networks (LB-CNN) are particularly useful\nwhen implemented in low-energy computing platforms as required in many\nindustrial applications. Herein, a framework for optimizing compact LB-CNN is\nintroduced and its effectiveness is evaluated. The framework is freely\navailable and may run on free-access cloud platforms, thus requiring no major\ninvestments. The optimized model is saved in the standardized .h5 format and\ncan be used as input to specialized tools for further deployment into specific\ntechnologies, thus enabling the rapid development of various intelligent image\nsensors. The main ingredient in accelerating the optimization of our model,\nparticularly the selection of binary convolution kernels, is the Chainer/Cupy\nmachine learning library offering significant speed-ups for training the output\nlayer as an extreme-learning machine. Additional training of the output layer\nusing Keras/Tensorflow is included, as it allows an increase in accuracy.\nResults for widely used datasets including MNIST, GTSRB, ORL, VGG show very\ngood compromise between accuracy and complexity. Particularly, for face\nrecognition problems a carefully optimized LB-CNN model provides up to 100%\naccuracies. Such TinyML solutions are well suited for industrial applications\nrequiring image recognition with low energy consumption.",
          "link": "http://arxiv.org/abs/2106.15350",
          "publishedOn": "2021-06-30T02:01:01.276Z",
          "wordCount": 657,
          "title": "LB-CNN: An Open Source Framework for Fast Training of Light Binary Convolutional Neural Networks using Chainer and Cupy. (arXiv:2106.15350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gabbur_P/0/1/0/all/0/1\">Prasad Gabbur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilkhu_M/0/1/0/all/0/1\">Manjot Bilkhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Movellan_J/0/1/0/all/0/1\">Javier Movellan</a>",
          "description": "We provide a probabilistic interpretation of attention and show that the\nstandard dot-product attention in transformers is a special case of Maximum A\nPosteriori (MAP) inference. The proposed approach suggests the use of\nExpectation Maximization algorithms for online adaptation of key and value\nmodel parameters. This approach is useful for cases in which external agents,\ne.g., annotators, provide inference-time information about the correct values\nof some tokens, e.g, the semantic category of some pixels, and we need for this\nnew information to propagate to other tokens in a principled manner. We\nillustrate the approach on an interactive semantic segmentation task in which\nannotators and models collaborate online to improve annotation efficiency.\nUsing standard benchmarks, we observe that key adaptation boosts model\nperformance ($\\sim10\\%$ mIoU) in the low feedback regime and value propagation\nimproves model responsiveness in the high feedback regime. A PyTorch layer\nimplementation of our probabilistic attention model will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2106.15338",
          "publishedOn": "2021-06-30T02:01:01.260Z",
          "wordCount": 602,
          "title": "Probabilistic Attention for Interactive Segmentation. (arXiv:2106.15338v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1\">Amit Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhanotiya_J/0/1/0/all/0/1\">Jitendra Dhanotiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_V/0/1/0/all/0/1\">Vimal Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Shashi Prakash</a>",
          "description": "Main visual techniques used to obtain information from speckle patterns are\nFujii method, generalized difference, weighted generalized difference, mean\nwindowed difference, structural function (SF), modified SF, etc. In this work,\na comparative analysis of major visual techniques for natural gum sample is\ncarried out. Obtained results conclusively establish SF based method as an\noptimum tool for visual inspection of dynamic speckle data.",
          "link": "http://arxiv.org/abs/2106.15507",
          "publishedOn": "2021-06-30T02:01:01.254Z",
          "wordCount": 508,
          "title": "Study of visual processing techniques for dynamic speckles: a comparative analysis. (arXiv:2106.15507v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "Scene text image super-resolution (STISR) aims to improve the resolution and\nvisual quality of low-resolution (LR) scene text images, and consequently boost\nthe performance of text recognition. However, most of existing STISR methods\nregard text images as natural scene images, ignoring the categorical\ninformation of text. In this paper, we make an inspiring attempt to embed\ncategorical text prior into STISR model training. Specifically, we adopt the\ncharacter probability sequence as the text prior, which can be obtained\nconveniently from a text recognition model. The text prior provides categorical\nguidance to recover high-resolution (HR) text images. On the other hand, the\nreconstructed HR image can refine the text prior in return. Finally, we present\na multi-stage text prior guided super-resolution (TPGSR) framework for STISR.\nOur experiments on the benchmark TextZoom dataset show that TPGSR can not only\neffectively improve the visual quality of scene text images, but also\nsignificantly improve the text recognition accuracy over existing STISR\nmethods. Our model trained on TextZoom also demonstrates certain generalization\ncapability to the LR images in other datasets.",
          "link": "http://arxiv.org/abs/2106.15368",
          "publishedOn": "2021-06-30T02:01:01.232Z",
          "wordCount": 607,
          "title": "Text Prior Guided Scene Text Image Super-resolution. (arXiv:2106.15368v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumakoshi_Y/0/1/0/all/0/1\">Yusuke Kumakoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoda_S/0/1/0/all/0/1\">Shigeaki Onoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_T/0/1/0/all/0/1\">Tetsuya Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshimura_Y/0/1/0/all/0/1\">Yuji Yoshimura</a>",
          "description": "The disorder of urban streetscapes would negatively affect people's\nperception of their aesthetic quality. The presence of billboards on building\nfacades has been regarded as an important factor of the disorder, but its\nquantification methodology has not yet been developed in a scalable manner. To\nfill the gap, this paper reports the performance of our deep learning model on\na unique data set prepared in Tokyo to recognize the areas covered by facades\nand billboards in streetscapes, respectively. The model achieved 63.17 % of\naccuracy, measured by Intersection-over-Union (IoU), thus enabling researchers\nand practitioners to obtain insights on urban streetscape design by combining\ndata of people's preferences.",
          "link": "http://arxiv.org/abs/2106.15361",
          "publishedOn": "2021-06-30T02:01:01.226Z",
          "wordCount": 556,
          "title": "Quantifying urban streetscapes with deep learning: focus on aesthetic evaluation. (arXiv:2106.15361v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasata_D/0/1/0/all/0/1\">Daniel Va&#x161;ata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halama_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Halama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedjungova_M/0/1/0/all/0/1\">Magda Friedjungov&#xe1;</a>",
          "description": "Image inpainting is one of the important tasks in computer vision which\nfocuses on the reconstruction of missing regions in an image. The aim of this\npaper is to introduce an image inpainting model based on Wasserstein Generative\nAdversarial Imputation Network. The generator network of the model uses\nbuilding blocks of convolutional layers with different dilation rates, together\nwith skip connections that help the model reproduce fine details of the output.\nThis combination yields a universal imputation model that is able to handle\nvarious scenarios of missingness with sufficient quality. To show this\nexperimentally, the model is simultaneously trained to deal with three\nscenarios given by missing pixels at random, missing various smaller square\nregions, and one missing square placed in the center of the image. It turns out\nthat our model achieves high-quality inpainting results on all scenarios.\nPerformance is evaluated using peak signal-to-noise ratio and structural\nsimilarity index on two real-world benchmark datasets, CelebA faces and Paris\nStreetView. The results of our model are compared to biharmonic imputation and\nto some of the other state-of-the-art image inpainting methods.",
          "link": "http://arxiv.org/abs/2106.15341",
          "publishedOn": "2021-06-30T02:01:01.195Z",
          "wordCount": 634,
          "title": "Image Inpainting Using Wasserstein Generative Adversarial Imputation Network. (arXiv:2106.15341v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhen Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongbin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_S/0/1/0/all/0/1\">Shuaicheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanxia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>",
          "description": "We study a practical domain adaptation task, called source-free unsupervised\ndomain adaptation (UDA) problem, in which we cannot access source domain data\ndue to data privacy issues but only a pre-trained source model and unlabeled\ntarget data are available. This task, however, is very difficult due to one key\nchallenge: the lack of source data and target domain labels makes model\nadaptation very challenging. To address this, we propose to mine the hidden\nknowledge in the source model and exploit it to generate source avatar\nprototypes (i.e., representative features for each source class) as well as\ntarget pseudo labels for domain alignment. To this end, we propose a\nContrastive Prototype Generation and Adaptation (CPGA) method. Specifically,\nCPGA consists of two stages: (1) prototype generation: by exploring the\nclassification boundary information of the source model, we train a prototype\ngenerator to generate avatar prototypes via contrastive learning. (2) prototype\nadaptation: based on the generated source prototypes and target pseudo labels,\nwe develop a new robust contrastive prototype adaptation strategy to align each\npseudo-labeled target data to the corresponding source prototypes. Extensive\nexperiments on three UDA benchmark datasets demonstrate the effectiveness and\nsuperiority of the proposed method.",
          "link": "http://arxiv.org/abs/2106.15326",
          "publishedOn": "2021-06-30T02:01:01.179Z",
          "wordCount": 644,
          "title": "Source-free Domain Adaptation via Avatar Prototype Generation and Adaptation. (arXiv:2106.15326v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1\">Nathan Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1\">Durga Sivasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dani_A/0/1/0/all/0/1\">Apurva Dani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "With the goal of making deep learning more label-efficient, a growing number\nof papers have been studying active learning (AL) for deep models. However,\nthere are a number of issues in the prevalent experimental settings, mainly\nstemming from a lack of unified implementation and benchmarking. Issues in the\ncurrent literature include sometimes contradictory observations on the\nperformance of different AL algorithms, unintended exclusion of important\ngeneralization approaches such as data augmentation and SGD for optimization, a\nlack of study of evaluation facets like the labeling efficiency of AL, and\nlittle or no clarity on the scenarios in which AL outperforms random sampling\n(RS). In this work, we present a unified re-implementation of state-of-the-art\nAL algorithms in the context of image classification, and we carefully study\nthese issues as facets of effective evaluation. On the positive side, we show\nthat AL techniques are 2x to 4x more label-efficient compared to RS with the\nuse of data augmentation. Surprisingly, when data augmentation is included,\nthere is no longer a consistent gain in using BADGE, a state-of-the-art\napproach, over simple uncertainty sampling. We then do a careful analysis of\nhow existing approaches perform with varying amounts of redundancy and number\nof examples per class. Finally, we provide several insights for AL\npractitioners to consider in future work, such as the effect of the AL batch\nsize, the effect of initialization, the importance of retraining a new model at\nevery round, and other insights.",
          "link": "http://arxiv.org/abs/2106.15324",
          "publishedOn": "2021-06-30T02:01:01.173Z",
          "wordCount": 726,
          "title": "Effective Evaluation of Deep Active Learning on Image Classification Tasks. (arXiv:2106.15324v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_D/0/1/0/all/0/1\">Dan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLaughlin_S/0/1/0/all/0/1\">Stephen McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altmann_Y/0/1/0/all/0/1\">Yoann Altmann</a>",
          "description": "This paper presents a new Expectation Propagation (EP) framework for image\nrestoration using patch-based prior distributions. While Monte Carlo techniques\nare classically used to sample from intractable posterior distributions, they\ncan suffer from scalability issues in high-dimensional inference problems such\nas image restoration. To address this issue, EP is used here to approximate the\nposterior distributions using products of multivariate Gaussian densities.\nMoreover, imposing structural constraints on the covariance matrices of these\ndensities allows for greater scalability and distributed computation. While the\nmethod is naturally suited to handle additive Gaussian observation noise, it\ncan also be extended to non-Gaussian noise. Experiments conducted for\ndenoising, inpainting and deconvolution problems with Gaussian and Poisson\nnoise illustrate the potential benefits of such flexible approximate Bayesian\nmethod for uncertainty quantification in imaging problems, at a reduced\ncomputational cost compared to sampling techniques.",
          "link": "http://arxiv.org/abs/2106.15327",
          "publishedOn": "2021-06-30T02:01:01.168Z",
          "wordCount": 571,
          "title": "Patch-Based Image Restoration using Expectation Propagation. (arXiv:2106.15327v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yaseen_M/0/1/0/all/0/1\">Muhammad Usman Yaseen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum_A/0/1/0/all/0/1\">Ashiq Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortino_G/0/1/0/all/0/1\">Giancarlo Fortino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liotta_A/0/1/0/all/0/1\">Antonio Liotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1\">Amir Hussain</a>",
          "description": "Object recognition from live video streams comes with numerous challenges\nsuch as the variation in illumination conditions and poses. Convolutional\nneural networks (CNNs) have been widely used to perform intelligent visual\nobject recognition. Yet, CNNs still suffer from severe accuracy degradation,\nparticularly on illumination-variant datasets. To address this problem, we\npropose a new CNN method based on orientation fusion for visual object\nrecognition. The proposed cloud-based video analytics system pioneers the use\nof bi-dimensional empirical mode decomposition to split a video frame into\nintrinsic mode functions (IMFs). We further propose these IMFs to endure Reisz\ntransform to produce monogenic object components, which are in turn used for\nthe training of CNNs. Past works have demonstrated how the object orientation\ncomponent may be used to pursue accuracy levels as high as 93\\%. Herein we\ndemonstrate how a feature-fusion strategy of the orientation components leads\nto further improving visual recognition accuracy to 97\\%. We also assess the\nscalability of our method, looking at both the number and the size of the video\nstreams under scrutiny. We carry out extensive experimentation on the publicly\navailable Yale dataset, including also a self generated video datasets, finding\nsignificant improvements (both in accuracy and scale), in comparison to\nAlexNet, LeNet and SE-ResNeXt, which are the three most commonly used deep\nlearning models for visual object recognition and classification.",
          "link": "http://arxiv.org/abs/2106.15329",
          "publishedOn": "2021-06-30T02:01:01.163Z",
          "wordCount": 675,
          "title": "Cloud based Scalable Object Recognition from Video Streams using Orientation Fusion and Convolutional Neural Networks. (arXiv:2106.15329v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1\">Abdelrahman Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berendeyev_A/0/1/0/all/0/1\">Alexander Berendeyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuradin_I/0/1/0/all/0/1\">Islam Nuradin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurseitov_D/0/1/0/all/0/1\">Daniyar Nurseitov</a>",
          "description": "We present TNCR, a new table dataset with varying image quality collected\nfrom free websites. The TNCR dataset can be used for table detection in scanned\ndocument images and their classification into 5 different classes. TNCR\ncontains 9428 high-quality labeled images. In this paper, we have implemented\nstate-of-the-art deep learning-based methods for table detection to create\nseveral strong baselines. Cascade Mask R-CNN with ResNeXt-101-64x4d Backbone\nNetwork achieves the highest performance compared to other methods with a\nprecision of 79.7%, recall of 89.8%, and f1 score of 84.4% on the TNCR dataset.\nWe have made TNCR open source in the hope of encouraging more deep learning\napproaches to table detection, classification, and structure recognition. The\ndataset and trained model checkpoints are available at\nhttps://github.com/abdoelsayed2016/TNCR_Dataset.",
          "link": "http://arxiv.org/abs/2106.15322",
          "publishedOn": "2021-06-30T02:01:01.156Z",
          "wordCount": 562,
          "title": "TNCR: Table Net Detection and Classification Dataset. (arXiv:2106.15322v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hafiz_A/0/1/0/all/0/1\">Abdul Mueed Hafiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Rouf Ul Alam Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parah_S/0/1/0/all/0/1\">Shabir Ahmad Parah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassaballah_M/0/1/0/all/0/1\">M. Hassaballah</a>",
          "description": "3D model generation from single 2D RGB images is a challenging and actively\nresearched computer vision task. Various techniques using conventional network\narchitectures have been proposed for the same. However, the body of research\nwork is limited and there are various issues like using inefficient 3D\nrepresentation formats, weak 3D model generation backbones, inability to\ngenerate dense point clouds, dependence of post-processing for generation of\ndense point clouds, and dependence on silhouettes in RGB images. In this paper,\na novel 2D RGB image to point cloud conversion technique is proposed, which\nimproves the state of art in the field due to its efficient, robust and simple\nmodel by using the concept of parallelization in network architecture. It not\nonly uses the efficient and rich 3D representation of point clouds, but also\nuses a novel and robust point cloud generation backbone in order to address the\nprevalent issues. This involves using a single-encoder multiple-decoder deep\nnetwork architecture wherein each decoder generates certain fixed viewpoints.\nThis is followed by fusing all the viewpoints to generate a dense point cloud.\nVarious experiments are conducted on the technique and its performance is\ncompared with those of other state of the art techniques and impressive gains\nin performance are demonstrated. Code is available at\nhttps://github.com/mueedhafiz1982/",
          "link": "http://arxiv.org/abs/2106.15325",
          "publishedOn": "2021-06-30T02:01:01.138Z",
          "wordCount": 672,
          "title": "SE-MD: A Single-encoder multiple-decoder deep network for point cloud generation from 2D images. (arXiv:2106.15325v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marti_Puig_P/0/1/0/all/0/1\">Pere Marti-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caiafa_C/0/1/0/all/0/1\">Cesar F. Caiafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhe Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_F/0/1/0/all/0/1\">Feng Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sole_Casals_J/0/1/0/all/0/1\">Jordi Sol&#xe9;-Casals</a>",
          "description": "Empirical mode decomposition (EMD) has developed into a prominent tool for\nadaptive, scale-based signal analysis in various fields like robotics, security\nand biomedical engineering. Since the dramatic increase in amount of data puts\nforward higher requirements for the capability of real-time signal analysis, it\nis difficult for existing EMD and its variants to trade off the growth of data\ndimension and the speed of signal analysis. In order to decompose\nmulti-dimensional signals at a faster speed, we present a novel\nsignal-serialization method (serial-EMD), which concatenates multi-variate or\nmulti-dimensional signals into a one-dimensional signal and uses various\none-dimensional EMD algorithms to decompose it. To verify the effects of the\nproposed method, synthetic multi-variate time series, artificial 2D images with\nvarious textures and real-world facial images are tested. Compared with\nexisting multi-EMD algorithms, the decomposition time becomes significantly\nreduced. In addition, the results of facial recognition with Intrinsic Mode\nFunctions (IMFs) extracted using our method can achieve a higher accuracy than\nthose obtained by existing multi-EMD algorithms, which demonstrates the\nsuperior performance of our method in terms of the quality of IMFs.\nFurthermore, this method can provide a new perspective to optimize the existing\nEMD algorithms, that is, transforming the structure of the input signal rather\nthan being constrained by developing envelope computation techniques or signal\ndecomposition methods. In summary, the study suggests that the serial-EMD\ntechnique is a highly competitive and fast alternative for multi-dimensional\nsignal analysis.",
          "link": "http://arxiv.org/abs/2106.15319",
          "publishedOn": "2021-06-30T02:01:01.133Z",
          "wordCount": 692,
          "title": "Serial-EMD: Fast Empirical Mode Decomposition Method for Multi-dimensional Signals Based on Serialization. (arXiv:2106.15319v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruijters_D/0/1/0/all/0/1\">Daniel Ruijters</a>",
          "description": "Minimally invasive image guided treatment procedures often employ advanced\nimage processing algorithms. The recent developments of artificial intelligence\nalgorithms harbor potential to further enhance this domain. In this article we\nexplore several application areas within the minimally invasive treatment space\nand discuss the deployment of artificial intelligence within these areas.",
          "link": "http://arxiv.org/abs/2106.15306",
          "publishedOn": "2021-06-30T02:01:01.128Z",
          "wordCount": 496,
          "title": "Artificial Intelligence in Minimally Invasive Interventional Treatment. (arXiv:2106.15306v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeckeln_G/0/1/0/all/0/1\">G&#xe9;raldine Jeckeln</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Ying Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavazos_J/0/1/0/all/0/1\">Jacqueline G. Cavazos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Amy N. Yates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_C/0/1/0/all/0/1\">Carina A. Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Larry Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jonathon Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OToole_A/0/1/0/all/0/1\">Alice J. O&#x27;Toole</a>",
          "description": "Measures of face identification proficiency are essential to ensure accurate\nand consistent performance by professional forensic face examiners and others\nwho perform face identification tasks in applied scenarios. Current proficiency\ntests rely on static sets of stimulus items, and so, cannot be administered\nvalidly to the same individual multiple times. To create a proficiency test, a\nlarge number of items of \"known\" difficulty must be assembled. Multiple tests\nof equal difficulty can be constructed then using subsets of items. Here, we\nintroduce a proficiency test, the Triad Identity Matching (TIM) test, based on\nstimulus difficulty measures based on Item Response Theory (IRT). Participants\nview face-image \"triads\" (N=225) (two images of one identity and one image of a\ndifferent identity) and select the different identity. In Experiment 1,\nuniversity students (N=197) showed wide-ranging accuracy on the TIM test.\nFurthermore, IRT modeling demonstrated that the TIM test produces items of\nvarious difficulty levels. In Experiment 2, IRT-based item difficulty measures\nwere used to partition the TIM test into three equally \"easy\" and three equally\n\"difficult\" subsets. Simulation results indicated that the full set, as well as\ncurated subsets, of the TIM items yielded reliable estimates of subject\nability. In summary, the TIM test can provide a starting point for developing a\nframework that is flexible, calibrated, and adaptive to measure proficiency\nacross various ability levels (e.g., professionals or populations with face\nprocessing deficits)",
          "link": "http://arxiv.org/abs/2106.15323",
          "publishedOn": "2021-06-30T02:01:01.122Z",
          "wordCount": 692,
          "title": "Face Identification Proficiency Test Designed Using Item Response Theory. (arXiv:2106.15323v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1\">Neil Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netravali_R/0/1/0/all/0/1\">Ravi Netravali</a>",
          "description": "Delivering fast responses to retrospective queries on video datasets is\ndifficult due to the large number of frames to consider and the high costs of\nrunning convolutional neural networks (CNNs) on each one. A natural solution is\nto perform a subset of the necessary computations ahead of time, as video is\ningested. However, existing ingest-time systems require knowledge of the\nspecific CNN that will be used in future queries -- a challenging requisite\ngiven the evergrowing space of CNN architectures and training\ndatasets/methodologies.\n\nThis paper presents Boggart, a retrospective video analytics system that\ndelivers ingest-time speedups in a model-agnostic manner. Our underlying\ninsight is that traditional computer vision (CV) algorithms are capable of\nperforming computations that can be used to accelerate diverse queries with\nwide-ranging CNNs. Building on this, at ingest-time, Boggart carefully employs\na variety of motion tracking algorithms to identify potential objects and their\ntrajectories across frames. Then, at query-time, Boggart uses several novel\ntechniques to collect the smallest sample of CNN results required to meet the\ntarget accuracy: (1) a clustering strategy to efficiently unearth the\ninevitable discrepancies between CV- and CNN-generated outputs, and (2) a set\nof accuracy-preserving propagation techniques to safely extend sampled results\nalong each trajectory. Across many videos, CNNs, and queries Boggart\nconsistently meets accuracy targets while using CNNs sparingly (on 3-54% of\nframes).",
          "link": "http://arxiv.org/abs/2106.15315",
          "publishedOn": "2021-06-30T02:01:01.117Z",
          "wordCount": 665,
          "title": "Boggart: Accelerating Retrospective Video Analytics via Model-Agnostic Ingest Processing. (arXiv:2106.15315v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_C/0/1/0/all/0/1\">Carrie Lu Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jian Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianming Yang</a>",
          "description": "Deep learning models for human activity recognition (HAR) based on sensor\ndata have been heavily studied recently. However, the generalization ability of\ndeep models on complex real-world HAR data is limited by the availability of\nhigh-quality labeled activity data, which are hard to obtain. In this paper, we\ndesign a similarity embedding neural network that maps input sensor signals\nonto real vectors through carefully designed convolutional and LSTM layers. The\nembedding network is trained with a pairwise similarity loss, encouraging the\nclustering of samples from the same class in the embedded real space, and can\nbe effectively trained on a small dataset and even on a noisy dataset with\nmislabeled samples. Based on the learned embeddings, we further propose both\nnonparametric and parametric approaches for activity recognition. Extensive\nevaluation based on two public datasets has shown that the proposed similarity\nembedding network significantly outperforms state-of-the-art deep models on HAR\nclassification tasks, is robust to mislabeled samples in the training set, and\ncan also be used to effectively denoise a noisy dataset.",
          "link": "http://arxiv.org/abs/2106.15283",
          "publishedOn": "2021-06-30T02:01:01.111Z",
          "wordCount": 626,
          "title": "Similarity Embedding Networks for Robust Human Activity Recognition. (arXiv:2106.15283v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolf_J/0/1/0/all/0/1\">Jan Niklas Kolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1\">Kiran Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengcheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montero_D/0/1/0/all/0/1\">David Montero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aginako_N/0/1/0/all/0/1\">Naiara Aginako</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sierra_B/0/1/0/all/0/1\">Basilio Sierra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_M/0/1/0/all/0/1\">Marcos Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erakin_M/0/1/0/all/0/1\">Mustafa Ekrem Erakin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1\">Ugur Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemal_H/0/1/0/all/0/1\">Hazim Kemal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel/0/1/0/all/0/1\">Ekenel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kataoka_A/0/1/0/all/0/1\">Asaki Kataoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichikawa_K/0/1/0/all/0/1\">Kohei Ichikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubo_S/0/1/0/all/0/1\">Shizuma Kubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingjie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grm_K/0/1/0/all/0/1\">Klemen Grm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struc_V/0/1/0/all/0/1\">Vitomir &#x160;truc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_S/0/1/0/all/0/1\">Sachith Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasthuriarachchi_N/0/1/0/all/0/1\">Nuran Kasthuriarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasnayaka_S/0/1/0/all/0/1\">Sanka Rasnayaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_P/0/1/0/all/0/1\">Pedro C. Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sequeira_A/0/1/0/all/0/1\">Ana F. Sequeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1\">Joao Ribeiro Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_M/0/1/0/all/0/1\">Mohsen Saffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1\">Jaime S. Cardoso</a>",
          "description": "This paper presents a summary of the Masked Face Recognition Competitions\n(MFR) held within the 2021 International Joint Conference on Biometrics (IJCB\n2021). The competition attracted a total of 10 participating teams with valid\nsubmissions. The affiliations of these teams are diverse and associated with\nacademia and industry in nine different countries. These teams successfully\nsubmitted 18 valid solutions. The competition is designed to motivate solutions\naiming at enhancing the face recognition accuracy of masked faces. Moreover,\nthe competition considered the deployability of the proposed solutions by\ntaking the compactness of the face recognition models into account. A private\ndataset representing a collaborative, multi-session, real masked, capture\nscenario is used to evaluate the submitted solutions. In comparison to one of\nthe top-performing academic face recognition solutions, 10 out of the 18\nsubmitted solutions did score higher masked face verification accuracy.",
          "link": "http://arxiv.org/abs/2106.15288",
          "publishedOn": "2021-06-30T02:01:01.095Z",
          "wordCount": 649,
          "title": "MFR 2021: Masked Face Recognition Competition. (arXiv:2106.15288v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kahu_S/0/1/0/all/0/1\">Sampanna Yashwant Kahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingram_W/0/1/0/all/0/1\">William A. Ingram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1\">Edward A. Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>",
          "description": "We focus on electronic theses and dissertations (ETDs), aiming to improve\naccess and expand their utility, since more than 6 million are publicly\navailable, and they constitute an important corpus to aid research and\neducation across disciplines. The corpus is growing as new born-digital\ndocuments are included, and since millions of older theses and dissertations\nhave been converted to digital form to be disseminated electronically in\ninstitutional repositories. In ETDs, as with other scholarly works, figures and\ntables can communicate a large amount of information in a concise way. Although\nmethods have been proposed for extracting figures and tables from born-digital\nPDFs, they do not work well with scanned ETDs. Considering this problem, our\nassessment of state-of-the-art figure extraction systems is that the reason\nthey do not function well on scanned PDFs is that they have only been trained\non born-digital documents. To address this limitation, we present ScanBank, a\nnew dataset containing 10 thousand scanned page images, manually labeled by\nhumans as to the presence of the 3.3 thousand figures or tables found therein.\nWe use this dataset to train a deep neural network model based on YOLOv5 to\naccurately extract figures and tables from scanned ETDs. We pose and answer\nimportant research questions aimed at finding better methods for figure\nextraction from scanned documents. One of those concerns the value for\ntraining, of data augmentation techniques applied to born-digital documents\nwhich are used to train models better suited for figure extraction from scanned\ndocuments. To the best of our knowledge, ScanBank is the first manually\nannotated dataset for figure and table extraction for scanned ETDs. A\nYOLOv5-based model, trained on ScanBank, outperforms existing comparable\nopen-source and freely available baseline methods by a considerable margin.",
          "link": "http://arxiv.org/abs/2106.15320",
          "publishedOn": "2021-06-30T02:01:01.089Z",
          "wordCount": 756,
          "title": "ScanBank: A Benchmark Dataset for Figure Extraction from Scanned Electronic Theses and Dissertations. (arXiv:2106.15320v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1\">Arthur Douillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yifu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1\">Arnaud Dapogny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>",
          "description": "Deep learning approaches are nowadays ubiquitously used to tackle computer\nvision tasks such as semantic segmentation, requiring large datasets and\nsubstantial computational power. Continual learning for semantic segmentation\n(CSS) is an emerging trend that consists in updating an old model by\nsequentially adding new classes. However, continual learning methods are\nusually prone to catastrophic forgetting. This issue is further aggravated in\nCSS where, at each step, old classes from previous iterations are collapsed\ninto the background. In this paper, we propose Local POD, a multi-scale pooling\ndistillation scheme that preserves long- and short-range spatial relationships\nat feature level. Furthermore, we design an entropy-based pseudo-labelling of\nthe background w.r.t. classes predicted by the old model to deal with\nbackground shift and avoid catastrophic forgetting of the old classes. Finally,\nwe introduce a novel rehearsal method that is particularly suited for\nsegmentation. Our approach, called PLOP, significantly outperforms\nstate-of-the-art methods in existing CSS scenarios, as well as in newly\nproposed challenging benchmarks.",
          "link": "http://arxiv.org/abs/2106.15287",
          "publishedOn": "2021-06-30T02:01:01.083Z",
          "wordCount": 609,
          "title": "Tackling Catastrophic Forgetting and Background Shift in Continual Semantic Segmentation. (arXiv:2106.15287v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boucaud_L/0/1/0/all/0/1\">Laurent Boucaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloise_D/0/1/0/all/0/1\">Daniel Aloise</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1\">Nicolas Saunier</a>",
          "description": "We consider the problem of predicting the future path of a pedestrian using\nits motion history and the motion history of the surrounding pedestrians,\ncalled social information. Since the seminal paper on Social-LSTM,\ndeep-learning has become the main tool used to model the impact of social\ninteractions on a pedestrian's motion. The demonstration that these models can\nlearn social interactions relies on an ablative study of these models. The\nmodels are compared with and without their social interactions module on two\nstandard metrics, the Average Displacement Error and Final Displacement Error.\nYet, these complex models were recently outperformed by a simple\nconstant-velocity approach. This questions if they actually allow to model\nsocial interactions as well as the validity of the proof. In this paper, we\nfocus on the deep-learning models with a soft-attention mechanism for social\ninteraction modeling and study whether they use social information at\nprediction time. We conduct two experiments across four state-of-the-art\napproaches on the ETH and UCY datasets, which were also used in previous work.\nFirst, the models are trained by replacing the social information with random\nnoise and compared to model trained with actual social information. Second, we\nuse a gating mechanism along with a $L_0$ penalty, allowing models to shut down\ntheir inner components. The models consistently learn to prune their\nsoft-attention mechanism. For both experiments, neither the course of the\nconvergence nor the prediction performance were altered. This demonstrates that\nthe soft-attention mechanism and therefore the social information are ignored\nby the models.",
          "link": "http://arxiv.org/abs/2106.15321",
          "publishedOn": "2021-06-30T02:01:01.078Z",
          "wordCount": 726,
          "title": "Soft Attention: Does it Actually Help to Learn Social Interactions in Pedestrian Trajectory Prediction?. (arXiv:2106.15321v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiesong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>",
          "description": "Automatically evaluating the quality of image captions can be very\nchallenging since human language is quite flexible that there can be various\nexpressions for the same meaning. Most of the current captioning metrics rely\non token level matching between candidate caption and the ground truth label\nsentences. It usually neglects the sentence-level information. Motivated by the\nauto-encoder mechanism and contrastive representation learning advances, we\npropose a learning-based metric for image captioning, which we call Intrinsic\nImage Captioning Evaluation($I^2CE$). We develop three progressive model\nstructures to learn the sentence level representations--single branch model,\ndual branches model, and triple branches model. Our empirical tests show that\n$I^2CE$ trained with dual branches structure achieves better consistency with\nhuman judgments to contemporary image captioning evaluation metrics.\nFurthermore, We select several state-of-the-art image captioning models and\ntest their performances on the MS COCO dataset concerning both contemporary\nmetrics and the proposed $I^2CE$. Experiment results show that our proposed\nmethod can align well with the scores generated from other contemporary\nmetrics. On this concern, the proposed metric could serve as a novel indicator\nof the intrinsic information between captions, which may be complementary to\nthe existing ones.",
          "link": "http://arxiv.org/abs/2106.15312",
          "publishedOn": "2021-06-30T02:01:01.071Z",
          "wordCount": 631,
          "title": "Contrastive Semantic Similarity Learning for Image Captioning Evaluation with Intrinsic Auto-encoder. (arXiv:2106.15312v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1\">Ege &#xd6;zsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ornek_E/0/1/0/all/0/1\">Evin P&#x131;nar &#xd6;rnek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_U/0/1/0/all/0/1\">Ulrich Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>",
          "description": "From a computer science viewpoint, a surgical domain model needs to be a\nconceptual one incorporating both behavior and data. It should therefore model\nactors, devices, tools, their complex interactions and data flow. To capture\nand model these, we take advantage of the latest computer vision methodologies\nfor generating 3D scene graphs from camera views. We then introduce the\nMultimodal Semantic Scene Graph (MSSG) which aims at providing a unified\nsymbolic, spatiotemporal and semantic representation of surgical procedures.\nThis methodology aims at modeling the relationship between different components\nin surgical domain including medical staff, imaging systems, and surgical\ndevices, opening the path towards holistic understanding and modeling of\nsurgical procedures. We then use MSSG to introduce a dynamically generated\ngraphical user interface tool for surgical procedure analysis which could be\nused for many applications including process optimization, OR design and\nautomatic report generation. We finally demonstrate that the proposed MSSGs\ncould also be used for synchronizing different complex surgical procedures.\nWhile the system still needs to be integrated into real operating rooms before\ngetting validated, this conference paper aims mainly at providing the community\nwith the basic principles of this novel concept through a first prototypal\npartial realization based on MVOR dataset.",
          "link": "http://arxiv.org/abs/2106.15309",
          "publishedOn": "2021-06-30T02:01:01.056Z",
          "wordCount": 645,
          "title": "Multimodal Semantic Scene Graphs for Holistic Modeling of Surgical Procedures. (arXiv:2106.15309v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jiexiong Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "The rapid development of autonomous driving, abnormal behavior detection, and\nbehavior recognition makes an increasing demand for multi-person pose\nestimation-based applications, especially on mobile platforms. However, to\nachieve high accuracy, state-of-the-art methods tend to have a large model size\nand complex post-processing algorithm, which costs intense computation and long\nend-to-end latency. To solve this problem, we propose an architecture\noptimization and weight pruning framework to accelerate inference of\nmulti-person pose estimation on mobile devices. With our optimization\nframework, we achieve up to 2.51x faster model inference speed with higher\naccuracy compared to representative lightweight multi-person pose estimator.",
          "link": "http://arxiv.org/abs/2106.15304",
          "publishedOn": "2021-06-30T02:01:01.051Z",
          "wordCount": 544,
          "title": "Towards Fast and Accurate Multi-Person Pose Estimation on Mobile Devices. (arXiv:2106.15304v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kollias_D/0/1/0/all/0/1\">Dimitrios Kollias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotsia_I/0/1/0/all/0/1\">Irene Kotsia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiyev_E/0/1/0/all/0/1\">Elnar Hajiyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>",
          "description": "The Affective Behavior Analysis in-the-wild (ABAW2) 2021 Competition is the\nsecond -- following the first very successful ABAW Competition held in\nconjunction with IEEE FG 2020- Competition that aims at automatically analyzing\naffect. ABAW2 is split into three Challenges, each one addressing one of the\nthree main behavior tasks of valence-arousal estimation, basic expression\nclassification and action unit detection. All three Challenges are based on a\ncommon benchmark database, Aff-Wild2, which is a large scale in-the-wild\ndatabase and the first one to be annotated for all these three tasks. In this\npaper, we describe this Competition, to be held in conjunction with ICCV 2021.\nWe present the three Challenges, with the utilized Competition corpora. We\noutline the evaluation metrics and present the baseline system with its\nresults. More information regarding the Competition is provided in the\nCompetition site: https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw.",
          "link": "http://arxiv.org/abs/2106.15318",
          "publishedOn": "2021-06-30T02:01:01.045Z",
          "wordCount": 577,
          "title": "Analysing Affective Behavior in the second ABAW2 Competition. (arXiv:2106.15318v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaiwen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinmei Tian</a>",
          "description": "Domain generalization in person re-identification is a highly important\nmeaningful and practical task in which a model trained with data from several\nsource domains is expected to generalize well to unseen target domains. Domain\nadversarial learning is a promising domain generalization method that aims to\nremove domain information in the latent representation through adversarial\ntraining. However, in person re-identification, the domain and class are\ncorrelated, and we theoretically show that domain adversarial learning will\nlose certain information about class due to this domain-class correlation.\nInspired by casual inference, we propose to perform interventions to the domain\nfactor $d$, aiming to decompose the domain-class correlation. To achieve this\ngoal, we proposed estimating the resulting representation $z^{*}$ caused by the\nintervention through first- and second-order statistical characteristic\nmatching. Specifically, we build a memory bank to restore the statistical\ncharacteristics of each domain. Then, we use the newly generated samples\n$\\{z^{*},y,d^{*}\\}$ to compute the loss function. These samples are\ndomain-class correlation decomposed; thus, we can learn a domain-invariant\nrepresentation that can capture more class-related features. Extensive\nexperiments show that our model outperforms the state-of-the-art methods on the\nlarge-scale domain generalization Re-ID benchmark.",
          "link": "http://arxiv.org/abs/2106.15206",
          "publishedOn": "2021-06-30T02:01:01.040Z",
          "wordCount": 624,
          "title": "Domain-Class Correlation Decomposition for Generalizable Person Re-Identification. (arXiv:2106.15206v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sugihara_K/0/1/0/all/0/1\">Kenichi Sugihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_M/0/1/0/all/0/1\">Martin Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kongwen/0/1/0/all/0/1\">Kongwen</a> (Frank) <a href=\"http://arxiv.org/find/cs/1/au:+Zhang/0/1/0/all/0/1\">Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khmelevsky_Y/0/1/0/all/0/1\">Youry Khmelevsky</a>",
          "description": "The 3D building modelling is important in urban planning and related domains\nthat draw upon the content of 3D models of urban scenes. Such 3D models can be\nused to visualize city images at multiple scales from individual buildings to\nentire cities prior to and after a change has occurred. This ability is of\ngreat importance in day-to-day work and special projects undertaken by\nplanners, geo-designers, and architects. In this research, we implemented a\nnovel approach to 3D building models for such matter, which included the\nintegration of geographic information systems (GIS) and 3D Computer Graphics\n(3DCG) components that generate 3D house models from building footprints\n(polygons), and the automated generation of simple and complex roof geometries\nfor rapid roof area damage reporting. These polygons (footprints) are usually\northogonal. A complicated orthogonal polygon can be partitioned into a set of\nrectangles. The proposed GIS and 3DCG integrated system partitions orthogonal\nbuilding polygons into a set of rectangles and places rectangular roofs and\nbox-shaped building bodies on these rectangles. Since technicians are drawing\nthese polygons manually with digitizers, depending on aerial photos, not all\nbuilding polygons are precisely orthogonal. But, when placing a set of boxes as\nbuilding bodies for creating the buildings, there may be gaps or overlaps\nbetween these boxes if building polygons are not precisely orthogonal. In our\nproposal, after approximately orthogonal building polygons are partitioned and\nrectified into a set of mutually orthogonal rectangles, each rectangle knows\nwhich rectangle is adjacent to and which edge of the rectangle is adjacent to,\nwhich will avoid unwanted intersection of windows and doors when building\nbodies combined.",
          "link": "http://arxiv.org/abs/2106.15294",
          "publishedOn": "2021-06-30T02:01:01.035Z",
          "wordCount": 701,
          "title": "Roof Damage Assessment from Automated 3D Building Models. (arXiv:2106.15294v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1\">P.S. Sastry</a>",
          "description": "Deep Neural Networks (DNNs) have been shown to be susceptible to memorization\nor overfitting in the presence of noisily labelled data. For the problem of\nrobust learning under such noisy data, several algorithms have been proposed. A\nprominent class of algorithms rely on sample selection strategies, motivated by\ncurriculum learning. For example, many algorithms use the `small loss trick'\nwherein a fraction of samples with loss values below a certain threshold are\nselected for training. These algorithms are sensitive to such thresholds, and\nit is difficult to fix or learn these thresholds. Often, these algorithms also\nrequire information such as label noise rates which are typically unavailable\nin practice. In this paper, we propose a data-dependent, adaptive sample\nselection strategy that relies only on batch statistics of a given mini-batch\nto provide robustness against label noise. The algorithm does not have any\nadditional hyperparameters for sample selection, does not need any information\non noise rates, and does not need access to separate data with clean labels. We\nempirically demonstrate the effectiveness of our algorithm on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.15292",
          "publishedOn": "2021-06-30T02:01:01.020Z",
          "wordCount": 620,
          "title": "Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Homan_R/0/1/0/all/0/1\">Robert Homan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijsselt_R/0/1/0/all/0/1\">Ren&#xe9; van Rijsselt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruijters_D/0/1/0/all/0/1\">Daniel Ruijters</a>",
          "description": "Fusing live fluoroscopy images with a 3D rotational reconstruction of the\nvasculature allows to navigate endovascular devices in minimally invasive\nneuro-vascular treatment, while reducing the usage of harmful iodine contrast\nmedium. The alignment of the fluoroscopy images and the 3D reconstruction is\ninitialized using the sensor information of the X-ray C-arm geometry. Patient\nmotion is then corrected by an image-based registration algorithm, based on a\ngradient difference similarity measure using digital reconstructed radiographs\nof the 3D reconstruction. This algorithm does not require the vessels in the\nfluoroscopy image to be filled with iodine contrast agent, but rather relies on\ngradients in the image (bone structures, sinuses) as landmark features. This\npaper investigates the accuracy, robustness and computation time aspects of the\nimage-based registration algorithm. Using phantom experiments 97% of the\nregistration attempts passed the success criterion of a residual registration\nerror of less than 1 mm translation and 3{\\deg} rotation. The paper establishes\na new method for validation of 2D-3D registration without requiring changes to\nthe clinical workflow, such as attaching fiducial markers. As a consequence,\nthis method can be retrospectively applied to pre-existing clinical data. For\nclinical data experiments, 87% of the registration attempts passed the\ncriterion of a residual translational error of < 1 mm, and 84% possessed a\nrotational error of < 3{\\deg}.",
          "link": "http://arxiv.org/abs/2106.15308",
          "publishedOn": "2021-06-30T02:01:01.014Z",
          "wordCount": 658,
          "title": "Automatic 2D-3D Registration without Contrast Agent during Neurovascular Interventions. (arXiv:2106.15308v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zehni_M/0/1/0/all/0/1\">Mona Zehni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shaona Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_K/0/1/0/all/0/1\">Krishna Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Sethu Raman</a>",
          "description": "Inverse rendering is the problem of decomposing an image into its intrinsic\ncomponents, i.e. albedo, normal and lighting. To solve this ill-posed problem\nfrom single image, state-of-the-art methods in shape from shading mostly resort\nto supervised training on all the components on either synthetic or real\ndatasets. Here, we propose a new self-supervised training paradigm that 1)\nreduces the need for full supervision on the decomposition task and 2) takes\ninto account the relighting task. We introduce new self-supervised loss terms\nthat leverage the consistencies between multi-lit images (images of the same\nscene under different illuminations). Our approach is applicable to multi-lit\ndatasets. We apply our training approach in two settings: 1) train on a mixture\nof synthetic and real data, 2) train on real datasets with limited supervision.\nWe show-case the effectiveness of our training paradigm on both intrinsic\ndecomposition and relighting and demonstrate how the model struggles in both\ntasks without the self-supervised loss terms in limited supervision settings.\nWe provide results of comprehensive experiments on SfSNet, CelebA and Photoface\ndatasets and verify the performance of our approach on images in the wild.",
          "link": "http://arxiv.org/abs/2106.15305",
          "publishedOn": "2021-06-30T02:01:01.009Z",
          "wordCount": 625,
          "title": "Joint Learning of Portrait Intrinsic Decomposition and Relighting. (arXiv:2106.15305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_M/0/1/0/all/0/1\">Monami Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1\">Rudrasis Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouza_J/0/1/0/all/0/1\">Jose Bouza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemuri_B/0/1/0/all/0/1\">Baba C. Vemuri</a>",
          "description": "Convolutional neural networks have been highly successful in image-based\nlearning tasks due to their translation equivariance property. Recent work has\ngeneralized the traditional convolutional layer of a convolutional neural\nnetwork to non-Euclidean spaces and shown group equivariance of the generalized\nconvolution operation. In this paper, we present a novel higher order Volterra\nconvolutional neural network (VolterraNet) for data defined as samples of\nfunctions on Riemannian homogeneous spaces. Analagous to the result for\ntraditional convolutions, we prove that the Volterra functional convolutions\nare equivariant to the action of the isometry group admitted by the Riemannian\nhomogeneous spaces, and under some restrictions, any non-linear equivariant\nfunction can be expressed as our homogeneous space Volterra convolution,\ngeneralizing the non-linear shift equivariant characterization of Volterra\nexpansions in Euclidean space. We also prove that second order functional\nconvolution operations can be represented as cascaded convolutions which leads\nto an efficient implementation. Beyond this, we also propose a dilated\nVolterraNet model. These advances lead to large parameter reductions relative\nto baseline non-Euclidean CNNs. To demonstrate the efficacy of the VolterraNet\nperformance, we present several real data experiments involving classification\ntasks on spherical-MNIST, atomic energy, Shrec17 data sets, and group testing\non diffusion MRI data. Performance comparisons to the state-of-the-art are also\npresented.",
          "link": "http://arxiv.org/abs/2106.15301",
          "publishedOn": "2021-06-30T02:01:01.003Z",
          "wordCount": 665,
          "title": "VolterraNet: A higher order convolutional network with group equivariance for homogeneous manifolds. (arXiv:2106.15301v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereg_D/0/1/0/all/0/1\">Deborah Pereg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Israel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassiliou_A/0/1/0/all/0/1\">Anthony A. Vassiliou</a>",
          "description": "In sparse coding, we attempt to extract features of input vectors, assuming\nthat the data is inherently structured as a sparse superposition of basic\nbuilding blocks. Similarly, neural networks perform a given task by learning\nfeatures of the training data set. Recently both data-driven and model-driven\nfeature extracting methods have become extremely popular and have achieved\nremarkable results. Nevertheless, practical implementations are often too slow\nto be employed in real-life scenarios, especially for real-time applications.\nWe propose a speed-up upgraded version of the classic iterative thresholding\nalgorithm, that produces a good approximation of the convolutional sparse code\nwithin 2-5 iterations. The speed advantage is gained mostly from the\nobservation that most solvers are slowed down by inefficient global\nthresholding. The main idea is to normalize each data point by the local\nreceptive field energy, before applying a threshold. This way, the natural\ninclination towards strong feature expressions is suppressed, so that one can\nrely on a global threshold that can be easily approximated, or learned during\ntraining. The proposed algorithm can be employed with a known predetermined\ndictionary, or with a trained dictionary. The trained version is implemented as\na neural net designed as the unfolding of the proposed solver. The performance\nof the proposed solution is demonstrated via the seismic inversion problem in\nboth synthetic and real data scenarios. We also provide theoretical guarantees\nfor a stable support recovery. Namely, we prove that under certain conditions\nthe true support is perfectly recovered within the first iteration.",
          "link": "http://arxiv.org/abs/2106.15296",
          "publishedOn": "2021-06-30T02:01:00.997Z",
          "wordCount": 697,
          "title": "Convolutional Sparse Coding Fast Approximation with Application to Seismic Reflectivity Estimation. (arXiv:2106.15296v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soares_D/0/1/0/all/0/1\">Daniel de Barros Soares</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Andrieux_F/0/1/0/all/0/1\">Fran&#xe7;ois Andrieux</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hell_B/0/1/0/all/0/1\">Bastien Hell</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lenhardt_J/0/1/0/all/0/1\">Julien Lenhardt</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Badosa_J/0/1/0/all/0/1\">Jordi Badosa</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Gavoille_S/0/1/0/all/0/1\">Sylvain Gavoille</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gaiffas_S/0/1/0/all/0/1\">St&#xe9;phane Gaiffas</a> (1, 4 and 5), <a href=\"http://arxiv.org/find/cs/1/au:+Bacry_E/0/1/0/all/0/1\">Emmanuel Bacry</a> (1 and 6), ((1) namR, Paris, France, (2) ENSTA Paris, France, (3) LMD, Ecole polytechnique, IP Paris, Palaiseau, France, (4) LPSM, Universit&#xe9; de Paris, France, (5) DMA, Ecole normale sup&#xe9;rieure, Paris, France, (6) CEREMADE, Universit&#xe9; Paris Dauphine, Paris, France)",
          "description": "Estimating the amount of electricity that can be produced by rooftop\nphotovoltaic systems is a time-consuming process that requires on-site\nmeasurements, a difficult task to achieve on a large scale. In this paper, we\npresent an approach to estimate the solar potential of rooftops based on their\nlocation and architectural characteristics, as well as the amount of solar\nradiation they receive annually. Our technique uses computer vision to achieve\nsemantic segmentation of roof sections and roof objects on the one hand, and a\nmachine learning model based on structured building features to predict roof\npitch on the other hand. We then compute the azimuth and maximum number of\nsolar panels that can be installed on a rooftop with geometric approaches.\nFinally, we compute precise shading masks and combine them with solar\nirradiation data that enables us to estimate the yearly solar potential of a\nrooftop.",
          "link": "http://arxiv.org/abs/2106.15268",
          "publishedOn": "2021-06-30T02:01:00.982Z",
          "wordCount": 655,
          "title": "Predicting the Solar Potential of Rooftops using Image Segmentation and Structured Data. (arXiv:2106.15268v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Richter_H/0/1/0/all/0/1\">Hendrik Richter</a>",
          "description": "Color symmetry implies that the colors of geometrical objects are assigned\naccording to their symmetry properties. It is defined by associating the\nelements of the symmetry group with a color permutation. I use this concept for\ngenerative art and apply symmetry-consistent color distortions to images of\npaintings by Johannes Vermeer. The color permutations are realized as mappings\nof the HSV color space onto itself.",
          "link": "http://arxiv.org/abs/2106.15179",
          "publishedOn": "2021-06-30T02:01:00.966Z",
          "wordCount": 492,
          "title": "Wrong Colored Vermeer: Color-Symmetric Image Distortion. (arXiv:2106.15179v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15299",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zamanitajeddin_N/0/1/0/all/0/1\">Neda Zamanitajeddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>",
          "description": "Digitization of histology images and the advent of new computational methods,\nlike deep learning, have helped the automatic grading of colorectal\nadenocarcinoma cancer (CRA). Present automated CRA grading methods, however,\nusually use tiny image patches and thus fail to integrate the entire tissue\nmicro-architecture for grading purposes. To tackle these challenges, we propose\nto use a statistical network analysis method to describe the complex structure\nof the tissue micro-environment by modelling nuclei and their connections as a\nnetwork. We show that by analyzing only the interactions between the cells in a\nnetwork, we can extract highly discriminative statistical features for CRA\ngrading. Unlike other deep learning or convolutional graph-based approaches,\nour method is highly scalable (can be used for cell networks consist of\nmillions of nodes), completely explainable, and computationally inexpensive. We\ncreate cell networks on a broad CRC histology image dataset, experiment with\nour method, and report state-of-the-art performance for the prediction of\nthree-class CRA grading.",
          "link": "http://arxiv.org/abs/2106.15299",
          "publishedOn": "2021-06-30T02:01:00.937Z",
          "wordCount": 607,
          "title": "Cells are Actors: Social Network Analysis with Classical ML for SOTA Histology Image Classification. (arXiv:2106.15299v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulits_P/0/1/0/all/0/1\">Peter Kulits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_J/0/1/0/all/0/1\">Jake Wall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedetti_A/0/1/0/all/0/1\">Anka Bedetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henley_M/0/1/0/all/0/1\">Michelle Henley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>",
          "description": "African elephants are vital to their ecosystems, but their populations are\nthreatened by a rise in human-elephant conflict and poaching. Monitoring\npopulation dynamics is essential in conservation efforts; however, tracking\nelephants is a difficult task, usually relying on the invasive and sometimes\ndangerous placement of GPS collars. Although there have been many recent\nsuccesses in the use of computer vision techniques for automated identification\nof other species, identification of elephants is extremely difficult and\ntypically requires expertise as well as familiarity with elephants in the\npopulation. We have built and deployed a web-based platform and database for\nhuman-in-the-loop re-identification of elephants combining manual attribute\nlabeling and state-of-the-art computer vision algorithms, known as\nElephantBook. Our system is currently in use at the Mara Elephant Project,\nhelping monitor the protected and at-risk population of elephants in the\nGreater Maasai Mara ecosystem. ElephantBook makes elephant re-identification\nusable by non-experts and scalable for use by multiple conservation NGOs.",
          "link": "http://arxiv.org/abs/2106.15083",
          "publishedOn": "2021-06-30T02:01:00.930Z",
          "wordCount": 595,
          "title": "ElephantBook: A Semi-Automated Human-in-the-Loop System for Elephant Re-Identification. (arXiv:2106.15083v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xingqun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caifeng Shan</a>",
          "description": "Face sketch synthesis has made significant progress with the development of\ndeep neural networks in these years. The delicate depiction of sketch portraits\nfacilitates a wide range of applications like digital entertainment and law\nenforcement. However, accurate and realistic face sketch generation is still a\nchallenging task due to the illumination variations and complex backgrounds in\nthe real scenes. To tackle these challenges, we propose a novel Semantic-Driven\nGenerative Adversarial Network (SDGAN) which embeds global structure-level\nstyle injection and local class-level knowledge re-weighting. Specifically, we\nconduct facial saliency detection on the input face photos to provide overall\nfacial texture structure, which could be used as a global type of prior\ninformation. In addition, we exploit face parsing layouts as the semantic-level\nspatial prior to enforce globally structural style injection in the generator\nof SDGAN. Furthermore, to enhance the realistic effect of the details, we\npropose a novel Adaptive Re-weighting Loss (ARLoss) which dedicates to balance\nthe contributions of different semantic classes. Experimentally, our extensive\nexperiments on CUFS and CUFSF datasets show that our proposed algorithm\nachieves state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.15121",
          "publishedOn": "2021-06-30T02:01:00.900Z",
          "wordCount": 618,
          "title": "Face Sketch Synthesis via Semantic-Driven Generative Adversarial Network. (arXiv:2106.15121v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1\">Simone Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1\">Ehsan Nowroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orazi_G/0/1/0/all/0/1\">Gabriele Orazi</a>",
          "description": "The last-generation video conferencing software allows users to utilize a\nvirtual background to conceal their personal environment due to privacy\nconcerns, especially in official meetings with other employers. On the other\nhand, users maybe want to fool people in the meeting by considering the virtual\nbackground to conceal where they are. In this case, developing tools to\nunderstand the virtual background utilize for fooling people in meeting plays\nan important role. Besides, such detectors must prove robust against different\nkinds of attacks since a malicious user can fool the detector by applying a set\nof adversarial editing steps on the video to conceal any revealing footprint.\nIn this paper, we study the feasibility of an efficient tool to detect whether\na videoconferencing user background is real. In particular, we provide the\nfirst tool which computes pixel co-occurrences matrices and uses them to search\nfor inconsistencies among spectral and spatial bands. Our experiments confirm\nthat cross co-occurrences matrices improve the robustness of the detector\nagainst different kinds of attacks. This work's performance is especially\nnoteworthy with regard to color SPAM features. Moreover, the performance\nespecially is significant with regard to robustness versus post-processing,\nlike geometric transformations, filtering, contrast enhancement, and JPEG\ncompression with different quality factors.",
          "link": "http://arxiv.org/abs/2106.15130",
          "publishedOn": "2021-06-30T02:01:00.884Z",
          "wordCount": 669,
          "title": "Do Not Deceive Your Employer with a Virtual Background: A Video Conferencing Manipulation-Detection System. (arXiv:2106.15130v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirsten_L/0/1/0/all/0/1\">Lucas N. Kirsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccoli_R/0/1/0/all/0/1\">Ricardo Piccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribani_R/0/1/0/all/0/1\">Ricardo Ribani</a>",
          "description": "This work evaluates six state-of-the-art deep neural network (DNN)\narchitectures applied to the problem of enhancing camera-captured document\nimages. The results from each network were evaluated both qualitatively and\nquantitatively using Image Quality Assessment (IQA) metrics, and also compared\nwith an existing approach based on traditional computer vision techniques. The\nbest performing architectures generally produced good enhancement compared to\nthe existing algorithm, showing that it is possible to use DNNs for document\nimage enhancement. Furthermore, the best performing architectures could work as\na baseline for future investigations on document enhancement using deep\nlearning techniques. The main contributions of this paper are: a baseline of\ndeep learning techniques that can be further improved to provide better\nresults, and a evaluation methodology using IQA metrics for quantitatively\ncomparing the produced images from the neural networks to a ground truth.",
          "link": "http://arxiv.org/abs/2106.15286",
          "publishedOn": "2021-06-30T02:01:00.879Z",
          "wordCount": 597,
          "title": "Evaluating Deep Neural Networks for Image Document Enhancement. (arXiv:2106.15286v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_R/0/1/0/all/0/1\">Ranyu Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Temporal action detection (TAD) is a challenging task which aims to\ntemporally localize and recognize the human action in untrimmed videos. Current\nmainstream one-stage TAD approaches localize and classify action proposals\nrelying on pre-defined anchors, where the location and scale for action\ninstances are set by designers. Obviously, such an anchor-based TAD method\nlimits its generalization capability and will lead to performance degradation\nwhen videos contain rich action variation. In this study, we explore to remove\nthe requirement of pre-defined anchors for TAD methods. A novel TAD model\ntermed as Selective Receptive Field Network (SRF-Net) is developed, in which\nthe location offsets and classification scores at each temporal location can be\ndirectly estimated in the feature map and SRF-Net is trained in an end-to-end\nmanner. Innovatively, a building block called Selective Receptive Field\nConvolution (SRFC) is dedicatedly designed which is able to adaptively adjust\nits receptive field size according to multiple scales of input information at\neach temporal location in the feature map. Extensive experiments are conducted\non the THUMOS14 dataset, and superior results are reported comparing to\nstate-of-the-art TAD approaches.",
          "link": "http://arxiv.org/abs/2106.15258",
          "publishedOn": "2021-06-30T02:01:00.862Z",
          "wordCount": 624,
          "title": "SRF-Net: Selective Receptive Field Network for Anchor-Free Temporal Action Detection. (arXiv:2106.15258v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>",
          "description": "We show that cascaded diffusion models are capable of generating high\nfidelity images on the class-conditional ImageNet generation challenge, without\nany assistance from auxiliary image classifiers to boost sample quality. A\ncascaded diffusion model comprises a pipeline of multiple diffusion models that\ngenerate images of increasing resolution, beginning with a standard diffusion\nmodel at the lowest resolution, followed by one or more super-resolution\ndiffusion models that successively upsample the image and add higher resolution\ndetails. We find that the sample quality of a cascading pipeline relies\ncrucially on conditioning augmentation, our proposed method of data\naugmentation of the lower resolution conditioning inputs to the\nsuper-resolution models. Our experiments show that conditioning augmentation\nprevents compounding error during sampling in a cascaded model, helping us to\ntrain cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at\n128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep.",
          "link": "http://arxiv.org/abs/2106.15282",
          "publishedOn": "2021-06-30T02:01:00.857Z",
          "wordCount": 593,
          "title": "Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhuangwei Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>",
          "description": "3D LiDAR (light detection and ranging) based semantic segmentation is\nimportant in scene understanding for many applications, such as auto-driving\nand robotics. For example, for autonomous cars equipped with RGB cameras and\nLiDAR, it is crucial to fuse complementary information from different sensors\nfor robust and accurate segmentation. Existing fusion-based methods, however,\nmay not achieve promising performance due to the vast difference between two\nmodalities. In this work, we investigate a collaborative fusion scheme called\nperception-aware multi-sensor fusion (PMF) to exploit perceptual information\nfrom two modalities, namely, appearance information from RGB images and\nspatio-depth information from point clouds. To this end, we first project point\nclouds to the camera coordinates to provide spatio-depth information for RGB\nimages. Then, we propose a two-stream network to extract features from the two\nmodalities, separately, and fuse the features by effective residual-based\nfusion modules. Moreover, we propose additional perception-aware losses to\nmeasure the great perceptual difference between the two modalities. Extensive\nexperiments on two benchmark data sets show the superiority of our method. For\nexample, on nuScenes, our PMF outperforms the state-of-the-art method by 0.8%\nin mIoU.",
          "link": "http://arxiv.org/abs/2106.15277",
          "publishedOn": "2021-06-30T02:01:00.849Z",
          "wordCount": 627,
          "title": "Perception-aware Multi-sensor Fusion for 3D LiDAR Semantic Segmentation. (arXiv:2106.15277v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15280",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiqin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tian Guo</a>",
          "description": "Omnidirectional lighting provides the foundation for achieving\nspatially-variant photorealistic 3D rendering, a desirable property for mobile\naugmented reality applications. However, in practice, estimating\nomnidirectional lighting can be challenging due to limitations such as partial\npanoramas of the rendering positions, and the inherent environment lighting and\nmobile user dynamics. A new opportunity arises recently with the advancements\nin mobile 3D vision, including built-in high-accuracy depth sensors and deep\nlearning-powered algorithms, which provide the means to better sense and\nunderstand the physical surroundings. Centering the key idea of 3D vision, in\nthis work, we design an edge-assisted framework called Xihe to provide mobile\nAR applications the ability to obtain accurate omnidirectional lighting\nestimation in real time. Specifically, we develop a novel sampling technique\nthat efficiently compresses the raw point cloud input generated at the mobile\ndevice. This technique is derived based on our empirical analysis of a recent\n3D indoor dataset and plays a key role in our 3D vision-based lighting\nestimator pipeline design. To achieve the real-time goal, we develop a tailored\nGPU pipeline for on-device point cloud processing and use an encoding technique\nthat reduces network transmitted bytes. Finally, we present an adaptive\ntriggering strategy that allows Xihe to skip unnecessary lighting estimations\nand a practical way to provide temporal coherent rendering integration with the\nmobile AR ecosystem. We evaluate both the lighting estimation accuracy and time\nof Xihe using a reference mobile application developed with Xihe's APIs. Our\nresults show that Xihe takes as fast as 20.67ms per lighting estimation and\nachieves 9.4% better estimation accuracy than a state-of-the-art neural\nnetwork.",
          "link": "http://arxiv.org/abs/2106.15280",
          "publishedOn": "2021-06-30T02:01:00.832Z",
          "wordCount": 705,
          "title": "Xihe: A 3D Vision-based Lighting Estimation Framework for Mobile Augmented Reality. (arXiv:2106.15280v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kashi_M/0/1/0/all/0/1\">Mohammad Amin Kashi</a>",
          "description": "Depth perception is fundamental for robots to understand the surrounding\nenvironment. As the view of cognitive neuroscience, visual depth perception\nmethods are divided into three categories, namely binocular, active, and\npictorial. The first two categories have been studied for decades in detail.\nHowever, research for the exploration of the third category is still in its\ninfancy and has got momentum by the advent of deep learning methods in recent\nyears. In cognitive neuroscience, it is known that pictorial depth perception\nmechanisms are dependent on the perception of seen objects. Inspired by this\nfact, in this thesis, we investigated the relation of perception of objects and\ndepth estimation convolutional neural networks. For this purpose, we developed\nnew network structures based on a simple depth estimation network that only\nused a single image at its input. Our proposed structures use both an image and\na semantic label of the image as their input. We used semantic labels as the\noutput of object perception. The obtained results of performance comparison\nbetween the developed network and original network showed that our novel\nstructures can improve the performance of depth estimation by 52\\% of relative\nerror of distance in the examined cases. Most of the experimental studies were\ncarried out on synthetic datasets that were generated by game engines to\nisolate the performance comparison from the effect of inaccurate depth and\nsemantic labels of non-synthetic datasets. It is shown that particular\nsynthetic datasets may be used for training of depth networks in cases that an\nappropriate dataset is not available. Furthermore, we showed that in these\ncases, usage of semantic labels improves the robustness of the network against\ndomain shift from synthetic training data to non-synthetic test data.",
          "link": "http://arxiv.org/abs/2106.15257",
          "publishedOn": "2021-06-30T02:01:00.826Z",
          "wordCount": 748,
          "title": "Predicting Depth from Semantic Segmentation using Game Engine Dataset. (arXiv:2106.15257v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shenghua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shaoqun Zeng</a>",
          "description": "Digital gigapixel whole slide image (WSI) is widely used in clinical\ndiagnosis, and automated WSI analysis is key for computer-aided diagnosis.\nCurrently, analyzing the integrated descriptor of probabilities or feature maps\nfrom massive local patches encoded by ResNet classifier is the main manner for\nWSI-level prediction. Feature representations of the sparse and tiny lesion\ncells in cervical slides, however, are still challengeable for the\nunder-promoted upstream encoders, while the unused spatial representations of\ncervical cells are the available features to supply the semantics analysis. As\nwell as patches sampling with overlap and repetitive processing incur the\ninefficiency and the unpredictable side effect. This study designs a novel\ninline connection network (InCNet) by enriching the multi-scale connectivity to\nbuild the lightweight model named You Only Look Cytopathology Once (YOLCO) with\nthe additional supervision of spatial information. The proposed model allows\nthe input size enlarged to megapixel that can stitch the WSI without any\noverlap by the average repeats decreased from $10^3\\sim10^4$ to $10^1\\sim10^2$\nfor collecting features and predictions at two scales. Based on Transformer for\nclassifying the integrated multi-scale multi-task features, the experimental\nresults appear $0.872$ AUC score better and $2.51\\times$ faster than the best\nconventional method in WSI classification on multicohort datasets of 2,019\nslides from four scanning devices.",
          "link": "http://arxiv.org/abs/2106.15113",
          "publishedOn": "2021-06-30T02:01:00.820Z",
          "wordCount": 672,
          "title": "An Efficient Cervical Whole Slide Image Analysis Framework Based on Multi-scale Semantic and Spatial Features using Deep Learning. (arXiv:2106.15113v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15097",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1\">Qing Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_R/0/1/0/all/0/1\">Ruiming Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1\">Hongjiang Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qing Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_B/0/1/0/all/0/1\">Boliang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>",
          "description": "For collecting high-quality high-resolution (HR) MR image, we propose a novel\nimage reconstruction network named IREM, which is trained on multiple\nlow-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR\nimage reconstruction. In this work, we suppose the desired HR image as an\nimplicit continuous function of the 3D image spatial coordinate and the\nthick-slice LR images as several sparse discrete samplings of this function.\nThen the super-resolution (SR) task is to learn the continuous volumetric\nfunction from a limited observations using an fully-connected neural network\ncombined with Fourier feature positional encoding. By simply minimizing the\nerror between the network prediction and the acquired LR image intensity across\neach imaging plane, IREM is trained to represent a continuous model of the\nobserved tissue anatomy. Experimental results indicate that IREM succeeds in\nrepresenting high frequency image feature, and in real scene data collection,\nIREM reduces scan time and achieves high-quality high-resolution MR imaging in\nterms of SNR and local image detail.",
          "link": "http://arxiv.org/abs/2106.15097",
          "publishedOn": "2021-06-30T02:01:00.815Z",
          "wordCount": 631,
          "title": "IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction via Implicit Neural Representation. (arXiv:2106.15097v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pucci_R/0/1/0/all/0/1\">Rita Pucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinel_N/0/1/0/all/0/1\">Niki Martinel</a>",
          "description": "Automatic image colourisation is the computer vision research path that\nstudies how to colourise greyscale images (for restoration). Deep learning\ntechniques improved image colourisation yielding astonishing results. These\ndiffer by various factors, such as structural differences, input types, user\nassistance, etc. Most of them, base the architectural structure on\nconvolutional layers with no emphasis on layers specialised in object features\nextraction. We introduce a novel downsampling upsampling architecture named\nTUCaN (Tiny UCapsNet) that exploits the collaboration of convolutional layers\nand capsule layers to obtain a neat colourisation of entities present in every\nsingle image. This is obtained by enforcing collaboration among such layers by\nskip and residual connections. We pose the problem as a per pixel colour\nclassification task that identifies colours as a bin in a quantized space. To\ntrain the network, in contrast with the standard end to end learning method, we\npropose the progressive learning scheme to extract the context of objects by\nonly manipulating the learning process without changing the model. In this\nscheme, the upsampling starts from the reconstruction of low resolution images\nand progressively grows to high resolution images throughout the training\nphase. Experimental results on three benchmark datasets show that our approach\nwith ImageNet10k dataset outperforms existing methods on standard quality\nmetrics and achieves state of the art performances on image colourisation. We\nperformed a user study to quantify the perceptual realism of the colourisation\nresults demonstrating: that progressive learning let the TUCaN achieve better\ncolours than the end to end scheme; and pointing out the limitations of the\nexisting evaluation metrics.",
          "link": "http://arxiv.org/abs/2106.15176",
          "publishedOn": "2021-06-30T02:01:00.808Z",
          "wordCount": 691,
          "title": "TUCaN: Progressively Teaching Colourisation to Capsules. (arXiv:2106.15176v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Son Nguyen Truong</a>",
          "description": "We present a novel data generation tool for document processing. The tool\nfocuses on providing a maximal level of visual information in a normal type\ndocument, ranging from character position to paragraph-level position. It also\nenables working with a large dataset on low-resource languages as well as\nproviding a mean of processing thorough full-level information of the\ndocumented text. The data generation tools come with a dataset of 320000\nVietnamese synthetic document images and an instruction to generate a dataset\nof similar size in other languages. The repository can be found at:\nhttps://github.com/tson1997/SDL-Document-Image-Generation",
          "link": "http://arxiv.org/abs/2106.15117",
          "publishedOn": "2021-06-30T02:01:00.793Z",
          "wordCount": 529,
          "title": "SDL: New data generation tools for full-level annotated document layout. (arXiv:2106.15117v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1\">Peng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yawen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Semi-supervised learning is a challenging problem which aims to construct a\nmodel by learning from a limited number of labeled examples. Numerous methods\nhave been proposed to tackle this problem, with most focusing on utilizing the\npredictions of unlabeled instances consistency alone to regularize networks.\nHowever, treating labeled and unlabeled data separately often leads to the\ndiscarding of mass prior knowledge learned from the labeled examples, and\nfailure to mine the feature interaction between the labeled and unlabeled image\npairs. In this paper, we propose a novel method for semi-supervised semantic\nsegmentation named GuidedMix-Net, by leveraging labeled information to guide\nthe learning of unlabeled instances. Specifically, we first introduce a feature\nalignment objective between labeled and unlabeled data to capture potentially\nsimilar image pairs and then generate mixed inputs from them. The proposed\nmutual information transfer (MITrans), based on the cluster assumption, is\nshown to be a powerful knowledge module for further progressive refining\nfeatures of unlabeled data in the mixed data space. To take advantage of the\nlabeled examples and guide unlabeled data learning, we further propose a mask\ngeneration module to generate high-quality pseudo masks for the unlabeled data.\nAlong with supervised learning for labeled data, the prediction of unlabeled\ndata is jointly learned with the generated pseudo masks from the mixed data.\nExtensive experiments on PASCAL VOC 2012, PASCAL-Context and Cityscapes\ndemonstrate the effectiveness of our GuidedMix-Net, which achieves competitive\nsegmentation accuracy and significantly improves the mIoU by +7$\\%$ compared to\nprevious state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2106.15064",
          "publishedOn": "2021-06-30T02:01:00.788Z",
          "wordCount": 694,
          "title": "GuidedMix-Net: Learning to Improve Pseudo Masks Using Labeled Images as Reference. (arXiv:2106.15064v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Fan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caifeng Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "One essential problem in skeleton-based action recognition is how to extract\ndiscriminative features over all skeleton joints. However, the complexity of\nthe recent State-Of-The-Art (SOTA) models for this task tends to be exceedingly\nsophisticated and over-parameterized. The low efficiency in model training and\ninference has increased the validation costs of model architectures in\nlarge-scale datasets. To address the above issue, recent advanced separable\nconvolutional layers are embedded into an early fused Multiple Input Branches\n(MIB) network, constructing an efficient Graph Convolutional Network (GCN)\nbaseline for skeleton-based action recognition. In addition, based on such the\nbaseline, we design a compound scaling strategy to expand the model's width and\ndepth synchronously, and eventually obtain a family of efficient GCN baselines\nwith high accuracies and small amounts of trainable parameters, termed\nEfficientGCN-Bx, where ''x'' denotes the scaling coefficient. On two\nlarge-scale datasets, i.e., NTU RGB+D 60 and 120, the proposed EfficientGCN-B4\nbaseline outperforms other SOTA methods, e.g., achieving 91.7% accuracy on the\ncross-subject benchmark of NTU 60 dataset, while being 3.15x smaller and 3.21x\nfaster than MS-G3D, which is one of the best SOTA methods. The source code in\nPyTorch version and the pretrained models are available at\nhttps://github.com/yfsong0709/EfficientGCNv1.",
          "link": "http://arxiv.org/abs/2106.15125",
          "publishedOn": "2021-06-30T02:01:00.783Z",
          "wordCount": 654,
          "title": "Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition. (arXiv:2106.15125v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geeho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>",
          "description": "Visual recognition tasks are often limited to dealing with a small subset of\nclasses simply because the labels for the remaining classes are unavailable. We\nare interested in identifying novel concepts in a dataset through\nrepresentation learning based on the examples in both labeled and unlabeled\nclasses, and extending the horizon of recognition to both known and novel\nclasses. To address this challenging task, we propose a combinatorial learning\napproach, which naturally clusters the examples in unseen classes using the\ncompositional knowledge given by multiple supervised meta-classifiers on\nheterogeneous label spaces. We also introduce a metric learning strategy to\nestimate pairwise pseudo-labels for improving representations of unlabeled\nexamples, which preserves semantic relations across known and novel classes\neffectively. The proposed algorithm discovers novel concepts via a joint\noptimization of enhancing the discrimitiveness of unseen classes as well as\nlearning the representations of known classes generalizable to novel ones. Our\nextensive experiments demonstrate remarkable performance gains by the proposed\napproach in multiple image retrieval and novel class discovery benchmarks.",
          "link": "http://arxiv.org/abs/2106.15278",
          "publishedOn": "2021-06-30T02:01:00.777Z",
          "wordCount": 609,
          "title": "Open-Set Representation Learning through Combinatorial Embedding. (arXiv:2106.15278v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuzhe Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>",
          "description": "Contrary to the vast literature in modeling, perceiving, and understanding\nagent-object (e.g., human-object, hand-object, robot-object) interaction in\ncomputer vision and robotics, very few past works have studied the task of\nobject-object interaction, which also plays an important role in robotic\nmanipulation and planning tasks. There is a rich space of object-object\ninteraction scenarios in our daily life, such as placing an object on a messy\ntabletop, fitting an object inside a drawer, pushing an object using a tool,\netc. In this paper, we propose a unified affordance learning framework to learn\nobject-object interaction for various tasks. By constructing four object-object\ninteraction task environments using physical simulation (SAPIEN) and thousands\nof ShapeNet models with rich geometric diversity, we are able to conduct\nlarge-scale object-object affordance learning without the need for human\nannotations or demonstrations. At the core of technical contribution, we\npropose an object-kernel point convolution network to reason about detailed\ninteraction between two objects. Experiments on large-scale synthetic data and\nreal-world data prove the effectiveness of the proposed approach. Please refer\nto the project webpage for code, data, video, and more materials:\nhttps://cs.stanford.edu/~kaichun/o2oafford",
          "link": "http://arxiv.org/abs/2106.15087",
          "publishedOn": "2021-06-30T02:01:00.772Z",
          "wordCount": 622,
          "title": "O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning. (arXiv:2106.15087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jinqi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>",
          "description": "Deep learning based image recognition systems have been widely deployed on\nmobile devices in today's world. In recent studies, however, deep learning\nmodels are shown vulnerable to adversarial examples. One variant of adversarial\nexamples, called adversarial patch, draws researchers' attention due to its\nstrong attack abilities. Though adversarial patches achieve high attack success\nrates, they are easily being detected because of the visual inconsistency\nbetween the patches and the original images. Besides, it usually requires a\nlarge amount of data for adversarial patch generation in the literature, which\nis computationally expensive and time-consuming. To tackle these challenges, we\npropose an approach to generate inconspicuous adversarial patches with one\nsingle image. In our approach, we first decide the patch locations basing on\nthe perceptual sensitivity of victim models, then produce adversarial patches\nin a coarse-to-fine way by utilizing multiple-scale generators and\ndiscriminators. The patches are encouraged to be consistent with the background\nimages with adversarial training while preserving strong attack abilities. Our\napproach shows the strong attack abilities in white-box settings and the\nexcellent transferability in black-box settings through extensive experiments\non various models with different architectures and training methods. Compared\nto other adversarial patches, our adversarial patches hold the most negligible\nrisks to be detected and can evade human observations, which is supported by\nthe illustrations of saliency maps and results of user evaluations. Lastly, we\nshow that our adversarial patches can be applied in the physical world.",
          "link": "http://arxiv.org/abs/2106.15202",
          "publishedOn": "2021-06-30T02:01:00.756Z",
          "wordCount": 691,
          "title": "Inconspicuous Adversarial Patches for Fooling Image Recognition Systems on Mobile Devices. (arXiv:2106.15202v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebuffi_S/0/1/0/all/0/1\">Sylvestre-Alvise Rebuffi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehrhardt_S/0/1/0/all/0/1\">S&#xe9;bastien Ehrhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "We tackle the problem of discovering novel classes in an image collection\ngiven labelled examples of other classes. We present a new approach called\nAutoNovel to address this problem by combining three ideas: (1) we suggest that\nthe common approach of bootstrapping an image representation using the labelled\ndata only introduces an unwanted bias, and that this can be avoided by using\nself-supervised learning to train the representation from scratch on the union\nof labelled and unlabelled data; (2) we use ranking statistics to transfer the\nmodel's knowledge of the labelled classes to the problem of clustering the\nunlabelled images; and, (3) we train the data representation by optimizing a\njoint objective function on the labelled and unlabelled subsets of the data,\nimproving both the supervised classification of the labelled data, and the\nclustering of the unlabelled data. Moreover, we propose a method to estimate\nthe number of classes for the case where the number of new categories is not\nknown a priori. We evaluate AutoNovel on standard classification benchmarks and\nsubstantially outperform current methods for novel category discovery. In\naddition, we also show that AutoNovel can be used for fully unsupervised image\nclustering, achieving promising results.",
          "link": "http://arxiv.org/abs/2106.15252",
          "publishedOn": "2021-06-30T02:01:00.750Z",
          "wordCount": 652,
          "title": "AutoNovel: Automatically Discovering and Learning Novel Visual Categories. (arXiv:2106.15252v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "In recent years, the growth of Machine Learning algorithms in a variety of\ndifferent applications has raised numerous studies on the applicability of\nthese algorithms in real scenarios. Among all, one of the hardest scenarios,\ndue to its physical requirements, is the aerospace one. In this context, the\nauthors of this work aim to propose a first prototype and a study of\nfeasibility for an AI model to be 'loaded' on board. As a case study, the\nauthors decided to investigate the detection of volcanic eruptions as a method\nto swiftly produce alerts. Two Convolutional Neural Networks have been proposed\nand created, also showing how to correctly implement them on real hardware and\nhow the complexity of a CNN can be adapted to fit computational requirements.",
          "link": "http://arxiv.org/abs/2106.15281",
          "publishedOn": "2021-06-30T02:01:00.745Z",
          "wordCount": 597,
          "title": "On-board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakhtiarnia_A/0/1/0/all/0/1\">Arian Bakhtiarnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>",
          "description": "Deep neural networks can be converted to multi-exit architectures by\ninserting early exit branches after some of their intermediate layers. This\nallows their inference process to become dynamic, which is useful for time\ncritical IoT applications with stringent latency requirements, but with\ntime-variant communication and computation resources. In particular, in edge\ncomputing systems and IoT networks where the exact computation time budget is\nvariable and not known beforehand. Vision Transformer is a recently proposed\narchitecture which has since found many applications across various domains of\ncomputer vision. In this work, we propose seven different architectures for\nearly exit branches that can be used for dynamic inference in Vision\nTransformer backbones. Through extensive experiments involving both\nclassification and regression problems, we show that each one of our proposed\narchitectures could prove useful in the trade-off between accuracy and speed.",
          "link": "http://arxiv.org/abs/2106.15183",
          "publishedOn": "2021-06-30T02:01:00.739Z",
          "wordCount": 568,
          "title": "Multi-Exit Vision Transformer for Dynamic Inference. (arXiv:2106.15183v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caldero_M/0/1/0/all/0/1\">Manuel Sarmiento Calder&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varas_D/0/1/0/all/0/1\">David Varas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bou_Balust_E/0/1/0/all/0/1\">Elisenda Bou-Balust</a>",
          "description": "Research in action detection has grown in the recentyears, as it plays a key\nrole in video understanding. Modelling the interactions (either spatial or\ntemporal) between actors and their context has proven to be essential for this\ntask. While recent works use spatial features with aggregated temporal\ninformation, this work proposes to use non-aggregated temporal information.\nThis is done by adding an attention based method that leverages spatio-temporal\ninteractions between elements in the scene along the clip.The main contribution\nof this work is the introduction of two cross attention blocks to effectively\nmodel the spatial relations and capture short range temporal\ninteractions.Experiments on the AVA dataset show the advantages of the proposed\napproach that models spatio-temporal relations between relevant elements in the\nscene, outperforming other methods that model actor interactions with their\ncontext by +0.31 mAP.",
          "link": "http://arxiv.org/abs/2106.15171",
          "publishedOn": "2021-06-30T02:01:00.734Z",
          "wordCount": 571,
          "title": "Spatio-Temporal Context for Action Detection. (arXiv:2106.15171v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aliyev_N/0/1/0/all/0/1\">Namig Aliyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezer_O/0/1/0/all/0/1\">Oguzhan Sezer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzel_M/0/1/0/all/0/1\">Mehmet Turan Guzel</a>",
          "description": "Autonomous systems require identifying the environment and it has a long way\nto go before putting it safely into practice. In autonomous driving systems,\nthe detection of obstacles and traffic lights are of importance as well as lane\ntracking. In this study, an autonomous driving system is developed and tested\nin the experimental environment designed for this purpose. In this system, a\nmodel vehicle having a camera is used to trace the lanes and avoid obstacles to\nexperimentally study autonomous driving behavior. Convolutional Neural Network\nmodels were trained for Lane tracking. For the vehicle to avoid obstacles,\ncorner detection, optical flow, focus of expansion, time to collision, balance\ncalculation, and decision mechanism were created, respectively.",
          "link": "http://arxiv.org/abs/2106.15274",
          "publishedOn": "2021-06-30T02:01:00.687Z",
          "wordCount": 573,
          "title": "Autonomous Driving Implementation in an Experimental Environment. (arXiv:2106.15274v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsuji_K/0/1/0/all/0/1\">Kaigen Tsuji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haraguchi_D/0/1/0/all/0/1\">Daichi Haraguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1\">Brian Kenji Iwana</a>",
          "description": "Fonts have had trends throughout their history, not only in when they were\ninvented but also in their usage and popularity. In this paper, we attempt to\nspecifically find the trends in font usage using robust regression on a large\ncollection of text images. We utilize movie posters as the source of fonts for\nthis task because movie posters can represent time periods by using their\nrelease date. In addition, movie posters are documents that are carefully\ndesigned and represent a wide range of fonts. To understand the relationship\nbetween the fonts of movie posters and time, we use a regression Convolutional\nNeural Network (CNN) to estimate the release year of a movie using an isolated\ntitle text image. Due to the difficulty of the task, we propose to use of a\nhybrid training regimen that uses a combination of Mean Squared Error (MSE) and\nTukey's biweight loss. Furthermore, we perform a thorough analysis on the\ntrends of fonts through time.",
          "link": "http://arxiv.org/abs/2106.15232",
          "publishedOn": "2021-06-30T02:01:00.682Z",
          "wordCount": 615,
          "title": "Using Robust Regression to Find Font Usage Trends. (arXiv:2106.15232v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Attention Mechanism is a widely used method for improving the performance of\nconvolutional neural networks (CNNs) on computer vision tasks. Despite its\npervasiveness, we have a poor understanding of what its effectiveness stems\nfrom. It is popularly believed that its effectiveness stems from the visual\nattention explanation, advocating focusing on the important part of input data\nrather than ingesting the entire input. In this paper, we find that there is\nonly a weak consistency between the attention weights of features and their\nimportance. Instead, we verify the crucial role of feature map multiplication\nin attention mechanism and uncover a fundamental impact of feature map\nmultiplication on the learned landscapes of CNNs: with the high order\nnon-linearity brought by the feature map multiplication, it played a\nregularization role on CNNs, which made them learn smoother and more stable\nlandscapes near real samples compared to vanilla CNNs. This smoothness and\nstability induce a more predictive and stable behavior in-between real samples,\nand make CNNs generate better. Moreover, motivated by the proposed\neffectiveness of feature map multiplication, we design feature map\nmultiplication network (FMMNet) by simply replacing the feature map addition in\nResNet with feature map multiplication. FMMNet outperforms ResNet on various\ndatasets, and this indicates that feature map multiplication plays a vital role\nin improving the performance even without finely designed attention mechanism\nin existing methods.",
          "link": "http://arxiv.org/abs/2106.15067",
          "publishedOn": "2021-06-30T02:01:00.585Z",
          "wordCount": 662,
          "title": "Towards Understanding the Effectiveness of Attention Mechanism. (arXiv:2106.15067v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1\">Nachiket Deo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1\">Eric M. Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>",
          "description": "Accurately predicting the future motion of surrounding vehicles requires\nreasoning about the inherent uncertainty in goals and driving behavior. This\nuncertainty can be loosely decoupled into lateral (e.g., keeping lane, turning)\nand longitudinal (e.g., accelerating, braking). We present a novel method that\ncombines learned discrete policy rollouts with a focused decoder on subsets of\nthe lane graph. The policy rollouts explore different goals given our current\nobservations, ensuring that the model captures lateral variability. The\nlongitudinal variability is captured by our novel latent variable model decoder\nthat is conditioned on various subsets of the lane graph. Our model achieves\nstate-of-the-art performance on the nuScenes motion prediction dataset, and\nqualitatively demonstrates excellent scene compliance. Detailed ablations\nhighlight the importance of both the policy rollouts and the decoder\narchitecture.",
          "link": "http://arxiv.org/abs/2106.15004",
          "publishedOn": "2021-06-30T02:01:00.579Z",
          "wordCount": 562,
          "title": "Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals. (arXiv:2106.15004v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boulahbal_H/0/1/0/all/0/1\">Houssem-eddine Boulahbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voicila_A/0/1/0/all/0/1\">Adrian Voicila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comport_A/0/1/0/all/0/1\">Andrew Comport</a>",
          "description": "This paper proposes two important contributions for conditional Generative\nAdversarial Networks (cGANs) to improve the wide variety of applications that\nexploit this architecture. The first main contribution is an analysis of cGANs\nto show that they are not explicitly conditional. In particular, it will be\nshown that the discriminator and subsequently the cGAN does not automatically\nlearn the conditionality between inputs. The second contribution is a new\nmethod, called acontrario, that explicitly models conditionality for both parts\nof the adversarial architecture via a novel acontrario loss that involves\ntraining the discriminator to learn unconditional (adverse) examples. This\nleads to a novel type of data augmentation approach for GANs (acontrario\nlearning) which allows to restrict the search space of the generator to\nconditional outputs using adverse examples. Extensive experimentation is\ncarried out to evaluate the conditionality of the discriminator by proposing a\nprobability distribution analysis. Comparisons with the cGAN architecture for\ndifferent applications show significant improvements in performance on well\nknown datasets including, semantic image synthesis, image segmentation and\nmonocular depth prediction using different metrics including Fr\\'echet\nInception Distance(FID), mean Intersection over Union (mIoU), Root Mean Square\nError log (RMSE log) and Number of statistically-Different Bins (NDB)",
          "link": "http://arxiv.org/abs/2106.15011",
          "publishedOn": "2021-06-30T02:01:00.563Z",
          "wordCount": 628,
          "title": "Are conditional GANs explicitly conditional?. (arXiv:2106.15011v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zihao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chilin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaolu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Face recognition is greatly improved by deep convolutional neural networks\n(CNNs). Recently, these face recognition models have been used for identity\nauthentication in security sensitive applications. However, deep CNNs are\nvulnerable to adversarial patches, which are physically realizable and\nstealthy, raising new security concerns on the real-world applications of these\nmodels. In this paper, we evaluate the robustness of face recognition models\nusing adversarial patches based on transferability, where the attacker has\nlimited accessibility to the target models. First, we extend the existing\ntransfer-based attack techniques to generate transferable adversarial patches.\nHowever, we observe that the transferability is sensitive to initialization and\ndegrades when the perturbation magnitude is large, indicating the overfitting\nto the substitute models. Second, we propose to regularize the adversarial\npatches on the low dimensional data manifold. The manifold is represented by\ngenerative models pre-trained on legitimate human face images. Using face-like\nfeatures as adversarial perturbations through optimization on the manifold, we\nshow that the gaps between the responses of substitute models and the target\nmodels dramatically decrease, exhibiting a better transferability. Extensive\ndigital world experiments are conducted to demonstrate the superiority of the\nproposed method in the black-box setting. We apply the proposed method in the\nphysical world as well.",
          "link": "http://arxiv.org/abs/2106.15058",
          "publishedOn": "2021-06-30T02:01:00.558Z",
          "wordCount": 673,
          "title": "Improving Transferability of Adversarial Patches on Face Recognition with Generative Models. (arXiv:2106.15058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bjork_S/0/1/0/all/0/1\">Sara Bj&#xf6;rk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anfinsen_S/0/1/0/all/0/1\">Stian Normann Anfinsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naesset_E/0/1/0/all/0/1\">Erik N&#xe6;sset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gobakken_T/0/1/0/all/0/1\">Terje Gobakken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahabu_E/0/1/0/all/0/1\">Eliakimu Zahabu</a>",
          "description": "This paper studies construction of above-ground biomass (AGB) prediction maps\nfrom synthetic aperture radar (SAR) intensity images. The purpose is to improve\ntraditional regression models based on SAR intensity, trained with a limited\namount of AGB in situ measurements. Although it is costly to collect, data from\nairborne laser scanning (ALS) sensors are highly correlated with AGB.\nTherefore, we propose using AGB predictions based on ALS data as surrogate\nresponse variables for SAR data in a sequential modelling fashion. This\nincreases the amount of training data dramatically. To model the regression\nfunction between SAR intensity and ALS-predicted AGB we propose to utilise a\nconditional generative adversarial network (cGAN), i.e. the Pix2Pix\nconvolutional neural network. This enables the recreation of existing ALS-based\nAGB prediction maps. The generated synthesised ALS-based AGB predictions are\nevaluated qualitatively and quantitatively against ALS-based AGB predictions\nretrieved from a traditional non-sequential regression model trained in the\nsame area. Results show that the proposed architecture manages to capture\ncharacteristics of the actual data. This suggests that the use of ALS-guided\ngenerative models is a promising avenue for AGB prediction from SAR intensity.\nFurther research on this area has the potential of providing both large-scale\nand low-cost predictions of AGB.",
          "link": "http://arxiv.org/abs/2106.15020",
          "publishedOn": "2021-06-30T02:01:00.551Z",
          "wordCount": 658,
          "title": "Constructing Forest Biomass Prediction Maps from Radar Backscatter by Sequential Regression with a Conditional Generative Adversarial Network. (arXiv:2106.15020v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1\">Stylianos I. Venieris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panopoulos_I/0/1/0/all/0/1\">Ioannis Panopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1\">Ilias Leontiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venieris_I/0/1/0/all/0/1\">Iakovos S. Venieris</a>",
          "description": "The unprecedented performance of deep neural networks (DNNs) has led to large\nstrides in various Artificial Intelligence (AI) inference tasks, such as object\nand speech recognition. Nevertheless, deploying such AI models across commodity\ndevices faces significant challenges: large computational cost, multiple\nperformance objectives, hardware heterogeneity and a common need for high\naccuracy, together pose critical problems to the deployment of DNNs across the\nvarious embedded and mobile devices in the wild. As such, we have yet to\nwitness the mainstream usage of state-of-the-art deep learning algorithms\nacross consumer devices. In this paper, we provide preliminary answers to this\npotentially game-changing question by presenting an array of design techniques\nfor efficient AI systems. We start by examining the major roadblocks when\ntargeting both programmable processors and custom accelerators. Then, we\npresent diverse methods for achieving real-time performance following a\ncross-stack approach. These span model-, system- and hardware-level techniques,\nand their combination. Our findings provide illustrative examples of AI systems\nthat do not overburden mobile hardware, while also indicating how they can\nimprove inference accuracy. Moreover, we showcase how custom ASIC- and\nFPGA-based accelerators can be an enabling factor for next-generation AI\napplications, such as multi-DNN systems. Collectively, these results highlight\nthe critical need for further exploration as to how the various cross-stack\nsolutions can be best combined in order to bring the latest advances in deep\nlearning close to users, in a robust and efficient manner.",
          "link": "http://arxiv.org/abs/2106.15021",
          "publishedOn": "2021-06-30T02:01:00.544Z",
          "wordCount": 707,
          "title": "How to Reach Real-Time AI on Consumer Devices? Solutions for Programmable and Custom Architectures. (arXiv:2106.15021v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuli Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yucheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_S/0/1/0/all/0/1\">Suting Miao</a>",
          "description": "We present an object detection based approach to localize handwritten regions\nfrom documents, which initially aims to enhance the anonymization during the\ndata transmission. The concatenated fusion of original and preprocessed images\ncontaining both printed texts and handwritten notes or signatures are fed into\nthe convolutional neural network, where the bounding boxes are learned to\ndetect the handwriting. Afterwards, the handwritten regions can be processed\n(e.g. replaced with redacted signatures) to conceal the personally identifiable\ninformation (PII). This processing pipeline based on the deep learning network\nCascade R-CNN works at 10 fps on a GPU during the inference, which ensures the\nenhanced anonymization with minimal computational overheads. Furthermore, the\nimpressive generalizability has been empirically showcased: the trained model\nbased on the English-dominant dataset works well on the fictitious unseen\ninvoices, even in Chinese. The proposed approach is also expected to facilitate\nother tasks such as handwriting recognition and signature verification.",
          "link": "http://arxiv.org/abs/2106.14989",
          "publishedOn": "2021-06-30T02:01:00.538Z",
          "wordCount": 588,
          "title": "Object Detection Based Handwriting Localization. (arXiv:2106.14989v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanket_N/0/1/0/all/0/1\">Nitin J. Sanket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Chahat Deep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parameshwara_C/0/1/0/all/0/1\">Chethan M. Parameshwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1\">Cornelia Ferm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1\">Guido C.H.E. de Croon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloimonos_Y/0/1/0/all/0/1\">Yiannis Aloimonos</a>",
          "description": "The rapid rise of accessibility of unmanned aerial vehicles or drones pose a\nthreat to general security and confidentiality. Most of the commercially\navailable or custom-built drones are multi-rotors and are comprised of multiple\npropellers. Since these propellers rotate at a high-speed, they are generally\nthe fastest moving parts of an image and cannot be directly \"seen\" by a\nclassical camera without severe motion blur. We utilize a class of sensors that\nare particularly suitable for such scenarios called event cameras, which have a\nhigh temporal resolution, low-latency, and high dynamic range.\n\nIn this paper, we model the geometry of a propeller and use it to generate\nsimulated events which are used to train a deep neural network called EVPropNet\nto detect propellers from the data of an event camera. EVPropNet directly\ntransfers to the real world without any fine-tuning or retraining. We present\ntwo applications of our network: (a) tracking and following an unmarked drone\nand (b) landing on a near-hover drone. We successfully evaluate and demonstrate\nthe proposed approach in many real-world experiments with different propeller\nshapes and sizes. Our network can detect propellers at a rate of 85.1% even\nwhen 60% of the propeller is occluded and can run at upto 35Hz on a 2W power\nbudget. To our knowledge, this is the first deep learning-based solution for\ndetecting propellers (to detect drones). Finally, our applications also show an\nimpressive success rate of 92% and 90% for the tracking and landing tasks\nrespectively.",
          "link": "http://arxiv.org/abs/2106.15045",
          "publishedOn": "2021-06-30T02:01:00.532Z",
          "wordCount": 719,
          "title": "EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing And Following. (arXiv:2106.15045v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>",
          "description": "For distant iris recognition, a long focal length lens is generally used to\nensure the resolution ofiris images, which reduces the depth of field and leads\nto potential defocus blur. To accommodate users at different distances, it is\nnecessary to control focus quickly and accurately. While for users in motion,\nit is expected to maintain the correct focus on the iris area continuously. In\nthis paper, we introduced a novel rapid autofocus camera for active refocusing\nofthe iris area ofthe moving objects using a focus-tunable lens. Our end-to-end\ncomputational algorithm can predict the best focus position from one single\nblurred image and generate a lens diopter control signal automatically. This\nscene-based active manipulation method enables real-time focus tracking of the\niris area ofa moving object. We built a testing bench to collect real-world\nfocal stacks for evaluation of the autofocus methods. Our camera has reached an\nautofocus speed ofover 50 fps. The results demonstrate the advantages of our\nproposed camera for biometric perception in static and dynamic scenes. The code\nis available at https://github.com/Debatrix/AquulaCam.",
          "link": "http://arxiv.org/abs/2106.15069",
          "publishedOn": "2021-06-30T02:01:00.509Z",
          "wordCount": 624,
          "title": "An End-to-End Autofocus Camera for Iris on the Move. (arXiv:2106.15069v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zongyao Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_N/0/1/0/all/0/1\">Nolan B. Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beksi_W/0/1/0/all/0/1\">William J. Beksi</a>",
          "description": "In this paper, we introduce a new technique that combines two popular methods\nto estimate uncertainty in object detection. Quantifying uncertainty is\ncritical in real-world robotic applications. Traditional detection models can\nbe ambiguous even when they provide a high-probability output. Robot actions\nbased on high-confidence, yet unreliable predictions, may result in serious\nrepercussions. Our framework employs deep ensembles and Monte Carlo dropout for\napproximating predictive uncertainty, and it improves upon the uncertainty\nestimation quality of the baseline method. The proposed approach is evaluated\non publicly available synthetic image datasets captured from sequences of\nvideo.",
          "link": "http://arxiv.org/abs/2106.15007",
          "publishedOn": "2021-06-30T02:01:00.498Z",
          "wordCount": 544,
          "title": "An Uncertainty Estimation Framework for Probabilistic Object Detection. (arXiv:2106.15007v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14922",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Xu_C/0/1/0/all/0/1\">Chengyuan Xu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+McCully_C/0/1/0/all/0/1\">Curtis McCully</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dong_B/0/1/0/all/0/1\">Boning Dong</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Howell_D/0/1/0/all/0/1\">D. Andrew Howell</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sen_P/0/1/0/all/0/1\">Pradeep Sen</a>",
          "description": "Rejecting cosmic rays (CRs) is essential for scientific interpretation of\nCCD-captured data, but detecting CRs in single-exposure images has remained\nchallenging. Conventional CR-detection algorithms require tuning multiple\nparameters experimentally making it hard to automate across different\ninstruments or observation requests. Recent work using deep learning to train\nCR-detection models has demonstrated promising results. However,\ninstrument-specific models suffer from performance loss on images from\nground-based facilities not included in the training data. In this work, we\npresent Cosmic-CoNN, a deep-learning framework designed to produce generic\nCR-detection models. We build a large, diverse ground-based CR dataset\nleveraging thousands of images from the Las Cumbres Observatory global\ntelescope network to produce a generic CR-detection model which achieves a\n99.91% true-positive detection rate and maintains over 96.40% true-positive\nrates on unseen data from Gemini GMOS-N/S, with a false-positive rate of 0.01%.\nApart from the open-source framework and dataset, we also build a suite of\ntools including console commands, a web-based application, and Python APIs to\nmake automatic, robust CR detection widely accessible by the community of\nastronomers.",
          "link": "http://arxiv.org/abs/2106.14922",
          "publishedOn": "2021-06-30T02:01:00.485Z",
          "wordCount": 649,
          "title": "Cosmic-CoNN: A Cosmic Ray Detection Deep-Learning Framework, Dataset, and Toolkit. (arXiv:2106.14922v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1\">Niluthpol Mithun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seymour_Z/0/1/0/all/0/1\">Zach Seymour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Han-Pang Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "Class imbalance is a fundamental problem in computer vision applications such\nas semantic segmentation. Specifically, uneven class distributions in a\ntraining dataset often result in unsatisfactory performance on\nunder-represented classes. Many works have proposed to weight the standard\ncross entropy loss function with pre-computed weights based on class\nstatistics, such as the number of samples and class margins. There are two\nmajor drawbacks to these methods: 1) constantly up-weighting minority classes\ncan introduce excessive false positives in semantic segmentation; 2) a minority\nclass is not necessarily a hard class. The consequence is low precision due to\nexcessive false positives. In this regard, we propose a hard-class mining loss\nby reshaping the vanilla cross entropy loss such that it weights the loss for\neach class dynamically based on instantaneous recall performance. We show that\nthe novel recall loss changes gradually between the standard cross entropy loss\nand the inverse frequency weighted loss. Recall loss also leads to improved\nmean accuracy while offering competitive mean Intersection over Union (IoU)\nperformance. On Synthia dataset, recall loss achieves 9% relative improvement\non mean accuracy with competitive mean IoU using DeepLab-ResNet18 compared to\nthe cross entropy loss. Code available at\nhttps://github.com/PotatoTian/recall-semseg.",
          "link": "http://arxiv.org/abs/2106.14917",
          "publishedOn": "2021-06-30T02:01:00.479Z",
          "wordCount": 637,
          "title": "Striking the Right Balance: Recall Loss for Semantic Segmentation. (arXiv:2106.14917v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yunxiao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenxu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>",
          "description": "Face anti-spoofing (FAS) has lately attracted increasing attention due to its\nvital role in securing face recognition systems from presentation attacks\n(PAs). As more and more realistic PAs with novel types spring up, traditional\nFAS methods based on handcrafted features become unreliable due to their\nlimited representation capacity. With the emergence of large-scale academic\ndatasets in the recent decade, deep learning based FAS achieves remarkable\nperformance and dominates this area. However, existing reviews in this field\nmainly focus on the handcrafted features, which are outdated and uninspiring\nfor the progress of FAS community. In this paper, to stimulate future research,\nwe present the first comprehensive review of recent advances in deep learning\nbased FAS. It covers several novel and insightful components: 1) besides\nsupervision with binary label (e.g., '0' for bonafide vs. '1' for PAs), we also\ninvestigate recent methods with pixel-wise supervision (e.g., pseudo depth\nmap); 2) in addition to traditional intra-dataset evaluation, we collect and\nanalyze the latest methods specially designed for domain generalization and\nopen-set FAS; and 3) besides commercial RGB camera, we summarize the deep\nlearning applications under multi-modal (e.g., depth and infrared) or\nspecialized (e.g., light field and flash) sensors. We conclude this survey by\nemphasizing current open issues and highlighting potential prospects.",
          "link": "http://arxiv.org/abs/2106.14948",
          "publishedOn": "2021-06-30T02:01:00.474Z",
          "wordCount": 658,
          "title": "Deep Learning for Face Anti-Spoofing: A Survey. (arXiv:2106.14948v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ashish Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Ashwin Ramesh Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadeh_M/0/1/0/all/0/1\">Mohammad Zaki Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makedon_F/0/1/0/all/0/1\">Fillia Makedon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wylie_G/0/1/0/all/0/1\">Glenn Wylie</a>",
          "description": "Functional magnetic resonance imaging (fMRI) is a neuroimaging technique that\nrecords neural activations in the brain by capturing the blood oxygen level in\ndifferent regions based on the task performed by a subject. Given fMRI data,\nthe problem of predicting the state of cognitive fatigue in a person has not\nbeen investigated to its full extent. This paper proposes tackling this issue\nas a multi-class classification problem by dividing the state of cognitive\nfatigue into six different levels, ranging from no-fatigue to extreme fatigue\nconditions. We built a spatio-temporal model that uses convolutional neural\nnetworks (CNN) for spatial feature extraction and a long short-term memory\n(LSTM) network for temporal modeling of 4D fMRI scans. We also applied a\nself-supervised method called MoCo to pre-train our model on a public dataset\nBOLD5000 and fine-tuned it on our labeled dataset to classify cognitive\nfatigue. Our novel dataset contains fMRI scans from Traumatic Brain Injury\n(TBI) patients and healthy controls (HCs) while performing a series of\ncognitive tasks. This method establishes a state-of-the-art technique to\nanalyze cognitive fatigue from fMRI data and beats previous approaches to solve\nthis problem.",
          "link": "http://arxiv.org/abs/2106.15009",
          "publishedOn": "2021-06-30T02:01:00.469Z",
          "wordCount": 634,
          "title": "Understanding Cognitive Fatigue from fMRI Scans with Self-supervised Learning. (arXiv:2106.15009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuxuan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>",
          "description": "Object detection plays an important role in self-driving cars for security\ndevelopment. However, mobile systems on self-driving cars with limited\ncomputation resources lead to difficulties for object detection. To facilitate\nthis, we propose a compiler-aware neural pruning search framework to achieve\nhigh-speed inference on autonomous vehicles for 2D and 3D object detection. The\nframework automatically searches the pruning scheme and rate for each layer to\nfind a best-suited pruning for optimizing detection accuracy and speed\nperformance under compiler optimization. Our experiments demonstrate that for\nthe first time, the proposed method achieves (close-to) real-time, 55ms and\n99ms inference times for YOLOv4 based 2D object detection and PointPillars\nbased 3D detection, respectively, on an off-the-shelf mobile phone with minor\n(or no) accuracy loss.",
          "link": "http://arxiv.org/abs/2106.14943",
          "publishedOn": "2021-06-30T02:01:00.457Z",
          "wordCount": 579,
          "title": "Achieving Real-Time Object Detection on MobileDevices with Neural Pruning Search. (arXiv:2106.14943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14947",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fabian_Z/0/1/0/all/0/1\">Zalan Fabian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heckel_R/0/1/0/all/0/1\">Reinhard Heckel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>",
          "description": "Deep neural networks have emerged as very successful tools for image\nrestoration and reconstruction tasks. These networks are often trained\nend-to-end to directly reconstruct an image from a noisy or corrupted\nmeasurement of that image. To achieve state-of-the-art performance, training on\nlarge and diverse sets of images is considered critical. However, it is often\ndifficult and/or expensive to collect large amounts of training images.\nInspired by the success of Data Augmentation (DA) for classification problems,\nin this paper, we propose a pipeline for data augmentation for accelerated MRI\nreconstruction and study its effectiveness at reducing the required training\ndata in a variety of settings. Our DA pipeline, MRAugment, is specifically\ndesigned to utilize the invariances present in medical imaging measurements as\nnaive DA strategies that neglect the physics of the problem fail. Through\nextensive studies on multiple datasets we demonstrate that in the low-data\nregime DA prevents overfitting and can match or even surpass the state of the\nart while using significantly fewer training data, whereas in the high-data\nregime it has diminishing returns. Furthermore, our findings show that DA can\nimprove the robustness of the model against various shifts in the test\ndistribution.",
          "link": "http://arxiv.org/abs/2106.14947",
          "publishedOn": "2021-06-30T02:01:00.452Z",
          "wordCount": 663,
          "title": "Data augmentation for deep learning based accelerated MRI reconstruction with limited data. (arXiv:2106.14947v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1\">Alexander W. Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellnhofer_P/0/1/0/all/0/1\">Petr Kellnhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>",
          "description": "Novel view synthesis is a long-standing problem in machine learning and\ncomputer vision. Significant progress has recently been made in developing\nneural scene representations and rendering techniques that synthesize\nphotorealistic images from arbitrary views. These representations, however, are\nextremely slow to train and often also slow to render. Inspired by neural\nvariants of image-based rendering, we develop a new neural rendering approach\nwith the goal of quickly learning a high-quality representation which can also\nbe rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a\nunique combination of a neural shape representation and 2D CNN-based image\nfeature extraction, aggregation, and re-projection. To push representation\nconvergence times down to minutes, we leverage meta learning to learn neural\nshape and image feature priors which accelerate training. The optimized shape\nand image features can then be extracted using traditional graphics techniques\nand rendered in real time. We show that MetaNLR++ achieves similar or better\nnovel view synthesis results in a fraction of the time that competing methods\nrequire.",
          "link": "http://arxiv.org/abs/2106.14942",
          "publishedOn": "2021-06-30T02:01:00.413Z",
          "wordCount": 616,
          "title": "Fast Training of Neural Lumigraph Representations using Meta Learning. (arXiv:2106.14942v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Comandur_B/0/1/0/all/0/1\">Bharath Comandur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kak_A/0/1/0/all/0/1\">Avinash C. Kak</a>",
          "description": "We present a novel multi-view training framework and CNN architecture for\ncombining information from multiple overlapping satellite images and noisy\ntraining labels derived from OpenStreetMap (OSM) to semantically label\nbuildings and roads across large geographic regions (100 km$^2$). Our approach\nto multi-view semantic segmentation yields a 4-7% improvement in the per-class\nIoU scores compared to the traditional approaches that use the views\nindependently of one another. A unique (and, perhaps, surprising) property of\nour system is that modifications that are added to the tail-end of the CNN for\nlearning from the multi-view data can be discarded at the time of inference\nwith a relatively small penalty in the overall performance. This implies that\nthe benefits of training using multiple views are absorbed by all the layers of\nthe network. Additionally, our approach only adds a small overhead in terms of\nthe GPU-memory consumption even when training with as many as 32 views per\nscene. The system we present is end-to-end automated, which facilitates\ncomparing the classifiers trained directly on true orthophotos vis-a-vis first\ntraining them on the off-nadir images and subsequently translating the\npredicted labels to geographical coordinates. With no human supervision, our\nIoU scores for the buildings and roads classes are 0.8 and 0.64 respectively\nwhich are better than state-of-the-art approaches that use OSM labels and that\nare not completely automated.",
          "link": "http://arxiv.org/abs/2008.10271",
          "publishedOn": "2021-06-29T01:55:17.839Z",
          "wordCount": 774,
          "title": "Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images and Noisy OSM Training Labels. (arXiv:2008.10271v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14758",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuwei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Ying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Daoye Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1\">Jingwei Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1\">Zhaoxiang Ye</a>",
          "description": "Low-dose CT has been a key diagnostic imaging modality to reduce the\npotential risk of radiation overdose to patient health. Despite recent\nadvances, CNN-based approaches typically apply filters in a spatially invariant\nway and adopt similar pixel-level losses, which treat all regions of the CT\nimage equally and can be inefficient when fine-grained structures coexist with\nnon-uniformly distributed noises. To address this issue, we propose a\nStructure-preserving Kernel Prediction Network (StructKPN) that combines the\nkernel prediction network with a structure-aware loss function that utilizes\nthe pixel gradient statistics and guides the model towards spatially-variant\nfilters that enhance noise removal, prevent over-smoothing and preserve\ndetailed structures for different regions in CT imaging. Extensive experiments\ndemonstrated that our approach achieved superior performance on both synthetic\nand non-synthetic datasets, and better preserves structures that are highly\ndesired in clinical screening and low-dose protocol optimization.",
          "link": "http://arxiv.org/abs/2105.14758",
          "publishedOn": "2021-06-29T01:55:17.641Z",
          "wordCount": 614,
          "title": "Low-Dose CT Denoising Using a Structure-Preserving Kernel Prediction Network. (arXiv:2105.14758v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10510",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Nasim_M/0/1/0/all/0/1\">M Quamer Nasim</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Maiti_T/0/1/0/all/0/1\">Tannistha Maiti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Srivastava_A/0/1/0/all/0/1\">Ayush Srivastava</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Singh_T/0/1/0/all/0/1\">Tarry Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>",
          "description": "Deep neural networks (DNNs) can learn accurately from large quantities of\nlabeled input data, but DNNs sometimes fail to generalize to test data sampled\nfrom different input distributions. Unsupervised Deep Domain Adaptation (DDA)\nproves useful when no input labels are available, and distribution shifts are\nobserved in the target domain (TD). Experiments are performed on seismic images\nof the F3 block 3D dataset from offshore Netherlands (source domain; SD) and\nPenobscot 3D survey data from Canada (target domain; TD). Three geological\nclasses from SD and TD that have similar reflection patterns are considered. In\nthe present study, an improved deep neural network architecture named\nEarthAdaptNet (EAN) is proposed to semantically segment the seismic images. We\nspecifically use a transposed residual unit to replace the traditional dilated\nconvolution in the decoder block. The EAN achieved a pixel-level accuracy >84%\nand an accuracy of ~70% for the minority classes, showing improved performance\ncompared to existing architectures. In addition, we introduced the CORAL\n(Correlation Alignment) method to the EAN to create an unsupervised deep domain\nadaptation network (EAN-DDA) for the classification of seismic reflections\nfromF3 and Penobscot. Maximum class accuracy achieved was ~99% for class 2 of\nPenobscot with >50% overall accuracy. Taken together, EAN-DDA has the potential\nto classify target domain seismic facies classes with high accuracy.",
          "link": "http://arxiv.org/abs/2011.10510",
          "publishedOn": "2021-06-29T01:55:17.568Z",
          "wordCount": 701,
          "title": "Seismic Facies Analysis: A Deep Domain Adaptation Approach. (arXiv:2011.10510v2 [physics.geo-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Devansh Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>",
          "description": "Multimodal Machine Translation (MMT) enriches the source text with visual\ninformation for translation. It has gained popularity in recent years, and\nseveral pipelines have been proposed in the same direction. Yet, the task lacks\nquality datasets to illustrate the contribution of visual modality in the\ntranslation systems. In this paper, we propose our system under the team name\nVolta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We\nalso participate in the textual-only subtask of the same language pair for\nwhich we use mBART, a pretrained multilingual sequence-to-sequence model. For\nmultimodal translation, we propose to enhance the textual input by bringing the\nvisual information to a textual domain by extracting object tags from the\nimage. We also explore the robustness of our system by systematically degrading\nthe source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test\nset and challenge set of the multimodal task.",
          "link": "http://arxiv.org/abs/2106.00250",
          "publishedOn": "2021-06-29T01:55:17.561Z",
          "wordCount": 625,
          "title": "ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07566",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fanyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Haotian Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1\">Cheng Shen</a>",
          "description": "Attention mechanism has shown enormous potential for single image\nsuper-resolution (SISR). However, existing works only proposed some attention\nmechanism for a specific network. A universal attention mechanism for SISR,\nwhich could further improve the performance of networks without attention and\nprovide a baseline for networks with attention, is still lacking. To fit this\ngap, we propose a lightweight and efficient Balanced Attention Mechanism (BAM),\nwhich consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial\nAttention Module (MSAM) in parallel. The information extraction mechanism of\nACAM and MSAM effectively filters redundant information, making the overall\nstructure of BAM very lightweight. Owing to the parallel structure, during the\ngradient backpropagation process of BAM, ACAM and MSAM not only conduct\nself-optimization, but also mutual optimization so as to generate more balanced\nattention information. To verify the effectiveness and robustness of BAM, we\napplied it to 12 state-ofthe-art SISR networks. The results on 4 benchmark\ndatasets demonstrate that BAM can efficiently improve the networks'\nperformance, and for those with attention, the substitution with BAM further\nreduces the amount of parameters and increase the inference speed. Moreover,\nablation experiments were conducted to prove the minimalism of BAM.",
          "link": "http://arxiv.org/abs/2104.07566",
          "publishedOn": "2021-06-29T01:55:17.539Z",
          "wordCount": 670,
          "title": "BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10762",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "Random field and random cluster theory are used to prove certain mathematical\nresults concerning the probability distribution of image pixel intensities\ncharacterized as generic $2D$ integer arrays. The size of the smallest bounded\nregion within an image is estimated for segmenting an image, from which, the\nequilibrium distribution of intensities can be recovered. From the estimated\nbounded regions, properties of the sub-optimal and equilibrium distributions of\nintensities are derived, which leads to an image compression methodology\nwhereby only slightly more than half of all pixels are required for a\nworst-case reconstruction of the original image. An example in unsupervised\nobject detection illustrates the mathematical results.",
          "link": "http://arxiv.org/abs/2104.10762",
          "publishedOn": "2021-06-29T01:55:17.533Z",
          "wordCount": 635,
          "title": "Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v8 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zha_J/0/1/0/all/0/1\">Jiajun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Attention has been proved to be an efficient mechanism to capture long-range\ndependencies. However, so far it has not been deployed in invertible networks.\nThis is due to the fact that in order to make a network invertible, every\ncomponent within the network needs to be a bijective transformation, but a\nnormal attention block is not. In this paper, we propose invertible attention\nthat can be plugged into existing invertible models. We mathematically and\nexperimentally prove that the invertibility of an attention model can be\nachieved by carefully constraining its Lipschitz constant. We validate the\ninvertibility of our invertible attention on image reconstruction task with 3\npopular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible\nattention achieves similar performance in comparison with normal non-invertible\nattention on dense prediction tasks. The code is available at\nhttps://github.com/Schwartz-Zha/InvertibleAttention",
          "link": "http://arxiv.org/abs/2106.09003",
          "publishedOn": "2021-06-29T01:55:17.527Z",
          "wordCount": 594,
          "title": "Invertible Attention. (arXiv:2106.09003v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tuong Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh X. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjiputra_E/0/1/0/all/0/1\">Erman Tjiputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quang D. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Transfer learning is an important step to extract meaningful features and\novercome the data limitation in the medical Visual Question Answering (VQA)\ntask. However, most of the existing medical VQA methods rely on external data\nfor transfer learning, while the meta-data within the dataset is not fully\nutilized. In this paper, we present a new multiple meta-model quantifying\nmethod that effectively learns meta-annotation and leverages meaningful\nfeatures to the medical VQA task. Our proposed method is designed to increase\nmeta-data by auto-annotation, deal with noisy labels, and output meta-models\nwhich provide robust features for medical VQA tasks. Extensively experimental\nresults on two public medical VQA datasets show that our approach achieves\nsuperior accuracy in comparison with other state-of-the-art methods, while does\nnot require external data to train meta-models.",
          "link": "http://arxiv.org/abs/2105.08913",
          "publishedOn": "2021-06-29T01:55:17.521Z",
          "wordCount": 604,
          "title": "Multiple Meta-model Quantifying for Medical Visual Question Answering. (arXiv:2105.08913v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Dongchen Lu</a>",
          "description": "Rotated object detection is a challenging issue of computer vision field.\nLoss of spatial information and confusion of parametric order have been the\nbottleneck for rotated detection accuracy. In this paper, we propose an\norientation-sensitive keypoint based rotated detector OSKDet. We adopt a set of\nkeypoints to characterize the target and predict the keypoint heatmap on ROI to\nform a rotated target. By proposing the orientation-sensitive heatmap, OSKDet\ncould learn the shape and direction of rotated target implicitly and has\nstronger modeling capabilities for target representation, which improves the\nlocalization accuracy and acquires high quality detection results. To extract\nhighly effective features at border areas, we design a rotation-aware\ndeformable convolution module. Furthermore, we explore a new keypoint reorder\nalgorithm and feature fusion module based on the angle distribution to\neliminate the confusion of keypoint order. Experimental results on several\npublic benchmarks show the state-of-the-art performance of OSKDet.\nSpecifically, we achieve an AP of 77.81% on DOTA, 89.91% on HRSC2016, and\n97.18% on UCAS-AOD, respectively.",
          "link": "http://arxiv.org/abs/2104.08697",
          "publishedOn": "2021-06-29T01:55:17.488Z",
          "wordCount": 621,
          "title": "OSKDet: Towards Orientation-sensitive Keypoint Localization for Rotated Object Detection. (arXiv:2104.08697v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Automatically generating radiology reports can improve current clinical\npractice in diagnostic radiology. On one hand, it can relieve radiologists from\nthe heavy burden of report writing; On the other hand, it can remind\nradiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.\nYet, this task remains a challenging job for data-driven neural networks, due\nto the serious visual and textual data biases. To this end, we propose a\nPosterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to\nimitate the working patterns of radiologists, who will first examine the\nabnormal regions and assign the disease topic tags to the abnormal regions, and\nthen rely on the years of prior medical knowledge and prior working experience\naccumulations to write reports. Thus, the PPKED includes three modules:\nPosterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and\nMulti-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior\nknowledge, which provides explicit abnormal visual regions to alleviate visual\ndata bias; PrKE explores the prior knowledge from the prior medical knowledge\ngraph (medical knowledge) and prior radiology reports (working experience) to\nalleviate textual data bias. The explored knowledge is distilled by the MKD to\ngenerate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our\nmethod is able to outperform previous state-of-the-art models on these two\ndatasets.",
          "link": "http://arxiv.org/abs/2106.06963",
          "publishedOn": "2021-06-29T01:55:17.464Z",
          "wordCount": 689,
          "title": "Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation. (arXiv:2106.06963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_T/0/1/0/all/0/1\">Tomas Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1\">Fabian Prada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1\">Takaaki Shiratori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shih-En Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1\">Yaser Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1\">Jason Saragih</a>",
          "description": "We present a learning-based method for building driving-signal aware\nfull-body avatars. Our model is a conditional variational autoencoder that can\nbe animated with incomplete driving signals, such as human pose and facial\nkeypoints, and produces a high-quality representation of human geometry and\nview-dependent appearance. The core intuition behind our method is that better\ndrivability and generalization can be achieved by disentangling the driving\nsignals and remaining generative factors, which are not available during\nanimation. To this end, we explicitly account for information deficiency in the\ndriving signal by introducing a latent space that exclusively captures the\nremaining information, thus enabling the imputation of the missing factors\nrequired during full-body animation, while remaining faithful to the driving\nsignal. We also propose a learnable localized compression for the driving\nsignal which promotes better generalization, and helps minimize the influence\nof global chance-correlations often found in real datasets. For a given driving\nsignal, the resulting variational model produces a compact space of uncertainty\nfor missing factors that allows for an imputation strategy best suited to a\nparticular application. We demonstrate the efficacy of our approach on the\nchallenging problem of full-body animation for virtual telepresence with\ndriving signals acquired from minimal sensors placed in the environment and\nmounted on a VR-headset.",
          "link": "http://arxiv.org/abs/2105.10441",
          "publishedOn": "2021-06-29T01:55:17.458Z",
          "wordCount": 686,
          "title": "Driving-Signal Aware Full-Body Avatars. (arXiv:2105.10441v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuxin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhexiong Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yunqiu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>",
          "description": "The transformer networks are particularly good at modeling long-range\ndependencies within a long sequence. In this paper, we conduct research on\napplying the transformer networks for salient object detection (SOD). We adopt\nthe dense transformer backbone for fully supervised RGB image based SOD, RGB-D\nimage pair based SOD, and weakly supervised SOD within a unified framework\nbased on the observation that the transformer backbone can provide accurate\nstructure modeling, which makes it powerful in learning from weak labels with\nless structure information. Further, we find that the vision transformer\narchitectures do not offer direct spatial supervision, instead encoding\nposition as a feature. Therefore, we investigate the contributions of two\nstrategies to provide stronger spatial supervision through the transformer\nlayers within our unified framework, namely deep supervision and\ndifficulty-aware learning. We find that deep supervision can get gradients back\ninto the higher level features, thus leads to uniform activation within the\nsame semantic object. Difficulty-aware learning on the other hand is capable of\nidentifying the hard pixels for effective hard negative mining. We also\nvisualize features of conventional backbone and transformer backbone before and\nafter fine-tuning them for SOD, and find that transformer backbone encodes more\naccurate object structure information and more distinct semantic information\nwithin the lower and higher level features respectively. We also apply our\nmodel to camouflaged object detection (COD) and achieve similar observations as\nthe above three SOD tasks. Extensive experimental results on various SOD and\nCOD tasks illustrate that transformer networks can transform SOD and COD,\nleading to new benchmarks for each related task. The source code and\nexperimental results are available via our project page:\nhttps://github.com/fupiao1998/TrasformerSOD.",
          "link": "http://arxiv.org/abs/2104.10127",
          "publishedOn": "2021-06-29T01:55:17.443Z",
          "wordCount": 757,
          "title": "Transformer Transforms Salient Object Detection and Camouflaged Object Detection. (arXiv:2104.10127v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Batra_H/0/1/0/all/0/1\">Himanshu Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "Sentiment analysis can provide a suitable lead for the tools used in software\nengineering along with the API recommendation systems and relevant libraries to\nbe used. In this context, the existing tools like SentiCR, SentiStrength-SE,\netc. exhibited low f1-scores that completely defeats the purpose of deployment\nof such strategies, thereby there is enough scope for performance improvement.\nRecent advancements show that transformer based pre-trained models (e.g., BERT,\nRoBERTa, ALBERT, etc.) have displayed better results in the text classification\ntask. Following this context, the present research explores different\nBERT-based models to analyze the sentences in GitHub comments, Jira comments,\nand Stack Overflow posts. The paper presents three different strategies to\nanalyse BERT based model for sentiment analysis, where in the first strategy\nthe BERT based pre-trained models are fine-tuned; in the second strategy an\nensemble model is developed from BERT variants, and in the third strategy a\ncompressed model (Distil BERT) is used. The experimental results show that the\nBERT based ensemble approach and the compressed BERT model attain improvements\nby 6-12% over prevailing tools for the F1 measure on all three datasets.",
          "link": "http://arxiv.org/abs/2106.02581",
          "publishedOn": "2021-06-29T01:55:17.436Z",
          "wordCount": 636,
          "title": "BERT based sentiment analysis: A software engineering perspective. (arXiv:2106.02581v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akiva_P/0/1/0/all/0/1\">Peri Akiva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dana_K/0/1/0/all/0/1\">Kristin Dana</a>",
          "description": "The costly process of obtaining semantic segmentation labels has driven\nresearch towards weakly supervised semantic segmentation (WSSS) methods, using\nonly image-level, point, or box labels. The lack of dense scene representation\nrequires methods to increase complexity to obtain additional semantic\ninformation about the scene, often done through multiple stages of training and\nrefinement. Current state-of-the-art (SOTA) models leverage image-level labels\nto produce class activation maps (CAMs) which go through multiple stages of\nrefinement before they are thresholded to make pseudo-masks for supervision.\nThe multi-stage approach is computationally expensive, and dependency on\nimage-level labels for CAMs generation lacks generalizability to more complex\nscenes. In contrary, our method offers a single-stage approach generalizable to\narbitrary dataset, that is trainable from scratch, without any dependency on\npre-trained backbones, classification, or separate refinement tasks. We utilize\npoint annotations to generate reliable, on-the-fly pseudo-masks through refined\nand filtered features. While our method requires point annotations that are\nonly slightly more expensive than image-level annotations, we are to\ndemonstrate SOTA performance on benchmark datasets (PascalVOC 2012), as well as\nsignificantly outperform other SOTA WSSS methods on recent real-world datasets\n(CRAID, CityPersons, IAD).",
          "link": "http://arxiv.org/abs/2106.10309",
          "publishedOn": "2021-06-29T01:55:17.398Z",
          "wordCount": 637,
          "title": "Towards Single Stage Weakly Supervised Semantic Segmentation. (arXiv:2106.10309v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Channel Pruning has been long studied to compress CNNs for efficient image\nclassification. Prior works implement channel pruning in an unexplainable\nmanner, which tends to reduce the final classification errors while failing to\nconsider the internal influence of each channel. In this paper, we conduct\nchannel pruning in a white box. Through deep visualization of feature maps\nactivated by different channels, we observe that different channels have a\nvarying contribution to different categories in image classification. Inspired\nby this, we choose to preserve channels contributing to most categories.\nSpecifically, to model the contribution of each channel to differentiating\ncategories, we develop a class-wise mask for each channel, implemented in a\ndynamic training manner w.r.t. the input image's category. On the basis of the\nlearned class-wise mask, we perform a global voting mechanism to remove\nchannels with less category discrimination. Lastly, a fine-tuning process is\nconducted to recover the performance of the pruned model. To our best\nknowledge, it is the first time that CNN interpretability theory is considered\nto guide channel pruning. Extensive experiments on representative image\nclassification tasks demonstrate the superiority of our White-Box over many\nstate-of-the-arts. For instance, on CIFAR-10, it reduces 65.23% FLOPs with even\n0.62% accuracy improvement for ResNet-110. On ILSVRC-2012, White-Box achieves a\n45.6% FLOPs reduction with only a small loss of 0.83% in the top-1 accuracy for\nResNet-50.",
          "link": "http://arxiv.org/abs/2104.11883",
          "publishedOn": "2021-06-29T01:55:17.096Z",
          "wordCount": 707,
          "title": "Channel Pruning in a White Box for Efficient Image Classification. (arXiv:2104.11883v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_X/0/1/0/all/0/1\">Xiaofei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangtao Xie</a>",
          "description": "This is a short technical report introducing the solution of Team Rat for\nShort-video Parsing Face Parsing Track of The 3rd Person in Context (PIC)\nWorkshop and Challenge at CVPR 2021.\n\nIn this report, we propose an Edge-Aware Network (EANet) that uses edge\ninformation to refine the segmentation edge. To further obtain the finer edge\nresults, we introduce edge attention loss that only compute cross entropy on\nthe edges, it can effectively reduce the classification error around edge and\nget more smooth boundary. Benefiting from the edge information and edge\nattention loss, the proposed EANet achieves 86.16\\% accuracy in the Short-video\nFace Parsing track of the 3rd Person in Context (PIC) Workshop and Challenge,\nranked the third place.",
          "link": "http://arxiv.org/abs/2106.07409",
          "publishedOn": "2021-06-29T01:55:17.090Z",
          "wordCount": 569,
          "title": "3rd Place Solution for Short-video Face Parsing Challenge. (arXiv:2106.07409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Ji Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_B/0/1/0/all/0/1\">Benjamin Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>",
          "description": "The rapid progress in 3D scene understanding has come with growing demand for\ndata; however, collecting and annotating 3D scenes (e.g. point clouds) are\nnotoriously hard. For example, the number of scenes (e.g. indoor rooms) that\ncan be accessed and scanned might be limited; even given sufficient data,\nacquiring 3D labels (e.g. instance masks) requires intensive human labor. In\nthis paper, we explore data-efficient learning for 3D point cloud. As a first\nstep towards this direction, we propose Contrastive Scene Contexts, a 3D\npre-training method that makes use of both point-level correspondences and\nspatial contexts in a scene. Our method achieves state-of-the-art results on a\nsuite of benchmarks where training data or labels are scarce. Our study reveals\nthat exhaustive labelling of 3D point clouds might be unnecessary; and\nremarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89%\n(instance segmentation) and 96% (semantic segmentation) of the baseline\nperformance that uses full annotations.",
          "link": "http://arxiv.org/abs/2012.09165",
          "publishedOn": "2021-06-29T01:55:17.067Z",
          "wordCount": 635,
          "title": "Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts. (arXiv:2012.09165v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khose_S/0/1/0/all/0/1\">Sahil Khose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1\">Abhiraj Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ankita Ghosh</a>",
          "description": "FloodNet is a high-resolution image dataset acquired by a small UAV platform,\nDJI Mavic Pro quadcopters, after Hurricane Harvey. The dataset presents a\nunique challenge of advancing the damage assessment process for post-disaster\nscenarios using unlabeled and limited labeled dataset. We propose a solution to\naddress their classification and semantic segmentation challenge. We approach\nthis problem by generating pseudo labels for both classification and\nsegmentation during training and slowly incrementing the amount by which the\npseudo label loss affects the final loss. Using this semi-supervised method of\ntraining helped us improve our baseline supervised loss by a huge margin for\nclassification, allowing the model to generalize and perform better on the\nvalidation and test splits of the dataset. In this paper, we compare and\ncontrast the various methods and models for image classification and semantic\nsegmentation on the FloodNet dataset.",
          "link": "http://arxiv.org/abs/2105.08655",
          "publishedOn": "2021-06-29T01:55:17.060Z",
          "wordCount": 611,
          "title": "Semi-Supervised Classification and Segmentation on High Resolution Aerial Images. (arXiv:2105.08655v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaves_L/0/1/0/all/0/1\">Levy Chaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bissoto_A/0/1/0/all/0/1\">Alceu Bissoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1\">Eduardo Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1\">Sandra Avila</a>",
          "description": "Self-supervised pre-training appears as an advantageous alternative to\nsupervised pre-trained for transfer learning. By synthesizing annotations on\npretext tasks, self-supervision allows to pre-train models on large amounts of\npseudo-labels before fine-tuning them on the target task. In this work, we\nassess self-supervision for the diagnosis of skin lesions, comparing three\nself-supervised pipelines to a challenging supervised baseline, on five test\ndatasets comprising in- and out-of-distribution samples. Our results show that\nself-supervision is competitive both in improving accuracies and in reducing\nthe variability of outcomes. Self-supervision proves particularly useful for\nlow training data scenarios ($<1\\,500$ and $<150$ samples), where its ability\nto stabilize the outcomes is essential to provide sound results.",
          "link": "http://arxiv.org/abs/2106.09229",
          "publishedOn": "2021-06-29T01:55:17.041Z",
          "wordCount": 568,
          "title": "An Evaluation of Self-Supervised Pre-Training for Skin-Lesion Analysis. (arXiv:2106.09229v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03072",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_H/0/1/0/all/0/1\">Haoming Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy S. Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_S/0/1/0/all/0/1\">Shuhang Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheon_M/0/1/0/all/0/1\">Manri Cheon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoon_S/0/1/0/all/0/1\">Sungjun Yoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_B/0/1/0/all/0/1\">Byungyeon Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Junwoo Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1\">Haiyang Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bin_Y/0/1/0/all/0/1\">Yi Bin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1\">Yuqing Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_H/0/1/0/all/0/1\">Hengliang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jingyu Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_Q/0/1/0/all/0/1\">Qingyan Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_S/0/1/0/all/0/1\">Shuwei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_W/0/1/0/all/0/1\">Weihao Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_M/0/1/0/all/0/1\">Mingdeng Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yifan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_L/0/1/0/all/0/1\">Longtao Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_Y/0/1/0/all/0/1\">Yiting Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Junlin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thong_W/0/1/0/all/0/1\">William Thong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_J/0/1/0/all/0/1\">Jose Costa Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_H/0/1/0/all/0/1\">Hengxing Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_P/0/1/0/all/0/1\">Pengfei Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayyoubzadeh_S/0/1/0/all/0/1\">Seyed Mehdi Ayyoubzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Royat_A/0/1/0/all/0/1\">Ali Royat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fezza_S/0/1/0/all/0/1\">Sid Ahmed Fezza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammou_D/0/1/0/all/0/1\">Dounia Hammou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahn_S/0/1/0/all/0/1\">Sewoong Ahn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoon_G/0/1/0/all/0/1\">Gwangjin Yoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsubota_K/0/1/0/all/0/1\">Koki Tsubota</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akutsu_H/0/1/0/all/0/1\">Hiroaki Akutsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aizawa_K/0/1/0/all/0/1\">Kiyoharu Aizawa</a>",
          "description": "This paper reports on the NTIRE 2021 challenge on perceptual image quality\nassessment (IQA), held in conjunction with the New Trends in Image Restoration\nand Enhancement workshop (NTIRE) workshop at CVPR 2021. As a new type of image\nprocessing technology, perceptual image processing algorithms based on\nGenerative Adversarial Networks (GAN) have produced images with more realistic\ntextures. These output images have completely different characteristics from\ntraditional distortions, thus pose a new challenge for IQA methods to evaluate\ntheir visual quality. In comparison with previous IQA challenges, the training\nand testing datasets in this challenge include the outputs of perceptual image\nprocessing algorithms and the corresponding subjective scores. Thus they can be\nused to develop and evaluate IQA methods on GAN-based distortions. The\nchallenge has 270 registered participants in total. In the final testing stage,\n13 participating teams submitted their models and fact sheets. Almost all of\nthem have achieved much better results than existing IQA methods, while the\nwinning method can demonstrate state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2105.03072",
          "publishedOn": "2021-06-29T01:55:17.004Z",
          "wordCount": 730,
          "title": "NTIRE 2021 Challenge on Perceptual Image Quality Assessment. (arXiv:2105.03072v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1\">Trung Tan Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ly_N/0/1/0/all/0/1\">Nam Tuan Ly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1\">Masaki Nakagawa</a>",
          "description": "In this paper, we propose an RNN-Transducer model for recognizing Japanese\nand Chinese offline handwritten text line images. As far as we know, it is the\nfirst approach that adopts the RNN-Transducer model for offline handwritten\ntext recognition. The proposed model consists of three main components: a\nvisual feature encoder that extracts visual features from an input image by CNN\nand then encodes the visual features by BLSTM; a linguistic context encoder\nthat extracts and encodes linguistic features from the input image by embedded\nlayers and LSTM; and a joint decoder that combines and then decodes the visual\nfeatures and the linguistic features into the final label sequence by fully\nconnected and softmax layers. The proposed model takes advantage of both visual\nand linguistic information from the input image. In the experiments, we\nevaluated the performance of the proposed model on the two datasets: Kuzushiji\nand SCUT-EPT. Experimental results show that the proposed model achieves\nstate-of-the-art performance on all datasets.",
          "link": "http://arxiv.org/abs/2106.14459",
          "publishedOn": "2021-06-29T01:55:16.982Z",
          "wordCount": 607,
          "title": "Recurrent neural network transducer for Japanese and Chinese offline handwritten text recognition. (arXiv:2106.14459v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wenyuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyong Li</a>",
          "description": "A table arranging data in rows and columns is a very effective data\nstructure, which has been widely used in business and scientific research.\nConsidering large-scale tabular data in online and offline documents, automatic\ntable recognition has attracted increasing attention from the document analysis\ncommunity. Though human can easily understand the structure of tables, it\nremains a challenge for machines to understand that, especially due to a\nvariety of different table layouts and styles. Existing methods usually model a\ntable as either the markup sequence or the adjacency matrix between different\ntable cells, failing to address the importance of the logical location of table\ncells, e.g., a cell is located in the first row and the second column of the\ntable. In this paper, we reformulate the problem of table structure recognition\nas the table graph reconstruction, and propose an end-to-end trainable table\ngraph reconstruction network (TGRNet) for table structure recognition.\nSpecifically, the proposed method has two main branches, a cell detection\nbranch and a cell logical location branch, to jointly predict the spatial\nlocation and the logical location of different cells. Experimental results on\nthree popular table recognition datasets and a new dataset with table graph\nannotations (TableGraph-350K) demonstrate the effectiveness of the proposed\nTGRNet for table structure recognition. Code and annotations will be made\npublicly available.",
          "link": "http://arxiv.org/abs/2106.10598",
          "publishedOn": "2021-06-29T01:55:16.953Z",
          "wordCount": 681,
          "title": "TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition. (arXiv:2106.10598v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lijin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1\">Yusuke Sugano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>",
          "description": "In this report, we describe the technical details of our submission to the\n2021 EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action\nRecognition. Leveraging multiple modalities has been proved to benefit the\nUnsupervised Domain Adaptation (UDA) task. In this work, we present Multi-Modal\nMutual Enhancement Module (M3EM), a deep module for jointly considering\ninformation from multiple modalities to find the most transferable\nrepresentations across domains. We achieve this by implementing two sub-modules\nfor enhancing each modality using the context of other modalities. The first\nsub-module exchanges information across modalities through the semantic space,\nwhile the second sub-module finds the most transferable spatial region based on\nthe consensus of all modalities.",
          "link": "http://arxiv.org/abs/2106.10026",
          "publishedOn": "2021-06-29T01:55:16.946Z",
          "wordCount": 586,
          "title": "EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2021: Team M3EM Technical Report. (arXiv:2106.10026v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guofa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1\">Dongpu Cao</a>",
          "description": "Multimodal learning mimics the reasoning process of the human multi-sensory\nsystem, which is used to perceive the surrounding world. While making a\nprediction, the human brain tends to relate crucial cues from multiple sources\nof information. In this work, we propose a novel multimodal fusion module that\nlearns to emphasize more contributive features across all modalities.\nSpecifically, the proposed Multimodal Split Attention Fusion (MSAF) module\nsplits each modality into channel-wise equal feature blocks and creates a joint\nrepresentation that is used to generate soft attention for each channel across\nthe feature blocks. Further, the MSAF module is designed to be compatible with\nfeatures of various spatial dimensions and sequence lengths, suitable for both\nCNNs and RNNs. Thus, MSAF can be easily added to fuse features of any unimodal\nnetworks and utilize existing pretrained unimodal model weights. To demonstrate\nthe effectiveness of our fusion module, we design three multimodal networks\nwith MSAF for emotion recognition, sentiment analysis, and action recognition\ntasks. Our approach achieves competitive results in each task and outperforms\nother application-specific networks and multimodal fusion benchmarks.",
          "link": "http://arxiv.org/abs/2012.07175",
          "publishedOn": "2021-06-29T01:55:16.924Z",
          "wordCount": 637,
          "title": "MSAF: Multimodal Split Attention Fusion. (arXiv:2012.07175v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1\">Luis Felipe M.O. Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_E/0/1/0/all/0/1\">Eduardo Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1\">Sergio Colcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1\">Ruy Luiz Milidi&#xfa;</a>",
          "description": "Non-Intrusive Load Monitoring (NILM) is a computational technique to estimate\nthe power loads' appliance-by-appliance from the whole consumption measured by\na single meter. In this paper, we propose a conditional density estimation\nmodel, based on deep neural networks, that joins a Conditional Variational\nAutoencoder with a Conditional Invertible Normalizing Flow model to estimate\nthe individual appliance's power demand. The resulting model is called Prior\nFlow Variational Autoencoder or, for simplicity PFVAE. Thus, instead of having\none model per appliance, the resulting model is responsible for estimating the\npower demand, appliance-by-appliance, at once. We train and evaluate our\nproposed model in a publicly available dataset composed of power demand\nmeasures from a poultry feed factory located in Brazil. The proposed model's\nquality is evaluated by comparing the obtained normalized disaggregation error\n(NDE) and signal aggregated error (SAE) with the previous work values on the\nsame dataset. Our proposal achieves highly competitive results, and for six of\nthe eight machines belonging to the dataset, we observe consistent improvements\nthat go from 28% up to 81% in NDE and from 27% up to 86% in SAE.",
          "link": "http://arxiv.org/abs/2011.14870",
          "publishedOn": "2021-06-29T01:55:16.900Z",
          "wordCount": 663,
          "title": "Prior Flow Variational Autoencoder: A density estimation model for Non-Intrusive Load Monitoring. (arXiv:2011.14870v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Ying Dai</a>",
          "description": "To establish an appropriate model for photo aesthetic assessment, in this\npaper, a D-measure which reflects the disentanglement degree of the final layer\nFC nodes of CNN is introduced. By combining F-measure with D-measure to obtain\na FD measure, an algorithm of determining the optimal model from the multiple\nphoto score prediction models generated by CNN-based repetitively self-revised\nlearning(RSRL) is proposed. Furthermore, the first fixation perspective(FFP)\nand the assessment interest region(AIR) of the models are defined and\ncalculated. The experimental results show that the FD measure is effective for\nestablishing the appropriate model from the multiple score prediction models\nwith different CNN structures. Moreover, the FD-determined optimal models with\nthe comparatively high FD always have the FFP an AIR which are close to the\nhuman's aesthetic perception when enjoying photos.",
          "link": "http://arxiv.org/abs/2106.03316",
          "publishedOn": "2021-06-29T01:55:16.876Z",
          "wordCount": 591,
          "title": "Exploring to establish an appropriate model for image aesthetic assessment via CNN-based RSRL: An empirical study. (arXiv:2106.03316v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02800",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1\">Konstantina Sampani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1\">Mengjia Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1\">Shengze Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1\">Yixiang Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">He Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer K. Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Em Karniadakis</a>",
          "description": "Microaneurysms (MAs) are one of the earliest signs of diabetic retinopathy\n(DR), a frequent complication of diabetes that can lead to visual impairment\nand blindness. Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides\nreal-time retinal images with resolution down to 2 $\\mu m$ and thus allows\ndetection of the morphologies of individual MAs, a potential marker that might\ndictate MA pathology and affect the progression of DR. In contrast to the\nnumerous automatic models developed for assessing the number of MAs on fundus\nphotographs, currently there is no high throughput image protocol available for\nautomatic analysis of AOSLO photographs. To address this urgency, we introduce\nAOSLO-net, a deep neural network framework with customized training policies to\nautomatically segment MAs from AOSLO images. We evaluate the performance of\nAOSLO-net using 87 DR AOSLO images and our results demonstrate that the\nproposed model outperforms the state-of-the-art segmentation model both in\naccuracy and cost and enables correct MA morphological classification.",
          "link": "http://arxiv.org/abs/2106.02800",
          "publishedOn": "2021-06-29T01:55:16.861Z",
          "wordCount": 651,
          "title": "AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Meng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sen Zha</a>",
          "description": "Transformer, which can benefit from global (long-range) information modeling\nusing self-attention mechanisms, has been successful in natural language\nprocessing and 2D image classification recently. However, both local and global\nfeatures are crucial for dense prediction tasks, especially for 3D medical\nimage segmentation. In this paper, we for the first time exploit Transformer in\n3D CNN for MRI Brain Tumor Segmentation and propose a novel network named\nTransBTS based on the encoder-decoder structure. To capture the local 3D\ncontext information, the encoder first utilizes 3D CNN to extract the\nvolumetric spatial feature maps. Meanwhile, the feature maps are reformed\nelaborately for tokens that are fed into Transformer for global feature\nmodeling. The decoder leverages the features embedded by Transformer and\nperforms progressive upsampling to predict the detailed segmentation map.\nExtensive experimental results on both BraTS 2019 and 2020 datasets show that\nTransBTS achieves comparable or higher results than previous state-of-the-art\n3D methods for brain tumor segmentation on 3D MRI scans. The source code is\navailable at https://github.com/Wenxuan-1119/TransBTS",
          "link": "http://arxiv.org/abs/2103.04430",
          "publishedOn": "2021-06-29T01:55:16.749Z",
          "wordCount": 640,
          "title": "TransBTS: Multimodal Brain Tumor Segmentation Using Transformer. (arXiv:2103.04430v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "Machine learning analysis of longitudinal neuroimaging data is typically\nbased on supervised learning, which requires a large number of ground-truth\nlabels to be informative. As ground-truth labels are often missing or expensive\nto obtain in neuroscience, we avoid them in our analysis by combing factor\ndisentanglement with self-supervised learning to identify changes and\nconsistencies across the multiple MRIs acquired of each individual over time.\nSpecifically, we propose a new definition of disentanglement by formulating a\nmultivariate mapping between factors (e.g., brain age) associated with an MRI\nand a latent image representation. Then, factors that evolve across\nacquisitions of longitudinal sequences are disentangled from that mapping by\nself-supervised learning in such a way that changes in a single factor induce\nchange along one direction in the representation space. We implement this\nmodel, named Longitudinal Self-Supervised Learning (LSSL), via a standard\nautoencoding structure with a cosine loss to disentangle brain age from the\nimage representation. We apply LSSL to two longitudinal neuroimaging studies to\nhighlight its strength in extracting the brain-age information from MRI and\nrevealing informative characteristics associated with neurodegenerative and\nneuropsychological disorders. Moreover, the representations learned by LSSL\nfacilitate supervised classification by recording faster convergence and higher\n(or similar) prediction accuracy compared to several other representation\nlearning techniques.",
          "link": "http://arxiv.org/abs/2006.06930",
          "publishedOn": "2021-06-29T01:55:16.698Z",
          "wordCount": 670,
          "title": "Longitudinal Self-Supervised Learning. (arXiv:2006.06930v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1\">Ivan Skorokhodov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ignatyev_S/0/1/0/all/0/1\">Savva Ignatyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>",
          "description": "In most existing learning systems, images are typically viewed as 2D pixel\narrays. However, in another paradigm gaining popularity, a 2D image is\nrepresented as an implicit neural representation (INR) - an MLP that predicts\nan RGB pixel value given its (x,y) coordinate. In this paper, we propose two\nnovel architectural techniques for building INR-based image decoders:\nfactorized multiplicative modulation and multi-scale INRs, and use them to\nbuild a state-of-the-art continuous image GAN. Previous attempts to adapt INRs\nfor image generation were limited to MNIST-like datasets and do not scale to\ncomplex real-world data. Our proposed INR-GAN architecture improves the\nperformance of continuous image generators by several times, greatly reducing\nthe gap between continuous image GANs and pixel-based ones. Apart from that, we\nexplore several exciting properties of the INR-based decoders, like\nout-of-the-box superresolution, meaningful image-space interpolation,\naccelerated inference of low-resolution images, an ability to extrapolate\noutside of image boundaries, and strong geometric prior. The project page is\nlocated at https://universome.github.io/inr-gan.",
          "link": "http://arxiv.org/abs/2011.12026",
          "publishedOn": "2021-06-29T01:55:16.644Z",
          "wordCount": 629,
          "title": "Adversarial Generation of Continuous Images. (arXiv:2011.12026v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Thao Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>",
          "description": "There are many real-life use cases such as barcode scanning or billboard\nreading where people need to detect objects and read the object contents.\nCommonly existing methods are first trying to localize object regions, then\ndetermine layout and lastly classify content units. However, for simple fixed\nstructured objects like license plates, this approach becomes overkill and\nlengthy to run. This work aims to solve this detect-and-read problem in a\nlightweight way by integrating multi-digit recognition into a one-stage object\ndetection model. Our unified method not only eliminates the duplication in\nfeature extraction (one for localizing, one again for classifying) but also\nprovides useful contextual information around object regions for\nclassification. Additionally, our choice of backbones and modifications in\narchitecture, loss function, data augmentation and training make the method\nrobust, efficient and speedy. Secondly, we made a public benchmark dataset of\ndiverse real-life 1D barcodes for a reliable evaluation, which we collected,\nannotated and checked carefully. Eventually, experimental results prove the\nmethod's efficiency on the barcode problem by outperforming industrial tools in\nboth detecting and decoding rates with a real-time fps at a VGA-similar\nresolution. It also did a great job expectedly on the license-plate recognition\ntask (on the AOLP dataset) by outperforming the current state-of-the-art method\nsignificantly in terms of recognition rate and inference time.",
          "link": "http://arxiv.org/abs/2102.07354",
          "publishedOn": "2021-06-29T01:55:16.303Z",
          "wordCount": 698,
          "title": "QuickBrowser: A Unified Model to Detect and Read Simple Object in Real-time. (arXiv:2102.07354v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macke_J/0/1/0/all/0/1\">J. Macke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedlar_J/0/1/0/all/0/1\">J. Sedlar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1\">M. Olsak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1\">J. Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">J. Sivic</a>",
          "description": "We describe a purely image-based method for finding geometric constructions\nwith a ruler and compass in the Euclidea geometric game. The method is based on\nadapting the Mask R-CNN state-of-the-art image processing neural architecture\nand adding a tree-based search procedure to it. In a supervised setting, the\nmethod learns to solve all 68 kinds of geometric construction problems from the\nfirst six level packs of Euclidea with an average 92% accuracy. When evaluated\non new kinds of problems, the method can solve 31 of the 68 kinds of Euclidea\nproblems. We believe that this is the first time that a purely image-based\nlearning has been trained to solve geometric construction problems of this\ndifficulty.",
          "link": "http://arxiv.org/abs/2106.14195",
          "publishedOn": "2021-06-29T01:55:16.269Z",
          "wordCount": 576,
          "title": "Learning to solve geometric construction problems from images. (arXiv:2106.14195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.02944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "Object recognition in the real-world requires handling long-tailed or even\nopen-ended data. An ideal visual system needs to recognize the populated head\nvisual concepts reliably and meanwhile efficiently learn about emerging new\ntail categories with a few training instances. Class-balanced many-shot\nlearning and few-shot learning tackle one side of this problem, by either\nlearning strong classifiers for head or learning to learn few-shot classifiers\nfor the tail. In this paper, we investigate the problem of generalized few-shot\nlearning (GFSL) -- a model during the deployment is required to learn about\ntail categories with few shots and simultaneously classify the head classes. We\npropose the ClAssifier SynThesis LEarning (CASTLE), a learning framework that\nlearns how to synthesize calibrated few-shot classifiers in addition to the\nmulti-class classifiers of head classes with a shared neural dictionary,\nshedding light upon the inductive GFSL. Furthermore, we propose an adaptive\nversion of CASTLE (ACASTLE) that adapts the head classifiers conditioned on the\nincoming tail training examples, yielding a framework that allows effective\nbackward knowledge transfer. As a consequence, ACASTLE can handle GFSL with\nclasses from heterogeneous domains effectively. CASTLE and ACASTLE demonstrate\nsuperior performances than existing GFSL algorithms and strong baselines on\nMiniImageNet as well as TieredImageNet datasets. More interestingly, they\noutperform previous state-of-the-art methods when evaluated with standard\nfew-shot learning criteria.",
          "link": "http://arxiv.org/abs/1906.02944",
          "publishedOn": "2021-06-29T01:55:16.216Z",
          "wordCount": 722,
          "title": "Learning Adaptive Classifiers Synthesis for Generalized Few-Shot Learning. (arXiv:1906.02944v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiawei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Songhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhiwei Liang</a>",
          "description": "Facial Expression Recognition (FER) is a classification task that points to\nface variants. Hence, there are certain affinity features between facial\nexpressions, receiving little attention in the FER literature. Convolution\npadding, despite helping capture the edge information, causes erosion of the\nfeature map simultaneously. After multi-layer filling convolution, the output\nfeature map named albino feature definitely weakens the representation of the\nexpression. To tackle these challenges, we propose a novel architecture named\nAmending Representation Module (ARM). ARM is a substitute for the pooling\nlayer. Theoretically, it can be embedded in the back end of any network to deal\nwith the Padding Erosion. ARM efficiently enhances facial expression\nrepresentation from two different directions: 1) reducing the weight of eroded\nfeatures to offset the side effect of padding, and 2) sharing affinity features\nover mini-batch to strengthen the representation learning. Experiments on\npublic benchmarks prove that our ARM boosts the performance of FER remarkably.\nThe validation accuracies are respectively 92.05% on RAF-DB, 65.2% on\nAffect-Net, and 58.71% on SFEW, exceeding current state-of-the-art methods. Our\nimplementation and trained models are available at\nhttps://github.com/JiaweiShiCV/Amend-Representation-Module.",
          "link": "http://arxiv.org/abs/2103.10189",
          "publishedOn": "2021-06-29T01:55:16.181Z",
          "wordCount": 647,
          "title": "Learning to Amend Facial Expression Representation via De-albino and Affinity. (arXiv:2103.10189v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1\">Chi-Wei Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning-Hsu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>",
          "description": "Indoor panorama typically consists of human-made structures parallel or\nperpendicular to gravity. We leverage this phenomenon to approximate the scene\nin a 360-degree image with (H)orizontal-planes and (V)ertical-planes. To this\nend, we propose an effective divide-and-conquer strategy that divides pixels\nbased on their plane orientation estimation; then, the succeeding instance\nsegmentation module conquers the task of planes clustering more easily in each\nplane orientation group. Besides, parameters of V-planes depend on camera yaw\nrotation, but translation-invariant CNNs are less aware of the yaw change. We\nthus propose a yaw-invariant V-planar reparameterization for CNNs to learn. We\ncreate a benchmark for indoor panorama planar reconstruction by extending\nexisting 360 depth datasets with ground truth H\\&V-planes (referred to as\nPanoH&V dataset) and adopt state-of-the-art planar reconstruction methods to\npredict H\\&V-planes as our baselines. Our method outperforms the baselines by a\nlarge margin on the proposed dataset.",
          "link": "http://arxiv.org/abs/2106.14166",
          "publishedOn": "2021-06-29T01:55:16.149Z",
          "wordCount": 584,
          "title": "Indoor Panorama Planar 3D Reconstruction via Divide and Conquer. (arXiv:2106.14166v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1810.01256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guanxiong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>",
          "description": "Deep neural networks (DNNs) are powerful tools in learning sophisticated but\nfixed mapping rules between inputs and outputs, thereby limiting their\napplication in more complex and dynamic situations in which the mapping rules\nare not kept the same but changing according to different contexts. To lift\nsuch limits, we developed a novel approach involving a learning algorithm,\ncalled orthogonal weights modification (OWM), with the addition of a\ncontext-dependent processing (CDP) module. We demonstrated that with OWM to\novercome the problem of catastrophic forgetting, and the CDP module to learn\nhow to reuse a feature representation and a classifier for different contexts,\na single network can acquire numerous context-dependent mapping rules in an\nonline and continual manner, with as few as $\\sim$10 samples to learn each.\nThis should enable highly compact systems to gradually learn myriad\nregularities of the real world and eventually behave appropriately within it.",
          "link": "http://arxiv.org/abs/1810.01256",
          "publishedOn": "2021-06-29T01:55:16.106Z",
          "wordCount": 634,
          "title": "Continual Learning of Context-dependent Processing in Neural Networks. (arXiv:1810.01256v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1\">Sai Rajeswar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_C/0/1/0/all/0/1\">Cyril Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1\">Nitin Surya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1\">Florian Golemo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinheiro_P/0/1/0/all/0/1\">Pedro O. Pinheiro</a>",
          "description": "Robots in many real-world settings have access to force/torque sensors in\ntheir gripper and tactile sensing is often necessary in tasks that involve\ncontact-rich motion. In this work, we leverage surprise from mismatches in\ntouch feedback to guide exploration in hard sparse-reward reinforcement\nlearning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible\nobjects interactions are supposed to \"feel\" like. We encourage exploration by\nrewarding interactions where the expectation and the experience don't match. In\nour proposed method, an initial task-independent exploration phase is followed\nby an on-task learning phase, in which the original interactions are relabeled\nwith on-task rewards. We test our approach on a range of touch-intensive robot\narm tasks (e.g. pushing objects, opening doors), which we also release as part\nof this work. Across multiple experiments in a simulated setting, we\ndemonstrate that our method is able to learn these difficult tasks through\nsparse reward and curiosity alone. We compare our cross-modal approach to\nsingle-modality (touch- or vision-only) approaches as well as other\ncuriosity-based methods and find that our method performs better and is more\nsample-efficient.",
          "link": "http://arxiv.org/abs/2104.00442",
          "publishedOn": "2021-06-29T01:55:16.057Z",
          "wordCount": 655,
          "title": "Touch-based Curiosity for Sparse-Reward Tasks. (arXiv:2104.00442v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gominski_D/0/1/0/all/0/1\">Dimitri Gominski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouet_Brunet_V/0/1/0/all/0/1\">Val&#xe9;rie Gouet-Brunet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>",
          "description": "Advances in high resolution remote sensing image analysis are currently\nhampered by the difficulty of gathering enough annotated data for training deep\nlearning methods, giving rise to a variety of small datasets and associated\ndataset-specific methods. Moreover, typical tasks such as classification and\nretrieval lack a systematic evaluation on standard benchmarks and training\ndatasets, which make it hard to identify durable and generalizable scientific\ncontributions. We aim at unifying remote sensing image retrieval and\nclassification with a new large-scale training and testing dataset, SF300,\nincluding both vertical and oblique aerial images and made available to the\nresearch community, and an associated fine-tuning method. We additionally\npropose a new adversarial fine-tuning method for global descriptors. We show\nthat our framework systematically achieves a boost of retrieval and\nclassification performance on nine different datasets compared to an ImageNet\npretrained baseline, with currently no other method to compare to.",
          "link": "http://arxiv.org/abs/2102.13392",
          "publishedOn": "2021-06-29T01:55:16.045Z",
          "wordCount": 646,
          "title": "Unifying Remote Sensing Image Retrieval and Classification with Robust Fine-tuning. (arXiv:2102.13392v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Law_H/0/1/0/all/0/1\">Ho Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_G/0/1/0/all/0/1\">Gary P. T. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Ka Chun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1\">Lok Ming Lui</a>",
          "description": "Image registration has been widely studied over the past several decades,\nwith numerous applications in science, engineering and medicine. Most of the\nconventional mathematical models for large deformation image registration rely\non prescribed landmarks, which usually require tedious manual labeling and are\nprone to error. In recent years, there has been a surge of interest in the use\nof machine learning for image registration. In this paper, we develop a novel\nmethod for large deformation image registration by a fusion of quasiconformal\ntheory and convolutional neural network (CNN). More specifically, we propose a\nquasiconformal energy model with a novel fidelity term that incorporates the\nfeatures extracted using a pre-trained CNN, thereby allowing us to obtain\nmeaningful registration results without any guidance of prescribed landmarks.\nMoreover, unlike many prior image registration methods, the bijectivity of our\nmethod is guaranteed by quasiconformal theory. Experimental results are\npresented to demonstrate the effectiveness of the proposed method. More\nbroadly, our work sheds light on how rigorous mathematical theories and\npractical machine learning approaches can be integrated for developing\ncomputational methods with improved performance.",
          "link": "http://arxiv.org/abs/2011.00731",
          "publishedOn": "2021-06-29T01:55:16.038Z",
          "wordCount": 681,
          "title": "Quasiconformal model with CNN features for large deformation image registration. (arXiv:2011.00731v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferianc_M/0/1/0/all/0/1\">Martin Ferianc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Divyansh Manocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hongxiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1\">Miguel Rodrigues</a>",
          "description": "Fully convolutional U-shaped neural networks have largely been the dominant\napproach for pixel-wise image segmentation. In this work, we tackle two defects\nthat hinder their deployment in real-world applications: 1) Predictions lack\nuncertainty quantification that may be crucial to many decision-making systems;\n2) Large memory storage and computational consumption demanding extensive\nhardware resources. To address these issues and improve their practicality we\ndemonstrate a few-parameter compact Bayesian convolutional architecture, that\nachieves a marginal improvement in accuracy in comparison to related work using\nsignificantly fewer parameters and compute operations. The architecture\ncombines parameter-efficient operations such as separable convolutions,\nbilinear interpolation, multi-scale feature propagation and Bayesian inference\nfor per-pixel uncertainty quantification through Monte Carlo Dropout. The best\nperforming configurations required fewer than 2.5 million parameters on diverse\nchallenging datasets with few observations.",
          "link": "http://arxiv.org/abs/2104.06957",
          "publishedOn": "2021-06-29T01:55:16.016Z",
          "wordCount": 610,
          "title": "ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation. (arXiv:2104.06957v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14403",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingfeng Liu</a>",
          "description": "We present an automatic COVID1-19 diagnosis framework from lung CT-scan slice\nimages. In this framework, the slice images of a CT-scan volume are first\nproprocessed using segmentation techniques to filter out images of closed lung,\nand to remove the useless background. Then a resampling method is used to\nselect one or multiple sets of a fixed number of slice images for training and\nvalidation. A 3D CNN network with BERT is used to classify this set of selected\nslice images. In this network, an embedding feature is also extracted. In cases\nwhere there are more than one set of slice images in a volume, the features of\nall sets are extracted and pooled into a global feature vector for the whole\nCT-scan volume. A simple multiple-layer perceptron (MLP) network is used to\nfurther classify the aggregated feature vector. The models are trained and\nevaluated on the provided training and validation datasets. On the validation\ndataset, the accuracy is 0.9278 and the F1 score is 0.9261.",
          "link": "http://arxiv.org/abs/2106.14403",
          "publishedOn": "2021-06-29T01:55:16.009Z",
          "wordCount": 660,
          "title": "A 3D CNN Network with BERT For Automatic COVID-19 Diagnosis From CT-Scan Images. (arXiv:2106.14403v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terhorst_P/0/1/0/all/0/1\">Philipp Terh&#xf6;rst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahrmann_D/0/1/0/all/0/1\">Daniel F&#xe4;hrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolf_J/0/1/0/all/0/1\">Jan Niklas Kolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>",
          "description": "Soft-biometrics play an important role in face biometrics and related fields\nsince these might lead to biased performances, threatens the user's privacy, or\nare valuable for commercial aspects. Current face databases are specifically\nconstructed for the development of face recognition applications. Consequently,\nthese databases contain large amount of face images but lack in the number of\nattribute annotations and the overall annotation correctness. In this work, we\npropose MAADFace, a new face annotations database that is characterized by the\nlarge number of its high-quality attribute annotations. MAADFace is build on\nthe VGGFace2 database and thus, consists of 3.3M faces of over 9k individuals.\nUsing a novel annotation transfer-pipeline that allows an accurate\nlabel-transfer from multiple source-datasets to a target-dataset, MAAD-Face\nconsists of 123.9M attribute annotations of 47 different binary attributes.\nConsequently, it provides 15 and 137 times more attribute labels than CelebA\nand LFW. Our investigation on the annotation quality by three human evaluators\ndemonstrated the superiority of the MAAD-Face annotations over existing\ndatabases. Additionally, we make use of the large amount of high-quality\nannotations from MAAD-Face to study the viability of soft-biometrics for\nrecognition, providing insights about which attributes support genuine and\nimposter decisions. The MAAD-Face annotations dataset is publicly available.",
          "link": "http://arxiv.org/abs/2012.01030",
          "publishedOn": "2021-06-29T01:55:15.962Z",
          "wordCount": 680,
          "title": "MAAD-Face: A Massively Annotated Attribute Dataset for Face Images. (arXiv:2012.01030v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhuotao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "Semantic segmentation has made tremendous progress in recent years. However,\nsatisfying performance highly depends on a large number of pixel-level\nannotations. Therefore, in this paper, we focus on the semi-supervised\nsegmentation problem where only a small set of labeled data is provided with a\nmuch larger collection of totally unlabeled images. Nevertheless, due to the\nlimited annotations, models may overly rely on the contexts available in the\ntraining data, which causes poor generalization to the scenes unseen before. A\npreferred high-level representation should capture the contextual information\nwhile not losing self-awareness. Therefore, we propose to maintain the\ncontext-aware consistency between features of the same identity but with\ndifferent contexts, making the representations robust to the varying\nenvironments. Moreover, we present the Directional Contrastive Loss (DC Loss)\nto accomplish the consistency in a pixel-to-pixel manner, only requiring the\nfeature with lower quality to be aligned towards its counterpart. In addition,\nto avoid the false-negative samples and filter the uncertain positive samples,\nwe put forward two sampling strategies. Extensive experiments show that our\nsimple yet effective method surpasses current state-of-the-art methods by a\nlarge margin and also generalizes well with extra image-level annotations.",
          "link": "http://arxiv.org/abs/2106.14133",
          "publishedOn": "2021-06-29T01:55:15.955Z",
          "wordCount": 643,
          "title": "Semi-supervised Semantic Segmentation with Directional Context-aware Consistency. (arXiv:2106.14133v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.12931",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_T/0/1/0/all/0/1\">Tashin Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sabab_N/0/1/0/all/0/1\">Noor Hossain Nuri Sabab</a>",
          "description": "Climate change has been a common interest and the forefront of crucial\npolitical discussion and decision-making for many years. Shallow clouds play a\nsignificant role in understanding the Earth's climate, but they are challenging\nto interpret and represent in a climate model. By classifying these cloud\nstructures, there is a better possibility of understanding the physical\nstructures of the clouds, which would improve the climate model generation,\nresulting in a better prediction of climate change or forecasting weather\nupdate. Clouds organise in many forms, which makes it challenging to build\ntraditional rule-based algorithms to separate cloud features. In this paper,\nclassification of cloud organization patterns was performed using a new\nscaled-up version of Convolutional Neural Network (CNN) named as EfficientNet\nas the encoder and UNet as decoder where they worked as feature extractor and\nreconstructor of fine grained feature map and was used as a classifier, which\nwill help experts to understand how clouds will shape the future climate. By\nusing a segmentation model in a classification task, it was shown that with a\ngood encoder alongside UNet, it is possible to obtain good performance from\nthis dataset. Dice coefficient has been used for the final evaluation metric,\nwhich gave the score of 66.26\\% and 66.02\\% for public and private (test set)\nleaderboard on Kaggle competition respectively.",
          "link": "http://arxiv.org/abs/2009.12931",
          "publishedOn": "2021-06-29T01:55:15.798Z",
          "wordCount": 699,
          "title": "Classification and understanding of cloud structures via satellite images with EfficientUNet. (arXiv:2009.12931v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14248",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1\">Chun-Mei Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yunlu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Accelerating multi-modal magnetic resonance (MR) imaging is a new and\neffective solution for fast MR imaging, providing superior performance in\nrestoring the target modality from its undersampled counterpart with guidance\nfrom an auxiliary modality. However, existing works simply introduce the\nauxiliary modality as prior information, lacking in-depth investigations on the\npotential mechanisms for fusing two modalities. Further, they usually rely on\nthe convolutional neural networks (CNNs), which focus on local information and\nprevent them from fully capturing the long-distance dependencies of global\nknowledge. To this end, we propose a multi-modal transformer (MTrans), which is\ncapable of transferring multi-scale features from the target modality to the\nauxiliary modality, for accelerated MR imaging. By restructuring the\ntransformer architecture, our MTrans gains a powerful ability to capture deep\nmulti-modal information. More specifically, the target modality and the\nauxiliary modality are first split into two branches and then fused using a\nmulti-modal transformer module. This module is based on an improved multi-head\nattention mechanism, named the cross attention module, which absorbs features\nfrom the auxiliary modality that contribute to the target modality. Our\nframework provides two appealing benefits: (i) MTrans is the first attempt at\nusing improved transformers for multi-modal MR imaging, affording more global\ninformation compared with CNN-based methods. (ii) A new cross attention module\nis proposed to exploit the useful information in each branch at different\nscales. It affords both distinct structural information and subtle pixel-level\ninformation, which supplement the target modality effectively.",
          "link": "http://arxiv.org/abs/2106.14248",
          "publishedOn": "2021-06-29T01:55:15.773Z",
          "wordCount": 687,
          "title": "MTrans: Multi-Modal Transformer for Accelerated MR Imaging. (arXiv:2106.14248v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hyungsik Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1\">Youngrock Oh</a>",
          "description": "Increasing demands for understanding the internal behavior of convolutional\nneural networks (CNNs) have led to remarkable improvements in explanation\nmethods. Particularly, several class activation mapping (CAM) based methods,\nwhich generate visual explanation maps by a linear combination of activation\nmaps from CNNs, have been proposed. However, the majority of the methods lack a\nclear theoretical basis on how they assign the coefficients of the linear\ncombination. In this paper, we revisit the intrinsic linearity of CAM with\nrespect to the activation maps; we construct an explanation model of CNN as a\nlinear function of binary variables that denote the existence of the\ncorresponding activation maps. With this approach, the explanation model can be\ndetermined by additive feature attribution methods in an analytic manner. We\nthen demonstrate the adequacy of SHAP values, which is a unique solution for\nthe explanation model with a set of desirable properties, as the coefficients\nof CAM. Since the exact SHAP values are unattainable, we introduce an efficient\napproximation method, LIFT-CAM, based on DeepLIFT. Our proposed LIFT-CAM can\nestimate the SHAP values of the activation maps with high speed and accuracy.\nFurthermore, it greatly outperforms other previous CAM-based methods in both\nqualitative and quantitative aspects.",
          "link": "http://arxiv.org/abs/2102.05228",
          "publishedOn": "2021-06-29T01:55:15.744Z",
          "wordCount": 653,
          "title": "Towards Better Explanations of Class Activation Mapping. (arXiv:2102.05228v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xu Huang</a>",
          "description": "Image-based 3D object modeling refers to the process of converting raw\noptical images to 3D digital representations of the objects. Very often, such\nmodels are desired to be dimensionally true, semantically labeled with\nphotorealistic appearance (reality-based modeling). Laser scanning was deemed\nas the standard (and direct) way to obtaining highly accurate 3D measurements\nof objects, while one would have to abide the high acquisition cost and its\nunavailability on some of the platforms. Nowadays the image-based methods\nbackboned by the recently developed advanced dense image matching algorithms\nand geo-referencing paradigms, are becoming the dominant approaches, due to its\nhigh flexibility, availability and low cost. The largely automated geometric\nprocessing of images in a 3D object reconstruction workflow, from\nordered/unordered raw imagery to textured meshes, is becoming a critical part\nof the reality-based 3D modeling. This article summarizes the overall geometric\nprocessing workflow, with focuses on introducing the state-of-the-art methods\nof three major components of geometric processing: 1) geo-referencing; 2) Image\ndense matching 3) texture mapping. Finally, we will draw conclusions and share\nour outlooks of the topics discussed in this article.",
          "link": "http://arxiv.org/abs/2106.14307",
          "publishedOn": "2021-06-29T01:55:15.738Z",
          "wordCount": 610,
          "title": "Geometric Processing for Image-based 3D Object Modeling. (arXiv:2106.14307v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maag_K/0/1/0/all/0/1\">Kira Maag</a>",
          "description": "Instance segmentation of images is an important tool for automated scene\nunderstanding. Neural networks are usually trained to optimize their overall\nperformance in terms of accuracy. Meanwhile, in applications such as automated\ndriving, an overlooked pedestrian seems more harmful than a falsely detected\none. In this work, we present a false negative detection method for image\nsequences based on inconsistencies in time series of tracked instances given\nthe availability of image sequences in online applications. As the number of\ninstances can be greatly increased by this algorithm, we apply a false positive\npruning using uncertainty estimates aggregated over instances. To this end,\ninstance-wise metrics are constructed which characterize uncertainty and\ngeometry of a given instance or are predicated on depth estimation. The\nproposed method serves as a post-processing step applicable to any neural\nnetwork that can also be trained on single frames only. In our tests, we obtain\nan improved trade-off between false negative and false positive instances by\nour fused detection approach in comparison to the use of an ordinary score\nvalue provided by the instance segmentation network during inference.",
          "link": "http://arxiv.org/abs/2106.14474",
          "publishedOn": "2021-06-29T01:55:15.732Z",
          "wordCount": 615,
          "title": "False Negative Reduction in Video Instance Segmentation using Uncertainty Estimates. (arXiv:2106.14474v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1907.12727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1\">Adolf Pfefferbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1\">Edith V. Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "With recent advances in deep learning, neuroimaging studies increasingly rely\non convolutional networks (ConvNets) to predict diagnosis based on MR images.\nTo gain a better understanding of how a disease impacts the brain, the studies\nvisualize the salience maps of the ConvNet highlighting voxels within the brain\nmajorly contributing to the prediction. However, these salience maps are\ngenerally confounded, i.e., some salient regions are more predictive of\nconfounding variables (such as age) than the diagnosis. To avoid such\nmisinterpretation, we propose in this paper an approach that aims to visualize\nconfounder-free saliency maps that only highlight voxels predictive of the\ndiagnosis. The approach incorporates univariate statistical tests to identify\nconfounding effects within the intermediate features learned by ConvNet. The\ninfluence from the subset of confounded features is then removed by a novel\npartial back-propagation procedure. We use this two-step approach to visualize\nconfounder-free saliency maps extracted from synthetic and two real datasets.\nThese experiments reveal the potential of our visualization in producing\nunbiased model-interpretation.",
          "link": "http://arxiv.org/abs/1907.12727",
          "publishedOn": "2021-06-29T01:55:15.697Z",
          "wordCount": 638,
          "title": "Confounder-Aware Visualization of ConvNets. (arXiv:1907.12727v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.06297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Thao Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolcha_Y/0/1/0/all/0/1\">Yalew Tolcha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_T/0/1/0/all/0/1\">Tae Joon Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>",
          "description": "Barcodes are ubiquitous and have been used in most of critical daily\nactivities for decades. However, most of traditional decoders require\nwell-founded barcode under a relatively standard condition. While wilder\nconditioned barcodes such as underexposed, occluded, blurry, wrinkled and\nrotated are commonly captured in reality, those traditional decoders show\nweakness of recognizing. Several works attempted to solve those challenging\nbarcodes, but many limitations still exist. This work aims to solve the\ndecoding problem using deep convolutional neural network with the possibility\nof running on portable devices. Firstly, we proposed a special modification of\ninference based on the feature of having checksum and test-time augmentation,\nnamed as Smart Inference (SI) in prediction phase of a trained model. SI\nconsiderably boosts accuracy and reduces the false prediction for trained\nmodels. Secondly, we have created a large practical evaluation dataset of real\ncaptured 1D barcode under various challenging conditions to test our methods\nvigorously, which is publicly available for other researchers. The experiments'\nresults demonstrated the SI effectiveness with the highest accuracy of 95.85%\nwhich outperformed many existing decoders on the evaluation set. Finally, we\nsuccessfully minimized the best model by knowledge distillation to a shallow\nmodel which is shown to have high accuracy (90.85%) with good inference speed\nof 34.2 ms per image on a real edge device.",
          "link": "http://arxiv.org/abs/2004.06297",
          "publishedOn": "2021-06-29T01:55:15.479Z",
          "wordCount": 706,
          "title": "Smart Inference for Multidigit Convolutional Neural Network based Barcode Decoding. (arXiv:2004.06297v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_D/0/1/0/all/0/1\">Donggyu Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>",
          "description": "Advances in technology have led to the development of methods that can create\ndesired visual multimedia. In particular, image generation using deep learning\nhas been extensively studied across diverse fields. In comparison, video\ngeneration, especially on conditional inputs, remains a challenging and less\nexplored area. To narrow this gap, we aim to train our model to produce a video\ncorresponding to a given text description. We propose a novel training\nframework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN),\nwhich evolves frame-by-frame and finally produces a full-length video. In the\nfirst phase, we focus on creating a high-quality single video frame while\nlearning the relationship between the text and an image. As the steps proceed,\nour model is trained gradually on more number of consecutive frames.This\nstep-by-step learning process helps stabilize the training and enables the\ncreation of high-resolution video based on conditional text descriptions.\nQualitative and quantitative experimental results on various datasets\ndemonstrate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2009.02018",
          "publishedOn": "2021-06-29T01:55:15.417Z",
          "wordCount": 628,
          "title": "TiVGAN: Text to Image to Video Generation with Step-by-Step Evolutionary Generator. (arXiv:2009.02018v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.05846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huatian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiannan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Cheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jigen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Shigang Yue</a>",
          "description": "Discriminating small moving objects within complex visual environments is a\nsignificant challenge for autonomous micro robots that are generally limited in\ncomputational power. By exploiting their highly evolved visual systems, flying\ninsects can effectively detect mates and track prey during rapid pursuits, even\nthough the small targets equate to only a few pixels in their visual field. The\nhigh degree of sensitivity to small target movement is supported by a class of\nspecialized neurons called small target motion detectors (STMDs). Existing\nSTMD-based computational models normally comprise four sequentially arranged\nneural layers interconnected via feedforward loops to extract information on\nsmall target motion from raw visual inputs. However, feedback, another\nimportant regulatory circuit for motion perception, has not been investigated\nin the STMD pathway and its functional roles for small target motion detection\nare not clear. In this paper, we propose an STMD-based neural network with\nfeedback connection (Feedback STMD), where the network output is temporally\ndelayed, then fed back to the lower layers to mediate neural responses. We\ncompare the properties of the model with and without the time-delay feedback\nloop, and find it shows preference for high-velocity objects. Extensive\nexperiments suggest that the Feedback STMD achieves superior detection\nperformance for fast-moving small targets, while significantly suppressing\nbackground false positive movements which display lower velocities. The\nproposed feedback model provides an effective solution in robotic visual\nsystems for detecting fast-moving small targets that are always salient and\npotentially threatening.",
          "link": "http://arxiv.org/abs/2001.05846",
          "publishedOn": "2021-06-29T01:55:15.002Z",
          "wordCount": 753,
          "title": "A Time-Delay Feedback Neural Network for Discriminating Small, Fast-Moving Targets in Complex Dynamic Environments. (arXiv:2001.05846v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Monocular depth prediction plays a crucial role in understanding 3D scene\ngeometry. Although recent methods have achieved impressive progress in terms of\nevaluation metrics such as the pixel-wise relative error, most methods neglect\nthe geometric constraints in the 3D space. In this work, we show the importance\nof the high-order 3D geometric constraints for depth prediction. By designing a\nloss term that enforces a simple geometric constraint, namely, virtual normal\ndirections determined by randomly sampled three points in the reconstructed 3D\nspace, we significantly improve the accuracy and robustness of monocular depth\nestimation. Significantly, the virtual normal loss can not only improve the\nperformance of learning metric depth, but also disentangle the scale\ninformation and enrich the model with better shape information. Therefore, when\nnot having access to absolute metric depth training data, we can use virtual\nnormal to learn a robust affine-invariant depth generated on diverse scenes. In\nexperiments, We show state-of-the-art results of learning metric depth on NYU\nDepth-V2 and KITTI. From the high-quality predicted depth, we are now able to\nrecover good 3D structures of the scene such as the point cloud and surface\nnormal directly, eliminating the necessity of relying on additional models as\nwas previously done. To demonstrate the excellent generalizability of learning\naffine-invariant depth on diverse data with the virtual normal loss, we\nconstruct a large-scale and diverse dataset for training affine-invariant\ndepth, termed Diverse Scene Depth dataset (DiverseDepth), and test on five\ndatasets with the zero-shot test setting. Code is available at:\nhttps://git.io/Depth",
          "link": "http://arxiv.org/abs/2103.04216",
          "publishedOn": "2021-06-29T01:55:14.996Z",
          "wordCount": 774,
          "title": "Virtual Normal: Enforcing Geometric Constraints for Accurate and Robust Depth Prediction. (arXiv:2103.04216v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nobis_F/0/1/0/all/0/1\">Felix Nobis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiei_E/0/1/0/all/0/1\">Ehsan Shafiei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karle_P/0/1/0/all/0/1\">Phillip Karle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betz_J/0/1/0/all/0/1\">Johannes Betz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lienkamp_M/0/1/0/all/0/1\">Markus Lienkamp</a>",
          "description": "Automotive traffic scenes are complex due to the variety of possible\nscenarios, objects, and weather conditions that need to be handled. In contrast\nto more constrained environments, such as automated underground trains,\nautomotive perception systems cannot be tailored to a narrow field of specific\ntasks but must handle an ever-changing environment with unforeseen events. As\ncurrently no single sensor is able to reliably perceive all relevant activity\nin the surroundings, sensor data fusion is applied to perceive as much\ninformation as possible. Data fusion of different sensors and sensor modalities\non a low abstraction level enables the compensation of sensor weaknesses and\nmisdetections among the sensors before the information-rich sensor data are\ncompressed and thereby information is lost after a sensor-individual object\ndetection. This paper develops a low-level sensor fusion network for 3D object\ndetection, which fuses lidar, camera, and radar data. The fusion network is\ntrained and evaluated on the nuScenes data set. On the test set, fusion of\nradar data increases the resulting AP (Average Precision) detection score by\nabout 5.1% in comparison to the baseline lidar network. The radar sensor fusion\nproves especially beneficial in inclement conditions such as rain and night\nscenes. Fusing additional camera data contributes positively only in\nconjunction with the radar fusion, which shows that interdependencies of the\nsensors are important for the detection result. Additionally, the paper\nproposes a novel loss to handle the discontinuity of a simple yaw\nrepresentation for object detection. Our updated loss increases the detection\nand orientation estimation performance for all sensor input configurations. The\ncode for this research has been made available on GitHub.",
          "link": "http://arxiv.org/abs/2106.14087",
          "publishedOn": "2021-06-29T01:55:14.991Z",
          "wordCount": 705,
          "title": "Radar Voxel Fusion for 3D Object Detection. (arXiv:2106.14087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>",
          "description": "In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have primarily focused on processing text for\nclassification and small-scale databases. This paper describes a novel\nmultimodal deep learning architecture, called TechDoc, for technical document\nclassification, which utilizes both natural language and descriptive images to\ntrain hierarchical classifiers. The architecture synthesizes convolutional\nneural networks and recurrent neural networks through an integrated training\nprocess. We applied the architecture to a large multimodal technical document\ndatabase and trained the model for classifying documents based on the\nhierarchical International Patent Classification system. Our results show that\nthe trained neural network presents a greater classification accuracy than\nthose using a single modality and several earlier text classification methods.\nThe trained model can potentially be scaled to millions of real-world technical\ndocuments with both text and figures, which is useful for data and knowledge\nmanagement in large technology companies and organizations.",
          "link": "http://arxiv.org/abs/2106.14269",
          "publishedOn": "2021-06-29T01:55:14.984Z",
          "wordCount": 625,
          "title": "Deep Learning for Technical Document Classification. (arXiv:2106.14269v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yuanfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingyun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "The recent vision transformer(i.e.for image classification) learns non-local\nattentive interaction of different patch tokens. However, prior arts miss\nlearning the cross-scale dependencies of different pixels, the semantic\ncorrespondence of different labels, and the consistency of the feature\nrepresentations and semantic embeddings, which are critical for biomedical\nsegmentation. In this paper, we tackle the above issues by proposing a unified\ntransformer network, termed Multi-Compound Transformer (MCTrans), which\nincorporates rich feature learning and semantic structure mining into a unified\nframework. Specifically, MCTrans embeds the multi-scale convolutional features\nas a sequence of tokens and performs intra- and inter-scale self-attention,\nrather than single-scale attention in previous works. In addition, a learnable\nproxy embedding is also introduced to model semantic relationship and feature\nenhancement by using self-attention and cross-attention, respectively. MCTrans\ncan be easily plugged into a UNet-like network and attains a significant\nimprovement over the state-of-the-art methods in biomedical image segmentation\nin six standard benchmarks. For example, MCTrans outperforms UNet by 3.64%,\n3.71%, 4.34%, 2.8%, 1.88%, 1.57% in Pannuke, CVC-Clinic, CVC-Colon, Etis,\nKavirs, ISIC2018 dataset, respectively. Code is available at\nhttps://github.com/JiYuanFeng/MCTrans.",
          "link": "http://arxiv.org/abs/2106.14385",
          "publishedOn": "2021-06-29T01:55:14.978Z",
          "wordCount": 623,
          "title": "Multi-Compound Transformer for Accurate Biomedical Image Segmentation. (arXiv:2106.14385v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>",
          "description": "The past few years have witnessed great progress in the domain of face\nrecognition thanks to advances in deep learning. However, cross pose face\nrecognition remains a significant challenge. It is difficult for many deep\nlearning algorithms to narrow the performance gap caused by pose variations;\nthe main reasons for this relate to the intra-class discrepancy between face\nimages in different poses and the pose imbalances of training datasets.\nLearning pose-robust features by traversing to the feature space of frontal\nfaces provides an effective and cheap way to alleviate this problem. In this\npaper, we present a method for progressively transforming profile face\nrepresentations to the canonical pose with an attentive pair-wise loss.\nFirstly, to reduce the difficulty of directly transforming the profile face\nfeatures into a frontal pose, we propose to learn the feature residual between\nthe source pose and its nearby pose in a block-byblock fashion, and thus\ntraversing to the feature space of a smaller pose by adding the learned\nresidual. Secondly, we propose an attentive pair-wise loss to guide the feature\ntransformation progressing in the most effective direction. Finally, our\nproposed progressive module and attentive pair-wise loss are light-weight and\neasy to implement, adding only about 7:5% extra parameters. Evaluations on the\nCFP and CPLFW datasets demonstrate the superiority of our proposed method. Code\nis available at https://github.com/hjy1312/AGPM.",
          "link": "http://arxiv.org/abs/2106.14124",
          "publishedOn": "2021-06-29T01:55:14.970Z",
          "wordCount": 662,
          "title": "Attention-guided Progressive Mapping for Profile Face Recognition. (arXiv:2106.14124v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dan Yang</a>",
          "description": "Data augmentation is a powerful technique for improving the performance of\nthe few-shot classification task. It generates more samples as supplements, and\nthen this task can be transformed into a common supervised learning issue for\nsolution. However, most mainstream data augmentation based approaches only\nconsider the single modality information, which leads to the low diversity and\nquality of generated features. In this paper, we present a novel multi-modal\ndata augmentation approach named Dizygotic Conditional Variational AutoEncoder\n(DCVAE) for addressing the aforementioned issue. DCVAE conducts feature\nsynthesis via pairing two Conditional Variational AutoEncoders (CVAEs) with the\nsame seed but different modality conditions in a dizygotic symbiosis manner.\nSubsequently, the generated features of two CVAEs are adaptively combined to\nyield the final feature, which can be converted back into its paired conditions\nwhile ensuring these conditions are consistent with the original conditions not\nonly in representation but also in function. DCVAE essentially provides a new\nidea of data augmentation in various multi-modal scenarios by exploiting the\ncomplement of different modality prior information. Extensive experimental\nresults demonstrate our work achieves state-of-the-art performances on\nminiImageNet, CIFAR-FS and CUB datasets, and is able to work well in the\npartial modality absence case.",
          "link": "http://arxiv.org/abs/2106.14467",
          "publishedOn": "2021-06-29T01:55:14.955Z",
          "wordCount": 647,
          "title": "Dizygotic Conditional Variational AutoEncoder for Multi-Modal and Partial Modality Absent Few-Shot Learning. (arXiv:2106.14467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.04830",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1\">Jiansheng Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Z/0/1/0/all/0/1\">Zunjie Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1\">Risa Higashita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>",
          "description": "Cataract is one of the leading causes of reversible visual impairment and\nblindness globally. Over the years, researchers have achieved significant\nprogress in developing state-of-the-art artificial intelligence techniques for\nautomatic cataract classification and grading, helping clinicians prevent and\ntreat cataract in time. This paper provides a comprehensive survey of recent\nadvances in machine learning for cataract classification and grading based on\nophthalmic images. We summarize existing literature from two research\ndirections: conventional machine learning techniques and deep learning\ntechniques. This paper also provides insights into existing works of both\nmerits and limitations. In addition, we discuss several challenges of automatic\ncataract classification and grading based on machine learning techniques and\npresent possible solutions to these challenges for future research.",
          "link": "http://arxiv.org/abs/2012.04830",
          "publishedOn": "2021-06-29T01:55:14.946Z",
          "wordCount": 612,
          "title": "Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shaozuo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Siwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>",
          "description": "This paper presents the Rail-5k dataset for benchmarking the performance of\nvisual algorithms in a real-world application scenario, namely the rail surface\ndefects detection task. We collected over 5k high-quality images from railways\nacross China, and annotated 1100 images with the help from railway experts to\nidentify the most common 13 types of rail defects. The dataset can be used for\ntwo settings both with unique challenges, the first is the fully-supervised\nsetting using the 1k+ labeled images for training, fine-grained nature and\nlong-tailed distribution of defect classes makes it hard for visual algorithms\nto tackle. The second is the semi-supervised learning setting facilitated by\nthe 4k unlabeled images, these 4k images are uncurated containing possible\nimage corruptions and domain shift with the labeled images, which can not be\neasily tackle by previous semi-supervised learning methods. We believe our\ndataset could be a valuable benchmark for evaluating robustness and reliability\nof visual algorithms.",
          "link": "http://arxiv.org/abs/2106.14366",
          "publishedOn": "2021-06-29T01:55:14.938Z",
          "wordCount": 593,
          "title": "Rail-5k: a Real-World Dataset for Rail Surface Defects Detection. (arXiv:2106.14366v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_H/0/1/0/all/0/1\">Hyuntak Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaeho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Recent breakthroughs in self-supervised learning show that such algorithms\nlearn visual representations that can be transferred better to unseen tasks\nthan joint-training methods relying on task-specific supervision. In this\npaper, we found that the similar holds in the continual learning con-text:\ncontrastively learned representations are more robust against the catastrophic\nforgetting than jointly trained representations. Based on this novel\nobservation, we propose a rehearsal-based continual learning algorithm that\nfocuses on continually learning and maintaining transferable representations.\nMore specifically, the proposed scheme (1) learns representations using the\ncontrastive learning objective, and (2) preserves learned representations using\na self-supervised distillation step. We conduct extensive experimental\nvalidations under popular benchmark image classification datasets, where our\nmethod sets the new state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.14413",
          "publishedOn": "2021-06-29T01:55:14.925Z",
          "wordCount": 550,
          "title": "Co$^2$L: Contrastive Continual Learning. (arXiv:2106.14413v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14292",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jain_R/0/1/0/all/0/1\">Rohit Kumar Jain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1\">Prasen Kumar Sharma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaj_S/0/1/0/all/0/1\">Sibaji Gaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sur_A/0/1/0/all/0/1\">Arijit Sur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_P/0/1/0/all/0/1\">Palash Ghosh</a>",
          "description": "Knee Osteoarthritis (OA) is a destructive joint disease identified by joint\nstiffness, pain, and functional disability concerning millions of lives across\nthe globe. It is generally assessed by evaluating physical symptoms, medical\nhistory, and other joint screening tests like radiographs, Magnetic Resonance\nImaging (MRI), and Computed Tomography (CT) scans. Unfortunately, the\nconventional methods are very subjective, which forms a barrier in detecting\nthe disease progression at an early stage. This paper presents a deep\nlearning-based framework, namely OsteoHRNet, that automatically assesses the\nKnee OA severity in terms of Kellgren and Lawrence (KL) grade classification\nfrom X-rays. As a primary novelty, the proposed approach is built upon one of\nthe most recent deep models, called the High-Resolution Network (HRNet), to\ncapture the multi-scale features of knee X-rays. In addition, we have also\nincorporated an attention mechanism to filter out the counterproductive\nfeatures and boost the performance further. Our proposed model has achieved the\nbest multiclass accuracy of 71.74% and MAE of 0.311 on the baseline cohort of\nthe OAI dataset, which is a remarkable gain over the existing best-published\nworks. We have also employed the Gradient-based Class Activation Maps\n(Grad-CAMs) visualization to justify the proposed network learning.",
          "link": "http://arxiv.org/abs/2106.14292",
          "publishedOn": "2021-06-29T01:55:14.920Z",
          "wordCount": 680,
          "title": "Knee Osteoarthritis Severity Prediction using an Attentive Multi-Scale Deep Convolutional Neural Network. (arXiv:2106.14292v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.04945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Szandala_T/0/1/0/all/0/1\">Tomasz Szandala</a>",
          "description": "In this paper, an enhancement technique for the class activation mapping\nmethods such as gradient-weighted class activation maps or excitation\nbackpropagation is proposed to present the visual explanations of decisions\nfrom convolutional neural network-based models. The proposed idea, called\nGradual Extrapolation, can supplement any method that generates a heatmap\npicture by sharpening the output. Instead of producing a coarse localization\nmap that highlights the important predictive regions in the image, the proposed\nmethod outputs the specific shape that most contributes to the model output.\nThus, the proposed method improves the accuracy of saliency maps. The effect\nhas been achieved by the gradual propagation of the crude map obtained in the\ndeep layer through all preceding layers with respect to their activations. In\nvalidation tests conducted on a selected set of images, the faithfulness,\ninterpretability, and applicability of the method are evaluated. The proposed\ntechnique significantly improves the localization detection of the neural\nnetworks attention at low additional computational costs. Furthermore, the\nproposed method is applicable to a variety deep neural network models. The code\nfor the method can be found at\nhttps://github.com/szandala/gradual-extrapolation",
          "link": "http://arxiv.org/abs/2104.04945",
          "publishedOn": "2021-06-29T01:55:14.914Z",
          "wordCount": 653,
          "title": "Enhancing Deep Neural Network Saliency Visualizations with Gradual Extrapolation. (arXiv:2104.04945v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08239",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>",
          "description": "Interpretability is a critical factor in applying complex deep learning\nmodels to advance the understanding of brain disorders in neuroimaging studies.\nTo interpret the decision process of a trained classifier, existing techniques\ntypically rely on saliency maps to quantify the voxel-wise or feature-level\nimportance for classification through partial derivatives. Despite providing\nsome level of localization, these maps are not human-understandable from the\nneuroscience perspective as they do not inform the specific meaning of the\nalteration linked to the brain disorder. Inspired by the image-to-image\ntranslation scheme, we propose to train simulator networks that can warp a\ngiven image to inject or remove patterns of the disease. These networks are\ntrained such that the classifier produces consistently increased or decreased\nprediction logits for the simulated images. Moreover, we propose to couple all\nthe simulators into a unified model based on conditional convolution. We\napplied our approach to interpreting classifiers trained on a synthetic dataset\nand two neuroimaging datasets to visualize the effect of the Alzheimer's\ndisease and alcohol use disorder. Compared to the saliency maps generated by\nbaseline approaches, our simulations and visualizations based on the Jacobian\ndeterminants of the warping field reveal meaningful and understandable patterns\nrelated to the diseases.",
          "link": "http://arxiv.org/abs/2102.08239",
          "publishedOn": "2021-06-29T01:55:14.908Z",
          "wordCount": 673,
          "title": "Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models. (arXiv:2102.08239v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_P/0/1/0/all/0/1\">Praveen Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1\">Rwik Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Varun Jain</a>",
          "description": "In self driving car applications, there is a requirement to predict the\nlocation of the lane given an input RGB front facing image. In this paper, we\npropose an architecture that allows us to increase the speed and robustness of\nroad detection without a large hit in accuracy by introducing an underlying\nshared feature space that is propagated over time, which serves as a flowing\ndynamic memory. By utilizing the gist of previous frames, we train the network\nto predict the current road with a greater accuracy and lesser deviation from\nprevious frames.",
          "link": "http://arxiv.org/abs/2106.14184",
          "publishedOn": "2021-06-29T01:55:14.902Z",
          "wordCount": 518,
          "title": "Memory Guided Road Detection. (arXiv:2106.14184v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.06013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Text spotting in natural scene images is of great importance for many image\nunderstanding tasks. It includes two sub-tasks: text detection and recognition.\nIn this work, we propose a unified network that simultaneously localizes and\nrecognizes text with a single forward pass, avoiding intermediate processes\nsuch as image cropping and feature re-calculation, word separation, and\ncharacter grouping.\n\nIn contrast to existing approaches that consider text detection and\nrecognition as two distinct tasks and tackle them one by one, the proposed\nframework settles these two tasks concurrently. The whole framework can be\ntrained end-to-end and is able to handle text of arbitrary shapes. The\nconvolutional features are calculated only once and shared by both detection\nand recognition modules. Through multi-task training, the learned features\nbecome more discriminate and improve the overall performance. By employing the\n$2$D attention model in word recognition, the irregularity of text can be\nrobustly addressed. It provides the spatial location for each character, which\nnot only helps local feature extraction in word recognition, but also indicates\nan orientation angle to refine text localization. Our proposed method has\nachieved state-of-the-art performance on several standard text spotting\nbenchmarks, including both regular and irregular ones.",
          "link": "http://arxiv.org/abs/1906.06013",
          "publishedOn": "2021-06-29T01:55:14.897Z",
          "wordCount": 703,
          "title": "Towards End-to-End Text Spotting in Natural Scenes. (arXiv:1906.06013v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14178",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quanziang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Renzhen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "Location information is proven to benefit the deep learning models on\ncapturing the manifold structure of target objects, and accordingly boosts the\naccuracy of medical image segmentation. However, most existing methods encode\nthe location information in an implicit way, e.g. the distance transform maps,\nwhich describe the relative distance from each pixel to the contour boundary,\nfor the network to learn. These implicit approaches do not fully exploit the\nposition information (i.e. absolute location) of targets. In this paper, we\npropose a novel loss function, namely residual moment (RM) loss, to explicitly\nembed the location information of segmentation targets during the training of\ndeep learning networks. Particularly, motivated by image moments, the\nsegmentation prediction map and ground-truth map are weighted by coordinate\ninformation. Then our RM loss encourages the networks to maintain the\nconsistency between the two weighted maps, which promotes the segmentation\nnetworks to easily locate the targets and extract manifold-structure-related\nfeatures. We validate the proposed RM loss by conducting extensive experiments\non two publicly available datasets, i.e., 2D optic cup and disk segmentation\nand 3D left atrial segmentation. The experimental results demonstrate the\neffectiveness of our RM loss, which significantly boosts the accuracy of\nsegmentation networks.",
          "link": "http://arxiv.org/abs/2106.14178",
          "publishedOn": "2021-06-29T01:55:14.887Z",
          "wordCount": 648,
          "title": "Residual Moment Loss for Medical Image Segmentation. (arXiv:2106.14178v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahishali_M/0/1/0/all/0/1\">Mete Ahishali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamac_M/0/1/0/all/0/1\">Mehmet Yamac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>",
          "description": "In this study, we propose a novel approach to predict the distances of the\ndetected objects in an observed scene. The proposed approach modifies the\nrecently proposed Convolutional Support Estimator Networks (CSENs). CSENs are\ndesigned to compute a direct mapping for the Support Estimation (SE) task in a\nrepresentation-based classification problem. We further propose and demonstrate\nthat representation-based methods (sparse or collaborative representation) can\nbe used in well-designed regression problems. To the best of our knowledge,\nthis is the first representation-based method proposed for performing a\nregression task by utilizing the modified CSENs; and hence, we name this novel\napproach as Representation-based Regression (RbR). The initial version of CSENs\nhas a proxy mapping stage (i.e., a coarse estimation for the support set) that\nis required for the input. In this study, we improve the CSEN model by\nproposing Compressive Learning CSEN (CL-CSEN) that has the ability to jointly\noptimize the so-called proxy mapping stage along with convolutional layers. The\nexperimental evaluations using the KITTI 3D Object Detection distance\nestimation dataset show that the proposed method can achieve a significantly\nimproved distance estimation performance over all competing methods. Finally,\nthe software implementations of the methods are publicly shared at\nhttps://github.com/meteahishali/CSENDistance.",
          "link": "http://arxiv.org/abs/2106.14208",
          "publishedOn": "2021-06-29T01:55:14.881Z",
          "wordCount": 642,
          "title": "Representation Based Regression for Object Distance Estimation. (arXiv:2106.14208v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalgaonkar_P/0/1/0/all/0/1\">Priyank Kalgaonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sharkawy_M/0/1/0/all/0/1\">Mohamed El-Sharkawy</a>",
          "description": "In this paper, we demonstrate the implementation of our ultra-efficient deep\nconvolutional neural network architecture: CondenseNeXt on NXP BlueBox, an\nautonomous driving development platform developed for self-driving vehicles. We\nshow that CondenseNeXt is remarkably efficient in terms of FLOPs, designed for\nARM-based embedded computing platforms with limited computational resources and\ncan perform image classification without the need of a CUDA enabled GPU.\nCondenseNeXt utilizes the state-of-the-art depthwise separable convolution and\nmodel compression techniques to achieve a remarkable computational efficiency.\nExtensive analyses are conducted on CIFAR-10, CIFAR-100 and ImageNet datasets\nto verify the performance of CondenseNeXt Convolutional Neural Network (CNN)\narchitecture. It achieves state-of-the-art image classification performance on\nthree benchmark datasets including CIFAR-10 (4.79% top-1 error), CIFAR-100\n(21.98% top-1 error) and ImageNet (7.91% single model, single crop top-5\nerror). CondenseNeXt achieves final trained model size improvement of 2.9+ MB\nand up to 59.98% reduction in forward FLOPs compared to CondenseNet and can\nperform image classification on ARM-Based computing platforms without needing a\nCUDA enabled GPU support, with outstanding efficiency.",
          "link": "http://arxiv.org/abs/2106.14102",
          "publishedOn": "2021-06-29T01:55:14.875Z",
          "wordCount": 620,
          "title": "Image Classification with CondenseNeXt for ARM-Based Computing Platforms. (arXiv:2106.14102v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14256",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1\">Boing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yinxi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weitz_P/0/1/0/all/0/1\">Philippe Weitz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lindberg_J/0/1/0/all/0/1\">Johan Lindberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egevad_L/0/1/0/all/0/1\">Lars Egevad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gronberg_H/0/1/0/all/0/1\">Henrik Gr&#xf6;nberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eklund_M/0/1/0/all/0/1\">Martin Eklund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rantalainen_M/0/1/0/all/0/1\">Mattias Rantalainen</a>",
          "description": "Background: Transrectal ultrasound guided systematic biopsies of the prostate\nis a routine procedure to establish a prostate cancer diagnosis. However, the\n10-12 prostate core biopsies only sample a relatively small volume of the\nprostate, and tumour lesions in regions between biopsy cores can be missed,\nleading to a well-known low sensitivity to detect clinically relevant cancer.\nAs a proof-of-principle, we developed and validated a deep convolutional neural\nnetwork model to distinguish between morphological patterns in benign prostate\nbiopsy whole slide images from men with and without established cancer.\nMethods: This study included 14,354 hematoxylin and eosin stained whole slide\nimages from benign prostate biopsies from 1,508 men in two groups: men without\nan established prostate cancer (PCa) diagnosis and men with at least one core\nbiopsy diagnosed with PCa. 80% of the participants were assigned as training\ndata and used for model optimization (1,211 men), and the remaining 20% (297\nmen) as a held-out test set used to evaluate model performance. An ensemble of\n10 deep convolutional neural network models was optimized for classification of\nbiopsies from men with and without established cancer. Hyperparameter\noptimization and model selection was performed by cross-validation in the\ntraining data . Results: Area under the receiver operating characteristic curve\n(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy\nlevel and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a\nspecificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:\nThe developed model has the ability to detect men with risk of missed PCa due\nto under-sampling of the prostate. The proposed model has the potential to\nreduce the number of false negative cases in routine systematic prostate\nbiopsies and to indicate men who could benefit from MRI-guided re-biopsy.",
          "link": "http://arxiv.org/abs/2106.14256",
          "publishedOn": "2021-06-29T01:55:14.832Z",
          "wordCount": 764,
          "title": "Using deep learning to detect patients at risk for prostate cancer despite benign biopsies. (arXiv:2106.14256v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>",
          "description": "We ask the question: to what extent can recent large-scale language and image\ngeneration models blend visual concepts? Given an arbitrary object, we identify\na relevant object and generate a single-sentence description of the blend of\nthe two using a language model. We then generate a visual depiction of the\nblend using a text-based image generation model. Quantitative and qualitative\nevaluations demonstrate the superiority of language models over classical\nmethods for conceptual blending, and of recent large-scale image generation\nmodels over prior models for the visual depiction.",
          "link": "http://arxiv.org/abs/2106.14127",
          "publishedOn": "2021-06-29T01:55:14.822Z",
          "wordCount": 527,
          "title": "Visual Conceptual Blending with Large-scale Language and Vision Models. (arXiv:2106.14127v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14349",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Jia_P/0/1/0/all/0/1\">Peng Jia</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sun_Y/0/1/0/all/0/1\">Yongyang Sun</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>",
          "description": "Wide field small aperture telescopes (WFSATs) are mainly used to obtain\nscientific information of point--like and streak--like celestial objects.\nHowever, qualities of images obtained by WFSATs are seriously affected by the\nbackground noise and variable point spread functions. Developing high speed and\nhigh efficiency data processing method is of great importance for further\nscientific research. In recent years, deep neural networks have been proposed\nfor detection and classification of celestial objects and have shown better\nperformance than classical methods. In this paper, we further extend abilities\nof the deep neural network based astronomical target detection framework to\nmake it suitable for photometry and astrometry. We add new branches into the\ndeep neural network to obtain types, magnitudes and positions of different\ncelestial objects at the same time. Tested with simulated data, we find that\nour neural network has better performance in photometry than classical methods.\nBecause photometry and astrometry are regression algorithms, which would obtain\nhigh accuracy measurements instead of rough classification results, the\naccuracy of photometry and astrometry results would be affected by different\nobservation conditions. To solve this problem, we further propose to use\nreference stars to train our deep neural network with transfer learning\nstrategy when observation conditions change. The photometry framework proposed\nin this paper could be used as an end--to--end quick data processing framework\nfor WFSATs, which can further increase response speed and scientific outputs of\nWFSATs.",
          "link": "http://arxiv.org/abs/2106.14349",
          "publishedOn": "2021-06-29T01:55:14.809Z",
          "wordCount": 715,
          "title": "The Deep Neural Network based Photometry Framework for Wide Field Small Aperture Telescopes. (arXiv:2106.14349v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Quanfu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun-Fu/0/1/0/all/0/1\">Chun-Fu</a> (Richard) <a href=\"http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1\">Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>",
          "description": "We propose a new perspective on video understanding by casting the video\nrecognition problem as an image recognition task. We show that an image\nclassifier alone can suffice for video understanding without temporal modeling.\nOur approach is simple and universal. It composes input frames into a super\nimage to train an image classifier to fulfill the task of action recognition,\nin exactly the same way as classifying an image. We prove the viability of such\nan idea by demonstrating strong and promising performance on four public\ndatasets including Kinetics400, Something-to-something (V2), MiT and Jester,\nusing a recently developed vision transformer. We also experiment with the\nprevalent ResNet image classifiers in computer vision to further validate our\nidea. The results on Kinetics400 are comparable to some of the best-performed\nCNN approaches based on spatio-temporal modeling. our code and models will be\nmade available at https://github.com/IBM/sifar-pytorch.",
          "link": "http://arxiv.org/abs/2106.14104",
          "publishedOn": "2021-06-29T01:55:14.793Z",
          "wordCount": 578,
          "title": "An Image Classifier Can Suffice Video Understanding. (arXiv:2106.14104v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "Polygonal meshes are ubiquitous, but have only played a relatively minor role\nin the deep learning revolution. State-of-the-art neural generative models for\n3D shapes learn implicit functions and generate meshes via expensive\niso-surfacing. We overcome these challenges by employing a classical spatial\ndata structure from computer graphics, Binary Space Partitioning (BSP), to\nfacilitate 3D learning. The core operation of BSP involves recursive\nsubdivision of 3D space to obtain convex sets. By exploiting this property, we\ndevise BSP-Net, a network that learns to represent a 3D shape via convex\ndecomposition without supervision. The network is trained to reconstruct a\nshape using a set of convexes obtained from a BSP-tree built over a set of\nplanes, where the planes and convexes are both defined by learned network\nweights. BSP-Net directly outputs polygonal meshes from the inferred convexes.\nThe generated meshes are watertight, compact (i.e., low-poly), and well suited\nto represent sharp geometry. We show that the reconstruction quality by BSP-Net\nis competitive with those from state-of-the-art methods while using much fewer\nprimitives. We also explore variations to BSP-Net including using a more\ngeneric decoder for reconstruction, more general primitives than planes, as\nwell as training a generative model with variational auto-encoders. Code is\navailable at https://github.com/czq142857/BSP-NET-original.",
          "link": "http://arxiv.org/abs/2106.14274",
          "publishedOn": "2021-06-29T01:55:14.777Z",
          "wordCount": 663,
          "title": "Learning Mesh Representations via Binary Space Partitioning Tree Networks. (arXiv:2106.14274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zizheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuelin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>",
          "description": "Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in\nhuman environments is an important yet challenging task for future\nhome-assistant robots. The space of 3D articulated objects is exceptionally\nrich in their myriad semantic categories, diverse shape geometry, and\ncomplicated part functionality. Previous works mostly abstract kinematic\nstructure with estimated joint parameters and part poses as the visual\nrepresentations for manipulating 3D articulated objects. In this paper, we\npropose object-centric actionable visual priors as a novel\nperception-interaction handshaking point that the perception system outputs\nmore actionable guidance than kinematic structure estimation, by predicting\ndense geometry-aware, interaction-aware, and task-aware visual action\naffordance and trajectory proposals. We design an interaction-for-perception\nframework VAT-Mart to learn such actionable visual representations by\nsimultaneously training a curiosity-driven reinforcement learning policy\nexploring diverse interaction trajectories and a perception module summarizing\nand generalizing the explored knowledge for pointwise predictions among diverse\nshapes. Experiments prove the effectiveness of the proposed approach using the\nlarge-scale PartNet-Mobility dataset in SAPIEN environment and show promising\ngeneralization capabilities to novel test shapes, unseen object categories, and\nreal-world data. Project page: https://hyperplane-lab.github.io/vat-mart",
          "link": "http://arxiv.org/abs/2106.14440",
          "publishedOn": "2021-06-29T01:55:14.751Z",
          "wordCount": 638,
          "title": "VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects. (arXiv:2106.14440v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murhij_Y/0/1/0/all/0/1\">Youshaa Murhij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudin_D/0/1/0/all/0/1\">Dmitry Yudin</a>",
          "description": "In this paper, we present a real-time 3D detection approach considering\ntime-spatial feature map aggregation from different time steps of deep neural\nmodel inference (named feature map flow, FMF). Proposed approach improves the\nquality of 3D detection center-based baseline and provides real-time\nperformance on the nuScenes and Waymo benchmark. Code is available at\nhttps://github.com/YoushaaMurhij/FMFNet",
          "link": "http://arxiv.org/abs/2106.14101",
          "publishedOn": "2021-06-29T01:55:14.743Z",
          "wordCount": 502,
          "title": "Real-time 3D Object Detection using Feature Map Flow. (arXiv:2106.14101v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Le Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jingyu Xin</a>",
          "description": "With rapidly evolving internet technologies and emerging tools, sports\nrelated videos generated online are increasing at an unprecedentedly fast pace.\nTo automate sports video editing/highlight generation process, a key task is to\nprecisely recognize and locate the events in the long untrimmed videos. In this\ntech report, we present a two-stage paradigm to detect what and when events\nhappen in soccer broadcast videos. Specifically, we fine-tune multiple action\nrecognition models on soccer data to extract high-level semantic features, and\ndesign a transformer based temporal detection module to locate the target\nevents. This approach achieved the state-of-the-art performance in both two\ntasks, i.e., action spotting and replay grounding, in the SoccerNet-v2\nChallenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features\nare released at https://github.com/baidu-research/vidpress-sports. By sharing\nthese features with the broader community, we hope to accelerate the research\ninto soccer video understanding.",
          "link": "http://arxiv.org/abs/2106.14447",
          "publishedOn": "2021-06-29T01:55:14.736Z",
          "wordCount": 611,
          "title": "Feature Combination Meets Attention: Baidu Soccer Embeddings and Transformer based Temporal Detection. (arXiv:2106.14447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Junru Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>",
          "description": "In autonomous driving, goal-based multi-trajectory prediction methods are\nproved to be effective recently, where they first score goal candidates, then\nselect a final set of goals, and finally complete trajectories based on the\nselected goals. However, these methods usually involve goal predictions based\non sparse predefined anchors. In this work, we propose an anchor-free model,\nnamed DenseTNT, which performs dense goal probability estimation for trajectory\nprediction. Our model achieves state-of-the-art performance, and ranks 1st on\nthe Waymo Open Dataset Motion Prediction Challenge.",
          "link": "http://arxiv.org/abs/2106.14160",
          "publishedOn": "2021-06-29T01:55:14.730Z",
          "wordCount": 523,
          "title": "DenseTNT: Waymo Open Dataset Motion Prediction Challenge 1st Place Solution. (arXiv:2106.14160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.04641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuhong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Di Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaofeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Naifu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>",
          "description": "In this paper, we propose a novel task, Manipulation Question Answering\n(MQA), where the robot performs manipulation actions to change the environment\nin order to answer a given question. To solve this problem, a framework\nconsisting of a QA module and a manipulation module is proposed. For the QA\nmodule, we adopt the method for the Visual Question Answering (VQA) task. For\nthe manipulation module, a Deep Q Network (DQN) model is designed to generate\nmanipulation actions for the robot to interact with the environment. We\nconsider the situation where the robot continuously manipulating objects inside\na bin until the answer to the question is found. Besides, a novel dataset that\ncontains a variety of object models, scenarios and corresponding\nquestion-answer pairs is established in a simulation environment. Extensive\nexperiments have been conducted to validate the effectiveness of the proposed\nframework.",
          "link": "http://arxiv.org/abs/2003.04641",
          "publishedOn": "2021-06-29T01:55:14.712Z",
          "wordCount": 631,
          "title": "MQA: Answering the Question via Robotic Manipulation. (arXiv:2003.04641v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1\">AJ Piergiovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>",
          "description": "Video understanding is a challenging problem with great impact on the\nabilities of autonomous agents working in the real-world. Yet, solutions so far\nhave been computationally intensive, with the fastest algorithms running for\nmore than half a second per video snippet on powerful GPUs. We propose a novel\nidea on video architecture learning - Tiny Video Networks - which automatically\ndesigns highly efficient models for video understanding. The tiny video models\nrun with competitive performance for as low as 37 milliseconds per video on a\nCPU and 10 milliseconds on a standard GPU.",
          "link": "http://arxiv.org/abs/1910.06961",
          "publishedOn": "2021-06-29T01:55:14.704Z",
          "wordCount": 548,
          "title": "Tiny Video Networks. (arXiv:1910.06961v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14192",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Yi_K/0/1/0/all/0/1\">Kai Yi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pang_J/0/1/0/all/0/1\">Jianye Pang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Y/0/1/0/all/0/1\">Yungeng Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrui Zeng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Cryo-electron tomography (Cryo-ET) is a 3D imaging technique that enables the\nsystemic study of shape, abundance, and distribution of macromolecular\nstructures in single cells in near-atomic resolution. However, the systematic\nand efficient $\\textit{de novo}$ recognition and recovery of macromolecular\nstructures captured by Cryo-ET are very challenging due to the structural\ncomplexity and imaging limits. Even macromolecules with identical structures\nhave various appearances due to different orientations and imaging limits, such\nas noise and the missing wedge effect. Explicitly disentangling the semantic\nfeatures of macromolecules is crucial for performing several downstream\nanalyses on the macromolecules. This paper has addressed the problem by\nproposing a 3D Spatial Variational Autoencoder that explicitly disentangle the\nstructure, orientation, and shift of macromolecules. Extensive experiments on\nboth synthesized and real cryo-ET datasets and cross-domain evaluations\ndemonstrate the efficacy of our method.",
          "link": "http://arxiv.org/abs/2106.14192",
          "publishedOn": "2021-06-29T01:55:14.697Z",
          "wordCount": 572,
          "title": "Disentangling semantic features of macromolecules in Cryo-Electron Tomography. (arXiv:2106.14192v1 [q-bio.BM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yulun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_F/0/1/0/all/0/1\">Fernando Herrera Arias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_Granda_C/0/1/0/all/0/1\">Carlos Nieto-Granda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1\">Jonathan P. How</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>",
          "description": "This paper presents Kimera-Multi, the first multi-robot system that (i) is\nrobust and capable of identifying and rejecting incorrect inter and intra-robot\nloop closures resulting from perceptual aliasing, (ii) is fully distributed and\nonly relies on local (peer-to-peer) communication to achieve distributed\nlocalization and mapping, and (iii) builds a globally consistent\nmetric-semantic 3D mesh model of the environment in real-time, where faces of\nthe mesh are annotated with semantic labels. Kimera-Multi is implemented by a\nteam of robots equipped with visual-inertial sensors. Each robot builds a local\ntrajectory estimate and a local mesh using Kimera. When communication is\navailable, robots initiate a distributed place recognition and robust pose\ngraph optimization protocol based on a novel distributed graduated\nnon-convexity algorithm. The proposed protocol allows the robots to improve\ntheir local trajectory estimates by leveraging inter-robot loop closures while\nbeing robust to outliers. Finally, each robot uses its improved trajectory\nestimate to correct the local mesh using mesh deformation techniques.\n\nWe demonstrate Kimera-Multi in photo-realistic simulations, SLAM benchmarking\ndatasets, and challenging outdoor datasets collected using ground robots. Both\nreal and simulated experiments involve long trajectories (e.g., up to 800\nmeters per robot). The experiments show that Kimera-Multi (i) outperforms the\nstate of the art in terms of robustness and accuracy, (ii) achieves estimation\nerrors comparable to a centralized SLAM system while being fully distributed,\n(iii) is parsimonious in terms of communication bandwidth, (iv) produces\naccurate metric-semantic 3D meshes, and (v) is modular and can be also used for\nstandard 3D reconstruction (i.e., without semantic labels) or for trajectory\nestimation (i.e., without reconstructing a 3D mesh).",
          "link": "http://arxiv.org/abs/2106.14386",
          "publishedOn": "2021-06-29T01:55:14.690Z",
          "wordCount": 715,
          "title": "Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems. (arXiv:2106.14386v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanbin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "In this paper, we propose a novel image process scheme called class-based\nexpansion learning for image classification, which aims at improving the\nsupervision-stimulation frequency for the samples of the confusing classes.\nClass-based expansion learning takes a bottom-up growing strategy in a\nclass-based expansion optimization fashion, which pays more attention to the\nquality of learning the fine-grained classification boundaries for the\npreferentially selected classes. Besides, we develop a class confusion\ncriterion to select the confusing class preferentially for training. In this\nway, the classification boundaries of the confusing classes are frequently\nstimulated, resulting in a fine-grained form. Experimental results demonstrate\nthe effectiveness of the proposed scheme on several benchmarks.",
          "link": "http://arxiv.org/abs/2106.14412",
          "publishedOn": "2021-06-29T01:55:14.683Z",
          "wordCount": 549,
          "title": "Progressive Class-based Expansion Learning For Image Classification. (arXiv:2106.14412v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiake Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+and_Y/0/1/0/all/0/1\">Yong Tang and</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>",
          "description": "Image matting is an ill-posed problem that aims to estimate the opacity of\nforeground pixels in an image. However, most existing deep learning-based\nmethods still suffer from the coarse-grained details. In general, these\nalgorithms are incapable of felicitously distinguishing the degree of\nexploration between deterministic domains (certain FG and BG pixels) and\nundetermined domains (uncertain in-between pixels), or inevitably lose\ninformation in the continuous sampling process, leading to a sub-optimal\nresult. In this paper, we propose a novel network named Prior-Induced\nInformation Alignment Matting Network (PIIAMatting), which can efficiently\nmodel the distinction of pixel-wise response maps and the correlation of\nlayer-wise feature maps. It mainly consists of a Dynamic Gaussian Modulation\nmechanism (DGM) and an Information Alignment strategy (IA). Specifically, the\nDGM can dynamically acquire a pixel-wise domain response map learned from the\nprior distribution. The response map can present the relationship between the\nopacity variation and the convergence process during training. On the other\nhand, the IA comprises an Information Match Module (IMM) and an Information\nAggregation Module (IAM), jointly scheduled to match and aggregate the adjacent\nlayer-wise features adaptively. Besides, we also develop a Multi-Scale\nRefinement (MSR) module to integrate multi-scale receptive field information at\nthe refinement stage to recover the fluctuating appearance details. Extensive\nquantitative and qualitative evaluations demonstrate that the proposed\nPIIAMatting performs favourably against state-of-the-art image matting methods\non the Alphamatting.com, Composition-1K and Distinctions-646 dataset.",
          "link": "http://arxiv.org/abs/2106.14439",
          "publishedOn": "2021-06-29T01:55:14.667Z",
          "wordCount": 671,
          "title": "Prior-Induced Information Alignment for Image Matting. (arXiv:2106.14439v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Razzhigaev_A/0/1/0/all/0/1\">Anton Razzhigaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kireev_K/0/1/0/all/0/1\">Klim Kireev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udovichenko_I/0/1/0/all/0/1\">Igor Udovichenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1\">Aleksandr Petiushko</a>",
          "description": "Several methods for inversion of face recognition models were recently\npresented, attempting to reconstruct a face from deep templates. Although some\nof these approaches work in a black-box setup using only face embeddings,\nusually, on the end-user side, only similarity scores are provided. Therefore,\nthese algorithms are inapplicable in such scenarios. We propose a novel\napproach that allows reconstructing the face querying only similarity scores of\nthe black-box model. While our algorithm operates in a more general setup,\nexperiments show that it is query efficient and outperforms the existing\nmethods.",
          "link": "http://arxiv.org/abs/2106.14290",
          "publishedOn": "2021-06-29T01:55:14.659Z",
          "wordCount": 526,
          "title": "Darker than Black-Box: Face Reconstruction from Similarity Queries. (arXiv:2106.14290v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalla_D/0/1/0/all/0/1\">Divya Chandrika Kalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_B/0/1/0/all/0/1\">Bedant Agarwal</a>",
          "description": "Powerful deep learning algorithms open an opportunity for solving non-image\nMachine Learning (ML) problems by transforming these problems to into the image\nrecognition problems. The CPC-R algorithm presented in this chapter converts\nnon-image data into images by visualizing non-image data. Then deep learning\nCNN algorithms solve the learning problems on these images. The design of the\nCPC-R algorithm allows preserving all high-dimensional information in 2-D\nimages. The use of pair values mapping instead of single value mapping used in\nthe alternative approaches allows encoding each n-D point with 2 times fewer\nvisual elements. The attributes of an n-D point are divided into pairs of its\nvalues and each pair is visualized as 2-D points in the same 2-D Cartesian\ncoordinates. Next, grey scale or color intensity values are assigned to each\npair to encode the order of pairs. This is resulted in the heatmap image. The\ncomputational experiments with CPC-R are conducted for different CNN\narchitectures, and methods to optimize the CPC-R images showing that the\ncombined CPC-R and deep learning CNN algorithms are able to solve non-image ML\nproblems reaching high accuracy on the benchmark datasets. This chapter expands\nour prior work by adding more experiments to test accuracy of classification,\nexploring saliency and informativeness of discovered features to test their\ninterpretability, and generalizing the approach.",
          "link": "http://arxiv.org/abs/2106.14350",
          "publishedOn": "2021-06-29T01:55:14.653Z",
          "wordCount": 655,
          "title": "Deep Learning Image Recognition for Non-images. (arXiv:2106.14350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14465",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1\">Sk Imran Hossain</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Herve_J/0/1/0/all/0/1\">Jocelyn de Go&#xeb;r de Herve</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1\">Md Shahriar Hassan</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Martineau_D/0/1/0/all/0/1\">Delphine Martineau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petrosyan_E/0/1/0/all/0/1\">Evelina Petrosyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corbain_V/0/1/0/all/0/1\">Violaine Corbain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beytout_J/0/1/0/all/0/1\">Jean Beytout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lebert_I/0/1/0/all/0/1\">Isabelle Lebert</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Baux_E/0/1/0/all/0/1\">Elisabeth Baux</a> (CHRU Nancy), <a href=\"http://arxiv.org/find/eess/1/au:+Cazorla_C/0/1/0/all/0/1\">C&#xe9;line Cazorla</a> (CHU de Saint-Etienne), <a href=\"http://arxiv.org/find/eess/1/au:+Eldin_C/0/1/0/all/0/1\">Carole Eldin</a> (IHU M&#xe9;diterran&#xe9;e Infection), <a href=\"http://arxiv.org/find/eess/1/au:+Hansmann_Y/0/1/0/all/0/1\">Yves Hansmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patrat_Delon_S/0/1/0/all/0/1\">Solene Patrat-Delon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prazuck_T/0/1/0/all/0/1\">Thierry Prazuck</a> (CHR), <a href=\"http://arxiv.org/find/eess/1/au:+Raffetin_A/0/1/0/all/0/1\">Alice Raffetin</a> (CHIV), <a href=\"http://arxiv.org/find/eess/1/au:+Tattevin_P/0/1/0/all/0/1\">Pierre Tattevin</a> (CHU Rennes), <a href=\"http://arxiv.org/find/eess/1/au:+VourcH_G/0/1/0/all/0/1\">Gwena&#xeb;l Vourc&#x27;H</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Lesens_O/0/1/0/all/0/1\">Olivier Lesens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguifo_E/0/1/0/all/0/1\">Engelbert Nguifo</a> (LIMOS)",
          "description": "Lyme disease is one of the most common infectious vector-borne diseases in\nthe world. In the early stage, the disease manifests itself in most cases with\nerythema migrans (EM) skin lesions. Better diagnosis of these early forms would\nallow improving the prognosis by preventing the transition to a severe late\nform thanks to appropriate antibiotic therapy. Recent studies show that\nconvolutional neural networks (CNNs) perform very well to identify skin lesions\nfrom the image but, there is not much work for Lyme disease prediction from EM\nlesion images. The main objective of this study is to extensively analyze the\neffectiveness of CNNs for diagnosing Lyme disease from images and to find out\nthe best CNN architecture for the purpose. There is no publicly available EM\nimage dataset for Lyme dis",
          "link": "http://arxiv.org/abs/2106.14465",
          "publishedOn": "2021-06-29T01:55:14.648Z",
          "wordCount": 857,
          "title": "Benchmarking convolutional neural networks for diagnosing Lyme disease from images. (arXiv:2106.14465v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Samira Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1\">Mojtaba Mahdavi</a>",
          "description": "Content-independent watermarks and block-wise independency can be considered\nas vulnerabilities in semi-fragile watermarking methods. In this paper to\nachieve the objectives of semi-fragile watermarking techniques, a method is\nproposed to not have the mentioned shortcomings. In the proposed method, the\nwatermark is generated by relying on image content and a key. Furthermore, the\nembedding scheme causes the watermarked blocks to become dependent on each\nother, using a key. In the embedding phase, the image is partitioned into\nnon-overlapping blocks. In order to detect and separate the different types of\nattacks more precisely, the proposed method embeds three copies of each\nwatermark bit into LWT coefficients of each 4x4 block. In the authentication\nphase, by voting between the extracted bits the error maps are created; these\nmaps indicate image authenticity and reveal the modified regions. Also, in\norder to automate the authentication, the images are classified into four\ncategories using seven features. Classification accuracy in the experiments is\n97.97 percent. It is noted that our experiments demonstrate that the proposed\nmethod is robust against JPEG compression and is competitive with a\nstate-of-the-art semi-fragile watermarking method, in terms of robustness and\nsemi-fragility.",
          "link": "http://arxiv.org/abs/2106.14150",
          "publishedOn": "2021-06-29T01:55:14.640Z",
          "wordCount": 633,
          "title": "Image content dependent semi-fragile watermarking with localized tamper detection. (arXiv:2106.14150v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1\">Townim Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalisha_M/0/1/0/all/0/1\">Mahira Jalisha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheraghian_A/0/1/0/all/0/1\">Ali Cheraghian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1\">Shafin Rahman</a>",
          "description": "When we fine-tune a well-trained deep learning model for a new set of\nclasses, the network learns new concepts but gradually forgets the knowledge of\nold training. In some real-life applications, we may be interested in learning\nnew classes without forgetting the capability of previous experience. Such\nlearning without forgetting problem is often investigated using 2D image\nrecognition tasks. In this paper, considering the growth of depth camera\ntechnology, we address the same problem for the 3D point cloud object data.\nThis problem becomes more challenging in the 3D domain than 2D because of the\nunavailability of large datasets and powerful pretrained backbone models. We\ninvestigate knowledge distillation techniques on 3D data to reduce catastrophic\nforgetting of the previous training. Moreover, we improve the distillation\nprocess by using semantic word vectors of object classes. We observe that\nexploring the interrelation of old and new knowledge during training helps to\nlearn new concepts without forgetting old ones. Experimenting on three 3D point\ncloud recognition backbones (PointNet, DGCNN, and PointConv) and synthetic\n(ModelNet40, ModelNet10) and real scanned (ScanObjectNN) datasets, we establish\nnew baseline results on learning without forgetting for 3D data. This research\nwill instigate many future works in this area.",
          "link": "http://arxiv.org/abs/2106.14275",
          "publishedOn": "2021-06-29T01:55:14.621Z",
          "wordCount": 647,
          "title": "Learning without Forgetting for 3D Point Cloud Objects. (arXiv:2106.14275v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noormandipour_M/0/1/0/all/0/1\">Mohammadreza Noormandipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanchen Wang</a>",
          "description": "In this work, we propose a parameterised quantum circuit learning approach to\npoint set matching problem. In contrast to previous annealing-based methods, we\npropose a quantum circuit-based framework whose parameters are optimised via\ndescending the gradients w.r.t a kernel-based loss function. We formulate the\nshape matching problem into a distribution learning task; that is, to learn the\ndistribution of the optimal transformation parameters. We show that this\nframework is able to find multiple optimal solutions for symmetric shapes and\nis more accurate, scalable and robust than the previous annealing-based method.\nCode, data and pre-trained weights are available at the project page:\n\\href{https://hansen7.github.io/qKC}{https://hansen7.github.io/qKC}",
          "link": "http://arxiv.org/abs/2102.06697",
          "publishedOn": "2021-06-29T01:55:14.564Z",
          "wordCount": 579,
          "title": "Matching Point Sets with Quantum Circuit Learning. (arXiv:2102.06697v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhashash_M/0/1/0/all/0/1\">Mostafa Elhashash</a>",
          "description": "3D recovery from multi-stereo and stereo images, as an important application\nof the image-based perspective geometry, serves many applications in computer\nvision, remote sensing and Geomatics. In this chapter, the authors utilize the\nimaging geometry and present approaches that perform 3D reconstruction from\ncross-view images that are drastically different in their viewpoints. We\nintroduce our framework that takes ground-view images and satellite images for\nfull 3D recovery, which includes necessary methods in satellite and\nground-based point cloud generation from images, 3D data co-registration,\nfusion and mesh generation. We demonstrate our proposed framework on a dataset\nconsisting of twelve satellite images and 150k video frames acquired through a\nvehicle-mounted Go-pro camera and demonstrate the reconstruction results. We\nhave also compared our results with results generated from an intuitive\nprocessing pipeline that involves typical geo-registration and meshing methods.",
          "link": "http://arxiv.org/abs/2106.14306",
          "publishedOn": "2021-06-29T01:55:14.548Z",
          "wordCount": 569,
          "title": "3D Reconstruction through Fusion of Cross-View Images. (arXiv:2106.14306v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1\">Dong Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoumzadeh_A/0/1/0/all/0/1\">Abbas Masoumzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yee-Hong Yang</a>",
          "description": "Many deep learning based methods are designed to remove non-uniform\n(spatially variant) motion blur caused by object motion and camera shake\nwithout knowing the blur kernel. Some methods directly output the latent sharp\nimage in one stage, while others utilize a multi-stage strategy (\\eg\nmulti-scale, multi-patch, or multi-temporal) to gradually restore the sharp\nimage. However, these methods have the following two main issues: 1) The\ncomputational cost of multi-stage is high; 2) The same convolution kernel is\napplied in different regions, which is not an ideal choice for non-uniform\nblur. Hence, non-uniform motion deblurring is still a challenging and open\nproblem. In this paper, we propose a new architecture which consists of\nmultiple Atrous Spatial Pyramid Deformable Convolution (ASPDC) modules to\ndeblur an image end-to-end with more flexibility. Multiple ASPDC modules\nimplicitly learn the pixel-specific motion with different dilation rates in the\nsame layer to handle movements of different magnitude. To improve the training,\nwe also propose a reblurring network to map the deblurred output back to the\nblurred input, which constrains the solution space. Our experimental results\nshow that the proposed method outperforms state-of-the-art methods on the\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2106.14336",
          "publishedOn": "2021-06-29T01:55:14.542Z",
          "wordCount": 638,
          "title": "Blind Non-Uniform Motion Deblurring using Atrous Spatial Pyramid Deformable Convolution and Deblurring-Reblurring Consistency. (arXiv:2106.14336v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14207",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1\">Amith Khandakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1\">Muhammad E. H. Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reaz_M/0/1/0/all/0/1\">Mamun Bin Ibne Reaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sawal Hamid Md Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_M/0/1/0/all/0/1\">Md Anwarul Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1\">Tawsifur Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alfkey_R/0/1/0/all/0/1\">Rashad Alfkey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakar_A/0/1/0/all/0/1\">Ahmad Ashrif A. Bakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_R/0/1/0/all/0/1\">Rayaz A. Malik</a>",
          "description": "Diabetes foot ulceration (DFU) and amputation are a cause of significant\nmorbidity. The prevention of DFU may be achieved by the identification of\npatients at risk of DFU and the institution of preventative measures through\neducation and offloading. Several studies have reported that thermogram images\nmay help to detect an increase in plantar temperature prior to DFU. However,\nthe distribution of plantar temperature may be heterogeneous, making it\ndifficult to quantify and utilize to predict outcomes. We have compared a\nmachine learning-based scoring technique with feature selection and\noptimization techniques and learning classifiers to several state-of-the-art\nConvolutional Neural Networks (CNNs) on foot thermogram images and propose a\nrobust solution to identify the diabetic foot. A comparatively shallow CNN\nmodel, MobilenetV2 achieved an F1 score of ~95% for a two-feet thermogram\nimage-based classification and the AdaBoost Classifier used 10 features and\nachieved an F1 score of 97 %. A comparison of the inference time for the\nbest-performing networks confirmed that the proposed algorithm can be deployed\nas a smartphone application to allow the user to monitor the progression of the\nDFU in a home setting.",
          "link": "http://arxiv.org/abs/2106.14207",
          "publishedOn": "2021-06-29T01:55:14.524Z",
          "wordCount": 670,
          "title": "A Machine Learning Model for Early Detection of Diabetic Foot using Thermogram Images. (arXiv:2106.14207v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alimi_R/0/1/0/all/0/1\">Roger Alimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_E/0/1/0/all/0/1\">Elad Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_E/0/1/0/all/0/1\">Eyal Weiss</a>",
          "description": "Modern magnetic sensor arrays conventionally utilize state of the art low\npower magnetometers such as parallel and orthogonal fluxgates. Low power\nfluxgates tend to have large Barkhausen jumps that appear as a dc jump in the\nfluxgate output. This phenomenon deteriorates the signal fidelity and\neffectively increases the internal sensor noise. Even if sensors that are more\nprone to dc jumps can be screened during production, the conventional noise\nmeasurement does not always catch the dc jump because of its sparsity.\nMoreover, dc jumps persist in almost all the sensor cores although at a slower\nbut still intolerable rate. Even if dc jumps can be easily detected in a\nshielded environment, when deployed in presence of natural noise and clutter,\nit can be hard to positively detect them. This work fills this gap and presents\nalgorithms that distinguish dc jumps embedded in natural magnetic field data.\nTo improve robustness to noise, we developed two machine learning algorithms\nthat employ temporal and statistical physical-based features of a pre-acquired\nand well-known experimental data set. The first algorithm employs a support\nvector machine classifier, while the second is based on a neural network\narchitecture. We compare these new approaches to a more classical kernel-based\nmethod. To that purpose, the receiver operating characteristic curve is\ngenerated, which allows diagnosis ability of the different classifiers by\ncomparing their performances across various operation points. The accuracy of\nthe machine learning-based algorithms over the classic method is highly\nemphasized. In addition, high generalization and robustness of the neural\nnetwork can be concluded, based on the rapid convergence of the corresponding\nreceiver operating characteristic curves.",
          "link": "http://arxiv.org/abs/2106.14148",
          "publishedOn": "2021-06-29T01:55:14.518Z",
          "wordCount": 733,
          "title": "Machine Learning Detection Algorithm for Large Barkhausen Jumps in Cluttered Environment. (arXiv:2106.14148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_H/0/1/0/all/0/1\">Hitoshi Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komorita_S/0/1/0/all/0/1\">Satoshi Komorita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawanishi_Y/0/1/0/all/0/1\">Yasutomo Kawanishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murase_H/0/1/0/all/0/1\">Hiroshi Murase</a>",
          "description": "Multiple human tracking is a fundamental problem for scene understanding.\nAlthough both accuracy and speed are required in real-world applications,\nrecent tracking methods based on deep learning have focused on accuracy and\nrequire substantial running time. This study aims to improve running speed by\nperforming human detection at a certain frame interval because it accounts for\nmost of the running time. The question is how to maintain accuracy while\nskipping human detection. In this paper, we propose a method that complements\nthe detection results with optical flow, based on the fact that someone's\nappearance does not change much between adjacent frames. To maintain the\ntracking accuracy, we introduce robust interest point selection within human\nregions and a tracking termination metric calculated by the distribution of the\ninterest points. On the MOT20 dataset in the MOTChallenge, the proposed\nSDOF-Tracker achieved the best performance in terms of the total running speed\nwhile maintaining the MOTA metric. Our code is available at\nhttps://anonymous.4open.science/r/sdof-tracker-75AE.",
          "link": "http://arxiv.org/abs/2106.14259",
          "publishedOn": "2021-06-29T01:55:14.511Z",
          "wordCount": 601,
          "title": "SDOF-Tracker: Fast and Accurate Multiple Human Tracking by Skipped-Detection and Optical-Flow. (arXiv:2106.14259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zichang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1\">Chilam Cheang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lingwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>",
          "description": "We propose a method of Category-level 6D Object Pose and Size Estimation\n(COPSE) from a single depth image, without external pose-annotated real-world\ntraining data. While previous works exploit visual cues in RGB(D) images, our\nmethod makes inferences based on the rich geometric information of the object\nin the depth channel alone. Essentially, our framework explores such geometric\ninformation by learning the unified 3D Orientation-Consistent Representations\n(3D-OCR) module, and further enforced by the property of Geometry-constrained\nReflection Symmetry (GeoReS) module. The magnitude information of object size\nand the center point is finally estimated by Mirror-Paired Dimensional\nEstimation (MPDE) module. Extensive experiments on the category-level NOCS\nbenchmark demonstrate that our framework competes with state-of-the-art\napproaches that require labeled real-world images. We also deploy our approach\nto a physical Baxter robot to perform manipulation tasks on unseen but\ncategory-known instances, and the results further validate the efficacy of our\nproposed model. Our videos are available in the supplementary material.",
          "link": "http://arxiv.org/abs/2106.14193",
          "publishedOn": "2021-06-29T01:55:14.465Z",
          "wordCount": 607,
          "title": "DONet: Learning Category-Level 6D Object Pose and Size Estimation from Depth Observation. (arXiv:2106.14193v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bowen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhenfei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>",
          "description": "Face anti-spoofing (FAS) is an indispensable and widely used module in face\nrecognition systems. Although high accuracy has been achieved, a FAS system\nwill never be perfect due to the non-stationary applied environments and the\npotential emergence of new types of presentation attacks in real-world\napplications. In practice, given a handful of labeled samples from a new\ndeployment scenario (target domain) and abundant labeled face images in the\nexisting source domain, the FAS system is expected to perform well in the new\nscenario without sacrificing the performance on the original domain. To this\nend, we identify and address a more practical problem: Few-Shot Domain\nExpansion for Face Anti-Spoofing (FSDE-FAS). This problem is challenging since\nwith insufficient target domain training samples, the model may suffer from\nboth overfitting to the target domain and catastrophic forgetting of the source\ndomain. To address the problem, this paper proposes a Style transfer-based\nAugmentation for Semantic Alignment (SASA) framework. We propose to augment the\ntarget data by generating auxiliary samples based on photorealistic style\ntransfer. With the assistant of the augmented data, we further propose a\ncarefully designed mechanism to align different domains from both\ninstance-level and distribution-level, and then stabilize the performance on\nthe source domain with a less-forgetting constraint. Two benchmarks are\nproposed to simulate the FSDE-FAS scenarios, and the experimental results show\nthat the proposed SASA method outperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.14162",
          "publishedOn": "2021-06-29T01:55:14.458Z",
          "wordCount": 664,
          "title": "Few-Shot Domain Expansion for Face Anti-Spoofing. (arXiv:2106.14162v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Buyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>",
          "description": "We propose a novel method on refining cross-person gaze prediction task with\neye/face images only by explicitly modelling the person-specific differences.\nSpecifically, we first assume that we can obtain some initial gaze prediction\nresults with existing method, which we refer to as InitNet, and then introduce\nthree modules, the Validity Module (VM), Self-Calibration (SC) and\nPerson-specific Transform (PT)) Module. By predicting the reliability of\ncurrent eye/face images, our VM is able to identify invalid samples, e.g. eye\nblinking images, and reduce their effects in our modelling process. Our SC and\nPT module then learn to compensate for the differences on valid samples only.\nThe former models the translation offsets by bridging the gap between initial\npredictions and dataset-wise distribution. And the later learns more general\nperson-specific transformation by incorporating the information from existing\ninitial predictions of the same person. We validate our ideas on three publicly\navailable datasets, EVE, XGaze and MPIIGaze and demonstrate that our proposed\nmethod outperforms the SOTA methods significantly on all of them, e.g.\nrespectively 21.7%, 36.0% and 32.9% relative performance improvements. We won\nthe GAZE 2021 Competition on the EVE dataset. Our code can be found here\nhttps://github.com/bjj9/EVE_SCPT.",
          "link": "http://arxiv.org/abs/2106.14183",
          "publishedOn": "2021-06-29T01:55:14.396Z",
          "wordCount": 636,
          "title": "The Story in Your Eyes: An Individual-difference-aware Model for Cross-person Gaze Estimation. (arXiv:2106.14183v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>",
          "description": "Recently, transformer has achieved remarkable performance on a variety of\ncomputer vision applications. Compared with mainstream convolutional neural\nnetworks, vision transformers are often of sophisticated architectures for\nextracting powerful feature representations, which are more difficult to be\ndeveloped on mobile devices. In this paper, we present an effective\npost-training quantization algorithm for reducing the memory storage and\ncomputational costs of vision transformers. Basically, the quantization task\ncan be regarded as finding the optimal low-bit quantization intervals for\nweights and inputs, respectively. To preserve the functionality of the\nattention mechanism, we introduce a ranking loss into the conventional\nquantization objective that aims to keep the relative order of the\nself-attention results after quantization. Moreover, we thoroughly analyze the\nrelationship between quantization loss of different layers and the feature\ndiversity, and explore a mixed-precision quantization scheme by exploiting the\nnuclear norm of each attention map and output feature. The effectiveness of the\nproposed method is verified on several benchmark models and datasets, which\noutperforms the state-of-the-art post-training quantization algorithms. For\ninstance, we can obtain an 81.29\\% top-1 accuracy using DeiT-B model on\nImageNet dataset with about 8-bit quantization.",
          "link": "http://arxiv.org/abs/2106.14156",
          "publishedOn": "2021-06-29T01:55:14.390Z",
          "wordCount": 618,
          "title": "Post-Training Quantization for Vision Transformer. (arXiv:2106.14156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14324",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Weimin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhadra_S/0/1/0/all/0/1\">Sayantan Bhadra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brooks_F/0/1/0/all/0/1\">Frank J. Brooks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>",
          "description": "In order to objectively assess new medical imaging technologies via\ncomputer-simulations, it is important to account for all sources of variability\nthat contribute to image data. One important source of variability that can\nsignificantly limit observer performance is associated with the variability in\nthe ensemble of objects to-be-imaged. This source of variability can be\ndescribed by stochastic object models (SOMs), which are generative models that\ncan be employed to sample from a distribution of to-be-virtually-imaged\nobjects. It is generally desirable to establish SOMs from experimental imaging\nmeasurements acquired by use of a well-characterized imaging system, but this\ntask has remained challenging. Deep generative neural networks, such as\ngenerative adversarial networks (GANs) hold potential for such tasks. To\nestablish SOMs from imaging measurements, an AmbientGAN has been proposed that\naugments a GAN with a measurement operator. However, the original AmbientGAN\ncould not immediately benefit from modern training procedures and GAN\narchitectures, which limited its ability to be applied to realistically sized\nmedical image data. To circumvent this, in this work, a modified AmbientGAN\ntraining strategy is proposed that is suitable for modern progressive or\nmulti-resolution training approaches such as employed in the Progressive\nGrowing of GANs and Style-based GANs. AmbientGANs established by use of the\nproposed training procedure are systematically validated in a controlled way by\nuse of computer-simulated measurement data corresponding to a stylized imaging\nsystem. Finally, emulated single-coil experimental magnetic resonance imaging\ndata are employed to demonstrate the methods under less stylized conditions.",
          "link": "http://arxiv.org/abs/2106.14324",
          "publishedOn": "2021-06-29T01:55:14.384Z",
          "wordCount": 728,
          "title": "Learning stochastic object models from medical imaging measurements by use of advanced AmbientGANs. (arXiv:2106.14324v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Battal_A/0/1/0/all/0/1\">Abdullah F. Al-Battal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morton_T/0/1/0/all/0/1\">Timothy Morton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Chen Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+1_Y/0/1/0/all/0/1\">Yifeng Bu 1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_I/0/1/0/all/0/1\">Imanuel R Lerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhavan_R/0/1/0/all/0/1\">Radhika Madhavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Truong Q. Nguyen</a>",
          "description": "Ultrasound scanning is essential in several medical diagnostic and\ntherapeutic applications. It is used to visualize and analyze anatomical\nfeatures and structures that influence treatment plans. However, it is both\nlabor intensive, and its effectiveness is operator dependent. Real-time\naccurate and robust automatic detection and tracking of anatomical structures\nwhile scanning would significantly impact diagnostic and therapeutic procedures\nto be consistent and efficient. In this paper, we propose a deep learning\nframework to automatically detect and track a specific anatomical target\nstructure in ultrasound scans. Our framework is designed to be accurate and\nrobust across subjects and imaging devices, to operate in real-time, and to not\nrequire a large training set. It maintains a localization precision and recall\nhigher than 90% when trained on training sets that are as small as 20% in size\nof the original training set. The framework backbone is a weakly trained\nsegmentation neural network based on U-Net. We tested the framework on two\ndifferent ultrasound datasets with the aim to detect and track the Vagus nerve,\nwhere it outperformed current state-of-the-art real-time object detection\nnetworks.",
          "link": "http://arxiv.org/abs/2106.13849",
          "publishedOn": "2021-06-29T01:55:14.359Z",
          "wordCount": 671,
          "title": "A CNN Segmentation-Based Approach to Object Detection and Tracking in Ultrasound Scans with Application to the Vagus Nerve Detection. (arXiv:2106.13849v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (52.73 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data will be made available.",
          "link": "http://arxiv.org/abs/2106.14118",
          "publishedOn": "2021-06-29T01:55:14.344Z",
          "wordCount": 593,
          "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuo_H/0/1/0/all/0/1\">Hongya Tuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Z/0/1/0/all/0/1\">Zhongliang Jing</a>",
          "description": "Domain shift is a major challenge for object detectors to generalize well to\nreal world applications. Emerging techniques of domain adaptation for two-stage\ndetectors help to tackle this problem. However, two-stage detectors are not the\nfirst choice for industrial applications due to its long time consumption. In\nthis paper, a novel Domain Adaptive YOLO (DA-YOLO) is proposed to improve\ncross-domain performance for one-stage detectors. Image level features\nalignment is used to strictly match for local features like texture, and\nloosely match for global features like illumination. Multi-scale instance level\nfeatures alignment is presented to reduce instance domain shift effectively ,\nsuch as variations in object appearance and viewpoint. A consensus\nregularization to these domain classifiers is employed to help the network\ngenerate domain-invariant detections. We evaluate our proposed method on\npopular datasets like Cityscapes, KITTI, SIM10K and etc.. The results\ndemonstrate significant improvement when tested under different cross-domain\nscenarios.",
          "link": "http://arxiv.org/abs/2106.13939",
          "publishedOn": "2021-06-29T01:55:14.323Z",
          "wordCount": 582,
          "title": "Domain Adaptive YOLO for One-Stage Cross-Domain Detection. (arXiv:2106.13939v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14186",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ferla_M/0/1/0/all/0/1\">Michele La Ferla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Montebello_M/0/1/0/all/0/1\">Matthew Montebello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seychell_D/0/1/0/all/0/1\">Dylan Seychell</a>",
          "description": "During the last decade or so, there has been an insurgence in the deep\nlearning community to solve health-related issues, particularly breast cancer.\nFollowing the Camelyon-16 challenge in 2016, several researchers have dedicated\ntheir time to build Convolutional Neural Networks (CNNs) to help radiologists\nand other clinicians diagnose breast cancer. In particular, there has been an\nemphasis on Ductal Carcinoma in Situ (DCIS); the clinical term for early-stage\nbreast cancer. Large companies have given their fair share of research into\nthis subject, among these Google Deepmind who developed a model in 2020 that\nhas proven to be better than radiologists themselves to diagnose breast cancer\ncorrectly.\n\nWe found that among the issues which exist, there is a need for an\nexplanatory system that goes through the hidden layers of a CNN to highlight\nthose pixels that contributed to the classification of a mammogram. We then\nchose an open-source, reasonably successful project developed by Prof. Shen,\nusing the CBIS-DDSM image database to run our experiments on. It was later\nimproved using the Resnet-50 and VGG-16 patch-classifiers, analytically\ncomparing the outcome of both. The results showed that the Resnet-50 one\nconverged earlier in the experiments.\n\nFollowing the research by Montavon and Binder, we used the DeepTaylor\nLayer-wise Relevance Propagation (LRP) model to highlight those pixels and\nregions within a mammogram which contribute most to its classification. This is\nrepresented as a map of those pixels in the original image, which contribute to\nthe diagnosis and the extent to which they contribute to the final\nclassification. The most significant advantage of this algorithm is that it\nperforms exceptionally well with the Resnet-50 patch classifier architecture.",
          "link": "http://arxiv.org/abs/2106.14186",
          "publishedOn": "2021-06-29T01:55:14.277Z",
          "wordCount": 739,
          "title": "An XAI Approach to Deep Learning Models in the Detection of Ductal Carcinoma in Situ. (arXiv:2106.14186v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>",
          "description": "Label Smoothing (LS) improves model generalization through penalizing models\nfrom generating overconfident output distributions. For each training sample\nthe LS strategy smooths the one-hot encoded training signal by distributing its\ndistribution mass over the non-ground truth classes. We extend this technique\nby considering example pairs, coined PLS. PLS first creates midpoint samples by\naveraging random sample pairs and then learns a smoothing distribution during\ntraining for each of these midpoint samples, resulting in midpoints with high\nuncertainty labels for training. We empirically show that PLS significantly\noutperforms LS, achieving up to 30% of relative classification error reduction.\nWe also visualize that PLS produces very low winning softmax scores for both in\nand out of distribution samples.",
          "link": "http://arxiv.org/abs/2106.13913",
          "publishedOn": "2021-06-29T01:55:14.271Z",
          "wordCount": 566,
          "title": "Midpoint Regularization: from High Uncertainty Training to Conservative Classification. (arXiv:2106.13913v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>",
          "description": "The geodatabase (vectorized data) nowadays becomes a rather standard digital\ncity infrastructure; however, updating geodatabase efficiently and economically\nremains a fundamental and practical issue in the geospatial industry. The cost\nof building a geodatabase is extremely high and labor intensive, and very often\nthe maps we use have several months and even years of latency. One solution is\nto develop more automated methods for (vectorized) geospatial data generation,\nwhich has been proven a difficult task in the past decades. An alternative\nsolution is to first detect the differences between the new data and the\nexisting geospatial data, and then only update the area identified as changes.\nThe second approach is becoming more favored due to its high practicality and\nflexibility. A highly relevant technique is change detection. This article aims\nto provide an overview the state-of-the-art change detection methods in the\nfield of Remote Sensing and Geomatics to support the task of updating\ngeodatabases. Data used for change detection are highly disparate, we therefore\nstructure our review intuitively based on the dimension of the data, being 1)\nchange detection with 2D data; 2) change detection with 3D data. Conclusions\nwill be drawn based on the reviewed efforts in the field, and we will share our\noutlooks of the topic of updating geodatabases.",
          "link": "http://arxiv.org/abs/2106.14309",
          "publishedOn": "2021-06-29T01:55:14.242Z",
          "wordCount": 642,
          "title": "Change Detection for Geodatabase Updating. (arXiv:2106.14309v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Nningchuan Xiao</a>",
          "description": "The growing number of real-time camera feeds in urban areas has made it\npossible to provide high-quality traffic data for effective transportation\nplanning, operations, and management. However, deriving reliable traffic\nmetrics from these camera feeds has been a challenge due to the limitations of\ncurrent vehicle detection techniques, as well as the various camera conditions\nsuch as height and resolution. In this work, a quadtree based algorithm is\ndeveloped to continuously partition the image extent until only regions with\nhigh detection accuracy are remained. These regions are referred to as the\nhigh-accuracy identification regions (HAIR) in this paper. We demonstrate how\nthe use of the HAIR can improve the accuracy of traffic density estimates using\nimages from traffic cameras at different heights and resolutions in Central\nOhio. Our experiments show that the proposed algorithm can be used to derive\nrobust HAIR where vehicle detection accuracy is 41 percent higher than that in\nthe original image extent. The use of the HAIR also significantly improves the\ntraffic density estimation with an overall decrease of 49 percent in root mean\nsquared error.",
          "link": "http://arxiv.org/abs/2106.14049",
          "publishedOn": "2021-06-29T01:55:14.227Z",
          "wordCount": 639,
          "title": "Identifying High Accuracy Regions in Traffic Camera Images to Enhance the Estimation of Road Traffic Metrics: A Quadtree Based Method. (arXiv:2106.14049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1\">Joao Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibert_X/0/1/0/all/0/1\">Xavier Gibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianqiao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dar-Shyang Lee</a>",
          "description": "Learning guarantees often rely on assumptions of i.i.d. data, which will\nlikely be violated in practice once predictors are deployed to perform\nreal-world tasks. Domain adaptation approaches thus appeared as a useful\nframework yielding extra flexibility in that distinct train and test data\ndistributions are supported, provided that other assumptions are satisfied such\nas covariate shift, which expects the conditional distributions over labels to\nbe independent of the underlying data distribution. Several approaches were\nintroduced in order to induce generalization across varying train and test data\nsources, and those often rely on the general idea of domain-invariance, in such\na way that the data-generating distributions are to be disregarded by the\nprediction model. In this contribution, we tackle the problem of generalizing\nacross data sources by approaching it from the opposite direction: we consider\na conditional modeling approach in which predictions, in addition to being\ndependent on the input data, use information relative to the underlying\ndata-generating distribution. For instance, the model has an explicit mechanism\nto adapt to changing environments and/or new data sources. We argue that such\nan approach is more generally applicable than current domain adaptation methods\nsince it does not require extra assumptions such as covariate shift and further\nyields simpler training algorithms that avoid a common source of training\ninstabilities caused by minimax formulations, often employed in\ndomain-invariant methods.",
          "link": "http://arxiv.org/abs/2106.13899",
          "publishedOn": "2021-06-29T01:55:14.219Z",
          "wordCount": 673,
          "title": "Domain Conditional Predictors for Domain Adaptation. (arXiv:2106.13899v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>",
          "description": "Current vision and language tasks usually take complete visual data (e.g.,\nraw images or videos) as input, however, practical scenarios may often consist\nthe situations where part of the visual information becomes inaccessible due to\nvarious reasons e.g., restricted view with fixed camera or intentional vision\nblock for security concerns. As a step towards the more practical application\nscenarios, we introduce a novel task that aims to describe a video using the\nnatural language dialog between two agents as a supplementary information\nsource given incomplete visual data. Different from most existing\nvision-language tasks where AI systems have full access to images or video\nclips, which may reveal sensitive information such as recognizable human faces\nor voices, we intentionally limit the visual input for AI systems and seek a\nmore secure and transparent information medium, i.e., the natural language\ndialog, to supplement the missing visual information. Specifically, one of the\nintelligent agents - Q-BOT - is given two semantic segmented frames from the\nbeginning and the end of the video, as well as a finite number of opportunities\nto ask relevant natural language questions before describing the unseen video.\nA-BOT, the other agent who has access to the entire video, assists Q-BOT to\naccomplish the goal by answering the asked questions. We introduce two\ndifferent experimental settings with either a generative (i.e., agents generate\nquestions and answers freely) or a discriminative (i.e., agents select the\nquestions and answers from candidates) internal dialog generation process. With\nthe proposed unified QA-Cooperative networks, we experimentally demonstrate the\nknowledge transfer process between the two dialog agents and the effectiveness\nof using the natural language dialog as a supplement for incomplete implicit\nvisions.",
          "link": "http://arxiv.org/abs/2106.14069",
          "publishedOn": "2021-06-29T01:55:14.202Z",
          "wordCount": 722,
          "title": "Saying the Unseen: Video Descriptions via Dialog Agents. (arXiv:2106.14069v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Djeraba_C/0/1/0/all/0/1\">Chaabane Djeraba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedi_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Riedi</a>",
          "description": "This paper overviews two interdependent issues important for mining remote\nsensing data (e.g. images) obtained from atmospheric monitoring missions. The\nfirst issue relates the building new public datasets and benchmarks, which are\nhot priority of the remote sensing community. The second issue is the\ninvestigation of deep learning methodologies for atmospheric data\nclassification based on vast amount of data without annotations and with\nlocalized annotated data provided by sparse observing networks at the surface.\nThe targeted application is air quality assessment and prediction. Air quality\nis defined as the pollution level linked with several atmospheric constituents\nsuch as gases and aerosols. There are dependency relationships between the bad\nair quality, caused by air pollution, and the public health. The target\napplication is the development of a fast prediction model for local and\nregional air quality assessment and tracking. The results of mining data will\nhave significant implication for citizen and decision makers by providing a\nfast prediction and reliable air quality monitoring system able to cover the\nlocal and regional scale through intelligent extrapolation of sparse\nground-based in situ measurement networks.",
          "link": "http://arxiv.org/abs/2106.13992",
          "publishedOn": "2021-06-29T01:55:14.196Z",
          "wordCount": 606,
          "title": "Mining atmospheric data. (arXiv:2106.13992v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huafeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaixiong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengtao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>",
          "description": "Since human-labeled samples are free for the target set, unsupervised person\nre-identification (Re-ID) has attracted much attention in recent years, by\nadditionally exploiting the source set. However, due to the differences on\ncamera styles, illumination and backgrounds, there exists a large gap between\nsource domain and target domain, introducing a great challenge on cross-domain\nmatching. To tackle this problem, in this paper we propose a novel method named\nDual-stream Reciprocal Disentanglement Learning (DRDL), which is quite\nefficient in learning domain-invariant features. In DRDL, two encoders are\nfirst constructed for id-related and id-unrelated feature extractions, which\nare respectively measured by their associated classifiers. Furthermore,\nfollowed by an adversarial learning strategy, both streams reciprocally and\npositively effect each other, so that the id-related features and id-unrelated\nfeatures are completely disentangled from a given image, allowing the encoder\nto be powerful enough to obtain the discriminative but domain-invariant\nfeatures. In contrast to existing approaches, our proposed method is free from\nimage generation, which not only reduces the computational complexity\nremarkably, but also removes redundant information from id-related features.\nExtensive experiments substantiate the superiority of our proposed method\ncompared with the state-of-the-arts. The source code has been released in\nhttps://github.com/lhf12278/DRDL.",
          "link": "http://arxiv.org/abs/2106.13929",
          "publishedOn": "2021-06-29T01:55:14.191Z",
          "wordCount": 647,
          "title": "Dual-Stream Reciprocal Disentanglement Learning for Domain Adaption Person Re-Identification. (arXiv:2106.13929v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>",
          "description": "The raw-RGB colors of a camera sensor vary due to the spectral sensitivity\ndifferences across different sensor makes and models. This paper focuses on the\ntask of mapping between different sensor raw-RGB color spaces. Prior work\naddressed this problem using a pairwise calibration to achieve accurate color\nmapping. Although being accurate, this approach is less practical as it\nrequires: (1) capturing pair of images by both camera devices with a color\ncalibration object placed in each new scene; (2) accurate image alignment or\nmanual annotation of the color calibration object. This paper aims to tackle\ncolor mapping in the raw space through a more practical setup. Specifically, we\npresent a semi-supervised raw-to-raw mapping method trained on a small set of\npaired images alongside an unpaired set of images captured by each camera\ndevice. Through extensive experiments, we show that our method achieves better\nresults compared to other domain adaptation alternatives in addition to the\nsingle-calibration solution. We have generated a new dataset of raw images from\ntwo different smartphone cameras as part of this effort. Our dataset includes\nunpaired and paired sets for our semi-supervised training and evaluation.",
          "link": "http://arxiv.org/abs/2106.13883",
          "publishedOn": "2021-06-29T01:55:14.184Z",
          "wordCount": 617,
          "title": "Semi-Supervised Raw-to-Raw Mapping. (arXiv:2106.13883v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1\">Kai Qiao</a>",
          "description": "In this paper, we propose a novel encoder, called ShapeEditor, for\nhigh-resolution, realistic and high-fidelity face exchange. First of all, in\norder to ensure sufficient clarity and authenticity, our key idea is to use an\nadvanced pretrained high-quality random face image generator, i.e. StyleGAN, as\nbackbone. Secondly, we design ShapeEditor, a two-step encoder, to make the\nswapped face integrate the identity and attribute of the input faces. In the\nfirst step, we extract the identity vector of the source image and the\nattribute vector of the target image respectively; in the second step, we map\nthe concatenation of identity vector and attribute vector into the\n$\\mathcal{W+}$ potential space. In addition, for learning to map into the\nlatent space of StyleGAN, we propose a set of self-supervised loss functions\nwith which the training data do not need to be labeled manually. Extensive\nexperiments on the test dataset show that the results of our method not only\nhave a great advantage in clarity and authenticity than other state-of-the-art\nmethods, but also reflect the sufficient integration of identity and attribute.",
          "link": "http://arxiv.org/abs/2106.13984",
          "publishedOn": "2021-06-29T01:55:14.176Z",
          "wordCount": 616,
          "title": "ShapeEditer: a StyleGAN Encoder for Face Swapping. (arXiv:2106.13984v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yang-tian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao-zhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>",
          "description": "Pose transfer of human videos aims to generate a high fidelity video of a\ntarget person imitating actions of a source person. A few studies have made\ngreat progress either through image translation with deep latent features or\nneural rendering with explicit 3D features. However, both of them rely on large\namounts of training data to generate realistic results, and the performance\ndegrades on more accessible internet videos due to insufficient training\nframes. In this paper, we demonstrate that the dynamic details can be preserved\neven trained from short monocular videos. Overall, we propose a neural video\nrendering framework coupled with an image-translation-based dynamic details\ngeneration network (D2G-Net), which fully utilizes both the stability of\nexplicit 3D features and the capacity of learning components. To be specific, a\nnovel texture representation is presented to encode both the static and\npose-varying appearance characteristics, which is then mapped to the image\nspace and rendered as a detail-rich frame in the neural rendering stage.\nMoreover, we introduce a concise temporal loss in the training stage to\nsuppress the detail flickering that is made more visible due to high-quality\ndynamic details generated by our method. Through extensive comparisons, we\ndemonstrate that our neural human video renderer is capable of achieving both\nclearer dynamic details and more robust performance even on accessible short\nvideos with only 2k - 4k frames.",
          "link": "http://arxiv.org/abs/2106.14132",
          "publishedOn": "2021-06-29T01:55:14.149Z",
          "wordCount": 677,
          "title": "Robust Pose Transfer with Dynamic Details using Neural Video Rendering. (arXiv:2106.14132v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>",
          "description": "Despite the success of various text generation metrics such as BERTScore, it\nis still difficult to evaluate the image captions without enough reference\ncaptions due to the diversity of the descriptions. In this paper, we introduce\na new metric UMIC, an Unreferenced Metric for Image Captioning which does not\nrequire reference captions to evaluate image captions. Based on\nVision-and-Language BERT, we train UMIC to discriminate negative captions via\ncontrastive learning. Also, we observe critical problems of the previous\nbenchmark dataset (i.e., human annotations) on image captioning metric, and\nintroduce a new collection of human annotations on the generated captions. We\nvalidate UMIC on four datasets, including our new dataset, and show that UMIC\nhas a higher correlation than all previous metrics that require multiple\nreferences. We release the benchmark dataset and pre-trained models to compute\nthe UMIC.",
          "link": "http://arxiv.org/abs/2106.14019",
          "publishedOn": "2021-06-29T01:55:14.142Z",
          "wordCount": 585,
          "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning. (arXiv:2106.14019v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anukriti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kartikeya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sujit_P/0/1/0/all/0/1\">P.B. Sujit</a>",
          "description": "We present OffRoadTranSeg, the first end-to-end framework for semi-supervised\nsegmentation in unstructured outdoor environment using transformers and\nautomatic data selection for labelling. The offroad segmentation is a scene\nunderstanding approach that is widely used in autonomous driving. The popular\noffroad segmentation method is to use fully connected convolution layers and\nlarge labelled data, however, due to class imbalance, there will be several\nmismatches and also some classes may not be detected. Our approach is to do the\ntask of offroad segmentation in a semi-supervised manner. The aim is to provide\na model where self supervised vision transformer is used to fine-tune offroad\ndatasets with self-supervised data collection for labelling using depth\nestimation. The proposed method is validated on RELLIS-3D and RUGD offroad\ndatasets. The experiments show that OffRoadTranSeg outperformed other state of\nthe art models, and also solves the RELLIS-3D class imbalance problem.",
          "link": "http://arxiv.org/abs/2106.13963",
          "publishedOn": "2021-06-29T01:55:14.136Z",
          "wordCount": 581,
          "title": "OffRoadTranSeg: Semi-Supervised Segmentation using Transformers on OffRoad environments. (arXiv:2106.13963v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bendre_N/0/1/0/all/0/1\">Nihar Bendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_K/0/1/0/all/0/1\">Kevin Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1\">Peyman Najafirad</a>",
          "description": "With the ever-increasing amount of data, the central challenge in multimodal\nlearning involves limitations of labelled samples. For the task of\nclassification, techniques such as meta-learning, zero-shot learning, and\nfew-shot learning showcase the ability to learn information about novel classes\nbased on prior knowledge. Recent techniques try to learn a cross-modal mapping\nbetween the semantic space and the image space. However, they tend to ignore\nthe local and global semantic knowledge. To overcome this problem, we propose a\nMultimodal Variational Auto-Encoder (M-VAE) which can learn the shared latent\nspace of image features and the semantic space. In our approach we concatenate\nmultimodal data to a single embedding before passing it to the VAE for learning\nthe latent space. We propose the use of a multi-modal loss during the\nreconstruction of the feature embedding through the decoder. Our approach is\ncapable to correlating modalities and exploit the local and global semantic\nknowledge for novel sample predictions. Our experimental results using a MLP\nclassifier on four benchmark datasets show that our proposed model outperforms\nthe current state-of-the-art approaches for generalized zero-shot learning.",
          "link": "http://arxiv.org/abs/2106.14082",
          "publishedOn": "2021-06-29T01:55:14.121Z",
          "wordCount": 630,
          "title": "Generalized Zero-Shot Learning using Multimodal Variational Auto-Encoder with Semantic Concepts. (arXiv:2106.14082v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussien_M/0/1/0/all/0/1\">Mostafa Hussien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>",
          "description": "Image style transfer aims to manipulate the appearance of a source image, or\n\"content\" image, to share similar texture and colors of a target \"style\" image.\nIdeally, the style transfer manipulation should also preserve the semantic\ncontent of the source image. A commonly used approach to assist in transferring\nstyles is based on Gram matrix optimization. One problem of Gram matrix-based\noptimization is that it does not consider the correlation between colors and\ntheir styles. Specifically, certain textures or structures should be associated\nwith specific colors. This is particularly challenging when the target style\nimage exhibits multiple style types. In this work, we propose a color-aware\nmulti-style transfer method that generates aesthetically pleasing results while\npreserving the style-color correlation between style and generated images. We\nachieve this desired outcome by introducing a simple but efficient modification\nto classic Gram matrix-based style transfer optimization. A nice feature of our\nmethod is that it enables the users to manually select the color associations\nbetween the target style and content image for more transfer flexibility. We\nvalidated our method with several qualitative comparisons, including a user\nstudy conducted with 30 participants. In comparison with prior work, our method\nis simple, easy to implement, and achieves visually appealing results when\ntargeting images that have multiple styles. Source code is available at\nhttps://github.com/mahmoudnafifi/color-aware-style-transfer.",
          "link": "http://arxiv.org/abs/2106.13920",
          "publishedOn": "2021-06-29T01:55:14.116Z",
          "wordCount": 652,
          "title": "CAMS: Color-Aware Multi-Style Transfer. (arXiv:2106.13920v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_A/0/1/0/all/0/1\">Ang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>",
          "description": "As a core problem in computer vision, the performance of object detection has\nimproved drastically in the past few years. Despite their impressive\nperformance, object detectors suffer from a lack of interpretability.\nVisualization techniques have been developed and widely applied to introspect\nthe decisions made by other kinds of deep learning models; however, visualizing\nobject detectors has been underexplored. In this paper, we propose using\ninversion as a primary tool to understand modern object detectors and develop\nan optimization-based approach to layout inversion, allowing us to generate\nsynthetic images recognized by trained detectors as containing a desired\nconfiguration of objects. We reveal intriguing properties of detectors by\napplying our layout inversion technique to a variety of modern object\ndetectors, and further investigate them via validation experiments: they rely\non qualitatively different features for classification and regression; they\nlearn canonical motifs of commonly co-occurring objects; they use diff erent\nvisual cues to recognize objects of varying sizes. We hope our insights can\nhelp practitioners improve object detectors.",
          "link": "http://arxiv.org/abs/2106.13933",
          "publishedOn": "2021-06-29T01:55:14.109Z",
          "wordCount": 593,
          "title": "Inverting and Understanding Object Detectors. (arXiv:2106.13933v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S.M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-06-29T01:55:14.097Z",
          "wordCount": 608,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>",
          "description": "Emerging from low-level vision theory, steerable filters found their\ncounterpart in deep learning. Earlier works used the steering theorems and\npresented convolutional networks equivariant to rigid transformations. In our\nwork, we propose a steerable feed-forward learning-based approach that consists\nof spherical decision surfaces and operates on point clouds. Due to the\ninherent geometric 3D structure of our theory, we derive a 3D steerability\nconstraint for its atomic parts, the hypersphere neurons. Exploiting the\nrotational equivariance, we show how the model parameters are fully steerable\nat inference time. The proposed spherical filter banks enable to make\nequivariant and, after online optimization, invariant class predictions for\nknown synthetic point sets in unknown orientations.",
          "link": "http://arxiv.org/abs/2106.13863",
          "publishedOn": "2021-06-29T01:55:14.063Z",
          "wordCount": 543,
          "title": "Fully Steerable 3D Spherical Neurons. (arXiv:2106.13863v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaofeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Principal Component Analysis (PCA) has been widely used for dimensionality\nreduction and feature extraction. Robust PCA (RPCA), under different robust\ndistance metrics, such as l1-norm and l2, p-norm, can deal with noise or\noutliers to some extent. However, real-world data may display structures that\ncan not be fully captured by these simple functions. In addition, existing\nmethods treat complex and simple samples equally. By contrast, a learning\npattern typically adopted by human beings is to learn from simple to complex\nand less to more. Based on this principle, we propose a novel method called\nSelf-paced PCA (SPCA) to further reduce the effect of noise and outliers.\nNotably, the complexity of each sample is calculated at the beginning of each\niteration in order to integrate samples from simple to more complex into\ntraining. Based on an alternating optimization, SPCA finds an optimal\nprojection matrix and filters out outliers iteratively. Theoretical analysis is\npresented to show the rationality of SPCA. Extensive experiments on popular\ndata sets demonstrate that the proposed method can improve the state of-the-art\nresults considerably.",
          "link": "http://arxiv.org/abs/2106.13880",
          "publishedOn": "2021-06-29T01:55:14.057Z",
          "wordCount": 617,
          "title": "Self-paced Principal Component Analysis. (arXiv:2106.13880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nimi_S/0/1/0/all/0/1\">Sumaiya Tabassum Nimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arefeen_M/0/1/0/all/0/1\">Md Adnan Arefeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uddin_M/0/1/0/all/0/1\">Md Yusuf Sarwar Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yugyung Lee</a>",
          "description": "Collaborative inference enables resource-constrained edge devices to make\ninferences by uploading inputs (e.g., images) to a server (i.e., cloud) where\nthe heavy deep learning models run. While this setup works cost-effectively for\nsuccessful inferences, it severely underperforms when the model faces input\nsamples on which the model was not trained (known as Out-of-Distribution (OOD)\nsamples). If the edge devices could, at least, detect that an input sample is\nan OOD, that could potentially save communication and computation resources by\nnot uploading those inputs to the server for inference workload. In this paper,\nwe propose a novel lightweight OOD detection approach that mines important\nfeatures from the shallow layers of a pretrained CNN model and detects an input\nsample as ID (In-Distribution) or OOD based on a distance function defined on\nthe reduced feature space. Our technique (a) works on pretrained models without\nany retraining of those models, and (b) does not expose itself to any OOD\ndataset (all detection parameters are obtained from the ID training dataset).\nTo this end, we develop EARLIN (EARLy OOD detection for Collaborative\nINference) that takes a pretrained model and partitions the model at the OOD\ndetection layer and deploys the considerably small OOD part on an edge device\nand the rest on the cloud. By experimenting using real datasets and a prototype\nimplementation, we show that our technique achieves better results than other\napproaches in terms of overall accuracy and cost when tested against popular\nOOD datasets on top of popular deep learning models pretrained on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.13842",
          "publishedOn": "2021-06-29T01:55:14.050Z",
          "wordCount": 702,
          "title": "EARLIN: Early Out-of-Distribution Detection for Resource-efficient Collaborative Inference. (arXiv:2106.13842v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morgan_A/0/1/0/all/0/1\">Andrew S. Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bowen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junchi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boularias_A/0/1/0/all/0/1\">Abdeslam Boularias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollar_A/0/1/0/all/0/1\">Aaron M. Dollar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1\">Kostas Bekris</a>",
          "description": "Highly constrained manipulation tasks continue to be challenging for\nautonomous robots as they require high levels of precision, typically less than\n1mm, which is often incompatible with what can be achieved by traditional\nperception systems. This paper demonstrates that the combination of\nstate-of-the-art object tracking with passively adaptive mechanical hardware\ncan be leveraged to complete precision manipulation tasks with tight,\nindustrially-relevant tolerances (0.25mm). The proposed control method closes\nthe loop through vision by tracking the relative 6D pose of objects in the\nrelevant workspace. It adjusts the control reference of both the compliant\nmanipulator and the hand to complete object insertion tasks via within-hand\nmanipulation. Contrary to previous efforts for insertion, our method does not\nrequire expensive force sensors, precision manipulators, or time-consuming,\nonline learning, which is data hungry. Instead, this effort leverages\nmechanical compliance and utilizes an object agnostic manipulation model of the\nhand learned offline, off-the-shelf motion planning, and an RGBD-based object\ntracker trained solely with synthetic data. These features allow the proposed\nsystem to easily generalize and transfer to new tasks and environments. This\npaper describes in detail the system components and showcases its efficacy with\nextensive experiments involving tight tolerance peg-in-hole insertion tasks of\nvarious geometries as well as open-world constrained placement tasks.",
          "link": "http://arxiv.org/abs/2106.14070",
          "publishedOn": "2021-06-29T01:55:14.043Z",
          "wordCount": 658,
          "title": "Vision-driven Compliant Manipulation for Reliable, High-Precision Assembly Tasks. (arXiv:2106.14070v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-06-29T01:55:14.024Z",
          "wordCount": 628,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasileiou_V/0/1/0/all/0/1\">Vasiliki I. Vasileiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardaris_N/0/1/0/all/0/1\">Nikolaos Kardaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1\">Petros Maragos</a>",
          "description": "Nowadays, the interaction between humans and robots is constantly expanding,\nrequiring more and more human motion recognition applications to operate in\nreal time. However, most works on temporal action detection and recognition\nperform these tasks in offline manner, i.e. temporally segmented videos are\nclassified as a whole. In this paper, based on the recently proposed framework\nof Temporal Recurrent Networks, we explore how temporal context and human\nmovement dynamics can be effectively employed for online action detection. Our\napproach uses various state-of-the-art architectures and appropriately combines\nthe extracted features in order to improve action detection. We evaluate our\nmethod on a challenging but widely used dataset for temporal action\nlocalization, THUMOS'14. Our experiments show significant improvement over the\nbaseline method, achieving state-of-the art results on THUMOS'14.",
          "link": "http://arxiv.org/abs/2106.13967",
          "publishedOn": "2021-06-29T01:55:14.017Z",
          "wordCount": 579,
          "title": "Exploring Temporal Context and Human Movement Dynamics for Online Action Detection in Videos. (arXiv:2106.13967v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowdra_N/0/1/0/all/0/1\">Nidhi Gowdra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1\">Roopak Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDonell_S/0/1/0/all/0/1\">Stephen MacDonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wei Qi Yan</a>",
          "description": "Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and\nResNeXt-56 are severely over-parameterized, necessitating a consequent increase\nin the computational resources required for model training which scales\nexponentially for increments in model depth. In this paper, we propose an\nEntropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust\nand simple, yet effective in resolving the problem of over-parameterization\nwith regards to network depth of CNN model. The EBCLE heuristic employs a\npriori knowledge of the entropic data distribution of input datasets to\ndetermine an upper bound for convolutional network depth, beyond which identity\ntransformations are prevalent offering insignificant contributions for\nenhancing model performance. Restricting depth redundancies by forcing feature\ncompression and abstraction restricts over-parameterization while decreasing\ntraining time by 24.99% - 78.59% without degradation in model performance. We\npresent empirical evidence to emphasize the relative effectiveness of broader,\nyet shallower models trained using the EBCLE heuristic, which maintains or\noutperforms baseline classification accuracies of narrower yet deeper models.\nThe EBCLE heuristic is architecturally agnostic and EBCLE based CNN models\nrestrict depth redundancies resulting in enhanced utilization of the available\ncomputational resources. The proposed EBCLE heuristic is a compelling technique\nfor researchers to analytically justify their HyperParameter (HP) choices for\nCNNs. Empirical validation of the EBCLE heuristic in training CNN models was\nestablished on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10,\nMNIST) and four network architectures (DenseNet, ResNet, ResNeXt and\nEfficientNet B0-B2) with appropriate statistical tests employed to infer any\nconclusive claims presented in this paper.",
          "link": "http://arxiv.org/abs/2106.14190",
          "publishedOn": "2021-06-29T01:55:14.012Z",
          "wordCount": 721,
          "title": "Mitigating severe over-parameterization in deep convolutional neural networks through forced feature abstraction and compression with an entropy-based heuristic. (arXiv:2106.14190v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jo_C/0/1/0/all/0/1\">Changho Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Im_W/0/1/0/all/0/1\">Woobin Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sung-Eui Yoon</a>",
          "description": "In computer vision, recovering spatial information by filling in masked\nregions, e.g., inpainting, has been widely investigated for its usability and\nwide applicability to other various applications: image inpainting, image\nextrapolation, and environment map estimation. Most of them are studied\nseparately depending on the applications. Our focus, however, is on\naccommodating the opposite task, e.g., image outpainting, which would benefit\nthe target applications, e.g., image inpainting. Our self-supervision method,\nIn-N-Out, is summarized as a training approach that leverages the knowledge of\nthe opposite task into the target model. We empirically show that In-N-Out --\nwhich explores the complementary information -- effectively takes advantage\nover the traditional pipelines where only task-specific learning takes place in\ntraining. In experiments, we compare our method to the traditional procedure\nand analyze the effectiveness of our method on different applications: image\ninpainting, image extrapolation, and environment map estimation. For these\ntasks, we demonstrate that In-N-Out consistently improves the performance of\nthe recent works with In-N-Out self-supervision to their training procedure.\nAlso, we show that our approach achieves better results than an existing\ntraining approach for outpainting.",
          "link": "http://arxiv.org/abs/2106.13953",
          "publishedOn": "2021-06-29T01:55:14.003Z",
          "wordCount": 623,
          "title": "In-N-Out: Towards Good Initialization for Inpainting and Outpainting. (arXiv:2106.13953v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14033",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_T/0/1/0/all/0/1\">Tiange Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Dongnan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>",
          "description": "The recurrent mechanism has recently been introduced into U-Net in various\nmedical image segmentation tasks. Existing studies have focused on promoting\nnetwork recursion via reusing building blocks. Although network parameters\ncould be greatly saved, computational costs still increase inevitably in\naccordance with the pre-set iteration time. In this work, we study a\nmulti-scale upgrade of a bi-directional skip connected network and then\nautomatically discover an efficient architecture by a novel two-phase Neural\nArchitecture Search (NAS) algorithm, namely BiX-NAS. Our proposed method\nreduces the network computational cost by sifting out ineffective multi-scale\nfeatures at different levels and iterations. We evaluate BiX-NAS on two\nsegmentation tasks using three different medical image datasets, and the\nexperimental results show that our BiX-NAS searched architecture achieves the\nstate-of-the-art performance with significantly lower computational cost.",
          "link": "http://arxiv.org/abs/2106.14033",
          "publishedOn": "2021-06-29T01:55:13.988Z",
          "wordCount": 584,
          "title": "BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation. (arXiv:2106.14033v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhicheng Cai</a>",
          "description": "Traditionally, CNN models possess hierarchical structures and utilize the\nfeature mapping of the last layer to obtain the prediction output. However, it\ncan be difficulty to settle the optimal network depth and make the middle\nlayers learn distinguished features. This paper proposes the Interflow\nalgorithm specially for traditional CNN models. Interflow divides CNNs into\nseveral stages according to the depth and makes predictions by the feature\nmappings in each stage. Subsequently, we input these prediction branches into a\nwell-designed attention module, which learns the weights of these prediction\nbranches, aggregates them and obtains the final output. Interflow weights and\nfuses the features learned in both shallower and deeper layers, making the\nfeature information at each stage processed reasonably and effectively,\nenabling the middle layers to learn more distinguished features, and enhancing\nthe model representation ability. In addition, Interflow can alleviate gradient\nvanishing problem, lower the difficulty of network depth selection, and lighten\npossible over-fitting problem by introducing attention mechanism. Besides, it\ncan avoid network degradation as a byproduct. Compared with the original model,\nthe CNN model with Interflow achieves higher test accuracy on multiple\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2106.14073",
          "publishedOn": "2021-06-29T01:55:13.979Z",
          "wordCount": 621,
          "title": "Interflow: Aggregating Multi-layer Feature Mappings with Attention Mechanism. (arXiv:2106.14073v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>",
          "description": "Ensemble methods are generally regarded to be better than a single model if\nthe base learners are deemed to be \"accurate\" and \"diverse.\" Here we\ninvestigate a semi-supervised ensemble learning strategy to produce\ngeneralizable blind image quality assessment models. We train a multi-head\nconvolutional network for quality prediction by maximizing the accuracy of the\nensemble (as well as the base learners) on labeled data, and the disagreement\n(i.e., diversity) among them on unlabeled data, both implemented by the\nfidelity loss. We conduct extensive experiments to demonstrate the advantages\nof employing unlabeled data for BIQA, especially in model generalization and\nfailure identification.",
          "link": "http://arxiv.org/abs/2106.14008",
          "publishedOn": "2021-06-29T01:55:13.973Z",
          "wordCount": 552,
          "title": "Semi-Supervised Deep Ensembles for Blind Image Quality Assessment. (arXiv:2106.14008v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1\">Stephanie Tsuei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1\">Aditya Golatkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a method to estimate the uncertainty of the outcome of an image\nclassifier on a given input datum. Deep neural networks commonly used for image\nclassification are deterministic maps from an input image to an output class.\nAs such, their outcome on a given datum involves no uncertainty, so we must\nspecify what variability we are referring to when defining, measuring and\ninterpreting \"confidence.\" To this end, we introduce the Wellington Posterior,\nwhich is the distribution of outcomes that would have been obtained in response\nto data that could have been generated by the same scene that produced the\ngiven image. Since there are infinitely many scenes that could have generated\nthe given image, the Wellington Posterior requires induction from scenes other\nthan the one portrayed. We explore alternate methods using data augmentation,\nensembling, and model linearization. Additional alternatives include generative\nadversarial networks, conditional prior networks, and supervised single-view\nreconstruction. We test these alternatives against the empirical posterior\nobtained by inferring the class of temporally adjacent frames in a video. These\ndevelopments are only a small step towards assessing the reliability of deep\nnetwork classifiers in a manner that is compatible with safety-critical\napplications.",
          "link": "http://arxiv.org/abs/2106.13870",
          "publishedOn": "2021-06-29T01:55:13.960Z",
          "wordCount": 649,
          "title": "Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers. (arXiv:2106.13870v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thao_N/0/1/0/all/0/1\">Nguyen Hieu Thao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soloviev_O/0/1/0/all/0/1\">Oleg Soloviev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noom_J/0/1/0/all/0/1\">Jacques Noom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verhaegen_M/0/1/0/all/0/1\">Michel Verhaegen</a>",
          "description": "We propose and study the single-frame anisoplanatic deconvolution problem\nassociated with image classification using machine learning algorithms, named\nthe nonuniform defocus removal (NDR) problem. Mathematical analysis of the NDR\nproblem is done and the so-called defocus removal (DR) algorithm for solving it\nis proposed. Global convergence of the DR algorithm is established without\nimposing any unverifiable assumption. Numerical results on simulation data show\nsignificant features of DR including solvability, noise robustness,\nconvergence, model insensitivity and computational efficiency. Physical\nrelevance of the NDR problem and practicability of the DR algorithm are tested\non experimental data. Back to the application that originally motivated the\ninvestigation of the NDR problem, we show that the DR algorithm can improve the\naccuracy of classifying distorted images using convolutional neural networks.\nThe key difference of this paper compared to most existing works on\nsingle-frame anisoplanatic deconvolution is that the new method does not\nrequire the data image to be decomposable into isoplanatic subregions.\nTherefore, solution approaches partitioning the image into isoplanatic zones\nare not applicable to the NDR problem and those handling the entire image such\nas the DR algorithm need to be developed and analyzed.",
          "link": "http://arxiv.org/abs/2106.13864",
          "publishedOn": "2021-06-29T01:55:13.936Z",
          "wordCount": 637,
          "title": "Nonuniform Defocus Removal for Image Classification. (arXiv:2106.13864v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>",
          "description": "In this paper, we propose a spectral-spatial graph reasoning network (SSGRN)\nfor hyperspectral image (HSI) classification. Concretely, this network contains\ntwo parts that separately named spatial graph reasoning subnetwork (SAGRN) and\nspectral graph reasoning subnetwork (SEGRN) to capture the spatial and spectral\ngraph contexts, respectively. Different from the previous approaches\nimplementing superpixel segmentation on the original image or attempting to\nobtain the category features under the guide of label image, we perform the\nsuperpixel segmentation on intermediate features of the network to adaptively\nproduce the homogeneous regions to get the effective descriptors. Then, we\nadopt a similar idea in spectral part that reasonably aggregating the channels\nto generate spectral descriptors for spectral graph contexts capturing. All\ngraph reasoning procedures in SAGRN and SEGRN are achieved through graph\nconvolution. To guarantee the global perception ability of the proposed\nmethods, all adjacent matrices in graph reasoning are obtained with the help of\nnon-local self-attention mechanism. At last, by combining the extracted spatial\nand spectral graph contexts, we obtain the SSGRN to achieve a high accuracy\nclassification. Extensive quantitative and qualitative experiments on three\npublic HSI benchmarks demonstrate the competitiveness of the proposed methods\ncompared with other state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2106.13952",
          "publishedOn": "2021-06-29T01:55:13.930Z",
          "wordCount": 641,
          "title": "Spectral-Spatial Graph Reasoning Network for Hyperspectral Image Classification. (arXiv:2106.13952v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abbad_Z/0/1/0/all/0/1\">Zakariae Abbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maliani_A/0/1/0/all/0/1\">Ahmed Drissi El Maliani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaoui_S/0/1/0/all/0/1\">Said Ouatik El Alaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassouni_M/0/1/0/all/0/1\">Mohammed El Hassouni</a>",
          "description": "In this paper, we leverage the properties of non-Euclidean Geometry to define\nthe Geodesic distance (GD) on the space of statistical manifolds. The Geodesic\ndistance is a real and intuitive similarity measure that is a good alternative\nto the purely statistical and extensively used Kullback-Leibler divergence\n(KLD). Despite the effectiveness of the GD, a closed-form does not exist for\nmany manifolds, since the geodesic equations are hard to solve. This explains\nthat the major studies have been content to use numerical approximations.\nNevertheless, most of those do not take account of the manifold properties,\nwhich leads to a loss of information and thus to low performances. We propose\nan approximation of the Geodesic distance through a graph-based method. This\nlatter permits to well represent the structure of the statistical manifold, and\nrespects its geometrical properties. Our main aim is to compare the graph-based\napproximation to the state of the art approximations. Thus, the proposed\napproach is evaluated for two statistical manifolds, namely the Weibull\nmanifold and the Gamma manifold, considering the Content-Based Texture\nRetrieval application on different databases.",
          "link": "http://arxiv.org/abs/2106.14060",
          "publishedOn": "2021-06-29T01:55:13.923Z",
          "wordCount": 638,
          "title": "A Graph-based approach to derive the geodesic distance on Statistical manifolds: Application to Multimedia Information Retrieval. (arXiv:2106.14060v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13959",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterjee_A/0/1/0/all/0/1\">Avishek Chatterjee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mazumder_S/0/1/0/all/0/1\">Satyaki Mazumder</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Das_K/0/1/0/all/0/1\">Koel Das</a>",
          "description": "In recent times, functional data analysis (FDA) has been successfully applied\nin the field of high dimensional data classification. In this paper, we present\na novel classification framework using functional data and classwise Principal\nComponent Analysis (PCA). Our proposed method can be used in high dimensional\ntime series data which typically suffers from small sample size problem. Our\nmethod extracts a piece wise linear functional feature space and is\nparticularly suitable for hard classification problems.The proposed framework\nconverts time series data into functional data and uses classwise functional\nPCA for feature extraction followed by classification using a Bayesian linear\nclassifier. We demonstrate the efficacy of our proposed method by applying it\nto both synthetic data sets and real time series data from diverse fields\nincluding but not limited to neuroscience, food science, medical sciences and\nchemometrics.",
          "link": "http://arxiv.org/abs/2106.13959",
          "publishedOn": "2021-06-29T01:55:13.916Z",
          "wordCount": 577,
          "title": "Functional Classwise Principal Component Analysis: A Novel Classification Framework. (arXiv:2106.13959v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_R/0/1/0/all/0/1\">Riko Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanaka_H/0/1/0/all/0/1\">Hitomi Yanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mineshima_K/0/1/0/all/0/1\">Koji Mineshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekki_D/0/1/0/all/0/1\">Daisuke Bekki</a>",
          "description": "This paper introduces a new video-and-language dataset with human actions for\nmultimodal logical inference, which focuses on intentional and aspectual\nexpressions that describe dynamic human actions. The dataset consists of 200\nvideos, 5,554 action labels, and 1,942 action triplets of the form <subject,\npredicate, object> that can be translated into logical semantic\nrepresentations. The dataset is expected to be useful for evaluating multimodal\ninference systems between videos and semantically complicated sentences\nincluding negation and quantification.",
          "link": "http://arxiv.org/abs/2106.14137",
          "publishedOn": "2021-06-29T01:55:13.898Z",
          "wordCount": 522,
          "title": "Building a Video-and-Language Dataset with Human Actions for Multimodal Logical Inference. (arXiv:2106.14137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_A/0/1/0/all/0/1\">Arturo Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trullo_R/0/1/0/all/0/1\">Roger Trullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wielhorski_Y/0/1/0/all/0/1\">Yanneck Wielhorski</a>",
          "description": "In this work we propose a novel and fully automated method for extracting the\nyarn geometrical features in woven composites so that a direct parametrization\nof the textile reinforcement is achieved (e.g., FE mesh). Thus, our aim is not\nonly to perform yarn segmentation from tomographic images but rather to provide\na complete descriptive modeling of the fabric. As such, this direct approach\nimproves on previous methods that use voxel-wise masks as intermediate\nrepresentations followed by re-meshing operations (yarn envelope estimation).\nThe proposed approach employs two deep neural network architectures (U-Net and\nMask RCNN). First, we train the U-Net to generate synthetic CT images from the\ncorresponding FE simulations. This allows to generate large quantities of\nannotated data without requiring costly manual annotations. This data is then\nused to train the Mask R-CNN, which is focused on predicting contour points\naround each of the yarns in the image. Experimental results show that our\nmethod is accurate and robust for performing yarn instance segmentation on CT\nimages, this is further validated by quantitative and qualitative analyses.",
          "link": "http://arxiv.org/abs/2106.13982",
          "publishedOn": "2021-06-29T01:55:13.881Z",
          "wordCount": 630,
          "title": "Descriptive Modeling of Textiles using FE Simulations and Deep Learning. (arXiv:2106.13982v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cortinhal_T/0/1/0/all/0/1\">Tiago Cortinhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurnaz_F/0/1/0/all/0/1\">Fatih Kurnaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksoy_E/0/1/0/all/0/1\">Eren Aksoy</a>",
          "description": "In this work, we present a simple yet effective framework to address the\ndomain translation problem between different sensor modalities with unique data\nformats. By relying only on the semantics of the scene, our modular generative\nframework can, for the first time, synthesize a panoramic color image from a\ngiven full 3D LiDAR point cloud. The framework starts with semantic\nsegmentation of the point cloud, which is initially projected onto a spherical\nsurface. The same semantic segmentation is applied to the corresponding camera\nimage. Next, our new conditional generative model adversarially learns to\ntranslate the predicted LiDAR segment maps to the camera image counterparts.\nFinally, generated image segments are processed to render the panoramic scene\nimages. We provide a thorough quantitative evaluation on the SemanticKitti\ndataset and show that our proposed framework outperforms other strong baseline\nmodels.\n\nOur source code is available at\nhttps://github.com/halmstad-University/TITAN-NET",
          "link": "http://arxiv.org/abs/2106.13974",
          "publishedOn": "2021-06-29T01:55:13.874Z",
          "wordCount": 585,
          "title": "Semantics-aware Multi-modal Domain Translation:From LiDAR Point Clouds to Panoramic Color Images. (arXiv:2106.13974v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Durasov_N/0/1/0/all/0/1\">Nikita Durasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1\">Pierre Baque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>",
          "description": "Deep neural networks have amply demonstrated their prowess but estimating the\nreliability of their predictions remains challenging. Deep Ensembles are widely\nconsidered as being one of the best methods for generating uncertainty\nestimates but are very expensive to train and evaluate. MC-Dropout is another\npopular alternative, which is less expensive, but also less reliable. Our\ncentral intuition is that there is a continuous spectrum of ensemble-like\nmodels of which MC-Dropout and Deep Ensembles are extreme examples. The first\nuses an effectively infinite number of highly correlated models while the\nsecond relies on a finite number of independent models.\n\nTo combine the benefits of both, we introduce Masksembles. Instead of\nrandomly dropping parts of the network as in MC-dropout, Masksemble relies on a\nfixed number of binary masks, which are parameterized in a way that allows to\nchange correlations between individual models. Namely, by controlling the\noverlap between the masks and their density one can choose the optimal\nconfiguration for the task at hand. This leads to a simple and easy to\nimplement method with performance on par with Ensembles at a fraction of the\ncost. We experimentally validate Masksembles on two widely used datasets,\nCIFAR10 and ImageNet.",
          "link": "http://arxiv.org/abs/2012.08334",
          "publishedOn": "2021-06-28T01:57:56.209Z",
          "wordCount": 672,
          "title": "Masksembles for Uncertainty Estimation. (arXiv:2012.08334v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.11752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Ting-Wu Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1\">Ari S. Morcos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1\">Diana Marculescu</a>",
          "description": "Slimmable neural networks provide a flexible trade-off front between\nprediction error and computational requirement (such as the number of\nfloating-point operations or FLOPs) with the same storage requirement as a\nsingle model. They are useful for reducing maintenance overhead for deploying\nmodels to devices with different memory constraints and are useful for\noptimizing the efficiency of a system with many CNNs. However, existing\nslimmable network approaches either do not optimize layer-wise widths or\noptimize the shared-weights and layer-wise widths independently, thereby\nleaving significant room for improvement by joint width and weight\noptimization. In this work, we propose a general framework to enable joint\noptimization for both width configurations and weights of slimmable networks.\nOur framework subsumes conventional and NAS-based slimmable methods as special\ncases and provides flexibility to improve over existing methods. From a\npractical standpoint, we propose Joslim, an algorithm that jointly optimizes\nboth the widths and weights for slimmable nets, which outperforms existing\nmethods for optimizing slimmable networks across various networks, datasets,\nand objectives. Quantitatively, improvements up to 1.7% and 8% in top-1\naccuracy on the ImageNet dataset can be attained for MobileNetV2 considering\nFLOPs and memory footprint, respectively. Our results highlight the potential\nof optimizing the channel counts for different layers jointly with the weights\nfor slimmable networks. Code available at https://github.com/cmu-enyac/Joslim.",
          "link": "http://arxiv.org/abs/2007.11752",
          "publishedOn": "2021-06-28T01:57:56.052Z",
          "wordCount": 730,
          "title": "Joslim: Joint Widths and Weights Optimization for Slimmable Neural Networks. (arXiv:2007.11752v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.00143",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Welk_M/0/1/0/all/0/1\">Martin Welk</a>",
          "description": "Having been studied since long by statisticians, multivariate median concepts\nfound their way into the image processing literature in the course of the last\ndecades, being used to construct robust and efficient denoising filters for\nmultivariate images such as colour images but also matrix-valued images. Based\non the similarities between image and geometric data as results of the sampling\nof continuous physical quantities, it can be expected that the understanding of\nmultivariate median filters for images provides a starting point for the\ndevelopment of shape processing techniques. This paper presents an overview of\nmultivariate median concepts relevant for image and shape processing. It\nfocusses on their mathematical principles and discusses important properties\nespecially in the context of image processing.",
          "link": "http://arxiv.org/abs/1911.00143",
          "publishedOn": "2021-06-28T01:57:55.954Z",
          "wordCount": 589,
          "title": "Multivariate Medians for Image and Shape Analysis. (arXiv:1911.00143v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balbastre_Y/0/1/0/all/0/1\">Yael Balbastre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brudfors_M/0/1/0/all/0/1\">Mikael Brudfors</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azzarito_M/0/1/0/all/0/1\">Michela Azzarito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambert_C/0/1/0/all/0/1\">Christian Lambert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callaghan_M/0/1/0/all/0/1\">Martina F. Callaghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashburner_J/0/1/0/all/0/1\">John Ashburner</a>",
          "description": "Quantitative MR imaging is increasingly favoured for its richer information\ncontent and standardised measures. However, computing quantitative parameter\nmaps, such as those encoding longitudinal relaxation rate (R1), apparent\ntransverse relaxation rate (R2*) or magnetisation-transfer saturation (MTsat),\ninvolves inverting a highly non-linear function. Many methods for deriving\nparameter maps assume perfect measurements and do not consider how noise is\npropagated through the estimation procedure, resulting in needlessly noisy\nmaps. Instead, we propose a probabilistic generative (forward) model of the\nentire dataset, which is formulated and inverted to jointly recover (log)\nparameter maps with a well-defined probabilistic interpretation (e.g., maximum\nlikelihood or maximum a posteriori). The second order optimisation we propose\nfor model fitting achieves rapid and stable convergence thanks to a novel\napproximate Hessian. We demonstrate the utility of our flexible framework in\nthe context of recovering more accurate maps from data acquired using the\npopular multi-parameter mapping protocol. We also show how to incorporate a\njoint total variation prior to further decrease the noise in the maps, noting\nthat the probabilistic formulation allows the uncertainty on the recovered\nparameter maps to be estimated. Our implementation uses a PyTorch backend and\nbenefits from GPU acceleration. It is available at\nhttps://github.com/balbasty/nitorch.",
          "link": "http://arxiv.org/abs/2102.01604",
          "publishedOn": "2021-06-28T01:57:55.936Z",
          "wordCount": 680,
          "title": "Model-based multi-parameter mapping. (arXiv:2102.01604v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_V/0/1/0/all/0/1\">Veronika A. Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>",
          "description": "Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly\nused to visualize and quantify left atrial (LA) scars. The position and extent\nof scars provide important information of the pathophysiology and progression\nof atrial fibrillation (AF). Hence, LA scar segmentation and quantification\nfrom LGE MRI can be useful in computer-assisted diagnosis and treatment\nstratification of AF patients. Since manual delineation can be time-consuming\nand subject to intra- and inter-expert variability, automating this computing\nis highly desired, which nevertheless is still challenging and\nunder-researched.\n\nThis paper aims to provide a systematic review on computing methods for LA\ncavity, wall, scar and ablation gap segmentation and quantification from LGE\nMRI, and the related literature for AF studies. Specifically, we first\nsummarize AF-related imaging techniques, particularly LGE MRI. Then, we review\nthe methodologies of the four computing tasks in detail, and summarize the\nvalidation strategies applied in each task. Finally, the possible future\ndevelopments are outlined, with a brief survey on the potential clinical\napplications of the aforementioned methods. The review shows that the research\ninto this topic is still in early stages. Although several methods have been\nproposed, especially for LA segmentation, there is still large scope for\nfurther algorithmic developments due to performance issues related to the high\nvariability of enhancement appearance and differences in image acquisition.",
          "link": "http://arxiv.org/abs/2106.09862",
          "publishedOn": "2021-06-28T01:57:55.764Z",
          "wordCount": 686,
          "title": "Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation Studies: A Review. (arXiv:2106.09862v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Transformer in computer vision has recently shown encouraging progress. In\nthis work, we improve the original Pyramid Vision Transformer (PVTv1) by adding\nthree improvement designs, which include (1) locally continuous features with\nconvolutions, (2) position encodings with zero paddings, and (3) linear\ncomplexity attention layers with average pooling. With these simple\nmodifications, our PVTv2 significantly improves PVTv1 on classification,\ndetection, and segmentation. Moreover, PVTv2 achieves much better performance\nthan recent works, including Swin Transformer, under ImageNet-1K pre-training.\nWe hope this work will make state-of-the-art vision Transformer research more\naccessible. Code is available at https://github.com/whai362/PVT .",
          "link": "http://arxiv.org/abs/2106.13797",
          "publishedOn": "2021-06-28T01:57:55.715Z",
          "wordCount": 543,
          "title": "PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neff_T/0/1/0/all/0/1\">Thomas Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stadlbauer_P/0/1/0/all/0/1\">Pascal Stadlbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parger_M/0/1/0/all/0/1\">Mathias Parger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_A/0/1/0/all/0/1\">Andreas Kurz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Joerg H. Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaitanya_C/0/1/0/all/0/1\">Chakravarty R. Alla Chaitanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplanyan_A/0/1/0/all/0/1\">Anton Kaplanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinberger_M/0/1/0/all/0/1\">Markus Steinberger</a>",
          "description": "The recent research explosion around implicit neural representations, such as\nNeRF, shows that there is immense potential for implicitly storing high-quality\nscene and lighting information in compact neural networks. However, one major\nlimitation preventing the use of NeRF in real-time rendering applications is\nthe prohibitive computational cost of excessive network evaluations along each\nview ray, requiring dozens of petaFLOPS. In this work, we bring compact neural\nrepresentations closer to practical rendering of synthetic content in real-time\napplications, such as games and virtual reality. We show that the number of\nsamples required for each view ray can be significantly reduced when samples\nare placed around surfaces in the scene without compromising image quality. To\nthis end, we propose a depth oracle network that predicts ray sample locations\nfor each view ray with a single network evaluation. We show that using a\nclassification network around logarithmically discretized and spherically\nwarped depth values is essential to encode surface locations rather than\ndirectly estimating depth. The combination of these techniques leads to DONeRF,\nour compact dual network design with a depth oracle network as its first step\nand a locally sampled shading network for ray accumulation. With DONeRF, we\nreduce the inference costs by up to 48x compared to NeRF when conditioning on\navailable ground truth depth information. Compared to concurrent acceleration\nmethods for raymarching-based neural representations, DONeRF does not require\nadditional memory for explicit caching or acceleration structures, and can\nrender interactively (20 frames per second) on a single GPU.",
          "link": "http://arxiv.org/abs/2103.03231",
          "publishedOn": "2021-06-28T01:57:55.677Z",
          "wordCount": 766,
          "title": "DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. (arXiv:2103.03231v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.09734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Na Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yuan-Hai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huajun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu-Ting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Ling-Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiu_N/0/1/0/all/0/1\">Naihua Xiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Nai-Yang Deng</a>",
          "description": "Considering the classification problem, we summarize the nonparallel support\nvector machines with the nonparallel hyperplanes to two types of frameworks.\nThe first type constructs the hyperplanes separately. It solves a series of\nsmall optimization problems to obtain a series of hyperplanes, but is hard to\nmeasure the loss of each sample. The other type constructs all the hyperplanes\nsimultaneously, and it solves one big optimization problem with the ascertained\nloss of each sample. We give the characteristics of each framework and compare\nthem carefully. In addition, based on the second framework, we construct a\nmax-min distance-based nonparallel support vector machine for multiclass\nclassification problem, called NSVM. It constructs hyperplanes with large\ndistance margin by solving an optimization problem. Experimental results on\nbenchmark data sets show the advantages of our NSVM.",
          "link": "http://arxiv.org/abs/1910.09734",
          "publishedOn": "2021-06-28T01:57:55.627Z",
          "wordCount": 616,
          "title": "Single and Union Non-parallel Support Vector Machine Frameworks. (arXiv:1910.09734v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08886",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Pengfei Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1\">Puyang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1\">Jinyuan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_S/0/1/0/all/0/1\">Shanshan Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Reconstructing magnetic resonance (MR) images from undersampled data is a\nchallenging problem due to various artifacts introduced by the under-sampling\noperation. Recent deep learning-based methods for MR image reconstruction\nusually leverage a generic auto-encoder architecture which captures low-level\nfeatures at the initial layers and high-level features at the deeper layers.\nSuch networks focus much on global features which may not be optimal to\nreconstruct the fully-sampled image. In this paper, we propose an\nOver-and-Under Complete Convolutional Recurrent Neural Network (OUCR), which\nconsists of an overcomplete and an undercomplete Convolutional Recurrent Neural\nNetwork(CRNN). The overcomplete branch gives special attention in learning\nlocal structures by restraining the receptive field of the network. Combining\nit with the undercomplete branch leads to a network which focuses more on\nlow-level features without losing out on the global structures. Extensive\nexperiments on two datasets demonstrate that the proposed method achieves\nsignificant improvements over the compressed sensing and popular deep\nlearning-based methods with less number of trainable parameters.",
          "link": "http://arxiv.org/abs/2106.08886",
          "publishedOn": "2021-06-28T01:57:55.574Z",
          "wordCount": 632,
          "title": "Over-and-Under Complete Convolutional RNN for MRI Reconstruction. (arXiv:2106.08886v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12525",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1\">Varun A. Kelkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>",
          "description": "Obtaining a useful estimate of an object from highly incomplete imaging\nmeasurements remains a holy grail of imaging science. Deep learning methods\nhave shown promise in learning object priors or constraints to improve the\nconditioning of an ill-posed imaging inverse problem. In this study, a\nframework for estimating an object of interest that is semantically related to\na known prior image, is proposed. An optimization problem is formulated in the\ndisentangled latent space of a style-based generative model, and semantically\nmeaningful constraints are imposed using the disentangled latent representation\nof the prior image. Stable recovery from incomplete measurements with the help\nof a prior image is theoretically analyzed. Numerical experiments demonstrating\nthe superior performance of our approach as compared to related methods are\npresented.",
          "link": "http://arxiv.org/abs/2102.12525",
          "publishedOn": "2021-06-28T01:57:55.566Z",
          "wordCount": 603,
          "title": "Prior Image-Constrained Reconstruction using Style-Based Generative Models. (arXiv:2102.12525v2 [eess.IV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_N/0/1/0/all/0/1\">Navdeep Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Sadegh R Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Si-Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yezzi_A/0/1/0/all/0/1\">Anthony Yezzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>",
          "description": "In current clinical practice, noisy and artifact-ridden weekly cone-beam\ncomputed tomography (CBCT) images are only used for patient setup during\nradiotherapy. Treatment planning is done once at the beginning of the treatment\nusing high-quality planning CT (pCT) images and manual contours for\norgans-at-risk (OARs) structures. If the quality of the weekly CBCT images can\nbe improved while simultaneously segmenting OAR structures, this can provide\ncritical information for adapting radiotherapy mid-treatment as well as for\nderiving biomarkers for treatment response. Using a novel physics-based data\naugmentation strategy, we synthesize a large dataset of perfectly/inherently\nregistered planning CT and synthetic-CBCT pairs for locally advanced lung\ncancer patient cohort, which are then used in a multitask 3D deep learning\nframework to simultaneously segment and translate real weekly CBCT images to\nhigh-quality planning CT-like images. We compared the synthetic CT and OAR\nsegmentations generated by the model to real planning CT and manual OAR\nsegmentations and showed promising results. The real week 1 (baseline) CBCT\nimages which had an average MAE of 162.77 HU compared to pCT images are\ntranslated to synthetic CT images that exhibit a drastically improved average\nMAE of 29.31 HU and average structural similarity of 92% with the pCT images.\nThe average DICE scores of the 3D organs-at-risk segmentations are: lungs 0.96,\nheart 0.88, spinal cord 0.83 and esophagus 0.66. This approach could allow\nclinicians to adjust treatment plans using only the routine low-quality CBCT\nimages, potentially improving patient outcomes. Our code, data, and pre-trained\nmodels will be made available via our physics-based data augmentation library,\nPhysics-ArX, at https://github.com/nadeemlab/Physics-ArX.",
          "link": "http://arxiv.org/abs/2103.05690",
          "publishedOn": "2021-06-28T01:57:55.546Z",
          "wordCount": 750,
          "title": "Multitask 3D CBCT-to-CT Translation and Organs-at-Risk Segmentation Using Physics-Based Data Augmentation. (arXiv:2103.05690v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bron_E/0/1/0/all/0/1\">Esther Bron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessen_W/0/1/0/all/0/1\">Wiro Niessen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolvius_E/0/1/0/all/0/1\">Eppo Wolvius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roshchupkin_G/0/1/0/all/0/1\">Gennady Roshchupkin</a>",
          "description": "Confounding bias is a crucial problem when applying machine learning to\npractice, especially in clinical practice. We consider the problem of learning\nrepresentations independent to multiple biases. In literature, this is mostly\nsolved by purging the bias information from learned representations. We however\nexpect this strategy to harm the diversity of information in the\nrepresentation, and thus limiting its prospective usage (e.g., interpretation).\nTherefore, we propose to mitigate the bias while keeping almost all information\nin the latent representations, which enables us to observe and interpret them\nas well. To achieve this, we project latent features onto a learned vector\ndirection, and enforce the independence between biases and projected features\nrather than all learned features. To interpret the mapping between projected\nfeatures and input data, we propose projection-wise disentangling: a sampling\nand reconstruction along the learned vector direction. The proposed method was\nevaluated on the analysis of 3D facial shape and patient characteristics\n(N=5011). Experiments showed that this conceptually simple method achieved\nstate-of-the-art fair prediction performance and interpretability, showing its\ngreat potential for clinical applications.",
          "link": "http://arxiv.org/abs/2106.13734",
          "publishedOn": "2021-06-28T01:57:55.532Z",
          "wordCount": 630,
          "title": "Projection-wise Disentangling for Fair and Interpretable Representation Learning: Application to 3D Facial Shape Analysis. (arXiv:2106.13734v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reimann_M/0/1/0/all/0/1\">Max Reimann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchheim_B/0/1/0/all/0/1\">Benito Buchheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semmo_A/0/1/0/all/0/1\">Amir Semmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollner_J/0/1/0/all/0/1\">J&#xfc;rgen D&#xf6;llner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1\">Matthias Trapp</a>",
          "description": "We present StyleTune, a mobile app for interactive multi-level control of\nneural style transfers that facilitates creative adjustments of style elements\nand enables high output fidelity. In contrast to current mobile neural style\ntransfer apps, StyleTune supports users to adjust both the size and orientation\nof style elements, such as brushstrokes and texture patches, on a global as\nwell as local level. To this end, we propose a novel stroke-adaptive\nfeed-forward style transfer network, that enables control over stroke size and\nintensity and allows a larger range of edits than current approaches. For\nadditional level-of-control, we propose a network agnostic method for\nstroke-orientation adjustment by utilizing the rotation-variance of CNNs. To\nachieve high output fidelity, we further add a patch-based style transfer\nmethod that enables users to obtain output resolutions of more than 20\nMegapixel. Our approach empowers users to create many novel results that are\nnot possible with current mobile neural style transfer apps.",
          "link": "http://arxiv.org/abs/2106.13787",
          "publishedOn": "2021-06-28T01:57:55.447Z",
          "wordCount": 598,
          "title": "Interactive Multi-level Stroke Control for Neural Style Transfer. (arXiv:2106.13787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tszhang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_K/0/1/0/all/0/1\">Kun Bai</a>",
          "description": "Deep Convolutional Neural Networks (DCNNs) and their variants have been\nwidely used in large scale face recognition(FR) recently. Existing methods have\nachieved good performance on many FR benchmarks. However, most of them suffer\nfrom two major problems. First, these methods converge quite slowly since they\noptimize the loss functions in a high-dimensional and sparse Gaussian Sphere.\nSecond, the high dimensionality of features, despite the powerful descriptive\nability, brings difficulty to the optimization, which may lead to a sub-optimal\nlocal optimum. To address these problems, we propose a simple yet efficient\ntraining mechanism called MultiFace, where we approximate the original\nhigh-dimensional features by the ensemble of low-dimensional features. The\nproposed mechanism is also generic and can be easily applied to many advanced\nFR models. Moreover, it brings the benefits of good interpretability to FR\nmodels via the clustering effect. In detail, the ensemble of these\nlow-dimensional features can capture complementary yet discriminative\ninformation, which can increase the intra-class compactness and inter-class\nseparability. Experimental results show that the proposed mechanism can\naccelerate 2-3 times with the softmax loss and 1.2-1.5 times with Arcface or\nCosface, while achieving state-of-the-art performances in several benchmark\ndatasets. Especially, the significant improvements on large-scale\ndatasets(e.g., IJB and MageFace) demonstrate the flexibility of our new\ntraining mechanism.",
          "link": "http://arxiv.org/abs/2101.09899",
          "publishedOn": "2021-06-28T01:57:55.435Z",
          "wordCount": 693,
          "title": "MultiFace: A Generic Training Mechanism for Boosting Face Recognition Performance. (arXiv:2101.09899v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13689",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wahab_N/0/1/0/all/0/1\">Noorul Wahab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miligy_I/0/1/0/all/0/1\">Islam M Miligy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dodd_K/0/1/0/all/0/1\">Katherine Dodd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahota_H/0/1/0/all/0/1\">Harvir Sahota</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toss_M/0/1/0/all/0/1\">Michael Toss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1\">Wenqi Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bilal_M/0/1/0/all/0/1\">Mohsin Bilal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1\">Simon Graham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Young Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hadjigeorghiou_G/0/1/0/all/0/1\">Giorgos Hadjigeorghiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhalerao_A/0/1/0/all/0/1\">Abhir Bhalerao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lashen_A/0/1/0/all/0/1\">Ayat Lashen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ibrahim_A/0/1/0/all/0/1\">Asmaa Ibrahim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katayama_A/0/1/0/all/0/1\">Ayaka Katayama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebili_H/0/1/0/all/0/1\">Henry O Ebili</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parkin_M/0/1/0/all/0/1\">Matthew Parkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sorell_T/0/1/0/all/0/1\">Tom Sorell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hero_E/0/1/0/all/0/1\">Emily Hero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldaly_H/0/1/0/all/0/1\">Hesham Eldaly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsang_Y/0/1/0/all/0/1\">Yee Wah Tsang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Kishore Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Snead_D/0/1/0/all/0/1\">David Snead</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rakha_E/0/1/0/all/0/1\">Emad Rakha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz Minhas</a>",
          "description": "Recent advances in whole slide imaging (WSI) technology have led to the\ndevelopment of a myriad of computer vision and artificial intelligence (AI)\nbased diagnostic, prognostic, and predictive algorithms. Computational\nPathology (CPath) offers an integrated solution to utilize information embedded\nin pathology WSIs beyond what we obtain through visual assessment. For\nautomated analysis of WSIs and validation of machine learning (ML) models,\nannotations at the slide, tissue and cellular levels are required. The\nannotation of important visual constructs in pathology images is an important\ncomponent of CPath projects. Improper annotations can result in algorithms\nwhich are hard to interpret and can potentially produce inaccurate and\ninconsistent results. Despite the crucial role of annotations in CPath\nprojects, there are no well-defined guidelines or best practices on how\nannotations should be carried out. In this paper, we address this shortcoming\nby presenting the experience and best practices acquired during the execution\nof a large-scale annotation exercise involving a multidisciplinary team of\npathologists, ML experts and researchers as part of the Pathology image data\nLake for Analytics, Knowledge and Education (PathLAKE) consortium. We present a\nreal-world case study along with examples of different types of annotations,\ndiagnostic algorithm, annotation data dictionary and annotation constructs. The\nanalyses reported in this work highlight best practice recommendations that can\nbe used as annotation guidelines over the lifecycle of a CPath project.",
          "link": "http://arxiv.org/abs/2106.13689",
          "publishedOn": "2021-06-28T01:57:55.427Z",
          "wordCount": 729,
          "title": "Semantic annotation for computational pathology: Multidisciplinary experience and best practice recommendations. (arXiv:2106.13689v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandivarapu_J/0/1/0/all/0/1\">Jaya Krishna Mandivarapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bunch_E/0/1/0/all/0/1\">Eric Bunch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1\">Qian You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1\">Glenn Fung</a>",
          "description": "Document image classification remains a popular research area because it can\nbe commercialized in many enterprise applications across different industries.\nRecent advancements in large pre-trained computer vision and language models\nand graph neural networks has lent document image classification many tools.\nHowever using large pre-trained models usually requires substantial computing\nresources which could defeat the cost-saving advantages of automatic document\nimage classification. In the paper we propose an efficient document image\nclassification framework that uses graph convolution neural networks and\nincorporates textual, visual and layout information of the document. We have\nrigorously benchmarked our proposed algorithm against several state-of-art\nvision and language models on both publicly available dataset and a real-life\ninsurance document classification dataset. Empirical results on both publicly\navailable and real-world data show that our methods achieve near SOTA\nperformance yet require much less computing resources and time for model\ntraining and inference. This results in solutions than offer better cost\nadvantages, especially in scalable deployment for enterprise applications. The\nresults showed that our algorithm can achieve classification performance quite\nclose to SOTA. We also provide comprehensive comparisons of computing\nresources, model sizes, train and inference time between our proposed methods\nand baselines. In addition we delineate the cost per image using our method and\nother baselines.",
          "link": "http://arxiv.org/abs/2106.13802",
          "publishedOn": "2021-06-28T01:57:55.419Z",
          "wordCount": 649,
          "title": "Efficient Document Image Classification Using Region-Based Graph Neural Network. (arXiv:2106.13802v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1\">David Dehaene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1\">R&#xe9;my Brossard</a>",
          "description": "We propose a theoretical approach towards the training numerical stability of\nVariational AutoEncoders (VAE). Our work is motivated by recent studies\nempowering VAEs to reach state of the art generative results on complex image\ndatasets. These very deep VAE architectures, as well as VAEs using more complex\noutput distributions, highlight a tendency to haphazardly produce high training\ngradients as well as NaN losses. The empirical fixes proposed to train them\ndespite their limitations are neither fully theoretically grounded nor\ngenerally sufficient in practice. Building on this, we localize the source of\nthe problem at the interface between the model's neural networks and their\noutput probabilistic distributions. We explain a common source of instability\nstemming from an incautious formulation of the encoded Normal distribution's\nvariance, and apply the same approach on other, less obvious sources. We show\nthat by implementing small changes to the way we parameterize the Normal\ndistributions on which they rely, VAEs can securely be trained.",
          "link": "http://arxiv.org/abs/2106.13739",
          "publishedOn": "2021-06-28T01:57:55.399Z",
          "wordCount": 583,
          "title": "Re-parameterizing VAEs for stability. (arXiv:2106.13739v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yudong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongjin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>",
          "description": "Generating high-fidelity talking head video by fitting with the input audio\nsequence is a challenging problem that receives considerable attentions\nrecently. In this paper, we address this problem with the aid of neural scene\nrepresentation networks. Our method is completely different from existing\nmethods that rely on intermediate representations like 2D landmarks or 3D face\nmodels to bridge the gap between audio input and video output. Specifically,\nthe feature of input audio signal is directly fed into a conditional implicit\nfunction to generate a dynamic neural radiance field, from which a\nhigh-fidelity talking-head video corresponding to the audio signal is\nsynthesized using volume rendering. Another advantage of our framework is that\nnot only the head (with hair) region is synthesized as previous methods did,\nbut also the upper body is generated via two individual neural radiance fields.\nExperimental results demonstrate that our novel framework can (1) produce\nhigh-fidelity and natural results, and (2) support free adjustment of audio\nsignals, viewing directions, and background images.",
          "link": "http://arxiv.org/abs/2103.11078",
          "publishedOn": "2021-06-28T01:57:55.390Z",
          "wordCount": 648,
          "title": "AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis. (arXiv:2103.11078v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maeoki_S/0/1/0/all/0/1\">Sho Maeoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1\">Yusuke Mukuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>",
          "description": "In this paper we undertake the task of text-based video moment retrieval from\na corpus of videos. To train the model, text-moment paired datasets were used\nto learn the correct correspondences. In typical training methods, ground-truth\ntext-moment pairs are used as positive pairs, whereas other pairs are regarded\nas negative pairs. However, aside from the ground-truth pairs, some text-moment\npairs should be regarded as positive. In this case, one text annotation can be\npositive for many video moments. Conversely, one video moment can be\ncorresponded to many text annotations. Thus, there are many-to-many\ncorrespondences between the text annotations and video moments. Based on these\ncorrespondences, we can form potentially relevant pairs, which are not given as\nground truth yet are not negative; effectively incorporating such relevant\npairs into training can improve the retrieval performance. The text query\nshould describe what is happening in a video moment. Hence, different video\nmoments annotated with similar texts, which contain a similar action, are\nlikely to hold the similar action, thus these pairs can be considered as\npotentially relevant pairs. In this paper, we propose a novel training method\nthat takes advantage of potentially relevant pairs, which are detected based on\nlinguistic analysis about text annotation. Experiments on two benchmark\ndatasets revealed that our method improves the retrieval performance both\nquantitatively and qualitatively.",
          "link": "http://arxiv.org/abs/2106.13566",
          "publishedOn": "2021-06-28T01:57:55.383Z",
          "wordCount": 668,
          "title": "Video Moment Retrieval with Text Query Considering Many-to-Many Correspondence Using Potentially Relevant Pair. (arXiv:2106.13566v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1\">Robert Leer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roma_H/0/1/0/all/0/1\">Hessi Roma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amelia_J/0/1/0/all/0/1\">James Amelia</a>",
          "description": "The performance of image recognition like human pose detection, trained with\nsimulated images would usually get worse due to the divergence between real and\nsimulated data. To make the distribution of a simulated image close to that of\nreal one, there are several works applying GAN-based image-to-image\ntransformation methods, e.g., SimGAN and CycleGAN. However, these methods would\nnot be sensitive enough to the various change in pose and shape of subjects,\nespecially when the training data are imbalanced, e.g., some particular poses\nand shapes are minor in the training data. To overcome this problem, we propose\nto introduce the label information of subjects, e.g., pose and type of objects\nin the training of CycleGAN, and lead it to obtain label-wise transforamtion\nmodels. We evaluate our proposed method called Label-CycleGAN, through\nexperiments on the digit image transformation from SVHN to MNIST and the\nsurveillance camera image transformation from simulated to real images.",
          "link": "http://arxiv.org/abs/2106.13696",
          "publishedOn": "2021-06-28T01:57:55.361Z",
          "wordCount": 579,
          "title": "Image-to-image Transformation with Auxiliary Condition. (arXiv:2106.13696v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jingnan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>",
          "description": "We consider a category-level perception problem, where one is given 3D sensor\ndata picturing an object of a given category (e.g. a car), and has to\nreconstruct the pose and shape of the object despite intra-class variability\n(i.e. different car models have different shapes). We consider an active shape\nmodel, where -- for an object category -- we are given a library of potential\nCAD models describing objects in that category, and we adopt a standard\nformulation where pose and shape estimation are formulated as a non-convex\noptimization. Our first contribution is to provide the first certifiably\noptimal solver for pose and shape estimation. In particular, we show that\nrotation estimation can be decoupled from the estimation of the object\ntranslation and shape, and we demonstrate that (i) the optimal object rotation\ncan be computed via a tight (small-size) semidefinite relaxation, and (ii) the\ntranslation and shape parameters can be computed in closed-form given the\nrotation. Our second contribution is to add an outlier rejection layer to our\nsolver, hence making it robust to a large number of misdetections. Towards this\ngoal, we wrap our optimal solver in a robust estimation scheme based on\ngraduated non-convexity. To further enhance robustness to outliers, we also\ndevelop the first graph-theoretic formulation to prune outliers in\ncategory-level perception, which removes outliers via convex hull and maximum\nclique computations; the resulting approach is robust to 70%-90% outliers. Our\nthird contribution is an extensive experimental evaluation. Besides providing\nan ablation study on a simulated dataset and on the PASCAL3D+ dataset, we\ncombine our solver with a deep-learned keypoint detector, and show that the\nresulting approach improves over the state of the art in vehicle pose\nestimation in the ApolloScape datasets.",
          "link": "http://arxiv.org/abs/2104.08383",
          "publishedOn": "2021-06-28T01:57:55.354Z",
          "wordCount": 762,
          "title": "Optimal Pose and Shape Estimation for Category-level 3D Object Perception. (arXiv:2104.08383v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13559",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1\">Gabriel Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Esteve_A/0/1/0/all/0/1\">Anna Esteve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1\">Adri&#xe1;n Colomer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramos_D/0/1/0/all/0/1\">David Ramos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>",
          "description": "Recently, bladder cancer has been significantly increased in terms of\nincidence and mortality. Currently, two subtypes are known based on tumour\ngrowth: non-muscle invasive (NMIBC) and muscle-invasive bladder cancer (MIBC).\nIn this work, we focus on the MIBC subtype because it is of the worst prognosis\nand can spread to adjacent organs. We present a self-learning framework to\ngrade bladder cancer from histological images stained via immunohistochemical\ntechniques. Specifically, we propose a novel Deep Convolutional Embedded\nAttention Clustering (DCEAC) which allows classifying histological patches into\ndifferent severity levels of the disease, according to the patterns established\nin the literature. The proposed DCEAC model follows a two-step fully\nunsupervised learning methodology to discern between non-tumour, mild and\ninfiltrative patterns from high-resolution samples of 512x512 pixels. Our\nsystem outperforms previous clustering-based methods by including a\nconvolutional attention module, which allows refining the features of the\nlatent space before the classification stage. The proposed network exceeds\nstate-of-the-art approaches by 2-3% across different metrics, achieving a final\naverage accuracy of 0.9034 in a multi-class scenario. Furthermore, the reported\nclass activation maps evidence that our model is able to learn by itself the\nsame patterns that clinicians consider relevant, without incurring prior\nannotation steps. This fact supposes a breakthrough in muscle-invasive bladder\ncancer grading which bridges the gap with respect to train the model on\nlabelled data.",
          "link": "http://arxiv.org/abs/2106.13559",
          "publishedOn": "2021-06-28T01:57:55.346Z",
          "wordCount": 686,
          "title": "A Novel Self-Learning Framework for Bladder Cancer Grading Using Histopathological Images. (arXiv:2106.13559v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Ming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arslanturk_S/0/1/0/all/0/1\">Suzan Arslanturk</a>",
          "description": "Point cloud upsampling using deep learning has been paid various efforts in\nthe past few years. Recent supervised deep learning methods are restricted to\nthe size of training data and is limited in terms of covering all shapes of\npoint clouds. Besides, the acquisition of such amount of data is unrealistic,\nand the network generally performs less powerful than expected on unseen\nrecords. In this paper, we present an unsupervised approach to upsample point\nclouds internally referred as \"Zero Shot\" Point Cloud Upsampling (ZSPU) at\nholistic level. Our approach is solely based on the internal information\nprovided by a particular point cloud without patching in both self-training and\ntesting phases. This single-stream design significantly reduces the training\ntime of the upsampling task, by learning the relation between low-resolution\n(LR) point clouds and their high (original) resolution (HR) counterparts. This\nassociation will provide super-resolution (SR) outputs when original point\nclouds are loaded as input. We demonstrate competitive performance on benchmark\npoint cloud datasets when compared to other upsampling methods. Furthermore,\nZSPU achieves superior qualitative results on shapes with complex local details\nor high curvatures.",
          "link": "http://arxiv.org/abs/2106.13765",
          "publishedOn": "2021-06-28T01:57:55.328Z",
          "wordCount": 611,
          "title": "\"Zero Shot\" Point Cloud Upsampling. (arXiv:2106.13765v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mingkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Recently, transformers have shown great superiority in solving computer\nvision tasks by modeling images as a sequence of manually-split patches with\nself-attention mechanism. However, current architectures of vision transformers\n(ViTs) are simply inherited from natural language processing (NLP) tasks and\nhave not been sufficiently investigated and optimized. In this paper, we make a\nfurther step by examining the intrinsic structure of transformers for vision\ntasks and propose an architecture search method, dubbed ViTAS, to search for\nthe optimal architecture with similar hardware budgets. Concretely, we design a\nnew effective yet efficient weight sharing paradigm for ViTs, such that\narchitectures with different token embedding, sequence size, number of heads,\nwidth, and depth can be derived from a single super-transformer. Moreover, to\ncater for the variance of distinct architectures, we introduce \\textit{private}\nclass token and self-attention maps in the super-transformer. In addition, to\nadapt the searching for different budgets, we propose to search the sampling\nprobability of identity operation. Experimental results show that our ViTAS\nattains excellent results compared to existing pure transformer architectures.\nFor example, with $1.3$G FLOPs budget, our searched architecture achieves\n$74.7\\%$ top-$1$ accuracy on ImageNet and is $2.5\\%$ superior than the current\nbaseline ViT architecture. Code is available at\n\\url{https://github.com/xiusu/ViTAS}.",
          "link": "http://arxiv.org/abs/2106.13700",
          "publishedOn": "2021-06-28T01:57:55.295Z",
          "wordCount": 646,
          "title": "Vision Transformer Architecture Search. (arXiv:2106.13700v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xueying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>",
          "description": "Cross-modal retrieval aims to enable flexible retrieval experience by\ncombining multimedia data such as image, video, text, and audio. One core of\nunsupervised approaches is to dig the correlations among different object\nrepresentations to complete satisfied retrieval performance without requiring\nexpensive labels. In this paper, we propose a Graph Pattern Loss based\nDiversified Attention Network(GPLDAN) for unsupervised cross-modal retrieval to\ndeeply analyze correlations among representations. First, we propose a\ndiversified attention feature projector by considering the interaction between\ndifferent representations to generate multiple representations of an instance.\nThen, we design a novel graph pattern loss to explore the correlations among\ndifferent representations, in this graph all possible distances between\ndifferent representations are considered. In addition, a modality classifier is\nadded to explicitly declare the corresponding modalities of features before\nfusion and guide the network to enhance discrimination ability. We test GPLDAN\non four public datasets. Compared with the state-of-the-art cross-modal\nretrieval methods, the experimental results demonstrate the performance and\ncompetitiveness of GPLDAN.",
          "link": "http://arxiv.org/abs/2106.13552",
          "publishedOn": "2021-06-28T01:57:55.288Z",
          "wordCount": 640,
          "title": "Graph Pattern Loss based Diversified Attention Network for Cross-Modal Retrieval. (arXiv:2106.13552v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13551",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1\">Gabriel Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amor_R/0/1/0/all/0/1\">Roc&#xed;o del Amor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1\">Adri&#xe1;n Colomer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verdu_Monedero_R/0/1/0/all/0/1\">Rafael Verd&#xfa;-Monedero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morales_Sanchez_J/0/1/0/all/0/1\">Juan Morales-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>",
          "description": "Glaucoma is one of the leading causes of blindness worldwide and Optical\nCoherence Tomography (OCT) is the quintessential imaging technique for its\ndetection. Unlike most of the state-of-the-art studies focused on glaucoma\ndetection, in this paper, we propose, for the first time, a novel framework for\nglaucoma grading using raw circumpapillary B-scans. In particular, we set out a\nnew OCT-based hybrid network which combines hand-driven and deep learning\nalgorithms. An OCT-specific descriptor is proposed to extract hand-crafted\nfeatures related to the retinal nerve fibre layer (RNFL). In parallel, an\ninnovative CNN is developed using skip-connections to include tailored residual\nand attention modules to refine the automatic features of the latent space. The\nproposed architecture is used as a backbone to conduct a novel few-shot\nlearning based on static and dynamic prototypical networks. The k-shot paradigm\nis redefined giving rise to a supervised end-to-end system which provides\nsubstantial improvements discriminating between healthy, early and advanced\nglaucoma samples. The training and evaluation processes of the dynamic\nprototypical network are addressed from two fused databases acquired via\nHeidelberg Spectralis system. Validation and testing results reach a\ncategorical accuracy of 0.9459 and 0.8788 for glaucoma grading, respectively.\nBesides, the high performance reported by the proposed model for glaucoma\ndetection deserves a special mention. The findings from the class activation\nmaps are directly in line with the clinicians' opinion since the heatmaps\npointed out the RNFL as the most relevant structure for glaucoma diagnosis.",
          "link": "http://arxiv.org/abs/2106.13551",
          "publishedOn": "2021-06-28T01:57:55.282Z",
          "wordCount": 708,
          "title": "Circumpapillary OCT-Focused Hybrid Learning for Glaucoma Grading Using Tailored Prototypical Neural Networks. (arXiv:2106.13551v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hongwei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yupan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Vision-Language Pre-training (VLP) aims to learn multi-modal representations\nfrom image-text pairs and serves for downstream vision-language tasks in a\nfine-tuning fashion. The dominant VLP models adopt a CNN-Transformer\narchitecture, which embeds images with a CNN, and then aligns images and text\nwith a Transformer. Visual relationship between visual contents plays an\nimportant role in image understanding and is the basic for inter-modal\nalignment learning. However, CNNs have limitations in visual relation learning\ndue to local receptive field's weakness in modeling long-range dependencies.\nThus the two objectives of learning visual relation and inter-modal alignment\nare encapsulated in the same Transformer network. Such design might restrict\nthe inter-modal alignment learning in the Transformer by ignoring the\nspecialized characteristic of each objective. To tackle this, we propose a\nfully Transformer visual embedding for VLP to better learn visual relation and\nfurther promote inter-modal alignment. Specifically, we propose a metric named\nInter-Modality Flow (IMF) to measure the interaction between vision and\nlanguage modalities (i.e., inter-modality). We also design a novel masking\noptimization mechanism named Masked Feature Regression (MFR) in Transformer to\nfurther promote the inter-modality learning. To the best of our knowledge, this\nis the first study to explore the benefit of Transformer for visual feature\nlearning in VLP. We verify our method on a wide range of vision-language tasks,\nincluding Visual Question Answering (VQA), Visual Entailment and Visual\nReasoning. Our approach not only outperforms the state-of-the-art VLP\nperformance, but also shows benefits on the IMF metric.",
          "link": "http://arxiv.org/abs/2106.13488",
          "publishedOn": "2021-06-28T01:57:55.274Z",
          "wordCount": 687,
          "title": "Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training. (arXiv:2106.13488v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianchuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>",
          "description": "We present animatable neural radiance fields for detailed human avatar\ncreation from monocular videos. Our approach extends neural radiance fields\n(NeRF) to the dynamic scenes with human movements via introducing explicit\npose-guided deformation while learning the scene representation network. In\nparticular, we estimate the human pose for each frame and learn a constant\ncanonical space for the detailed human template, which enables natural shape\ndeformation from the observation space to the canonical space under the\nexplicit control of the pose parameters. To compensate for inaccurate pose\nestimation, we introduce the pose refinement strategy that updates the initial\npose during the learning process, which not only helps to learn more accurate\nhuman reconstruction but also accelerates the convergence. In experiments we\nshow that the proposed approach achieves 1) implicit human geometry and\nappearance reconstruction with high-quality details, 2) photo-realistic\nrendering of the human from arbitrary views, and 3) animation of the human with\narbitrary poses.",
          "link": "http://arxiv.org/abs/2106.13629",
          "publishedOn": "2021-06-28T01:57:55.255Z",
          "wordCount": 599,
          "title": "Animatable Neural Radiance Fields from Monocular RGB Video. (arXiv:2106.13629v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>",
          "description": "Recent advances in image synthesis enables one to translate images by\nlearning the mapping between a source domain and a target domain. Existing\nmethods tend to learn the distributions by training a model on a variety of\ndatasets, with results evaluated largely in a subjective manner. Relatively few\nworks in this area, however, study the potential use of semantic image\ntranslation methods for image recognition tasks. In this paper, we explore the\nuse of Single Image Texture Translation (SITT) for data augmentation. We first\npropose a lightweight model for translating texture to images based on a single\ninput of source texture, allowing for fast training and testing. Based on SITT,\nwe then explore the use of augmented data in long-tailed and few-shot image\nclassification tasks. We find the proposed method is capable of translating\ninput data into a target domain, leading to consistent improved image\nrecognition performance. Finally, we examine how SITT and related image\ntranslation methods can provide a basis for a data-efficient, augmentation\nengineering approach to model training.",
          "link": "http://arxiv.org/abs/2106.13804",
          "publishedOn": "2021-06-28T01:57:55.244Z",
          "wordCount": 612,
          "title": "Single Image Texture Translation for Data Augmentation. (arXiv:2106.13804v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Pei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yuning Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1\">Gamaleldin Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1\">Alex Bewley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>",
          "description": "The detection of 3D objects from LiDAR data is a critical component in most\nautonomous driving systems. Safe, high speed driving needs larger detection\nranges, which are enabled by new LiDARs. These larger detection ranges require\nmore efficient and accurate detection models. Towards this goal, we propose\nRange Sparse Net (RSN), a simple, efficient, and accurate 3D object detector in\norder to tackle real time 3D object detection in this extended detection\nregime. RSN predicts foreground points from range images and applies sparse\nconvolutions on the selected foreground points to detect objects. The\nlightweight 2D convolutions on dense range images results in significantly\nfewer selected foreground points, thus enabling the later sparse convolutions\nin RSN to efficiently operate. Combining features from the range image further\nenhance detection accuracy. RSN runs at more than 60 frames per second on a\n150m x 150m detection region on Waymo Open Dataset (WOD) while being more\naccurate than previously published detectors. As of 11/2020, RSN is ranked\nfirst in the WOD leaderboard based on the APH/LEVEL 1 metrics for LiDAR-based\npedestrian and vehicle detection, while being several times faster than\nalternatives.",
          "link": "http://arxiv.org/abs/2106.13365",
          "publishedOn": "2021-06-28T01:57:55.237Z",
          "wordCount": 640,
          "title": "RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection. (arXiv:2106.13365v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yibao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xingru Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>",
          "description": "The detection of nuclei and cells in histology images is of great value in\nboth clinical practice and pathological studies. However, multiple reasons such\nas morphological variations of nuclei or cells make it a challenging task where\nconventional object detection methods cannot obtain satisfactory performance in\nmany cases. A detection task consists of two sub-tasks, classification and\nlocalization. Under the condition of dense object detection, classification is\na key to boost the detection performance. Considering this, we propose\nsimilarity based region proposal networks (SRPN) for nuclei and cells detection\nin histology images. In particular, a customized convolution layer termed as\nembedding layer is designed for network building. The embedding layer is added\ninto the region proposal networks, enabling the networks to learn\ndiscriminative features based on similarity learning. Features obtained by\nsimilarity learning can significantly boost the classification performance\ncompared to conventional methods. SRPN can be easily integrated into standard\nconvolutional neural networks architectures such as the Faster R-CNN and\nRetinaNet. We test the proposed approach on tasks of multi-organ nuclei\ndetection and signet ring cells detection in histological images. Experimental\nresults show that networks applying similarity learning achieved superior\nperformance on both tasks when compared to their counterparts. In particular,\nthe proposed SRPN achieve state-of-the-art performance on the MoNuSeg benchmark\nfor nuclei segmentation and detection while compared to previous methods, and\non the signet ring cell detection benchmark when compared with baselines. The\nsourcecode is publicly available at:\nhttps://github.com/sigma10010/nuclei_cells_det.",
          "link": "http://arxiv.org/abs/2106.13556",
          "publishedOn": "2021-06-28T01:57:55.231Z",
          "wordCount": 701,
          "title": "SRPN: similarity-based region proposal networks for nuclei and cells detection in histology images. (arXiv:2106.13556v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pan Gao</a>",
          "description": "Recent studies have shown that neural network (NN) based image classifiers\nare highly vulnerable to adversarial examples, which poses a threat to\nsecurity-sensitive image recognition task. Prior work has shown that JPEG\ncompression can combat the drop in classification accuracy on adversarial\nexamples to some extent. But, as the compression ratio increases, traditional\nJPEG compression is insufficient to defend those attacks but can cause an\nabrupt accuracy decline to the benign images. In this paper, with the aim of\nfully filtering the adversarial perturbations, we firstly make modifications to\ntraditional JPEG compression algorithm which becomes more favorable for NN.\nSpecifically, based on an analysis of the frequency coefficient, we design a\nNN-favored quantization table for compression. Considering compression as a\ndata augmentation strategy, we then combine our model-agnostic preprocess with\nnoisy training. We fine-tune the pre-trained model by training with images\nencoded at different compression levels, thus generating multiple classifiers.\nFinally, since lower (higher) compression ratio can remove both perturbations\nand original features slightly (aggressively), we use these trained multiple\nmodels for model ensemble. The majority vote of the ensemble of models is\nadopted as final predictions. Experiments results show our method can improve\ndefense efficiency while maintaining original accuracy.",
          "link": "http://arxiv.org/abs/2106.13394",
          "publishedOn": "2021-06-28T01:57:55.222Z",
          "wordCount": 640,
          "title": "Countering Adversarial Examples: Combining Input Transformation and Noisy Training. (arXiv:2106.13394v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zhipeng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1\">Martial Hebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>",
          "description": "Generative modeling has recently shown great promise in computer vision, but\nit has mostly focused on synthesizing visually realistic images. In this paper,\nmotivated by multi-task learning of shareable feature representations, we\nconsider a novel problem of learning a shared generative model that is useful\nacross various visual perception tasks. Correspondingly, we propose a general\nmulti-task oriented generative modeling (MGM) framework, by coupling a\ndiscriminative multi-task network with a generative network. While it is\nchallenging to synthesize both RGB images and pixel-level annotations in\nmulti-task scenarios, our framework enables us to use synthesized images paired\nwith only weak annotations (i.e., image-level scene labels) to facilitate\nmultiple visual tasks. Experimental evaluation on challenging multi-task\nbenchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework\nimproves the performance of all the tasks by large margins, consistently\noutperforming state-of-the-art multi-task approaches.",
          "link": "http://arxiv.org/abs/2106.13409",
          "publishedOn": "2021-06-28T01:57:55.202Z",
          "wordCount": 568,
          "title": "Generative Modeling for Multi-task Visual Learning. (arXiv:2106.13409v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaohui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Renjie Liao</a>",
          "description": "In this paper, we present a non-parametric structured latent variable model\nfor image generation, called NP-DRAW, which sequentially draws on a latent\ncanvas in a part-by-part fashion and then decodes the image from the canvas.\nOur key contributions are as follows. 1) We propose a non-parametric prior\ndistribution over the appearance of image parts so that the latent variable\n``what-to-draw'' per step becomes a categorical random variable. This improves\nthe expressiveness and greatly eases the learning compared to Gaussians used in\nthe literature. 2) We model the sequential dependency structure of parts via a\nTransformer, which is more powerful and easier to train compared to RNNs used\nin the literature. 3) We propose an effective heuristic parsing algorithm to\npre-train the prior. Experiments on MNIST, Omniglot, CIFAR-10, and CelebA show\nthat our method significantly outperforms previous structured image models like\nDRAW and AIR and is competitive to other generic generative models. Moreover,\nwe show that our model's inherent compositionality and interpretability bring\nsignificant benefits in the low-data learning regime and latent space editing.\nCode is available at \\url{https://github.com/ZENGXH/NPDRAW}.",
          "link": "http://arxiv.org/abs/2106.13435",
          "publishedOn": "2021-06-28T01:57:55.196Z",
          "wordCount": 627,
          "title": "NP-DRAW: A Non-Parametric Structured Latent Variable Modelfor Image Generation. (arXiv:2106.13435v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1\">Giovanni Trappolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosmo_L/0/1/0/all/0/1\">Luca Cosmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1\">Luca Moschella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1\">Riccardo Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>",
          "description": "In this paper, we propose a transformer-based procedure for the efficient\nregistration of non-rigid 3D point clouds. The proposed approach is data-driven\nand adopts for the first time the transformer architecture in the registration\ntask. Our method is general and applies to different settings. Given a fixed\ntemplate with some desired properties (e.g. skinning weights or other animation\ncues), we can register raw acquired data to it, thereby transferring all the\ntemplate properties to the input geometry. Alternatively, given a pair of\nshapes, our method can register the first onto the second (or vice-versa),\nobtaining a high-quality dense correspondence between the two. In both\ncontexts, the quality of our results enables us to target real applications\nsuch as texture transfer and shape interpolation. Furthermore, we also show\nthat including an estimation of the underlying density of the surface eases the\nlearning process. By exploiting the potential of this architecture, we can\ntrain our model requiring only a sparse set of ground truth correspondences\n($10\\sim20\\%$ of the total points). The proposed model and the analysis that we\nperform pave the way for future exploration of transformer-based architectures\nfor registration and matching applications. Qualitative and quantitative\nevaluations demonstrate that our pipeline outperforms state-of-the-art methods\nfor deformable and unordered 3D data registration on different datasets and\nscenarios.",
          "link": "http://arxiv.org/abs/2106.13679",
          "publishedOn": "2021-06-28T01:57:55.190Z",
          "wordCount": 657,
          "title": "Shape registration in the time of transformers. (arXiv:2106.13679v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_V/0/1/0/all/0/1\">Vignesh Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strodthoff_N/0/1/0/all/0/1\">Nils Strodthoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jackie Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Klaus-Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>",
          "description": "There is an increasing number of medical use-cases where classification\nalgorithms based on deep neural networks reach performance levels that are\ncompetitive with human medical experts. To alleviate the challenges of small\ndataset sizes, these systems often rely on pretraining. In this work, we aim to\nassess the broader implications of these approaches. For diabetic retinopathy\ngrading as exemplary use case, we compare the impact of different training\nprocedures including recently established self-supervised pretraining methods\nbased on contrastive learning. To this end, we investigate different aspects\nsuch as quantitative performance, statistics of the learned feature\nrepresentations, interpretability and robustness to image distortions. Our\nresults indicate that models initialized from ImageNet pretraining report a\nsignificant increase in performance, generalization and robustness to image\ndistortions. In particular, self-supervised models show further benefits to\nsupervised models. Self-supervised models with initialization from ImageNet\npretraining not only report higher performance, they also reduce overfitting to\nlarge lesions along with improvements in taking into account minute lesions\nindicative of the progression of the disease. Understanding the effects of\npretraining in a broader sense that goes beyond simple performance comparisons\nis of crucial importance for the broader medical imaging community beyond the\nuse-case considered in this work.",
          "link": "http://arxiv.org/abs/2106.13497",
          "publishedOn": "2021-06-28T01:57:55.181Z",
          "wordCount": 654,
          "title": "On the Robustness of Pretraining and Self-Supervision for a Deep Learning-based Analysis of Diabetic Retinopathy. (arXiv:2106.13497v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yuning Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Pei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1\">Jiquan Ngiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1\">Benjamin Caine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Vijay Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>",
          "description": "3D object detection is vital for many robotics applications. For tasks where\na 2D perspective range image exists, we propose to learn a 3D representation\ndirectly from this range image view. To this end, we designed a 2D\nconvolutional network architecture that carries the 3D spherical coordinates of\neach pixel throughout the network. Its layers can consume any arbitrary\nconvolution kernel in place of the default inner product kernel and exploit the\nunderlying local geometry around each pixel. We outline four such kernels: a\ndense kernel according to the bag-of-words paradigm, and three graph kernels\ninspired by recent graph neural network advances: the Transformer, the\nPointNet, and the Edge Convolution. We also explore cross-modality fusion with\nthe camera image, facilitated by operating in the perspective range image view.\nOur method performs competitively on the Waymo Open Dataset and improves the\nstate-of-the-art AP for pedestrian detection from 69.7% to 75.5%. It is also\nefficient in that our smallest model, which still outperforms the popular\nPointPillars in quality, requires 180 times fewer FLOPS and model parameters",
          "link": "http://arxiv.org/abs/2106.13381",
          "publishedOn": "2021-06-28T01:57:55.175Z",
          "wordCount": 635,
          "title": "To the Point: Efficient 3D Object Detection in the Range Image with Graph Convolution Kernels. (arXiv:2106.13381v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scieur_D/0/1/0/all/0/1\">Damien Scieur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngsung Kim</a>",
          "description": "This paper considers classification problems with hierarchically organized\nclasses. We force the classifier (hyperplane) of each class to belong to a\nsphere manifold, whose center is the classifier of its super-class. Then,\nindividual sphere manifolds are connected based on their hierarchical\nrelations. Our technique replaces the last layer of a neural network by\ncombining a spherical fully-connected layer with a hierarchical layer. This\nregularization is shown to improve the performance of widely used deep neural\nnetwork architectures (ResNet and DenseNet) on publicly available datasets\n(CIFAR100, CUB200, Stanford dogs, Stanford cars, and Tiny-ImageNet).",
          "link": "http://arxiv.org/abs/2106.13549",
          "publishedOn": "2021-06-28T01:57:55.155Z",
          "wordCount": 524,
          "title": "Connecting Sphere Manifolds Hierarchically for Regularization. (arXiv:2106.13549v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Endo_Y/0/1/0/all/0/1\">Yuki Endo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanamori_Y/0/1/0/all/0/1\">Yoshihiro Kanamori</a>",
          "description": "Semantic image synthesis is a process for generating photorealistic images\nfrom a single semantic mask. To enrich the diversity of multimodal image\nsynthesis, previous methods have controlled the global appearance of an output\nimage by learning a single latent space. However, a single latent code is often\ninsufficient for capturing various object styles because object appearance\ndepends on multiple factors. To handle individual factors that determine object\nstyles, we propose a class- and layer-wise extension to the variational\nautoencoder (VAE) framework that allows flexible control over each object class\nat the local to global levels by learning multiple latent spaces. Furthermore,\nwe demonstrate that our method generates images that are both plausible and\nmore diverse compared to state-of-the-art methods via extensive experiments\nwith real and synthetic datasets inthree different domains. We also show that\nour method enables a wide range of applications in image synthesis and editing\ntasks.",
          "link": "http://arxiv.org/abs/2106.13416",
          "publishedOn": "2021-06-28T01:57:55.148Z",
          "wordCount": 603,
          "title": "Diversifying Semantic Image Synthesis and Editing via Class- and Layer-wise VAEs. (arXiv:2106.13416v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samelak_J/0/1/0/all/0/1\">Jaros&#x142;aw Samelak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domanski_M/0/1/0/all/0/1\">Marek Doma&#x144;ski</a>",
          "description": "The paper presents a new approach to multiview video coding using Screen\nContent Coding. It is assumed that for a time instant the frames corresponding\nto all views are packed into a single frame, i.e. the frame-compatible approach\nto multiview coding is applied. For such coding scenario, the paper\ndemonstrates that Screen Content Coding can be efficiently used for multiview\nvideo coding. Two approaches are considered: the first using standard HEVC\nScreen Content Coding, and the second using Advanced Screen Content Coding. The\nlatter is the original proposal of the authors that exploits quarter-pel motion\nvectors and other nonstandard extensions of HEVC Screen Content Coding. The\nexperimental results demonstrate that multiview video coding even using\nstandard HEVC Screen Content Coding is much more efficient than simulcast HEVC\ncoding. The proposed Advanced Screen Content Coding provides virtually the same\ncoding efficiency as MV-HEVC, which is the state-of-the-art multiview video\ncompression technique. The authors suggest that Advanced Screen Content Coding\ncan be efficiently used within the new Versatile Video Coding (VVC) technology.\nNevertheless a reference multiview extension of VVC does not exist yet,\ntherefore, for VVC-based coding, the experimental comparisons are left for\nfuture work.",
          "link": "http://arxiv.org/abs/2106.13574",
          "publishedOn": "2021-06-28T01:57:55.139Z",
          "wordCount": 627,
          "title": "Multiview Video Compression Using Advanced HEVC Screen Content Coding. (arXiv:2106.13574v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bongini_F/0/1/0/all/0/1\">Francesco Bongini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berlincioni_L/0/1/0/all/0/1\">Lorenzo Berlincioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertini_M/0/1/0/all/0/1\">Marco Bertini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>",
          "description": "In this paper we propose a novel data augmentation approach for visual\ncontent domains that have scarce training datasets, compositing synthetic 3D\nobjects within real scenes. We show the performance of the proposed system in\nthe context of object detection in thermal videos, a domain where 1) training\ndatasets are very limited compared to visible spectrum datasets and 2) creating\nfull realistic synthetic scenes is extremely cumbersome and expensive due to\nthe difficulty in modeling the thermal properties of the materials of the\nscene. We compare different augmentation strategies, including state of the art\napproaches obtained through RL techniques, the injection of simulated data and\nthe employment of a generative model, and study how to best combine our\nproposed augmentation with these other techniques.Experimental results\ndemonstrate the effectiveness of our approach, and our single-modality detector\nachieves state-of-the-art results on the FLIR ADAS dataset.",
          "link": "http://arxiv.org/abs/2106.13603",
          "publishedOn": "2021-06-28T01:57:55.114Z",
          "wordCount": 597,
          "title": "Partially fake it till you make it: mixing real and fake thermal images for improved object detection. (arXiv:2106.13603v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1\">Qiang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kang Wang</a>",
          "description": "Model-based eye tracking has been a dominant approach for eye gaze tracking\nbecause of its ability to generalize to different subjects, without the need of\nany training data and eye gaze annotations. Model-based eye tracking, however,\nis susceptible to eye feature detection errors, in particular for eye tracking\nin the wild. To address this issue, we propose a Bayesian framework for\nmodel-based eye tracking. The proposed system consists of a cascade-Bayesian\nConvolutional Neural Network (c-BCNN) to capture the probabilistic\nrelationships between eye appearance and its landmarks, and a geometric eye\nmodel to estimate eye gaze from the eye landmarks. Given a testing eye image,\nthe Bayesian framework can generate, through Bayesian inference, the eye gaze\ndistribution without explicit landmark detection and model training, based on\nwhich it not only estimates the most likely eye gaze but also its uncertainty.\nFurthermore, with Bayesian inference instead of point-based inference, our\nmodel can not only generalize better to different sub-jects, head poses, and\nenvironments but also is robust to image noise and landmark detection errors.\nFinally, with the estimated gaze uncertainty, we can construct a cascade\narchitecture that allows us to progressively improve gaze estimation accuracy.\nCompared to state-of-the-art model-based and learning-based methods, the\nproposed Bayesian framework demonstrates significant improvement in\ngeneralization capability across several benchmark datasets and in accuracy and\nrobustness under challenging real-world conditions.",
          "link": "http://arxiv.org/abs/2106.13387",
          "publishedOn": "2021-06-28T01:57:55.106Z",
          "wordCount": 644,
          "title": "Bayesian Eye Tracking. (arXiv:2106.13387v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1\">Sai Vemprala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyde_N/0/1/0/all/0/1\">Nicholas Gyde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1\">Hadi Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1\">Ashish Kapoor</a>",
          "description": "The ability to perform causal and counterfactual reasoning are central\nproperties of human intelligence. Decision-making systems that can perform\nthese types of reasoning have the potential to be more generalizable and\ninterpretable. Simulations have helped advance the state-of-the-art in this\ndomain, by providing the ability to systematically vary parameters (e.g.,\nconfounders) and generate examples of the outcomes in the case of\ncounterfactual scenarios. However, simulating complex temporal causal events in\nmulti-agent scenarios, such as those that exist in driving and vehicle\nnavigation, is challenging. To help address this, we present a high-fidelity\nsimulation environment that is designed for developing algorithms for causal\ndiscovery and counterfactual reasoning in the safety-critical context. A core\ncomponent of our work is to introduce \\textit{agency}, such that it is simple\nto define and create complex scenarios using high-level definitions. The\nvehicles then operate with agency to complete these objectives, meaning\nlow-level behaviors need only be controlled if necessary. We perform\nexperiments with three state-of-the-art methods to create baselines and\nhighlight the affordances of this environment. Finally, we highlight challenges\nand opportunities for future work.",
          "link": "http://arxiv.org/abs/2106.13364",
          "publishedOn": "2021-06-28T01:57:55.087Z",
          "wordCount": 636,
          "title": "CausalCity: Complex Simulations with Agency for Causal Discovery and Reasoning. (arXiv:2106.13364v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Shiming Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chunhong Pan</a>",
          "description": "Previous methods for skeleton-based gesture recognition mostly arrange the\nskeleton sequence into a pseudo picture or spatial-temporal graph and apply\ndeep Convolutional Neural Network (CNN) or Graph Convolutional Network (GCN)\nfor feature extraction. Although achieving superior results, these methods have\ninherent limitations in dynamically capturing local features of interactive\nhand parts, and the computing efficiency still remains a serious issue. In this\nwork, the self-attention mechanism is introduced to alleviate this problem.\nConsidering the hierarchical structure of hand joints, we propose an efficient\nhierarchical self-attention network (HAN) for skeleton-based gesture\nrecognition, which is based on pure self-attention without any CNN, RNN or GCN\noperators. Specifically, the joint self-attention module is used to capture\nspatial features of fingers, the finger self-attention module is designed to\naggregate features of the whole hand. In terms of temporal features, the\ntemporal self-attention module is utilized to capture the temporal dynamics of\nthe fingers and the entire hand. Finally, these features are fused by the\nfusion self-attention module for gesture classification. Experiments show that\nour method achieves competitive results on three gesture recognition datasets\nwith much lower computational complexity.",
          "link": "http://arxiv.org/abs/2106.13391",
          "publishedOn": "2021-06-28T01:57:55.010Z",
          "wordCount": 629,
          "title": "HAN: An Efficient Hierarchical Self-Attention Network for Skeleton-Based Gesture Recognition. (arXiv:2106.13391v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moya_B/0/1/0/all/0/1\">Beatriz Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1\">Alberto Badias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1\">David Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1\">Francisco Chinesta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1\">Elias Cueto</a>",
          "description": "Physics perception very often faces the problem that only limited data or\npartial measurements on the scene are available. In this work, we propose a\nstrategy to learn the full state of sloshing liquids from measurements of the\nfree surface. Our approach is based on recurrent neural networks (RNN) that\nproject the limited information available to a reduced-order manifold so as to\nnot only reconstruct the unknown information, but also to be capable of\nperforming fluid reasoning about future scenarios in real time. To obtain\nphysically consistent predictions, we train deep neural networks on the\nreduced-order manifold that, through the employ of inductive biases, ensure the\nfulfillment of the principles of thermodynamics. RNNs learn from history the\nrequired hidden information to correlate the limited information with the\nlatent space where the simulation occurs. Finally, a decoder returns data back\nto the high-dimensional manifold, so as to provide the user with insightful\ninformation in the form of augmented reality. This algorithm is connected to a\ncomputer vision system to test the performance of the proposed methodology with\nreal information, resulting in a system capable of understanding and predicting\nfuture states of the observed fluid in real-time.",
          "link": "http://arxiv.org/abs/2106.13301",
          "publishedOn": "2021-06-28T01:57:55.003Z",
          "wordCount": 644,
          "title": "Physics perception in sloshing scenes with guaranteed thermodynamic consistency. (arXiv:2106.13301v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13328",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1\">Yize Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patney_A/0/1/0/all/0/1\">Anjul Patney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Webb_R/0/1/0/all/0/1\">Richard Webb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan Bovik</a>",
          "description": "Previous blind or No Reference (NR) video quality assessment (VQA) models\nlargely rely on features drawn from natural scene statistics (NSS), but under\nthe assumption that the image statistics are stationary in the spatial domain.\nSeveral of these models are quite successful on standard pictures. However, in\nVirtual Reality (VR) applications, foveated video compression is regaining\nattention, and the concept of space-variant quality assessment is of interest,\ngiven the availability of increasingly high spatial and temporal resolution\ncontents and practical ways of measuring gaze direction. Distortions from\nfoveated video compression increase with increased eccentricity, implying that\nthe natural scene statistics are space-variant. Towards advancing the\ndevelopment of foveated compression / streaming algorithms, we have devised a\nno-reference (NR) foveated video quality assessment model, called FOVQA, which\nis based on new models of space-variant natural scene statistics (NSS) and\nnatural video statistics (NVS). Specifically, we deploy a space-variant\ngeneralized Gaussian distribution (SV-GGD) model and a space-variant\nasynchronous generalized Gaussian distribution (SV-AGGD) model of mean\nsubtracted contrast normalized (MSCN) coefficients and products of neighboring\nMSCN coefficients, respectively. We devise a foveated video quality predictor\nthat extracts radial basis features, and other features that capture\nperceptually annoying rapid quality fall-offs. We find that FOVQA achieves\nstate-of-the-art (SOTA) performance on the new 2D LIVE-FBT-FCVR database, as\ncompared with other leading FIQA / VQA models. we have made our implementation\nof FOVQA available at: this http URL",
          "link": "http://arxiv.org/abs/2106.13328",
          "publishedOn": "2021-06-28T01:57:54.996Z",
          "wordCount": 673,
          "title": "FOVQA: Blind Foveated Video Quality Assessment. (arXiv:2106.13328v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13315",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_A/0/1/0/all/0/1\">Angela F. Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasmussen_B/0/1/0/all/0/1\">Brandon Rasmussen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kulits_P/0/1/0/all/0/1\">Peter Kulits</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scheller_E/0/1/0/all/0/1\">Eva L. Scheller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greenberger_R/0/1/0/all/0/1\">Rebecca Greenberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ehlmann_B/0/1/0/all/0/1\">Bethany L. Ehlmann</a>",
          "description": "The application of infrared hyperspectral imagery to geological problems is\nbecoming more popular as data become more accessible and cost-effective.\nClustering and classifying spectrally similar materials is often a first step\nin applications ranging from economic mineral exploration on Earth to planetary\nexploration on Mars. Semi-manual classification guided by expertly developed\nspectral parameters can be time consuming and biased, while supervised methods\nrequire abundant labeled data and can be difficult to generalize. Here we\ndevelop a fully unsupervised workflow for feature extraction and clustering\ninformed by both expert spectral geologist input and quantitative metrics. Our\npipeline uses a lightweight autoencoder followed by Gaussian mixture modeling\nto map the spectral diversity within any image. We validate the performance of\nour pipeline at submillimeter-scale with expert-labelled data from the Oman\nophiolite drill core and evaluate performance at meters-scale with partially\nclassified orbital data of Jezero Crater on Mars (the landing site for the\nPerseverance rover). We additionally examine the effects of various\npreprocessing techniques used in traditional analysis of hyperspectral imagery.\nThis pipeline provides a fast and accurate clustering map of similar geological\nmaterials and consistently identifies and separates major mineral classes in\nboth laboratory imagery and remote sensing imagery. We refer to our pipeline as\n\"Generalized Pipeline for Spectroscopic Unsupervised clustering of Minerals\n(GyPSUM).\"",
          "link": "http://arxiv.org/abs/2106.13315",
          "publishedOn": "2021-06-28T01:57:54.988Z",
          "wordCount": 685,
          "title": "Generalized Unsupervised Clustering of Hyperspectral Images of Geological Targets in the Near Infrared. (arXiv:2106.13315v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Worrall_G/0/1/0/all/0/1\">George Worrall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1\">Anand Rangarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Judge_J/0/1/0/all/0/1\">Jasmeet Judge</a>",
          "description": "Advanced machine learning techniques have been used in remote sensing (RS)\napplications such as crop mapping and yield prediction, but remain\nunder-utilized for tracking crop progress. In this study, we demonstrate the\nuse of agronomic knowledge of crop growth drivers in a Long Short-Term\nMemory-based, Domain-guided neural network (DgNN) for in-season crop progress\nestimation. The DgNN uses a branched structure and attention to separate\nindependent crop growth drivers and capture their varying importance throughout\nthe growing season. The DgNN is implemented for corn, using RS data in Iowa for\nthe period 2003-2019, with USDA crop progress reports used as ground truth.\nState-wide DgNN performance shows significant improvement over sequential and\ndense-only NN structures, and a widely-used Hidden Markov Model method. The\nDgNN had a 3.5% higher Nash-Sutfliffe efficiency over all growth stages and 33%\nmore weeks with highest cosine similarity than the other NNs during test years.\nThe DgNN and Sequential NN were more robust during periods of abnormal crop\nprogress, though estimating the Silking-Grainfill transition was difficult for\nall methods. Finally, Uniform Manifold Approximation and Projection\nvisualizations of layer activations showed how LSTM-based NNs separate crop\ngrowth time-series differently from a dense-only structure. Results from this\nstudy exhibit both the viability of NNs in crop growth stage estimation (CGSE)\nand the benefits of using domain knowledge. The DgNN methodology presented here\ncan be extended to provide near-real time CGSE of other crops.",
          "link": "http://arxiv.org/abs/2106.13323",
          "publishedOn": "2021-06-28T01:57:54.963Z",
          "wordCount": 679,
          "title": "Domain-guided Machine Learning for Remotely Sensed In-Season Crop Growth Estimation. (arXiv:2106.13323v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wanqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lizhong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>",
          "description": "Self-Rating Depression Scale (SDS) questionnaire has frequently been used for\nefficient depression preliminary screening. However, the uncontrollable\nself-administered measure can be easily affected by insouciantly or deceptively\nanswering, and producing the different results with the clinician-administered\nHamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,\nfacial expression (FE) and actions play a vital role in clinician-administered\nevaluation, while FE and action are underexplored for self-administered\nevaluations. In this work, we collect a novel dataset of 200 subjects to\nevidence the validity of self-rating questionnaires with their corresponding\nquestion-wise video recording. To automatically interpret depression from the\nSDS evaluation and the paired video, we propose an end-to-end hierarchical\nframework for the long-term variable-length video, which is also conditioned on\nthe questionnaire results and the answering time. Specifically, we resort to a\nhierarchical model which utilizes a 3D CNN for local temporal pattern\nexploration and a redundancy-aware self-attention (RAS) scheme for\nquestion-wise global feature aggregation. Targeting for the redundant long-term\nFE video processing, our RAS is able to effectively exploit the correlations of\neach video clip within a question set to emphasize the discriminative\ninformation and eliminate the redundancy based on feature pair-wise affinity.\nThen, the question-wise video feature is concatenated with the questionnaire\nscores for final depression detection. Our thorough evaluations also show the\nvalidity of fusing SDS evaluation and its video recording, and the superiority\nof our framework to the conventional state-of-the-art temporal modeling\nmethods.",
          "link": "http://arxiv.org/abs/2106.13393",
          "publishedOn": "2021-06-28T01:57:54.947Z",
          "wordCount": 702,
          "title": "Interpreting Depression From Question-wise Long-term Video Recording of SDS Evaluation. (arXiv:2106.13393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "Generalising deep models to new data from new centres (termed here domains)\nremains a challenge. This is largely attributed to shifts in data statistics\n(domain shifts) between source and unseen domains. Recently, gradient-based\nmeta-learning approaches where the training data are split into meta-train and\nmeta-test sets to simulate and handle the domain shifts during training have\nshown improved generalisation performance. However, the current fully\nsupervised meta-learning approaches are not scalable for medical image\nsegmentation, where large effort is required to create pixel-wise annotations.\nMeanwhile, in a low data regime, the simulated domain shifts may not\napproximate the true domain shifts well across source and unseen domains. To\naddress this problem, we propose a novel semi-supervised meta-learning\nframework with disentanglement. We explicitly model the representations related\nto domain shifts. Disentangling the representations and combining them to\nreconstruct the input image allows unlabeled data to be used to better\napproximate the true domain shifts for meta-learning. Hence, the model can\nachieve better generalisation performance, especially when there is a limited\namount of labeled data. Experiments show that the proposed method is robust on\ndifferent segmentation tasks and achieves state-of-the-art generalisation\nperformance on two public benchmarks.",
          "link": "http://arxiv.org/abs/2106.13292",
          "publishedOn": "2021-06-28T01:57:54.877Z",
          "wordCount": 637,
          "title": "Semi-supervised Meta-learning with Disentanglement for Domain-generalised Medical Image Segmentation. (arXiv:2106.13292v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hirota_Y/0/1/0/all/0/1\">Yusuke Hirota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Noa Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otani_M/0/1/0/all/0/1\">Mayu Otani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_I/0/1/0/all/0/1\">Ittetsu Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoye_T/0/1/0/all/0/1\">Takao Onoye</a>",
          "description": "How far can we go with textual representations for understanding pictures? In\nimage understanding, it is essential to use concise but detailed image\nrepresentations. Deep visual features extracted by vision models, such as\nFaster R-CNN, are prevailing used in multiple tasks, and especially in visual\nquestion answering (VQA). However, conventional deep visual features may\nstruggle to convey all the details in an image as we humans do. Meanwhile, with\nrecent language models' progress, descriptive text may be an alternative to\nthis problem. This paper delves into the effectiveness of textual\nrepresentations for image understanding in the specific context of VQA. We\npropose to take description-question pairs as input, instead of deep visual\nfeatures, and fed them into a language-only Transformer model, simplifying the\nprocess and the computational cost. We also experiment with data augmentation\ntechniques to increase the diversity in the training set and avoid learning\nstatistical bias. Extensive evaluations have shown that textual representations\nrequire only about a hundred words to compete with deep visual features on both\nVQA 2.0 and VQA-CP v2.",
          "link": "http://arxiv.org/abs/2106.13445",
          "publishedOn": "2021-06-28T01:57:54.820Z",
          "wordCount": 623,
          "title": "A Picture May Be Worth a Hundred Words for Visual Question Answering. (arXiv:2106.13445v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Singh Chaplot</a>",
          "description": "Breakthroughs in machine learning in the last decade have led to `digital\nintelligence', i.e. machine learning models capable of learning from vast\namounts of labeled data to perform several digital tasks such as speech\nrecognition, face recognition, machine translation and so on. The goal of this\nthesis is to make progress towards designing algorithms capable of `physical\nintelligence', i.e. building intelligent autonomous navigation agents capable\nof learning to perform complex navigation tasks in the physical world involving\nvisual perception, natural language understanding, reasoning, planning, and\nsequential decision making. Despite several advances in classical navigation\nmethods in the last few decades, current navigation agents struggle at\nlong-term semantic navigation tasks. In the first part of the thesis, we\ndiscuss our work on short-term navigation using end-to-end reinforcement\nlearning to tackle challenges such as obstacle avoidance, semantic perception,\nlanguage grounding, and reasoning. In the second part, we present a new class\nof navigation methods based on modular learning and structured explicit map\nrepresentations, which leverage the strengths of both classical and end-to-end\nlearning methods, to tackle long-term navigation tasks. We show that these\nmethods are able to effectively tackle challenges such as localization,\nmapping, long-term planning, exploration and learning semantic priors. These\nmodular learning methods are capable of long-term spatial and semantic\nunderstanding and achieve state-of-the-art results on various navigation tasks.",
          "link": "http://arxiv.org/abs/2106.13415",
          "publishedOn": "2021-06-28T01:57:54.800Z",
          "wordCount": 671,
          "title": "Building Intelligent Autonomous Navigation Agents. (arXiv:2106.13415v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianwen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>",
          "description": "Conventional saliency prediction models typically learn a deterministic\nmapping from images to the corresponding ground truth saliency maps. In this\npaper, we study the saliency prediction problem from the perspective of\ngenerative models by learning a conditional probability distribution over\nsaliency maps given an image, and treating the prediction as a sampling\nprocess. Specifically, we propose a generative cooperative saliency prediction\nframework based on the generative cooperative networks, where a conditional\nlatent variable model and a conditional energy-based model are jointly trained\nto predict saliency in a cooperative manner. We call our model the SalCoopNets.\nThe latent variable model serves as a fast but coarse predictor to efficiently\nproduce an initial prediction, which is then refined by the iterative Langevin\nrevision of the energy-based model that serves as a fine predictor. Such a\ncoarse-to-fine cooperative saliency prediction strategy offers the best of both\nworlds. Moreover, we generalize our framework to the scenario of weakly\nsupervised saliency prediction, where saliency annotation of training images is\npartially observed, by proposing a cooperative learning while recovering\nstrategy. Lastly, we show that the learned energy function can serve as a\nrefinement module that can refine the results of other pre-trained saliency\nprediction models. Experimental results show that our generative model can\nachieve state-of-the-art performance. Our code is publicly available at:\n\\url{https://github.com/JingZhang617/SalCoopNets}.",
          "link": "http://arxiv.org/abs/2106.13389",
          "publishedOn": "2021-06-28T01:57:54.791Z",
          "wordCount": 649,
          "title": "Energy-Based Generative Cooperative Saliency Prediction. (arXiv:2106.13389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1\">Long Hoang Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thao Minh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vuong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>",
          "description": "Video Question Answering (Video QA) is a powerful testbed to develop new AI\ncapabilities. This task necessitates learning to reason about objects,\nrelations, and events across visual and linguistic domains in space-time.\nHigh-level reasoning demands lifting from associative visual pattern\nrecognition to symbol-like manipulation over objects, their behavior and\ninteractions. Toward reaching this goal we propose an object-oriented reasoning\napproach in that video is abstracted as a dynamic stream of interacting\nobjects. At each stage of the video event flow, these objects interact with\neach other, and their interactions are reasoned about with respect to the query\nand under the overall context of a video. This mechanism is materialized into a\nfamily of general-purpose neural units and their multi-level architecture\ncalled Hierarchical Object-oriented Spatio-Temporal Reasoning (HOSTR) networks.\nThis neural model maintains the objects' consistent lifelines in the form of a\nhierarchically nested spatio-temporal graph. Within this graph, the dynamic\ninteractive object-oriented representations are built up along the video\nsequence, hierarchically abstracted in a bottom-up manner, and converge toward\nthe key information for the correct answer. The method is evaluated on multiple\nmajor Video QA datasets and establishes new state-of-the-arts in these tasks.\nAnalysis into the model's behavior indicates that object-oriented reasoning is\na reliable, interpretable and efficient approach to Video QA.",
          "link": "http://arxiv.org/abs/2106.13432",
          "publishedOn": "2021-06-28T01:57:54.783Z",
          "wordCount": 654,
          "title": "Hierarchical Object-oriented Spatio-Temporal Reasoning for Video Question Answering. (arXiv:2106.13432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1\">Giorgos Kordopatis-Zilos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1\">Christos Tzelepis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>",
          "description": "In this paper, we address the problem of high performance and computationally\nefficient content-based video retrieval in large-scale datasets. Current\nmethods typically propose either: (i) fine-grained approaches employing\nspatio-temporal representations and similarity calculations, achieving high\nperformance at a high computational cost or (ii) coarse-grained approaches\nrepresenting/indexing videos as global vectors, where the spatio-temporal\nstructure is lost, providing low performance but also having low computational\ncost. In this work, we propose a Knowledge Distillation framework, which we\ncall Distill-and-Select (DnS), that starting from a well-performing\nfine-grained Teacher Network learns: a) Student Networks at different retrieval\nperformance and computational efficiency trade-offs and b) a Selection Network\nthat at test time rapidly directs samples to the appropriate student to\nmaintain both high retrieval performance and high computational efficiency. We\ntrain several students with different architectures and arrive at different\ntrade-offs of performance and efficiency, i.e., speed and storage requirements,\nincluding fine-grained students that store index videos using binary\nrepresentations. Importantly, the proposed scheme allows Knowledge Distillation\nin large, unlabelled datasets -- this leads to good students. We evaluate DnS\non five public datasets on three different video retrieval tasks and\ndemonstrate a) that our students achieve state-of-the-art performance in\nseveral cases and b) that our DnS framework provides an excellent trade-off\nbetween retrieval performance, computational speed, and storage space. In\nspecific configurations, our method achieves similar mAP with the teacher but\nis 20 times faster and requires 240 times less storage space. Our collected\ndataset and implementation are publicly available:\nhttps://github.com/mever-team/distill-and-select.",
          "link": "http://arxiv.org/abs/2106.13266",
          "publishedOn": "2021-06-28T01:57:54.776Z",
          "wordCount": 699,
          "title": "DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>",
          "description": "One-class learning is the classic problem of fitting a model to the data for\nwhich annotations are available only for a single class. In this paper, we\nexplore novel objectives for one-class learning, which we collectively refer to\nas Generalized One-class Discriminative Subspaces (GODS). Our key idea is to\nlearn a pair of complementary classifiers to flexibly bound the one-class data\ndistribution, where the data belongs to the positive half-space of one of the\nclassifiers in the complementary pair and to the negative half-space of the\nother. To avoid redundancy while allowing non-linearity in the classifier\ndecision surfaces, we propose to design each classifier as an orthonormal frame\nand seek to learn these frames via jointly optimizing for two conflicting\nobjectives, namely: i) to minimize the distance between the two frames, and ii)\nto maximize the margin between the frames and the data. The learned orthonormal\nframes will thus characterize a piecewise linear decision surface that allows\nfor efficient inference, while our objectives seek to bound the data within a\nminimal volume that maximizes the decision margin, thereby robustly capturing\nthe data distribution. We explore several variants of our formulation under\ndifferent constraints on the constituent classifiers, including kernelized\nfeature maps. We demonstrate the empirical benefits of our approach via\nexperiments on data from several applications in computer vision, such as\nanomaly detection in video sequences, human poses, and human activities. We\nalso explore the generality and effectiveness of GODS for non-vision tasks via\nexperiments on several UCI datasets, demonstrating state-of-the-art results.",
          "link": "http://arxiv.org/abs/2106.13272",
          "publishedOn": "2021-06-28T01:57:54.764Z",
          "wordCount": 699,
          "title": "Generalized One-Class Learning Using Pairs of Complementary Classifiers. (arXiv:2106.13272v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13299",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1\">Julien Philip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgenthaler_S/0/1/0/all/0/1\">S&#xe9;bastien Morgenthaler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharbi_M/0/1/0/all/0/1\">Micha&#xeb;l Gharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drettakis_G/0/1/0/all/0/1\">George Drettakis</a>",
          "description": "We introduce a neural relighting algorithm for captured indoors scenes, that\nallows interactive free-viewpoint navigation. Our method allows illumination to\nbe changed synthetically, while coherently rendering cast shadows and complex\nglossy materials. We start with multiple images of the scene and a 3D mesh\nobtained by multi-view stereo (MVS) reconstruction. We assume that lighting is\nwell-explained as the sum of a view-independent diffuse component and a\nview-dependent glossy term concentrated around the mirror reflection direction.\nWe design a convolutional network around input feature maps that facilitate\nlearning of an implicit representation of scene materials and illumination,\nenabling both relighting and free-viewpoint navigation. We generate these input\nmaps by exploiting the best elements of both image-based and physically-based\nrendering. We sample the input views to estimate diffuse scene irradiance, and\ncompute the new illumination caused by user-specified light sources using path\ntracing. To facilitate the network's understanding of materials and synthesize\nplausible glossy reflections, we reproject the views and compute mirror images.\nWe train the network on a synthetic dataset where each scene is also\nreconstructed with MVS. We show results of our algorithm relighting real indoor\nscenes and performing free-viewpoint navigation with complex and realistic\nglossy reflections, which so far remained out of reach for view-synthesis\ntechniques.",
          "link": "http://arxiv.org/abs/2106.13299",
          "publishedOn": "2021-06-28T01:57:54.640Z",
          "wordCount": 645,
          "title": "Free-viewpoint Indoor Neural Relighting from Multi-view Stereo. (arXiv:2106.13299v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Federated learning (FL) collaboratively aggregates a shared global model\ndepending on multiple local clients, while keeping the training data\ndecentralized in order to preserve data privacy. However, standard FL methods\nignore the noisy client issue, which may harm the overall performance of the\naggregated model. In this paper, we first analyze the noisy client statement,\nand then model noisy clients with different noise distributions (e.g.,\nBernoulli and truncated Gaussian distributions). To learn with noisy clients,\nwe propose a simple yet effective FL framework, named Federated Noisy Client\nLearning (Fed-NCL), which is a plug-and-play algorithm and contains two main\ncomponents: a data quality measurement (DQM) to dynamically quantify the data\nquality of each participating client, and a noise robust aggregation (NRA) to\nadaptively aggregate the local models of each client by jointly considering the\namount of local training data and the data quality of each client. Our Fed-NCL\ncan be easily applied in any standard FL workflow to handle the noisy client\nissue. Experimental results on various datasets demonstrate that our algorithm\nboosts the performances of different state-of-the-art systems with noisy\nclients.",
          "link": "http://arxiv.org/abs/2106.13239",
          "publishedOn": "2021-06-28T01:57:54.589Z",
          "wordCount": 623,
          "title": "Federated Noisy Client Learning. (arXiv:2106.13239v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1\">Bowen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1\">Aude Oliva</a>",
          "description": "The self-attention-based model, transformer, is recently becoming the leading\nbackbone in the field of computer vision. In spite of the impressive success\nmade by transformers in a variety of vision tasks, it still suffers from heavy\ncomputation and intensive memory cost. To address this limitation, this paper\npresents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).\nWe start by observing a large amount of redundant computation, mainly spent on\nuncorrelated input patches, and then introduce an interpretable module to\ndynamically and gracefully drop these redundant patches. This novel framework\nis then extended to a hierarchical structure, where uncorrelated tokens at\ndifferent stages are gradually removed, resulting in a considerable shrinkage\nof computational cost. We include extensive experiments on both image and video\ntasks, where our method could deliver up to 1.4X speed-up for state-of-the-art\nmodels like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy.\nMore importantly, contrary to other acceleration approaches, our method is\ninherently interpretable with substantial visual evidence, making vision\ntransformer closer to a more human-understandable architecture while being\nlighter. We demonstrate that the interpretability that naturally emerged in our\nframework can outperform the raw attention learned by the original visual\ntransformer, as well as those generated by off-the-shelf interpretation\nmethods, with both qualitative and quantitative results. Project Page:\nthis http URL",
          "link": "http://arxiv.org/abs/2106.12620",
          "publishedOn": "2021-06-25T02:00:47.371Z",
          "wordCount": 652,
          "title": "IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers. (arXiv:2106.12620v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xuelong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>",
          "description": "Meta-learning model can quickly adapt to new tasks using few-shot labeled\ndata. However, despite achieving good generalization on few-shot classification\ntasks, it is still challenging to improve the adversarial robustness of the\nmeta-learning model in few-shot learning. Although adversarial training (AT)\nmethods such as Adversarial Query (AQ) can improve the adversarially robust\nperformance of meta-learning models, AT is still computationally expensive\ntraining. On the other hand, meta-learning models trained with AT will drop\nsignificant accuracy on the original clean images. This paper proposed a\nmeta-learning method on the adversarially robust neural network called\nLong-term Cross Adversarial Training (LCAT). LCAT will update meta-learning\nmodel parameters cross along the natural and adversarial sample distribution\ndirection with long-term to improve both adversarial and clean few-shot\nclassification accuracy. Due to cross-adversarial training, LCAT only needs\nhalf of the adversarial training epoch than AQ, resulting in a low adversarial\ntraining computation. Experiment results show that LCAT achieves superior\nperformance both on the clean and adversarial few-shot classification accuracy\nthan SOTA adversarial training methods for meta-learning models.",
          "link": "http://arxiv.org/abs/2106.12900",
          "publishedOn": "2021-06-25T02:00:47.304Z",
          "wordCount": 620,
          "title": "Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (arXiv:2106.12900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_I/0/1/0/all/0/1\">Indrani Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_I/0/1/0/all/0/1\">Indranil Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omprakash_C/0/1/0/all/0/1\">Charitha Omprakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stober_S/0/1/0/all/0/1\">Sebastian Stober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikulovic_S/0/1/0/all/0/1\">Sanja Mikulovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_P/0/1/0/all/0/1\">Pavol Bauer</a>",
          "description": "The assessment of laboratory animal behavior is of central interest in modern\nneuroscience research. Behavior is typically studied in terms of pose changes,\nwhich are ideally captured in three dimensions. This requires triangulation\nover a multi-camera system which view the animal from different angles.\nHowever, this is challenging in realistic laboratory setups due to occlusions\nand other technical constrains. Here we propose the usage of lift-pose models\nthat allow for robust 3D pose estimation of freely moving rodents from a single\nview camera view. To obtain high-quality training data for the pose-lifting, we\nfirst perform geometric calibration in a camera setup involving bottom as well\nas side views of the behaving animal. We then evaluate the performance of two\npreviously proposed model architectures under given inference perspectives and\nconclude that reliable 3D pose inference can be obtained using temporal\nconvolutions. With this work we would like to contribute to a more robust and\ndiverse behavior tracking of freely moving rodents for a wide range of\nexperiments and setups in the neuroscience community.",
          "link": "http://arxiv.org/abs/2106.12993",
          "publishedOn": "2021-06-25T02:00:47.267Z",
          "wordCount": 649,
          "title": "Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data. (arXiv:2106.12993v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12776",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lyngdoh_R/0/1/0/all/0/1\">Rosly Boy Lyngdoh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahadevan_A/0/1/0/all/0/1\">Anand S Sahadevan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahmad_T/0/1/0/all/0/1\">Touseef Ahmad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rathore_P/0/1/0/all/0/1\">Pradyuman Singh Rathore</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mishra_M/0/1/0/all/0/1\">Manoj Mishra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_P/0/1/0/all/0/1\">Praveen Kumar Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Misra_A/0/1/0/all/0/1\">Arundhati Misra</a>",
          "description": "Advanced Hyperspectral Data Analysis Software (AVHYAS) plugin is a python3\nbased quantum GIS (QGIS) plugin designed to process and analyse hyperspectral\n(Hx) images. It is developed to guarantee full usage of present and future Hx\nairborne or spaceborne sensors and provides access to advanced algorithms for\nHx data processing. The software is freely available and offers a range of\nbasic and advanced tools such as atmospheric correction (for airborne AVIRISNG\nimage), standard processing tools as well as powerful machine learning and Deep\nLearning interfaces for Hx data analysis.",
          "link": "http://arxiv.org/abs/2106.12776",
          "publishedOn": "2021-06-25T02:00:47.097Z",
          "wordCount": 566,
          "title": "AVHYAS: A Free and Open Source QGIS Plugin for Advanced Hyperspectral Image Analysis. (arXiv:2106.12776v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1\">Samay Pashine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandiya_S/0/1/0/all/0/1\">Sagar Mandiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Praveen Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_R/0/1/0/all/0/1\">Rashid Sheikh</a>",
          "description": "Deep Learning as a field has been successfully used to solve a plethora of\ncomplex problems, the likes of which we could not have imagined a few decades\nback. But as many benefits as it brings, there are still ways in which it can\nbe used to bring harm to our society. Deep fakes have been proven to be one\nsuch problem, and now more than ever, when any individual can create a fake\nimage or video simply using an application on the smartphone, there need to be\nsome countermeasures, with which we can detect if the image or video is a fake\nor real and dispose of the problem threatening the trustworthiness of online\ninformation. Although the Deep fakes created by neural networks, may seem to be\nas real as a real image or video, it still leaves behind spatial and temporal\ntraces or signatures after moderation, these signatures while being invisible\nto a human eye can be detected with the help of a neural network trained to\nspecialize in Deep fake detection. In this paper, we analyze several such\nstates of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception\nNet) and compare them against each other, to find an optimal solution for\nvarious scenarios like real-time deep fake detection to be deployed in online\nsocial media platforms where the classification should be made as fast as\npossible or for a small news agency where the classification need not be in\nreal-time but requires utmost accuracy.",
          "link": "http://arxiv.org/abs/2106.12605",
          "publishedOn": "2021-06-25T02:00:46.822Z",
          "wordCount": 713,
          "title": "Deep Fake Detection: Survey of Facial Manipulation Detection Solutions. (arXiv:2106.12605v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1\">Samay Pashine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_R/0/1/0/all/0/1\">Ritik Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushwah_R/0/1/0/all/0/1\">Rishika Kushwah</a>",
          "description": "The reliance of humans over machines has never been so high such that from\nobject classification in photographs to adding sound to silent movies\neverything can be performed with the help of deep learning and machine learning\nalgorithms. Likewise, Handwritten text recognition is one of the significant\nareas of research and development with a streaming number of possibilities that\ncould be attained. Handwriting recognition (HWR), also known as Handwritten\nText Recognition (HTR), is the ability of a computer to receive and interpret\nintelligible handwritten input from sources such as paper documents,\nphotographs, touch-screens and other devices [1]. Apparently, in this paper, we\nhave performed handwritten digit recognition with the help of MNIST datasets\nusing Support Vector Machines (SVM), Multi-Layer Perceptron (MLP) and\nConvolution Neural Network (CNN) models. Our main objective is to compare the\naccuracy of the models stated above along with their execution time to get the\nbest possible model for digit recognition.",
          "link": "http://arxiv.org/abs/2106.12614",
          "publishedOn": "2021-06-25T02:00:46.719Z",
          "wordCount": 620,
          "title": "Handwritten Digit Recognition using Machine and Deep Learning Algorithms. (arXiv:2106.12614v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keltjens_B/0/1/0/all/0/1\">Benjamin Keltjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijk_T/0/1/0/all/0/1\">Tom van Dijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1\">Guido de Croon</a>",
          "description": "Self-supervised deep learning methods have leveraged stereo images for\ntraining monocular depth estimation. Although these methods show strong results\non outdoor datasets such as KITTI, they do not match performance of supervised\nmethods on indoor environments with camera rotation. Indoor, rotated scenes are\ncommon for less constrained applications and pose problems for two reasons:\nabundance of low texture regions and increased complexity of depth cues for\nimages under rotation. In an effort to extend self-supervised learning to more\ngeneralised environments we propose two additions. First, we propose a novel\nFilled Disparity Loss term that corrects for ambiguity of image reconstruction\nerror loss in textureless regions. Specifically, we interpolate disparity in\nuntextured regions, using the estimated disparity from surrounding textured\nareas, and use L1 loss to correct the original estimation. Our experiments show\nthat depth estimation is substantially improved on low-texture scenes, without\nany loss on textured scenes, when compared to Monodepth by Godard et al.\nSecondly, we show that training with an application's representative rotations,\nin both pitch and roll, is sufficient to significantly improve performance over\nthe entire range of expected rotation. We demonstrate that depth estimation is\nsuccessfully generalised as performance is not lost when evaluated on test sets\nwith no camera rotation. Together these developments enable a broader use of\nself-supervised learning of monocular depth estimation for complex\nenvironments.",
          "link": "http://arxiv.org/abs/2106.12958",
          "publishedOn": "2021-06-25T02:00:46.620Z",
          "wordCount": 664,
          "title": "Self-Supervised Monocular Depth Estimation of Untextured Indoor Rotated Scenes. (arXiv:2106.12958v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.07639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michalke_T/0/1/0/all/0/1\">Thomas Michalke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Di Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaser_C/0/1/0/all/0/1\">Claudius Gl&#xe4;ser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timm_F/0/1/0/all/0/1\">Fabian Timm</a>",
          "description": "Lane detection is an essential part of the perception sub-architecture of any\nautomated driving (AD) or advanced driver assistance system (ADAS). When\nfocusing on low-cost, large scale products for automated driving, model-driven\napproaches for the detection of lane markings have proven good performance.\nMore recently, data-driven approaches have been proposed that target the\ndrivable area / freespace mainly in inner-city applications. Focus of these\napproaches is less on lane-based driving due to the fact that the lane concept\ndoes not fully apply in unstructured, residential inner-city environments.\nSo-far the concept of drivable area is seldom used for highway and inter-urban\napplications due to the specific requirements of these scenarios that require\nclear lane associations of all traffic participants. We believe that\nlane-based, mapless driving in inter-urban and highway scenarios is still not\nfully handled with sufficient robustness and availability. Especially for\nchallenging weather situations such as heavy rain, fog, low-standing sun,\ndarkness or reflections in puddles, the mapless detection of lane markings\ndecreases significantly or completely fails. We see potential in applying\nspecifically designed data-driven freespace approaches in more lane-based\ndriving applications for highways and inter-urban use. Therefore, we propose to\nclassify specifically a drivable corridor of the ego lane on pixel level with a\ndeep learning approach. Our approach is kept computationally efficient with\nonly 0.66 million parameters allowing its application in large scale products.\nThus, we were able to easily integrate into an online AD system of a test\nvehicle. We demonstrate the performance of our approach under challenging\nconditions qualitatively and quantitatively in comparison to a state-of-the-art\nmodel-driven approach.",
          "link": "http://arxiv.org/abs/2004.07639",
          "publishedOn": "2021-06-25T02:00:46.614Z",
          "wordCount": 748,
          "title": "Where can I drive? A System Approach: Deep Ego Corridor Estimation for Robust Automated Driving. (arXiv:2004.07639v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changgong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Feiying Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Accurate lighting estimation is challenging yet critical to many computer\nvision and computer graphics tasks such as high-dynamic-range (HDR) relighting.\nExisting approaches model lighting in either frequency domain or spatial domain\nwhich is insufficient to represent the complex lighting conditions in scenes\nand tends to produce inaccurate estimation. This paper presents NeedleLight, a\nnew lighting estimation model that represents illumination with needlets and\nallows lighting estimation in both frequency domain and spatial domain jointly.\nAn optimal thresholding function is designed to achieve sparse needlets which\ntrims redundant lighting parameters and demonstrates superior localization\nproperties for illumination representation. In addition, a novel spherical\ntransport loss is designed based on optimal transport theory which guides to\nregress lighting representation parameters with consideration of the spatial\ninformation. Furthermore, we propose a new metric that is concise yet effective\nby directly evaluating the estimated illumination maps rather than rendered\nimages. Extensive experiments show that NeedleLight achieves superior lighting\nestimation consistently across multiple evaluation metrics as compared with\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.13090",
          "publishedOn": "2021-06-25T02:00:46.597Z",
          "wordCount": 613,
          "title": "Sparse Needlets for Lighting Estimation with Spherical Transport Loss. (arXiv:2106.13090v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.04960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baoquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xutao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhichao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lisai Zhang</a>",
          "description": "Few-shot learning is a challenging task, which aims to learn a classifier for\nnovel classes with few examples. Pre-training based meta-learning methods\neffectively tackle the problem by pre-training a feature extractor and then\nfine-tuning it through the nearest centroid based meta-learning. However,\nresults show that the fine-tuning step makes very marginal improvements. In\nthis paper, 1) we figure out the key reason, i.e., in the pre-trained feature\nspace, the base classes already form compact clusters while novel classes\nspread as groups with large variances, which implies that fine-tuning the\nfeature extractor is less meaningful; 2) instead of fine-tuning the feature\nextractor, we focus on estimating more representative prototypes during\nmeta-learning. Consequently, we propose a novel prototype completion based\nmeta-learning framework. This framework first introduces primitive knowledge\n(i.e., class-level part or attribute annotations) and extracts representative\nattribute features as priors. Then, we design a prototype completion network to\nlearn to complete prototypes with these priors. To avoid the prototype\ncompletion error caused by primitive knowledge noises or class differences, we\nfurther develop a Gaussian based prototype fusion strategy that combines the\nmean-based and completed prototypes by exploiting the unlabeled samples.\nExtensive experiments show that our method: (i) can obtain more accurate\nprototypes; (ii) outperforms state-of-the-art techniques by 2% - 9% in terms of\nclassification accuracy. Our code is available online.",
          "link": "http://arxiv.org/abs/2009.04960",
          "publishedOn": "2021-06-25T02:00:46.214Z",
          "wordCount": 725,
          "title": "Prototype Completion with Primitive Knowledge for Few-Shot Learning. (arXiv:2009.04960v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guzhov_A/0/1/0/all/0/1\">Andrey Guzhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1\">Federico Raue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1\">J&#xf6;rn Hees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>",
          "description": "In the past, the rapidly evolving field of sound classification greatly\nbenefited from the application of methods from other domains. Today, we observe\nthe trend to fuse domain-specific tasks and approaches together, which provides\nthe community with new outstanding models.\n\nIn this work, we present an extension of the CLIP model that handles audio in\naddition to text and images. Our proposed model incorporates the ESResNeXt\naudio-model into the CLIP framework using the AudioSet dataset. Such a\ncombination enables the proposed model to perform bimodal and unimodal\nclassification and querying, while keeping CLIP's ability to generalize to\nunseen datasets in a zero-shot inference fashion.\n\nAudioCLIP achieves new state-of-the-art results in the Environmental Sound\nClassification (ESC) task, out-performing other approaches by reaching\naccuracies of 90.07% on the UrbanSound8K and 97.15% on the ESC-50 datasets.\nFurther it sets new baselines in the zero-shot ESC-task on the same datasets\n68.78% and 69.40%, respectively).\n\nFinally, we also assess the cross-modal querying performance of the proposed\nmodel as well as the influence of full and partial training on the results. For\nthe sake of reproducibility, our code is published.",
          "link": "http://arxiv.org/abs/2106.13043",
          "publishedOn": "2021-06-25T02:00:46.136Z",
          "wordCount": 631,
          "title": "AudioCLIP: Extending CLIP to Image, Text and Audio. (arXiv:2106.13043v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yihang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>",
          "description": "Recently, language-guided global image editing draws increasing attention\nwith growing application potentials. However, previous GAN-based methods are\nnot only confined to domain-specific, low-resolution data but also lacking in\ninterpretability. To overcome the collective difficulties, we develop a\ntext-to-operation model to map the vague editing language request into a series\nof editing operations, e.g., change contrast, brightness, and saturation. Each\noperation is interpretable and differentiable. Furthermore, the only\nsupervision in the task is the target image, which is insufficient for a stable\ntraining of sequential decisions. Hence, we propose a novel operation planning\nalgorithm to generate possible editing sequences from the target image as\npseudo ground truth. Comparison experiments on the newly collected MA5k-Req\ndataset and GIER dataset show the advantages of our methods. Code is available\nat https://jshi31.github.io/T2ONet.",
          "link": "http://arxiv.org/abs/2106.13156",
          "publishedOn": "2021-06-25T02:00:46.129Z",
          "wordCount": 572,
          "title": "Learning by Planning: Language-Guided Global Image Editing. (arXiv:2106.13156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jizong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>",
          "description": "The scarcity of labeled data often impedes the application of deep learning\nto the segmentation of medical images. Semi-supervised learning seeks to\novercome this limitation by exploiting unlabeled examples in the learning\nprocess. In this paper, we present a novel semi-supervised segmentation method\nthat leverages mutual information (MI) on categorical distributions to achieve\nboth global representation invariance and local smoothness. In this method, we\nmaximize the MI for intermediate feature embeddings that are taken from both\nthe encoder and decoder of a segmentation network. We first propose a global MI\nloss constraining the encoder to learn an image representation that is\ninvariant to geometric transformations. Instead of resorting to\ncomputationally-expensive techniques for estimating the MI on continuous\nfeature embeddings, we use projection heads to map them to a discrete cluster\nassignment where MI can be computed efficiently. Our method also includes a\nlocal MI loss to promote spatial consistency in the feature maps of the decoder\nand provide a smoother segmentation. Since mutual information does not require\na strict ordering of clusters in two different assignments, we incorporate a\nfinal consistency regularization loss on the output which helps align the\ncluster labels throughout the network. We evaluate the method on four\nchallenging publicly-available datasets for medical image segmentation.\nExperimental results show our method to outperform recently-proposed approaches\nfor semi-supervised segmentation and provide an accuracy near to full\nsupervision while training with very few annotated images.",
          "link": "http://arxiv.org/abs/2103.04813",
          "publishedOn": "2021-06-25T02:00:46.008Z",
          "wordCount": 703,
          "title": "Boosting Semi-supervised Image Segmentation with Global and Local Mutual Information Regularization. (arXiv:2103.04813v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Punnakkal_A/0/1/0/all/0/1\">Abhinanda R. Punnakkal</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1\">Arjun Chandrasekaran</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1\">Nikos Athanasiou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Quiros_Ramirez_A/0/1/0/all/0/1\">Alejandra Quiros-Ramirez</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a> (1) ((1) Max Planck Institute for Intelligent Systems, (2) Universitat Konstanz)",
          "description": "Understanding the semantics of human movement -- the what, how and why of the\nmovement -- is an important problem that requires datasets of human actions\nwith semantic labels. Existing datasets take one of two approaches. Large-scale\nvideo datasets contain many action labels but do not contain ground-truth 3D\nhuman motion. Alternatively, motion-capture (mocap) datasets have precise body\nmotions but are limited to a small number of actions. To address this, we\npresent BABEL, a large dataset with language labels describing the actions\nbeing performed in mocap sequences. BABEL consists of action labels for about\n43 hours of mocap sequences from AMASS. Action labels are at two levels of\nabstraction -- sequence labels describe the overall action in the sequence, and\nframe labels describe all actions in every frame of the sequence. Each frame\nlabel is precisely aligned with the duration of the corresponding action in the\nmocap sequence, and multiple actions can overlap. There are over 28k sequence\nlabels, and 63k frame labels in BABEL, which belong to over 250 unique action\ncategories. Labels from BABEL can be leveraged for tasks like action\nrecognition, temporal action localization, motion synthesis, etc. To\ndemonstrate the value of BABEL as a benchmark, we evaluate the performance of\nmodels on 3D action recognition. We demonstrate that BABEL poses interesting\nlearning challenges that are applicable to real-world scenarios, and can serve\nas a useful benchmark of progress in 3D action recognition. The dataset,\nbaseline method, and evaluation code is made available, and supported for\nacademic research purposes at https://babel.is.tue.mpg.de/.",
          "link": "http://arxiv.org/abs/2106.09696",
          "publishedOn": "2021-06-25T02:00:45.986Z",
          "wordCount": 745,
          "title": "BABEL: Bodies, Action and Behavior with English Labels. (arXiv:2106.09696v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schulze_H/0/1/0/all/0/1\">Henning Schulze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1\">Dogucan Yaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>",
          "description": "Generating images according to natural language descriptions is a challenging\ntask. Prior research has mainly focused to enhance the quality of generation by\ninvestigating the use of spatial attention and/or textual attention thereby\nneglecting the relationship between channels. In this work, we propose the\nCombined Attention Generative Adversarial Network (CAGAN) to generate\nphoto-realistic images according to textual descriptions. The proposed CAGAN\nutilises two attention models: word attention to draw different sub-regions\nconditioned on related words; and squeeze-and-excitation attention to capture\nnon-linear interaction among channels. With spectral normalisation to stabilise\ntraining, our proposed CAGAN improves the state of the art on the IS and FID on\nthe CUB dataset and the FID on the more challenging COCO dataset. Furthermore,\nwe demonstrate that judging a model by a single evaluation metric can be\nmisleading by developing an additional model adding local self-attention which\nscores a higher IS, outperforming the state of the art on the CUB dataset, but\ngenerates unrealistic images through feature repetition.",
          "link": "http://arxiv.org/abs/2104.12663",
          "publishedOn": "2021-06-25T02:00:45.981Z",
          "wordCount": 619,
          "title": "CAGAN: Text-To-Image Generation with Combined Attention GANs. (arXiv:2104.12663v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>",
          "description": "This paper introduces EfficientNetV2, a new family of convolutional networks\nthat have faster training speed and better parameter efficiency than previous\nmodels. To develop this family of models, we use a combination of\ntraining-aware neural architecture search and scaling, to jointly optimize\ntraining speed and parameter efficiency. The models were searched from the\nsearch space enriched with new ops such as Fused-MBConv. Our experiments show\nthat EfficientNetV2 models train much faster than state-of-the-art models while\nbeing up to 6.8x smaller.\n\nOur training can be further sped up by progressively increasing the image\nsize during training, but it often causes a drop in accuracy. To compensate for\nthis accuracy drop, we propose to adaptively adjust regularization (e.g.,\ndropout and data augmentation) as well, such that we can achieve both fast\ntraining and good accuracy.\n\nWith progressive learning, our EfficientNetV2 significantly outperforms\nprevious models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on\nthe same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on\nImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while\ntraining 5x-11x faster using the same computing resources. Code will be\navailable at https://github.com/google/automl/tree/master/efficientnetv2.",
          "link": "http://arxiv.org/abs/2104.00298",
          "publishedOn": "2021-06-25T02:00:45.853Z",
          "wordCount": 663,
          "title": "EfficientNetV2: Smaller Models and Faster Training. (arXiv:2104.00298v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.11034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shijie Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanrong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "Nowadays, vision-based computing tasks play an important role in various\nreal-world applications. However, many vision computing tasks, e.g. semantic\nsegmentation, are usually computationally expensive, posing a challenge to the\ncomputing systems that are resource-constrained but require fast response\nspeed. Therefore, it is valuable to develop accurate and real-time vision\nprocessing models that only require limited computational resources. To this\nend, we propose the Spatial-detail Guided Context Propagation Network (SGCPNet)\nfor achieving real-time semantic segmentation. In SGCPNet, we propose the\nstrategy of spatial-detail guided context propagation. It uses the spatial\ndetails of shallow layers to guide the propagation of the low-resolution global\ncontexts, in which the lost spatial information can be effectively\nreconstructed. In this way, the need for maintaining high-resolution features\nalong the network is freed, therefore largely improving the model efficiency.\nOn the other hand, due to the effective reconstruction of spatial details, the\nsegmentation accuracy can be still preserved. In the experiments, we validate\nthe effectiveness and efficiency of the proposed SGCPNet model. On the\nCitysacpes dataset, for example, our SGCPNet achieves 69.5 % mIoU segmentation\naccuracy, while its speed reaches 178.5 FPS on 768x1536 images on a GeForce GTX\n1080 Ti GPU card.",
          "link": "http://arxiv.org/abs/2005.11034",
          "publishedOn": "2021-06-25T02:00:45.810Z",
          "wordCount": 686,
          "title": "Real-time Semantic Segmentation via Spatial-detail Guided Context Propagation. (arXiv:2005.11034v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dymond_J/0/1/0/all/0/1\">Jack Dymond</a>",
          "description": "When machine learning models encounter data which is out of the distribution\non which they were trained they have a tendency to behave poorly, most\nprominently over-confidence in erroneous predictions. Such behaviours will have\ndisastrous effects on real-world machine learning systems. In this field\ngraceful degradation refers to the optimisation of model performance as it\nencounters this out-of-distribution data. This work presents a definition and\ndiscussion of graceful degradation and where it can be applied in deployed\nvisual systems. Following this a survey of relevant areas is undertaken,\nnovelly splitting the graceful degradation problem into active and passive\napproaches. In passive approaches, graceful degradation is handled and achieved\nby the model in a self-contained manner, in active approaches the model is\nupdated upon encountering epistemic uncertainties. This work communicates the\nimportance of the problem and aims to prompt the development of machine\nlearning strategies that are aware of graceful degradation.",
          "link": "http://arxiv.org/abs/2106.11119",
          "publishedOn": "2021-06-25T02:00:45.805Z",
          "wordCount": 592,
          "title": "Graceful Degradation and Related Fields. (arXiv:2106.11119v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chiou_M/0/1/0/all/0/1\">Meng-Jiun Chiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Chun-Yu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li-Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roger Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "Detecting human-object interactions (HOI) is an important step toward a\ncomprehensive visual understanding of machines. While detecting non-temporal\nHOIs (e.g., sitting on a chair) from static images is feasible, it is unlikely\neven for humans to guess temporal-related HOIs (e.g., opening/closing a door)\nfrom a single video frame, where the neighboring frames play an essential role.\nHowever, conventional HOI methods operating on only static images have been\nused to predict temporal-related interactions, which is essentially guessing\nwithout temporal contexts and may lead to sub-optimal performance. In this\npaper, we bridge this gap by detecting video-based HOIs with explicit temporal\ninformation. We first show that a naive temporal-aware variant of a common\naction detection baseline does not work on video-based HOIs due to a\nfeature-inconsistency issue. We then propose a simple yet effective\narchitecture named Spatial-Temporal HOI Detection (ST-HOI) utilizing temporal\ninformation such as human and object trajectories, correctly-localized visual\nfeatures, and spatial-temporal masking pose features. We construct a new video\nHOI benchmark dubbed VidHOI where our proposed approach serves as a solid\nbaseline.",
          "link": "http://arxiv.org/abs/2105.11731",
          "publishedOn": "2021-06-25T02:00:45.784Z",
          "wordCount": 666,
          "title": "ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos. (arXiv:2105.11731v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kaicong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_S/0/1/0/all/0/1\">Sven Simon</a>",
          "description": "Deformable image registration is a fundamental task in medical imaging. Due\nto the large computational complexity of deformable registration of volumetric\nimages, conventional iterative methods usually face the tradeoff between the\nregistration accuracy and the computation time in practice. In order to boost\nthe registration performance in both accuracy and runtime, we propose a fast\nconvolutional neural network. Specially, to efficiently utilize the memory\nresources and enlarge the model capacity, we adopt additive forwarding instead\nof channel concatenation and deepen the network in each encoder and decoder\nstage. To facilitate the learning efficiency, we leverage skip connection\nwithin the encoder and decoder stages to enable residual learning and employ an\nauxiliary loss at the bottom layer with lowest resolution to involve deep\nsupervision. Particularly, the low-resolution auxiliary loss is weighted by an\nexponentially decayed parameter during the training phase. In conjunction with\nthe main loss in high-resolution grid, a coarse-to-fine learning strategy is\nachieved. Last but not least, we introduce an auxiliary loss based on the\nsegmentation prior to improve the registration performance in Dice score.\nComparing to the auxiliary loss using average Dice score, the proposed\nmulti-label segmentation loss does not induce additional memory cost in the\ntraining phase and can be employed on images with arbitrary amount of\ncategories. In the experiments, we show FDRN outperforms the existing\nstate-of-the-art registration methods for brain MR images by resorting to the\ncompact network structure and efficient learning. Besides, FDRN is a\ngeneralized framework for image registration which is not confined to a\nparticular type of medical images or anatomy.",
          "link": "http://arxiv.org/abs/2011.02307",
          "publishedOn": "2021-06-25T02:00:45.735Z",
          "wordCount": 744,
          "title": "FDRN: A Fast Deformable Registration Network for Medical Images. (arXiv:2011.02307v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheth_I/0/1/0/all/0/1\">Ivaxi Sheth</a>",
          "description": "Understanding accurate information on human behaviours is one of the most\nimportant tasks in machine intelligence. Human Activity Recognition that aims\nto understand human activities from a video is a challenging task due to\nvarious problems including background, camera motion and dataset variations.\nThis paper proposes two CNN based architectures with three streams which allow\nthe model to exploit the dataset under different settings. The three pathways\nare differentiated in frame rates. The single pathway, operates at a single\nframe rate captures spatial information, the slow pathway operates at low frame\nrates captures the spatial information and the fast pathway operates at high\nframe rates that capture fine temporal information. Post CNN encoders, we add\nbidirectional LSTM and attention heads respectively to capture the context and\ntemporal features. By experimenting with various algorithms on UCF-101,\nKinetics-600 and AVA dataset, we observe that the proposed models achieve\nstate-of-art performance for human action recognition task.",
          "link": "http://arxiv.org/abs/2104.13051",
          "publishedOn": "2021-06-25T02:00:45.724Z",
          "wordCount": 607,
          "title": "Three-stream network for enriched Action Recognition. (arXiv:2104.13051v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Si Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1\">Samy Bengio</a>",
          "description": "Attentional mechanisms are order-invariant. Positional encoding is a crucial\ncomponent to allow attention-based deep model architectures such as Transformer\nto address sequences or images where the position of information matters. In\nthis paper, we propose a novel positional encoding method based on learnable\nFourier features. Instead of hard-coding each position as a token or a vector,\nwe represent each position, which can be multi-dimensional, as a trainable\nencoding based on learnable Fourier feature mapping, modulated with a\nmulti-layer perceptron. The representation is particularly advantageous for a\nspatial multi-dimensional position, e.g., pixel positions on an image, where\n$L_2$ distances or more complex positional relationships need to be captured.\nOur experiments based on several public benchmark tasks show that our learnable\nFourier feature representation for multi-dimensional positional encoding\noutperforms existing methods by both improving the accuracy and allowing faster\nconvergence.",
          "link": "http://arxiv.org/abs/2106.02795",
          "publishedOn": "2021-06-25T02:00:45.715Z",
          "wordCount": 598,
          "title": "Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding. (arXiv:2106.02795v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10745",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1\">Adrian Celaya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Actor_J/0/1/0/all/0/1\">Jonas A. Actor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muthusivarajan_R/0/1/0/all/0/1\">Rajarajeswari Muthusivarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gates_E/0/1/0/all/0/1\">Evan Gates</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_C/0/1/0/all/0/1\">Caroline Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schellingerhout_D/0/1/0/all/0/1\">Dawid Schellingerhout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1\">Beatrice Riviere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1\">David Fuentes</a>",
          "description": "Medical imaging deep learning models are often large and complex, requiring\nspecialized hardware to train and evaluate these models. To address such\nissues, we propose the PocketNet paradigm to reduce the size of deep learning\nmodels by throttling the growth of the number of channels in convolutional\nneural networks. We demonstrate that, for a range of segmentation and\nclassification tasks, PocketNet architectures produce results comparable to\nthat of conventional neural networks while reducing the number of parameters by\nmultiple orders of magnitude, using up to 90% less GPU memory, and speeding up\ntraining times by up to 40%, thereby allowing such models to be trained and\ndeployed in resource-constrained settings.",
          "link": "http://arxiv.org/abs/2104.10745",
          "publishedOn": "2021-06-25T02:00:45.674Z",
          "wordCount": 586,
          "title": "PocketNet: A Smaller Neural Network for Medical Image Analysis. (arXiv:2104.10745v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1\">Jia Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>",
          "description": "The vision community is witnessing a modeling shift from CNNs to\nTransformers, where pure Transformer architectures have attained top accuracy\non the major video recognition benchmarks. These video models are all built on\nTransformer layers that globally connect patches across the spatial and\ntemporal dimensions. In this paper, we instead advocate an inductive bias of\nlocality in video Transformers, which leads to a better speed-accuracy\ntrade-off compared to previous approaches which compute self-attention globally\neven with spatial-temporal factorization. The locality of the proposed video\narchitecture is realized by adapting the Swin Transformer designed for the\nimage domain, while continuing to leverage the power of pre-trained image\nmodels. Our approach achieves state-of-the-art accuracy on a broad range of\nvideo recognition benchmarks, including on action recognition (84.9 top-1\naccuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less\npre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1\naccuracy on Something-Something v2). The code and models will be made publicly\navailable at https://github.com/SwinTransformer/Video-Swin-Transformer.",
          "link": "http://arxiv.org/abs/2106.13230",
          "publishedOn": "2021-06-25T02:00:45.650Z",
          "wordCount": 606,
          "title": "Video Swin Transformer. (arXiv:2106.13230v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_D/0/1/0/all/0/1\">Divyansh Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiayu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>",
          "description": "DNN-based face recognition models require large centrally aggregated face\ndatasets for training. However, due to the growing data privacy concerns and\nlegal restrictions, accessing and sharing face datasets has become exceedingly\ndifficult. We propose FedFace, a federated learning (FL) framework for\ncollaborative learning of face recognition models in a privacy-aware manner.\nFedFace utilizes the face images available on multiple clients to learn an\naccurate and generalizable face recognition model where the face images stored\nat each client are neither shared with other clients nor the central host and\neach client is a mobile device containing face images pertaining to only the\nowner of the device (one identity per client). Our experiments show the\neffectiveness of FedFace in enhancing the verification performance of\npre-trained face recognition system on standard face verification benchmarks\nnamely LFW, IJB-A, and IJB-C.",
          "link": "http://arxiv.org/abs/2104.03008",
          "publishedOn": "2021-06-25T02:00:45.641Z",
          "wordCount": 598,
          "title": "FedFace: Collaborative Learning of Face Recognition Model. (arXiv:2104.03008v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Keunhong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_U/0/1/0/all/0/1\">Utkarsh Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1\">Peter Hedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaziz_S/0/1/0/all/0/1\">Sofien Bouaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_D/0/1/0/all/0/1\">Dan B Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1\">Ricardo Martin-Brualla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1\">Steven M. Seitz</a>",
          "description": "Neural Radiance Fields (NeRF) are able to reconstruct scenes with\nunprecedented fidelity, and various recent works have extended NeRF to handle\ndynamic scenes. A common approach to reconstruct such non-rigid scenes is\nthrough the use of a learned deformation field mapping from coordinates in each\ninput image into a canonical template coordinate space. However, these\ndeformation-based approaches struggle to model changes in topology, as\ntopological changes require a discontinuity in the deformation field, but these\ndeformation fields are necessarily continuous. We address this limitation by\nlifting NeRFs into a higher dimensional space, and by representing the 5D\nradiance field corresponding to each individual input image as a slice through\nthis \"hyper-space\". Our method is inspired by level set methods, which model\nthe evolution of surfaces as slices through a higher dimensional surface. We\nevaluate our method on two tasks: (i) interpolating smoothly between \"moments\",\ni.e., configurations of the scene, seen in the input images while maintaining\nvisual plausibility, and (ii) novel-view synthesis at fixed moments. We show\nthat our method, which we dub HyperNeRF, outperforms existing methods on both\ntasks by significant margins. Compared to Nerfies, HyperNeRF reduces average\nerror rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as\nmeasured by LPIPS.",
          "link": "http://arxiv.org/abs/2106.13228",
          "publishedOn": "2021-06-25T02:00:45.625Z",
          "wordCount": 664,
          "title": "HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. (arXiv:2106.13228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1\">Kevin Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1\">Fredrick Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yingyong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jack Xin</a>",
          "description": "Convolutional neural networks (CNNs) have developed to become powerful models\nfor various computer vision tasks ranging from object detection to semantic\nsegmentation. However, most of state-of-the-art CNNs can not be deployed\ndirectly on edge devices such as smartphones and drones, which need low latency\nunder limited power and memory bandwidth. One popular, straightforward approach\nto compressing CNNs is network slimming, which imposes $\\ell_1$ regularization\non the channel-associated scaling factors via the batch normalization layers\nduring training. Network slimming thereby identifies insignificant channels\nthat can be pruned for inference. In this paper, we propose replacing the\n$\\ell_1$ penalty with an alternative sparse, nonconvex penalty in order to\nyield a more compressed and/or accurate CNN architecture. We investigate\n$\\ell_p (0 < p < 1)$, transformed $\\ell_1$ (T$\\ell_1$), minimax concave penalty\n(MCP), and smoothly clipped absolute deviation (SCAD) due to their recent\nsuccesses and popularity in solving sparse optimization problems, such as\ncompressed sensing and variable selection. We demonstrate the effectiveness of\nnetwork slimming with nonconvex penalties on VGGNet, Densenet, and Resnet on\nstandard image classification datasets. Based on the numerical experiments,\nT$\\ell_1$ preserves model accuracy against channel pruning, $\\ell_{1/2, 3/4}$\nyield better compressed models with similar accuracies after retraining as\n$\\ell_1$, and MCP and SCAD provide more accurate models after retraining with\nsimilar compression as $\\ell_1$. Network slimming with T$\\ell_1$ regularization\nalso outperforms the latest Bayesian modification of network slimming in\ncompressing a CNN architecture in terms of memory storage while preserving its\nmodel accuracy after channel pruning.",
          "link": "http://arxiv.org/abs/2010.01242",
          "publishedOn": "2021-06-25T02:00:45.613Z",
          "wordCount": 736,
          "title": "Improving Network Slimming with Nonconvex Regularization. (arXiv:2010.01242v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanjun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuanping Hu</a>",
          "description": "Temporal grounding aims to localize temporal boundaries within untrimmed\nvideos by language queries, but it faces the challenge of two types of\ninevitable human uncertainties: query uncertainty and label uncertainty. The\ntwo uncertainties stem from human subjectivity, leading to limited\ngeneralization ability of temporal grounding. In this work, we propose a novel\nDeNet (Decoupling and De-bias) to embrace human uncertainty: Decoupling - We\nexplicitly disentangle each query into a relation feature and a modified\nfeature. The relation feature, which is mainly based on skeleton-like words\n(including nouns and verbs), aims to extract basic and consistent information\nin the presence of query uncertainty. Meanwhile, modified feature assigned with\nstyle-like words (including adjectives, adverbs, etc) represents the subjective\ninformation, and thus brings personalized predictions; De-bias - We propose a\nde-bias mechanism to generate diverse predictions, aim to alleviate the bias\ncaused by single-style annotations in the presence of label uncertainty.\nMoreover, we put forward new multi-label metrics to diversify the performance\nevaluation. Extensive experiments show that our approach is more effective and\nrobust than state-of-the-arts on Charades-STA and ActivityNet Captions\ndatasets.",
          "link": "http://arxiv.org/abs/2103.16848",
          "publishedOn": "2021-06-25T02:00:45.599Z",
          "wordCount": 655,
          "title": "Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding. (arXiv:2103.16848v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.03376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Persand_K/0/1/0/all/0/1\">Kaveena Persand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Andrew Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1\">David Gregg</a>",
          "description": "The computation and memory needed for Convolutional Neural Network (CNN)\ninference can be reduced by pruning weights from the trained network. Pruning\nis guided by a pruning saliency, which heuristically approximates the change in\nthe loss function associated with the removal of specific weights. Many pruning\nsignals have been proposed, but the performance of each heuristic depends on\nthe particular trained network. This leaves the data scientist with a difficult\nchoice. When using any one saliency metric for the entire pruning process, we\nrun the risk of the metric assumptions being invalidated, leading to poor\ndecisions being made by the metric. Ideally we could combine the best aspects\nof different saliency metrics. However, despite an extensive literature review,\nwe are unable to find any prior work on composing different saliency metrics.\nThe chief difficulty lies in combining the numerical output of different\nsaliency metrics, which are not directly comparable.\n\nWe propose a method to compose several primitive pruning saliencies, to\nexploit the cases where each saliency measure does well. Our experiments show\nthat the composition of saliencies avoids many poor pruning choices identified\nby individual saliencies. In most cases our method finds better selections than\neven the best individual pruning saliency.",
          "link": "http://arxiv.org/abs/2004.03376",
          "publishedOn": "2021-06-25T02:00:45.594Z",
          "wordCount": 690,
          "title": "Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle. (arXiv:2004.03376v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1\">Giang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Explaining the decisions of an Artificial Intelligence (AI) model is\nincreasingly critical in many real-world, high-stake applications. Hundreds of\npapers have either proposed new feature attribution methods, discussed or\nharnessed these tools in their work. However, despite humans being the target\nend-users, most attribution methods were only evaluated on proxy\nautomatic-evaluation metrics. In this paper, we conduct the first, large-scale\nuser study on 320 lay and 11 expert users to shed light on the effectiveness of\nstate-of-the-art attribution methods in assisting humans in ImageNet\nclassification, Stanford Dogs fine-grained classification, and these two tasks\nbut when the input image contains adversarial perturbations. We found that, in\noverall, feature attribution is surprisingly not more effective than showing\nhumans nearest training-set examples. On a hard task of fine-grained dog\ncategorization, presenting attribution maps to humans does not help, but\ninstead hurts the performance of human-AI teams compared to AI alone.\nImportantly, we found automatic attribution-map evaluation measures to\ncorrelate poorly with the actual human-AI team performance. Our findings\nencourage the community to rigorously test their methods on the downstream\nhuman-in-the-loop applications and to rethink the existing evaluation metrics.",
          "link": "http://arxiv.org/abs/2105.14944",
          "publishedOn": "2021-06-25T02:00:45.588Z",
          "wordCount": 653,
          "title": "The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (arXiv:2105.14944v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aadith Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotti_S/0/1/0/all/0/1\">Sasikanth Kotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>",
          "description": "Given the increase in the use of personal data for training Deep Neural\nNetworks (DNNs) in tasks such as medical imaging and diagnosis, differentially\nprivate training of DNNs is surging in importance and there is a huge body of\nwork focusing on providing better privacy-utility trade-off. However, little\nattention is given to the interpretability of these models, and how the\napplication of DP affects the quality of interpretations. We propose an\nextensive study into the effects of DP training on DNNs, especially on medical\nimaging applications, on the APTOS dataset.",
          "link": "http://arxiv.org/abs/2106.13203",
          "publishedOn": "2021-06-25T02:00:45.571Z",
          "wordCount": 544,
          "title": "When Differential Privacy Meets Interpretability: A Case Study. (arXiv:2106.13203v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mejjati_Y/0/1/0/all/0/1\">Youssef A.Mejjati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milefchik_I/0/1/0/all/0/1\">Isa Milefchik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1\">Oliver Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwang In Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1\">James Tompkin</a>",
          "description": "We present an algorithm that learns a coarse 3D representation of objects\nfrom unposed multi-view 2D mask supervision, then uses it to generate detailed\nmask and image texture. In contrast to existing voxel-based methods for unposed\nobject reconstruction, our approach learns to represent the generated shape and\npose with a set of self-supervised canonical 3D anisotropic Gaussians via a\nperspective camera, and a set of per-image transforms. We show that this\napproach can robustly estimate a 3D space for the camera and object, while\nrecent baselines sometimes struggle to reconstruct coherent 3D spaces in this\nsetting. We show results on synthetic datasets with realistic lighting, and\ndemonstrate object insertion with interactive posing. With our work, we help\nmove towards structured representations that handle more real-world variation\nin learning-based object reconstruction.",
          "link": "http://arxiv.org/abs/2106.13215",
          "publishedOn": "2021-06-25T02:00:45.565Z",
          "wordCount": 574,
          "title": "GaussiGAN: Controllable Image Synthesis with 3D Gaussians from Unposed Silhouettes. (arXiv:2106.13215v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Babaeizadeh_M/0/1/0/all/0/1\">Mohammad Babaeizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffar_M/0/1/0/all/0/1\">Mohammad Taghi Saffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erhan_D/0/1/0/all/0/1\">Dumitru Erhan</a>",
          "description": "An agent that is capable of predicting what happens next can perform a\nvariety of tasks through planning with no additional training. Furthermore,\nsuch an agent can internally represent the complex dynamics of the real-world\nand therefore can acquire a representation useful for a variety of visual\nperception tasks. This makes predicting the future frames of a video,\nconditioned on the observed past and potentially future actions, an interesting\ntask which remains exceptionally challenging despite many recent advances.\nExisting video prediction models have shown promising results on simple narrow\nbenchmarks but they generate low quality predictions on real-life datasets with\nmore complicated dynamics or broader domain. There is a growing body of\nevidence that underfitting on the training data is one of the primary causes\nfor the low quality predictions. In this paper, we argue that the inefficient\nuse of parameters in the current video models is the main reason for\nunderfitting. Therefore, we introduce a new architecture, named FitVid, which\nis capable of severe overfitting on the common benchmarks while having similar\nparameter count as the current state-of-the-art models. We analyze the\nconsequences of overfitting, illustrating how it can produce unexpected\noutcomes such as generating high quality output by repeating the training data,\nand how it can be mitigated using existing image augmentation techniques. As a\nresult, FitVid outperforms the current state-of-the-art models across four\ndifferent video prediction benchmarks on four different metrics.",
          "link": "http://arxiv.org/abs/2106.13195",
          "publishedOn": "2021-06-25T02:00:45.559Z",
          "wordCount": 675,
          "title": "FitVid: Overfitting in Pixel-Level Video Prediction. (arXiv:2106.13195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.03840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_C/0/1/0/all/0/1\">Claudio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berretti_S/0/1/0/all/0/1\">Stefano Berretti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pala_P/0/1/0/all/0/1\">Pietro Pala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>",
          "description": "The 3D Morphable Model (3DMM) is a powerful statistical tool for representing\n3D face shapes. To build a 3DMM, a training set of face scans in full\npoint-to-point correspondence is required, and its modeling capabilities\ndirectly depend on the variability contained in the training data. Thus, to\nincrease the descriptive power of the 3DMM, establishing a dense correspondence\nacross heterogeneous scans with sufficient diversity in terms of identities,\nethnicities, or expressions becomes essential. In this manuscript, we present a\nfully automatic approach that leverages a 3DMM to transfer its dense semantic\nannotation across raw 3D faces, establishing a dense correspondence between\nthem. We propose a novel formulation to learn a set of sparse deformation\ncomponents with local support on the face that, together with an original\nnon-rigid deformation algorithm, allow the 3DMM to precisely fit unseen faces\nand transfer its semantic annotation. We extensively experimented our approach,\nshowing it can effectively generalize to highly diverse samples and accurately\nestablish a dense correspondence even in presence of complex facial\nexpressions. The accuracy of the dense registration is demonstrated by building\na heterogeneous, large-scale 3DMM from more than 9,000 fully registered scans\nobtained by joining three large datasets together.",
          "link": "http://arxiv.org/abs/2006.03840",
          "publishedOn": "2021-06-25T02:00:45.554Z",
          "wordCount": 709,
          "title": "A Sparse and Locally Coherent Morphable Face Model for Dense Semantic Correspondence Across Heterogeneous 3D Faces. (arXiv:2006.03840v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13150",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lotz_J/0/1/0/all/0/1\">Johannes Lotz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weiss_N/0/1/0/all/0/1\">Nick Weiss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laak_J/0/1/0/all/0/1\">Jeroen van der Laak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+StefanHeldmann/0/1/0/all/0/1\">StefanHeldmann</a>",
          "description": "We compare variational image registration in consectutive and re-stained\nsections from histopathology. We present a fully-automatic algorithm for\nnon-parametric (nonlinear) image registration and apply it to a previously\nexisting dataset from the ANHIR challenge (230 slide pairs, consecutive\nsections) and a new dataset (hybrid re-stained and consecutive, 81 slide pairs,\nca. 3000 landmarks) which is made publicly available. Registration\nhyperparameters are obtained in the ANHIR dataset and applied to the new\ndataset without modification. In the new dataset, landmark errors after\nregistration range from 13.2 micrometers for consecutive sections to 1\nmicrometer for re-stained sections. We observe that non-parametric registration\nleads to lower landmark errors in both cases, even though the effect is smaller\nin re-stained sections. The nucleus-level alignment after non-parametric\nregistration of re-stained sections provides a valuable tool to generate\nautomatic ground-truth for machine learning applications in histopathology.",
          "link": "http://arxiv.org/abs/2106.13150",
          "publishedOn": "2021-06-25T02:00:45.548Z",
          "wordCount": 589,
          "title": "High-resolution Image Registration of Consecutive and Re-stained Sections in Histopathology. (arXiv:2106.13150v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1\">Baaria Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1\">Poorya Aghdaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "Face recognition systems are extremely vulnerable to morphing attacks, in\nwhich a morphed facial reference image can be successfully verified as two or\nmore distinct identities. In this paper, we propose a morph attack detection\nalgorithm that leverages an undecimated 2D Discrete Wavelet Transform (DWT) for\nidentifying morphed face images. The core of our framework is that artifacts\nresulting from the morphing process that are not discernible in the image\ndomain can be more easily identified in the spatial frequency domain. A\ndiscriminative wavelet sub-band can accentuate the disparity between a real and\na morphed image. To this end, multi-level DWT is applied to all images,\nyielding 48 mid and high-frequency sub-bands each. The entropy distributions\nfor each sub-band are calculated separately for both bona fide and morph\nimages. For some of the sub-bands, there is a marked difference between the\nentropy of the sub-band in a bona fide image and the identical sub-band's\nentropy in a morphed image. Consequently, we employ Kullback-Liebler Divergence\n(KLD) to exploit these differences and isolate the sub-bands that are the most\ndiscriminative. We measure how discriminative a sub-band is by its KLD value\nand the 22 sub-bands with the highest KLD values are chosen for network\ntraining. Then, we train a deep Siamese neural network using these 22 selected\nsub-bands for differential morph attack detection. We examine the efficacy of\ndiscriminative wavelet sub-bands for morph attack detection and show that a\ndeep neural network trained on these sub-bands can accurately identify morph\nimagery.",
          "link": "http://arxiv.org/abs/2106.13178",
          "publishedOn": "2021-06-25T02:00:45.531Z",
          "wordCount": 711,
          "title": "Differential Morph Face Detection using Discriminative Wavelet Sub-bands. (arXiv:2106.13178v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khoshrou_A/0/1/0/all/0/1\">Abdolrahman Khoshrou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Eric J. Pauwels</a>",
          "description": "Singular Value Decomposition (SVD) and its close relative, Principal\nComponent Analysis (PCA), are well-known linear matrix decomposition techniques\nthat are widely used in applications such as dimension reduction and\nclustering. However, an important limitation of SVD/PCA is its sensitivity to\nnoise in the input data. In this paper, we take another look at the problem of\nregularisation and show that different formulations of the minimisation problem\nlead to qualitatively different solutions.",
          "link": "http://arxiv.org/abs/2106.12955",
          "publishedOn": "2021-06-25T02:00:45.526Z",
          "wordCount": 508,
          "title": "Regularisation for PCA- and SVD-type matrix factorisations. (arXiv:2106.12955v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Ke-Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Bo-Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuan-Yu Chen</a>",
          "description": "In this paper, inspired by the successes of visionlanguage pre-trained models\nand the benefits from training with adversarial attacks, we present a novel\ntransformerbased cross-modal fusion modeling by incorporating the both notions\nfor VQA challenge 2021. Specifically, the proposed model is on top of the\narchitecture of VinVL model [19], and the adversarial training strategy [4] is\napplied to make the model robust and generalized. Moreover, two implementation\ntricks are also used in our system to obtain better results. The experiments\ndemonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.",
          "link": "http://arxiv.org/abs/2106.13033",
          "publishedOn": "2021-06-25T02:00:45.521Z",
          "wordCount": 548,
          "title": "A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021. (arXiv:2106.13033v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chiyu &quot;Max&quot; Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jingwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>",
          "description": "We present ShapeFlow, a flow-based model for learning a deformation space for\nentire classes of 3D shapes with large intra-class variations. ShapeFlow allows\nlearning a multi-template deformation space that is agnostic to shape topology,\nyet preserves fine geometric details. Different from a generative space where a\nlatent vector is directly decoded into a shape, a deformation space decodes a\nvector into a continuous flow that can advect a source shape towards a target.\nSuch a space naturally allows the disentanglement of geometric style (coming\nfrom the source) and structural pose (conforming to the target). We parametrize\nthe deformation between geometries as a learned continuous flow field via a\nneural network and show that such deformations can be guaranteed to have\ndesirable properties, such as be bijectivity, freedom from self-intersections,\nor volume preservation. We illustrate the effectiveness of this learned\ndeformation space for various downstream applications, including shape\ngeneration via deformation, geometric style transfer, unsupervised learning of\na consistent parameterization for entire classes of shapes, and shape\ninterpolation.",
          "link": "http://arxiv.org/abs/2006.07982",
          "publishedOn": "2021-06-25T02:00:45.506Z",
          "wordCount": 634,
          "title": "ShapeFlow: Learnable Deformations Among 3D Shapes. (arXiv:2006.07982v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.03936",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dorman_K/0/1/0/all/0/1\">Karin S. Dorman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "Mining clusters from data is an important endeavor in many applications. The\n$k$-means method is a popular, efficient, and distribution-free approach for\nclustering numerical-valued data, but does not apply for categorical-valued\nobservations. The $k$-modes method addresses this lacuna by replacing the\nEuclidean with the Hamming distance and the means with the modes in the\n$k$-means objective function. We provide a novel, computationally efficient\nimplementation of $k$-modes, called OTQT. We prove that OTQT finds updates to\nimprove the objective function that are undetectable to existing $k$-modes\nalgorithms. Although slightly slower per iteration due to algorithmic\ncomplexity, OTQT is always more accurate per iteration and almost always faster\n(and only barely slower on some datasets) to the final optimum. Thus, we\nrecommend OTQT as the preferred, default algorithm for $k$-modes optimization.",
          "link": "http://arxiv.org/abs/2006.03936",
          "publishedOn": "2021-06-25T02:00:45.499Z",
          "wordCount": 614,
          "title": "An Efficient $k$-modes Algorithm for Clustering Categorical Datasets. (arXiv:2006.03936v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13064",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Yang_T/0/1/0/all/0/1\">Tianjie Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1\">Yaoru Luo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_G/0/1/0/all/0/1\">Ge Yang</a>",
          "description": "Super-resolution microscopy overcomes the diffraction limit of conventional\nlight microscopy in spatial resolution. By providing novel spatial or\nspatio-temporal information on biological processes at nanometer resolution\nwith molecular specificity, it plays an increasingly important role in life\nsciences. However, its technical limitations require trade-offs to balance its\nspatial resolution, temporal resolution, and light exposure of samples.\nRecently, deep learning has achieved breakthrough performance in many image\nprocessing and computer vision tasks. It has also shown great promise in\npushing the performance envelope of super-resolution microscopy. In this brief\nReview, we survey recent advances in using deep learning to enhance performance\nof super-resolution microscopy. We focus primarily on how deep learning\nad-vances reconstruction of super-resolution images. Related key technical\nchallenges are discussed. Despite the challenges, deep learning is set to play\nan indispensable and transformative role in the development of super-resolution\nmicroscopy. We conclude with an outlook on how deep learning could shape the\nfuture of this new generation of light microscopy technology.",
          "link": "http://arxiv.org/abs/2106.13064",
          "publishedOn": "2021-06-25T02:00:45.484Z",
          "wordCount": 608,
          "title": "Advancing biological super-resolution microscopy through deep learning: a brief review. (arXiv:2106.13064v1 [physics.bio-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hossam_M/0/1/0/all/0/1\">Mahmoud Hossam</a>",
          "description": "Real-time remote sensing applications like search and rescue missions,\nmilitary target detection, environmental monitoring, hazard prevention and\nother time-critical applications require onboard real time processing\ncapabilities or autonomous decision making. Some unmanned remote systems like\nsatellites are physically remote from their operators, and all control of the\nspacecraft and data returned by the spacecraft must be transmitted over a\nwireless radio link. This link may not be available for extended periods when\nthe satellite is out of line of sight of its ground station. Therefore,\nlightweight, small size and low power consumption hardware is essential for\nonboard real time processing systems. With increasing dimensionality, size and\nresolution of recent hyperspectral imaging sensors, additional challenges are\nposed upon remote sensing processing systems and more capable computing\narchitectures are needed. Graphical Processing Units (GPUs) emerged as\npromising architecture for light weight high performance computing that can\naddress these computational requirements for onboard systems. The goal of this\nstudy is to build high performance methods for onboard hyperspectral analysis.\nWe propose accelerated methods for the well-known recursive hierarchical\nsegmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a\nGPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the\nNational Aeronautics and Space Administration (NASA), which is designed to\nprovide rich classification information with several output levels. The\nachieved speedups by parallel solutions compared to CPU sequential\nimplementations are 21x for parallel single GPU and 240x for hybrid multi-node\ncomputer clusters with 16 computing nodes. The energy consumption is reduced to\n74% using a single GPU compared to the equivalent parallel CPU cluster.",
          "link": "http://arxiv.org/abs/2106.12942",
          "publishedOn": "2021-06-25T02:00:45.478Z",
          "wordCount": 715,
          "title": "High Performance Hyperspectral Image Classification using Graphics Processing Units. (arXiv:2106.12942v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1\">Niranjan Balachandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel Rubin</a>",
          "description": "Collaborative learning, which enables collaborative and decentralized\ntraining of deep neural networks at multiple institutions in a\nprivacy-preserving manner, is rapidly emerging as a valuable technique in\nhealthcare applications. However, its distributed nature often leads to\nsignificant heterogeneity in data distributions across institutions. Existing\ncollaborative learning approaches generally do not account for the presence of\nheterogeneity in data among institutions, or only mildly skewed label\ndistributions are studied. In this paper, we present a novel generative replay\nstrategy to address the challenge of data heterogeneity in collaborative\nlearning methods. Instead of directly training a model for task performance, we\nleverage recent image synthesis techniques to develop a novel dual model\narchitecture: a primary model learns the desired task, and an auxiliary\n\"generative replay model\" either synthesizes images that closely resemble the\ninput images or helps extract latent variables. The generative replay strategy\nis flexible to use, can either be incorporated into existing collaborative\nlearning methods to improve their capability of handling data heterogeneity\nacross institutions, or be used as a novel and individual collaborative\nlearning framework (termed FedReplay) to reduce communication cost.\nExperimental results demonstrate the capability of the proposed method in\nhandling heterogeneous data across institutions. On highly heterogeneous data\npartitions, our model achieves ~4.88% improvement in the prediction accuracy on\na diabetic retinopathy classification dataset, and ~49.8% reduction of mean\nabsolution value on a Bone Age prediction dataset, respectively, compared to\nthe state-of-the art collaborative learning methods.",
          "link": "http://arxiv.org/abs/2106.13208",
          "publishedOn": "2021-06-25T02:00:45.471Z",
          "wordCount": 683,
          "title": "Handling Data Heterogeneity with Generative Replay in Collaborative Learning for Medical Imaging. (arXiv:2106.13208v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yunqiu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_M/0/1/0/all/0/1\">Mochu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>",
          "description": "Camouflaged object detection (COD) aims to segment camouflaged objects hiding\nin the environment, which is challenging due to the similar appearance of\ncamouflaged objects and their surroundings. Research in biology suggests that\ndepth can provide useful object localization cues for camouflaged object\ndiscovery, as all the animals have 3D perception ability. However, the depth\ninformation has not been exploited for camouflaged object detection. To explore\nthe contribution of depth for camouflage detection, we present a depth-guided\ncamouflaged object detection network with pre-computed depth maps from existing\nmonocular depth estimation methods. Due to the domain gap between the depth\nestimation dataset and our camouflaged object detection dataset, the generated\ndepth may not be accurate enough to be directly used in our framework. We then\nintroduce a depth quality assessment module to evaluate the quality of depth\nbased on the model prediction from both RGB COD branch and RGB-D COD branch.\nDuring training, only high-quality depth is used to update the modal\ninteraction module for multi-modal learning. During testing, our depth quality\nassessment module can effectively determine the contribution of depth and\nselect the RGB branch or RGB-D branch for camouflage prediction. Extensive\nexperiments on various camouflaged object detection datasets prove the\neffectiveness of our solution in exploring the depth information for\ncamouflaged object detection. Our code and data is publicly available at:\n\\url{https://github.com/JingZhang617/RGBD-COD}.",
          "link": "http://arxiv.org/abs/2106.13217",
          "publishedOn": "2021-06-25T02:00:45.466Z",
          "wordCount": 676,
          "title": "Depth Confidence-aware Camouflaged Object Detection. (arXiv:2106.13217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stanley H. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ting Chen</a>",
          "description": "A massive number of traffic fatalities are due to driver errors. To reduce\nfatalities, developing intelligent driving systems assisting drivers to\nidentify potential risks is in urgent need. Risky situations are generally\ndefined based on collision prediction in existing research. However, collisions\nare only one type of risk in traffic scenarios. We believe a more generic\ndefinition is required. In this work, we propose a novel driver-centric\ndefinition of risk, i.e., risky objects influence driver behavior. Based on\nthis definition, a new task called risk object identification is introduced. We\nformulate the task as a cause-effect problem and present a novel two-stage risk\nobject identification framework, taking inspiration from models of situation\nawareness and causal inference. A driver-centric Risk Object Identification\n(ROI) dataset is curated to evaluate the proposed system. We demonstrate\nstate-of-the-art risk object identification performance compared with strong\nbaselines on the ROI dataset. In addition, we conduct extensive ablative\nstudies to justify our design choices.",
          "link": "http://arxiv.org/abs/2106.13201",
          "publishedOn": "2021-06-25T02:00:45.460Z",
          "wordCount": 591,
          "title": "Driver-centric Risk Object Identification. (arXiv:2106.13201v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soler_M/0/1/0/all/0/1\">Marti Soler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayache_S/0/1/0/all/0/1\">Stephane Ayache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guclu_U/0/1/0/all/0/1\">Umut Guclu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madadi_M/0/1/0/all/0/1\">Meysam Madadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baro_X/0/1/0/all/0/1\">Xavier Baro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalante_H/0/1/0/all/0/1\">Hugo Jair Escalante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a>",
          "description": "Dealing with incomplete information is a well studied problem in the context\nof machine learning and computational intelligence. However, in the context of\ncomputer vision, the problem has only been studied in specific scenarios (e.g.,\ncertain types of occlusions in specific types of images), although it is common\nto have incomplete information in visual data. This chapter describes the\ndesign of an academic competition focusing on inpainting of images and video\nsequences that was part of the competition program of WCCI2018 and had a\nsatellite event collocated with ECCV2018. The ChaLearn Looking at People\nInpainting Challenge aimed at advancing the state of the art on visual\ninpainting by promoting the development of methods for recovering missing and\noccluded information from images and video. Three tracks were proposed in which\nvisual inpainting might be helpful but still challenging: human body pose\nestimation, text overlays removal and fingerprint denoising. This chapter\ndescribes the design of the challenge, which includes the release of three\nnovel datasets, and the description of evaluation metrics, baselines and\nevaluation protocol. The results of the challenge are analyzed and discussed in\ndetail and conclusions derived from this event are outlined.",
          "link": "http://arxiv.org/abs/2106.13071",
          "publishedOn": "2021-06-25T02:00:45.445Z",
          "wordCount": 658,
          "title": "ChaLearn Looking at People: Inpainting and Denoising challenges. (arXiv:2106.13071v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morrison_K/0/1/0/all/0/1\">Katelyn Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilby_B/0/1/0/all/0/1\">Benjamin Gilby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipchak_C/0/1/0/all/0/1\">Colton Lipchak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattioli_A/0/1/0/all/0/1\">Adam Mattioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>",
          "description": "Recently, vision transformers and MLP-based models have been developed in\norder to address some of the prevalent weaknesses in convolutional neural\nnetworks. Due to the novelty of transformers being used in this domain along\nwith the self-attention mechanism, it remains unclear to what degree these\narchitectures are robust to corruptions. Despite some works proposing that data\naugmentation remains essential for a model to be robust against corruptions, we\npropose to explore the impact that the architecture has on corruption\nrobustness. We find that vision transformer architectures are inherently more\nrobust to corruptions than the ResNet-50 and MLP-Mixers. We also find that\nvision transformers with 5 times fewer parameters than a ResNet-50 have more\nshape bias. Our code is available to reproduce.",
          "link": "http://arxiv.org/abs/2106.13122",
          "publishedOn": "2021-06-25T02:00:45.439Z",
          "wordCount": 593,
          "title": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers. (arXiv:2106.13122v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xueqing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuxin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newsam_S/0/1/0/all/0/1\">Shawn Newsam</a>",
          "description": "Neural network-based semantic segmentation has achieved remarkable results\nwhen large amounts of annotated data are available, that is, in the supervised\ncase. However, such data is expensive to collect and so methods have been\ndeveloped to adapt models trained on related, often synthetic data for which\nlabels are readily available. Current adaptation approaches do not consider the\ndependence of the generalization/transferability of these models on network\narchitecture. In this paper, we perform neural architecture search (NAS) to\nprovide architecture-level perspective and analysis for domain adaptation. We\nidentify the optimization gap that exists when searching architectures for\nunsupervised domain adaptation which makes this NAS problem uniquely difficult.\nWe propose bridging this gap by using maximum mean discrepancy and regional\nweighted entropy to estimate the accuracy metric. Experimental results on\nseveral widely adopted benchmarks show that our proposed AutoAdapt framework\nindeed discovers architectures that improve the performance of a number of\nexisting adaptation techniques.",
          "link": "http://arxiv.org/abs/2106.13227",
          "publishedOn": "2021-06-25T02:00:45.434Z",
          "wordCount": 603,
          "title": "AutoAdapt: Automated Segmentation Network Search for Unsupervised Domain Adaptation. (arXiv:2106.13227v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhuang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Huajun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhihai Xu</a>",
          "description": "Moving target detection plays an important role in computer vision. However,\ntraditional algorithms such as frame difference and optical flow usually suffer\nfrom low accuracy or heavy computation. Recent algorithms such as deep\nlearning-based convolutional neural networks have achieved high accuracy and\nreal-time performance, but they usually need to know the classes of targets in\nadvance, which limits the practical applications. Therefore, we proposed a\nmodel free moving target detection algorithm. This algorithm extracts the\nmoving area through the difference of image features. Then, the color and\nlocation probability map of the moving area will be calculated through maximum\na posteriori probability. And the target probability map can be obtained\nthrough the dot multiply between the two maps. Finally, the optimal moving\ntarget area can be solved by stochastic gradient descent on the target\nprobability map. Results show that the proposed algorithm achieves the highest\naccuracy compared with state-of-the-art algorithms, without needing to know the\nclasses of targets. Furthermore, as the existing datasets are not suitable for\nmoving target detection, we proposed a method for producing evaluation dataset.\nBesides, we also proved the proposed algorithm can be used to assist target\ntracking.",
          "link": "http://arxiv.org/abs/2106.12966",
          "publishedOn": "2021-06-25T02:00:45.429Z",
          "wordCount": 637,
          "title": "Class agnostic moving target detection by color and location prediction of moving area. (arXiv:2106.12966v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13188",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ren_M/0/1/0/all/0/1\">Mengwei Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Heejong Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_N/0/1/0/all/0/1\">Neel Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gerig_G/0/1/0/all/0/1\">Guido Gerig</a>",
          "description": "Current deep learning approaches for diffusion MRI modeling circumvent the\nneed for densely-sampled diffusion-weighted images (DWIs) by directly\npredicting microstructural indices from sparsely-sampled DWIs. However, they\nimplicitly make unrealistic assumptions of static $q$-space sampling during\ntraining and reconstruction. Further, such approaches can restrict downstream\nusage of variably sampled DWIs for usages including the estimation of\nmicrostructural indices or tractography. We propose a generative adversarial\ntranslation framework for high-quality DWI synthesis with arbitrary $q$-space\nsampling given commonly acquired structural images (e.g., B0, T1, T2). Our\ntranslation network linearly modulates its internal representations conditioned\non continuous $q$-space information, thus removing the need for fixed sampling\nschemes. Moreover, this approach enables downstream estimation of high-quality\nmicrostructural maps from arbitrarily subsampled DWIs, which may be\nparticularly important in cases with sparsely sampled DWIs. Across several\nrecent methodologies, the proposed approach yields improved DWI synthesis\naccuracy and fidelity with enhanced downstream utility as quantified by the\naccuracy of scalar microstructure indices estimated from the synthesized\nimages. Code is available at\nhttps://github.com/mengweiren/q-space-conditioned-dwi-synthesis.",
          "link": "http://arxiv.org/abs/2106.13188",
          "publishedOn": "2021-06-25T02:00:45.410Z",
          "wordCount": 645,
          "title": "Q-space Conditioned Translation Networks for Directional Synthesis of Diffusion Weighted Images from Multi-modal Structural MRI. (arXiv:2106.13188v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_j/0/1/0/all/0/1\">jianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>",
          "description": "Temporal action localization aims to localize starting and ending time with\naction category. Limited by GPU memory, mainstream methods pre-extract features\nfor each video. Therefore, feature quality determines the upper bound of\ndetection performance. In this technical report, we explored classic\nconvolution-based backbones and the recent surge of transformer-based\nbackbones. We found that the transformer-based methods can achieve better\nclassification performance than convolution-based, but they cannot generate\naccuracy action proposals. In addition, extracting features with larger frame\nresolution to reduce the loss of spatial information can also effectively\nimprove the performance of temporal action localization. Finally, we achieve\n42.42% in terms of mAP on validation set with a single SlowFast feature by a\nsimple combination: BMN+TCANet, which is 1.87% higher than the result of 2020's\nmulti-model ensemble. Finally, we achieve Rank 1st on the CVPR2021 HACS\nsupervised Temporal Action Localization Challenge.",
          "link": "http://arxiv.org/abs/2106.13014",
          "publishedOn": "2021-06-25T02:00:45.398Z",
          "wordCount": 597,
          "title": "Exploring Stronger Feature for Temporal Action Localization. (arXiv:2106.13014v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rochow_A/0/1/0/all/0/1\">Andre Rochow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1\">Max Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1\">Michael Weinmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "We introduce FaDIV-Syn, a fast depth-independent view synthesis method. Our\nmulti-view approach addresses the problem that view synthesis methods are often\nlimited by their depth estimation stage, where incorrect depth predictions can\nlead to large projection errors. To avoid this issue, we efficiently warp\nmultiple input images into the target frame for a range of assumed depth\nplanes. The resulting tensor representation is fed into a U-Net-like CNN with\ngated convolutions, which directly produces the novel output view. We therefore\nside-step explicit depth estimation. This improves efficiency and performance\non transparent, reflective, and feature-less scene parts. FaDIV-Syn can handle\nboth interpolation and extrapolation tasks and outperforms state-of-the-art\nextrapolation methods on the large-scale RealEstate10k dataset. In contrast to\ncomparable methods, it is capable of real-time operation due to its lightweight\narchitecture. We further demonstrate data efficiency of FaDIV-Syn by training\nfrom fewer examples as well as its generalization to higher resolutions and\narbitrary depth ranges under severe depth discretization.",
          "link": "http://arxiv.org/abs/2106.13139",
          "publishedOn": "2021-06-25T02:00:45.392Z",
          "wordCount": 588,
          "title": "FaDIV-Syn: Fast Depth-Independent View Synthesis. (arXiv:2106.13139v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Sun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>",
          "description": "Leveraging the framework of Optimal Transport, we introduce a new family of\ngenerative autoencoders with a learnable prior, called Symmetric Wasserstein\nAutoencoders (SWAEs). We propose to symmetrically match the joint distributions\nof the observed data and the latent representation induced by the encoder and\nthe decoder. The resulting algorithm jointly optimizes the modelling losses in\nboth the data and the latent spaces with the loss in the data space leading to\nthe denoising effect. With the symmetric treatment of the data and the latent\nrepresentation, the algorithm implicitly preserves the local structure of the\ndata in the latent space. To further improve the quality of the latent\nrepresentation, we incorporate a reconstruction loss into the objective, which\nsignificantly benefits both the generation and reconstruction. We empirically\nshow the superior performance of SWAEs over the state-of-the-art generative\nautoencoders in terms of classification, reconstruction, and generation.",
          "link": "http://arxiv.org/abs/2106.13024",
          "publishedOn": "2021-06-25T02:00:45.386Z",
          "wordCount": 575,
          "title": "Symmetric Wasserstein Autoencoders. (arXiv:2106.13024v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>",
          "description": "Visual recognition has been dominated by convolutionalneural networks (CNNs)\nfor years. Though recently the pre-vailing vision transformers (ViTs) have\nshown great poten-tial of self-attention based models in ImageNet\nclassifica-tion, their performance is still inferior to latest SOTA CNNsif no\nextra data are provided. In this work, we aim to closethe performance gap and\ndemonstrate that attention-basedmodels are indeed able to outperform CNNs. We\nfound thatthe main factor limiting the performance of ViTs for Ima-geNet\nclassification is their low efficacy in encoding fine-level features into the\ntoken representations. To resolvethis, we introduce a noveloutlook attentionand\npresent asimple and general architecture, termed Vision Outlooker(VOLO). Unlike\nself-attention that focuses on global depen-dency modeling at a coarse level,\nthe outlook attention aimsto efficiently encode finer-level features and\ncontexts intotokens, which are shown to be critical for recognition\nper-formance but largely ignored by the self-attention. Experi-ments show that\nour VOLO achieves 87.1% top-1 accuracyon ImageNet-1K classification, being the\nfirst model exceed-ing 87% accuracy on this competitive benchmark, withoutusing\nany extra training data. In addition, the pre-trainedVOLO transfers well to\ndownstream tasks, such as seman-tic segmentation. We achieve 84.3% mIoU score\non thecityscapes validation set and 54.3% on the ADE20K valida-tion set. Code\nis available at https://github.com/sail-sg/volo.",
          "link": "http://arxiv.org/abs/2106.13112",
          "publishedOn": "2021-06-25T02:00:45.360Z",
          "wordCount": 642,
          "title": "VOLO: Vision Outlooker for Visual Recognition. (arXiv:2106.13112v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wickramanayake_S/0/1/0/all/0/1\">Sandareka Wickramanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wynne Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mong Li Lee</a>",
          "description": "Despite the remarkable performance, Deep Neural Networks (DNNs) behave as\nblack-boxes hindering user trust in Artificial Intelligence (AI) systems.\nResearch on opening black-box DNN can be broadly categorized into post-hoc\nmethods and inherently interpretable DNNs. While many surveys have been\nconducted on post-hoc interpretation methods, little effort is devoted to\ninherently interpretable DNNs. This paper provides a review of existing methods\nto develop DNNs with intrinsic interpretability, with a focus on Convolutional\nNeural Networks (CNNs). The aim is to understand the current progress towards\nfully interpretable DNNs that can cater to different interpretation\nrequirements. Finally, we identify gaps in current work and suggest potential\nresearch directions.",
          "link": "http://arxiv.org/abs/2106.13164",
          "publishedOn": "2021-06-25T02:00:45.352Z",
          "wordCount": 563,
          "title": "Towards Fully Interpretable Deep Neural Networks: Are We There Yet?. (arXiv:2106.13164v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12917",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohler_G/0/1/0/all/0/1\">Gregor K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jager_P/0/1/0/all/0/1\">Paul F. J&#xe4;ger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zimmerer_D/0/1/0/all/0/1\">David Zimmerer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neuberger_U/0/1/0/all/0/1\">Ulf Neuberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wick_W/0/1/0/all/0/1\">Wolfgang Wick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Debus_J/0/1/0/all/0/1\">J&#xfc;rgen Debus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heiland_S/0/1/0/all/0/1\">Sabine Heiland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bendszus_M/0/1/0/all/0/1\">Martin Bendszus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vollmuth_P/0/1/0/all/0/1\">Philipp Vollmuth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1\">Klaus H. Maier-Hein</a>",
          "description": "The ability to estimate how a tumor might evolve in the future could have\ntremendous clinical benefits, from improved treatment decisions to better dose\ndistribution in radiation therapy. Recent work has approached the glioma growth\nmodeling problem via deep learning and variational inference, thus learning\ngrowth dynamics entirely from a real patient data distribution. So far, this\napproach was constrained to predefined image acquisition intervals and\nsequences of fixed length, which limits its applicability in more realistic\nscenarios. We overcome these limitations by extending Neural Processes, a class\nof conditional generative models for stochastic time series, with a\nhierarchical multi-scale representation encoding including a spatio-temporal\nattention mechanism. The result is a learned growth model that can be\nconditioned on an arbitrary number of observations, and that can produce a\ndistribution of temporally consistent growth trajectories on a continuous time\naxis. On a dataset of 379 patients, the approach successfully captures both\nglobal and finer-grained variations in the images, exhibiting superior\nperformance compared to other learned growth models.",
          "link": "http://arxiv.org/abs/2106.12917",
          "publishedOn": "2021-06-25T02:00:45.325Z",
          "wordCount": 626,
          "title": "Continuous-Time Deep Glioma Growth Models. (arXiv:2106.12917v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hengjie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shugong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shan Cao</a>",
          "description": "Depth completion aims to generate a dense depth map from the sparse depth map\nand aligned RGB image. However, current depth completion methods use extremely\nexpensive 64-line LiDAR(about $100,000) to obtain sparse depth maps, which will\nlimit their application scenarios. Compared with the 64-line LiDAR, the\nsingle-line LiDAR is much less expensive and much more robust. Therefore, we\npropose a method to tackle the problem of single-line depth completion, in\nwhich we aim to generate a dense depth map from the single-line LiDAR info and\nthe aligned RGB image. A single-line depth completion dataset is proposed based\non the existing 64-line depth completion dataset(KITTI). A network called\nSemantic Guided Two-Branch Network(SGTBN) which contains global and local\nbranches to extract and fuse global and local info is proposed for this task. A\nSemantic guided depth upsampling module is used in our network to make full use\nof the semantic info in RGB images. Except for the usual MSE loss, we add the\nvirtual normal loss to increase the constraint of high-order 3D geometry in our\nnetwork. Our network outperforms the state-of-the-art in the single-line depth\ncompletion task. Besides, compared with the monocular depth estimation, our\nmethod also has significant advantages in precision and model size.",
          "link": "http://arxiv.org/abs/2106.12994",
          "publishedOn": "2021-06-25T02:00:45.319Z",
          "wordCount": 640,
          "title": "SGTBN: Generating Dense Depth Maps from Single-Line LiDAR. (arXiv:2106.12994v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1\">Takuhiro Kaneko</a>",
          "description": "Understanding the 3D world from 2D projected natural images is a fundamental\nchallenge in computer vision and graphics. Recently, an unsupervised learning\napproach has garnered considerable attention owing to its advantages in data\ncollection. However, to mitigate training limitations, typical methods need to\nimpose assumptions for viewpoint distribution (e.g., a dataset containing\nvarious viewpoint images) or object shape (e.g., symmetric objects). These\nassumptions often restrict applications; for instance, the application to\nnon-rigid objects or images captured from similar viewpoints (e.g., flower or\nbird images) remains a challenge. To complement these approaches, we propose\naperture rendering generative adversarial networks (AR-GANs), which equip\naperture rendering on top of GANs, and adopt focus cues to learn the depth and\ndepth-of-field (DoF) effect of unlabeled natural images. To address the\nambiguities triggered by unsupervised setting (i.e., ambiguities between smooth\ntexture and out-of-focus blurs, and between foreground and background blurs),\nwe develop DoF mixture learning, which enables the generator to learn real\nimage distribution while generating diverse DoF images. In addition, we devise\na center focus prior to guiding the learning direction. In the experiments, we\ndemonstrate the effectiveness of AR-GANs in various datasets, such as flower,\nbird, and face images, demonstrate their portability by incorporating them into\nother 3D representation learning GANs, and validate their applicability in\nshallow DoF rendering.",
          "link": "http://arxiv.org/abs/2106.13041",
          "publishedOn": "2021-06-25T02:00:45.303Z",
          "wordCount": 689,
          "title": "Unsupervised Learning of Depth and Depth-of-Field Effect from Natural Images with Aperture Rendering Generative Adversarial Networks. (arXiv:2106.13041v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hongtao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shancheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yadong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>",
          "description": "Existing scene text removal methods mainly train an elaborate network with\npaired images to realize the function of text localization and background\nreconstruction simultaneously, but there exists two problems: 1) lacking the\nexhaustive erasure of text region and 2) causing the excessive erasure to\ntext-free areas. To handle these issues, this paper provides a novel\nProgrEssively Region-based scene Text eraser (PERT), which introduces\nregion-based modification strategy to progressively erase the pixels in only\ntext region. Firstly, PERT decomposes the STR task to several erasing stages.\nAs each stage aims to take a further step toward the text-removed image rather\nthan directly regress to the final result, the decomposed operation reduces the\nlearning difficulty in each stage, and an exhaustive erasure result can be\nobtained by iterating over lightweight erasing blocks with shared parameters.\nThen, PERT introduces a region-based modification strategy to ensure the\nintegrity of text-free areas by decoupling text localization from erasure\nprocess to guide the removal. Benefiting from the simplicity architecture, PERT\nis a simple and strong baseline, and is easy to be followed and developed.\nExtensive experiments demonstrate that PERT obtains the state-of-the-art\nresults on both synthetic and real-world datasets. Code is available\nathttps://github.com/wangyuxin87/PERT.",
          "link": "http://arxiv.org/abs/2106.13029",
          "publishedOn": "2021-06-25T02:00:45.297Z",
          "wordCount": 645,
          "title": "A Simple and Strong Baseline: Progressively Region-based Scene Text Removal Networks. (arXiv:2106.13029v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1\">Guozhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lele Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiapeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yaqiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>",
          "description": "Visual Information Extraction (VIE) task aims to extract key information from\nmultifarious document images (e.g., invoices and purchase receipts). Most\nprevious methods treat the VIE task simply as a sequence labeling problem or\nclassification problem, which requires models to carefully identify each kind\nof semantics by introducing multimodal features, such as font, color, layout.\nBut simply introducing multimodal features couldn't work well when faced with\nnumeric semantic categories or some ambiguous texts. To address this issue, in\nthis paper we propose a novel key-value matching model based on a graph neural\nnetwork for VIE (MatchVIE). Through key-value matching based on relevancy\nevaluation, the proposed MatchVIE can bypass the recognitions to various\nsemantics, and simply focuses on the strong relevancy between entities.\nBesides, we introduce a simple but effective operation, Num2Vec, to tackle the\ninstability of encoded values, which helps model converge more smoothly.\nComprehensive experiments demonstrate that the proposed MatchVIE can\nsignificantly outperform previous methods. Notably, to the best of our\nknowledge, MatchVIE may be the first attempt to tackle the VIE task by modeling\nthe relevancy between keys and values and it is a good complement to the\nexisting methods.",
          "link": "http://arxiv.org/abs/2106.12940",
          "publishedOn": "2021-06-25T02:00:45.288Z",
          "wordCount": 650,
          "title": "MatchVIE: Exploiting Match Relevancy between Entities for Visual Information Extraction. (arXiv:2106.12940v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qiqi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_C/0/1/0/all/0/1\">Carl S Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panneer_S/0/1/0/all/0/1\">Selvakumar Panneer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>",
          "description": "Monte Carlo rendering algorithms are widely used to produce photorealistic\ncomputer graphics images. However, these algorithms need to sample a\nsubstantial amount of rays per pixel to enable proper global illumination and\nthus require an immense amount of computation. In this paper, we present a\nhybrid rendering method to speed up Monte Carlo rendering algorithms. Our\nmethod first generates two versions of a rendering: one at a low resolution\nwith a high sample rate (LRHS) and the other at a high resolution with a low\nsample rate (HRLS). We then develop a deep convolutional neural network to fuse\nthese two renderings into a high-quality image as if it were rendered at a high\nresolution with a high sample rate. Specifically, we formulate this fusion task\nas a super resolution problem that generates a high resolution rendering from a\nlow resolution input (LRHS), assisted with the HRLS rendering. The HRLS\nrendering provides critical high frequency details which are difficult to\nrecover from the LRHS for any super resolution methods. Our experiments show\nthat our hybrid rendering algorithm is significantly faster than the\nstate-of-the-art Monte Carlo denoising methods while rendering high-quality\nimages when tested on both our own BCR dataset and the Gharbi dataset.\n\\url{https://github.com/hqqxyy/msspl}",
          "link": "http://arxiv.org/abs/2106.12802",
          "publishedOn": "2021-06-25T02:00:45.282Z",
          "wordCount": 648,
          "title": "Fast Monte Carlo Rendering via Multi-Resolution Sampling. (arXiv:2106.12802v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12733",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Ruibing Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Bingpeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xinqian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>",
          "description": "Person re-identification (reID) plays an important role in computer vision.\nHowever, existing methods suffer from performance degradation in occluded\nscenes. In this work, we propose an occlusion-robust block, Region Feature\nCompletion (RFC), for occluded reID. Different from most previous works that\ndiscard the occluded regions, RFC block can recover the semantics of occluded\nregions in feature space. Firstly, a Spatial RFC (SRFC) module is developed.\nSRFC exploits the long-range spatial contexts from non-occluded regions to\npredict the features of occluded regions. The unit-wise prediction task leads\nto an encoder/decoder architecture, where the region-encoder models the\ncorrelation between non-occluded and occluded region, and the region-decoder\nutilizes the spatial correlation to recover occluded region features. Secondly,\nwe introduce Temporal RFC (TRFC) module which captures the long-term temporal\ncontexts to refine the prediction of SRFC. RFC block is lightweight, end-to-end\ntrainable and can be easily plugged into existing CNNs to form RFCnet.\nExtensive experiments are conducted on occluded and commonly holistic reID\nbenchmarks. Our method significantly outperforms existing methods on the\nocclusion datasets, while remains top even superior performance on holistic\ndatasets. The source code is available at\nhttps://github.com/blue-blue272/OccludedReID-RFCnet.",
          "link": "http://arxiv.org/abs/2106.12733",
          "publishedOn": "2021-06-25T02:00:45.276Z",
          "wordCount": 655,
          "title": "Feature Completion for Occluded Person Re-Identification. (arXiv:2106.12733v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yulei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yun Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1\">Feng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yue-Min Zhu</a>",
          "description": "To investigate whether the pleurae, airways and vessels surrounding a nodule\non non-contrast computed tomography (CT) can discriminate benign and malignant\npulmonary nodules. The LIDC-IDRI dataset, one of the largest publicly available\nCT database, was exploited for study. A total of 1556 nodules from 694 patients\nwere involved in statistical analysis, where nodules with average scorings <3\nand >3 were respectively denoted as benign and malignant. Besides, 339 nodules\nfrom 113 patients with diagnosis ground-truth were independently evaluated.\nComputer algorithms were developed to segment pulmonary structures and quantify\nthe distances to pleural surface, airways and vessels, as well as the counting\nnumber and normalized volume of airways and vessels near a nodule. Odds ratio\n(OR) and Chi-square (\\chi^2) testing were performed to demonstrate the\ncorrelation between features of surrounding structures and nodule malignancy. A\nnon-parametric receiver operating characteristic (ROC) analysis was conducted\nin logistic regression to evaluate discrimination ability of each structure.\nFor benign and malignant groups, the average distances from nodules to pleural\nsurface, airways and vessels are respectively (6.56, 5.19), (37.08, 26.43) and\n(1.42, 1.07) mm. The correlation between nodules and the counting number of\nairways and vessels that contact or project towards nodules are respectively\n(OR=22.96, \\chi^2=105.04) and (OR=7.06, \\chi^2=290.11). The correlation between\nnodules and the volume of airways and vessels are (OR=9.19, \\chi^2=159.02) and\n(OR=2.29, \\chi^2=55.89). The areas-under-curves (AUCs) for pleurae, airways and\nvessels are respectively 0.5202, 0.6943 and 0.6529. Our results show that\nmalignant nodules are often surrounded by more pulmonary structures compared\nwith benign ones, suggesting that features of these structures could be viewed\nas lung cancer biomarkers.",
          "link": "http://arxiv.org/abs/2106.12991",
          "publishedOn": "2021-06-25T02:00:45.270Z",
          "wordCount": 751,
          "title": "Relationship between pulmonary nodule malignancy and surrounding pleurae, airways and vessels: a quantitative study using the public LIDC-IDRI dataset. (arXiv:2106.12991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12746",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yue_J/0/1/0/all/0/1\">Jian Yue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yanbo Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_H/0/1/0/all/0/1\">Hui Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dufaux_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Dufaux</a>",
          "description": "In-loop filtering is used in video coding to process the reconstructed frame\nin order to remove blocking artifacts. With the development of convolutional\nneural networks (CNNs), CNNs have been explored for in-loop filtering\nconsidering it can be treated as an image de-noising task. However, in addition\nto being a distorted image, the reconstructed frame is also obtained by a fixed\nline of block based encoding operations in video coding. It carries coding-unit\nbased coding distortion of some similar characteristics. Therefore, in this\npaper, we address the filtering problem from two aspects, global appearance\nrestoration for disrupted texture and local coding distortion restoration\ncaused by fixed pipeline of coding. Accordingly, a three-stream global\nappearance and local coding distortion based fusion network is developed with a\nhigh-level global feature stream, a high-level local feature stream and a\nlow-level local feature stream. Ablation study is conducted to validate the\nnecessity of different features, demonstrating that the global features and\nlocal features can complement each other in filtering and achieve better\nperformance when combined. To the best of our knowledge, we are the first one\nthat clearly characterizes the video filtering process from the above global\nappearance and local coding distortion restoration aspects with experimental\nverification, providing a clear pathway to developing filter techniques.\nExperimental results demonstrate that the proposed method significantly\noutperforms the existing single-frame based methods and achieves 13.5%, 11.3%,\n11.7% BD-Rate saving on average for AI, LDP and RA configurations,\nrespectively, compared with the HEVC reference software.",
          "link": "http://arxiv.org/abs/2106.12746",
          "publishedOn": "2021-06-25T02:00:45.264Z",
          "wordCount": 710,
          "title": "A Global Appearance and Local Coding Distortion based Fusion Framework for CNN based Filtering in Video Coding. (arXiv:2106.12746v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_P/0/1/0/all/0/1\">Parul Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_R/0/1/0/all/0/1\">Rudrabha Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1\">Sindhu B Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay Namboodiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C V Jawahar</a>",
          "description": "We aim to solve the highly challenging task of generating continuous sign\nlanguage videos solely from speech segments for the first time. Recent efforts\nin this space have focused on generating such videos from human-annotated text\ntranscripts without considering other modalities. However, replacing speech\nwith sign language proves to be a practical solution while communicating with\npeople suffering from hearing loss. Therefore, we eliminate the need of using\ntext as input and design techniques that work for more natural, continuous,\nfreely uttered speech covering an extensive vocabulary. Since the current\ndatasets are inadequate for generating sign language directly from speech, we\ncollect and release the first Indian sign language dataset comprising\nspeech-level annotations, text transcripts, and the corresponding sign-language\nvideos. Next, we propose a multi-tasking transformer network trained to\ngenerate signer's poses from speech segments. With speech-to-text as an\nauxiliary task and an additional cross-modal discriminator, our model learns to\ngenerate continuous sign pose sequences in an end-to-end manner. Extensive\nexperiments and comparisons with other baselines demonstrate the effectiveness\nof our approach. We also conduct additional ablation studies to analyze the\neffect of different modules of our network. A demo video containing several\nresults is attached to the supplementary material.",
          "link": "http://arxiv.org/abs/2106.12790",
          "publishedOn": "2021-06-25T02:00:45.258Z",
          "wordCount": 648,
          "title": "Towards Automatic Speech to Sign Language Generation. (arXiv:2106.12790v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niloy_F/0/1/0/all/0/1\">Fahim Faisal Niloy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">M. Ashraful Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Amin Ahsan Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">AKM Mahbubur Rahman</a>",
          "description": "High-resolution image segmentation remains challenging and error-prone due to\nthe enormous size of intermediate feature maps. Conventional methods avoid this\nproblem by using patch based approaches where each patch is segmented\nindependently. However, independent patch segmentation induces errors,\nparticularly at the patch boundary due to the lack of contextual information in\nvery high-resolution images where the patch size is much smaller compared to\nthe full image. To overcome these limitations, in this paper, we propose a\nnovel framework to segment a particular patch by incorporating contextual\ninformation from its neighboring patches. This allows the segmentation network\nto see the target patch with a wider field of view without the need of larger\nfeature maps. Comparative analysis from a number of experiments shows that our\nproposed framework is able to segment high resolution images with significantly\nimproved mean Intersection over Union and overall accuracy.",
          "link": "http://arxiv.org/abs/2106.12902",
          "publishedOn": "2021-06-25T02:00:45.238Z",
          "wordCount": 596,
          "title": "Attention Toward Neighbors: A Context Aware Framework for High Resolution Image Segmentation. (arXiv:2106.12902v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12930",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu T. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia T. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huynh_T/0/1/0/all/0/1\">Thang Q. Huynh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dao_M/0/1/0/all/0/1\">Minh Dao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vu_V/0/1/0/all/0/1\">Van Vu</a>",
          "description": "Radiographs are used as the most important imaging tool for identifying spine\nanomalies in clinical practice. The evaluation of spinal bone lesions, however,\nis a challenging task for radiologists. This work aims at developing and\nevaluating a deep learning-based framework, named VinDr-SpineXR, for the\nclassification and localization of abnormalities from spine X-rays. First, we\nbuild a large dataset, comprising 10,468 spine X-ray images from 5,000 studies,\neach of which is manually annotated by an experienced radiologist with bounding\nboxes around abnormal findings in 13 categories. Using this dataset, we then\ntrain a deep learning classifier to determine whether a spine scan is abnormal\nand a detector to localize 7 crucial findings amongst the total 13. The\nVinDr-SpineXR is evaluated on a test set of 2,078 images from 1,000 studies,\nwhich is kept separate from the training set. It demonstrates an area under the\nreceiver operating characteristic curve (AUROC) of 88.61% (95% CI 87.19%,\n90.02%) for the image-level classification task and a mean average precision\n(mAP@0.5) of 33.56% for the lesion-level localization task. These results serve\nas a proof of concept and set a baseline for future research in this direction.\nTo encourage advances, the dataset, codes, and trained deep learning models are\nmade publicly available.",
          "link": "http://arxiv.org/abs/2106.12930",
          "publishedOn": "2021-06-25T02:00:45.228Z",
          "wordCount": 702,
          "title": "VinDr-SpineXR: A deep learning framework for spinal lesions detection and classification from radiographs. (arXiv:2106.12930v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Guotao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sijin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "Existing video super-resolution methods often utilize a few neighboring\nframes to generate a higher-resolution image for each frame. However, the\nredundant information between distant frames has not been fully exploited in\nthese methods: corresponding patches of the same instance appear across distant\nframes at different scales. Based on this observation, we propose a video\nsuper-resolution method with long-term cross-scale aggregation that leverages\nsimilar patches (self-exemplars) across distant frames. Our model also consists\nof a multi-reference alignment module to fuse the features derived from similar\npatches: we fuse the features of distant references to perform high-quality\nsuper-resolution. We also propose a novel and practical training strategy for\nreferenced-based super-resolution. To evaluate the performance of our proposed\nmethod, we conduct extensive experiments on our collected CarCam dataset and\nthe Waymo Open dataset, and the results demonstrate our method outperforms\nstate-of-the-art methods. Our source code will be publicly available.",
          "link": "http://arxiv.org/abs/2106.12778",
          "publishedOn": "2021-06-25T02:00:45.220Z",
          "wordCount": 576,
          "title": "Video Super-Resolution with Long-Term Self-Exemplars. (arXiv:2106.12778v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhadip Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rout_S/0/1/0/all/0/1\">Swapna Sourav Rout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Sudeep Choudhary</a>",
          "description": "Detection of semantic data types is a very crucial task in data science for\nautomated data cleaning, schema matching, data discovery, semantic data type\nnormalization and sensitive data identification. Existing methods include\nregular expression-based or dictionary lookup-based methods that are not robust\nto dirty as well unseen data and are limited to a very less number of semantic\ndata types to predict. Existing Machine Learning methods extract large number\nof engineered features from data and build logistic regression, random forest\nor feedforward neural network for this purpose. In this paper, we introduce\nDCoM, a collection of multi-input NLP-based deep neural networks to detect\nsemantic data types where instead of extracting large number of features from\nthe data, we feed the raw values of columns (or instances) to the model as\ntexts. We train DCoM on 686,765 data columns extracted from VizNet corpus with\n78 different semantic data types. DCoM outperforms other contemporary results\nwith a quite significant margin on the same dataset.",
          "link": "http://arxiv.org/abs/2106.12871",
          "publishedOn": "2021-06-25T02:00:45.198Z",
          "wordCount": 617,
          "title": "DCoM: A Deep Column Mapper for Semantic Data Type Detection. (arXiv:2106.12871v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12954",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jia_C/0/1/0/all/0/1\">Chuanmin Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1\">Ziqing Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>",
          "description": "End-to-end optimization capability offers neural image compression (NIC)\nsuperior lossy compression performance. However, distinct models are required\nto be trained to reach different points in the rate-distortion (R-D) space. In\nthis paper, we consider the problem of R-D characteristic analysis and modeling\nfor NIC. We make efforts to formulate the essential mathematical functions to\ndescribe the R-D behavior of NIC using deep network and statistical modeling.\nThus continuous bit-rate points could be elegantly realized by leveraging such\nmodel via a single trained network. In this regard, we propose a plugin-in\nmodule to learn the relationship between the target bit-rate and the binary\nrepresentation for the latent variable of auto-encoder. Furthermore, we model\nthe rate and distortion characteristic of NIC as a function of the coding\nparameter $\\lambda$ respectively. Our experiments show our proposed method is\neasy to adopt and obtains competitive coding performance with fixed-rate coding\napproaches, which would benefit the practical deployment of NIC. In addition,\nthe proposed model could be applied to NIC rate control with limited bit-rate\nerror using a single network.",
          "link": "http://arxiv.org/abs/2106.12954",
          "publishedOn": "2021-06-25T02:00:45.183Z",
          "wordCount": 630,
          "title": "Rate Distortion Characteristic Modeling for Neural Image Compression. (arXiv:2106.12954v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12864",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Johann Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_G/0/1/0/all/0/1\">Guangming Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hua_C/0/1/0/all/0/1\">Cong Hua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_M/0/1/0/all/0/1\">Mingtao Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+BasheerBennamoun/0/1/0/all/0/1\">BasheerBennamoun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoyuan Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Juan Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_P/0/1/0/all/0/1\">Peiyi Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mei_L/0/1/0/all/0/1\">Lin Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_S/0/1/0/all/0/1\">Syed Afaq Ali Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>",
          "description": "The astounding success made by artificial intelligence (AI) in healthcare and\nother fields proves that AI can achieve human-like performance. However,\nsuccess always comes with challenges. Deep learning algorithms are\ndata-dependent and require large datasets for training. The lack of data in the\nmedical imaging field creates a bottleneck for the application of deep learning\nto medical image analysis. Medical image acquisition, annotation, and analysis\nare costly, and their usage is constrained by ethical restrictions. They also\nrequire many resources, such as human expertise and funding. That makes it\ndifficult for non-medical researchers to have access to useful and large\nmedical data. Thus, as comprehensive as possible, this paper provides a\ncollection of medical image datasets with their associated challenges for deep\nlearning research. We have collected information of around three hundred\ndatasets and challenges mainly reported between 2013 and 2020 and categorized\nthem into four categories: head & neck, chest & abdomen, pathology & blood, and\n``others''. Our paper has three purposes: 1) to provide a most up to date and\ncomplete list that can be used as a universal reference to easily find the\ndatasets for clinical image analysis, 2) to guide researchers on the\nmethodology to test and evaluate their methods' performance and robustness on\nrelevant datasets, 3) to provide a ``route'' to relevant algorithms for the\nrelevant medical topics, and challenge leaderboards.",
          "link": "http://arxiv.org/abs/2106.12864",
          "publishedOn": "2021-06-25T02:00:45.167Z",
          "wordCount": 708,
          "title": "A Systematic Collection of Medical Image Datasets for Deep Learning. (arXiv:2106.12864v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junwei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xianfeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yicong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>",
          "description": "With the rapid progress of deepfake techniques in recent years, facial video\nforgery can generate highly deceptive video contents and bring severe security\nthreats. And detection of such forgery videos is much more urgent and\nchallenging. Most existing detection methods treat the problem as a vanilla\nbinary classification problem. In this paper, the problem is treated as a\nspecial fine-grained classification problem since the differences between fake\nand real faces are very subtle. It is observed that most existing face forgery\nmethods left some common artifacts in the spatial domain and time domain,\nincluding generative defects in the spatial domain and inter-frame\ninconsistencies in the time domain. And a spatial-temporal model is proposed\nwhich has two components for capturing spatial and temporal forgery traces in\nglobal perspective respectively. The two components are designed using a novel\nlong distance attention mechanism. The one component of the spatial domain is\nused to capture artifacts in a single frame, and the other component of the\ntime domain is used to capture artifacts in consecutive frames. They generate\nattention maps in the form of patches. The attention method has a broader\nvision which contributes to better assembling global information and extracting\nlocal statistic information. Finally, the attention maps are used to guide the\nnetwork to focus on pivotal parts of the face, just like other fine-grained\nclassification methods. The experimental results on different public datasets\ndemonstrate that the proposed method achieves the state-of-the-art performance,\nand the proposed long distance attention method can effectively capture pivotal\nparts for face forgery.",
          "link": "http://arxiv.org/abs/2106.12832",
          "publishedOn": "2021-06-25T02:00:45.150Z",
          "wordCount": 696,
          "title": "Detection of Deepfake Videos Using Long Distance Attention. (arXiv:2106.12832v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1\">Ee Fey Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">ZhiYuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_W/0/1/0/all/0/1\">Wei Xiang Lim</a>",
          "description": "The conventional spatial convolution layers in the Convolutional Neural\nNetworks (CNNs) are computationally expensive at the point where the training\ntime could take days unless the number of layers, the number of training images\nor the size of the training images are reduced. The image size of 256x256\npixels is commonly used for most of the applications of CNN, but this image\nsize is too small for applications like Diabetic Retinopathy (DR)\nclassification where the image details are important for accurate\nclassification. This research proposed Frequency Domain Convolution (FDC) and\nFrequency Domain Pooling (FDP) layers which were built with RFFT, kernel\ninitialization strategy, convolution artifact removal and Channel Independent\nConvolution (CIC) to replace the conventional convolution and pooling layers.\nThe FDC and FDP layers are used to build a Frequency Domain Convolutional\nNeural Network (FDCNN) to accelerate the training of large images for DR\nclassification. The Full FDC layer is an extension of the FDC layer to allow\ndirect use in conventional CNNs, it is also used to modify the VGG16\narchitecture. FDCNN is shown to be at least 54.21% faster and 70.74% more\nmemory efficient compared to an equivalent CNN architecture. The modified VGG16\narchitecture with Full FDC layer is reported to achieve a shorter training time\nand a higher accuracy at 95.63% compared to the original VGG16 architecture for\nDR classification.",
          "link": "http://arxiv.org/abs/2106.12736",
          "publishedOn": "2021-06-25T02:00:45.143Z",
          "wordCount": 680,
          "title": "Frequency Domain Convolutional Neural Network: Accelerated CNN for Large Diabetic Retinopathy Image Classification. (arXiv:2106.12736v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1\">Qiuyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianmin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanyong Zhang</a>",
          "description": "In the past few years, we have witnessed rapid development of autonomous\ndriving. However, achieving full autonomy remains a daunting task due to the\ncomplex and dynamic driving environment. As a result, self-driving cars are\nequipped with a suite of sensors to conduct robust and accurate environment\nperception. As the number and type of sensors keep increasing, combining them\nfor better perception is becoming a natural trend. So far, there has been no\nindepth review that focuses on multi-sensor fusion based perception. To bridge\nthis gap and motivate future research, this survey devotes to review recent\nfusion-based 3D detection deep learning models that leverage multiple sensor\ndata sources, especially cameras and LiDARs. In this survey, we first introduce\nthe background of popular sensors for autonomous cars, including their common\ndata representations as well as object detection networks developed for each\ntype of sensor data. Next, we discuss some popular datasets for multi-modal 3D\nobject detection, with a special focus on the sensor data included in each\ndataset. Then we present in-depth reviews of recent multi-modal 3D detection\nnetworks by considering the following three aspects of the fusion: fusion\nlocation, fusion data representation, and fusion granularity. After a detailed\nreview, we discuss open challenges and point out possible solutions. We hope\nthat our detailed review can help researchers to embark investigations in the\narea of multi-modal 3D object detection.",
          "link": "http://arxiv.org/abs/2106.12735",
          "publishedOn": "2021-06-25T02:00:45.127Z",
          "wordCount": 669,
          "title": "Multi-Modal 3D Object Detection in Autonomous Driving: a Survey. (arXiv:2106.12735v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xue Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yuanbin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengyang Li</a>",
          "description": "The autonomous real-time optical navigation of planetary UAV is of the key\ntechnologies to ensure the success of the exploration. In such a GPS denied\nenvironment, vision-based localization is an optimal approach. In this paper,\nwe proposed a multi-modal registration based SLAM algorithm, which estimates\nthe location of a planet UAV using a nadir view camera on the UAV compared with\npre-existing digital terrain model. To overcome the scale and appearance\ndifference between on-board UAV images and pre-installed digital terrain model,\na theoretical model is proposed to prove that topographic features of UAV image\nand DEM can be correlated in frequency domain via cross power spectrum. To\nprovide the six-DOF of the UAV, we also developed an optimization approach\nwhich fuses the geo-referencing result into a SLAM system via LBA (Local Bundle\nAdjustment) to achieve robust and accurate vision-based navigation even in\nfeatureless planetary areas. To test the robustness and effectiveness of the\nproposed localization algorithm, a new cross-source drone-based localization\ndataset for planetary exploration is proposed. The proposed dataset includes\n40200 synthetic drone images taken from nine planetary scenes with related DEM\nquery images. Comparison experiments carried out demonstrate that over the\nflight distance of 33.8km, the proposed method achieved average localization\nerror of 0.45 meters, compared to 1.31 meters by ORB-SLAM, with the processing\nspeed of 12hz which will ensure a real-time performance. We will make our\ndatasets available to encourage further work on this promising topic.",
          "link": "http://arxiv.org/abs/2106.12738",
          "publishedOn": "2021-06-25T02:00:45.120Z",
          "wordCount": 679,
          "title": "Planetary UAV localization based on Multi-modal Registration with Pre-existing Digital Terrain Model. (arXiv:2106.12738v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1\">Rahaf Aljundi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1\">Daniel Olmeda Reino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chumerin_N/0/1/0/all/0/1\">Nikolay Chumerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1\">Richard E. Turner</a>",
          "description": "Novelty Detection methods identify samples that are not representative of a\nmodel's training set thereby flagging misleading predictions and bringing a\ngreater flexibility and transparency at deployment time. However, research in\nthis area has only considered Novelty Detection in the offline setting.\nRecently, there has been a growing realization in the computer vision community\nthat applications demand a more flexible framework - Continual Learning - where\nnew batches of data representing new domains, new classes or new tasks become\navailable at different points in time. In this setting, Novelty Detection\nbecomes more important, interesting and challenging. This work identifies the\ncrucial link between the two problems and investigates the Novelty Detection\nproblem under the Continual Learning setting. We formulate the Continual\nNovelty Detection problem and present a benchmark, where we compare several\nNovelty Detection methods under different Continual Learning settings.\n\nWe show that Continual Learning affects the behaviour of novelty detection\nalgorithms, while novelty detection can pinpoint insights in the behaviour of a\ncontinual learner. We further propose baselines and discuss possible research\ndirections. We believe that the coupling of the two problems is a promising\ndirection to bring vision models into practice.",
          "link": "http://arxiv.org/abs/2106.12964",
          "publishedOn": "2021-06-25T02:00:45.114Z",
          "wordCount": 621,
          "title": "Continual Novelty Detection. (arXiv:2106.12964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schubert_S/0/1/0/all/0/1\">Stefan Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubert_P/0/1/0/all/0/1\">Peer Neubert</a>",
          "description": "Visual place recognition is a fundamental capability for the localization of\nmobile robots. It places image retrieval in the practical context of physical\nagents operating in a physical world. It is an active field of research and\nmany different approaches have been proposed and evaluated in many different\nexperiments. In the following, we argue that due to variations of this\npractical context and individual design decisions, place recognition\nexperiments are barely comparable across different papers and that there is a\nvariety of properties that can change from one experiment to another. We\nprovide an extensive list of such properties and give examples how they can be\nused to setup a place recognition experiment easier or harder. This might be\ninteresting for different involved parties: (1) people who just want to select\na place recognition approach that is suitable for the properties of their\nparticular task at hand, (2) researchers that look for open research questions\nand are interested in particularly difficult instances, (3) authors that want\nto create reproducible papers on this topic, and (4) also reviewers that have\nthe task to identify potential problems in papers under review.",
          "link": "http://arxiv.org/abs/2106.12671",
          "publishedOn": "2021-06-25T02:00:45.108Z",
          "wordCount": 624,
          "title": "What makes visual place recognition easy or hard?. (arXiv:2106.12671v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>",
          "description": "Traditional feature-based image stitching technologies rely heavily on\nfeature detection quality, often failing to stitch images with few features or\nlow resolution. The learning-based image stitching solutions are rarely studied\ndue to the lack of labeled data, making the supervised methods unreliable. To\naddress the above limitations, we propose an unsupervised deep image stitching\nframework consisting of two stages: unsupervised coarse image alignment and\nunsupervised image reconstruction. In the first stage, we design an\nablation-based loss to constrain an unsupervised homography network, which is\nmore suitable for large-baseline scenes. Moreover, a transformer layer is\nintroduced to warp the input images in the stitching-domain space. In the\nsecond stage, motivated by the insight that the misalignments in pixel-level\ncan be eliminated to a certain extent in feature-level, we design an\nunsupervised image reconstruction network to eliminate the artifacts from\nfeatures to pixels. Specifically, the reconstruction network can be implemented\nby a low-resolution deformation branch and a high-resolution refined branch,\nlearning the deformation rules of image stitching and enhancing the resolution\nsimultaneously. To establish an evaluation benchmark and train the learning\nframework, a comprehensive real-world image dataset for unsupervised deep image\nstitching is presented and released. Extensive experiments well demonstrate the\nsuperiority of our method over other state-of-the-art solutions. Even compared\nwith the supervised solutions, our image stitching quality is still preferred\nby users.",
          "link": "http://arxiv.org/abs/2106.12859",
          "publishedOn": "2021-06-25T02:00:45.101Z",
          "wordCount": 670,
          "title": "Unsupervised Deep Image Stitching: Reconstructing Stitched Features to Images. (arXiv:2106.12859v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1\">Crystal Gagne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kini_J/0/1/0/all/0/1\">Jyoti Kini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1\">Daniel Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Trail camera imagery has increasingly gained popularity amongst biologists\nfor conservation and ecological research. Minimal human interference required\nto operate camera traps allows capturing unbiased species activities. Several\nstudies - based on human and wildlife interactions, migratory patterns of\nvarious species, risk of extinction in endangered populations - are limited by\nthe lack of rich data and the time-consuming nature of manually annotating\ntrail camera imagery. We introduce a challenging wildlife camera trap\nclassification dataset collected from two different locations in Southwestern\nFlorida, consisting of 104,495 images featuring visually similar species,\nvarying illumination conditions, skewed class distribution, and including\nsamples of endangered species, i.e. Florida panthers. Experimental evaluations\nwith ResNet-50 architecture indicate that this image classification-based\ndataset can further push the advancements in wildlife statistical modeling. We\nwill make the dataset publicly available.",
          "link": "http://arxiv.org/abs/2106.12628",
          "publishedOn": "2021-06-25T02:00:45.078Z",
          "wordCount": 589,
          "title": "Florida Wildlife Camera Trap Dataset. (arXiv:2106.12628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Arbitrary-shaped text detection is a challenging task since curved texts in\nthe wild are of the complex geometric layouts. Existing mainstream methods\nfollow the instance segmentation pipeline to obtain the text regions. However,\narbitraryshaped texts are difficult to be depicted through one single\nsegmentation network because of the varying scales. In this paper, we propose a\ntwo-stage segmentation-based detector, termed as NASK (Need A Second looK), for\narbitrary-shaped text detection. Compared to the traditional single-stage\nsegmentation network, our NASK conducts the detection in a coarse-to-fine\nmanner with the first stage segmentation spotting the rectangle text proposals\nand the second one retrieving compact representations. Specifically, NASK is\ncomposed of a Text Instance Segmentation (TIS) network (1st stage), a\nGeometry-aware Text RoI Alignment (GeoAlign) module, and a Fiducial pOint\neXpression (FOX) module (2nd stage). Firstly, TIS extracts the augmented\nfeatures with a novel Group Spatial and Channel Attention (GSCA) module and\nconducts instance segmentation to obtain rectangle proposals. Then, GeoAlign\nconverts these rectangles into the fixed size and encodes RoI-wise feature\nrepresentation. Finally, FOX disintegrates the text instance into serval\npivotal geometrical attributes to refine the detection results. Extensive\nexperimental results on three public benchmarks including Total-Text,\nSCUTCTW1500, and ICDAR 2015 verify that our NASK outperforms recent\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.12720",
          "publishedOn": "2021-06-25T02:00:45.072Z",
          "wordCount": 652,
          "title": "All You Need is a Second Look: Towards Arbitrary-Shaped Text Detection. (arXiv:2106.12720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12728",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nie_G/0/1/0/all/0/1\">Guanxiong Nie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yajian Zhou</a>",
          "description": "Compressed Sensing (CS) theory simultaneously realizes the signal sampling\nand compression process, and can use fewer observations to achieve accurate\nsignal recovery, providing a solution for better and faster transmission of\nmassive data. In this paper, a ternary sampling matrix-based method with\nattention mechanism is proposed with the purpose to solve the problem that the\nCS sampling matrices in most cases are random matrices, which are irrelative to\nthe sampled signal and need a large storage space. The proposed method consists\nof three components, i.e., ternary sampling, initial reconstruction and deep\nreconstruction, with the emphasis on the ternary sampling. The main idea of the\nternary method (-1, 0, +1) is to introduce the attention mechanism to evaluate\nthe importance of parameters at the sampling layer after the sampling matrix is\nbinarized (-1, +1), followed by pruning weight of parameters, whose importance\nis below a predefined threshold, to achieve ternarization. Furthermore, a\ncompressed sensing algorithm especially for image reconstruction is\nimplemented, on the basis of the ternary sampling matrix, which is called\nATP-Net, i.e., Attention-based ternary projection network. Experimental results\nshow that the quality of image reconstruction by means of ATP-Net maintains a\nsatisfactory level with the employment of the ternary sampling matrix, i.e.,\nthe average PSNR on Set11 is 30.4 when the sampling rate is 0.25, approximately\n6% improvement compared with that of DR2-Net.",
          "link": "http://arxiv.org/abs/2106.12728",
          "publishedOn": "2021-06-25T02:00:45.044Z",
          "wordCount": 668,
          "title": "ATP-Net: An Attention-based Ternary Projection Network For Compressed Sensing. (arXiv:2106.12728v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nedorubova_A/0/1/0/all/0/1\">Anna Nedorubova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadyrova_A/0/1/0/all/0/1\">Alena Kadyrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khlyupin_A/0/1/0/all/0/1\">Aleksey Khlyupin</a>",
          "description": "Quite a few people in the world have to stay under permanent surveillance for\nhealth reasons; they include diabetic people or people with some other chronic\nconditions, the elderly and the disabled.These groups may face heightened risk\nof having life-threatening falls or of being struck by a syncope. Due to\nlimited availability of resources a substantial part of people at risk can not\nreceive necessary monitoring and thus are exposed to excessive danger.\nNowadays, this problem is usually solved via applying Human Activity\nRecognition (HAR) methods. HAR is a perspective and fast-paced Data Science\nfield, which has a wide range of application areas such as healthcare, sport,\nsecurity etc. However, the currently techniques of recognition are markedly\nlacking in accuracy, hence, the present paper suggests a highly accurate method\nfor human activity classification. Wepropose a new workflow to address the HAR\nproblem and evaluate it on the UniMiB SHAR dataset, which consists of the\naccelerometer signals. The model we suggest is based on continuous wavelet\ntransform (CWT) and convolutional neural networks (CNNs). Wavelet transform\nlocalizes signal features both in time and frequency domains and after that a\nCNN extracts these features and recognizes activity. It is also worth noting\nthat CWT converts 1D accelerometer signal into 2D images and thus enables to\nobtain better results as 2D networks have a significantly higher predictive\ncapacity. In the course of the work we build a convolutional neural network and\nvary such model parameters as number of spatial axes, number of layers, number\nof neurons in each layer, image size, type of mother wavelet, the order of zero\nmoment of mother wavelet etc. Besides, we also apply models with residual\nblocks which resulted in significantly higher metric values. Finally, we\nsucceed to reach 99.26 % accuracy and it is a worthy performance for this\nproblem.",
          "link": "http://arxiv.org/abs/2106.12666",
          "publishedOn": "2021-06-25T02:00:45.035Z",
          "wordCount": 745,
          "title": "Human Activity Recognition using Continuous Wavelet Transform and Convolutional Neural Networks. (arXiv:2106.12666v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sousa_Y/0/1/0/all/0/1\">Ygor C. N. Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassani_H/0/1/0/all/0/1\">Hansenclever F. Bassani</a>",
          "description": "Many works in the recent literature introduce semantic mapping methods that\nuse CNNs (Convolutional Neural Networks) to recognize semantic properties in\nimages. The types of properties (eg.: room size, place category, and objects)\nand their classes (eg.: kitchen and bathroom, for place category) are usually\npredefined and restricted to a specific task. Thus, all the visual data\nacquired and processed during the construction of the maps are lost and only\nthe recognized semantic properties remain on the maps. In contrast, this work\nintroduces a topological semantic mapping method that uses deep visual features\nextracted by a CNN, the GoogLeNet, from 2D images captured in multiple views of\nthe environment as the robot operates, to create consolidated representations\nof visual features acquired in the regions covered by each topological node.\nThese consolidated representations allow flexible recognition of semantic\nproperties of the regions and use in a range of visual tasks. The experiments,\nperformed using a real-world indoor dataset, showed that the method is able to\nconsolidate the visual features of regions and use them to recognize objects\nand place categories as semantic properties, and to indicate the topological\nlocation of images, with very promising results. The objects are classified\nusing the classification layer of GoogLeNet, without retraining, and the place\ncategories are recognized using a shallow Multilayer Perceptron.",
          "link": "http://arxiv.org/abs/2106.12709",
          "publishedOn": "2021-06-25T02:00:45.015Z",
          "wordCount": 662,
          "title": "Topological Semantic Mapping by Consolidation of Deep Visual Features. (arXiv:2106.12709v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mok_T/0/1/0/all/0/1\">Tony C. W. Mok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_A/0/1/0/all/0/1\">Albert C. S. Chung</a>",
          "description": "Recent deep learning-based methods have shown promising results and runtime\nadvantages in deformable image registration. However, analyzing the effects of\nhyperparameters and searching for optimal regularization parameters prove to be\ntoo prohibitive in deep learning-based methods. This is because it involves\ntraining a substantial number of separate models with distinct hyperparameter\nvalues. In this paper, we propose a conditional image registration method and a\nnew self-supervised learning paradigm for deep deformable image registration.\nBy learning the conditional features that correlated with the regularization\nhyperparameter, we demonstrate that optimal solutions with arbitrary\nhyperparameters can be captured by a single deep convolutional neural network.\nIn addition, the smoothness of the resulting deformation field can be\nmanipulated with arbitrary strength of smoothness regularization during\ninference. Extensive experiments on a large-scale brain MRI dataset show that\nour proposed method enables the precise control of the smoothness of the\ndeformation field without sacrificing the runtime advantage or registration\naccuracy.",
          "link": "http://arxiv.org/abs/2106.12673",
          "publishedOn": "2021-06-25T02:00:44.981Z",
          "wordCount": 604,
          "title": "Conditional Deformable Image Registration with Convolutional Neural Network. (arXiv:2106.12673v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2103.05247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kevin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1\">Aditya Grover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>",
          "description": "We investigate the capability of a transformer pretrained on natural language\nto generalize to other modalities with minimal finetuning -- in particular,\nwithout finetuning of the self-attention and feedforward layers of the residual\nblocks. We consider such a model, which we call a Frozen Pretrained Transformer\n(FPT), and study finetuning it on a variety of sequence classification tasks\nspanning numerical computation, vision, and protein fold prediction. In\ncontrast to prior works which investigate finetuning on the same modality as\nthe pretraining dataset, we show that pretraining on natural language can\nimprove performance and compute efficiency on non-language downstream tasks.\nAdditionally, we perform an analysis of the architecture, comparing the\nperformance of a random initialized transformer to a random LSTM. Combining the\ntwo insights, we find language-pretrained transformers can obtain strong\nperformance on a variety of non-language tasks.",
          "link": "http://arxiv.org/abs/2103.05247",
          "publishedOn": "2021-07-01T01:59:34.732Z",
          "wordCount": 600,
          "title": "Pretrained Transformers as Universal Computation Engines. (arXiv:2103.05247v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06315",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wen_L/0/1/0/all/0/1\">Linjie Wen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Jinglai Li</a>",
          "description": "We propose a linear-mapping based variational Ensemble Kalman filter for\nsequential Bayesian filtering problems with generic observation models.\nSpecifically, the proposed method is formulated as to construct a linear\nmapping from the prior ensemble to the posterior one, and the linear mapping is\ncomputed via a variational Bayesian formulation, i.e., by minimizing the\nKullback-Leibler divergence between the transformed distribution by the linear\nmapping and the actual posterior. A gradient descent scheme is proposed to\nsolve the resulting optimization problem. With numerical examples we\ndemonstrate that the method has competitive performance against existing\nmethods.",
          "link": "http://arxiv.org/abs/2103.06315",
          "publishedOn": "2021-07-01T01:59:34.726Z",
          "wordCount": 550,
          "title": "Linear-Mapping based Variational Ensemble Kalman Filter. (arXiv:2103.06315v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Saurabh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1\">Shivaram Venkataraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>",
          "description": "A rich body of prior work has highlighted the existence of communication\nbottlenecks in synchronous data-parallel training. To alleviate these\nbottlenecks, a long line of recent work proposes gradient and model compression\nmethods. In this work, we evaluate the efficacy of gradient compression methods\nand compare their scalability with optimized implementations of synchronous\ndata-parallel SGD across more than 200 different setups. Surprisingly, we\nobserve that only in 6 cases out of more than 200, gradient compression methods\nprovide speedup over optimized synchronous data-parallel training in the\ntypical data-center setting. We conduct an extensive investigation to identify\nthe root causes of this phenomenon, and offer a performance model that can be\nused to identify the benefits of gradient compression for a variety of system\nsetups. Based on our analysis, we propose a list of desirable properties that\ngradient compression methods should satisfy, in order for them to provide a\nmeaningful end-to-end speedup.",
          "link": "http://arxiv.org/abs/2103.00543",
          "publishedOn": "2021-07-01T01:59:34.720Z",
          "wordCount": 634,
          "title": "On the Utility of Gradient Compression in Distributed Training Systems. (arXiv:2103.00543v3 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henry_R/0/1/0/all/0/1\">Robin Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1\">Damien Ernst</a>",
          "description": "Active network management (ANM) of electricity distribution networks include\nmany complex stochastic sequential optimization problems. These problems need\nto be solved for integrating renewable energies and distributed storage into\nfuture electrical grids. In this work, we introduce Gym-ANM, a framework for\ndesigning reinforcement learning (RL) environments that model ANM tasks in\nelectricity distribution networks. These environments provide new playgrounds\nfor RL research in the management of electricity networks that do not require\nan extensive knowledge of the underlying dynamics of such systems. Along with\nthis work, we are releasing an implementation of an introductory\ntoy-environment, ANM6-Easy, designed to emphasize common challenges in ANM. We\nalso show that state-of-the-art RL algorithms can already achieve good\nperformance on ANM6-Easy when compared against a model predictive control (MPC)\napproach. Finally, we provide guidelines to create new Gym-ANM environments\ndiffering in terms of (a) the distribution network topology and parameters, (b)\nthe observation space, (c) the modelling of the stochastic processes present in\nthe system, and (d) a set of hyperparameters influencing the reward signal.\nGym-ANM can be downloaded at https://github.com/robinhenry/gym-anm.",
          "link": "http://arxiv.org/abs/2103.07932",
          "publishedOn": "2021-07-01T01:59:34.714Z",
          "wordCount": 669,
          "title": "Gym-ANM: Reinforcement Learning Environments for Active Network Management Tasks in Electricity Distribution Systems. (arXiv:2103.07932v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Transformers have become a standard architecture for many NLP problems. This\nhas motivated theoretically analyzing their capabilities as models of language,\nin order to understand what makes them successful, and what their potential\nweaknesses might be. Recent work has shown that transformers with hard\nattention are quite limited in capacity, and in fact can be simulated by\nconstant-depth circuits. However, hard attention is a restrictive assumption,\nwhich may complicate the relevance of these results for practical transformers.\nIn this work, we analyze the circuit complexity of transformers with saturated\nattention: a generalization of hard attention that more closely captures the\nattention patterns learnable in practical transformers. We show that saturated\ntransformers transcend the limitations of hard-attention transformers. With\nsome minor assumptions, we prove that the number of bits needed to represent a\nsaturated transformer memory vector is $O(\\log n)$, which implies saturated\ntransformers can be simulated by log-depth circuits. Thus, the jump from hard\nto saturated attention can be understood as increasing the transformer's\neffective circuit depth by a factor of $O(\\log n)$.",
          "link": "http://arxiv.org/abs/2106.16213",
          "publishedOn": "2021-07-01T01:59:34.708Z",
          "wordCount": 622,
          "title": "On the Power of Saturated Transformers: A View from Circuit Complexity. (arXiv:2106.16213v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zanette_A/0/1/0/all/0/1\">Andrea Zanette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Ching-An Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Alekh Agarwal</a>",
          "description": "Policy optimization methods are popular reinforcement learning algorithms,\nbecause their incremental and on-policy nature makes them more stable than the\nvalue-based counterparts. However, the same properties also make them slow to\nconverge and sample inefficient, as the on-policy requirement precludes data\nreuse and the incremental updates couple large iteration complexity into the\nsample complexity. These characteristics have been observed in experiments as\nwell as in theory in the recent work of~\\citet{agarwal2020pc}, which provides a\npolicy optimization method PCPG that can robustly find near optimal polices for\napproximately linear Markov decision processes but suffers from an extremely\npoor sample complexity compared with value-based techniques.\n\nIn this paper, we propose a new algorithm, COPOE, that overcomes the sample\ncomplexity issue of PCPG while retaining its robustness to model\nmisspecification. Compared with PCPG, COPOE makes several important algorithmic\nenhancements, such as enabling data reuse, and uses more refined analysis\ntechniques, which we expect to be more broadly applicable to designing new\nreinforcement learning algorithms. The result is an improvement in sample\ncomplexity from $\\widetilde{O}(1/\\epsilon^{11})$ for PCPG to\n$\\widetilde{O}(1/\\epsilon^3)$ for PCPG, nearly bridging the gap with\nvalue-based techniques.",
          "link": "http://arxiv.org/abs/2103.12923",
          "publishedOn": "2021-07-01T01:59:34.692Z",
          "wordCount": 650,
          "title": "Cautiously Optimistic Policy Optimization and Exploration with Linear Function Approximation. (arXiv:2103.12923v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gamlath_B/0/1/0/all/0/1\">Buddhima Gamlath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xinrui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polak_A/0/1/0/all/0/1\">Adam Polak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_O/0/1/0/all/0/1\">Ola Svensson</a>",
          "description": "We study the problem of explainable clustering in the setting first\nformalized by Moshkovitz, Dasgupta, Rashtchian, and Frost (ICML 2020). A\n$k$-clustering is said to be explainable if it is given by a decision tree\nwhere each internal node splits data points with a threshold cut in a single\ndimension (feature), and each of the $k$ leaves corresponds to a cluster. We\ngive an algorithm that outputs an explainable clustering that loses at most a\nfactor of $O(\\log^2 k)$ compared to an optimal (not necessarily explainable)\nclustering for the $k$-medians objective, and a factor of $O(k \\log^2 k)$ for\nthe $k$-means objective. This improves over the previous best upper bounds of\n$O(k)$ and $O(k^2)$, respectively, and nearly matches the previous $\\Omega(\\log\nk)$ lower bound for $k$-medians and our new $\\Omega(k)$ lower bound for\n$k$-means. The algorithm is remarkably simple. In particular, given an initial\nnot necessarily explainable clustering in $\\mathbb{R}^d$, it is oblivious to\nthe data points and runs in time $O(dk \\log^2 k)$, independent of the number of\ndata points $n$. Our upper and lower bounds also generalize to objectives given\nby higher $\\ell_p$-norms.",
          "link": "http://arxiv.org/abs/2106.16147",
          "publishedOn": "2021-07-01T01:59:34.686Z",
          "wordCount": 621,
          "title": "Nearly-Tight and Oblivious Algorithms for Explainable Clustering. (arXiv:2106.16147v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2011.03813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yiyuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Panpan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">David Hsu</a>",
          "description": "The partially observable Markov decision process (POMDP) is a principled\ngeneral framework for robot decision making under uncertainty, but POMDP\nplanning suffers from high computational complexity, when long-term planning is\nrequired. While temporally-extended macro-actions help to cut down the\neffective planning horizon and significantly improve computational efficiency,\nhow do we acquire good macro-actions? This paper proposes Macro-Action\nGenerator-Critic (MAGIC), which performs offline learning of macro-actions\noptimized for online POMDP planning. Specifically, MAGIC learns a macro-action\ngenerator end-to-end, using an online planner's performance as the feedback.\nDuring online planning, the generator generates on the fly situation-aware\nmacro-actions conditioned on the robot's belief and the environment context. We\nevaluated MAGIC on several long-horizon planning tasks both in simulation and\non a real robot. The experimental results show that the learned macro-actions\noffer significant benefits in online planning performance, compared with\nprimitive actions and handcrafted macro-actions.",
          "link": "http://arxiv.org/abs/2011.03813",
          "publishedOn": "2021-07-01T01:59:34.680Z",
          "wordCount": 622,
          "title": "MAGIC: Learning Macro-Actions for Online POMDP Planning. (arXiv:2011.03813v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amir_I/0/1/0/all/0/1\">Idan Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1\">Roi Livni</a>",
          "description": "We give a new separation result between the generalization performance of\nstochastic gradient descent (SGD) and of full-batch gradient descent (GD) in\nthe fundamental stochastic convex optimization model. While for SGD it is\nwell-known that $O(1/\\epsilon^2)$ iterations suffice for obtaining a solution\nwith $\\epsilon$ excess expected risk, we show that with the same number of\nsteps GD may overfit and emit a solution with $\\Omega(1)$ generalization error.\nMoreover, we show that in fact $\\Omega(1/\\epsilon^4)$ iterations are necessary\nfor GD to match the generalization performance of SGD, which is also tight due\nto recent work by Bassily et al. (2020). We further discuss how regularizing\nthe empirical risk minimized by GD essentially does not change the above\nresult, and revisit the concepts of stability, implicit bias and the role of\nthe learning algorithm in generalization.",
          "link": "http://arxiv.org/abs/2102.01117",
          "publishedOn": "2021-07-01T01:59:34.658Z",
          "wordCount": 604,
          "title": "SGD Generalizes Better Than GD (And Regularization Doesn't Help). (arXiv:2102.01117v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhaka_A/0/1/0/all/0/1\">Akash Kumar Dhaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catalina_A/0/1/0/all/0/1\">Alejandro Catalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welandawe_M/0/1/0/all/0/1\">Manushi Welandawe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_M/0/1/0/all/0/1\">Michael Riis Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huggins_J/0/1/0/all/0/1\">Jonathan Huggins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vehtari_A/0/1/0/all/0/1\">Aki Vehtari</a>",
          "description": "Current black-box variational inference (BBVI) methods require the user to\nmake numerous design choices -- such as the selection of variational objective\nand approximating family -- yet there is little principled guidance on how to\ndo so. We develop a conceptual framework and set of experimental tools to\nunderstand the effects of these choices, which we leverage to propose best\npractices for maximizing posterior approximation accuracy. Our approach is\nbased on studying the pre-asymptotic tail behavior of the density ratios\nbetween the joint distribution and the variational approximation, then\nexploiting insights and tools from the importance sampling literature. Our\nframework and supporting experiments help to distinguish between the behavior\nof BBVI methods for approximating low-dimensional versus\nmoderate-to-high-dimensional posteriors. In the latter case, we show that\nmass-covering variational objectives are difficult to optimize and do not\nimprove accuracy, but flexible variational families can improve accuracy and\nthe effectiveness of importance sampling -- at the cost of additional\noptimization challenges. Therefore, for moderate-to-high-dimensional posteriors\nwe recommend using the (mode-seeking) exclusive KL divergence since it is the\neasiest to optimize, and improving the variational family or using model\nparameter transformations to make the posterior and optimal variational\napproximation more similar. On the other hand, in low-dimensional settings, we\nshow that heavy-tailed variational families and mass-covering divergences are\neffective and can increase the chances that the approximation can be improved\nby importance sampling.",
          "link": "http://arxiv.org/abs/2103.01085",
          "publishedOn": "2021-07-01T01:59:34.652Z",
          "wordCount": 704,
          "title": "Challenges and Opportunities in High-dimensional Variational Inference. (arXiv:2103.01085v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hanlin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_S/0/1/0/all/0/1\">Shaoduo Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1\">Ammar Ahmad Awan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1\">Samyam Rajbhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Conglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1\">Xiangru Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>",
          "description": "Scalable training of large models (like BERT and GPT-3) requires careful\noptimization rooted in model design, architecture, and system capabilities.\nFrom a system standpoint, communication has become a major bottleneck,\nespecially on commodity systems with standard TCP interconnects that offer\nlimited network bandwidth. Communication compression is an important technique\nto reduce training time on such systems. One of the most effective methods is\nerror-compensated compression, which offers robust convergence speed even under\n1-bit compression. However, state-of-the-art error compensation techniques only\nwork with basic optimizers like SGD and momentum SGD, which are linearly\ndependent on the gradients. They do not work with non-linear gradient-based\noptimizers like Adam, which offer state-of-the-art convergence efficiency and\naccuracy for models like BERT. In this paper, we propose 1-bit Adam that\nreduces the communication volume by up to $5\\times$, offers much better\nscalability, and provides the same convergence speed as uncompressed Adam. Our\nkey finding is that Adam's variance (non-linear term) becomes stable (after a\nwarmup phase) and can be used as a fixed precondition for the rest of the\ntraining (compression phase). Experiments on up to 256 GPUs show that 1-bit\nAdam enables up to $3.3\\times$ higher throughput for BERT-Large pre-training\nand up to $2.9\\times$ higher throughput for SQuAD fine-tuning. In addition, we\nprovide theoretical analysis for our proposed work.",
          "link": "http://arxiv.org/abs/2102.02888",
          "publishedOn": "2021-07-01T01:59:34.646Z",
          "wordCount": 706,
          "title": "1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. (arXiv:2102.02888v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Napoles_G/0/1/0/all/0/1\">Gonzalo N&#xe1;poles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grau_I/0/1/0/all/0/1\">Isel Grau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jastrzebska_A/0/1/0/all/0/1\">Agnieszka Jastrzebska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salgueiro_Y/0/1/0/all/0/1\">Yamisleydi Salgueiro</a>",
          "description": "In this paper, we present a recurrent neural system named Long Short-term\nCognitive Networks (LSTCNs) as a generalisation of the Short-term Cognitive\nNetwork (STCN) model. Such a generalisation is motivated by the difficulty of\nforecasting very long time series in an efficient, greener fashion. The LSTCN\nmodel can be defined as a collection of STCN blocks, each processing a specific\ntime patch of the (multivariate) time series being modelled. In this neural\nensemble, each block passes information to the subsequent one in the form of a\nweight matrix referred to as the prior knowledge matrix. As a second\ncontribution, we propose a deterministic learning algorithm to compute the\nlearnable weights while preserving the prior knowledge resulting from previous\nlearning processes. As a third contribution, we introduce a feature influence\nscore as a proxy to explain the forecasting process in multivariate time\nseries. The simulations using three case studies show that our neural system\nreports small forecasting errors while being up to thousands of times faster\nthan state-of-the-art recurrent models.",
          "link": "http://arxiv.org/abs/2106.16233",
          "publishedOn": "2021-07-01T01:59:34.640Z",
          "wordCount": 594,
          "title": "Long Short-term Cognitive Networks. (arXiv:2106.16233v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goutay_M/0/1/0/all/0/1\">Mathieu Goutay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorce_J/0/1/0/all/0/1\">Jean-Marie Gorce</a>",
          "description": "Machine learning (ML) starts to be widely used to enhance the performance of\nmulti-user multiple-input multiple-output (MU-MIMO) receivers. However, it is\nstill unclear if such methods are truly competitive with respect to\nconventional methods in realistic scenarios and under practical constraints. In\naddition to enabling accurate signal reconstruction on realistic channel\nmodels, MU-MIMO receive algorithms must allow for easy adaptation to a varying\nnumber of users without the need for retraining. In contrast to existing work,\nwe propose an ML-enhanced MU-MIMO receiver that builds on top of a conventional\nlinear minimum mean squared error (LMMSE) architecture. It preserves the\ninterpretability and scalability of the LMMSE receiver, while improving its\naccuracy in two ways. First, convolutional neural networks (CNNs) are used to\ncompute an approximation of the second-order statistics of the channel\nestimation error which are required for accurate equalization. Second, a\nCNN-based demapper jointly processes a large number of orthogonal\nfrequency-division multiplexing (OFDM) symbols and subcarriers, which allows it\nto compute better log likelihood ratios (LLRs) by compensating for channel\naging. The resulting architecture can be used in the up- and downlink and is\ntrained in an end-to-end manner, removing the need for hard-to-get perfect\nchannel state information (CSI) during the training phase. Simulation results\ndemonstrate consistent performance improvements over the baseline which are\nespecially pronounced in high mobility scenarios.",
          "link": "http://arxiv.org/abs/2012.08177",
          "publishedOn": "2021-07-01T01:59:34.634Z",
          "wordCount": 699,
          "title": "Machine Learning for MU-MIMO Receive Processing in OFDM Systems. (arXiv:2012.08177v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mylonas_C/0/1/0/all/0/1\">Charilaos Mylonas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_I/0/1/0/all/0/1\">Imad Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzi_E/0/1/0/all/0/1\">Eleni Chatzi</a>",
          "description": "Graph Networks (GNs) enable the fusion of prior knowledge and relational\nreasoning with flexible function approximations. In this work, a general\nGN-based model is proposed which takes full advantage of the relational\nmodeling capabilities of GNs and extends these to probabilistic modeling with\nVariational Bayes (VB). To that end, we combine complementary pre-existing\napproaches on VB for graph data and propose an approach that relies on\ngraph-structured latent and conditioning variables. It is demonstrated that\nNeural Processes can also be viewed through the lens of the proposed model. We\nshow applications on the problem of structured probability density modeling for\nsimulated and real wind farm monitoring data, as well as on the meta-learning\nof simulated Gaussian Process data. We release the source code, along with the\nsimulated datasets.",
          "link": "http://arxiv.org/abs/2106.16049",
          "publishedOn": "2021-07-01T01:59:34.628Z",
          "wordCount": 590,
          "title": "Relational VAE: A Continuous Latent Variable Model for Graph Structured Data. (arXiv:2106.16049v1 [cs.CE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ohnishi_M/0/1/0/all/0/1\">Motoya Ohnishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_I/0/1/0/all/0/1\">Isao Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowrey_K/0/1/0/all/0/1\">Kendall Lowrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeda_M/0/1/0/all/0/1\">Masahiro Ikeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_Y/0/1/0/all/0/1\">Yoshinobu Kawahara</a>",
          "description": "Most modern reinforcement learning algorithms optimize a cumulative\nsingle-step cost along a trajectory. The optimized motions are often\n'unnatural', representing, for example, behaviors with sudden accelerations\nthat waste energy and lack predictability. In this work, we present a novel\nparadigm of controlling nonlinear systems via the minimization of the Koopman\nspectrum cost: a cost over the Koopman operator of the controlled dynamics.\nThis induces a broader class of dynamical behaviors that evolve over stable\nmanifolds such as nonlinear oscillators, closed loops, and smooth movements. We\ndemonstrate that some dynamics realizations that are not possible with a\ncumulative cost are feasible in this paradigm. Moreover, we present a provably\nefficient online learning algorithm for our problem that enjoys a sub-linear\nregret bound under some structural assumptions.",
          "link": "http://arxiv.org/abs/2106.15775",
          "publishedOn": "2021-07-01T01:59:34.622Z",
          "wordCount": 571,
          "title": "Koopman Spectrum Nonlinear Regulator and Provably Efficient Online Learning. (arXiv:2106.15775v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unal_A/0/1/0/all/0/1\">Ali Burak &#xdc;nal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeifer_N/0/1/0/all/0/1\">Nico Pfeifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akgun_M/0/1/0/all/0/1\">Mete Akg&#xfc;n</a>",
          "description": "Computing an AUC as a performance measure to compare the quality of different\nmachine learning models is one of the final steps of many research projects.\nMany of these methods are trained on privacy-sensitive data and there are\nseveral different approaches like $\\epsilon$-differential privacy, federated\nmachine learning and methods based on cryptographic approaches if the datasets\ncannot be shared or evaluated jointly at one place. In this setting, it can\nalso be a problem to compute the global performance measure like an AUC, since\nthe labels might also contain privacy-sensitive information. There have been\napproaches based on $\\epsilon$-differential privacy to deal with this problem,\nbut to the best of our knowledge, no exact privacy preserving solution has been\nintroduced. In this paper, we propose an MPC-based framework, called \\fw{},\nwith private merging of sorted lists and novel methods for comparing two\nsecret-shared values, selecting between two secret-shared values, converting\nthe modulus, and performing division to compute the exact AUC as one could\nobtain on the pooled original test samples. With \\fw{} computation of the exact\narea under precision-recall curve and receiver operating characteristic curve\nis even possible when ties between prediction confidence values exist. To show\nthe applicability of \\fw{}, we use it to evaluate a model trained to predict\nacute myeloid leukemia therapy response and we also assess its scalability via\nexperiments on synthetic data. The experiments show that we efficiently compute\nexactly the same AUC with both evaluation metrics in a privacy preserving\nmanner as one can obtain on the pooled test samples in the plaintext domain.\nOur solution provides security against semi-honest corruption of at most one of\nthe servers performing the secure computation.",
          "link": "http://arxiv.org/abs/2102.08788",
          "publishedOn": "2021-07-01T01:59:34.605Z",
          "wordCount": 753,
          "title": "ppAURORA: Privacy Preserving Area Under Receiver Operating Characteristic and Precision-Recall Curves with Secure 3-Party Computation. (arXiv:2102.08788v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07850",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Corenflos_A/0/1/0/all/0/1\">Adrien Corenflos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thornton_J/0/1/0/all/0/1\">James Thornton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1\">George Deligiannidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>",
          "description": "Particle Filtering (PF) methods are an established class of procedures for\nperforming inference in non-linear state-space models. Resampling is a key\ningredient of PF, necessary to obtain low variance likelihood and states\nestimates. However, traditional resampling methods result in PF-based loss\nfunctions being non-differentiable with respect to model and PF parameters. In\na variational inference context, resampling also yields high variance gradient\nestimates of the PF-based evidence lower bound. By leveraging optimal transport\nideas, we introduce a principled differentiable particle filter and provide\nconvergence results. We demonstrate this novel method on a variety of\napplications.",
          "link": "http://arxiv.org/abs/2102.07850",
          "publishedOn": "2021-07-01T01:59:34.588Z",
          "wordCount": 571,
          "title": "Differentiable Particle Filtering via Entropy-Regularized Optimal Transport. (arXiv:2102.07850v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gama_R/0/1/0/all/0/1\">Ricardo Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_H/0/1/0/all/0/1\">Hugo L. Fernandes</a>",
          "description": "The Orienteering Problem with Time Windows (OPTW) is a combinatorial\noptimization problem where the goal is to maximize the total score collected\nfrom different visited locations. The application of neural network models to\ncombinatorial optimization has recently shown promising results in dealing with\nsimilar problems, like the Travelling Salesman Problem. A neural network allows\nlearning solutions using reinforcement learning or supervised learning,\ndepending on the available data. After the learning stage, it can be\ngeneralized and quickly fine-tuned to further improve performance and\npersonalization. The advantages are evident since, for real-world applications,\nsolution quality, personalization, and execution times are all important\nfactors that should be taken into account.\n\nThis study explores the use of Pointer Network models trained using\nreinforcement learning to solve the OPTW problem. We propose a modified\narchitecture that leverages Pointer Networks to better address problems related\nwith dynamic time-dependent constraints. Among its various applications, the\nOPTW can be used to model the Tourist Trip Design Problem (TTDP). We train the\nPointer Network with the TTDP problem in mind, by sampling variables that can\nchange across tourists visiting a particular instance-region: starting\nposition, starting time, available time, and the scores given to each point of\ninterest. Once a model-region is trained, it can infer a solution for a\nparticular tourist using beam search. We based the assessment of our approach\non several existing benchmark OPTW instances. We show that it generalizes\nacross different tourists that visit each region and that it generally\noutperforms the most commonly used heuristic, while computing the solution in\nrealistic times.",
          "link": "http://arxiv.org/abs/2011.03647",
          "publishedOn": "2021-07-01T01:59:34.575Z",
          "wordCount": 718,
          "title": "A Reinforcement Learning Approach to the Orienteering Problem with Time Windows. (arXiv:2011.03647v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puchkin_N/0/1/0/all/0/1\">Nikita Puchkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhivotovskiy_N/0/1/0/all/0/1\">Nikita Zhivotovskiy</a>",
          "description": "We show that in pool-based active classification without assumptions on the\nunderlying distribution, if the learner is given the power to abstain from some\npredictions by paying the price marginally smaller than the average loss $1/2$\nof a random guess, exponential savings in the number of label requests are\npossible whenever they are possible in the corresponding realizable problem. We\nextend this result to provide a necessary and sufficient condition for\nexponential savings in pool-based active classification under the model\nmisspecification.",
          "link": "http://arxiv.org/abs/2102.00451",
          "publishedOn": "2021-07-01T01:59:34.558Z",
          "wordCount": 556,
          "title": "Exponential Savings in Agnostic Active Learning through Abstention. (arXiv:2102.00451v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16239",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Piotrowski_T/0/1/0/all/0/1\">Tomasz Piotrowski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cavalcante_R/0/1/0/all/0/1\">Renato L. G. Cavalcante</a>",
          "description": "We derive conditions for the existence of fixed points of neural networks, an\nimportant research objective to understand their behavior in modern\napplications involving autoencoders and loop unrolling techniques, among\nothers. In particular, we focus on networks with nonnegative inputs and\nnonnegative network parameters, as often considered in the literature. We show\nthat such networks can be recognized as monotonic and (weakly) scalable\nfunctions within the framework of nonlinear Perron-Frobenius theory. This fact\nenables us to derive conditions for the existence of a nonempty fixed point set\nof the neural networks, and these conditions are weaker than those obtained\nrecently using arguments in convex analysis, which are typically based on the\nassumption of nonexpansivity of the activation functions. Furthermore, we prove\nthat the shape of the fixed point set of monotonic and weakly scalable neural\nnetworks is often an interval, which degenerates to a point for the case of\nscalable networks. The chief results of this paper are verified in numerical\nsimulations, where we consider an autoencoder-type network that first\ncompresses angular power spectra in massive MIMO systems, and, second,\nreconstruct the input spectra from the compressed signal.",
          "link": "http://arxiv.org/abs/2106.16239",
          "publishedOn": "2021-07-01T01:59:34.553Z",
          "wordCount": 624,
          "title": "Fixed points of monotonic and (weakly) scalable neural networks. (arXiv:2106.16239v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leye Wang</a>",
          "description": "In the big data and AI era, context is widely exploited as extra information\nwhich makes it easier to learn a more complex pattern in machine learning\nsystems. However, most of the existing related studies seldom take context into\naccount. The difficulty lies in the unknown generalization ability of both\ncontext and its modeling techniques across different scenarios. To fill the\nabove gaps, we conduct a large-scale analytical and empirical study on the\nspatiotemporal crowd prediction (STCFP) problem that is a widely-studied and\nhot research topic. We mainly make three efforts:(i) we develop new taxonomy\nabout both context features and context modeling techniques based on extensive\ninvestigations in prevailing STCFP research; (ii) we conduct extensive\nexperiments on seven datasets with hundreds of millions of records to\nquantitatively evaluate the generalization ability of both distinct context\nfeatures and context modeling techniques; (iii) we summarize some guidelines\nfor researchers to conveniently utilize context in diverse applications.",
          "link": "http://arxiv.org/abs/2106.16046",
          "publishedOn": "2021-07-01T01:59:34.547Z",
          "wordCount": 588,
          "title": "Exploring Context Modeling Techniques on the Spatiotemporal Crowd Flow Prediction. (arXiv:2106.16046v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15980",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jerfel_G/0/1/0/all/0/1\">Ghassen Jerfel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Serena Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fannjiang_C/0/1/0/all/0/1\">Clara Fannjiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heller_K/0/1/0/all/0/1\">Katherine A. Heller</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ma_Y/0/1/0/all/0/1\">Yian Ma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Variational Inference (VI) is a popular alternative to asymptotically exact\nsampling in Bayesian inference. Its main workhorse is optimization over a\nreverse Kullback-Leibler divergence (RKL), which typically underestimates the\ntail of the posterior leading to miscalibration and potential degeneracy.\nImportance sampling (IS), on the other hand, is often used to fine-tune and\nde-bias the estimates of approximate Bayesian inference procedures. The quality\nof IS crucially depends on the choice of the proposal distribution. Ideally,\nthe proposal distribution has heavier tails than the target, which is rarely\nachievable by minimizing the RKL. We thus propose a novel combination of\noptimization and sampling techniques for approximate Bayesian inference by\nconstructing an IS proposal distribution through the minimization of a forward\nKL (FKL) divergence. This approach guarantees asymptotic consistency and a fast\nconvergence towards both the optimal IS estimator and the optimal variational\napproximation. We empirically demonstrate on real data that our method is\ncompetitive with variational boosting and MCMC.",
          "link": "http://arxiv.org/abs/2106.15980",
          "publishedOn": "2021-07-01T01:59:34.541Z",
          "wordCount": 616,
          "title": "Variational Refinement for Importance Sampling Using the Forward Kullback-Leibler Divergence. (arXiv:2106.15980v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1\">Felix Leeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "The encoders and decoders of autoencoders effectively project the input onto\nlearned manifolds in the latent space and data space respectively. We propose a\nframework, called latent responses, for probing the learned data manifold using\ninterventions in the latent space. Using this framework, we investigate \"holes\"\nin the representation to quantitatively ascertain to what extent the latent\nspace of a trained VAE is consistent with the chosen prior. Furthermore, we use\nthe identified structure to improve interpolation between latent vectors. We\nevaluate how our analyses improve the quality of the generated samples using\nthe VAE on a variety of benchmark datasets.",
          "link": "http://arxiv.org/abs/2106.16091",
          "publishedOn": "2021-07-01T01:59:34.491Z",
          "wordCount": 541,
          "title": "Interventional Assays for the Latent Space of Autoencoders. (arXiv:2106.16091v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sansone_E/0/1/0/all/0/1\">Emanuele Sansone</a>",
          "description": "This work considers the problem of learning structured representations from\nraw images using self-supervised learning. We propose a principled framework\nbased on a mutual information objective, which integrates self-supervised and\nstructure learning. Furthermore, we devise a post-hoc procedure to interpret\nthe meaning of the learnt representations. Preliminary experiments on CIFAR-10\nshow that the proposed framework achieves higher generalization performance in\ndownstream classification tasks and provides more interpretable representations\ncompared to the ones learnt through traditional self-supervised learning.",
          "link": "http://arxiv.org/abs/2106.16060",
          "publishedOn": "2021-07-01T01:59:34.477Z",
          "wordCount": 503,
          "title": "Leveraging Hidden Structure in Self-Supervised Learning. (arXiv:2106.16060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.05944",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kirschner_J/0/1/0/all/0/1\">Johannes Kirschner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lattimore_T/0/1/0/all/0/1\">Tor Lattimore</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vernade_C/0/1/0/all/0/1\">Claire Vernade</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>",
          "description": "We introduce a simple and efficient algorithm for stochastic linear bandits\nwith finitely many actions that is asymptotically optimal and (nearly)\nworst-case optimal in finite time. The approach is based on the frequentist\ninformation-directed sampling (IDS) framework, with a surrogate for the\ninformation gain that is informed by the optimization problem that defines the\nasymptotic lower bound. Our analysis sheds light on how IDS balances the\ntrade-off between regret and information and uncovers a surprising connection\nbetween the recently proposed primal-dual methods and the IDS algorithm. We\ndemonstrate empirically that IDS is competitive with UCB in finite-time, and\ncan be significantly better in the asymptotic regime.",
          "link": "http://arxiv.org/abs/2011.05944",
          "publishedOn": "2021-07-01T01:59:34.464Z",
          "wordCount": 574,
          "title": "Asymptotically Optimal Information-Directed Sampling. (arXiv:2011.05944v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16087",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kokalj_Filipovic_S/0/1/0/all/0/1\">Silvija Kokalj-Filipovic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toliver_P/0/1/0/all/0/1\">Paul Toliver</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johnson_W/0/1/0/all/0/1\">William Johnson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miller_R/0/1/0/all/0/1\">Rob Miller</a>",
          "description": "Current radio frequency (RF) sensors at the Edge lack the computational\nresources to support practical, in-situ training for intelligent spectrum\nmonitoring, and sensor data classification in general. We propose a solution\nvia Deep Delay Loop Reservoir Computing (DLR), a processing architecture that\nsupports general machine learning algorithms on compact mobile devices by\nleveraging delay-loop reservoir computing in combination with innovative\nelectrooptical hardware. With both digital and photonic realizations of our\ndesign of the loops, DLR delivers reductions in form factor, hardware\ncomplexity and latency, compared to the State-of-the-Art (SoA). The main impact\nof the reservoir is to project the input data into a higher dimensional space\nof reservoir state vectors in order to linearly separate the input classes.\nOnce the classes are well separated, traditionally complex, power-hungry\nclassification models are no longer needed for the learning process. Yet, even\nwith simple classifiers based on Ridge regression (RR), the complexity grows at\nleast quadratically with the input size. Hence, the hardware reduction required\nfor training on compact devices is in contradiction with the large dimension of\nstate vectors. DLR employs a RR-based classifier to exceed the SoA accuracy,\nwhile further reducing power consumption by leveraging the architecture of\nparallel (split) loops. We present DLR architectures composed of multiple\nsmaller loops whose state vectors are linearly combined to create a lower\ndimensional input into Ridge regression. We demonstrate the advantages of using\nDLR for two distinct applications: RF Specific Emitter Identification (SEI) for\nIoT authentication, and wireless protocol recognition for IoT situational\nawareness.",
          "link": "http://arxiv.org/abs/2106.16087",
          "publishedOn": "2021-07-01T01:59:34.417Z",
          "wordCount": 710,
          "title": "Reservoir Based Edge Training on RF Data To Deliver Intelligent and Efficient IoT Spectrum Sensors. (arXiv:2106.16087v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_V/0/1/0/all/0/1\">Vincent Gao</a>",
          "description": "E-commerce stores collect customer feedback to let sellers learn about\ncustomer concerns and enhance customer order experience. Because customer\nfeedback often contains redundant information, a concise summary of the\nfeedback can be generated to help sellers better understand the issues causing\ncustomer dissatisfaction. Previous state-of-the-art abstractive text\nsummarization models make two major types of factual errors when producing\nsummaries from customer feedback, which are wrong entity detection (WED) and\nincorrect product-defect description (IPD). In this work, we introduce a set of\nmethods to enhance the factual consistency of abstractive summarization on\ncustomer feedback. We augment the training data with artificially corrupted\nsummaries, and use them as counterparts of the target summaries. We add a\ncontrastive loss term into the training objective so that the model learns to\navoid certain factual errors. Evaluation results show that a large portion of\nWED and IPD errors are alleviated for BART and T5. Furthermore, our approaches\ndo not depend on the structure of the summarization model and thus are\ngeneralizable to any abstractive summarization systems.",
          "link": "http://arxiv.org/abs/2106.16188",
          "publishedOn": "2021-07-01T01:59:34.392Z",
          "wordCount": 606,
          "title": "Improving Factual Consistency of Abstractive Summarization on Customer Feedback. (arXiv:2106.16188v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2001.10133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Ping Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>",
          "description": "This paper studies the decentralized optimization and learning problem where\nmultiple interconnected agents aim to learn an optimal decision function\ndefined over a reproducing kernel Hilbert space by jointly minimizing a global\nobjective function, with access to their own locally observed dataset. As a\nnon-parametric approach, kernel learning faces a major challenge in distributed\nimplementation: the decision variables of local objective functions are\ndata-dependent and thus cannot be optimized under the decentralized consensus\nframework without any raw data exchange among agents. To circumvent this major\nchallenge, we leverage the random feature (RF) approximation approach to enable\nconsensus on the function modeled in the RF space by data-independent\nparameters across different agents. We then design an iterative algorithm,\ntermed DKLA, for fast-convergent implementation via ADMM. Based on DKLA, we\nfurther develop a communication-censored kernel learning (COKE) algorithm that\nreduces the communication load of DKLA by preventing an agent from transmitting\nat every iteration unless its local updates are deemed informative. Theoretical\nresults in terms of linear convergence guarantee and generalization performance\nanalysis of DKLA and COKE are provided. Comprehensive tests on both synthetic\nand real datasets are conducted to verify the communication efficiency and\nlearning effectiveness of COKE.",
          "link": "http://arxiv.org/abs/2001.10133",
          "publishedOn": "2021-07-01T01:59:34.375Z",
          "wordCount": 655,
          "title": "COKE: Communication-Censored Decentralized Kernel Learning. (arXiv:2001.10133v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16079",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pihlajasalo_J/0/1/0/all/0/1\">Jaakko Pihlajasalo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korpi_D/0/1/0/all/0/1\">Dani Korpi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Honkala_M/0/1/0/all/0/1\">Mikko Honkala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huttunen_J/0/1/0/all/0/1\">Janne M.J. Huttunen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riihonen_T/0/1/0/all/0/1\">Taneli Riihonen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Talvitie_J/0/1/0/all/0/1\">Jukka Talvitie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brihuega_A/0/1/0/all/0/1\">Alberto Brihuega</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uusitalo_M/0/1/0/all/0/1\">Mikko A. Uusitalo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valkama_M/0/1/0/all/0/1\">Mikko Valkama</a>",
          "description": "In this paper, we propose a machine learning (ML) based physical layer\nreceiver solution for demodulating OFDM signals that are subject to a high\nlevel of nonlinear distortion. Specifically, a novel deep learning based\nconvolutional neural network receiver is devised, containing layers in both\ntime- and frequency domains, allowing to demodulate and decode the transmitted\nbits reliably despite the high error vector magnitude (EVM) in the transmit\nsignal. Extensive set of numerical results is provided, in the context of 5G NR\nuplink incorporating also measured terminal power amplifier characteristics.\nThe obtained results show that the proposed receiver system is able to clearly\noutperform classical linear receivers as well as existing ML receiver\napproaches, especially when the EVM is high in comparison with modulation\norder. The proposed ML receiver can thus facilitate pushing the terminal power\namplifier (PA) systems deeper into saturation, and thereon improve the terminal\npower-efficiency, radiated power and network coverage.",
          "link": "http://arxiv.org/abs/2106.16079",
          "publishedOn": "2021-07-01T01:59:34.369Z",
          "wordCount": 617,
          "title": "HybridDeepRx: Deep Learning Receiver for High-EVM Signals. (arXiv:2106.16079v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sidak Pal Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachmann_G/0/1/0/all/0/1\">Gregor Bachmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>",
          "description": "The Hessian of a neural network captures parameter interactions through\nsecond-order derivatives of the loss. It is a fundamental object of study,\nclosely tied to various problems in deep learning, including model design,\noptimization, and generalization. Most prior work has been empirical, typically\nfocusing on low-rank approximations and heuristics that are blind to the\nnetwork structure. In contrast, we develop theoretical tools to analyze the\nrange of the Hessian map, providing us with a precise understanding of its rank\ndeficiency as well as the structural reasons behind it. This yields exact\nformulas and tight upper bounds for the Hessian rank of deep linear networks,\nallowing for an elegant interpretation in terms of rank deficiency. Moreover,\nwe demonstrate that our bounds remain faithful as an estimate of the numerical\nHessian rank, for a larger class of models such as rectified and hyperbolic\ntangent networks. Further, we also investigate the implications of model\narchitecture (e.g.~width, depth, bias) on the rank deficiency. Overall, our\nwork provides novel insights into the source and extent of redundancy in\noverparameterized networks.",
          "link": "http://arxiv.org/abs/2106.16225",
          "publishedOn": "2021-07-01T01:59:34.364Z",
          "wordCount": 627,
          "title": "Analytic Insights into Structure and Rank of Neural Network Hessian Maps. (arXiv:2106.16225v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Franzese_G/0/1/0/all/0/1\">Giulio Franzese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milios_D/0/1/0/all/0/1\">Dimitrios Milios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippone_M/0/1/0/all/0/1\">Maurizio Filippone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michiardi_P/0/1/0/all/0/1\">Pietro Michiardi</a>",
          "description": "In this work, we revisit the theoretical properties of Hamiltonian stochastic\ndifferential equations (SDEs) for Bayesian posterior sampling, and we study the\ntwo types of errors that arise from numerical SDE simulation: the\ndiscretization error and the error due to noisy gradient estimates in the\ncontext of data subsampling. We consider overlooked results describing the\nergodic convergence rates of numerical integration schemes, and we produce a\nnovel analysis for the effect of mini-batches through the lens of differential\noperator splitting. In our analysis, the stochastic component of the proposed\nHamiltonian SDE is decoupled from the gradient noise, for which we make no\nnormality assumptions. This allows us to derive interesting connections among\ndifferent sampling schemes, including the original Hamiltonian Monte Carlo\n(HMC) algorithm, and explain their performance. We show that for a careful\nselection of numerical integrators, both errors vanish at a rate\n$\\mathcal{O}(\\eta^2)$, where $\\eta$ is the integrator step size. Our\ntheoretical results are supported by an empirical study on a variety of\nregression and classification tasks for Bayesian neural networks.",
          "link": "http://arxiv.org/abs/2106.16200",
          "publishedOn": "2021-07-01T01:59:34.358Z",
          "wordCount": 603,
          "title": "A Unified View of Stochastic Hamiltonian Sampling. (arXiv:2106.16200v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shaan Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "Despite a series of recent successes in reinforcement learning (RL), many RL\nalgorithms remain sensitive to hyperparameters. As such, there has recently\nbeen interest in the field of AutoRL, which seeks to automate design decisions\nto create more general algorithms. Recent work suggests that population based\napproaches may be effective AutoRL algorithms, by learning hyperparameter\nschedules on the fly. In particular, the PB2 algorithm is able to achieve\nstrong performance in RL tasks by formulating online hyperparameter\noptimization as time varying GP-bandit problem, while also providing\ntheoretical guarantees. However, PB2 is only designed to work for continuous\nhyperparameters, which severely limits its utility in practice. In this paper\nwe introduce a new (provably) efficient hierarchical approach for optimizing\nboth continuous and categorical variables, using a new time-varying bandit\nalgorithm specifically designed for the population based training regime. We\nevaluate our approach on the challenging Procgen benchmark, where we show that\nexplicitly modelling dependence between data augmentation and other\nhyperparameters improves generalization.",
          "link": "http://arxiv.org/abs/2106.15883",
          "publishedOn": "2021-07-01T01:59:34.352Z",
          "wordCount": 599,
          "title": "Tuning Mixed Input Hyperparameters on the Fly for Efficient Population Based AutoRL. (arXiv:2106.15883v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabani_H/0/1/0/all/0/1\">Hamid Tabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramaniam_A/0/1/0/all/0/1\">Ajay Balasubramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1\">Shabbir Marzban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Transformers provide promising accuracy and have become popular and used in\nvarious domains such as natural language processing and computer vision.\nHowever, due to their massive number of model parameters, memory and\ncomputation requirements, they are not suitable for resource-constrained\nlow-power devices. Even with high-performance and specialized devices, the\nmemory bandwidth can become a performance-limiting bottleneck. In this paper,\nwe present a performance analysis of state-of-the-art vision transformers on\nseveral devices. We propose to reduce the overall memory footprint and memory\ntransfers by clustering the model parameters. We show that by using only 64\nclusters to represent model parameters, it is possible to reduce the data\ntransfer from the main memory by more than 4x, achieve up to 22% speedup and\n39% energy savings on mobile devices with less than 0.1% accuracy loss.",
          "link": "http://arxiv.org/abs/2106.16006",
          "publishedOn": "2021-07-01T01:59:34.333Z",
          "wordCount": 589,
          "title": "Improving the Efficiency of Transformers for Resource-Constrained Devices. (arXiv:2106.16006v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhongyuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_D/0/1/0/all/0/1\">Dong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>",
          "description": "We introduce a unified framework, formulated as general latent space models,\nto study complex higher-order network interactions among multiple entities. Our\nframework covers several popular models in recent network analysis literature,\nincluding mixture multi-layer latent space model and hypergraph latent space\nmodel. We formulate the relationship between the latent positions and the\nobserved data via a generalized multilinear kernel as the link function. While\nour model enjoys decent generality, its maximum likelihood parameter estimation\nis also convenient via a generalized tensor decomposition procedure.We propose\na novel algorithm using projected gradient descent on Grassmannians. We also\ndevelop original theoretical guarantees for our algorithm. First, we show its\nlinear convergence under mild conditions. Second, we establish finite-sample\nstatistical error rates of latent position estimation, determined by the signal\nstrength, degrees of freedom and the smoothness of link function, for both\ngeneral and specific latent space models. We demonstrate the effectiveness of\nour method on synthetic data. We also showcase the merit of our method on two\nreal-world datasets that are conventionally described by different specific\nmodels in producing meaningful and interpretable parameter estimations and\naccurate link prediction. We demonstrate the effectiveness of our method on\nsynthetic data. We also showcase the merit of our method on two real-world\ndatasets that are conventionally described by different specific models in\nproducing meaningful and interpretable parameter estimations and accurate link\nprediction.",
          "link": "http://arxiv.org/abs/2106.16042",
          "publishedOn": "2021-07-01T01:59:34.324Z",
          "wordCount": 667,
          "title": "Latent Space Model for Higher-order Networks and Generalized Tensor Decomposition. (arXiv:2106.16042v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ceran_E/0/1/0/all/0/1\">Elif Tugce Ceran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1\">Andras Gyorgy</a>",
          "description": "The time average expected age of information (AoI) is studied for status\nupdates sent over an error-prone channel from an energy-harvesting transmitter\nwith a finite-capacity battery. Energy cost of sensing new status updates is\ntaken into account as well as the transmission energy cost better capturing\npractical systems. The optimal scheduling policy is first studied under the\nhybrid automatic repeat request (HARQ) protocol when the channel and energy\nharvesting statistics are known, and the existence of a threshold-based optimal\npolicy is shown. For the case of unknown environments, average-cost\nreinforcement-learning algorithms are proposed that learn the system parameters\nand the status update policy in real-time. The effectiveness of the proposed\nmethods is demonstrated through numerical results.",
          "link": "http://arxiv.org/abs/2106.16037",
          "publishedOn": "2021-07-01T01:59:34.307Z",
          "wordCount": 577,
          "title": "Learning to Minimize Age of Information over an Unreliable Channel with Energy Harvesting. (arXiv:2106.16037v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16048",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mou_Z/0/1/0/all/0/1\">Zhiyu Mou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1\">Feifei Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1\">Qihui Wu</a>",
          "description": "In this paper, we study the self-healing problem of unmanned aerial vehicle\n(UAV) swarm network (USNET) that is required to quickly rebuild the\ncommunication connectivity under unpredictable external disruptions (UEDs).\nFirstly, to cope with the one-off UEDs, we propose a graph convolutional neural\nnetwork (GCN) and find the recovery topology of the USNET in an on-line manner.\nSecondly, to cope with general UEDs, we develop a GCN based trajectory planning\nalgorithm that can make UAVs rebuild the communication connectivity during the\nself-healing process. We also design a meta learning scheme to facilitate the\non-line executions of the GCN. Numerical results show that the proposed\nalgorithms can rebuild the communication connectivity of the USNET more quickly\nthan the existing algorithms under both one-off UEDs and general UEDs. The\nsimulation results also show that the meta learning scheme can not only enhance\nthe performance of the GCN but also reduce the time complexity of the on-line\nexecutions.",
          "link": "http://arxiv.org/abs/2106.16048",
          "publishedOn": "2021-07-01T01:59:34.290Z",
          "wordCount": 595,
          "title": "Resilient UAV Swarm Communications with Graph Convolutional Neural Network. (arXiv:2106.16048v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_D/0/1/0/all/0/1\">Dan Geiger</a>",
          "description": "We develop simple methods for constructing likelihoods and parameter priors\nfor learning about the parameters and structure of a Bayesian network. In\nparticular, we introduce several assumptions that permit the construction of\nlikelihoods and parameter priors for a large number of Bayesian-network\nstructures from a small set of assessments. The most notable assumption is that\nof likelihood equivalence, which says that data can not help to discriminate\nnetwork structures that encode the same assertions of conditional independence.\nWe describe the constructions that follow from these assumptions, and also\npresent a method for directly computing the marginal likelihood of a random\nsample with no missing observations. Also, we show how these assumptions lead\nto a general framework for characterizing parameter priors of multivariate\ndistributions.",
          "link": "http://arxiv.org/abs/2105.06241",
          "publishedOn": "2021-07-01T01:59:34.181Z",
          "wordCount": 592,
          "title": "Likelihoods and Parameter Priors for Bayesian Networks. (arXiv:2105.06241v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "Commonsense inference to understand and explain human language is a\nfundamental research problem in natural language processing. Explaining human\nconversations poses a great challenge as it requires contextual understanding,\nplanning, inference, and several aspects of reasoning including causal,\ntemporal, and commonsense reasoning. In this work, we introduce CIDER -- a\nmanually curated dataset that contains dyadic dialogue explanations in the form\nof implicit and explicit knowledge triplets inferred using contextual\ncommonsense inference. Extracting such rich explanations from conversations can\nbe conducive to improving several downstream applications. The annotated\ntriplets are categorized by the type of commonsense knowledge present (e.g.,\ncausal, conditional, temporal). We set up three different tasks conditioned on\nthe annotated dataset: Dialogue-level Natural Language Inference, Span\nExtraction, and Multi-choice Span Selection. Baseline results obtained with\ntransformer-based models reveal that the tasks are difficult, paving the way\nfor promising future research. The dataset and the baseline implementations are\npublicly available at https://cider-task.github.io/cider/.",
          "link": "http://arxiv.org/abs/2106.00510",
          "publishedOn": "2021-07-01T01:59:34.166Z",
          "wordCount": 618,
          "title": "CIDER: Commonsense Inference for Dialogue Explanation and Reasoning. (arXiv:2106.00510v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15921",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Thin_A/0/1/0/all/0/1\">Achille Thin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kotelevskii_N/0/1/0/all/0/1\">Nikita Kotelevskii</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1\">Eric Moulines</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Panov_M/0/1/0/all/0/1\">Maxim Panov</a>",
          "description": "Variational auto-encoders (VAE) are popular deep latent variable models which\nare trained by maximizing an Evidence Lower Bound (ELBO). To obtain tighter\nELBO and hence better variational approximations, it has been proposed to use\nimportance sampling to get a lower variance estimate of the evidence. However,\nimportance sampling is known to perform poorly in high dimensions. While it has\nbeen suggested many times in the literature to use more sophisticated\nalgorithms such as Annealed Importance Sampling (AIS) and its Sequential\nImportance Sampling (SIS) extensions, the potential benefits brought by these\nadvanced techniques have never been realized for VAE: the AIS estimate cannot\nbe easily differentiated, while SIS requires the specification of carefully\nchosen backward Markov kernels. In this paper, we address both issues and\ndemonstrate the performance of the resulting Monte Carlo VAEs on a variety of\napplications.",
          "link": "http://arxiv.org/abs/2106.15921",
          "publishedOn": "2021-07-01T01:59:34.160Z",
          "wordCount": 567,
          "title": "Monte Carlo Variational Auto-Encoders. (arXiv:2106.15921v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pitchforth_D/0/1/0/all/0/1\">Daniel J Pitchforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_T/0/1/0/all/0/1\">Timothy J Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tygesen_U/0/1/0/all/0/1\">Ulf T Tygesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_E/0/1/0/all/0/1\">Elizabeth J Cross</a>",
          "description": "The quantification of wave loading on offshore structures and components is a\ncrucial element in the assessment of their useful remaining life. In many\napplications the well-known Morison's equation is employed to estimate the\nforcing from waves with assumed particle velocities and accelerations. This\npaper develops a grey-box modelling approach to improve the predictions of the\nforce on structural members. A grey-box model intends to exploit the enhanced\npredictive capabilities of data-based modelling whilst retaining physical\ninsight into the behaviour of the system; in the context of the work carried\nout here, this can be considered as physics-informed machine learning. There\nare a number of possible approaches to establish a grey-box model. This paper\ndemonstrates two means of combining physics (white box) and data-based (black\nbox) components; one where the model is a simple summation of the two\ncomponents, the second where the white-box prediction is fed into the black box\nas an additional input. Here Morison's equation is used as the physics-based\ncomponent in combination with a data-based Gaussian process NARX - a dynamic\nvariant of the more well-known Gaussian process regression. Two key challenges\nwith employing the GP-NARX formulation that are addressed here are the\nselection of appropriate lag terms and the proper treatment of uncertainty\npropagation within the dynamic GP. The best performing grey-box model, the\nresidual modelling GP-NARX, was able to achieve a 29.13\\% and 5.48\\% relative\nreduction in NMSE over Morison's Equation and a black-box GP-NARX respectively,\nalongside significant benefits in extrapolative capabilities of the model, in\ncircumstances of low dataset coverage.",
          "link": "http://arxiv.org/abs/2105.13813",
          "publishedOn": "2021-07-01T01:59:34.129Z",
          "wordCount": 732,
          "title": "Grey-box models for wave loading prediction. (arXiv:2105.13813v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05739",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Domingo_Enrich_C/0/1/0/all/0/1\">Carles Domingo-Enrich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mroueh_Y/0/1/0/all/0/1\">Youssef Mroueh</a>",
          "description": "Several works in implicit and explicit generative modeling empirically\nobserved that feature-learning discriminators outperform fixed-kernel\ndiscriminators in terms of the sample quality of the models. We provide\nseparation results between probability metrics with fixed-kernel and\nfeature-learning discriminators using the function classes $\\mathcal{F}_2$ and\n$\\mathcal{F}_1$ respectively, which were developed to study overparametrized\ntwo-layer neural networks. In particular, we construct pairs of distributions\nover hyper-spheres that can not be discriminated by fixed kernel\n$(\\mathcal{F}_2)$ integral probability metric (IPM) and Stein discrepancy (SD)\nin high dimensions, but that can be discriminated by their feature learning\n($\\mathcal{F}_1$) counterparts. To further study the separation we provide\nlinks between the $\\mathcal{F}_1$ and $\\mathcal{F}_2$ IPMs with sliced\nWasserstein distances. Our work suggests that fixed-kernel discriminators\nperform worse than their feature learning counterparts because their\ncorresponding metrics are weaker.",
          "link": "http://arxiv.org/abs/2106.05739",
          "publishedOn": "2021-07-01T01:59:34.100Z",
          "wordCount": 601,
          "title": "Separation Results between Fixed-Kernel and Feature-Learning Probability Metrics. (arXiv:2106.05739v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byeonggeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Simyung Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_D/0/1/0/all/0/1\">Dooyong Sung</a>",
          "description": "Keyword spotting is an important research field because it plays a key role\nin device wake-up and user interaction on smart devices. However, it is\nchallenging to minimize errors while operating efficiently in devices with\nlimited resources such as mobile phones. We present a broadcasted residual\nlearning method to achieve high accuracy with small model size and\ncomputational load. Our method configures most of the residual functions as 1D\ntemporal convolution while still allows 2D convolution together using a\nbroadcasted-residual connection that expands temporal output to\nfrequency-temporal dimension. This residual mapping enables the network to\neffectively represent useful audio features with much less computation than\nconventional convolutional neural networks. We also propose a novel network\narchitecture, Broadcasting-residual network (BC-ResNet), based on broadcasted\nresidual learning and describe how to scale up the model according to the\ntarget device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7%\ntop-1 accuracy on Google speech command datasets v1 and v2, respectively, and\nconsistently outperform previous approaches, using fewer computations and\nparameters.",
          "link": "http://arxiv.org/abs/2106.04140",
          "publishedOn": "2021-07-01T01:59:34.094Z",
          "wordCount": 624,
          "title": "Broadcasted Residual Learning for Efficient Keyword Spotting. (arXiv:2106.04140v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1\">Thomas Kollar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskey_M/0/1/0/all/0/1\">Michael Laskey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_K/0/1/0/all/0/1\">Kevin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1\">Brijen Thananjeyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjersland_M/0/1/0/all/0/1\">Mark Tjersland</a>",
          "description": "Robot manipulation of unknown objects in unstructured environments is a\nchallenging problem due to the variety of shapes, materials, arrangements and\nlighting conditions. Even with large-scale real-world data collection, robust\nperception and manipulation of transparent and reflective objects across\nvarious lighting conditions remain challenging. To address these challenges we\npropose an approach to performing sim-to-real transfer of robotic perception.\nThe underlying model, SimNet, is trained as a single multi-headed neural\nnetwork using simulated stereo data as input and simulated object segmentation\nmasks, 3D oriented bounding boxes (OBBs), object keypoints, and disparity as\noutput. A key component of SimNet is the incorporation of a learned stereo\nsub-network that predicts disparity. SimNet is evaluated on 2D car detection,\nunknown object detection, and deformable object keypoint detection and\nsignificantly outperforms a baseline that uses a structured light RGB-D sensor.\nBy inferring grasp positions using the OBB and keypoint predictions, SimNet can\nbe used to perform end-to-end manipulation of unknown objects in both easy and\nhard scenarios using our fleet of Toyota HSR robots in four home environments.\nIn unknown object grasping experiments, the predictions from the baseline RGB-D\nnetwork and SimNet enable successful grasps of most of the easy objects.\nHowever, the RGB-D baseline only grasps 35% of the hard (e.g., transparent)\nobjects, while SimNet grasps 95%, suggesting that SimNet can enable robust\nmanipulation of unknown objects, including transparent objects, in unknown\nenvironments.",
          "link": "http://arxiv.org/abs/2106.16118",
          "publishedOn": "2021-07-01T01:59:34.088Z",
          "wordCount": 683,
          "title": "SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo. (arXiv:2106.16118v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16088",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Bajpai_S/0/1/0/all/0/1\">Supriya Bajpai</a>",
          "description": "In stock trading, feature extraction and trading strategy design are the two\nimportant tasks to achieve long-term benefits using machine learning\ntechniques. Several methods have been proposed to design trading strategy by\nacquiring trading signals to maximize the rewards. In the present paper the\ntheory of deep reinforcement learning is applied for stock trading strategy and\ninvestment decisions to Indian markets. The experiments are performed\nsystematically with three classical Deep Reinforcement Learning models Deep\nQ-Network, Double Deep Q-Network and Dueling Double Deep Q-Network on ten\nIndian stock datasets. The performance of the models are evaluated and\ncomparison is made.",
          "link": "http://arxiv.org/abs/2106.16088",
          "publishedOn": "2021-07-01T01:59:34.076Z",
          "wordCount": 540,
          "title": "Application of deep reinforcement learning for Indian stock trading automation. (arXiv:2106.16088v1 [q-fin.TR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07036",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Clyde_A/0/1/0/all/0/1\">Austin Clyde</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Brettin_T/0/1/0/all/0/1\">Thomas Brettin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Partin_A/0/1/0/all/0/1\">Alexander Partin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yoo_H/0/1/0/all/0/1\">Hyunseung Yoo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Babuji_Y/0/1/0/all/0/1\">Yadu Babuji</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Blaiszik_B/0/1/0/all/0/1\">Ben Blaiszik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Merzky_A/0/1/0/all/0/1\">Andre Merzky</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Turilli_M/0/1/0/all/0/1\">Matteo Turilli</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jha_S/0/1/0/all/0/1\">Shantenu Jha</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ramanathan_A/0/1/0/all/0/1\">Arvind Ramanathan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Stevens_R/0/1/0/all/0/1\">Rick Stevens</a>",
          "description": "We propose a benchmark to study surrogate model accuracy for protein-ligand\ndocking. We share a dataset consisting of 200 million 3D complex structures and\n2D structure scores across a consistent set of 13 million \"in-stock\" molecules\nover 15 receptors, or binding sites, across the SARS-CoV-2 proteome. Our work\nshows surrogate docking models have six orders of magnitude more throughput\nthan standard docking protocols on the same supercomputer node types. We\ndemonstrate the power of high-speed surrogate models by running each target\nagainst 1 billion molecules in under a day (50k predictions per GPU seconds).\nWe showcase a workflow for docking utilizing surrogate ML models as a\npre-filter. Our workflow is ten times faster at screening a library of\ncompounds than the standard technique, with an error rate less than 0.01\\% of\ndetecting the underlying best scoring 0.1\\% of compounds. Our analysis of the\nspeedup explains that to screen more molecules under a docking paradigm,\nanother order of magnitude speedup must come from model accuracy rather than\ncomputing speed (which, if increased, will not anymore alter our throughput to\nscreen molecules). We believe this is strong evidence for the community to\nbegin focusing on improving the accuracy of surrogate models to improve the\nability to screen massive compound libraries 100x or even 1000x faster than\ncurrent techniques.",
          "link": "http://arxiv.org/abs/2106.07036",
          "publishedOn": "2021-07-01T01:59:34.051Z",
          "wordCount": 734,
          "title": "Protein-Ligand Docking Surrogate Models: A SARS-CoV-2 Benchmark for Deep Learning Accelerated Virtual Screening. (arXiv:2106.07036v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wanying Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panariello_M/0/1/0/all/0/1\">Michele Panariello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patino_J/0/1/0/all/0/1\">Jose Patino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todisco_M/0/1/0/all/0/1\">Massimiliano Todisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1\">Nicholas Evans</a>",
          "description": "This paper reports the first successful application of a differentiable\narchitecture search (DARTS) approach to the deepfake and spoofing detection\nproblems. An example of neural architecture search, DARTS operates upon a\ncontinuous, differentiable search space which enables both the architecture and\nparameters to be optimised via gradient descent. Solutions based on\npartially-connected DARTS use random channel masking in the search space to\nreduce GPU time and automatically learn and optimise complex neural\narchitectures composed of convolutional operations and residual blocks. Despite\nbeing learned quickly with little human effort, the resulting networks are\ncompetitive with the best performing systems reported in the literature. Some\nare also far less complex, containing 85% fewer parameters than a Res2Net\ncompetitor.",
          "link": "http://arxiv.org/abs/2104.03123",
          "publishedOn": "2021-07-01T01:59:34.038Z",
          "wordCount": 594,
          "title": "Partially-Connected Differentiable Architecture Search for Deepfake and Spoofing Detection. (arXiv:2104.03123v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilhan_E/0/1/0/all/0/1\">Ercument Ilhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gow_J/0/1/0/all/0/1\">Jeremy Gow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Liebana_D/0/1/0/all/0/1\">Diego Perez-Liebana</a>",
          "description": "Deep Reinforcement Learning (RL) techniques can benefit greatly from\nleveraging prior experience, which can be either self-generated or acquired\nfrom other entities. Action advising is a framework that provides a flexible\nway to transfer such knowledge in the form of actions between teacher-student\npeers. However, due to the realistic concerns, the number of these interactions\nis limited with a budget; therefore, it is crucial to perform these in the most\nappropriate moments. There have been several promising studies recently that\naddress this problem setting especially from the student's perspective. Despite\ntheir success, they have some shortcomings when it comes to the practical\napplicability and integrity as an overall solution to the learning from advice\nchallenge. In this paper, we extend the idea of advice reusing via teacher\nimitation to construct a unified approach that addresses both advice collection\nand advice utilisation problems. We also propose a method to automatically tune\nthe relevant hyperparameters of these components on-the-fly to make it able to\nadapt to any task with minimal human intervention. The experiments we performed\nin 5 different Atari games verify that our algorithm either surpasses or\nperforms on-par with its top competitors while being far simpler to be\nemployed. Furthermore, its individual components are also found to be providing\nsignificant advantages alone.",
          "link": "http://arxiv.org/abs/2104.08440",
          "publishedOn": "2021-07-01T01:59:34.032Z",
          "wordCount": 678,
          "title": "Learning on a Budget via Teacher Imitation. (arXiv:2104.08440v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fompeyrine_D/0/1/0/all/0/1\">D. Fompeyrine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorm_E/0/1/0/all/0/1\">E. S. Vorm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricka_N/0/1/0/all/0/1\">N. Ricka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_F/0/1/0/all/0/1\">F. Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellegrin_G/0/1/0/all/0/1\">G. Pellegrin</a>",
          "description": "Machine Learning (ML) has recently been demonstrated to rival expert-level\nhuman accuracy in prediction and detection tasks in a variety of domains,\nincluding medicine. Despite these impressive findings, however, a key barrier\nto the full realization of ML's potential in medical prognoses is technology\nacceptance. Recent efforts to produce explainable AI (XAI) have made progress\nin improving the interpretability of some ML models, but these efforts suffer\nfrom limitations intrinsic to their design: they work best at identifying why a\nsystem fails, but do poorly at explaining when and why a model's prediction is\ncorrect. We posit that the acceptability of ML predictions in expert domains is\nlimited by two key factors: the machine's horizon of prediction that extends\nbeyond human capability, and the inability for machine predictions to\nincorporate human intuition into their models. We propose the use of a novel ML\narchitecture, Neural Ordinary Differential Equations (NODEs) to enhance human\nunderstanding and encourage acceptability. Our approach prioritizes human\ncognitive intuition at the center of the algorithm design, and offers a\ndistribution of predictions rather than single outputs. We explain how this\napproach may significantly improve human-machine collaboration in prediction\ntasks in expert domains such as medical prognoses. We propose a model and\ndemonstrate, by expanding a concrete example from the literature, how our model\nadvances the vision of future hybrid Human-AI systems.",
          "link": "http://arxiv.org/abs/2102.04121",
          "publishedOn": "2021-07-01T01:59:34.016Z",
          "wordCount": 702,
          "title": "Enhancing Human-Machine Teaming for Medical Prognosis Through Neural Ordinary Differential Equations (NODEs). (arXiv:2102.04121v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nikhil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, along with their unprecedented size and\namounts of training data, many have come to believe that they are not suitable\nfor small sets of data. This trend leads to great concerns, including but not\nlimited to: limited availability of data in certain scientific domains and the\nexclusion of those with limited resource from research in the field. In this\npaper, we dispel the myth that transformers are \"data hungry\" and therefore can\nonly be applied to large sets of data. We show for the first time that with the\nright size and tokenization, transformers can perform head-to-head with\nstate-of-the-art CNNs on small datasets. Our model eliminates the requirement\nfor class token and positional embeddings through a novel sequence pooling\nstrategy and the use of convolutions. We show that compared to CNNs, our\ncompact transformers have fewer parameters and MACs, while obtaining similar\naccuracies. Our method is flexible in terms of model size, and can have as\nlittle as 0.28M parameters and achieve reasonable results. It can reach an\naccuracy of 95.29 % when training from scratch on CIFAR-10, which is comparable\nwith modern CNN based approaches, and a significant improvement over previous\nTransformer based models. Our simple and compact design democratizes\ntransformers by making them accessible to those equipped with basic computing\nresources and/or dealing with important small datasets. Our method works on\nlarger datasets, such as ImageNet (80.28% accuracy with 29% parameters of ViT),\nand NLP tasks as well. Our code and pre-trained models are publicly available\nat https://github.com/SHI-Labs/Compact-Transformers.",
          "link": "http://arxiv.org/abs/2104.05704",
          "publishedOn": "2021-07-01T01:59:34.010Z",
          "wordCount": 748,
          "title": "Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>",
          "description": "Transferability estimation is an essential problem in transfer learning to\npredict how good the performance is when transferring a source model (or source\ntask) to a target task. Recent analytical transferability metrics have been\nwidely used for source model selection and multi-task learning. A major\nchallenge is how to make transfereability estimation robust under the\ncross-domain cross-task settings. The recently proposed OTCE score solves this\nproblem by considering both domain and task differences, with the help of\ntransfer experiences on auxiliary tasks, which causes an efficiency overhead.\nIn this work, we propose a practical transferability metric called JC-NCE score\nthat dramatically improves the robustness of the task difference estimation in\nOTCE, thus removing the need for auxiliary tasks. Specifically, we build the\njoint correspondences between source and target data via solving an optimal\ntransport problem with a ground cost considering both the sample distance and\nlabel distance, and then compute the transferability score as the negative\nconditional entropy of the matched labels. Extensive validations under the\nintra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE\nscore outperforms the auxiliary-task free version of OTCE for 7% and 12%,\nrespectively, and is also more robust than other existing transferability\nmetrics on average.",
          "link": "http://arxiv.org/abs/2106.10479",
          "publishedOn": "2021-07-01T01:59:34.004Z",
          "wordCount": 660,
          "title": "Practical Transferability Estimation for Image Classification Tasks. (arXiv:2106.10479v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kratsios_A/0/1/0/all/0/1\">Anastasis Kratsios</a>",
          "description": "We introduce a general framework for approximating regular conditional\ndistributions (RCDs). Our approximations of these RCDs are implemented by a new\nclass of geometric deep learning models with inputs in $\\mathbb{R}^d$ and\noutputs in the Wasserstein-$1$ space $\\mathcal{P}_1(\\mathbb{R}^D)$. We find\nthat the models built using our framework can approximate any continuous\nfunctions from $\\mathbb{R}^d$ to $\\mathcal{P}_1(\\mathbb{R}^D)$ uniformly on\ncompacts, and quantitative rates are obtained. We identify two methods for\navoiding the \"curse of dimensionality\"; i.e.: the number of parameters\ndetermining the approximating neural network depends only polynomially on the\ninvolved dimension and the approximation error. The first solution describes\nfunctions in $C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ which can be\nefficiently approximated on any compact subset of $\\mathbb{R}^d$. Conversely,\nthe second approach describes sets in $\\mathbb{R}^d$, on which any function in\n$C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ can be efficiently approximated.\nOur framework is used to obtain an affirmative answer to the open conjecture of\nBishop (1994); namely: mixture density networks are universal regular\nconditional distributions. The predictive performance of the proposed models is\nevaluated against comparable learning models on various probabilistic\npredictions tasks in the context of ELMs, model uncertainty, and\nheteroscedastic regression. All the results are obtained for more general input\nand output spaces and thus apply to geometric deep learning contexts.",
          "link": "http://arxiv.org/abs/2105.07743",
          "publishedOn": "2021-07-01T01:59:33.997Z",
          "wordCount": 698,
          "title": "Universal Regular Conditional Distributions. (arXiv:2105.07743v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12199",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jaiswal_P/0/1/0/all/0/1\">Prateek Jaiswal</a>, <a href=\"http://arxiv.org/find/math/1/au:+Honnappa_H/0/1/0/all/0/1\">Harsha Honnappa</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rao_V/0/1/0/all/0/1\">Vinayak A. Rao</a>",
          "description": "This paper considers data-driven chance-constrained stochastic optimization\nproblems in a Bayesian framework. Bayesian posteriors afford a principled\nmechanism to incorporate data and prior knowledge into stochastic optimization\nproblems. However, the computation of Bayesian posteriors is typically an\nintractable problem, and has spawned a large literature on approximate Bayesian\ncomputation. Here, in the context of chance-constrained optimization, we focus\non the question of statistical consistency (in an appropriate sense) of the\noptimal value, computed using an approximate posterior distribution. To this\nend, we rigorously prove a frequentist consistency result demonstrating the\nconvergence of the optimal value to the optimal value of a fixed, parameterized\nconstrained optimization problem. We augment this by also establishing a\nprobabilistic rate of convergence of the optimal value. We also prove the\nconvex feasibility of the approximate Bayesian stochastic optimization problem.\nFinally, we demonstrate the utility of our approach on an optimal staffing\nproblem for an M/M/c queueing model.",
          "link": "http://arxiv.org/abs/2106.12199",
          "publishedOn": "2021-07-01T01:59:33.992Z",
          "wordCount": 614,
          "title": "Bayesian Joint Chance Constrained Optimization: Approximations and Statistical Consistency. (arXiv:2106.12199v2 [math.ST] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11396",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Bilevel optimization recently has attracted increased interest in machine\nlearning due to its many applications such as hyper-parameter optimization and\npolicy optimization. Although some methods recently have been proposed to solve\nthe bilevel problems, these methods do not consider using adaptive learning\nrates. To fill this gap, in the paper, we propose a class of fast and effective\nadaptive methods for solving bilevel optimization problems that the outer\nproblem is possibly nonconvex and the inner problem is strongly-convex.\nSpecifically, we propose a fast single-loop BiAdam algorithm based on the basic\nmomentum technique, which achieves a sample complexity of\n$\\tilde{O}(\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point. At the\nsame time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by\nusing variance reduced technique, which reaches the best known sample\ncomplexity of $\\tilde{O}(\\epsilon^{-3})$. To further reduce computation in\nestimating derivatives, we propose a fast single-loop stochastic approximated\nBiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still\nachieves a sample complexity of $\\tilde{O}(\\epsilon^{-4})$ without large\nbatches. We further present an accelerated version of saBiAdam algorithm\n(VR-saBiAdam), which also reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$. We apply the unified adaptive matrices to our\nmethods as the SUPER-ADAM \\citep{huang2021super}, which including many types of\nadaptive learning rates. Moreover, our framework can flexibly use the momentum\nand variance reduced techniques. In particular, we provide a useful convergence\nanalysis framework for both the constrained and unconstrained bilevel\noptimization. To the best of our knowledge, we first study the adaptive bilevel\noptimization methods with adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.11396",
          "publishedOn": "2021-07-01T01:59:33.986Z",
          "wordCount": 714,
          "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alchihabi_A/0/1/0/all/0/1\">Abdullah Alchihabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhong Guo</a>",
          "description": "Graph Neural Networks (GNNs) require a relatively large number of labeled\nnodes and a reliable/uncorrupted graph connectivity structure in order to\nobtain good performance on the semi-supervised node classification task. The\nperformance of GNNs can degrade significantly as the number of labeled nodes\ndecreases or the graph connectivity structure is corrupted by adversarial\nattacks or due to noises in data measurement /collection. Therefore, it is\nimportant to develop GNN models that are able to achieve good performance when\nthere is limited supervision knowledge -- a few labeled nodes and noisy graph\nstructures. In this paper, we propose a novel Dual GNN learning framework to\naddress this challenge task. The proposed framework has two GNN based node\nprediction modules. The primary module uses the input graph structure to induce\nregular node embeddings and predictions with a regular GNN baseline, while the\nauxiliary module constructs a new graph structure through fine-grained spectral\nclusterings and learns new node embeddings and predictions. By integrating the\ntwo modules in a dual GNN learning framework, we perform joint learning in an\nend-to-end fashion. This general framework can be applied on many GNN baseline\nmodels. The experimental results validate that the proposed dual GNN framework\ncan greatly outperform the GNN baseline methods when the labeled nodes are\nscarce and the graph connectivity structure is noisy.",
          "link": "http://arxiv.org/abs/2106.15755",
          "publishedOn": "2021-07-01T01:59:33.968Z",
          "wordCount": 650,
          "title": "Dual GNNs: Graph Neural Network Learning with Limited Supervision. (arXiv:2106.15755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandran_P/0/1/0/all/0/1\">Pravin Chandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Raghavendra Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_A/0/1/0/all/0/1\">Avinash Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Srikanth Chandar</a>",
          "description": "Federated Learning allows training of data stored in distributed devices\nwithout the need for centralizing training data, thereby maintaining data\nprivacy. Addressing the ability to handle data heterogeneity (non-identical and\nindependent distribution or non-IID) is a key enabler for the wider deployment\nof Federated Learning. In this paper, we propose a novel Divide-and-Conquer\ntraining methodology that enables the use of the popular FedAvg aggregation\nalgorithm by overcoming the acknowledged FedAvg limitations in non-IID\nenvironments. We propose a novel use of Cosine-distance based Weight Divergence\nmetric to determine the exact point where a Deep Learning network can be\ndivided into class agnostic initial layers and class-specific deep layers for\nperforming a Divide and Conquer training. We show that the methodology achieves\ntrained model accuracy at par (and in certain cases exceeding) with numbers\nachieved by state-of-the-art Aggregation algorithms like FedProx, FedMA, etc.\nAlso, we show that this methodology leads to compute and bandwidth\noptimizations under certain documented conditions.",
          "link": "http://arxiv.org/abs/2106.14503",
          "publishedOn": "2021-07-01T01:59:33.961Z",
          "wordCount": 618,
          "title": "Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data. (arXiv:2106.14503v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1\">Kerem Turgutlu</a>",
          "description": "Existing computer vision research in categorization struggles with\nfine-grained attributes recognition due to the inherently high intra-class\nvariances and low inter-class variances. SOTA methods tackle this challenge by\nlocating the most informative image regions and rely on them to classify the\ncomplete image. The most recent work, Vision Transformer (ViT), shows its\nstrong performance in both traditional and fine-grained classification tasks.\nIn this work, we propose a multi-stage ViT framework for fine-grained image\nclassification tasks, which localizes the informative image regions without\nrequiring architectural changes using the inherent multi-head self-attention\nmechanism. We also introduce attention-guided augmentations for improving the\nmodel's capabilities. We demonstrate the value of our approach by experimenting\nwith four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,\nStanford Dogs, and FGVC7 Plant Pathology. We also prove our model's\ninterpretability via qualitative results.",
          "link": "http://arxiv.org/abs/2106.10587",
          "publishedOn": "2021-07-01T01:59:33.955Z",
          "wordCount": 621,
          "title": "Exploring Vision Transformers for Fine-grained Classification. (arXiv:2106.10587v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1\">Tharindu Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaemmaghami_H/0/1/0/all/0/1\">Houman Ghaemmaghami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>",
          "description": "This paper proposes a novel framework for lung sound event detection,\nsegmenting continuous lung sound recordings into discrete events and performing\nrecognition on each event. Exploiting the lightweight nature of Temporal\nConvolution Networks (TCNs) and their superior results compared to their\nrecurrent counterparts, we propose a lightweight, yet robust, and completely\ninterpretable framework for lung sound event detection. We propose the use of a\nmulti-branch TCN architecture and exploit a novel fusion strategy to combine\nthe resultant features from these branches. This not only allows the network to\nretain the most salient information across different temporal granularities and\ndisregards irrelevant information, but also allows our network to process\nrecordings of arbitrary length. Results: The proposed method is evaluated on\nmultiple public and in-house benchmarks of irregular and noisy recordings of\nthe respiratory auscultation process for the identification of numerous\nauscultation events including inhalation, exhalation, crackles, wheeze,\nstridor, and rhonchi. We exceed the state-of-the-art results in all\nevaluations. Furthermore, we empirically analyse the effect of the proposed\nmulti-branch TCN architecture and the feature fusion strategy and provide\nquantitative and qualitative evaluations to illustrate their efficiency.\nMoreover, we provide an end-to-end model interpretation pipeline that\ninterprets the operations of all the components of the proposed framework. Our\nanalysis of different feature fusion strategies shows that the proposed feature\nconcatenation method leads to better suppression of non-informative features,\nwhich drastically reduces the classifier overhead resulting in a robust\nlightweight network.The lightweight nature of our model allows it to be\ndeployed in end-user devices such as smartphones, and it has the ability to\ngenerate predictions in real-time.",
          "link": "http://arxiv.org/abs/2106.15835",
          "publishedOn": "2021-07-01T01:59:33.939Z",
          "wordCount": 719,
          "title": "Robust and Interpretable Temporal Convolution Network for Event Detection in Lung Sound Recordings. (arXiv:2106.15835v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Nan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1\">Kyle Kai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1\">Arian Prabowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Mohammad Saiedur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>",
          "description": "Generative Adversarial Networks (GANs) have shown remarkable success in\nproducing realistic-looking images in the computer vision area. Recently,\nGAN-based techniques are shown to be promising for spatio-temporal-based\napplications such as trajectory prediction, events generation and time-series\ndata imputation. While several reviews for GANs in computer vision have been\npresented, no one has considered addressing the practical applications and\nchallenges relevant to spatio-temporal data. In this paper, we have conducted a\ncomprehensive review of the recent developments of GANs for spatio-temporal\ndata. We summarise the application of popular GAN architectures for\nspatio-temporal data and the common practices for evaluating the performance of\nspatio-temporal applications with GANs. Finally, we point out future research\ndirections to benefit researchers in this area.",
          "link": "http://arxiv.org/abs/2008.08903",
          "publishedOn": "2021-07-01T01:59:33.933Z",
          "wordCount": 625,
          "title": "Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dumas_J/0/1/0/all/0/1\">Jonathan Dumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanaspeze_A/0/1/0/all/0/1\">Antoine Wehenkel Damien Lanaspeze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornelusse_B/0/1/0/all/0/1\">Bertrand Corn&#xe9;lusse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutera_A/0/1/0/all/0/1\">Antonio Sutera</a>",
          "description": "Greater direct electrification of end-use sectors with a higher share of\nrenewables is one of the pillars to power a carbon-neutral society by 2050.\nThis study uses a recent deep learning technique, the normalizing flows, to\nproduce accurate probabilistic forecasts that are crucial for decision-makers\nto face the new challenges in power systems applications. Through comprehensive\nempirical evaluations using the open data of the Global Energy Forecasting\nCompetition 2014, we demonstrate that our methodology is competitive with other\nstate-of-the-art deep learning generative models: generative adversarial\nnetworks and variational autoencoders. The models producing weather-based wind,\nsolar power, and load scenarios are properly compared both in terms of forecast\nvalue, by considering the case study of an energy retailer, and quality using\nseveral complementary metrics.",
          "link": "http://arxiv.org/abs/2106.09370",
          "publishedOn": "2021-07-01T01:59:33.927Z",
          "wordCount": 585,
          "title": "Deep generative modeling for probabilistic forecasting in power systems. (arXiv:2106.09370v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1\">Sivakanth Gopi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1\">Lukas Wutschitz</a>",
          "description": "We give a fast algorithm to optimally compose privacy guarantees of\ndifferentially private (DP) algorithms to arbitrary accuracy. Our method is\nbased on the notion of privacy loss random variables to quantify the privacy\nloss of DP algorithms. The running time and memory needed for our algorithm to\napproximate the privacy curve of a DP algorithm composed with itself $k$ times\nis $\\tilde{O}(\\sqrt{k})$. This improves over the best prior method by Koskela\net al. (2020) which requires $\\tilde{\\Omega}(k^{1.5})$ running time. We\ndemonstrate the utility of our algorithm by accurately computing the privacy\nloss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm\nspeeds up the privacy computations by a few orders of magnitude compared to\nprior work, while maintaining similar accuracy.",
          "link": "http://arxiv.org/abs/2106.02848",
          "publishedOn": "2021-07-01T01:59:33.903Z",
          "wordCount": 578,
          "title": "Numerical Composition of Differential Privacy. (arXiv:2106.02848v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bilos_M/0/1/0/all/0/1\">Marin Bilo&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Modeling sets is an important problem in machine learning since this type of\ndata can be found in many domains. A promising approach defines a family of\npermutation invariant densities with continuous normalizing flows. This allows\nus to maximize the likelihood directly and sample new realizations with ease.\nIn this work, we demonstrate how calculating the trace, a crucial step in this\nmethod, raises issues that occur both during training and inference, limiting\nits practicality. We propose an alternative way of defining permutation\nequivariant transformations that give closed form trace. This leads not only to\nimprovements while training, but also to better final performance. We\ndemonstrate the benefits of our approach on point processes and general set\nmodeling.",
          "link": "http://arxiv.org/abs/2010.03242",
          "publishedOn": "2021-07-01T01:59:33.897Z",
          "wordCount": 581,
          "title": "Scalable Normalizing Flows for Permutation Invariant Densities. (arXiv:2010.03242v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1809.11165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1\">Jacob R. Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1\">Geoff Pleiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bindel_D/0/1/0/all/0/1\">David Bindel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>",
          "description": "Despite advances in scalable models, the inference tools used for Gaussian\nprocesses (GPs) have yet to fully capitalize on developments in computing\nhardware. We present an efficient and general approach to GP inference based on\nBlackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified\nbatched version of the conjugate gradients algorithm to derive all terms for\ntraining and inference in a single call. BBMM reduces the asymptotic complexity\nof exact GP inference from $O(n^3)$ to $O(n^2)$. Adapting this algorithm to\nscalable approximations and complex GP models simply requires a routine for\nefficient matrix-matrix multiplication with the kernel and its derivative. In\naddition, BBMM uses a specialized preconditioner to substantially speed up\nconvergence. In experiments we show that BBMM effectively uses GPU hardware to\ndramatically accelerate both exact GP inference and scalable approximations.\nAdditionally, we provide GPyTorch, a software platform for scalable GP\ninference via BBMM, built on PyTorch.",
          "link": "http://arxiv.org/abs/1809.11165",
          "publishedOn": "2021-07-01T01:59:33.888Z",
          "wordCount": 671,
          "title": "GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration. (arXiv:1809.11165v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10159",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Nguyen_D/0/1/0/all/0/1\">Du Nguyen</a>",
          "description": "We provide an explicit formula for the Levi-Civita connection and Riemannian\nHessian for a Riemannian manifold that is a quotient of a manifold embedded in\nan inner product space with a non-constant metric function. Together with a\nclassical formula for projection, this allows us to evaluate Riemannian\ngradient and Hessian for several families of metrics on classical manifolds,\nincluding a family of metrics on Stiefel manifolds connecting both the constant\nand canonical ambient metrics with closed-form geodesics. Using these formulas,\nwe derive Riemannian optimization frameworks on quotients of Stiefel manifolds,\nincluding flag manifolds, and a new family of complete quotient metrics on the\nmanifold of positive-semidefinite matrices of fixed rank, considered as a\nquotient of a product of Stiefel and positive-definite matrix manifold with\naffine-invariant metrics. The method is procedural, and in many instances, the\nRiemannian gradient and Hessian formulas could be derived by symbolic calculus.\nThe method extends the list of potential metrics that could be used in manifold\noptimization and machine learning.",
          "link": "http://arxiv.org/abs/2009.10159",
          "publishedOn": "2021-07-01T01:59:33.882Z",
          "wordCount": 639,
          "title": "Operator-valued formulas for Riemannian Gradient and Hessian and families of tractable metrics. (arXiv:2009.10159v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goutay_M/0/1/0/all/0/1\">Mathieu Goutay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorce_J/0/1/0/all/0/1\">Jean-Marie Gorce</a>",
          "description": "Machine learning (ML) can be used in various ways to improve multi-user\nmultiple-input multiple-output (MU-MIMO) receive processing. Typical approaches\neither augment a single processing step, such as symbol detection, or replace\nmultiple steps jointly by a single neural network (NN). These techniques\ndemonstrate promising results but often assume perfect channel state\ninformation (CSI) or fail to satisfy the interpretability and scalability\nconstraints imposed by practical systems. In this paper, we propose a new\nstrategy which preserves the benefits of a conventional receiver, but enhances\nspecific parts with ML components. The key idea is to exploit the orthogonal\nfrequency-division multiplexing (OFDM) signal structure to improve both the\ndemapping and the computation of the channel estimation error statistics.\nEvaluation results show that the proposed ML-enhanced receiver beats practical\nbaselines on all considered scenarios, with significant gains at high speeds.",
          "link": "http://arxiv.org/abs/2106.16074",
          "publishedOn": "2021-07-01T01:59:33.856Z",
          "wordCount": 579,
          "title": "Machine Learning-enhanced Receive Processing for MU-MIMO OFDM Systems. (arXiv:2106.16074v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2104.09866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhat_P/0/1/0/all/0/1\">Prashant Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Self-supervised learning solves pretext prediction tasks that do not require\nannotations to learn feature representations. For vision tasks, pretext tasks\nsuch as predicting rotation, solving jigsaw are solely created from the input\ndata. Yet, predicting this known information helps in learning representations\nuseful for downstream tasks. However, recent works have shown that wider and\ndeeper models benefit more from self-supervised learning than smaller models.\nTo address the issue of self-supervised pre-training of smaller models, we\npropose Distill-on-the-Go (DoGo), a self-supervised learning paradigm using\nsingle-stage online knowledge distillation to improve the representation\nquality of the smaller models. We employ deep mutual learning strategy in which\ntwo models collaboratively learn from each other to improve one another.\nSpecifically, each model is trained using self-supervised learning along with\ndistillation that aligns each model's softmax probabilities of similarity\nscores with that of the peer model. We conduct extensive experiments on\nmultiple benchmark datasets, learning objectives, and architectures to\ndemonstrate the potential of our proposed method. Our results show significant\nperformance gain in the presence of noisy and limited labels and generalization\nto out-of-distribution data.",
          "link": "http://arxiv.org/abs/2104.09866",
          "publishedOn": "2021-07-01T01:59:33.843Z",
          "wordCount": 663,
          "title": "Distill on the Go: Online knowledge distillation in self-supervised learning. (arXiv:2104.09866v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.12278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castera_C/0/1/0/all/0/1\">Camille Castera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Bolte</a> (UT1), <a href=\"http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1\">C&#xe9;dric F&#xe9;votte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Edouard Pauwels</a> (UT3)",
          "description": "We introduce a new second-order inertial optimization method for machine\nlearning called INNA. It exploits the geometry of the loss function while only\nrequiring stochastic approximations of the function values and the generalized\ngradients. This makes INNA fully implementable and adapted to large-scale\noptimization problems such as the training of deep neural networks. The\nalgorithm combines both gradient-descent and Newton-like behaviors as well as\ninertia. We prove the convergence of INNA for most deep learning problems. To\ndo so, we provide a well-suited framework to analyze deep learning loss\nfunctions involving tame optimization in which we study a continuous dynamical\nsystem together with its discrete stochastic approximations. We prove sublinear\nconvergence for the continuous-time differential inclusion which underlies our\nalgorithm. Additionally, we also show how standard optimization mini-batch\nmethods applied to non-smooth non-convex problems can yield a certain type of\nspurious stationary points never discussed before. We address this issue by\nproviding a theoretical framework around the new idea of $D$-criticality; we\nthen give a simple asymptotic analysis of INNA. Our algorithm allows for using\nan aggressive learning rate of $o(1/\\log k)$. From an empirical viewpoint, we\nshow that INNA returns competitive results with respect to state of the art\n(stochastic gradient descent, ADAGRAD, ADAM) on popular deep learning benchmark\nproblems.",
          "link": "http://arxiv.org/abs/1905.12278",
          "publishedOn": "2021-07-01T01:59:33.817Z",
          "wordCount": 726,
          "title": "An Inertial Newton Algorithm for Deep Learning. (arXiv:1905.12278v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Jiafei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ya_J/0/1/0/all/0/1\">Jiangpeng Ya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Feng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dijun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>",
          "description": "Multi-goal reinforcement learning is widely applied in planning and robot\nmanipulation. Two main challenges in multi-goal reinforcement learning are\nsparse rewards and sample inefficiency. Hindsight Experience Replay (HER) aims\nto tackle the two challenges via goal relabeling. However, HER-related works\nstill need millions of samples and a huge computation. In this paper, we\npropose Multi-step Hindsight Experience Replay (MHER), incorporating multi-step\nrelabeled returns based on $n$-step relabeling to improve sample efficiency.\nDespite the advantages of $n$-step relabeling, we theoretically and\nexperimentally prove the off-policy $n$-step bias introduced by $n$-step\nrelabeling may lead to poor performance in many environments. To address the\nabove issue, two bias-reduced MHER algorithms, MHER($\\lambda$) and Model-based\nMHER (MMHER) are presented. MHER($\\lambda$) exploits the $\\lambda$ return while\nMMHER benefits from model-based value expansions. Experimental results on\nnumerous multi-goal robotic tasks show that our solutions can successfully\nalleviate off-policy $n$-step bias and achieve significantly higher sample\nefficiency than HER and Curriculum-guided HER with little additional\ncomputation beyond HER.",
          "link": "http://arxiv.org/abs/2102.12962",
          "publishedOn": "2021-07-01T01:59:33.811Z",
          "wordCount": 643,
          "title": "Bias-reduced Multi-step Hindsight Experience Replay for Efficient Multi-goal Reinforcement Learning. (arXiv:2102.12962v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08208",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Adaptive gradient methods have shown excellent performance for solving many\nmachine learning problems. Although multiple adaptive methods were recently\nstudied, they mainly focus on either empirical or theoretical aspects and also\nonly work for specific problems by using specific adaptive learning rates. It\nis desired to design a universal framework for practical algorithms of adaptive\ngradients with theoretical guarantee to solve general problems. To fill this\ngap, we propose a faster and universal framework of adaptive gradients (i.e.,\nSUPER-ADAM) by introducing a universal adaptive matrix that includes most\nexisting adaptive gradient forms. Moreover, our framework can flexibly\nintegrates the momentum and variance reduced techniques. In particular, our\nnovel framework provides the convergence analysis support for adaptive gradient\nmethods under the nonconvex setting. In theoretical analysis, we prove that our\nnew algorithm can achieve the best known complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms.",
          "link": "http://arxiv.org/abs/2106.08208",
          "publishedOn": "2021-07-01T01:59:33.793Z",
          "wordCount": 648,
          "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "Episodic reinforcement learning and contextual bandits are two widely studied\nsequential decision-making problems. Episodic reinforcement learning\ngeneralizes contextual bandits and is often perceived to be more difficult due\nto long planning horizon and unknown state-dependent transitions. The current\npaper shows that the long planning horizon and the unknown state-dependent\ntransitions (at most) pose little additional difficulty on sample complexity.\n\nWe consider the episodic reinforcement learning with $S$ states, $A$ actions,\nplanning horizon $H$, total reward bounded by $1$, and the agent plays for $K$\nepisodes. We propose a new algorithm, \\textbf{M}onotonic \\textbf{V}alue\n\\textbf{P}ropagation (MVP), which relies on a new Bernstein-type bonus.\nCompared to existing bonus constructions, the new bonus is tighter since it is\nbased on a well-designed monotonic value function. In particular, the\n\\emph{constants} in the bonus should be subtly setting to ensure optimism and\nmonotonicity.\n\nWe show MVP enjoys an $O\\left(\\left(\\sqrt{SAK} + S^2A\\right) \\poly\\log\n\\left(SAHK\\right)\\right)$ regret, approaching the\n$\\Omega\\left(\\sqrt{SAK}\\right)$ lower bound of \\emph{contextual bandits} up to\nlogarithmic terms. Notably, this result 1) \\emph{exponentially} improves the\nstate-of-the-art polynomial-time algorithms by Dann et al. [2019] and Zanette\net al. [2019] in terms of the dependency on $H$, and 2) \\emph{exponentially}\nimproves the running time in [Wang et al. 2020] and significantly improves the\ndependency on $S$, $A$ and $K$ in sample complexity.",
          "link": "http://arxiv.org/abs/2009.13503",
          "publishedOn": "2021-07-01T01:59:33.785Z",
          "wordCount": 690,
          "title": "Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon. (arXiv:2009.13503v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoxue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liexin Cheng</a>",
          "description": "Loan risk for small businesses has long been a complex problem worthy of\nexploring. Predicting the loan risk can benefit entrepreneurship by developing\nmore jobs for the society. CatBoost (Categorical Boosting) is a powerful\nmachine learning algorithm suitable for dataset with many categorical variables\nlike the dataset for forecasting loan risk. In this paper, we identify the\nimportant risk factors that contribute to loan status classification problem.\nThen we compare the performance between boosting-type algorithms(especially\nCatBoost) with other traditional yet popular ones. The dataset we adopt in the\nresearch comes from the U.S. Small Business Administration (SBA) and holds a\nvery large sample size (899,164 observations and 27 features). In order to make\nthe best use of the important features in the dataset, we propose a technique\nnamed \"synthetic generation\" to develop more combined features based on\narithmetic operation, which ends up improving the accuracy and AUC of the\noriginal CatBoost model. We obtain a high accuracy of 95.84% and well-performed\nAUC of 98.80% compared with the existent literature of related research.",
          "link": "http://arxiv.org/abs/2106.07954",
          "publishedOn": "2021-07-01T01:59:33.778Z",
          "wordCount": 646,
          "title": "CatBoost model with synthetic features in application to loan risk assessment of small businesses. (arXiv:2106.07954v3 [cs.CE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1\">Zelin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "Dimension reduction (DR) aims to learn low-dimensional representations of\nhigh-dimensional data with the preservation of essential information. In the\ncontext of manifold learning, we define that the representation after\ninformation-lossless DR preserves the topological and geometric properties of\ndata manifolds formally, and propose a novel two-stage DR method, called\ninvertible manifold learning (inv-ML) to bridge the gap between theoretical\ninformation-lossless and practical DR. The first stage includes a homeomorphic\nsparse coordinate transformation to learn low-dimensional representations\nwithout destroying topology and a local isometry constraint to preserve local\ngeometry. In the second stage, a linear compression is implemented for the\ntrade-off between the target dimension and the incurred information loss in\nexcessive DR scenarios. Experiments are conducted on seven datasets with a\nneural network implementation of inv-ML, called i-ML-Enc. Empirically, i-ML-Enc\nachieves invertible DR in comparison with typical existing methods as well as\nreveals the characteristics of the learned manifolds. Through latent space\ninterpolation on real-world datasets, we find that the reliability of tangent\nspace approximated by the local neighborhood is the key to the success of\nmanifold-based DR algorithms.",
          "link": "http://arxiv.org/abs/2010.04012",
          "publishedOn": "2021-07-01T01:59:33.772Z",
          "wordCount": 652,
          "title": "Invertible Manifold Learning for Dimension Reduction. (arXiv:2010.04012v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiashuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Machine learning algorithms with empirical risk minimization are vulnerable\nunder distributional shifts due to the greedy adoption of all the correlations\nfound in training data. There is an emerging literature on tackling this\nproblem by minimizing the worst-case risk over an uncertainty set. However,\nexisting methods mostly construct ambiguity sets by treating all variables\nequally regardless of the stability of their correlations with the target,\nresulting in the overwhelmingly-large uncertainty set and low confidence of the\nlearner. In this paper, we propose a novel Stable Adversarial Learning (SAL)\nalgorithm that leverages heterogeneous data sources to construct a more\npractical uncertainty set and conduct differentiated robustness optimization,\nwhere covariates are differentiated according to the stability of their\ncorrelations with the target. We theoretically show that our method is\ntractable for stochastic gradient-based optimization and provide the\nperformance guarantees for our method. Empirical studies on both simulation and\nreal datasets validate the effectiveness of our method in terms of uniformly\ngood performance across unknown distributional shifts.",
          "link": "http://arxiv.org/abs/2106.15791",
          "publishedOn": "2021-07-01T01:59:33.756Z",
          "wordCount": 604,
          "title": "Distributionally Robust Learning with Stable Adversarial Training. (arXiv:2106.15791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1\">Lucas Caccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1\">Rahaf Aljundi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadi_N/0/1/0/all/0/1\">Nader Asadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1\">Eugene Belilovsky</a>",
          "description": "In the online continual learning paradigm, agents must learn from a changing\ndistribution while respecting memory and compute constraints. Previous work in\nthis setting often tries to reduce catastrophic forgetting by limiting changes\nin the space of model parameters. In this work we instead focus on the change\nin representations of observed data that arises when previously unobserved\nclasses appear in the incoming data stream, and new classes must be\ndistinguished from previous ones. Starting from a popular approach, experience\nreplay, we consider metric learning based loss functions which, when adjusted\nto appropriately select negative samples, can effectively nudge the learned\nrepresentations to be more robust to new future classes. We show that this\nselection of negatives is in fact critical for reducing representation drift of\npreviously observed data. Motivated by this we further introduce a simple\nadjustment to the standard cross entropy loss used in prior experience replay\nthat achieves similar effect. Our approach directly improves the performance of\nexperience replay for this setting, obtaining state-of-the-art results on\nseveral existing benchmarks in online continual learning, while remaining\nefficient in both memory and compute. We release an implementation of our\nexperiments at https://github.com/naderAsadi/AML",
          "link": "http://arxiv.org/abs/2104.05025",
          "publishedOn": "2021-07-01T01:59:33.750Z",
          "wordCount": 657,
          "title": "Reducing Representation Drift in Online Continual Learning. (arXiv:2104.05025v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15933",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jacot_A/0/1/0/all/0/1\">Arthur Jacot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ged_F/0/1/0/all/0/1\">Fran&#xe7;ois Ged</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gabriel_F/0/1/0/all/0/1\">Franck Gabriel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simsek_B/0/1/0/all/0/1\">Berfin &#x15e;im&#x15f;ek</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hongler_C/0/1/0/all/0/1\">Cl&#xe9;ment Hongler</a>",
          "description": "For deep linear networks (DLN), various hyperparameters alter the dynamics of\ntraining dramatically. We investigate how the rank of the linear map found by\ngradient descent is affected by (1) the initialization norm and (2) the\naddition of $L_{2}$ regularization on the parameters. For (1), we study two\nregimes: (1a) the linear/lazy regime, for large norm initialization; (1b) a\n\\textquotedbl saddle-to-saddle\\textquotedbl{} regime for small initialization\nnorm. In the (1a) setting, the dynamics of a DLN of any depth is similar to\nthat of a standard linear model, without any low-rank bias. In the (1b)\nsetting, we conjecture that throughout training, gradient descent approaches a\nsequence of saddles, each corresponding to linear maps of increasing rank,\nuntil reaching a minimal rank global minimum. We support this conjecture with a\npartial proof and some numerical experiments. For (2), we show that adding a\n$L_{2}$ regularization on the parameters corresponds to the addition to the\ncost of a $L_{p}$-Schatten (quasi)norm on the linear map with $p=\\frac{2}{L}$\n(for a depth-$L$ network), leading to a stronger low-rank bias as $L$ grows.\nThe effect of $L_{2}$ regularization on the loss surface depends on the depth:\nfor shallow networks, all critical points are either strict saddles or global\nminima, whereas for deep networks, some local minima appear. We numerically\nobserve that these local minima can generalize better than global ones in some\nsettings.",
          "link": "http://arxiv.org/abs/2106.15933",
          "publishedOn": "2021-07-01T01:59:33.744Z",
          "wordCount": 672,
          "title": "Deep Linear Networks Dynamics: Low-Rank Biases Induced by Initialization Scale and L2 Regularization. (arXiv:2106.15933v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1301.6697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geiger_D/0/1/0/all/0/1\">Dan Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>",
          "description": "We show that the only parameter prior for complete Gaussian DAG models that\nsatisfies global parameter independence, complete model equivalence, and some\nweak regularity assumptions, is the normal-Wishart distribution. Our analysis\nis based on the following new characterization of the Wishart distribution: let\nW be an n x n, n >= 3, positive-definite symmetric matrix of random variables\nand f(W) be a pdf of W. Then, f(W) is a Wishart distribution if and only if\nW_{11}-W_{12}W_{22}^{-1}W_{12}' is independent of {W_{12}, W_{22}} for every\nblock partitioning W_{11}, W_{12}, W_{12}', W_{22} of W. Similar\ncharacterizations of the normal and normal-Wishart distributions are provided\nas well. We also show how to construct a prior for every DAG model over X from\nthe prior of a single regression model.",
          "link": "http://arxiv.org/abs/1301.6697",
          "publishedOn": "2021-07-01T01:59:33.737Z",
          "wordCount": 643,
          "title": "Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions. (arXiv:1301.6697v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zeyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>",
          "description": "Factorization machine (FM) is a prevalent approach to modeling pairwise\n(second-order) feature interactions when dealing with high-dimensional sparse\ndata. However, on the one hand, FM fails to capture higher-order feature\ninteractions suffering from combinatorial expansion, on the other hand, taking\ninto account interaction between every pair of features may introduce noise and\ndegrade prediction accuracy. To solve the problems, we propose a novel approach\nGraph Factorization Machine (GraphFM) by naturally representing features in the\ngraph structure. In particular, a novel mechanism is designed to select the\nbeneficial feature interactions and formulate them as edges between features.\nThen our proposed model which integrates the interaction function of FM into\nthe feature aggregation strategy of Graph Neural Network (GNN), can model\narbitrary-order feature interactions on the graph-structured features by\nstacking layers. Experimental results on several real-world datasets has\ndemonstrated the rationality and effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2105.11866",
          "publishedOn": "2021-07-01T01:59:33.731Z",
          "wordCount": 615,
          "title": "GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1\">Stefan Zernetsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trupp_O/0/1/0/all/0/1\">Oliver Trupp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1\">Viktor Kress</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1\">Konrad Doll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1\">Bernhard Sick</a>",
          "description": "This article presents a novel approach to incorporate visual cues from\nvideo-data from a wide-angle stereo camera system mounted at an urban\nintersection into the forecast of cyclist trajectories. We extract features\nfrom image and optical flow (OF) sequences using 3D convolutional neural\nnetworks (3D-ConvNet) and combine them with features extracted from the\ncyclist's past trajectory to forecast future cyclist positions. By the use of\nadditional information, we are able to improve positional accuracy by about 7.5\n% for our test dataset and by up to 22 % for specific motion types compared to\na method solely based on past trajectories. Furthermore, we compare the use of\nimage sequences to the use of OF sequences as additional information, showing\nthat OF alone leads to significant improvements in positional accuracy. By\ntraining and testing our methods using a real-world dataset recorded at a\nheavily frequented public intersection and evaluating the methods' runtimes, we\ndemonstrate the applicability in real traffic scenarios. Our code and parts of\nour dataset are made publicly available.",
          "link": "http://arxiv.org/abs/2106.15991",
          "publishedOn": "2021-07-01T01:59:33.725Z",
          "wordCount": 613,
          "title": "Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information. (arXiv:2106.15991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16174",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_P/0/1/0/all/0/1\">Pingjun Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Aminu_M/0/1/0/all/0/1\">Muhammad Aminu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hussein_S/0/1/0/all/0/1\">Siba El Hussein</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khoury_J/0/1/0/all/0/1\">Joseph Khoury</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>",
          "description": "The cells and their spatial patterns in the tumor microenvironment (TME) play\na key role in tumor evolution, and yet remains an understudied topic in\ncomputational pathology. This study, to the best of our knowledge, is among the\nfirst to hybrid local and global graph methods to profile orchestration and\ninteraction of cellular components. To address the challenge in hematolymphoid\ncancers where the cell classes in TME are unclear, we first implemented cell\nlevel unsupervised learning and identified two new cell subtypes. Local cell\ngraphs or supercells were built for each image by considering the individual\ncell's geospatial location and classes. Then, we applied supercell level\nclustering and identified two new cell communities. In the end, we built global\ngraphs to abstract spatial interaction patterns and extract features for\ndisease diagnosis. We evaluate the proposed algorithm on H\\&E slides of 60\nhematolymphoid neoplasm patients and further compared it with three cell level\ngraph-based algorithms, including the global cell graph, cluster cell graph,\nand FLocK. The proposed algorithm achieves a mean diagnosis accuracy of 0.703\nwith the repeated 5-fold cross-validation scheme. In conclusion, our algorithm\nshows superior performance over the existing methods and can be potentially\napplied to other cancer types.",
          "link": "http://arxiv.org/abs/2106.16174",
          "publishedOn": "2021-07-01T01:59:33.700Z",
          "wordCount": 661,
          "title": "Hierarchical Phenotyping and Graph Modeling of Spatial Architecture in Lymphoid Neoplasms. (arXiv:2106.16174v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yiping Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Transformer models have achieved great progress on computer vision tasks\nrecently. The rapid development of vision transformers is mainly contributed by\ntheir high representation ability for extracting informative features from\ninput images. However, the mainstream transformer models are designed with deep\narchitectures, and the feature diversity will be continuously reduced as the\ndepth increases, i.e., feature collapse. In this paper, we theoretically\nanalyze the feature collapse phenomenon and study the relationship between\nshortcuts and feature diversity in these transformer models. Then, we present\nan augmented shortcut scheme, which inserts additional paths with learnable\nparameters in parallel on the original shortcuts. To save the computational\ncosts, we further explore an efficient approach that uses the block-circulant\nprojection to implement augmented shortcuts. Extensive experiments conducted on\nbenchmark datasets demonstrate the effectiveness of the proposed method, which\nbrings about 1% accuracy increase of the state-of-the-art visual transformers\nwithout obviously increasing their parameters and FLOPs.",
          "link": "http://arxiv.org/abs/2106.15941",
          "publishedOn": "2021-07-01T01:59:33.694Z",
          "wordCount": 591,
          "title": "Augmented Shortcuts for Vision Transformers. (arXiv:2106.15941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15649",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Abbas_A/0/1/0/all/0/1\">Ammar Abbas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bollepalli_B/0/1/0/all/0/1\">Bajibabu Bollepalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1\">Alexis Moinet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1\">Arnaud Joly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karanasou_P/0/1/0/all/0/1\">Penny Karanasou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makarov_P/0/1/0/all/0/1\">Peter Makarov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slangens_S/0/1/0/all/0/1\">Simon Slangens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karlapati_S/0/1/0/all/0/1\">Sri Karlapati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1\">Thomas Drugman</a>",
          "description": "We propose a novel Multi-Scale Spectrogram (MSS) modelling approach to\nsynthesise speech with an improved coarse and fine-grained prosody. We present\na generic multi-scale spectrogram prediction mechanism where the system first\npredicts coarser scale mel-spectrograms that capture the suprasegmental\ninformation in speech, and later uses these coarser scale mel-spectrograms to\npredict finer scale mel-spectrograms capturing fine-grained prosody.\n\nWe present details for two specific versions of MSS called Word-level MSS and\nSentence-level MSS where the scales in our system are motivated by the\nlinguistic units. The Word-level MSS models word, phoneme, and frame-level\nspectrograms while Sentence-level MSS models sentence-level spectrogram in\naddition.\n\nSubjective evaluations show that Word-level MSS performs statistically\nsignificantly better compared to the baseline on two voices.",
          "link": "http://arxiv.org/abs/2106.15649",
          "publishedOn": "2021-07-01T01:59:33.688Z",
          "wordCount": 580,
          "title": "Multi-Scale Spectrogram Modelling for Neural Text-to-Speech. (arXiv:2106.15649v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_E/0/1/0/all/0/1\">Ermin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_R/0/1/0/all/0/1\">Randall Berry</a>",
          "description": "Federated learning enables machine learning algorithms to be trained over a\nnetwork of multiple decentralized edge devices without requiring the exchange\nof local datasets. Successfully deploying federated learning requires ensuring\nthat agents (e.g., mobile devices) faithfully execute the intended algorithm,\nwhich has been largely overlooked in the literature. In this study, we first\nuse risk bounds to analyze how the key feature of federated learning,\nunbalanced and non-i.i.d. data, affects agents' incentives to voluntarily\nparticipate and obediently follow traditional federated learning algorithms.\n\nTo be more specific, our analysis reveals that agents with less typical data\ndistributions and relatively more samples are more likely to opt out of or\ntamper with federated learning algorithms. To this end, we formulate the first\nfaithful implementation problem of federated learning and design two faithful\nfederated learning mechanisms which satisfy economic properties, scalability,\nand privacy. Further, the time complexity of computing all agents' payments in\nthe number of agents is $\\mathcal{O}(1)$. First, we design a Faithful Federated\nLearning (FFL) mechanism which approximates the Vickrey-Clarke-Groves (VCG)\npayments via an incremental computation. We show that it achieves (probably\napproximate) optimality, faithful implementation, voluntary participation, and\nsome other economic properties (such as budget balance). Second, by\npartitioning agents into several subsets, we present a scalable VCG mechanism\napproximation. We further design a scalable and Differentially Private FFL\n(DP-FFL) mechanism, the first differentially private faithful mechanism, that\nmaintains the economic properties. Our mechanism enables one to make three-way\nperformance tradeoffs among privacy, the iterations needed, and payment\naccuracy loss.",
          "link": "http://arxiv.org/abs/2106.15905",
          "publishedOn": "2021-07-01T01:59:33.670Z",
          "wordCount": 701,
          "title": "Faithful Edge Federated Learning: Scalability and Privacy. (arXiv:2106.15905v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyue Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "We study whether and how can we model a joint distribution $p(x,z)$ using two\nconditional models $p(x|z)$ and $q(z|x)$ that form a cycle. This is motivated\nby the observation that deep generative models, in addition to a likelihood\nmodel $p(x|z)$, often also use an inference model $q(z|x)$ for data\nrepresentation, but they rely on a usually uninformative prior distribution\n$p(z)$ to define a joint distribution, which may render problems like posterior\ncollapse and manifold mismatch. To explore the possibility to model a joint\ndistribution using only $p(x|z)$ and $q(z|x)$, we study their compatibility and\ndeterminacy, corresponding to the existence and uniqueness of a joint\ndistribution whose conditional distributions coincide with them. We develop a\ngeneral theory for novel and operable equivalence criteria for compatibility,\nand sufficient conditions for determinacy. Based on the theory, we propose the\nCyGen framework for cyclic-conditional generative modeling, including methods\nto enforce compatibility and use the determined distribution to fit and\ngenerate data. With the prior constraint removed, CyGen better fits data and\ncaptures more representative features, supported by experiments showing better\ngeneration and downstream classification performance.",
          "link": "http://arxiv.org/abs/2106.15962",
          "publishedOn": "2021-07-01T01:59:33.664Z",
          "wordCount": 620,
          "title": "On the Generative Utility of Cyclic Conditionals. (arXiv:2106.15962v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1\">Marcos Vendramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1\">Alexei Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identify\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.",
          "link": "http://arxiv.org/abs/2105.10013",
          "publishedOn": "2021-07-01T01:59:33.658Z",
          "wordCount": 626,
          "title": "Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07636",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>",
          "description": "We present SR3, an approach to image Super-Resolution via Repeated\nRefinement. SR3 adapts denoising diffusion probabilistic models to conditional\nimage generation and performs super-resolution through a stochastic denoising\nprocess. Inference starts with pure Gaussian noise and iteratively refines the\nnoisy output using a U-Net model trained on denoising at various noise levels.\nSR3 exhibits strong performance on super-resolution tasks at different\nmagnification factors, on faces and natural images. We conduct human evaluation\non a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA\nGAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic\noutputs, while GANs do not exceed a fool rate of 34%. We further show the\neffectiveness of SR3 in cascaded image generation, where generative models are\nchained with super-resolution models, yielding a competitive FID score of 11.3\non ImageNet.",
          "link": "http://arxiv.org/abs/2104.07636",
          "publishedOn": "2021-07-01T01:59:33.652Z",
          "wordCount": 600,
          "title": "Image Super-Resolution via Iterative Refinement. (arXiv:2104.07636v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03248",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Geiger_D/0/1/0/all/0/1\">Dan Geiger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>",
          "description": "We develop simple methods for constructing parameter priors for model choice\namong Directed Acyclic Graphical (DAG) models. In particular, we introduce\nseveral assumptions that permit the construction of parameter priors for a\nlarge number of DAG models from a small set of assessments. We then present a\nmethod for directly computing the marginal likelihood of every DAG model given\na random sample with no missing observations. We apply this methodology to\nGaussian DAG models which consist of a recursive set of linear regression\nmodels. We show that the only parameter prior for complete Gaussian DAG models\nthat satisfies our assumptions is the normal-Wishart distribution. Our analysis\nis based on the following new characterization of the Wishart distribution: let\n$W$ be an $n \\times n$, $n \\ge 3$, positive-definite symmetric matrix of random\nvariables and $f(W)$ be a pdf of $W$. Then, f$(W)$ is a Wishart distribution if\nand only if $W_{11} - W_{12} W_{22}^{-1} W'_{12}$ is independent of\n$\\{W_{12},W_{22}\\}$ for every block partitioning $W_{11},W_{12}, W'_{12},\nW_{22}$ of $W$. Similar characterizations of the normal and normal-Wishart\ndistributions are provided as well.",
          "link": "http://arxiv.org/abs/2105.03248",
          "publishedOn": "2021-07-01T01:59:33.646Z",
          "wordCount": 675,
          "title": "Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions. (arXiv:2105.03248v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tihon_S/0/1/0/all/0/1\">Simon Tihon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javaid_M/0/1/0/all/0/1\">Muhammad Usama Javaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fourure_D/0/1/0/all/0/1\">Damien Fourure</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posocco_N/0/1/0/all/0/1\">Nicolas Posocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peel_T/0/1/0/all/0/1\">Thomas Peel</a>",
          "description": "Missing data is a recurrent and challenging problem, especially when using\nmachine learning algorithms for real-world applications. For this reason,\nmissing data imputation has become an active research area, in which recent\ndeep learning approaches have achieved state-of-the-art results. We propose\nDAEMA (Denoising Autoencoder with Mask Attention), an algorithm based on a\ndenoising autoencoder architecture with an attention mechanism. While most\nimputation algorithms use incomplete inputs as they would use complete data -\nup to basic preprocessing (e.g. mean imputation) - DAEMA leverages a mask-based\nattention mechanism to focus on the observed values of its inputs. We evaluate\nDAEMA both in terms of reconstruction capabilities and downstream prediction\nand show that it achieves superior performance to state-of-the-art algorithms\non several publicly available real-world datasets under various missingness\nsettings.",
          "link": "http://arxiv.org/abs/2106.16057",
          "publishedOn": "2021-07-01T01:59:33.629Z",
          "wordCount": 578,
          "title": "DAEMA: Denoising Autoencoder with Mask Attention. (arXiv:2106.16057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15910",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nagahama_M/0/1/0/all/0/1\">Masatoshi Nagahama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamada_K/0/1/0/all/0/1\">Koki Yamada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanaka_Y/0/1/0/all/0/1\">Yuichi Tanaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1\">Stanley H. Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>",
          "description": "Graph signal processing is a ubiquitous task in many applications such as\nsensor, social, transportation and brain networks, point cloud processing, and\ngraph neural networks. Graph signals are often corrupted through sensing\nprocesses, and need to be restored for the above applications. In this paper,\nwe propose two graph signal restoration methods based on deep algorithm\nunrolling (DAU). First, we present a graph signal denoiser by unrolling\niterations of the alternating direction method of multiplier (ADMM). We then\npropose a general restoration method for linear degradation by unrolling\niterations of Plug-and-Play ADMM (PnP-ADMM). In the second method, the unrolled\nADMM-based denoiser is incorporated as a submodule. Therefore, our restoration\nmethod has a nested DAU structure. Thanks to DAU, parameters in the proposed\ndenoising/restoration methods are trainable in an end-to-end manner. Since the\nproposed restoration methods are based on iterations of a (convex) optimization\nalgorithm, the method is interpretable and keeps the number of parameters small\nbecause we only need to tune graph-independent regularization parameters. We\nsolve two main problems in existing graph signal restoration methods: 1)\nlimited performance of convex optimization algorithms due to fixed parameters\nwhich are often determined manually. 2) large number of parameters of graph\nneural networks that result in difficulty of training. Several experiments for\ngraph signal denoising and interpolation are performed on synthetic and\nreal-world data. The proposed methods show performance improvements to several\nexisting methods in terms of root mean squared error in both tasks.",
          "link": "http://arxiv.org/abs/2106.15910",
          "publishedOn": "2021-07-01T01:59:33.623Z",
          "wordCount": 683,
          "title": "Graph Signal Restoration Using Nested Deep Algorithm Unrolling. (arXiv:2106.15910v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rogoz_A/0/1/0/all/0/1\">Ana-Cristina Rogoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaman_M/0/1/0/all/0/1\">Mihaela Gaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "In this work, we introduce a corpus for satire detection in Romanian news. We\ngathered 55,608 public news articles from multiple real and satirical news\nsources, composing one of the largest corpora for satire detection regardless\nof language and the only one for the Romanian language. We provide an official\nsplit of the text samples, such that training news articles belong to different\nsources than test news articles, thus ensuring that models do not achieve high\nperformance simply due to overfitting. We conduct experiments with two\nstate-of-the-art deep neural models, resulting in a set of strong baselines for\nour novel corpus. Our results show that the machine-level accuracy for satire\ndetection in Romanian is quite low (under 73% on the test set) compared to the\nhuman-level accuracy (87%), leaving enough room for improvement in future\nresearch.",
          "link": "http://arxiv.org/abs/2105.06456",
          "publishedOn": "2021-07-01T01:59:33.617Z",
          "wordCount": 621,
          "title": "SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles. (arXiv:2105.06456v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1801.10502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aschenbach_M/0/1/0/all/0/1\">Martin Aschenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotzing_T/0/1/0/all/0/1\">Timo K&#xf6;tzing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_K/0/1/0/all/0/1\">Karen Seidel</a>",
          "description": "Learning from positive and negative information, so-called \\emph{informants},\nbeing one of the models for human and machine learning introduced by\nE.~M.~Gold, is investigated. Particularly, naturally arising questions about\nthis learning setting, originating in results on learning from solely positive\ninformation, are answered. By a carefully arranged argument learners can be\nassumed to only change their hypothesis in case it is inconsistent with the\ndata (such a learning behavior is called \\emph{conservative}). The deduced main\ntheorem states the relations between the most important delayable learning\nsuccess criteria, being the ones not ruined by a delayed in time hypothesis\noutput. Additionally, our investigations concerning the non-delayable\nrequirement of consistent learning underpin the claim for \\emph{delayability}\nbeing the right structural property to gain a deeper understanding concerning\nthe nature of learning success criteria. Moreover, we obtain an anomalous\n\\emph{hierarchy} when allowing for an increasing finite number of\n\\emph{anomalies} of the hypothesized language by the learner compared with the\nlanguage to be learned. In contrast to the vacillatory hierarchy for learning\nfrom solely positive information, we observe a \\emph{duality} depending on\nwhether infinitely many \\emph{vacillations} between different (almost) correct\nhypotheses are still considered a successful learning behavior.",
          "link": "http://arxiv.org/abs/1801.10502",
          "publishedOn": "2021-07-01T01:59:33.610Z",
          "wordCount": 686,
          "title": "Learning from Informants: Relations between Learning Success Criteria. (arXiv:1801.10502v5 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "Detecting out-of-distribution (OOD) inputs is critical for safely deploying\ndeep learning models in an open-world setting. However, existing OOD detection\nsolutions can be brittle in the open world, facing various types of adversarial\nOOD inputs. While methods leveraging auxiliary OOD data have emerged, our\nanalysis on illuminative examples reveals a key insight that the majority of\nauxiliary OOD examples may not meaningfully improve or even hurt the decision\nboundary of the OOD detector, which is also observed in empirical results on\nreal data. In this paper, we provide a theoretically motivated method,\nAdversarial Training with informative Outlier Mining (ATOM), which improves the\nrobustness of OOD detection. We show that, by mining informative auxiliary OOD\ndata, one can significantly improve OOD detection performance, and somewhat\nsurprisingly, generalize to unseen adversarial attacks. ATOM achieves\nstate-of-the-art performance under a broad family of classic and adversarial\nOOD evaluation tasks. For example, on the CIFAR-10 in-distribution dataset,\nATOM reduces the FPR (at TPR 95%) by up to 57.99% under adversarial OOD inputs,\nsurpassing the previous best baseline by a large margin.",
          "link": "http://arxiv.org/abs/2006.15207",
          "publishedOn": "2021-07-01T01:59:33.587Z",
          "wordCount": 669,
          "title": "ATOM: Robustifying Out-of-distribution Detection Using Outlier Mining. (arXiv:2006.15207v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiaoben_Y/0/1/0/all/0/1\">You Qiaoben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1\">Chengyang Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinning Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>",
          "description": "Recent works demonstrate that deep reinforcement learning (DRL) models are\nvulnerable to adversarial attacks which can decrease the victim's total reward\nby manipulating the observations. Compared with adversarial attacks in\nsupervised learning, it is much more challenging to deceive a DRL model since\nthe adversary has to infer the environmental dynamics. To address this issue,\nwe reformulate the problem of adversarial attacks in function space and\nseparate the previous gradient based attacks into several subspace. Following\nthe analysis of the function space, we design a generic two-stage framework in\nthe subspace where the adversary lures the agent to a target trajectory or a\ndeceptive policy. In the first stage, we train a deceptive policy by hacking\nthe environment, and discover a set of trajectories routing to the lowest\nreward. The adversary then misleads the victim to imitate the deceptive policy\nby perturbing the observations. Our method provides a tighter theoretical upper\nbound for the attacked agent's performance than the existing approaches.\nExtensive experiments demonstrate the superiority of our method and we achieve\nthe state-of-the-art performance on both Atari and MuJoCo environments.",
          "link": "http://arxiv.org/abs/2106.15860",
          "publishedOn": "2021-07-01T01:59:33.581Z",
          "wordCount": 617,
          "title": "Understanding Adversarial Attacks on Observations in Deep Reinforcement Learning. (arXiv:2106.15860v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.07192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Ling Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>",
          "description": "Hash coding has been widely used in approximate nearest neighbor search for\nlarge-scale image retrieval. Given semantic annotations such as class labels\nand pairwise similarities of the training data, hashing methods can learn and\ngenerate effective and compact binary codes. While some newly introduced images\nmay contain undefined semantic labels, which we call unseen images, zeor-shot\nhashing techniques have been studied. However, existing zeor-shot hashing\nmethods focus on the retrieval of single-label images, and cannot handle\nmulti-label images. In this paper, for the first time, a novel transductive\nzero-shot hashing method is proposed for multi-label unseen image retrieval. In\norder to predict the labels of the unseen/target data, a visual-semantic bridge\nis built via instance-concept coherence ranking on the seen/source data. Then,\npairwise similarity loss and focal quantization loss are constructed for\ntraining a hashing model using both the seen/source and unseen/target data.\nExtensive evaluations on three popular multi-label datasets demonstrate that,\nthe proposed hashing method achieves significantly better results than the\ncompeting methods.",
          "link": "http://arxiv.org/abs/1911.07192",
          "publishedOn": "2021-07-01T01:59:33.575Z",
          "wordCount": 648,
          "title": "Transductive Zero-Shot Hashing for Multilabel Image Retrieval. (arXiv:1911.07192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15988",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tsirtsis_S/0/1/0/all/0/1\">Stratis Tsirtsis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+De_A/0/1/0/all/0/1\">Abir De</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lorch_L/0/1/0/all/0/1\">Lars Lorch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1\">Manuel Gomez-Rodriguez</a>",
          "description": "Testing is recommended for all close contacts of confirmed COVID-19 patients.\nHowever, existing group testing methods are oblivious to the circumstances of\ncontagion provided by contact tracing. Here, we build upon a well-known\nsemi-adaptive pool testing method, Dorfman's method with imperfect tests, and\nderive a simple group testing method based on dynamic programming that is\nspecifically designed to use the information provided by contact tracing.\nExperiments using a variety of reproduction numbers and dispersion levels,\nincluding those estimated in the context of the COVID-19 pandemic, show that\nthe pools found using our method result in a significantly lower number of\ntests than those found using standard Dorfman's method, especially when the\nnumber of contacts of an infected individual is small. Moreover, our results\nshow that our method can be more beneficial when the secondary infections are\nhighly overdispersed.",
          "link": "http://arxiv.org/abs/2106.15988",
          "publishedOn": "2021-07-01T01:59:33.568Z",
          "wordCount": 614,
          "title": "Group Testing under Superspreading Dynamics. (arXiv:2106.15988v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10657",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Zaidenberg_D/0/1/0/all/0/1\">Daniela A. Zaidenberg</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Saux_B/0/1/0/all/0/1\">Bertrand Le Saux</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "This concept paper aims to provide a brief outline of quantum computers,\nexplore existing methods of quantum image classification techniques, so\nfocusing on remote sensing applications, and discuss the bottlenecks of\nperforming these algorithms on currently available open source platforms.\nInitial results demonstrate feasibility. Next steps include expanding the size\nof the quantum hidden layer and increasing the variety of output image options.",
          "link": "http://arxiv.org/abs/2101.10657",
          "publishedOn": "2021-07-01T01:59:33.543Z",
          "wordCount": 542,
          "title": "Advantages and Bottlenecks of Quantum Machine Learning for Remote Sensing. (arXiv:2101.10657v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hengyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1\">Adam Lerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Brandon Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">David Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Noam Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "The standard problem setting in Dec-POMDPs is self-play, where the goal is to\nfind a set of policies that play optimally together. Policies learned through\nself-play may adopt arbitrary conventions and implicitly rely on multi-step\nreasoning based on fragile assumptions about other agents' actions and thus\nfail when paired with humans or independently trained agents at test time. To\naddress this, we present off-belief learning (OBL). At each timestep OBL agents\nfollow a policy $\\pi_1$ that is optimized assuming past actions were taken by a\ngiven, fixed policy ($\\pi_0$), but assuming that future actions will be taken\nby $\\pi_1$. When $\\pi_0$ is uniform random, OBL converges to an optimal policy\nthat does not rely on inferences based on other agents' behavior (an optimal\ngrounded policy). OBL can be iterated in a hierarchy, where the optimal policy\nfrom one level becomes the input to the next, thereby introducing multi-level\ncognitive reasoning in a controlled manner. Unlike existing approaches, which\nmay converge to any equilibrium policy, OBL converges to a unique policy,\nmaking it suitable for zero-shot coordination (ZSC). OBL can be scaled to\nhigh-dimensional settings with a fictitious transition mechanism and shows\nstrong performance in both a toy-setting and the benchmark human-AI & ZSC\nproblem Hanabi.",
          "link": "http://arxiv.org/abs/2103.04000",
          "publishedOn": "2021-07-01T01:59:33.530Z",
          "wordCount": 672,
          "title": "Off-Belief Learning. (arXiv:2103.04000v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bradley_T/0/1/0/all/0/1\">Tai-Danae Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlassopoulos_Y/0/1/0/all/0/1\">Yiannis Vlassopoulos</a>",
          "description": "This work originates from the observation that today's state of the art\nstatistical language models are impressive not only for their performance, but\nalso - and quite crucially - because they are built entirely from correlations\nin unstructured text data. The latter observation prompts a fundamental\nquestion that lies at the heart of this paper: What mathematical structure\nexists in unstructured text data? We put forth enriched category theory as a\nnatural answer. We show that sequences of symbols from a finite alphabet, such\nas those found in a corpus of text, form a category enriched over\nprobabilities. We then address a second fundamental question: How can this\ninformation be stored and modeled in a way that preserves the categorical\nstructure? We answer this by constructing a functor from our enriched category\nof text to a particular enriched category of reduced density operators. The\nlatter leverages the Loewner order on positive semidefinite operators, which\ncan further be interpreted as a toy example of entailment.",
          "link": "http://arxiv.org/abs/2007.03834",
          "publishedOn": "2021-07-01T01:59:33.524Z",
          "wordCount": 650,
          "title": "Language Modeling with Reduced Densities. (arXiv:2007.03834v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Model-agnostic meta-learning (MAML) is arguably the most popular\nmeta-learning algorithm nowadays, given its flexibility to incorporate various\nmodel architectures and to be applied to different problems. Nevertheless, its\nperformance on few-shot classification is far behind many recent algorithms\ndedicated to the problem. In this paper, we point out several key facets of how\nto train MAML to excel in few-shot classification. First, we find that a large\nnumber of gradient steps are needed for the inner loop update, which\ncontradicts the common usage of MAML for few-shot classification. Second, we\nfind that MAML is sensitive to the permutation of class assignments in\nmeta-testing: for a few-shot task of $N$ classes, there are exponentially many\nways to assign the learned initialization of the $N$-way classifier to the $N$\nclasses, leading to an unavoidably huge variance. Third, we investigate several\nways for permutation invariance and find that learning a shared classifier\ninitialization for all the classes performs the best. On benchmark datasets\nsuch as MiniImageNet and TieredImageNet, our approach, which we name\nUNICORN-MAML, performs on a par with or even outperforms state-of-the-art\nalgorithms, while keeping the simplicity of MAML without adding any extra\nsub-networks.",
          "link": "http://arxiv.org/abs/2106.16245",
          "publishedOn": "2021-07-01T01:59:33.501Z",
          "wordCount": 632,
          "title": "How to Train Your MAML to Excel in Few-Shot Classification. (arXiv:2106.16245v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vlaar_T/0/1/0/all/0/1\">Tiffany Vlaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1\">Jonathan Frankle</a>",
          "description": "Studying neural network loss landscapes provides insights into the nature of\nthe underlying optimization problems. Unfortunately, loss landscapes are\nnotoriously difficult to visualize in a human-comprehensible fashion. One\ncommon way to address this problem is to plot linear slices of the landscape,\nfor example from the initial state of the network to the final state after\noptimization. On the basis of this analysis, prior work has drawn broader\nconclusions about the difficulty of the optimization problem. In this paper, we\nput inferences of this kind to the test, systematically evaluating how linear\ninterpolation and final performance vary when altering the data, choice of\ninitialization, and other optimizer and architecture design choices. Further,\nwe use linear interpolation to study the role played by individual layers and\nsubstructures of the network. We find that certain layers are more sensitive to\nthe choice of initialization and optimizer hyperparameter settings, and we\nexploit these observations to design custom optimization schemes. However, our\nresults cast doubt on the broader intuition that the presence or absence of\nbarriers when interpolating necessarily relates to the success of optimization.",
          "link": "http://arxiv.org/abs/2106.16004",
          "publishedOn": "2021-07-01T01:59:33.495Z",
          "wordCount": 622,
          "title": "What can linear interpolation of neural network loss landscapes tell us?. (arXiv:2106.16004v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tzu-Mao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>",
          "description": "Neural networks are susceptible to small transformations including 2D\nrotations and shifts, image crops, and even changes in object colors. This is\noften attributed to biases in the training dataset, and the lack of 2D\nshift-invariance due to not respecting the sampling theorem. In this paper, we\nchallenge this hypothesis by training and testing on unbiased datasets, and\nshowing that networks are brittle to both small 3D perspective changes and\nlighting variations which cannot be explained by dataset bias or lack of\nshift-invariance. To find these in-distribution errors, we introduce an\nevolution strategies (ES) based approach, which we call CMA-Search. Despite\ntraining with a large-scale (0.5 million images), unbiased dataset of camera\nand light variations, in over 71% cases CMA-Search can find camera parameters\nin the vicinity of a correctly classified image which lead to in-distribution\nmisclassifications with < 3.6% change in parameters. With lighting changes,\nCMA-Search finds misclassifications in 33% cases with < 11.6% change in\nparameters. Finally, we extend this method to find misclassifications in the\nvicinity of ImageNet images for both ResNet and OpenAI's CLIP model.",
          "link": "http://arxiv.org/abs/2106.16198",
          "publishedOn": "2021-07-01T01:59:33.489Z",
          "wordCount": 630,
          "title": "Small in-distribution changes in 3D perspective and lighting fool both CNNs and Transformers. (arXiv:2106.16198v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.14317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arindam Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varambally_S/0/1/0/all/0/1\">Sumanth Varambally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Amitabha Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1\">Srikanta Bedathur</a>",
          "description": "We present Fast Random projection-based One-Class Classification (FROCC), an\nextremely efficient method for one-class classification. Our method is based on\na simple idea of transforming the training data by projecting it onto a set of\nrandom unit vectors that are chosen uniformly and independently from the unit\nsphere, and bounding the regions based on separation of the data. FROCC can be\nnaturally extended with kernels. We theoretically prove that FROCC generalizes\nwell in the sense that it is stable and has low bias. FROCC achieves up to 3.1\npercent points better ROC, with 1.2--67.8x speedup in training and test times\nover a range of state-of-the-art benchmarks including the SVM and the deep\nlearning based models for the OCC task.",
          "link": "http://arxiv.org/abs/2011.14317",
          "publishedOn": "2021-07-01T01:59:33.483Z",
          "wordCount": 586,
          "title": "FROCC: Fast Random projection-based One-Class Classification. (arXiv:2011.14317v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiahua Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yang Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Gan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>",
          "description": "Online metric learning has been widely exploited for large-scale data\nclassification due to the low computational cost. However, amongst online\npractical scenarios where the features are evolving (e.g., some features are\nvanished and some new features are augmented), most metric learning models\ncannot be successfully applied to these scenarios, although they can tackle the\nevolving instances efficiently. To address the challenge, we develop a new\nonline Evolving Metric Learning (EML) model for incremental and decremental\nfeatures, which can handle the instance and feature evolutions simultaneously\nby incorporating with a smoothed Wasserstein metric distance. Specifically, our\nmodel contains two essential stages: a Transforming stage (T-stage) and a\nInheriting stage (I-stage). For the T-stage, we propose to extract important\ninformation from vanished features while neglecting non-informative knowledge,\nand forward it into survived features by transforming them into a low-rank\ndiscriminative metric space. It further explores the intrinsic low-rank\nstructure of heterogeneous samples to reduce the computation and memory burden\nespecially for highly-dimensional large-scale data. For the I-stage, we inherit\nthe metric performance of survived features from the T-stage and then expand to\ninclude the new augmented features. Moreover, a smoothed Wasserstein distance\nis utilized to characterize the similarity relationships among the\nheterogeneous and complex samples, since the evolving features are not strictly\naligned in the different stages. In addition to tackling the challenges in\none-shot case, we also extend our model into multishot scenario. After deriving\nan efficient optimization strategy for both T-stage and I-stage, extensive\nexperiments on several datasets verify the superior performance of our EML\nmodel.",
          "link": "http://arxiv.org/abs/2006.15334",
          "publishedOn": "2021-07-01T01:59:33.465Z",
          "wordCount": 738,
          "title": "Evolving Metric Learning for Incremental and Decremental Features. (arXiv:2006.15334v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04259",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Even_M/0/1/0/all/0/1\">Mathieu Even</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>",
          "description": "Dimension is an inherent bottleneck to some modern learning tasks, where\noptimization methods suffer from the size of the data. In this paper, we study\nnon-isotropic distributions of data and develop tools that aim at reducing\nthese dimensional costs by a dependency on an effective dimension rather than\nthe ambient one. Based on non-asymptotic estimates of the metric entropy of\nellipsoids -- that prove to generalize to infinite dimensions -- and on a\nchaining argument, our uniform concentration bounds involve an effective\ndimension instead of the global dimension, improving over existing results. We\nshow the importance of taking advantage of non-isotropic properties in learning\nproblems with the following applications: i) we improve state-of-the-art\nresults in statistical preconditioning for communication-efficient distributed\noptimization, ii) we introduce a non-isotropic randomized smoothing for\nnon-smooth optimization. Both applications cover a class of functions that\nencompasses empirical risk minization (ERM) for linear models.",
          "link": "http://arxiv.org/abs/2102.04259",
          "publishedOn": "2021-07-01T01:59:33.418Z",
          "wordCount": 619,
          "title": "Concentration of Non-Isotropic Random Tensors with Applications to Learning and Empirical Risk Minimization. (arXiv:2102.04259v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>",
          "description": "This work is an update of a previous paper on the same topic published a few\nyears ago. With the dramatic progress in generative modeling, a suite of new\nquantitative and qualitative techniques to evaluate models has emerged.\nAlthough some measures such as Inception Score, Frechet Inception Distance,\nPrecision-Recall, and Perceptual Path Length are relatively more popular, GAN\nevaluation is not a settled issue and there is still room for improvement.\nHere, I describe new dimensions that are becoming important in assessing models\n(e.g. bias and fairness) and discuss the connection between GAN evaluation and\ndeepfakes. These are important areas of concern in the machine learning\ncommunity today and progress in GAN evaluation can help mitigate them.",
          "link": "http://arxiv.org/abs/2103.09396",
          "publishedOn": "2021-07-01T01:59:33.411Z",
          "wordCount": 581,
          "title": "Pros and Cons of GAN Evaluation Measures: New Developments. (arXiv:2103.09396v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amich_A/0/1/0/all/0/1\">Abderrahmen Amich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshete_B/0/1/0/all/0/1\">Birhanu Eshete</a>",
          "description": "Machine Learning (ML) models are susceptible to evasion attacks. Evasion\naccuracy is typically assessed using aggregate evasion rate, and it is an open\nquestion whether aggregate evasion rate enables feature-level diagnosis on the\neffect of adversarial perturbations on evasive predictions. In this paper, we\nintroduce a novel framework that harnesses explainable ML methods to guide\nhigh-fidelity assessment of ML evasion attacks. Our framework enables\nexplanation-guided correlation analysis between pre-evasion perturbations and\npost-evasion explanations. Towards systematic assessment of ML evasion attacks,\nwe propose and evaluate a novel suite of model-agnostic metrics for\nsample-level and dataset-level correlation analysis. Using malware and image\nclassifiers, we conduct comprehensive evaluations across diverse model\narchitectures and complementary feature representations. Our explanation-guided\ncorrelation analysis reveals correlation gaps between adversarial samples and\nthe corresponding perturbations performed on them. Using a case study on\nexplanation-guided evasion, we show the broader usage of our methodology for\nassessing robustness of ML models.",
          "link": "http://arxiv.org/abs/2106.15820",
          "publishedOn": "2021-07-01T01:59:33.406Z",
          "wordCount": 602,
          "title": "Explanation-Guided Diagnosis of Machine Learning Evasion Attacks. (arXiv:2106.15820v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Abhay Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sijia Linda Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalerao_O/0/1/0/all/0/1\">Omkar Bhalerao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Horace He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benson_A/0/1/0/all/0/1\">Austin R. Benson</a>",
          "description": "Graphs are a common model for complex relational data such as social networks\nand protein interactions, and such data can evolve over time (e.g., new\nfriendships) and be noisy (e.g., unmeasured interactions). Link prediction aims\nto predict future edges or infer missing edges in the graph, and has diverse\napplications in recommender systems, experimental design, and complex systems.\nEven though link prediction algorithms strongly depend on the set of edges in\nthe graph, existing approaches typically do not modify the graph topology to\nimprove performance. Here, we demonstrate how simply adding a set of edges,\nwhich we call a \\emph{proposal set}, to the graph as a pre-processing step can\nimprove the performance of several link prediction algorithms. The underlying\nidea is that if the edges in the proposal set generally align with the\nstructure of the graph, link prediction algorithms are further guided towards\npredicting the right edges; in other words, adding a proposal set of edges is a\nsignal-boosting pre-processing step. We show how to use existing link\nprediction algorithms to generate effective proposal sets and evaluate this\napproach on various synthetic and empirical datasets. We find that proposal\nsets meaningfully improve the accuracy of link prediction algorithms based on\nboth neighborhood heuristics and graph neural networks. Code is available at\n\\url{https://github.com/CUAI/Edge-Proposal-Sets}.",
          "link": "http://arxiv.org/abs/2106.15810",
          "publishedOn": "2021-07-01T01:59:33.389Z",
          "wordCount": 657,
          "title": "Edge Proposal Sets for Link Prediction. (arXiv:2106.15810v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mirsky_Y/0/1/0/all/0/1\">Yisroel Mirsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotak_J/0/1/0/all/0/1\">Jaidip Kotak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_R/0/1/0/all/0/1\">Ram Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelei_D/0/1/0/all/0/1\">Deng Gelei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wenke Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>",
          "description": "AI has provided us with the ability to automate tasks, extract information\nfrom vast amounts of data, and synthesize media that is nearly\nindistinguishable from the real thing. However, positive tools can also be used\nfor negative purposes. In particular, cyber adversaries can use AI (such as\nmachine learning) to enhance their attacks and expand their campaigns.\n\nAlthough offensive AI has been discussed in the past, there is a need to\nanalyze and understand the threat in the context of organizations. For example,\nhow does an AI-capable adversary impact the cyber kill chain? Does AI benefit\nthe attacker more than the defender? What are the most significant AI threats\nfacing organizations today and what will be their impact on the future?\n\nIn this survey, we explore the threat of offensive AI on organizations.\nFirst, we present the background and discuss how AI changes the adversary's\nmethods, strategies, goals, and overall attack model. Then, through a\nliterature review, we identify 33 offensive AI capabilities which adversaries\ncan use to enhance their attacks. Finally, through a user study spanning\nindustry and academia, we rank the AI threats and provide insights on the\nadversaries.",
          "link": "http://arxiv.org/abs/2106.15764",
          "publishedOn": "2021-07-01T01:59:33.382Z",
          "wordCount": 645,
          "title": "The Threat of Offensive AI to Organizations. (arXiv:2106.15764v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15842",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_W/0/1/0/all/0/1\">Wen Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiqiang Li</a>",
          "description": "Remaining useful life prediction (RUL) is one of the key technologies of\ncondition-based maintenance, which is important to maintain the reliability and\nsafety of industrial equipments. While deep learning has achieved great success\nin RUL prediction, existing methods have difficulties in processing long\nsequences and extracting information from the sensor and time step aspects. In\nthis paper, we propose Dual Aspect Self-attention based on Transformer (DAST),\na novel deep RUL prediction method. DAST consists of two encoders, which work\nin parallel to simultaneously extract features of different sensors and time\nsteps. Solely based on self-attention, the DAST encoders are more effective in\nprocessing long data sequences, and are capable of adaptively learning to focus\non more important parts of input. Moreover, the parallel feature extraction\ndesign avoids mutual influence of information from two aspects. Experimental\nresults on two real turbofan engine datasets show that our method significantly\noutperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.15842",
          "publishedOn": "2021-07-01T01:59:33.370Z",
          "wordCount": 596,
          "title": "Dual Aspect Self-Attention based on Transformer for Remaining Useful Life Prediction. (arXiv:2106.15842v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciliberto_C/0/1/0/all/0/1\">Carlo Ciliberto</a>",
          "description": "Finding a good way to model probability densities is key to probabilistic\ninference. An ideal model should be able to concisely approximate any\nprobability, while being also compatible with two main operations:\nmultiplications of two models (product rule) and marginalization with respect\nto a subset of the random variables (sum rule). In this work, we show that a\nrecently proposed class of positive semi-definite (PSD) models for non-negative\nfunctions is particularly suited to this end. In particular, we characterize\nboth approximation and generalization capabilities of PSD models, showing that\nthey enjoy strong theoretical guarantees. Moreover, we show that we can perform\nefficiently both sum and product rule in closed form via matrix operations,\nenjoying the same versatility of mixture models. Our results open the way to\napplications of PSD models to density estimation, decision theory and\ninference. Preliminary empirical evaluation supports our findings.",
          "link": "http://arxiv.org/abs/2106.16116",
          "publishedOn": "2021-07-01T01:59:33.364Z",
          "wordCount": 581,
          "title": "PSD Representations for Effective Probability Models. (arXiv:2106.16116v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03415",
          "author": "<a href=\"http://arxiv.org/find/nlin/1/au:+Jiahao_T/0/1/0/all/0/1\">Tom Z. Jiahao</a>, <a href=\"http://arxiv.org/find/nlin/1/au:+Hsieh_M/0/1/0/all/0/1\">M. Ani Hsieh</a>, <a href=\"http://arxiv.org/find/nlin/1/au:+Forgoston_E/0/1/0/all/0/1\">Eric Forgoston</a>",
          "description": "Extracting predictive models from nonlinear systems is a central task in\nscientific machine learning. One key problem is the reconciliation between\nmodern data-driven approaches and first principles. Despite rapid advances in\nmachine learning techniques, embedding domain knowledge into data-driven models\nremains a challenge. In this work, we present a universal learning framework\nfor extracting predictive models from nonlinear systems based on observations.\nOur framework can readily incorporate first principle knowledge because it\nnaturally models nonlinear systems as continuous-time systems. This both\nimproves the extracted models' extrapolation power and reduces the amount of\ndata needed for training. In addition, our framework has the advantages of\nrobustness to observational noise and applicability to irregularly sampled\ndata. We demonstrate the effectiveness of our scheme by learning predictive\nmodels for a wide variety of systems including a stiff Van der Pol oscillator,\nthe Lorenz system, and the Kuramoto-Sivashinsky equation. For the Lorenz\nsystem, different types of domain knowledge are incorporated to demonstrate the\nstrength of knowledge embedding in data-driven system identification.",
          "link": "http://arxiv.org/abs/2010.03415",
          "publishedOn": "2021-07-01T01:59:33.358Z",
          "wordCount": 642,
          "title": "Knowledge-Based Learning of Nonlinear Dynamics and Chaos. (arXiv:2010.03415v3 [nlin.CD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Seungwoong Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">Hawoong Jeong</a>",
          "description": "Invariants and conservation laws convey critical information about the\nunderlying dynamics of a system, yet it is generally infeasible to find them\nfrom large-scale data without any prior knowledge or human insight. We propose\nConservNet to achieve this goal, a neural network that spontaneously discovers\na conserved quantity from grouped data where the members of each group share\ninvariants, similar to a general experimental setting where trajectories from\ndifferent trials are observed. As a neural network trained with a novel and\nintuitive loss function called noise-variance loss, ConservNet learns the\nhidden invariants in each group of multi-dimensional observables in a\ndata-driven, end-to-end manner. Our model successfully discovers underlying\ninvariants from the simulated systems having invariants as well as a real-world\ndouble pendulum trajectory. Since the model is robust to various noises and\ndata conditions compared to baseline, our approach is directly applicable to\nexperimental data for discovering hidden conservation laws and further, general\nrelationships between variables.",
          "link": "http://arxiv.org/abs/2102.04008",
          "publishedOn": "2021-07-01T01:59:33.342Z",
          "wordCount": 635,
          "title": "Discovering conservation laws from trajectories via machine learning. (arXiv:2102.04008v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wehenkel_A/0/1/0/all/0/1\">Antoine Wehenkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louppe_G/0/1/0/all/0/1\">Gilles Louppe</a>",
          "description": "Among likelihood-based approaches for deep generative modelling, variational\nautoencoders (VAEs) offer scalable amortized posterior inference and fast\nsampling. However, VAEs are also more and more outperformed by competing models\nsuch as normalizing flows (NFs), deep-energy models, or the new denoising\ndiffusion probabilistic models (DDPMs). In this preliminary work, we improve\nVAEs by demonstrating how DDPMs can be used for modelling the prior\ndistribution of the latent variables. The diffusion prior model improves upon\nGaussian priors of classical VAEs and is competitive with NF-based priors.\nFinally, we hypothesize that hierarchical VAEs could similarly benefit from the\nenhanced capacity of diffusion priors.",
          "link": "http://arxiv.org/abs/2106.15671",
          "publishedOn": "2021-07-01T01:59:33.284Z",
          "wordCount": 520,
          "title": "Diffusion Priors In Variational Autoencoders. (arXiv:2106.15671v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saboo_K/0/1/0/all/0/1\">Krishnakant V. Saboo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_A/0/1/0/all/0/1\">Anirudh Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yurui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worrell_G/0/1/0/all/0/1\">Gregory A. Worrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1\">David T. Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Ravishankar K. Iyer</a>",
          "description": "We model Alzheimer's disease (AD) progression by combining differential\nequations (DEs) and reinforcement learning (RL) with domain knowledge. DEs\nprovide relationships between some, but not all, factors relevant to AD. We\nassume that the missing relationships must satisfy general criteria about the\nworking of the brain, for e.g., maximizing cognition while minimizing the cost\nof supporting cognition. This allows us to extract the missing relationships by\nusing RL to optimize an objective (reward) function that captures the above\ncriteria. We use our model consisting of DEs (as a simulator) and the trained\nRL agent to predict individualized 10-year AD progression using baseline (year\n0) features on synthetic and real data. The model was comparable or better at\npredicting 10-year cognition trajectories than state-of-the-art learning-based\nmodels. Our interpretable model demonstrated, and provided insights into,\n\"recovery/compensatory\" processes that mitigate the effect of AD, even though\nthose processes were not explicitly encoded in the model. Our framework\ncombines DEs with RL for modelling AD progression and has broad applicability\nfor understanding other neurological disorders.",
          "link": "http://arxiv.org/abs/2106.16187",
          "publishedOn": "2021-07-01T01:59:33.202Z",
          "wordCount": 621,
          "title": "Reinforcement Learning based Disease Progression Model for Alzheimer's Disease. (arXiv:2106.16187v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1\">Chris Chafe</a>",
          "description": "This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet \\cite{oord2016wavenet}. This is fully\nprobabilistic, auto-regressive, and causal, i.e. each sample generated depends\nonly on the previously observed samples. Our approach outperforms a widely used\nwavenet architecture by up to 9\\% on a similar dataset for predicting the next\nstep. Using the attention mechanism, we enable the architecture to learn which\naudio samples are important for the prediction of the future sample. We show\nhow causal transformer generative models can be used for raw waveform\nsynthesis. We also show that this performance can be improved by another 2\\% by\nconditioning samples over a wider context. The flexibility of the current model\nto synthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.",
          "link": "http://arxiv.org/abs/2106.16036",
          "publishedOn": "2021-07-01T01:59:33.177Z",
          "wordCount": 620,
          "title": "A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goutay_M/0/1/0/all/0/1\">Mathieu Goutay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorce_J/0/1/0/all/0/1\">Jean-Marie Gorce</a>",
          "description": "Orthogonal frequency-division multiplexing (OFDM) is widely used in modern\nwireless networks thanks to its efficient handling of multipath environment.\nHowever, it suffers from a poor peak-to-average power ratio (PAPR) which\nrequires a large power backoff, degrading the power amplifier (PA) efficiency.\nIn this work, we propose to use a neural network (NN) at the transmitter to\nlearn a high-dimensional modulation scheme allowing to control the PAPR and\nadjacent channel leakage ratio (ACLR). On the receiver side, a NN-based\nreceiver is implemented to carry out demapping of the transmitted bits. The two\nNNs operate on top of OFDM, and are jointly optimized in and end-to-end manner\nusing a training algorithm that enforces constraints on the PAPR and ACLR.\nSimulation results show that the learned waveforms enable higher information\nrates than a tone reservation baseline, while satisfying predefined PAPR and\nACLR targets.",
          "link": "http://arxiv.org/abs/2106.16039",
          "publishedOn": "2021-07-01T01:59:33.148Z",
          "wordCount": 586,
          "title": "End-to-End Learning of OFDM Waveforms with PAPR and ACLR Constraints. (arXiv:2106.16039v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16194",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hojatian_H/0/1/0/all/0/1\">Hamed Hojatian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadal_J/0/1/0/all/0/1\">Jeremy Nadal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frigon_J/0/1/0/all/0/1\">Jean-Francois Frigon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leduc_Primeau_F/0/1/0/all/0/1\">Francois Leduc-Primeau</a>",
          "description": "Cell-free massive MIMO (CF-mMIMO) systems represent a promising approach to\nincrease the spectral efficiency of wireless communication systems. However,\nnear-optimal solutions require a large amount of signaling exchange between\naccess points (APs) and the network controller (NC). In addition, the use of\nhybrid beamforming in each AP reduces the number of power hungry RF chains, but\nimposes a large computational complexity to find near-optimal precoders. In\nthis letter, we propose two unsupervised deep neural networks (DNN)\narchitectures, fully and partially distributed, that can perform coordinated\nhybrid beamforming with zero or limited communication overhead between APs and\nNC, while achieving near-optimal sum-rate with a reduced computational\ncomplexity compared to conventional near-optimal solutions.",
          "link": "http://arxiv.org/abs/2106.16194",
          "publishedOn": "2021-07-01T01:59:33.112Z",
          "wordCount": 568,
          "title": "Limited-Fronthaul Cell-Free Hybrid Beamforming with Distributed Deep Neural Network. (arXiv:2106.16194v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15698",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Consoli_S/0/1/0/all/0/1\">Sergio Consoli</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Pezzoli_L/0/1/0/all/0/1\">Luca Tiozzo Pezzoli</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Tosetti_E/0/1/0/all/0/1\">Elisa Tosetti</a>",
          "description": "We show how emotions extracted from macroeconomic news can be used to explain\nand forecast future behaviour of sovereign bond yield spreads in Italy and\nSpain. We use a big, open-source, database known as Global Database of Events,\nLanguage and Tone to construct emotion indicators of bond market affective\nstates. We find that negative emotions extracted from news improve the\nforecasting power of government yield spread models during distressed periods\neven after controlling for the number of negative words present in the text. In\naddition, stronger negative emotions, such as panic, reveal useful information\nfor predicting changes in spread at the short-term horizon, while milder\nemotions, such as distress, are useful at longer time horizons. Emotions\ngenerated by the Italian political turmoil propagate to the Spanish news\naffecting this neighbourhood market.",
          "link": "http://arxiv.org/abs/2106.15698",
          "publishedOn": "2021-07-01T01:59:33.100Z",
          "wordCount": 606,
          "title": "Emotions in Macroeconomic News and their Impact on the European Bond Market. (arXiv:2106.15698v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2009.07439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dachao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruoyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihua Zhang</a>",
          "description": "Sparse neural networks have received increasing interests due to their small\nsize compared to dense networks. Nevertheless, most existing works on neural\nnetwork theory have focused on dense neural networks, and our understanding of\nsparse networks is very limited. In this paper, we study the loss landscape of\none-hidden-layer sparse networks. We first consider sparse networks with linear\nactivations. We show that sparse linear networks can have spurious strict\nminima, which is in sharp contrast to dense linear networks which do not even\nhave spurious minima. Second, we show that spurious valleys can exist for wide\nsparse non-linear networks. This is different from wide dense networks which do\nnot have spurious valleys under mild assumptions.",
          "link": "http://arxiv.org/abs/2009.07439",
          "publishedOn": "2021-07-01T01:59:33.085Z",
          "wordCount": 586,
          "title": "On the Landscape of One-hidden-layer Sparse Networks and Beyond. (arXiv:2009.07439v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fourure_D/0/1/0/all/0/1\">Damien Fourure</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javaid_M/0/1/0/all/0/1\">Muhammad Usama Javaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posocco_N/0/1/0/all/0/1\">Nicolas Posocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tihon_S/0/1/0/all/0/1\">Simon Tihon</a>",
          "description": "Anomaly detection is a widely explored domain in machine learning. Many\nmodels are proposed in the literature, and compared through different metrics\nmeasured on various datasets. The most popular metrics used to compare\nperformances are F1-score, AUC and AVPR. In this paper, we show that F1-score\nand AVPR are highly sensitive to the contamination rate. One consequence is\nthat it is possible to artificially increase their values by modifying the\ntrain-test split procedure. This leads to misleading comparisons between\nalgorithms in the literature, especially when the evaluation protocol is not\nwell detailed. Moreover, we show that the F1-score and the AVPR cannot be used\nto compare performances on different datasets as they do not reflect the\nintrinsic difficulty of modeling such data. Based on these observations, we\nclaim that F1-score and AVPR should not be used as metrics for anomaly\ndetection. We recommend a generic evaluation procedure for unsupervised anomaly\ndetection, including the use of other metrics such as the AUC, which are more\nrobust to arbitrary choices in the evaluation protocol.",
          "link": "http://arxiv.org/abs/2106.16020",
          "publishedOn": "2021-07-01T01:59:33.079Z",
          "wordCount": 634,
          "title": "Anomaly Detection: How to Artificially Increase your F1-Score with a Biased Evaluation Protocol. (arXiv:2106.16020v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lijia Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiao-Shan Gao</a>",
          "description": "In this paper, we present a robust classification-autoencoder (CAE) which has\nstrong ability to recognize outliers and defend adversaries. The basic idea is\nto change the autoencoder from an unsupervised learning method into a\nclassifier. The CAE is a modified autoencoder, where the encoder is used to\ncompress samples with different labels into disjoint compression spaces and the\ndecoder is used to recover a sample with a given label from the corresponding\ncompression space. The encoder is used as a classifier and the decoder is used\nto decide whether the classification given by the encoder is correct by\ncomparing the input sample with the output. Since adversary samples are seeming\ninevitable for the current DNN framework, we introduce the list classification\nbased on CAE to defend adversaries, which outputs several labels and the\ncorresponding samples recovered by the CAE. The CAE is evaluated using the\nMNIST dataset in great detail. It is shown that the CAE network can recognize\nalmost all outliers and the list classification contains the correct label for\nalmost all adversaries.",
          "link": "http://arxiv.org/abs/2106.15927",
          "publishedOn": "2021-07-01T01:59:33.060Z",
          "wordCount": 604,
          "title": "A Robust Classification-autoencoder to Defend Outliers and Adversaries. (arXiv:2106.15927v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wiens_D/0/1/0/all/0/1\">Daniel Wiens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Even though deep neural networks succeed on many different tasks including\nsemantic segmentation, they lack on robustness against adversarial examples. To\ncounteract this exploit, often adversarial training is used. However, it is\nknown that adversarial training with weak adversarial attacks (e.g. using the\nFast Gradient Method) does not improve the robustness against stronger attacks.\nRecent research shows that it is possible to increase the robustness of such\nsingle-step methods by choosing an appropriate step size during the training.\nFinding such a step size, without increasing the computational effort of\nsingle-step adversarial training, is still an open challenge. In this work we\naddress the computationally particularly demanding task of semantic\nsegmentation and propose a new step size control algorithm that increases the\nrobustness of single-step adversarial training. The proposed algorithm does not\nincrease the computational effort of single-step adversarial training\nconsiderably and also simplifies training, because it is free of\nmeta-parameter. We show that the robustness of our approach can compete with\nmulti-step adversarial training on two popular benchmarks for semantic\nsegmentation.",
          "link": "http://arxiv.org/abs/2106.15998",
          "publishedOn": "2021-07-01T01:59:33.024Z",
          "wordCount": 609,
          "title": "Single-Step Adversarial Training for Semantic Segmentation. (arXiv:2106.15998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhen Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anjin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangquan Zhang</a>",
          "description": "Traditional supervised learning aims to train a classifier in the closed-set\nworld, where training and test samples share the same label space. In this\npaper, we target a more challenging and realistic setting: open-set learning\n(OSL), where there exist test samples from the classes that are unseen during\ntraining. Although researchers have designed many methods from the algorithmic\nperspectives, there are few methods that provide generalization guarantees on\ntheir ability to achieve consistent performance on different training samples\ndrawn from the same distribution. Motivated by the transfer learning and\nprobably approximate correct (PAC) theory, we make a bold attempt to study OSL\nby proving its generalization error-given training samples with size n, the\nestimation error will get close to order O_p(1/\\sqrt{n}). This is the first\nstudy to provide a generalization bound for OSL, which we do by theoretically\ninvestigating the risk of the target classifier on unknown classes. According\nto our theory, a novel algorithm, called auxiliary open-set risk (AOSR) is\nproposed to address the OSL problem. Experiments verify the efficacy of AOSR.\nThe code is available at github.com/Anjin-Liu/Openset_Learning_AOSR.",
          "link": "http://arxiv.org/abs/2106.15792",
          "publishedOn": "2021-07-01T01:59:32.980Z",
          "wordCount": 619,
          "title": "Learning Bounds for Open-Set Learning. (arXiv:2106.15792v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Waqas_A/0/1/0/all/0/1\">Asim Waqas</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Farooq_H/0/1/0/all/0/1\">Hamza Farooq</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal C. Bouaynaya</a> (1), ((1) Rowan University, (2) University of Minnesota)",
          "description": "Motivated by graph theory, artificial neural networks (ANNs) are\ntraditionally structured as layers of neurons (nodes), which learn useful\ninformation by the passage of data through interconnections (edges). In the\nmachine learning realm, graph structures (i.e., neurons and connections) of\nANNs have recently been explored using various graph-theoretic measures linked\nto their predictive performance. On the other hand, in network science\n(NetSci), certain graph measures including entropy and curvature are known to\nprovide insight into the robustness and fragility of real-world networks. In\nthis work, we use these graph measures to explore the robustness of various\nANNs to adversarial attacks. To this end, we (1) explore the design space of\ninter-layer and intra-layers connectivity regimes of ANNs in the graph domain\nand record their predictive performance after training under different types of\nadversarial attacks, (2) use graph representations for both inter-layer and\nintra-layers connectivity regimes to calculate various graph-theoretic\nmeasures, including curvature and entropy, and (3) analyze the relationship\nbetween these graph measures and the adversarial performance of ANNs. We show\nthat curvature and entropy, while operating in the graph domain, can quantify\nthe robustness of ANNs without having to train these ANNs. Our results suggest\nthat the real-world networks, including brain networks, financial networks, and\nsocial networks may provide important clues to the neural architecture search\nfor robust ANNs. We propose a search strategy that efficiently finds robust\nANNs amongst a set of well-performing ANNs without having a need to train all\nof these ANNs.",
          "link": "http://arxiv.org/abs/2106.15850",
          "publishedOn": "2021-07-01T01:59:32.967Z",
          "wordCount": 696,
          "title": "Exploring Robustness of Neural Networks through Graph Measures. (arXiv:2106.15850v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1\">Anders Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_Y/0/1/0/all/0/1\">Yasaman Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>",
          "description": "Although machine learning models typically experience a drop in performance\non out-of-distribution data, accuracies on in- versus out-of-distribution data\nare widely observed to follow a single linear trend when evaluated across a\ntestbed of models. Models that are more accurate on the out-of-distribution\ndata relative to this baseline exhibit \"effective robustness\" and are\nexceedingly rare. Identifying such models, and understanding their properties,\nis key to improving out-of-distribution performance. We conduct a thorough\nempirical investigation of effective robustness during fine-tuning and\nsurprisingly find that models pre-trained on larger datasets exhibit effective\nrobustness during training that vanishes at convergence. We study how\nproperties of the data influence effective robustness, and we show that it\nincreases with the larger size, more diversity, and higher example difficulty\nof the dataset. We also find that models that display effective robustness are\nable to correctly classify 10% of the examples that no other current testbed\nmodel gets correct. Finally, we discuss several strategies for scaling\neffective robustness to the high-accuracy regime to improve the\nout-of-distribution accuracy of state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.15831",
          "publishedOn": "2021-07-01T01:59:32.949Z",
          "wordCount": 617,
          "title": "The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning. (arXiv:2106.15831v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16101",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "In the paper, we propose a class of faster adaptive gradient descent ascent\nmethods for solving the nonconvex-strongly-concave minimax problems by using\nunified adaptive matrices used in the SUPER-ADAM \\citep{huang2021super}.\nSpecifically, we propose a fast adaptive gradient decent ascent (AdaGDA) method\nbased on the basic momentum technique, which reaches a low sample complexity of\n$O(\\kappa^4\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point without\nlarge batches, which improves the existing result of adaptive minimax\noptimization method by a factor of $O(\\sqrt{\\kappa})$. Moreover, we present an\naccelerated version of AdaGDA (VR-AdaGDA) method based on the momentum-based\nvariance reduced technique, which achieves the best known sample complexity of\n$O(\\kappa^3\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point without\nlarge batches. Further assume the bounded Lipschitz parameter of objective\nfunction, we prove that our VR-AdaGDA method reaches a lower sample complexity\nof $O(\\kappa^{2.5}\\epsilon^{-3})$ with the mini-batch size $O(\\kappa)$. In\nparticular, we provide an effective convergence analysis framework for our\nadaptive methods based on unified adaptive matrices, which include almost\nexisting adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.16101",
          "publishedOn": "2021-07-01T01:59:32.935Z",
          "wordCount": 610,
          "title": "AdaGDA: Faster Adaptive Gradient Descent Ascent Methods for Minimax Optimization. (arXiv:2106.16101v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zaiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1\">Siva Theja Maguluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>",
          "description": "Stochastic Approximation (SA) is a popular approach for solving fixed-point\nequations where the information is corrupted by noise. In this paper, we\nconsider an SA involving a contraction mapping with respect to an arbitrary\nnorm, and show its finite-sample error bounds while using different stepsizes.\nThe idea is to construct a smooth Lyapunov function using the generalized\nMoreau envelope, and show that the iterates of SA have negative drift with\nrespect to that Lyapunov function. Our result is applicable in Reinforcement\nLearning (RL). In particular, we use it to establish the first-known\nconvergence rate of the V-trace algorithm for off-policy TD-learning. Moreover,\nwe also use it to study TD-learning in the on-policy setting, and recover the\nexisting state-of-the-art results for $Q$-learning. Importantly, our\nconstruction results in only a logarithmic dependence of the convergence bound\non the size of the state-space.",
          "link": "http://arxiv.org/abs/2002.00874",
          "publishedOn": "2021-07-01T01:59:32.917Z",
          "wordCount": 649,
          "title": "Finite-Sample Analysis of Stochastic Approximation Using Smooth Convex Envelopes. (arXiv:2002.00874v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruize Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiwen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">James Cheng</a>",
          "description": "Instances-reweighted adversarial training (IRAT) can significantly boost the\nrobustness of trained models, where data being less/more vulnerable to the\ngiven attack are assigned smaller/larger weights during training. However, when\ntested on attacks different from the given attack simulated in training, the\nrobustness may drop significantly (e.g., even worse than no reweighting). In\nthis paper, we study this problem and propose our solution--locally reweighted\nadversarial training (LRAT). The rationale behind IRAT is that we do not need\nto pay much attention to an instance that is already safe under the attack. We\nargue that the safeness should be attack-dependent, so that for the same\ninstance, its weight can change given different attacks based on the same\nmodel. Thus, if the attack simulated in training is mis-specified, the weights\nof IRAT are misleading. To this end, LRAT pairs each instance with its\nadversarial variants and performs local reweighting inside each pair, while\nperforming no global reweighting--the rationale is to fit the instance itself\nif it is immune to the attack, but not to skip the pair, in order to passively\ndefend different attacks in future. Experiments show that LRAT works better\nthan both IRAT (i.e., global reweighting) and the standard AT (i.e., no\nreweighting) when trained with an attack and tested on different attacks.",
          "link": "http://arxiv.org/abs/2106.15776",
          "publishedOn": "2021-07-01T01:59:32.895Z",
          "wordCount": 640,
          "title": "Local Reweighting for Adversarial Training. (arXiv:2106.15776v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lobacheva_E/0/1/0/all/0/1\">Ekaterina Lobacheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodryan_M/0/1/0/all/0/1\">Maxim Kodryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1\">Andrey Malinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Despite the conventional wisdom that using batch normalization with weight\ndecay may improve neural network training, some recent works show their joint\nusage may cause instabilities at the late stages of training. Other works, in\ncontrast, show convergence to the equilibrium, i.e., the stabilization of\ntraining metrics. In this paper, we study this contradiction and show that\ninstead of converging to a stable equilibrium, the training dynamics converge\nto consistent periodic behavior. That is, the training process regularly\nexhibits instabilities which, however, do not lead to complete training\nfailure, but cause a new period of training. We rigorously investigate the\nmechanism underlying this discovered periodic behavior both from an empirical\nand theoretical point of view and show that this periodic behavior is indeed\ncaused by the interaction between batch normalization and weight decay.",
          "link": "http://arxiv.org/abs/2106.15739",
          "publishedOn": "2021-07-01T01:59:32.889Z",
          "wordCount": 589,
          "title": "On the Periodic Behavior of Neural Network Training with Batch Normalization and Weight Decay. (arXiv:2106.15739v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yingbin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erkun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanhua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiatong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yinian Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>",
          "description": "The memorization effect of deep neural network (DNN) plays a pivotal role in\nmany state-of-the-art label-noise learning methods. To exploit this property,\nthe early stopping trick, which stops the optimization at the early stage of\ntraining, is usually adopted. Current methods generally decide the early\nstopping point by considering a DNN as a whole. However, a DNN can be\nconsidered as a composition of a series of layers, and we find that the latter\nlayers in a DNN are much more sensitive to label noise, while their former\ncounterparts are quite robust. Therefore, selecting a stopping point for the\nwhole network may make different DNN layers antagonistically affected each\nother, thus degrading the final performance. In this paper, we propose to\nseparate a DNN into different parts and progressively train them to address\nthis problem. Instead of the early stopping, which trains a whole DNN all at\nonce, we initially train former DNN layers by optimizing the DNN with a\nrelatively large number of epochs. During training, we progressively train the\nlatter DNN layers by using a smaller number of epochs with the preceding layers\nfixed to counteract the impact of noisy labels. We term the proposed method as\nprogressive early stopping (PES). Despite its simplicity, compared with the\nearly stopping, PES can help to obtain more promising and stable results.\nFurthermore, by combining PES with existing approaches on noisy label training,\nwe achieve state-of-the-art performance on image classification benchmarks.",
          "link": "http://arxiv.org/abs/2106.15853",
          "publishedOn": "2021-07-01T01:59:32.871Z",
          "wordCount": 686,
          "title": "Understanding and Improving Early Stopping for Learning with Noisy Labels. (arXiv:2106.15853v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>",
          "description": "To reduce annotation labor associated with object detection, an increasing\nnumber of studies focus on transferring the learned knowledge from a labeled\nsource domain to another unlabeled target domain. However, existing methods\nassume that the labeled data are sampled from a single source domain, which\nignores a more generalized scenario, where labeled data are from multiple\nsource domains. For the more challenging task, we propose a unified Faster\nR-CNN based framework, termed Divide-and-Merge Spindle Network (DMSN), which\ncan simultaneously enhance domain invariance and preserve discriminative power.\nSpecifically, the framework contains multiple source subnets and a pseudo\ntarget subnet. First, we propose a hierarchical feature alignment strategy to\nconduct strong and weak alignments for low- and high-level features,\nrespectively, considering their different effects for object detection. Second,\nwe develop a novel pseudo subnet learning algorithm to approximate optimal\nparameters of pseudo target subset by weighted combination of parameters in\ndifferent source subnets. Finally, a consistency regularization for region\nproposal network is proposed to facilitate each subnet to learn more abstract\ninvariances. Extensive experiments on different adaptation scenarios\ndemonstrate the effectiveness of the proposed model.",
          "link": "http://arxiv.org/abs/2106.15793",
          "publishedOn": "2021-07-01T01:59:32.864Z",
          "wordCount": 623,
          "title": "Multi-Source Domain Adaptation for Object Detection. (arXiv:2106.15793v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>",
          "description": "Procedural fairness has been a public concern, which leads to controversy\nwhen making decisions with respect to protected classes, such as race, social\nstatus, and disability. Some protected classes can be inferred according to\nsome safe proxies like surname and geolocation for the race. Hence, implicitly\nutilizing the predicted protected classes based on the related proxies when\nmaking decisions is an efficient approach to circumvent this issue and seek\njust decisions. In this article, we propose a hierarchical random forest model\nfor prediction without explicitly involving protected classes. Simulation\nexperiments are conducted to show the performance of the hierarchical random\nforest model. An example is analyzed from Boston police interview records to\nillustrate the usefulness of the proposed model.",
          "link": "http://arxiv.org/abs/2106.15767",
          "publishedOn": "2021-07-01T01:59:32.805Z",
          "wordCount": 551,
          "title": "Unaware Fairness: Hierarchical Random Forest for Protected Classes. (arXiv:2106.15767v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>",
          "description": "In light of the COVID-19 pandemic, it is an open challenge and critical\npractical problem to find a optimal way to dynamically prescribe the best\npolicies that balance both the governmental resources and epidemic control in\ndifferent countries and regions. To solve this multi-dimensional tradeoff of\nexploitation and exploration, we formulate this technical challenge as a\ncontextual combinatorial bandit problem that jointly optimizes a multi-criteria\nreward function. Given the historical daily cases in a region and the past\nintervention plans in place, the agent should generate useful intervention\nplans that policy makers can implement in real time to minimizing both the\nnumber of daily COVID-19 cases and the stringency of the recommended\ninterventions. We prove this concept with simulations of multiple realistic\npolicy making scenarios.",
          "link": "http://arxiv.org/abs/2106.15808",
          "publishedOn": "2021-07-01T01:59:32.799Z",
          "wordCount": 620,
          "title": "Optimal Epidemic Control as a Contextual Combinatorial Bandit with Budget. (arXiv:2106.15808v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaehyeong Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jinheon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seul Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongki Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minki Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Graph neural networks have recently achieved remarkable success in\nrepresenting graph-structured data, with rapid progress in both the node\nembedding and graph pooling methods. Yet, they mostly focus on capturing\ninformation from the nodes considering their connectivity, and not much work\nhas been done in representing the edges, which are essential components of a\ngraph. However, for tasks such as graph reconstruction and generation, as well\nas graph classification tasks for which the edges are important for\ndiscrimination, accurately representing edges of a given graph is crucial to\nthe success of the graph representation learning. To this end, we propose a\nnovel edge representation learning framework based on Dual Hypergraph\nTransformation (DHT), which transforms the edges of a graph into the nodes of a\nhypergraph. This dual hypergraph construction allows us to apply message\npassing techniques for node representations to edges. After obtaining edge\nrepresentations from the hypergraphs, we then cluster or drop edges to obtain\nholistic graph-level edge representations. We validate our edge representation\nlearning method with hypergraphs on diverse graph datasets for graph\nrepresentation and generation performance, on which our method largely\noutperforms existing graph representation learning methods. Moreover, our edge\nrepresentation learning and pooling method also largely outperforms\nstate-of-the-art graph pooling methods on graph classification, not only\nbecause of its accurate edge representation learning, but also due to its\nlossless compression of the nodes and removal of irrelevant edges for effective\nmessage passing.",
          "link": "http://arxiv.org/abs/2106.15845",
          "publishedOn": "2021-07-01T01:59:32.775Z",
          "wordCount": 665,
          "title": "Edge Representation Learning with Hypergraphs. (arXiv:2106.15845v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1\">Seyyedali Hosseinalipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorlatova_M/0/1/0/all/0/1\">Maria Gorlatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1\">Christopher G. Brinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1\">Mung Chiang</a>",
          "description": "We consider distributed machine learning (ML) through unmanned aerial\nvehicles (UAVs) for geo-distributed device clusters. We propose five new\ntechnologies/techniques: (i) stratified UAV swarms with leader, worker, and\ncoordinator UAVs, (ii) hierarchical nested personalized federated learning\n(HN-PFL): a holistic distributed ML framework for personalized model training\nacross the worker-leader-core network hierarchy, (iii) cooperative UAV resource\npooling for distributed ML using the UAVs' local computational capabilities,\n(iv) aerial data caching and relaying for efficient data relaying to conduct\nML, and (v) concept/model drift, capturing online data variations at the\ndevices. We split the UAV-enabled model training problem as two parts. (a)\nNetwork-aware HN-PFL, where we optimize a tradeoff between energy consumption\nand ML model performance by configuring data offloading among devices-UAVs and\nUAV-UAVs, UAVs' CPU frequencies, and mini-batch sizes subject to\ncommunication/computation network heterogeneity. We tackle this optimization\nproblem via the method of posynomial condensation and propose a distributed\nalgorithm with a performance guarantee. (b) Macro-trajectory and learning\nduration design, which we formulate as a sequential decision making problem,\ntackled via deep reinforcement learning. Our simulations demonstrate the\nsuperiority of our methodology with regards to the distributed ML performance,\nthe optimization of network resources, and the swarm trajectory efficiency.",
          "link": "http://arxiv.org/abs/2106.15734",
          "publishedOn": "2021-07-01T01:59:32.744Z",
          "wordCount": 656,
          "title": "UAV-assisted Online Machine Learning over Multi-Tiered Networks: A Hierarchical Nested Personalized Federated Learning Approach. (arXiv:2106.15734v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guohua Wu</a>",
          "description": "Graph neural networks (GNNs) have achieved great success in many graph-based\ntasks. Much work is dedicated to empowering GNNs with the adaptive locality\nability, which enables measuring the importance of neighboring nodes to the\ntarget node by a node-specific mechanism. However, the current node-specific\nmechanisms are deficient in distinguishing the importance of nodes in the\ntopology structure. We believe that the structural importance of neighboring\nnodes is closely related to their importance in aggregation. In this paper, we\nintroduce discrete graph curvature (the Ricci curvature) to quantify the\nstrength of structural connection of pairwise nodes. And we propose Curvature\nGraph Neural Network (CGNN), which effectively improves the adaptive locality\nability of GNNs by leveraging the structural property of graph curvature. To\nimprove the adaptability of curvature to various datasets, we explicitly\ntransform curvature into the weights of neighboring nodes by the necessary\nNegative Curvature Processing Module and Curvature Normalization Module. Then,\nwe conduct numerous experiments on various synthetic datasets and real-world\ndatasets. The experimental results on synthetic datasets show that CGNN\neffectively exploits the topology structure information, and the performance is\nimproved significantly. CGNN outperforms the baselines on 5 dense node\nclassification benchmark datasets. This study deepens the understanding of how\nto utilize advanced topology information and assign the importance of\nneighboring nodes from the perspective of graph curvature and encourages us to\nbridge the gap between graph theory and neural networks.",
          "link": "http://arxiv.org/abs/2106.15762",
          "publishedOn": "2021-07-01T01:59:32.711Z",
          "wordCount": 668,
          "title": "Curvature Graph Neural Network. (arXiv:2106.15762v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15666",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Miller_J/0/1/0/all/0/1\">Jacob Miller</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roeder_G/0/1/0/all/0/1\">Geoffrey Roeder</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bradley_T/0/1/0/all/0/1\">Tai-Danae Bradley</a>",
          "description": "We investigate a correspondence between two formalisms for discrete\nprobabilistic modeling: probabilistic graphical models (PGMs) and tensor\nnetworks (TNs), a powerful modeling framework for simulating complex quantum\nsystems. The graphical calculus of PGMs and TNs exhibits many similarities,\nwith discrete undirected graphical models (UGMs) being a special case of TNs.\nHowever, more general probabilistic TN models such as Born machines (BMs)\nemploy complex-valued hidden states to produce novel forms of correlation among\nthe probabilities. While representing a new modeling resource for capturing\nstructure in discrete probability distributions, this behavior also renders the\ndirect application of standard PGM tools impossible. We aim to bridge this gap\nby introducing a hybrid PGM-TN formalism that integrates quantum-like\ncorrelations into PGM models in a principled manner, using the\nphysically-motivated concept of decoherence. We first prove that applying\ndecoherence to the entirety of a BM model converts it into a discrete UGM, and\nconversely, that any subgraph of a discrete UGM can be represented as a\ndecohered BM. This method allows a broad family of probabilistic TN models to\nbe encoded as partially decohered BMs, a fact we leverage to combine the\nrepresentational strengths of both model families. We experimentally verify the\nperformance of such hybrid models in a sequential modeling task, and identify\npromising uses of our method within the context of existing applications of\ngraphical models.",
          "link": "http://arxiv.org/abs/2106.15666",
          "publishedOn": "2021-07-01T01:59:32.688Z",
          "wordCount": 668,
          "title": "Probabilistic Graphical Models and Tensor Networks: A Hybrid Framework. (arXiv:2106.15666v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1\">Cory Braker Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mjolsness_E/0/1/0/all/0/1\">Eric Mjolsness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyen_D/0/1/0/all/0/1\">Diane Oyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodera_C/0/1/0/all/0/1\">Chie Kodera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchez_D/0/1/0/all/0/1\">David Bouchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uyttewaal_M/0/1/0/all/0/1\">Magalie Uyttewaal</a>",
          "description": "We present a method for learning \"spectrally descriptive\" edge weights for\ngraphs. We generalize a previously known distance measure on graphs (Graph\nDiffusion Distance), thereby allowing it to be tuned to minimize an arbitrary\nloss function. Because all steps involved in calculating this modified GDD are\ndifferentiable, we demonstrate that it is possible for a small neural network\nmodel to learn edge weights which minimize loss. GDD alone does not effectively\ndiscriminate between graphs constructed from shoot apical meristem images of\nwild-type vs. mutant \\emph{Arabidopsis thaliana} specimens. However, training\nedge weights and kernel parameters with contrastive loss produces a learned\ndistance metric with large margins between these graph categories. We\ndemonstrate this by showing improved performance of a simple\nk-nearest-neighbors classifier on the learned distance matrix. We also\ndemonstrate a further application of this method to biological image analysis:\nonce trained, we use our model to compute the distance between the biological\ngraphs and a set of graphs output by a cell division simulator. This allows us\nto identify simulation parameter regimes which are similar to each class of\ngraph in our original dataset.",
          "link": "http://arxiv.org/abs/2106.15716",
          "publishedOn": "2021-07-01T01:59:32.553Z",
          "wordCount": 638,
          "title": "Diff2Dist: Learning Spectrally Distinct Edge Functions, with Applications to Cell Morphology Analysis. (arXiv:2106.15716v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1\">Mingda Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1\">Gregory Valiant</a>",
          "description": "We study the selective learning problem introduced by Qiao and Valiant\n(2019), in which the learner observes $n$ labeled data points one at a time. At\na time of its choosing, the learner selects a window length $w$ and a model\n$\\hat\\ell$ from the model class $\\mathcal{L}$, and then labels the next $w$\ndata points using $\\hat\\ell$. The excess risk incurred by the learner is\ndefined as the difference between the average loss of $\\hat\\ell$ over those $w$\ndata points and the smallest possible average loss among all models in\n$\\mathcal{L}$ over those $w$ data points.\n\nWe give an improved algorithm, termed the hybrid exponential weights\nalgorithm, that achieves an expected excess risk of $O((\\log\\log|\\mathcal{L}| +\n\\log\\log n)/\\log n)$. This result gives a doubly exponential improvement in the\ndependence on $|\\mathcal{L}|$ over the best known bound of\n$O(\\sqrt{|\\mathcal{L}|/\\log n})$. We complement the positive result with an\nalmost matching lower bound, which suggests the worst-case optimality of the\nalgorithm.\n\nWe also study a more restrictive family of learning algorithms that are\nbounded-recall in the sense that when a prediction window of length $w$ is\nchosen, the learner's decision only depends on the most recent $w$ data points.\nWe analyze an exponential weights variant of the ERM algorithm in Qiao and\nValiant (2019). This new algorithm achieves an expected excess risk of\n$O(\\sqrt{\\log |\\mathcal{L}|/\\log n})$, which is shown to be nearly optimal\namong all bounded-recall learners. Our analysis builds on a generalized version\nof the selective mean prediction problem in Drucker (2013); Qiao and Valiant\n(2019), which may be of independent interest.",
          "link": "http://arxiv.org/abs/2106.15662",
          "publishedOn": "2021-07-01T01:59:32.547Z",
          "wordCount": 699,
          "title": "Exponential Weights Algorithms for Selective Learning. (arXiv:2106.15662v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avci_B/0/1/0/all/0/1\">Besim Avci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "When a deep learning model is deployed in the wild, it can encounter test\ndata drawn from distributions different from the training data distribution and\nsuffer drop in performance. For safe deployment, it is essential to estimate\nthe accuracy of the pre-trained model on the test data. However, the labels for\nthe test inputs are usually not immediately available in practice, and\nobtaining them can be expensive. This observation leads to two challenging\ntasks: (1) unsupervised accuracy estimation, which aims to estimate the\naccuracy of a pre-trained classifier on a set of unlabeled test inputs; (2)\nerror detection, which aims to identify mis-classified test inputs. In this\npaper, we propose a principled and practically effective framework that\nsimultaneously addresses the two tasks. The proposed framework iteratively\nlearns an ensemble of models to identify mis-classified data points and\nperforms self-training to improve the ensemble with the identified points.\nTheoretical analysis demonstrates that our framework enjoys provable guarantees\nfor both accuracy estimation and error detection under mild conditions readily\nsatisfied by practical deep learning models. Along with the framework, we\nproposed and experimented with two instantiations and achieved state-of-the-art\nresults on 59 tasks. For example, on iWildCam, one instantiation reduces the\nestimation error for unsupervised accuracy estimation by at least 70% and\nimproves the F1 score for error detection by at least 4.7% compared to existing\nmethods.",
          "link": "http://arxiv.org/abs/2106.15728",
          "publishedOn": "2021-07-01T01:59:32.505Z",
          "wordCount": 665,
          "title": "Detecting Errors and Estimating Accuracy on Unlabeled Data with Self-training Ensembles. (arXiv:2106.15728v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Annie Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Back_T/0/1/0/all/0/1\">Thomas B&#xe4;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kononova_A/0/1/0/all/0/1\">Anna V. Kononova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1\">Aske Plaat</a>",
          "description": "This paper surveys the field of multiagent deep reinforcement learning. The\ncombination of deep neural networks with reinforcement learning has gained\nincreased traction in recent years and is slowly shifting the focus from\nsingle-agent to multiagent environments. Dealing with multiple agents is\ninherently more complex as (a) the future rewards depend on the joint actions\nof multiple players and (b) the computational complexity of functions\nincreases. We present the most common multiagent problem representations and\ntheir main challenges, and identify five research areas that address one or\nmore of these challenges: centralised training and decentralised execution,\nopponent modelling, communication, efficient coordination, and reward shaping.\nWe find that many computational studies rely on unrealistic assumptions or are\nnot generalisable to other settings; they struggle to overcome the curse of\ndimensionality or nonstationarity. Approaches from psychology and sociology\ncapture promising relevant behaviours such as communication and coordination.\nWe suggest that, for multiagent reinforcement learning to be successful, future\nresearch addresses these challenges with an interdisciplinary approach to open\nup new possibilities for more human-oriented solutions in multiagent\nreinforcement learning.",
          "link": "http://arxiv.org/abs/2106.15691",
          "publishedOn": "2021-07-01T01:59:32.499Z",
          "wordCount": 636,
          "title": "Multiagent Deep Reinforcement Learning: Challenges and Directions Towards Human-Like Approaches. (arXiv:2106.15691v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmoudi_S/0/1/0/all/0/1\">Seyyed Ehsan Mahmoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>",
          "description": "In recent years there has been a special interest in word embeddings as a new\napproach to convert words to vectors. It has been a focal point to understand\nhow much of the semantics of the the words has been transferred into embedding\nvectors. This is important as the embedding is going to be used as the basis\nfor downstream NLP applications and it will be costly to evaluate the\napplication end-to-end in order to identify quality of the used embedding\nmodel. Generally the word embeddings are evaluated through a number of tests,\nincluding analogy test. In this paper we propose a test framework for Persian\nembedding models. Persian is a low resource language and there is no rich\nsemantic benchmark to evaluate word embedding models for this language. In this\npaper we introduce an evaluation framework including a hand crafted Persian SAT\nbased analogy dataset, a colliquial test set (specific to Persian) and a\nbenchmark to study the impact of various parameters on the semantic evaluation\ntask.",
          "link": "http://arxiv.org/abs/2106.15674",
          "publishedOn": "2021-07-01T01:59:32.482Z",
          "wordCount": 607,
          "title": "SAT Based Analogy Evaluation Framework for Persian Word Embeddings. (arXiv:2106.15674v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.01619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yonghao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Ying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun-Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng Ann Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>",
          "description": "Automatic surgical gesture recognition is fundamentally important to enable\nintelligent cognitive assistance in robotic surgery. With recent advancement in\nrobot-assisted minimally invasive surgery, rich information including surgical\nvideos and robotic kinematics can be recorded, which provide complementary\nknowledge for understanding surgical gestures. However, existing methods either\nsolely adopt uni-modal data or directly concatenate multi-modal\nrepresentations, which can not sufficiently exploit the informative\ncorrelations inherent in visual and kinematics data to boost gesture\nrecognition accuracies. In this regard, we propose a novel online approach of\nmulti-modal relational graph network (i.e., MRG-Net) to dynamically integrate\nvisual and kinematics information through interactive message propagation in\nthe latent feature space. In specific, we first extract embeddings from video\nand kinematics sequences with temporal convolutional networks and LSTM units.\nNext, we identify multi-relations in these multi-modal embeddings and leverage\nthem through a hierarchical relational graph learning module. The effectiveness\nof our method is demonstrated with state-of-the-art results on the public\nJIGSAWS dataset, outperforming current uni-modal and multi-modal methods on\nboth suturing and knot typing tasks. Furthermore, we validated our method on\nin-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK)\nplatforms in two centers, with consistent promising performance achieved.",
          "link": "http://arxiv.org/abs/2011.01619",
          "publishedOn": "2021-06-30T02:01:04.203Z",
          "wordCount": 701,
          "title": "Relational Graph Learning on Visual and Kinematics Embeddings for Accurate Gesture Recognition in Robotic Surgery. (arXiv:2011.01619v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhanlin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_J/0/1/0/all/0/1\">Jeremy Goldwasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuckman_P/0/1/0/all/0/1\">Philip Tuckman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1\">Mark Gerstein</a>",
          "description": "With the rise of single-cell sequencing technologies, there is a growing need\nfor robust clustering algorithms to extract deeper insights from data. Here, we\nintroduce an intuitive and efficient clustering method, Forest Fire Clustering,\nfor discovering and validating cell types in single-cell sequencing analysis.\nCompared to existing methods, our clustering algorithm makes minimum prior\nassumptions about the data distribution and can provide a point-wise\nsignificance value via Monte Carlo simulations for internal validation.\nAdditionally, point-wise label entropies can highlight novel transition cell\ntypes \\emph{de novo} along developmental pseudo-time manifolds. Lastly, our\ninductive algorithm has the ability to make robust inferences in an\nonline-learning context. In this paper, we describe the method, provide a\nsummary of its performance against common clustering benchmarks, and\ndemonstrate that Forest Fire Clustering is uniquely suitable for single-cell\nsequencing analysis.",
          "link": "http://arxiv.org/abs/2103.11802",
          "publishedOn": "2021-06-30T02:01:04.189Z",
          "wordCount": 625,
          "title": "Forest Fire Clustering: Iterative Label Propagation Clustering and Monte Carlo Validation For Single-cell Sequencing Analysis. (arXiv:2103.11802v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barenboim_G/0/1/0/all/0/1\">Gabriela Barenboim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirn_J/0/1/0/all/0/1\">Johannes Hirn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanz_V/0/1/0/all/0/1\">Veronica Sanz</a>",
          "description": "We explore whether Neural Networks (NNs) can {\\it discover} the presence of\nsymmetries as they learn to perform a task. For this, we train hundreds of NNs\non a {\\it decoy task} based on well-controlled Physics templates, where no\ninformation on symmetry is provided. We use the output from the last hidden\nlayer of all these NNs, projected to fewer dimensions, as the input for a\nsymmetry classification task, and show that information on symmetry had indeed\nbeen identified by the original NN without guidance. As an interdisciplinary\napplication of this procedure, we identify the presence and level of symmetry\nin artistic paintings from different styles such as those of Picasso, Pollock\nand Van Gogh.",
          "link": "http://arxiv.org/abs/2103.06115",
          "publishedOn": "2021-06-30T02:01:04.183Z",
          "wordCount": 579,
          "title": "Symmetry meets AI. (arXiv:2103.06115v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perov_I/0/1/0/all/0/1\">Ivan Perov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Daiheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chervoniy_N/0/1/0/all/0/1\">Nikolay Chervoniy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marangonda_S/0/1/0/all/0/1\">Sugasa Marangonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ume_C/0/1/0/all/0/1\">Chris Um&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dpfks_M/0/1/0/all/0/1\">Mr. Dpfks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facenheim_C/0/1/0/all/0/1\">Carl Shift Facenheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+RP_L/0/1/0/all/0/1\">Luis RP</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Pingyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>",
          "description": "Deepfake defense not only requires the research of detection but also\nrequires the efforts of generation methods. However, current deepfake methods\nsuffer the effects of obscure workflow and poor performance. To solve this\nproblem, we present DeepFaceLab, the current dominant deepfake framework for\nface-swapping. It provides the necessary tools as well as an easy-to-use way to\nconduct high-quality face-swapping. It also offers a flexible and loose\ncoupling structure for people who need to strengthen their pipeline with other\nfeatures without writing complicated boilerplate code. We detail the principles\nthat drive the implementation of DeepFaceLab and introduce its pipeline,\nthrough which every aspect of the pipeline can be modified painlessly by users\nto achieve their customization purpose. It is noteworthy that DeepFaceLab could\nachieve cinema-quality results with high fidelity. We demonstrate the advantage\nof our system by comparing our approach with other face-swapping methods.For\nmore information, please visit:https://github.com/iperov/DeepFaceLab/.",
          "link": "http://arxiv.org/abs/2005.05535",
          "publishedOn": "2021-06-30T02:01:04.171Z",
          "wordCount": 674,
          "title": "DeepFaceLab: Integrated, flexible and extensible face-swapping framework. (arXiv:2005.05535v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.03936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fatras_K/0/1/0/all/0/1\">Kilian Fatras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1\">Bharath Bhushan Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobry_S/0/1/0/all/0/1\">Sylvain Lobry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1\">R&#xe9;mi Flamary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1\">Devis Tuia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1\">Nicolas Courty</a>",
          "description": "Noisy labels often occur in vision datasets, especially when they are\nobtained from crowdsourcing or Web scraping. We propose a new regularization\nmethod, which enables learning robust classifiers in presence of noisy data. To\nachieve this goal, we propose a new adversarial regularization scheme based on\nthe Wasserstein distance. Using this distance allows taking into account\nspecific relations between classes by leveraging the geometric properties of\nthe labels space. Our Wasserstein Adversarial Regularization (WAR) encodes a\nselective regularization, which promotes smoothness of the classifier between\nsome classes, while preserving sufficient complexity of the decision boundary\nbetween others. We first discuss how and why adversarial regularization can be\nused in the context of label noise and then show the effectiveness of our\nmethod on five datasets corrupted with noisy labels: in both benchmarks and\nreal datasets, WAR outperforms the state-of-the-art competitors.",
          "link": "http://arxiv.org/abs/1904.03936",
          "publishedOn": "2021-06-30T02:01:04.147Z",
          "wordCount": 638,
          "title": "Wasserstein Adversarial Regularization (WAR) on label noise. (arXiv:1904.03936v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spooner_T/0/1/0/all/0/1\">Thomas Spooner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadori_N/0/1/0/all/0/1\">Nelson Vadori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_S/0/1/0/all/0/1\">Sumitra Ganesh</a>",
          "description": "Policy gradient methods can solve complex tasks but often fail when the\ndimensionality of the action-space or objective multiplicity grow very large.\nThis occurs, in part, because the variance on score-based gradient estimators\nscales quadratically. In this paper, we address this problem through a causal\nbaseline which exploits independence structure encoded in a novel action-target\ninfluence network. Causal policy gradients (CPGs), which follow, provide a\ncommon framework for analysing key state-of-the-art algorithms, are shown to\ngeneralise traditional policy gradients, and yield a principled way of\nincorporating prior knowledge of a problem domain's generative processes. We\nprovide an analysis of the proposed estimator and identify the conditions under\nwhich variance is reduced. The algorithmic aspects of CPGs are discussed,\nincluding optimal policy factorisation, as characterised by minimum biclique\ncoverings, and the implications for the bias-variance trade-off of incorrectly\nspecifying the network. Finally, we demonstrate the performance advantages of\nour algorithm on large-scale bandit and traffic intersection problems,\nproviding a novel contribution to the latter in the form of a spatio-causal\napproximation.",
          "link": "http://arxiv.org/abs/2102.10362",
          "publishedOn": "2021-06-30T02:01:04.141Z",
          "wordCount": 648,
          "title": "Causal Policy Gradients: Leveraging Structure for Efficient Learning in (Factored) MOMDPs. (arXiv:2102.10362v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brooks_E/0/1/0/all/0/1\">Ethan A. Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1\">Janarthanan Rajendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_R/0/1/0/all/0/1\">Richard L. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satinder Singh</a>",
          "description": "Learning to flexibly follow task instructions in dynamic environments poses\ninteresting challenges for reinforcement learning agents. We focus here on the\nproblem of learning control flow that deviates from a strict step-by-step\nexecution of instructions -- that is, control flow that may skip forward over\nparts of the instructions or return backward to previously completed or skipped\nsteps. Demand for such flexible control arises in two fundamental ways:\nexplicitly when control is specified in the instructions themselves (such as\nconditional branching and looping) and implicitly when stochastic environment\ndynamics require re-completion of instructions whose effects have been\nperturbed, or opportunistic skipping of instructions whose effects are already\npresent. We formulate an attention-based architecture that meets these\nchallenges by learning, from task reward only, to flexibly attend to and\ncondition behavior on an internal encoding of the instructions. We test the\narchitecture's ability to learn both explicit and implicit control in two\nillustrative domains -- one inspired by Minecraft and the other by StarCraft --\nand show that the architecture exhibits zero-shot generalization to novel\ninstructions of length greater than those in a training set, at a performance\nlevel unmatched by two baseline recurrent architectures and one ablation\narchitecture.",
          "link": "http://arxiv.org/abs/2102.13195",
          "publishedOn": "2021-06-30T02:01:04.136Z",
          "wordCount": 662,
          "title": "Reinforcement Learning of Implicit and Explicit Control Flow in Instructions. (arXiv:2102.13195v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.13607",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jin_Q/0/1/0/all/0/1\">Qiujiang Jin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mokhtari_A/0/1/0/all/0/1\">Aryan Mokhtari</a>",
          "description": "In this paper, we study and prove the non-asymptotic superlinear convergence\nrate of the Broyden class of quasi-Newton methods including\nDavidon--Fletcher--Powell (DFP) method and Broyden--Fletcher--Goldfarb--Shanno\n(BFGS) method. The asymptotic superlinear convergence rate of these\nquasi-Newton methods has been extensively studied, but their explicit finite\ntime local convergence rate is not fully investigated. In this paper, we\nprovide a finite time (non-asymptotic) convergence analysis for BFGS and DFP\nmethods under the assumptions that the objective function is strongly convex,\nits gradient is Lipschitz continuous, and its Hessian is Lipschitz continuous\nonly in the direction of the optimal solution. We show that in a local\nneighborhood of the optimal solution, the iterates generated by both DFP and\nBFGS converge to the optimal solution at a superlinear rate of $(1/k)^{k/2}$,\nwhere $k$ is the number of iterations. We also prove the same local superlinear\nconvergence rate in the case that the objective function is self-concordant.\nNumerical experiments on different objective functions confirm our explicit\nconvergence rates. Our theoretical guarantee is one of the first results that\nprovide a non-asymptotic superlinear convergence rate for DFP and BFGS\nquasi-Newton methods.",
          "link": "http://arxiv.org/abs/2003.13607",
          "publishedOn": "2021-06-30T02:01:04.131Z",
          "wordCount": 641,
          "title": "Non-asymptotic Superlinear Convergence of Standard Quasi-Newton Methods. (arXiv:2003.13607v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1\">Idan Achituve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1\">Aviv Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yemini_Y/0/1/0/all/0/1\">Yochai Yemini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1\">Ethan Fetaya</a>",
          "description": "Gaussian processes (GPs) are non-parametric, flexible, models that work well\nin many tasks. Combining GPs with deep learning methods via deep kernel\nlearning (DKL) is especially compelling due to the strong representational\npower induced by the network. However, inference in GPs, whether with or\nwithout DKL, can be computationally challenging on large datasets. Here, we\npropose GP-Tree, a novel method for multi-class classification with Gaussian\nprocesses and DKL. We develop a tree-based hierarchical model in which each\ninternal node of the tree fits a GP to the data using the P\\'olya Gamma\naugmentation scheme. As a result, our method scales well with both the number\nof classes and data size. We demonstrate the effectiveness of our method\nagainst other Gaussian process training baselines, and we show how our general\nGP approach achieves improved accuracy on standard incremental few-shot\nlearning benchmarks.",
          "link": "http://arxiv.org/abs/2102.07868",
          "publishedOn": "2021-06-30T02:01:04.126Z",
          "wordCount": 609,
          "title": "GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning. (arXiv:2102.07868v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Potapczynski_A/0/1/0/all/0/1\">Andres Potapczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Luhuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_D/0/1/0/all/0/1\">Dan Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1\">Geoff Pleiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1\">John P. Cunningham</a>",
          "description": "Scalable Gaussian Process methods are computationally attractive, yet\nintroduce modeling biases that require rigorous study. This paper analyzes two\ncommon techniques: early truncated conjugate gradients (CG) and random Fourier\nfeatures (RFF). We find that both methods introduce a systematic bias on the\nlearned hyperparameters: CG tends to underfit while RFF tends to overfit. We\naddress these issues using randomized truncation estimators that eliminate bias\nin exchange for increased variance. In the case of RFF, we show that the\nbias-to-variance conversion is indeed a trade-off: the additional variance\nproves detrimental to optimization. However, in the case of CG, our unbiased\nlearning procedure meaningfully outperforms its biased counterpart with minimal\nadditional computation.",
          "link": "http://arxiv.org/abs/2102.06695",
          "publishedOn": "2021-06-30T02:01:04.110Z",
          "wordCount": 584,
          "title": "Bias-Free Scalable Gaussian Processes via Randomized Truncations. (arXiv:2102.06695v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagadeesan_M/0/1/0/all/0/1\">Meena Jagadeesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razenshteyn_I/0/1/0/all/0/1\">Ilya Razenshteyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekar_S/0/1/0/all/0/1\">Suriya Gunasekar</a>",
          "description": "We study the function space characterization of the inductive bias resulting\nfrom controlling the $\\ell_2$ norm of the weights in linear convolutional\nnetworks. We view this in terms of an induced regularizer in the function space\ngiven by the minimum norm of weights required to realize a linear function. For\ntwo layer linear convolutional networks with $C$ output channels and kernel\nsize $K$, we show the following: (a) If the inputs to the network have a single\nchannel, the induced regularizer for any $K$ is a norm given by a semidefinite\nprogram (SDP) that is independent of the number of output channels $C$. (b) In\ncontrast, for networks with multi-channel inputs, multiple output channels can\nbe necessary to merely realize all matrix-valued linear functions and thus the\ninductive bias does depend on $C$. Further, for sufficiently large $C$, the\ninduced regularizer for $K=1$ and $K=D$ are the nuclear norm and the\n$\\ell_{2,1}$ group-sparse norm, respectively, of the Fourier coefficients. (c)\nComplementing our theoretical results, we show through experiments on MNIST and\nCIFAR-10 that our key findings extend to implicit biases from gradient descent\nin overparameterized networks.",
          "link": "http://arxiv.org/abs/2102.12238",
          "publishedOn": "2021-06-30T02:01:04.105Z",
          "wordCount": 662,
          "title": "Inductive Bias of Multi-Channel Linear Convolutional Networks with Bounded Weight Norm. (arXiv:2102.12238v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kratsios_A/0/1/0/all/0/1\">Anastasis Kratsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papon_L/0/1/0/all/0/1\">Leonie Papon</a>",
          "description": "This paper addresses the growing need to process non-Euclidean data, by\nintroducing a geometric deep learning (GDL) framework for building universal\nfeedforward-type models compatible with differentiable manifold geometries. We\nshow that our GDL models can approximate any continuous target function\nuniformly on compacts of a controlled maximum diameter. We obtain curvature\ndependant lower-bounds on this maximum diameter and upper-bounds on the depth\nof our approximating GDL models. Conversely, we find that there is always a\ncontinuous function between any two non-degenerate compact manifolds that any\n\"locally-defined\" GDL model cannot uniformly approximate. Our last main result\nidentifies data-dependent conditions guaranteeing that the GDL model\nimplementing our approximation breaks \"the curse of dimensionality.\" We find\nthat any \"real-world\" (i.e. finite) dataset always satisfies our condition and,\nconversely, any dataset satisfies our requirement if the target function is\nsmooth. As applications, we confirm the universal approximation capabilities of\nthe following GDL models: Ganea et al. (2018)'s hyperbolic feedforward\nnetworks, the architecture implementing Krishnan et al. (2015)'s deep\nKalman-Filter, and deep softmax classifiers. We build universal\nextensions/variants of: the SPD-matrix regressor of Meyer et al. (2011), and\nFletcher et al. (2009)'s Procrustean regressor. In the Euclidean setting, our\nresults imply a quantitative version of Kidger and Lyons (2020)'s approximation\ntheorem and a data-dependent version of Yarotsky and Zhevnerchuk (2020)'s\nuncursed approximation rates.",
          "link": "http://arxiv.org/abs/2101.05390",
          "publishedOn": "2021-06-30T02:01:04.099Z",
          "wordCount": 734,
          "title": "Universal Approximation Theorems for Differentiable Geometric Deep Learning. (arXiv:2101.05390v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Jungmin Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeongseop Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyunseo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1\">In Kwon Choi</a>",
          "description": "Recently, learning algorithms motivated from sharpness of loss surface as an\neffective measure of generalization gap have shown state-of-the-art\nperformances. Nevertheless, sharpness defined in a rigid region with a fixed\nradius, has a drawback in sensitivity to parameter re-scaling which leaves the\nloss unaffected, leading to weakening of the connection between sharpness and\ngeneralization gap. In this paper, we introduce the concept of adaptive\nsharpness which is scale-invariant and propose the corresponding generalization\nbound. We suggest a novel learning method, adaptive sharpness-aware\nminimization (ASAM), utilizing the proposed generalization bound. Experimental\nresults in various benchmark datasets show that ASAM contributes to significant\nimprovement of model generalization performance.",
          "link": "http://arxiv.org/abs/2102.11600",
          "publishedOn": "2021-06-30T02:01:04.094Z",
          "wordCount": 593,
          "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks. (arXiv:2102.11600v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08454",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>",
          "description": "Recent advances in automatic speech recognition (ASR) have achieved accuracy\nlevels comparable to human transcribers, which led researchers to debate if the\nmachine has reached human performance. Previous work focused on the English\nlanguage and modular hidden Markov model-deep neural network (HMM-DNN) systems.\nIn this paper, we perform a comprehensive benchmarking for end-to-end\ntransformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the\nArabic language and its dialects. For the HSR, we evaluate linguist performance\nand lay-native speaker performance on a new dataset collected as a part of this\nstudy. For ASR the end-to-end work led to 12.5%, 27.5%, 33.8% WER; a new\nperformance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our\nresults suggest that human performance in the Arabic language is still\nconsiderably better than the machine with an absolute WER gap of 3.5% on\naverage.",
          "link": "http://arxiv.org/abs/2101.08454",
          "publishedOn": "2021-06-30T02:01:04.082Z",
          "wordCount": 608,
          "title": "Arabic Speech Recognition by End-to-End, Modular Systems and Human. (arXiv:2101.08454v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08663",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Visschers_J/0/1/0/all/0/1\">Jim C. Visschers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Budker_D/0/1/0/all/0/1\">Dmitry Budker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bougas_L/0/1/0/all/0/1\">Lykourgos Bougas</a>",
          "description": "In this work we demonstrate the use of neural networks for rapid extraction\nof signal parameters of discretely sampled signals. In particular, we use dense\nautoencoder networks to extract the parameters of interest from exponentially\ndecaying signals and decaying oscillations. By using a three-stage training\nmethod and careful choice of the neural network size, we are able to retrieve\nthe relevant signal parameters directly from the latent space of the\nautoencoder network at significantly improved rates compared to traditional\nalgorithmic signal-analysis approaches. We show that the achievable precision\nand accuracy of this method of analysis is similar to conventional\nalgorithm-based signal analysis methods, by demonstrating that the extracted\nsignal parameters are approaching their fundamental parameter estimation limit\nas provided by the Cram\\'er-Rao bound. Furthermore, we demonstrate that\nautoencoder networks are able to achieve signal analysis, and, hence, parameter\nextraction, at rates of 75 kHz, orders-of-magnitude faster than conventional\ntechniques with similar precision. Finally, we explore the limitations of our\napproach, demonstrating that analysis rates of $>$200 kHz are feasible with\nfurther optimization of the transfer rate between the data-acquisition system\nand data-analysis system.",
          "link": "http://arxiv.org/abs/2103.08663",
          "publishedOn": "2021-06-30T02:01:04.077Z",
          "wordCount": 658,
          "title": "Rapid parameter estimation of discrete decaying signals using autoencoder networks. (arXiv:2103.08663v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.07702",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lan_G/0/1/0/all/0/1\">Guanghui Lan</a>",
          "description": "Stochastic dual dynamic programming is a cutting plane type algorithm for\nmulti-stage stochastic optimization originated about 30 years ago. In spite of\nits popularity in practice, there does not exist any analysis on the\nconvergence rates of this method. In this paper, we first establish the number\nof iterations, i.e., iteration complexity, required by a basic dynamic cutting\nplane method for solving relatively simple multi-stage optimization problems,\nby introducing novel mathematical tools including the saturation of search\npoints. We then refine these basic tools and establish the iteration complexity\nfor both deterministic and stochastic dual dynamic programming methods for\nsolving more general multi-stage stochastic optimization problems under the\nstandard stage-wise independence assumption. Our results indicate that the\ncomplexity of these methods mildly increases with the number of stages $T$, in\nfact linearly dependent on $T$ for discounted problems. Therefore, they are\nefficient for strategic decision making which involves a large number of\nstages, but with a relatively small number of decision variables in each stage.\nWithout explicitly discretizing the state and action spaces, these methods\nmight also be pertinent to the related reinforcement learning and stochastic\ncontrol areas.",
          "link": "http://arxiv.org/abs/1912.07702",
          "publishedOn": "2021-06-30T02:01:04.061Z",
          "wordCount": 668,
          "title": "Complexity of Stochastic Dual Dynamic Programming. (arXiv:1912.07702v6 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02316",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Kong_Z/0/1/0/all/0/1\">Zhihui Kong</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Jiang_J/0/1/0/all/0/1\">Jonathan H. Jiang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Zhu_Z/0/1/0/all/0/1\">Zong-Hong Zhu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Fahy_K/0/1/0/all/0/1\">Kristen A. Fahy</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Burn_R/0/1/0/all/0/1\">Remo Burn</a>",
          "description": "Exoplanet detection in the past decade by efforts including NASA's Kepler and\nTESS missions has discovered many worlds that differ substantially from planets\nin our own Solar system, including more than 400 exoplanets orbiting binary or\nmulti-star systems. This not only broadens our understanding of the diversity\nof exoplanets, but also promotes our study of exoplanets in the complex binary\nand multi-star systems and provides motivation to explore their habitability.\nIn this study, we analyze orbital stability of exoplanets in non-coplanar\ncircumbinary systems using a numerical simulation method, with which a large\nnumber of circumbinary planet samples are generated in order to quantify the\neffects of various orbital parameters on orbital stability. We also train a\nmachine learning model that can quickly determine the stability of the\ncircumbinary planetary systems. Our results indicate that larger inclinations\nof the planet tend to increase the stability of its orbit, but change in the\nplanet's mass range between Earth and Jupiter has little effect on the\nstability of the system. In addition, we find that Deep Neural Networks (DNNs)\nhave higher accuracy and precision than other machine learning algorithms.",
          "link": "http://arxiv.org/abs/2101.02316",
          "publishedOn": "2021-06-30T02:01:04.056Z",
          "wordCount": 661,
          "title": "Analyzing the Stability of Non-coplanar Circumbinary Planets using Machine Learning. (arXiv:2101.02316v2 [astro-ph.EP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Charikar_M/0/1/0/all/0/1\">Moses Charikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lunjia Hu</a>",
          "description": "Many clustering algorithms are guided by certain cost functions such as the\nwidely-used $k$-means cost. These algorithms divide data points into clusters\nwith often complicated boundaries, creating difficulties in explaining the\nclustering decision. In a recent work, Dasgupta, Frost, Moshkovitz, and\nRashtchian (ICML'20) introduced explainable clustering, where the cluster\nboundaries are axis-parallel hyperplanes and the clustering is obtained by\napplying a decision tree to the data. The central question here is: how much\ndoes the explainability constraint increase the value of the cost function?\n\nGiven $d$-dimensional data points, we show an efficient algorithm that finds\nan explainable clustering whose $k$-means cost is at most $k^{1 -\n2/d}\\mathrm{poly}(d\\log k)$ times the minimum cost achievable by a clustering\nwithout the explainability constraint, assuming $k,d\\ge 2$. Combining this with\nan independent work by Makarychev and Shan (ICML'21), we get an improved bound\nof $k^{1 - 2/d}\\mathrm{polylog}(k)$, which we show is optimal for every choice\nof $k,d\\ge 2$ up to a poly-logarithmic factor in $k$. For $d = 2$ in\nparticular, we show an $O(\\log k\\log\\log k)$ bound, improving exponentially\nover the previous best bound of $\\widetilde O(k)$.",
          "link": "http://arxiv.org/abs/2106.15566",
          "publishedOn": "2021-06-30T02:01:04.050Z",
          "wordCount": 624,
          "title": "Near-Optimal Explainable $k$-Means for All Dimensions. (arXiv:2106.15566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.12365",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1\">Jonathan W. Siegel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1\">Jinchao Xu</a>",
          "description": "This article addresses the problem of approximating a function in a Hilbert\nspace by an expansion over a dictionary $\\mathbb{D}$. We introduce the notion\nof a smoothly parameterized dictionary and give upper bounds on the\napproximation rates, metric entropy and $n$-widths of the absolute convex hull,\nwhich we denote $B_1(\\mathbb{D})$, of such dictionaries. The upper bounds\ndepend upon the order of smoothness of the parameterization, and improve upon\nexisting results in many cases. The main applications of these results is to\nthe dictionaries $\\mathbb{D} = \\{\\sigma(\\omega\\cdot x + b)\\}\\subset L^2$\ncorresponding to shallow neural networks with activation function $\\sigma$, and\nto the dictionary of decaying Fourier modes corresponding to the spectral\nBarron space. This improves upon existing approximation rates for shallow\nneural networks when $\\sigma = \\text{ReLU}^k$ for $k\\geq 2$, sharpens bounds on\nthe metric entropy, and provides the first bounds on the Gelfand $n$-widths of\nthe Barron space and spectral Barron space.",
          "link": "http://arxiv.org/abs/2101.12365",
          "publishedOn": "2021-06-30T02:01:04.045Z",
          "wordCount": 652,
          "title": "Improved Approximation Properties of Dictionaries and Applications to Neural Networks. (arXiv:2101.12365v6 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krause_S/0/1/0/all/0/1\">Stefanie Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otto_O/0/1/0/all/0/1\">Oliver Otto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolzenburg_F/0/1/0/all/0/1\">Frieder Stolzenburg</a>",
          "description": "Recurrent neural networks are a powerful means in diverse applications. We\nshow that, together with so-called conceptors, they also allow fast learning,\nin contrast to other deep learning methods. In addition, a relatively small\nnumber of examples suffices to train neural networks with high accuracy. We\ndemonstrate this with two applications, namely speech recognition and detecting\ncar driving maneuvers. We improve the state of the art by application-specific\npreparation techniques: For speech recognition, we use mel frequency cepstral\ncoefficients leading to a compact representation of the frequency spectra, and\ndetecting car driving maneuvers can be done without the commonly used\npolynomial interpolation, as our evaluation suggests.",
          "link": "http://arxiv.org/abs/2102.05588",
          "publishedOn": "2021-06-30T02:01:04.040Z",
          "wordCount": 628,
          "title": "Fast Classification Learning with Neural Networks and Conceptors for Speech Recognition and Car Driving Maneuvers. (arXiv:2102.05588v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiari_Y/0/1/0/all/0/1\">Yasaman Esfandiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sin Yong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhanhong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balu_A/0/1/0/all/0/1\">Aditya Balu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herron_E/0/1/0/all/0/1\">Ethan Herron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1\">Chinmay Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Soumik Sarkar</a>",
          "description": "Decentralized learning enables a group of collaborative agents to learn\nmodels using a distributed dataset without the need for a central parameter\nserver. Recently, decentralized learning algorithms have demonstrated\nstate-of-the-art results on benchmark data sets, comparable with centralized\nalgorithms. However, the key assumption to achieve competitive performance is\nthat the data is independently and identically distributed (IID) among the\nagents which, in real-life applications, is often not applicable. Inspired by\nideas from continual learning, we propose Cross-Gradient Aggregation (CGA), a\nnovel decentralized learning algorithm where (i) each agent aggregates\ncross-gradient information, i.e., derivatives of its model with respect to its\nneighbors' datasets, and (ii) updates its model using a projected gradient\nbased on quadratic programming (QP). We theoretically analyze the convergence\ncharacteristics of CGA and demonstrate its efficiency on non-IID data\ndistributions sampled from the MNIST and CIFAR-10 datasets. Our empirical\ncomparisons show superior learning performance of CGA over existing\nstate-of-the-art decentralized learning algorithms, as well as maintaining the\nimproved performance under information compression to reduce peer-to-peer\ncommunication overhead. The code is available here on GitHub.",
          "link": "http://arxiv.org/abs/2103.02051",
          "publishedOn": "2021-06-30T02:01:04.027Z",
          "wordCount": 654,
          "title": "Cross-Gradient Aggregation for Decentralized Learning from Non-IID data. (arXiv:2103.02051v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lounici_K/0/1/0/all/0/1\">Karim Lounici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meziani_K/0/1/0/all/0/1\">Katia Meziani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riu_B/0/1/0/all/0/1\">Benjamin Riu</a>",
          "description": "Deep Learning (DL) is considered the state-of-the-art in computer vision,\nspeech recognition and natural language processing. Until recently, it was also\nwidely accepted that DL is irrelevant for learning tasks on tabular data,\nespecially in the small sample regime where ensemble methods are acknowledged\nas the gold standard. We present a new end-to-end differentiable method to\ntrain a standard FFNN. Our method, \\textbf{Muddling labels for Regularization}\n(\\texttt{MLR}), penalizes memorization through the generation of uninformative\nlabels and the application of a differentiable close-form regularization scheme\non the last hidden layer during training. \\texttt{MLR} outperforms classical NN\nand the gold standard (GBDT, RF) for regression and classification tasks on\nseveral datasets from the UCI database and Kaggle covering a large range of\nsample sizes and feature to sample ratios. Researchers and practitioners can\nuse \\texttt{MLR} on its own as an off-the-shelf \\DL{} solution or integrate it\ninto the most advanced ML pipelines.",
          "link": "http://arxiv.org/abs/2106.04462",
          "publishedOn": "2021-06-30T02:01:04.021Z",
          "wordCount": 606,
          "title": "Muddling Label Regularization: Deep Learning for Tabular Datasets. (arXiv:2106.04462v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1\">Audrey Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leqi_L/0/1/0/all/0/1\">Liu Leqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1\">Kamyar Azizzadenesheli</a>",
          "description": "Even when unable to run experiments, practitioners can evaluate prospective\npolicies, using previously logged data. However, while the bandits literature\nhas adopted a diverse set of objectives, most research on off-policy evaluation\nto date focuses on the expected reward. In this paper, we introduce Lipschitz\nrisk functionals, a broad class of objectives that subsumes conditional\nvalue-at-risk (CVaR), variance, mean-variance, many distorted risks, and CPT\nrisks, among others. We propose Off-Policy Risk Assessment (OPRA), a framework\nthat first estimates a target policy's CDF and then generates plugin estimates\nfor any collection of Lipschitz risks, providing finite sample guarantees that\nhold simultaneously over the entire class. We instantiate OPRA with both\nimportance sampling and doubly robust estimators. Our primary theoretical\ncontributions are (i) the first uniform concentration inequalities for both CDF\nestimators in contextual bandits and (ii) error bounds on our Lipschitz risk\nestimates, which all converge at a rate of $O(1/\\sqrt{n})$.",
          "link": "http://arxiv.org/abs/2104.08977",
          "publishedOn": "2021-06-30T02:01:04.015Z",
          "wordCount": 611,
          "title": "Off-Policy Risk Assessment in Contextual Bandits. (arXiv:2104.08977v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gabbay_A/0/1/0/all/0/1\">Aviv Gabbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Niv Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>",
          "description": "Unsupervised disentanglement has been shown to be theoretically impossible\nwithout inductive biases on the models and the data. As an alternative\napproach, recent methods rely on limited supervision to disentangle the factors\nof variation and allow their identifiability. While annotating the true\ngenerative factors is only required for a limited number of observations, we\nargue that it is infeasible to enumerate all the factors of variation that\ndescribe a real-world image distribution. To this end, we propose a method for\ndisentangling a set of factors which are only partially labeled, as well as\nseparating the complementary set of residual factors that are never explicitly\nspecified. Our success in this challenging setting, demonstrated on synthetic\nbenchmarks, gives rise to leveraging off-the-shelf image descriptors to\npartially annotate a subset of attributes in real image domains (e.g. of human\nfaces) with minimal manual effort. Specifically, we use a recent language-image\nembedding model (CLIP) to annotate a set of attributes of interest in a\nzero-shot manner and demonstrate state-of-the-art disentangled image\nmanipulation results.",
          "link": "http://arxiv.org/abs/2106.15610",
          "publishedOn": "2021-06-30T02:01:04.009Z",
          "wordCount": 625,
          "title": "An Image is Worth More Than a Thousand Words: Towards Disentanglement in the Wild. (arXiv:2106.15610v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15546",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_N/0/1/0/all/0/1\">Naresh Balaji Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lansner_A/0/1/0/all/0/1\">Anders Lansner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herman_P/0/1/0/all/0/1\">Pawel Herman</a>",
          "description": "Learning internal representations from data using no or few labels is useful\nfor machine learning research, as it allows using massive amounts of unlabeled\ndata. In this work, we use the Bayesian Confidence Propagation Neural Network\n(BCPNN) model developed as a biologically plausible model of the cortex. Recent\nwork has demonstrated that these networks can learn useful internal\nrepresentations from data using local Bayesian-Hebbian learning rules. In this\nwork, we show how such representations can be leveraged in a semi-supervised\nsetting by introducing and comparing different classifiers. We also evaluate\nand compare such networks with other popular semi-supervised classifiers.",
          "link": "http://arxiv.org/abs/2106.15546",
          "publishedOn": "2021-06-30T02:01:04.003Z",
          "wordCount": 533,
          "title": "Semi-supervised learning with Bayesian Confidence Propagation Neural Network. (arXiv:2106.15546v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Saem Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Donghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>",
          "description": "Video frame interpolation is the task of creating an interframe between two\nadjacent frames along the time axis. So, instead of simply averaging two\nadjacent frames to create an intermediate image, this operation should maintain\nsemantic continuity with the adjacent frames. Most conventional methods use\noptical flow, and various tools such as occlusion handling and object smoothing\nare indispensable. Since the use of these various tools leads to complex\nproblems, we tried to tackle the video interframe generation problem without\nusing problematic optical flow . To enable this , we have tried to use a deep\nneural network with an invertible structure, and developed an U-Net based\nGenerative Flow which is a modified normalizing flow. In addition, we propose a\nlearning method with a new consistency loss in the latent space to maintain\nsemantic temporal consistency between frames. The resolution of the generated\nimage is guaranteed to be identical to that of the original images by using an\ninvertible network. Furthermore, as it is not a random image like the ones by\ngenerative models, our network guarantees stable outputs without flicker.\nThrough experiments, we \\sam {confirmed the feasibility of the proposed\nalgorithm and would like to suggest the U-Net based Generative Flow as a new\npossibility for baseline in video frame interpolation. This paper is meaningful\nin that it is the world's first attempt to use invertible networks instead of\noptical flows for video interpolation.",
          "link": "http://arxiv.org/abs/2103.09576",
          "publishedOn": "2021-06-30T02:01:03.991Z",
          "wordCount": 718,
          "title": "The U-Net based GLOW for Optical-Flow-free Video Interframe Generation. (arXiv:2103.09576v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Baoyu Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>",
          "description": "Networks have been widely used to represent the relations between objects\nsuch as academic networks and social networks, and learning embedding for\nnetworks has thus garnered plenty of research attention. Self-supervised\nnetwork representation learning aims at extracting node embedding without\nexternal supervision. Recently, maximizing the mutual information between the\nlocal node embedding and the global summary (e.g. Deep Graph Infomax, or DGI\nfor short) has shown promising results on many downstream tasks such as node\nclassification. However, there are two major limitations of DGI. Firstly, DGI\nmerely considers the extrinsic supervision signal (i.e., the mutual information\nbetween node embedding and global summary) while ignores the intrinsic signal\n(i.e., the mutual dependence between node embedding and node attributes).\nSecondly, nodes in a real-world network are usually connected by multiple edges\nwith different relations, while DGI does not fully explore the various\nrelations among nodes. To address the above-mentioned problems, we propose a\nnovel framework, called High-order Deep Multiplex Infomax (HDMI), for learning\nnode embedding on multiplex networks in a self-supervised way. To be more\nspecific, we first design a joint supervision signal containing both extrinsic\nand intrinsic mutual information by high-order mutual information, and we\npropose a High-order Deep Infomax (HDI) to optimize the proposed supervision\nsignal. Then we propose an attention based fusion module to combine node\nembedding from different layers of the multiplex network. Finally, we evaluate\nthe proposed HDMI on various downstream tasks such as unsupervised clustering\nand supervised classification. The experimental results show that HDMI achieves\nstate-of-the-art performance on these tasks.",
          "link": "http://arxiv.org/abs/2102.07810",
          "publishedOn": "2021-06-30T02:01:03.985Z",
          "wordCount": 751,
          "title": "HDMI: High-order Deep Multiplex Infomax. (arXiv:2102.07810v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02685",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ganassali_L/0/1/0/all/0/1\">Luca Ganassali</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lelarge_M/0/1/0/all/0/1\">Marc Lelarge</a>",
          "description": "Random graph alignment refers to recovering the underlying vertex\ncorrespondence between two random graphs with correlated edges. This can be\nviewed as an average-case and noisy version of the well-known graph isomorphism\nproblem. For the correlated Erd\\\"os-R\\'enyi model, we prove an impossibility\nresult for partial recovery in the sparse regime, with constant average degree\nand correlation, as well as a general bound on the maximal reachable overlap.\nOur bound is tight in the noiseless case (the graph isomorphism problem) and we\nconjecture that it is still tight with noise. Our proof technique relies on a\ncareful application of the probabilistic method to build automorphisms between\ntree components of a subcritical Erd\\\"os-R\\'enyi graph.",
          "link": "http://arxiv.org/abs/2102.02685",
          "publishedOn": "2021-06-30T02:01:03.980Z",
          "wordCount": 582,
          "title": "Impossibility of Partial Recovery in the Graph Alignment Problem. (arXiv:2102.02685v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.05715",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Olshevsky_V/0/1/0/all/0/1\">Vyacheslav Olshevsky</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Khotyaintsev_Y/0/1/0/all/0/1\">Yuri V. Khotyaintsev</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lalti_A/0/1/0/all/0/1\">Ahmad Lalti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Divin_A/0/1/0/all/0/1\">Andrey Divin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Delzanno_G/0/1/0/all/0/1\">Gian Luca Delzanno</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Anderzen_S/0/1/0/all/0/1\">Sven Anderzen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Herman_P/0/1/0/all/0/1\">Pawel Herman</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chien_S/0/1/0/all/0/1\">Steven W.D. Chien</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Avanov_L/0/1/0/all/0/1\">Levon Avanov</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dimmock_A/0/1/0/all/0/1\">Andrew P. Dimmock</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Markidis_S/0/1/0/all/0/1\">Stefano Markidis</a>",
          "description": "We investigate the properties of the ion sky maps produced by the Dual Ion\nSpectrometers (DIS) from the Fast Plasma Investigation (FPI). We have trained a\nconvolutional neural network classifier to predict four regions crossed by the\nMMS on the dayside magnetosphere: solar wind, ion foreshock, magnetosheath, and\nmagnetopause using solely DIS spectrograms. The accuracy of the classifier is\n>98%. We use the classifier to detect mixed plasma regions, in particular to\nfind the bow shock regions. A similar approach can be used to identify the\nmagnetopause crossings and reveal regions prone to magnetic reconnection. Data\nprocessing through the trained classifier is fast and efficient and thus can be\nused for classification for the whole MMS database.",
          "link": "http://arxiv.org/abs/1908.05715",
          "publishedOn": "2021-06-30T02:01:03.975Z",
          "wordCount": 612,
          "title": "Automated classification of plasma regions using 3D particle energy distributions. (arXiv:1908.05715v3 [physics.space-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>",
          "description": "Recently generating natural language explanations has shown very promising\nresults in not only offering interpretable explanations but also providing\nadditional information and supervision for prediction. However, existing\napproaches usually require a large set of human annotated explanations for\ntraining while collecting a large set of explanations is not only time\nconsuming but also expensive. In this paper, we develop a general framework for\ninterpretable natural language understanding that requires only a small set of\nhuman annotated explanations for training. Our framework treats natural\nlanguage explanations as latent variables that model the underlying reasoning\nprocess of a neural model. We develop a variational EM framework for\noptimization where an explanation generation module and an\nexplanation-augmented prediction module are alternatively optimized and\nmutually enhance each other. Moreover, we further propose an explanation-based\nself-training method under this framework for semi-supervised learning. It\nalternates between assigning pseudo-labels to unlabeled data and generating new\nexplanations to iteratively improve each other. Experiments on two natural\nlanguage understanding tasks demonstrate that our framework can not only make\neffective predictions in both supervised and semi-supervised settings, but also\ngenerate good natural language explanation.",
          "link": "http://arxiv.org/abs/2011.05268",
          "publishedOn": "2021-06-30T02:01:03.961Z",
          "wordCount": 670,
          "title": "Towards Interpretable Natural Language Understanding with Explanations as Latent Variables. (arXiv:2011.05268v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mhaskar_H/0/1/0/all/0/1\">H.N. Mhaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereverzyev_S/0/1/0/all/0/1\">S.V. Pereverzyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walt_M/0/1/0/all/0/1\">M.D. van der Walt</a>",
          "description": "The problem of real time prediction of blood glucose (BG) levels based on the\nreadings from a continuous glucose monitoring (CGM) device is a problem of\ngreat importance in diabetes care, and therefore, has attracted a lot of\nresearch in recent years, especially based on machine learning. An accurate\nprediction with a 30, 60, or 90 minute prediction horizon has the potential of\nsaving millions of dollars in emergency care costs. In this paper, we treat the\nproblem as one of function approximation, where the value of the BG level at\ntime $t+h$ (where $h$ the prediction horizon) is considered to be an unknown\nfunction of $d$ readings prior to the time $t$. This unknown function may be\nsupported in particular on some unknown submanifold of the $d$-dimensional\nEuclidean space. While manifold learning is classically done in a\nsemi-supervised setting, where the entire data has to be known in advance, we\nuse recent ideas to achieve an accurate function approximation in a supervised\nsetting; i.e., construct a model for the target function. We use the\nstate-of-the-art clinically relevant PRED-EGA grid to evaluate our results, and\ndemonstrate that for a real life dataset, our method performs better than a\nstandard deep network, especially in hypoglycemic and hyperglycemic regimes.\nOne noteworthy aspect of this work is that the training data and test data may\ncome from different distributions.",
          "link": "http://arxiv.org/abs/2105.05893",
          "publishedOn": "2021-06-30T02:01:03.956Z",
          "wordCount": 709,
          "title": "A function approximation approach to the prediction of blood glucose levels. (arXiv:2105.05893v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "This work makes multiple scientific contributions to the field of Indoor\nLocalization for Ambient Assisted Living in Smart Homes. First, it presents a\nBig-Data driven methodology that studies the multimodal components of user\ninteractions and analyzes the data from Bluetooth Low Energy (BLE) beacons and\nBLE scanners to detect a user's indoor location in a specific activity-based\nzone during Activities of Daily Living. Second, it introduces a context\nindependent approach that can interpret the accelerometer and gyroscope data\nfrom diverse behavioral patterns to detect the zone-based indoor location of a\nuser in any Internet of Things (IoT)-based environment. These two approaches\nachieved performance accuracies of 81.36% and 81.13%, respectively, when tested\non a dataset. Third, it presents a methodology to detect the spatial\ncoordinates of a user's indoor position that outperforms all similar works in\nthis field, as per the associated root mean squared error - one of the\nperformance evaluation metrics in ISO/IEC18305:2016- an international standard\nfor testing Localization and Tracking Systems. Finally, it presents a\ncomprehensive comparative study that includes Random Forest, Artificial Neural\nNetwork, Decision Tree, Support Vector Machine, k-NN, Gradient Boosted Trees,\nDeep Learning, and Linear Regression, to address the challenge of identifying\nthe optimal machine learning approach for Indoor Localization.",
          "link": "http://arxiv.org/abs/2106.15606",
          "publishedOn": "2021-06-30T02:01:03.951Z",
          "wordCount": 668,
          "title": "Multimodal Approaches for Indoor Localization for Ambient Assisted Living in Smart Homes. (arXiv:2106.15606v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03546",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gatti_A/0/1/0/all/0/1\">Alice Gatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhixiong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smidt_T/0/1/0/all/0/1\">Tess Smidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_E/0/1/0/all/0/1\">Esmond G. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghysels_P/0/1/0/all/0/1\">Pieter Ghysels</a>",
          "description": "We present a novel method for graph partitioning, based on reinforcement\nlearning and graph convolutional neural networks. Our approach is to\nrecursively partition coarser representations of a given graph. The neural\nnetwork is implemented using SAGE graph convolution layers, and trained using\nan advantage actor critic (A2C) agent. We present two variants, one for finding\nan edge separator that minimizes the normalized cut or quotient cut, and one\nthat finds a small vertex separator. The vertex separators are then used to\nconstruct a nested dissection ordering to permute a sparse matrix so that its\ntriangular factorization will incur less fill-in. The partitioning quality is\ncompared with partitions obtained using METIS and SCOTCH, and the nested\ndissection ordering is evaluated in the sparse solver SuperLU. Our results show\nthat the proposed method achieves similar partitioning quality as METIS and\nSCOTCH. Furthermore, the method generalizes across different classes of graphs,\nand works well on a variety of graphs from the SuiteSparse sparse matrix\ncollection.",
          "link": "http://arxiv.org/abs/2104.03546",
          "publishedOn": "2021-06-30T02:01:03.940Z",
          "wordCount": 634,
          "title": "Graph Partitioning and Sparse Matrix Ordering using Reinforcement Learning and Graph Neural Networks. (arXiv:2104.03546v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.10613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gadgil_S/0/1/0/all/0/1\">Soham Gadgil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1\">Adolf Pfefferbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1\">Edith V. Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "The Blood-Oxygen-Level-Dependent (BOLD) signal of resting-state fMRI\n(rs-fMRI) records the temporal dynamics of intrinsic functional networks in the\nbrain. However, existing deep learning methods applied to rs-fMRI either\nneglect the functional dependency between different brain regions in a network\nor discard the information in the temporal dynamics of brain activity. To\novercome those shortcomings, we propose to formulate functional connectivity\nnetworks within the context of spatio-temporal graphs. We train a\nspatio-temporal graph convolutional network (ST-GCN) on short sub-sequences of\nthe BOLD time series to model the non-stationary nature of functional\nconnectivity. Simultaneously, the model learns the importance of graph edges\nwithin ST-GCN to gain insight into the functional connectivities contributing\nto the prediction. In analyzing the rs-fMRI of the Human Connectome Project\n(HCP, N=1,091) and the National Consortium on Alcohol and Neurodevelopment in\nAdolescence (NCANDA, N=773), ST-GCN is significantly more accurate than common\napproaches in predicting gender and age based on BOLD signals. Furthermore, the\nbrain regions and functional connections significantly contributing to the\npredictions of our model are important markers according to the neuroscience\nliterature.",
          "link": "http://arxiv.org/abs/2003.10613",
          "publishedOn": "2021-06-30T02:01:03.925Z",
          "wordCount": 654,
          "title": "Spatio-Temporal Graph Convolution for Resting-State fMRI Analysis. (arXiv:2003.10613v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1\">Sai Praneeth Karimireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Byzantine robustness has received significant attention recently given its\nimportance for distributed and federated learning. In spite of this, we\nidentify severe flaws in existing algorithms even when the data across the\nparticipants is identically distributed. First, we show realistic examples\nwhere current state of the art robust aggregation rules fail to converge even\nin the absence of any Byzantine attackers. Secondly, we prove that even if the\naggregation rules may succeed in limiting the influence of the attackers in a\nsingle round, the attackers can couple their attacks across time eventually\nleading to divergence. To address these issues, we present two surprisingly\nsimple strategies: a new robust iterative clipping procedure, and incorporating\nworker momentum to overcome time-coupled attacks. This is the first provably\nrobust method for the standard stochastic optimization setting. Our code is\nopen sourced at https://github.com/epfml/byzantine-robust-optimizer.",
          "link": "http://arxiv.org/abs/2012.10333",
          "publishedOn": "2021-06-30T02:01:03.880Z",
          "wordCount": 643,
          "title": "Learning from History for Byzantine Robust Optimization. (arXiv:2012.10333v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olivieri_M/0/1/0/all/0/1\">Marco Olivieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pezzoli_M/0/1/0/all/0/1\">Mirco Pezzoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonacci_F/0/1/0/all/0/1\">Fabio Antonacci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarti_A/0/1/0/all/0/1\">Augusto Sarti</a>",
          "description": "Near-field Acoustic Holography (NAH) is a well-known problem aimed at\nestimating the vibrational velocity field of a structure by means of acoustic\nmeasurements. In this paper, we propose a NAH technique based on Convolutional\nNeural Network (CNN). The devised CNN predicts the vibrational field on the\nsurface of arbitrary shaped plates (violin plates) with orthotropic material\nproperties from a limited number of measurements. In particular, the\narchitecture, named Super Resolution CNN (SRCNN), is able to estimate the\nvibrational field with a higher spatial resolution compared to the input\npressure. The pressure and velocity datasets have been generated through Finite\nElement Method simulations. We validate the proposed method by comparing the\nestimates with the synthesized ground truth and with a state-of-the-art\ntechnique. Moreover, we evaluate the robustness of the devised network against\nnoisy input data.",
          "link": "http://arxiv.org/abs/2103.16935",
          "publishedOn": "2021-06-30T02:01:03.872Z",
          "wordCount": 612,
          "title": "Near field Acoustic Holography on arbitrary shapes using Convolutional Neural Network. (arXiv:2103.16935v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.08613",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ozbay_A/0/1/0/all/0/1\">Ali Girayhan &#xd6;zbay</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hamzehloo_A/0/1/0/all/0/1\">Arash Hamzehloo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Laizet_S/0/1/0/all/0/1\">Sylvain Laizet</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tzirakis_P/0/1/0/all/0/1\">Panagiotis Tzirakis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rizos_G/0/1/0/all/0/1\">Georgios Rizos</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn Schuller</a>",
          "description": "The Poisson equation is commonly encountered in engineering, for instance in\ncomputational fluid dynamics (CFD) where it is needed to compute corrections to\nthe pressure field to ensure the incompressibility of the velocity field. In\nthe present work, we propose a novel fully convolutional neural network (CNN)\narchitecture to infer the solution of the Poisson equation on a 2D Cartesian\ngrid with different resolutions given the right hand side term, arbitrary\nboundary conditions and grid parameters. It provides unprecedented versatility\nfor a CNN approach dealing with partial differential equations. The boundary\nconditions are handled using a novel approach by decomposing the original\nPoisson problem into a homogeneous Poisson problem plus four inhomogeneous\nLaplace sub-problems. The model is trained using a novel loss function\napproximating the continuous $L^p$ norm between the prediction and the target.\nEven when predicting on grids denser than previously encountered, our model\ndemonstrates encouraging capacity to reproduce the correct solution profile.\nThe proposed model, which outperforms well-known neural network models, can be\nincluded in a CFD solver to help with solving the Poisson equation. Analytical\ntest cases indicate that our CNN architecture is capable of predicting the\ncorrect solution of a Poisson problem with mean percentage errors below 10%, an\nimprovement by comparison to the first step of conventional iterative methods.\nPredictions from our model, used as the initial guess to iterative algorithms\nlike Multigrid, can reduce the RMS error after a single iteration by more than\n90% compared to a zero initial guess.",
          "link": "http://arxiv.org/abs/1910.08613",
          "publishedOn": "2021-06-30T02:01:03.780Z",
          "wordCount": 765,
          "title": "Poisson CNN: Convolutional neural networks for the solution of the Poisson equation on a Cartesian mesh. (arXiv:1910.08613v3 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Divyat Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1\">Shruti Tople</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Amit Sharma</a>",
          "description": "In the domain generalization literature, a common objective is to learn\nrepresentations independent of the domain after conditioning on the class\nlabel. We show that this objective is not sufficient: there exist\ncounter-examples where a model fails to generalize to unseen domains even after\nsatisfying class-conditional domain invariance. We formalize this observation\nthrough a structural causal model and show the importance of modeling\nwithin-class variations for generalization. Specifically, classes contain\nobjects that characterize specific causal features, and domains can be\ninterpreted as interventions on these objects that change non-causal features.\nWe highlight an alternative condition: inputs across domains should have the\nsame representation if they are derived from the same object. Based on this\nobjective, we propose matching-based algorithms when base objects are observed\n(e.g., through data augmentation) and approximate the objective when objects\nare not observed (MatchDG). Our simple matching-based algorithms are\ncompetitive to prior work on out-of-domain accuracy for rotated MNIST,\nFashion-MNIST, PACS, and Chest-Xray datasets. Our method MatchDG also recovers\nground-truth object matches: on MNIST and Fashion-MNIST, top-10 matches from\nMatchDG have over 50% overlap with ground-truth matches.",
          "link": "http://arxiv.org/abs/2006.07500",
          "publishedOn": "2021-06-30T02:01:03.760Z",
          "wordCount": 665,
          "title": "Domain Generalization using Causal Matching. (arXiv:2006.07500v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14100",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zheng_H/0/1/0/all/0/1\">Huangjie Zheng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "To measure the difference between two probability distributions, referred to\nas the source and target, respectively, we exploit both the chain rule and\nBayes' theorem to construct conditional transport (CT), which is constituted by\nboth a forward component and a backward one. The forward CT is the expected\ncost of moving a source data point to a target one, with their joint\ndistribution defined by the product of the source probability density function\n(PDF) and a source-dependent conditional distribution, which is related to the\ntarget PDF via Bayes' theorem. The backward CT is defined by reversing the\ndirection. The CT cost can be approximated by replacing the source and target\nPDFs with their discrete empirical distributions supported on mini-batches,\nmaking it amenable to implicit distributions and stochastic gradient\ndescent-based optimization. When applied to train a generative model, CT is\nshown to strike a good balance between mode-covering and mode-seeking behaviors\nand strongly resist mode collapse. On a wide variety of benchmark datasets for\ngenerative modeling, substituting the default statistical distance of an\nexisting generative adversarial network with CT is shown to consistently\nimprove the performance. PyTorch-style code is provided.",
          "link": "http://arxiv.org/abs/2012.14100",
          "publishedOn": "2021-06-30T02:01:03.755Z",
          "wordCount": 666,
          "title": "Exploiting Chain Rule and Bayes' Theorem to Compare Probability Distributions. (arXiv:2012.14100v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Sangmin Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungnyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Jongwoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gihun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_S/0/1/0/all/0/1\">Seungjong Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>",
          "description": "This paper proposes a novel contrastive learning framework, coined as\nSelf-Contrastive (SelfCon) Learning, that self-contrasts within multiple\noutputs from the different levels of a network. We confirmed that SelfCon loss\nguarantees the lower bound of mutual information (MI) between the intermediate\nand last representations. Besides, we empirically showed, via various MI\nestimators, that SelfCon loss highly correlates to the increase of MI and\nbetter classification performance. In our experiments, SelfCon surpasses\nsupervised contrastive (SupCon) learning without the need for a multi-viewed\nbatch and with the cheaper computational cost. Especially on ResNet-18, we\nachieved top-1 classification accuracy of 76.45% for the CIFAR-100 dataset,\nwhich is 2.87% and 4.36% higher than SupCon and cross-entropy loss,\nrespectively. We found that mitigating both vanishing gradient and overfitting\nissue makes our method outperform the counterparts.",
          "link": "http://arxiv.org/abs/2106.15499",
          "publishedOn": "2021-06-30T02:01:03.728Z",
          "wordCount": 555,
          "title": "Self-Contrastive Learning. (arXiv:2106.15499v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vardi_G/0/1/0/all/0/1\">Gal Vardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichman_D/0/1/0/all/0/1\">Daniel Reichman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitassi_T/0/1/0/all/0/1\">Toniann Pitassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>",
          "description": "When studying the expressive power of neural networks, a main challenge is to\nunderstand how the size and depth of the network affect its ability to\napproximate real functions. However, not all functions are interesting from a\npractical viewpoint: functions of interest usually have a polynomially-bounded\nLipschitz constant, and can be computed efficiently. We call functions that\nsatisfy these conditions \"benign\", and explore the benefits of size and depth\nfor approximation of benign functions with ReLU networks. As we show, this\nproblem is more challenging than the corresponding problem for non-benign\nfunctions. We give barriers to showing depth-lower-bounds: Proving existence of\na benign function that cannot be approximated by polynomial-size networks of\ndepth $4$ would settle longstanding open problems in computational complexity.\nIt implies that beyond depth $4$ there is a barrier to showing depth-separation\nfor benign functions, even between networks of constant depth and networks of\nnonconstant depth. We also study size-separation, namely, whether there are\nbenign functions that can be approximated with networks of size $O(s(d))$, but\nnot with networks of size $O(s'(d))$. We show a complexity-theoretic barrier to\nproving such results beyond size $O(d\\log^2(d))$, but also show an explicit\nbenign function, that can be approximated with networks of size $O(d)$ and not\nwith networks of size $o(d/\\log d)$. For approximation in $L_\\infty$ we achieve\nsuch separation already between size $O(d)$ and size $o(d)$. Moreover, we show\nsuperpolynomial size lower bounds and barriers to such lower bounds, depending\non the assumptions on the function. Our size-separation results rely on an\nanalysis of size lower bounds for Boolean functions, which is of independent\ninterest: We show linear size lower bounds for computing explicit Boolean\nfunctions with neural networks and threshold circuits.",
          "link": "http://arxiv.org/abs/2102.00314",
          "publishedOn": "2021-06-30T02:01:03.723Z",
          "wordCount": 780,
          "title": "Size and Depth Separation in Approximating Benign Functions with Neural Networks. (arXiv:2102.00314v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yufei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>",
          "description": "Despite of the pervasive existence of multi-label evasion attack, it is an\nopen yet essential problem to characterize the origin of the adversarial\nvulnerability of a multi-label learning system and assess its attackability. In\nthis study, we focus on non-targeted evasion attack against multi-label\nclassifiers. The goal of the threat is to cause miss-classification with\nrespect to as many labels as possible, with the same input perturbation. Our\nwork gains in-depth understanding about the multi-label adversarial attack by\nfirst characterizing the transferability of the attack based on the functional\nproperties of the multi-label classifier. We unveil how the transferability\nlevel of the attack determines the attackability of the classifier via\nestablishing an information-theoretic analysis of the adversarial risk.\nFurthermore, we propose a transferability-centered attackability assessment,\nnamed Soft Attackability Estimator (SAE), to evaluate the intrinsic\nvulnerability level of the targeted multi-label classifier. This estimator is\nthen integrated as a transferability-tuning regularization term into the\nmulti-label learning paradigm to achieve adversarially robust classification.\nThe experimental study on real-world data echos the theoretical analysis and\nverify the validity of the transferability-regularized multi-label learning\nmethod.",
          "link": "http://arxiv.org/abs/2106.15360",
          "publishedOn": "2021-06-30T02:01:03.717Z",
          "wordCount": 618,
          "title": "Attack Transferability Characterization for Adversarially Robust Multi-label Classification. (arXiv:2106.15360v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15432",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1\">Yuxuan Du</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Quantum auto-encoder (QAE) is a powerful tool to relieve the curse of\ndimensionality encountered in quantum physics, celebrated by the ability to\nextract low-dimensional patterns from quantum states living in the\nhigh-dimensional space. Despite its attractive properties, little is known\nabout the practical applications of QAE with provable advantages. To address\nthese issues, here we prove that QAE can be used to efficiently calculate the\neigenvalues and prepare the corresponding eigenvectors of a high-dimensional\nquantum state with the low-rank property. With this regard, we devise three\neffective QAE-based learning protocols to solve the low-rank state fidelity\nestimation, the quantum Gibbs state preparation, and the quantum metrology\ntasks, respectively. Notably, all of these protocols are scalable and can be\nreadily executed on near-term quantum machines. Moreover, we prove that the\nerror bounds of the proposed QAE-based methods outperform those in previous\nliterature. Numerical simulations collaborate with our theoretical analysis.\nOur work opens a new avenue of utilizing QAE to tackle various quantum physics\nand quantum information processing problems in a scalable way.",
          "link": "http://arxiv.org/abs/2106.15432",
          "publishedOn": "2021-06-30T02:01:03.703Z",
          "wordCount": 601,
          "title": "On exploring practical potentials of quantum auto-encoder with advantages. (arXiv:2106.15432v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fujiwara_T/0/1/0/all/0/1\">Takanori Fujiwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xinhai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kwan-Liu Ma</a>",
          "description": "Finding the similarities and differences between two or more groups of\ndatasets is a fundamental analysis task. For high-dimensional data,\ndimensionality reduction (DR) methods are often used to find the\ncharacteristics of each group. However, existing DR methods provide limited\ncapability and flexibility for such comparative analysis as each method is\ndesigned only for a narrow analysis target, such as identifying factors that\nmost differentiate groups. In this work, we introduce an interactive DR\nframework where we integrate our new DR method, called ULCA (unified linear\ncomparative analysis), with an interactive visual interface. ULCA unifies two\nDR schemes, discriminant analysis and contrastive learning, to support various\ncomparative analysis tasks. To provide flexibility for comparative analysis, we\ndevelop an optimization algorithm that enables analysts to interactively refine\nULCA results. Additionally, we provide an interactive visualization interface\nto examine ULCA results with a rich set of analysis libraries. We evaluate ULCA\nand the optimization algorithm to show their efficiency as well as present\nmultiple case studies using real-world datasets to demonstrate the usefulness\nof our framework.",
          "link": "http://arxiv.org/abs/2106.15481",
          "publishedOn": "2021-06-30T02:01:03.698Z",
          "wordCount": 616,
          "title": "Interactive Dimensionality Reduction for Comparative Analysis. (arXiv:2106.15481v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00153",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lan_G/0/1/0/all/0/1\">Guanghui Lan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Romeijn_E/0/1/0/all/0/1\">Edwin Romeijn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiqiang Zhou</a>",
          "description": "Conditional gradient methods have attracted much attention in both machine\nlearning and optimization communities recently. These simple methods can\nguarantee the generation of sparse solutions. In addition, without the\ncomputation of full gradients, they can handle huge-scale problems sometimes\neven with an exponentially increasing number of decision variables. This paper\naims to significantly expand the application areas of these methods by\npresenting new conditional gradient methods for solving convex optimization\nproblems with general affine and nonlinear constraints. More specifically, we\nfirst present a new constraint extrapolated condition gradient (CoexCG) method\nthat can achieve an ${\\cal O}(1/\\epsilon^2)$ iteration complexity for both\nsmooth and structured nonsmooth function constrained convex optimization. We\nfurther develop novel variants of CoexCG, namely constraint extrapolated and\ndual regularized conditional gradient (CoexDurCG) methods, that can achieve\nsimilar iteration complexity to CoexCG but allow adaptive selection for\nalgorithmic parameters. We illustrate the effectiveness of these methods for\nsolving an important class of radiation therapy treatment planning problems\narising from healthcare industry. To the best of our knowledge, all the\nalgorithmic schemes and their complexity results are new in the area of\nprojection-free methods.",
          "link": "http://arxiv.org/abs/2007.00153",
          "publishedOn": "2021-06-30T02:01:03.693Z",
          "wordCount": 652,
          "title": "Conditional Gradient Methods for Convex Optimization with General Affine and Nonlinear Constraints. (arXiv:2007.00153v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Siddharth Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "Finding anomalous snapshots from a graph has garnered huge attention\nrecently. Existing studies address the problem using shallow learning\nmechanisms such as subspace selection, ego-network, or community analysis.\nThese models do not take into account the multifaceted interactions between the\nstructure and attributes in the network. In this paper, we propose GraphAnoGAN,\nan anomalous snapshot ranking framework, which consists of two core components\n-- generative and discriminative models. Specifically, the generative model\nlearns to approximate the distribution of anomalous samples from the candidate\nset of graph snapshots, and the discriminative model detects whether the\nsampled snapshot is from the ground-truth or not. Experiments on 4 real-world\nnetworks show that GraphAnoGAN outperforms 6 baselines with a significant\nmargin (28.29% and 22.01% higher precision and recall, respectively compared to\nthe best baseline, averaged across all datasets).",
          "link": "http://arxiv.org/abs/2106.15504",
          "publishedOn": "2021-06-30T02:01:03.688Z",
          "wordCount": 576,
          "title": "GraphAnoGAN: Detecting Anomalous Snapshots from Attributed Graphs. (arXiv:2106.15504v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sikchi_H/0/1/0/all/0/1\">Harshit Sikchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>",
          "description": "Reinforcement learning (RL) in low-data and risk-sensitive domains requires\nperformant and flexible deployment policies that can readily incorporate\nconstraints during deployment. One such class of policies are the\nsemi-parametric H-step lookahead policies, which select actions using\ntrajectory optimization over a dynamics model for a fixed horizon with a\nterminal value function. In this work, we investigate a novel instantiation of\nH-step lookahead with a learned model and a terminal value function learned by\na model-free off-policy algorithm, named Learning Off-Policy with Online\nPlanning (LOOP). We provide a theoretical analysis of this method, suggesting a\ntradeoff between model errors and value function errors and empirically\ndemonstrate this tradeoff to be beneficial in deep reinforcement learning.\nFurthermore, we identify the \"Actor Divergence\" issue in this framework and\npropose Actor Regularized Control (ARC), a modified trajectory optimization\nprocedure. We evaluate our method on a set of robotic tasks for Offline and\nOnline RL and demonstrate improved performance. We also show the flexibility of\nLOOP to incorporate safety constraints during deployment with a set of\nnavigation environments. We demonstrate that LOOP is a desirable framework for\nrobotics applications based on its strong performance in various important RL\nsettings.",
          "link": "http://arxiv.org/abs/2008.10066",
          "publishedOn": "2021-06-30T02:01:03.684Z",
          "wordCount": 671,
          "title": "Learning Off-Policy with Online Planning. (arXiv:2008.10066v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chaochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doss_R/0/1/0/all/0/1\">Robin Ram Mohan Doss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiangshan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_K/0/1/0/all/0/1\">Keshav Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Longxiang Gao</a>",
          "description": "With the development of blockchain technologies, the number of smart\ncontracts deployed on blockchain platforms is growing exponentially, which\nmakes it difficult for users to find desired services by manual screening. The\nautomatic classification of smart contracts can provide blockchain users with\nkeyword-based contract searching and helps to manage smart contracts\neffectively. Current research on smart contract classification focuses on\nNatural Language Processing (NLP) solutions which are based on contract source\ncode. However, more than 94% of smart contracts are not open-source, so the\napplication scenarios of NLP methods are very limited. Meanwhile, NLP models\nare vulnerable to adversarial attacks. This paper proposes a classification\nmodel based on features from contract bytecode instead of source code to solve\nthese problems. We also use feature selection and ensemble learning to optimize\nthe model. Our experimental studies on over 3,300 real-world Ethereum smart\ncontracts show that our model can classify smart contracts without source code\nand has better performance than baseline models. Our model also has good\nresistance to adversarial attacks compared with NLP-based models. In addition,\nour analysis reveals that account features used in many smart contract\nclassification models have little effect on classification and can be excluded.",
          "link": "http://arxiv.org/abs/2106.15497",
          "publishedOn": "2021-06-30T02:01:03.678Z",
          "wordCount": 645,
          "title": "A Bytecode-based Approach for Smart Contract Classification. (arXiv:2106.15497v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1\">Alexander N. Gorban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grechuk_B/0/1/0/all/0/1\">Bogdan Grechuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirkes_E/0/1/0/all/0/1\">Evgeny M. Mirkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stasenko_S/0/1/0/all/0/1\">Sergey V. Stasenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1\">Ivan Y. Tyukin</a>",
          "description": "This work is driven by a practical question, corrections of Artificial\nIntelligence (AI) errors. Systematic re-training of a large AI system is hardly\npossible. To solve this problem, special external devices, correctors, are\ndeveloped. They should provide quick and non-iterative system fix without\nmodification of a legacy AI system. A common universal part of the AI corrector\nis a classifier that should separate undesired and erroneous behavior from\nnormal operation. Training of such classifiers is a grand challenge at the\nheart of the one- and few-shot learning methods. Effectiveness of one- and\nfew-short methods is based on either significant dimensionality reductions or\nthe blessing of dimensionality effects. Stochastic separability is a blessing\nof dimensionality phenomenon that allows one-and few-shot error correction: in\nhigh-dimensional datasets under broad assumptions each point can be separated\nfrom the rest of the set by simple and robust linear discriminant. The\nhierarchical structure of data universe is introduced where each data cluster\nhas a granular internal structure, etc. New stochastic separation theorems for\nthe data distributions with fine-grained structure are formulated and proved.\nSeparation theorems in infinite-dimensional limits are proven under assumptions\nof compact embedding of patterns into data space. New multi-correctors of AI\nsystems are presented and illustrated with examples of predicting errors and\nlearning new classes of objects by a deep convolutional neural network.",
          "link": "http://arxiv.org/abs/2106.15416",
          "publishedOn": "2021-06-30T02:01:03.673Z",
          "wordCount": 662,
          "title": "High-dimensional separability for one- and few-shot learning. (arXiv:2106.15416v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhikang T. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masahito Ueda</a>",
          "description": "Despite the empirical success of the deep Q network (DQN) reinforcement\nlearning algorithm and its variants, DQN is still not well understood and it\ndoes not guarantee convergence. In this work, we show that DQN can diverge and\ncease to operate in realistic settings. Although there exist gradient-based\nconvergent methods, we show that they actually have inherent problems in\nlearning behaviour and elucidate why they often fail in practice. To overcome\nthese problems, we propose a convergent DQN algorithm (C-DQN) by carefully\nmodifying DQN, and we show that the algorithm is convergent and can work with\nlarge discount factors (0.9998). It learns robustly in difficult settings and\ncan learn several difficult games in the Atari 2600 benchmark where DQN fail,\nwithin a moderate computational budget. Our codes have been publicly released\nand can be used to reproduce our results.",
          "link": "http://arxiv.org/abs/2106.15419",
          "publishedOn": "2021-06-30T02:01:03.660Z",
          "wordCount": 571,
          "title": "A Convergent and Efficient Deep Q Network Algorithm. (arXiv:2106.15419v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saachi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1\">Adityanarayanan Radhakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1\">Caroline Uhler</a>",
          "description": "Aligned latent spaces, where meaningful semantic shifts in the input space\ncorrespond to a translation in the embedding space, play an important role in\nthe success of downstream tasks such as unsupervised clustering and data\nimputation. In this work, we prove that linear and nonlinear autoencoders\nproduce aligned latent spaces by stretching along the left singular vectors of\nthe data. We fully characterize the amount of stretching in linear autoencoders\nand provide an initialization scheme to arbitrarily stretch along the top\ndirections using these networks. We also quantify the amount of stretching in\nnonlinear autoencoders in a simplified setting. We use our theoretical results\nto align drug signatures across cell types in gene expression space and\nsemantic shifts in word embedding spaces.",
          "link": "http://arxiv.org/abs/2106.15456",
          "publishedOn": "2021-06-30T02:01:03.654Z",
          "wordCount": 556,
          "title": "A Mechanism for Producing Aligned Latent Spaces with Autoencoders. (arXiv:2106.15456v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spathis_D/0/1/0/all/0/1\">Dimitris Spathis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondareva_E/0/1/0/all/0/1\">Erika Bondareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_C/0/1/0/all/0/1\">Chlo&#xeb; Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1\">Jagmohan Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Ting Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grammenos_A/0/1/0/all/0/1\">Andreas Grammenos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasthanasombat_A/0/1/0/all/0/1\">Apinan Hasthanasombat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Floto_A/0/1/0/all/0/1\">Andres Floto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicuta_P/0/1/0/all/0/1\">Pietro Cicuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1\">Cecilia Mascolo</a>",
          "description": "Researchers have been battling with the question of how we can identify\nCoronavirus disease (COVID-19) cases efficiently, affordably and at scale.\nRecent work has shown how audio based approaches, which collect respiratory\naudio data (cough, breathing and voice) can be used for testing, however there\nis a lack of exploration of how biases and methodological decisions impact\nthese tools' performance in practice. In this paper, we explore the realistic\nperformance of audio-based digital testing of COVID-19. To investigate this, we\ncollected a large crowdsourced respiratory audio dataset through a mobile app,\nalongside recent COVID-19 test result and symptoms intended as a ground truth.\nWithin the collected dataset, we selected 5,240 samples from 2,478 participants\nand split them into different participant-independent sets for model\ndevelopment and validation. Among these, we controlled for potential\nconfounding factors (such as demographics and language). The unbiased model\ntakes features extracted from breathing, coughs, and voice signals as\npredictors and yields an AUC-ROC of 0.71 (95\\% CI: 0.65$-$0.77). We further\nexplore different unbalanced distributions to show how biases and participant\nsplits affect performance. Finally, we discuss how the realistic model\npresented could be integrated in clinical practice to realize continuous,\nubiquitous, sustainable and affordable testing at population scale.",
          "link": "http://arxiv.org/abs/2106.15523",
          "publishedOn": "2021-06-30T02:01:03.650Z",
          "wordCount": 706,
          "title": "Sounds of COVID-19: exploring realistic performance of audio-based digital testing. (arXiv:2106.15523v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bogatinovski_J/0/1/0/all/0/1\">Jasmin Bogatinovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todorovski_L/0/1/0/all/0/1\">Ljup&#x10d;o Todorovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dzeroski_S/0/1/0/all/0/1\">Sa&#x161;o D&#x17e;eroski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocev_D/0/1/0/all/0/1\">Dragi Kocev</a>",
          "description": "Meta learning generalizes the empirical experience with different learning\ntasks and holds promise for providing important empirical insight into the\nbehaviour of machine learning algorithms. In this paper, we present a\ncomprehensive meta-learning study of data sets and methods for multi-label\nclassification (MLC). MLC is a practically relevant machine learning task where\neach example is labelled with multiple labels simultaneously. Here, we analyze\n40 MLC data sets by using 50 meta features describing different properties of\nthe data. The main findings of this study are as follows. First, the most\nprominent meta features that describe the space of MLC data sets are the ones\nassessing different aspects of the label space. Second, the meta models show\nthat the most important meta features describe the label space, and, the meta\nfeatures describing the relationships among the labels tend to occur a bit more\noften than the meta features describing the distributions between and within\nthe individual labels. Third, the optimization of the hyperparameters can\nimprove the predictive performance, however, quite often the extent of the\nimprovements does not always justify the resource utilization.",
          "link": "http://arxiv.org/abs/2106.15411",
          "publishedOn": "2021-06-30T02:01:03.636Z",
          "wordCount": 621,
          "title": "Explaining the Performance of Multi-label Classification Methods with Data Set Properties. (arXiv:2106.15411v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_R/0/1/0/all/0/1\">Rongfei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaowen Chu</a>",
          "description": "Federated learning utilizes various resources provided by participants to\ncollaboratively train a global model, which potentially address the data\nprivacy issue of machine learning. In such promising paradigm, the performance\nwill be deteriorated without sufficient training data and other resources in\nthe learning process. Thus, it is quite crucial to inspire more participants to\ncontribute their valuable resources with some payments for federated learning.\nIn this paper, we present a comprehensive survey of incentive schemes for\nfederate learning. Specifically, we identify the incentive problem in federated\nlearning and then provide a taxonomy for various schemes. Subsequently, we\nsummarize the existing incentive mechanisms in terms of the main techniques,\nsuch as Stackelberg game, auction, contract theory, Shapley value,\nreinforcement learning, blockchain. By reviewing and comparing some impressive\nresults, we figure out three directions for the future study.",
          "link": "http://arxiv.org/abs/2106.15406",
          "publishedOn": "2021-06-30T02:01:03.631Z",
          "wordCount": 582,
          "title": "A Comprehensive Survey of Incentive Mechanism for Federated Learning. (arXiv:2106.15406v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15348",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Razghandi_M/0/1/0/all/0/1\">Mina Razghandi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Erol_Kantarci_M/0/1/0/all/0/1\">Melike Erol-Kantarci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turgut_D/0/1/0/all/0/1\">Damla Turgut</a>",
          "description": "Appliance-level load forecasting plays a critical role in residential energy\nmanagement, besides having significant importance for ancillary services\nperformed by the utilities. In this paper, we propose to use an LSTM-based\nsequence-to-sequence (seq2seq) learning model that can capture the load\nprofiles of appliances. We use a real dataset collected fromfour residential\nbuildings and compare our proposed schemewith three other techniques, namely\nVARMA, Dilated One Dimensional Convolutional Neural Network, and an LSTM\nmodel.The results show that the proposed LSTM-based seq2seq model outperforms\nother techniques in terms of prediction error in most cases.",
          "link": "http://arxiv.org/abs/2106.15348",
          "publishedOn": "2021-06-30T02:01:03.626Z",
          "wordCount": 556,
          "title": "Short-Term Load Forecasting for Smart HomeAppliances with Sequence to Sequence Learning. (arXiv:2106.15348v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chuanbiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>",
          "description": "In the field of adversarial robustness, there is a common practice that\nadopts the single-step adversarial training for quickly developing\nadversarially robust models. However, the single-step adversarial training is\nmost likely to cause catastrophic overfitting, as after a few training epochs\nit will be hard to generate strong adversarial examples to continuously boost\nthe adversarial robustness. In this work, we aim to avoid the catastrophic\noverfitting by introducing multi-step adversarial examples during the\nsingle-step adversarial training. Then, to balance the large training overhead\nof generating multi-step adversarial examples, we propose a Multi-stage\nOptimization based Adversarial Training (MOAT) method that periodically trains\nthe model on mixed benign examples, single-step adversarial examples, and\nmulti-step adversarial examples stage by stage. In this way, the overall\ntraining overhead is reduced significantly, meanwhile, the model could avoid\ncatastrophic overfitting. Extensive experiments on CIFAR-10 and CIFAR-100\ndatasets demonstrate that under similar amount of training overhead, the\nproposed MOAT exhibits better robustness than either single-step or multi-step\nadversarial training methods.",
          "link": "http://arxiv.org/abs/2106.15357",
          "publishedOn": "2021-06-30T02:01:03.621Z",
          "wordCount": 597,
          "title": "Multi-stage Optimization based Adversarial Training. (arXiv:2106.15357v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yang Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_Z/0/1/0/all/0/1\">Zhi Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhangjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>",
          "description": "With the development of deep networks on various large-scale datasets, a\nlarge zoo of pretrained models are available. When transferring from a model\nzoo, applying classic single-model based transfer learning methods to each\nsource model suffers from high computational burden and cannot fully utilize\nthe rich knowledge in the zoo. We propose \\emph{Zoo-Tuning} to address these\nchallenges, which learns to adaptively transfer the parameters of pretrained\nmodels to the target task. With the learnable channel alignment layer and\nadaptive aggregation layer, Zoo-Tuning \\emph{adaptively aggregates channel\naligned pretrained parameters} to derive the target model, which promotes\nknowledge transfer by simultaneously adapting multiple source models to\ndownstream tasks. The adaptive aggregation substantially reduces the\ncomputation cost at both training and inference. We further propose lite\nZoo-Tuning with the temporal ensemble of batch average gating values to reduce\nthe storage cost at the inference time. We evaluate our approach on a variety\nof tasks, including reinforcement learning, image classification, and facial\nlandmark detection. Experiment results demonstrate that the proposed adaptive\ntransfer learning approach can transfer knowledge from a zoo of models more\neffectively and efficiently.",
          "link": "http://arxiv.org/abs/2106.15434",
          "publishedOn": "2021-06-30T02:01:03.616Z",
          "wordCount": 618,
          "title": "Zoo-Tuning: Adaptive Transfer from a Zoo of Models. (arXiv:2106.15434v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Quanxue Gao</a>",
          "description": "Graph-based multi-view clustering has become an active topic due to the\nefficiency in characterizing both the complex structure and relationship\nbetween multimedia data. However, existing methods have the following\nshortcomings: (1) They are inefficient or even fail for graph learning in large\nscale due to the graph construction and eigen-decomposition. (2) They cannot\nwell exploit both the complementary information and spatial structure embedded\nin graphs of different views. To well exploit complementary information and\ntackle the scalability issue plaguing graph-based multi-view clustering, we\npropose an efficient multiple graph learning model via a small number of anchor\npoints and tensor Schatten p-norm minimization. Specifically, we construct a\nhidden and tractable large graph by anchor graph for each view and well exploit\ncomplementary information embedded in anchor graphs of different views by\ntensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm,\nwhich scales linearly with the data size, to solve our proposed model.\nExtensive experimental results on several datasets indicate that our proposed\nmethod outperforms some state-of-the-art multi-view clustering algorithms.",
          "link": "http://arxiv.org/abs/2106.15382",
          "publishedOn": "2021-06-30T02:01:03.611Z",
          "wordCount": 603,
          "title": "Multiple Graph Learning for Scalable Multi-view Clustering. (arXiv:2106.15382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qiuqiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chuanhou Gao</a>",
          "description": "Click-Through Rate prediction aims to predict the ratio of clicks to\nimpressions of a specific link. This is a challenging task since (1) there are\nusually categorical features, and the inputs will be extremely high-dimensional\nif one-hot encoding is applied, (2) not only the original features but also\ntheir interactions are important, (3) an effective prediction may rely on\ndifferent features and interactions in different time periods. To overcome\nthese difficulties, we propose a new interaction detection method, named Online\nRandom Intersection Chains. The method, which is based on the idea of frequent\nitemset mining, detects informative interactions by observing the intersections\nof randomly chosen samples. The discovered interactions enjoy high\ninterpretability as they can be comprehended as logical expressions. ORIC can\nbe updated every time new data is collected, without being retrained on\nhistorical data. What's more, the importance of the historical and latest data\ncan be controlled by a tuning parameter. A framework is designed to deal with\nthe streaming interactions, so almost all existing models for CTR prediction\ncan be applied after interaction detection. Empirical results demonstrate the\nefficiency and effectiveness of ORIC on three benchmark datasets.",
          "link": "http://arxiv.org/abs/2106.15400",
          "publishedOn": "2021-06-30T02:01:03.606Z",
          "wordCount": 625,
          "title": "Online Interaction Detection for Click-Through Rate Prediction. (arXiv:2106.15400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1\">Idan Achituve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsian_A/0/1/0/all/0/1\">Aviv Shamsian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1\">Aviv Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1\">Ethan Fetaya</a>",
          "description": "Federated learning aims to learn a global model that performs well on client\ndevices with limited cross-client communication. Personalized federated\nlearning (PFL) further extends this setup to handle data heterogeneity between\nclients by learning personalized models. A key challenge in this setting is to\nlearn effectively across clients even though each client has unique data that\nis often limited in size. Here we present pFedGP, a solution to PFL that is\nbased on Gaussian processes (GPs) with deep kernel learning. GPs are highly\nexpressive models that work well in the low data regime due to their Bayesian\nnature. However, applying GPs to PFL raises multiple challenges. Mainly, GPs\nperformance depends heavily on access to a good kernel function, and learning a\nkernel requires a large training set. Therefore, we propose learning a shared\nkernel function across all clients, parameterized by a neural network, with a\npersonal GP classifier for each client. We further extend pFedGP to include\ninducing points using two novel methods, the first helps to improve\ngeneralization in the low data regime and the second reduces the computational\ncost. We derive a PAC-Bayes generalization bound on novel clients and\nempirically show that it gives non-vacuous guarantees. Extensive experiments on\nstandard PFL benchmarks with CIFAR-10, CIFAR-100, and CINIC-10, and on a new\nsetup of learning under input noise show that pFedGP achieves well-calibrated\npredictions while significantly outperforming baseline methods, reaching up to\n21% in accuracy gain.",
          "link": "http://arxiv.org/abs/2106.15482",
          "publishedOn": "2021-06-30T02:01:03.602Z",
          "wordCount": 669,
          "title": "Personalized Federated Learning with Gaussian Processes. (arXiv:2106.15482v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_A/0/1/0/all/0/1\">Ankush Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wichern_G/0/1/0/all/0/1\">Gordon Wichern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laughman_C/0/1/0/all/0/1\">Christopher Laughman</a>",
          "description": "Physics-informed dynamical system models form critical components of digital\ntwins of the built environment. These digital twins enable the design of\nenergy-efficient infrastructure, but must be properly calibrated to accurately\nreflect system behavior for downstream prediction and analysis. Dynamical\nsystem models of modern buildings are typically described by a large number of\nparameters and incur significant computational expenditure during simulations.\nTo handle large-scale calibration of digital twins without exorbitant\nsimulations, we propose ANP-BBO: a scalable and parallelizable batch-wise\nBayesian optimization (BBO) methodology that leverages attentive neural\nprocesses (ANPs).",
          "link": "http://arxiv.org/abs/2106.15502",
          "publishedOn": "2021-06-30T02:01:03.583Z",
          "wordCount": 553,
          "title": "Attentive Neural Processes and Batch Bayesian Optimization for Scalable Calibration of Physics-Informed Digital Twins. (arXiv:2106.15502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15427",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nadjahi_K/0/1/0/all/0/1\">Kimia Nadjahi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jacob_P/0/1/0/all/0/1\">Pierre E. Jacob</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Badeau_R/0/1/0/all/0/1\">Roland Badeau</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1\">Umut &#x15e;im&#x15f;ekli</a>",
          "description": "The Sliced-Wasserstein distance (SW) is being increasingly used in machine\nlearning applications as an alternative to the Wasserstein distance and offers\nsignificant computational and statistical benefits. Since it is defined as an\nexpectation over random projections, SW is commonly approximated by Monte\nCarlo. We adopt a new perspective to approximate SW by making use of the\nconcentration of measure phenomenon: under mild assumptions, one-dimensional\nprojections of a high-dimensional random vector are approximately Gaussian.\nBased on this observation, we develop a simple deterministic approximation for\nSW. Our method does not require sampling a number of random projections, and is\ntherefore both accurate and easy to use compared to the usual Monte Carlo\napproximation. We derive nonasymptotical guarantees for our approach, and show\nthat the approximation error goes to zero as the dimension increases, under a\nweak dependence condition on the data distribution. We validate our theoretical\nfindings on synthetic datasets, and illustrate the proposed approximation on a\ngenerative modeling problem.",
          "link": "http://arxiv.org/abs/2106.15427",
          "publishedOn": "2021-06-30T02:01:03.568Z",
          "wordCount": 602,
          "title": "Fast Approximation of the Sliced-Wasserstein Distance Using Concentration of Random Projections. (arXiv:2106.15427v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Ge Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Pulkit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>",
          "description": "Current model-based reinforcement learning methods struggle when operating\nfrom complex visual scenes due to their inability to prioritize task-relevant\nfeatures. To mitigate this problem, we propose learning Task Informed\nAbstractions (TIA) that explicitly separates reward-correlated visual features\nfrom distractors. For learning TIA, we introduce the formalism of Task Informed\nMDP (TiMDP) that is realized by training two models that learn visual features\nvia cooperative reconstruction, but one model is adversarially dissociated from\nthe reward signal. Empirical evaluation shows that TIA leads to significant\nperformance gains over state-of-the-art methods on many visual control tasks\nwhere natural and unconstrained visual distractions pose a formidable\nchallenge.",
          "link": "http://arxiv.org/abs/2106.15612",
          "publishedOn": "2021-06-30T02:01:03.556Z",
          "wordCount": 537,
          "title": "Learning Task Informed Abstraction. (arXiv:2106.15612v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "The population of elderly people has been increasing at a rapid rate over the\nlast few decades and their population is expected to further increase in the\nupcoming future. Their increasing population is associated with their\nincreasing needs due to problems like physical disabilities, cognitive issues,\nweakened memory and disorganized behavior, that elderly people face with\nincreasing age. To reduce their financial burden on the world economy and to\nenhance their quality of life, it is essential to develop technology-based\nsolutions that are adaptive, assistive and intelligent in nature. Intelligent\nAffect Aware Systems that can not only analyze but also predict the behavior of\nelderly people in the context of their day to day interactions with technology\nin an IoT-based environment, holds immense potential for serving as a long-term\nsolution for improving the user experience of elderly in smart homes. This work\ntherefore proposes the framework for an Intelligent Affect Aware environment\nfor elderly people that can not only analyze the affective components of their\ninteractions but also predict their likely user experience even before they\nstart engaging in any activity in the given smart home environment. This\nforecasting of user experience would provide scope for enhancing the same,\nthereby increasing the assistive and adaptive nature of such intelligent\nsystems. To uphold the efficacy of this proposed framework for improving the\nquality of life of elderly people in smart homes, it has been tested on three\ndatasets and the results are presented and discussed.",
          "link": "http://arxiv.org/abs/2106.15599",
          "publishedOn": "2021-06-30T02:01:03.540Z",
          "wordCount": 723,
          "title": "Framework for an Intelligent Affect Aware Smart Home Environment for Elderly People. (arXiv:2106.15599v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15594",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Quinteiro_R/0/1/0/all/0/1\">Ricardo Quinteiro</a>, <a href=\"http://arxiv.org/find/math/1/au:+Melo_F/0/1/0/all/0/1\">Francisco S. Melo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Santos_P/0/1/0/all/0/1\">Pedro A. Santos</a>",
          "description": "This paper addresses the problem of optimal control using search trees. We\nstart by considering multi-armed bandit problems with continuous action spaces\nand propose LD-HOO, a limited depth variant of the hierarchical optimistic\noptimization (HOO) algorithm. We provide a regret analysis for LD-HOO and show\nthat, asymptotically, our algorithm exhibits the same cumulative regret as the\noriginal HOO while being faster and more memory efficient. We then propose a\nMonte Carlo tree search algorithm based on LD-HOO for optimal control problems\nand illustrate the resulting approach's application in several optimal control\nproblems.",
          "link": "http://arxiv.org/abs/2106.15594",
          "publishedOn": "2021-06-30T02:01:03.535Z",
          "wordCount": 535,
          "title": "Limited depth bandit-based strategy for Monte Carlo planning in continuous action spaces. (arXiv:2106.15594v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1909.07755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eberts_M/0/1/0/all/0/1\">Markus Eberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulges_A/0/1/0/all/0/1\">Adrian Ulges</a>",
          "description": "We introduce SpERT, an attention model for span-based joint entity and\nrelation extraction. Our key contribution is a light-weight reasoning on BERT\nembeddings, which features entity recognition and filtering, as well as\nrelation classification with a localized, marker-free context representation.\nThe model is trained using strong within-sentence negative samples, which are\nefficiently extracted in a single BERT pass. These aspects facilitate a search\nover all spans in the sentence.\n\nIn ablation studies, we demonstrate the benefits of pre-training, strong\nnegative sampling and localized context. Our model outperforms prior work by up\nto 2.6% F1 score on several datasets for joint entity and relation extraction.",
          "link": "http://arxiv.org/abs/1909.07755",
          "publishedOn": "2021-06-30T02:01:03.529Z",
          "wordCount": 618,
          "title": "Span-based Joint Entity and Relation Extraction with Transformer Pre-training. (arXiv:1909.07755v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruizhi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1\">Greg Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehrmann_A/0/1/0/all/0/1\">Andreas M. Lehrmann</a>",
          "description": "Partial observations of continuous time-series dynamics at arbitrary time\nstamps exist in many disciplines. Fitting this type of data using statistical\nmodels with continuous dynamics is not only promising at an intuitive level but\nalso has practical benefits, including the ability to generate continuous\ntrajectories and to perform inference on previously unseen time stamps. Despite\nexciting progress in this area, the existing models still face challenges in\nterms of their representational power and the quality of their variational\napproximations. We tackle these challenges with continuous latent process flows\n(CLPF), a principled architecture decoding continuous latent processes into\ncontinuous observable processes using a time-dependent normalizing flow driven\nby a stochastic differential equation. To optimize our model using maximum\nlikelihood, we propose a novel piecewise construction of a variational\nposterior process and derive the corresponding variational lower bound using\ntrajectory re-weighting. Our ablation studies demonstrate the effectiveness of\nour contributions in various inference tasks on irregular time grids.\nComparisons to state-of-the-art baselines show our model's favourable\nperformance on both synthetic and real-world time-series data.",
          "link": "http://arxiv.org/abs/2106.15580",
          "publishedOn": "2021-06-30T02:01:03.523Z",
          "wordCount": 601,
          "title": "Continuous Latent Process Flows. (arXiv:2106.15580v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kosasih_E/0/1/0/all/0/1\">Edward Elson Kosasih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabezas_J/0/1/0/all/0/1\">Joaquin Cabezas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumba_X/0/1/0/all/0/1\">Xavier Sumba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielak_P/0/1/0/all/0/1\">Piotr Bielak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagowski_K/0/1/0/all/0/1\">Kamil Tagowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idanwekhai_K/0/1/0/all/0/1\">Kelvin Idanwekhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjandra_B/0/1/0/all/0/1\">Benedict Aaron Tjandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamasb_A/0/1/0/all/0/1\">Arian Rokkum Jamasb</a>",
          "description": "In order to advance large-scale graph machine learning, the Open Graph\nBenchmark Large Scale Challenge (OGB-LSC) was proposed at the KDD Cup 2021. The\nPCQM4M-LSC dataset defines a molecular HOMO-LUMO property prediction task on\nabout 3.8M graphs. In this short paper, we show our current work-in-progress\nsolution which builds an ensemble of three graph neural networks models based\non GIN, Bayesian Neural Networks and DiffPool. Our approach outperforms the\nprovided baseline by 7.6%. Moreover, using uncertainty in our ensemble's\nprediction, we can identify molecules whose HOMO-LUMO gaps are harder to\npredict (with Pearson's correlation of 0.5181). We anticipate that this will\nfacilitate active learning.",
          "link": "http://arxiv.org/abs/2106.15529",
          "publishedOn": "2021-06-30T02:01:03.518Z",
          "wordCount": 557,
          "title": "On Graph Neural Network Ensembles for Large-Scale Molecular Property Prediction. (arXiv:2106.15529v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15575",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Awate_S/0/1/0/all/0/1\">Suyash Awate</a>",
          "description": "Deep neural networks for image quality enhancement typically need large\nquantities of highly-curated training data comprising pairs of low-quality\nimages and their corresponding high-quality images. While high-quality image\nacquisition is typically expensive and time-consuming, medium-quality images\nare faster to acquire, at lower equipment costs, and available in larger\nquantities. Thus, we propose a novel generative adversarial network (GAN) that\ncan leverage training data at multiple levels of quality (e.g., high and medium\nquality) to improve performance while limiting costs of data curation. We apply\nour mixed-supervision GAN to (i) super-resolve histopathology images and (ii)\nenhance laparoscopy images by combining super-resolution and surgical smoke\nremoval. Results on large clinical and pre-clinical datasets show the benefits\nof our mixed-supervision GAN over the state of the art.",
          "link": "http://arxiv.org/abs/2106.15575",
          "publishedOn": "2021-06-30T02:01:03.503Z",
          "wordCount": 573,
          "title": "A Mixed-Supervision Multilevel GAN Framework for Image Quality Enhancement. (arXiv:2106.15575v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_B/0/1/0/all/0/1\">Bumju Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jeonghee Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byunghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Recently, graph neural networks (GNNs) have achieved remarkable performances\nfor quantum mechanical problems. However, a graph convolution can only cover a\nlocalized region, and cannot capture long-range interactions of atoms. This\nbehavior is contrary to theoretical interatomic potentials, which is a\nfundamental limitation of the spatial based GNNs. In this work, we propose a\nnovel attention-based framework for molecular property prediction tasks. We\nrepresent a molecular conformation as a discrete atomic sequence combined by\natom-atom distance attributes, named Geometry-aware Transformer (GeoT). In\nparticular, we adopt a Transformer architecture, which has been widely used for\nsequential data. Our proposed model trains sequential representations of\nmolecular graphs based on globally constructed attentions, maintaining all\nspatial arrangements of atom pairs. Our method does not suffer from cost\nintensive computations, such as angle calculations. The experimental results on\nseveral public benchmarks and visualization maps verified that keeping the\nlong-range interatomic attributes can significantly improve the model\npredictability.",
          "link": "http://arxiv.org/abs/2106.15516",
          "publishedOn": "2021-06-30T02:01:03.472Z",
          "wordCount": 588,
          "title": "Geometry-aware Transformer for molecular property prediction. (arXiv:2106.15516v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Junwei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1\">Qiaozhu Mei</a>",
          "description": "Despite enormous successful applications of graph neural networks (GNNs)\nrecently, theoretical understandings of their generalization ability,\nespecially for node-level tasks where data are not independent and\nidentically-distributed (IID), have been sparse. The theoretical investigation\nof the generalization performance is beneficial for understanding fundamental\nissues (such as fairness) of GNN models and designing better learning methods.\nIn this paper, we present a novel PAC-Bayesian analysis for GNNs under a\nnon-IID semi-supervised learning setup. Moreover, we analyze the generalization\nperformances on different subgroups of unlabeled nodes, which allows us to\nfurther study an accuracy-(dis)parity-style (un)fairness of GNNs from a\ntheoretical perspective. Under reasonable assumptions, we demonstrate that the\ndistance between a test subgroup and the training set can be a key factor\naffecting the GNN performance on that subgroup, which calls special attention\nto the training node selection for fair learning. Experiments across multiple\nGNN models and datasets support our theoretical results.",
          "link": "http://arxiv.org/abs/2106.15535",
          "publishedOn": "2021-06-30T02:01:03.467Z",
          "wordCount": 582,
          "title": "Subgroup Generalization and Fairness of Graph Neural Networks. (arXiv:2106.15535v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1\">Anthony Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughey_L/0/1/0/all/0/1\">Lacey Hughey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stabach_J/0/1/0/all/0/1\">Jared A. Stabach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan M. Lavista Ferres</a>",
          "description": "Localizing and counting large ungulates -- hoofed mammals like cows and elk\n-- in very high-resolution satellite imagery is an important task for\nsupporting ecological studies. Prior work has shown that this is feasible with\ndeep learning based methods and sub-meter multi-spectral satellite imagery. We\nextend this line of work by proposing a baseline method, CowNet, that\nsimultaneously estimates the number of animals in an image (counts), as well as\npredicts their location at a pixel level (localizes). We also propose an\nmethodology for evaluating such models on counting and localization tasks\nacross large scenes that takes the uncertainty of noisy labels and the\ninformation needed by stakeholders in ecological monitoring tasks into account.\nFinally, we benchmark our baseline method with state of the art vision methods\nfor counting objects in scenes. We specifically test the temporal\ngeneralization of the resulting models over a large landscape in Point Reyes\nSeashore, CA. We find that the LC-FCN model performs the best and achieves an\naverage precision between 0.56 and 0.61 and an average recall between 0.78 and\n0.92 over three held out test scenes.",
          "link": "http://arxiv.org/abs/2106.15448",
          "publishedOn": "2021-06-30T02:01:03.458Z",
          "wordCount": 638,
          "title": "Detecting Cattle and Elk in the Wild from Space. (arXiv:2106.15448v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kivva_B/0/1/0/all/0/1\">Bohdan Kivva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajendran_G/0/1/0/all/0/1\">Goutham Rajendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1\">Bryon Aragam</a>",
          "description": "We study the problem of reconstructing a causal graphical model from data in\nthe presence of latent variables. The main problem of interest is recovering\nthe causal structure over the latent variables while allowing for general,\npotentially nonlinear dependence between the variables. In many practical\nproblems, the dependence between raw observations (e.g. pixels in an image) is\nmuch less relevant than the dependence between certain high-level, latent\nfeatures (e.g. concepts or objects), and this is the setting of interest. We\nprovide conditions under which both the latent representations and the\nunderlying latent causal model are identifiable by a reduction to a mixture\noracle. The proof is constructive, and leads to several algorithms for\nexplicitly reconstructing the full graphical model. We discuss efficient\nalgorithms and provide experiments illustrating the algorithms in practice.",
          "link": "http://arxiv.org/abs/2106.15563",
          "publishedOn": "2021-06-30T02:01:03.449Z",
          "wordCount": 571,
          "title": "Learning latent causal graphs via mixture oracles. (arXiv:2106.15563v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nikitin_N/0/1/0/all/0/1\">Nikolay O. Nikitin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vychuzhanin_P/0/1/0/all/0/1\">Pavel Vychuzhanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarafanov_M/0/1/0/all/0/1\">Mikhail Sarafanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polonskaia_I/0/1/0/all/0/1\">Iana S. Polonskaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revin_I/0/1/0/all/0/1\">Ilia Revin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barabanova_I/0/1/0/all/0/1\">Irina V. Barabanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maximov_G/0/1/0/all/0/1\">Gleb Maximov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1\">Anna V. Kalyuzhnaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhanovsky_A/0/1/0/all/0/1\">Alexander Boukhanovsky</a>",
          "description": "The effectiveness of the machine learning methods for real-world tasks\ndepends on the proper structure of the modeling pipeline. The proposed approach\nis aimed to automate the design of composite machine learning pipelines, which\nis equivalent to computation workflows that consist of models and data\noperations. The approach combines key ideas of both automated machine learning\nand workflow management systems. It designs the pipelines with a customizable\ngraph-based structure, analyzes the obtained results, and reproduces them. The\nevolutionary approach is used for the flexible identification of pipeline\nstructure. The additional algorithms for sensitivity analysis, atomization, and\nhyperparameter tuning are implemented to improve the effectiveness of the\napproach. Also, the software implementation on this approach is presented as an\nopen-source framework. The set of experiments is conducted for the different\ndatasets and tasks (classification, regression, time series forecasting). The\nobtained results confirm the correctness and effectiveness of the proposed\napproach in the comparison with the state-of-the-art competitors and baseline\nsolutions.",
          "link": "http://arxiv.org/abs/2106.15397",
          "publishedOn": "2021-06-30T02:01:03.435Z",
          "wordCount": 609,
          "title": "Automated Evolutionary Approach for the Design of Composite Machine Learning Pipelines. (arXiv:2106.15397v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Symeonidis_C/0/1/0/all/0/1\">C. Symeonidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nousi_P/0/1/0/all/0/1\">P. Nousi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tosidis_P/0/1/0/all/0/1\">P. Tosidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsampazis_K/0/1/0/all/0/1\">K. Tsampazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1\">N. Passalis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1\">A. Tefas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaidis_N/0/1/0/all/0/1\">N. Nikolaidis</a>",
          "description": "The performance of supervised deep learning algorithms depends significantly\non the scale, quality and diversity of the data used for their training.\nCollecting and manually annotating large amount of data can be both\ntime-consuming and costly tasks to perform. In the case of tasks related to\nvisual human-centric perception, the collection and distribution of such data\nmay also face restrictions due to legislation regarding privacy. In addition,\nthe design and testing of complex systems, e.g., robots, which often employ\ndeep learning-based perception models, may face severe difficulties as even\nstate-of-the-art methods trained on real and large-scale datasets cannot always\nperform adequately as they have not adapted to the visual differences between\nthe virtual and the real world data. As an attempt to tackle and mitigate the\neffect of these issues, we present a method that automatically generates\nrealistic synthetic data with annotations for a) person detection, b) face\nrecognition, and c) human pose estimation. The proposed method takes as input\nreal background images and populates them with human figures in various poses.\nInstead of using hand-made 3D human models, we propose the use of models\ngenerated through deep learning methods, further reducing the dataset creation\ncosts, while maintaining a high level of realism. In addition, we provide\nopen-source and easy to use tools that implement the proposed pipeline,\nallowing for generating highly-realistic synthetic datasets for a variety of\ntasks. A benchmarking and evaluation in the corresponding tasks shows that\nsynthetic data can be effectively used as a supplement to real data.",
          "link": "http://arxiv.org/abs/2106.15409",
          "publishedOn": "2021-06-30T02:01:03.430Z",
          "wordCount": 700,
          "title": "Efficient Realistic Data Generation Framework leveraging Deep Learning-based Human Digitization. (arXiv:2106.15409v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Changhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xuan Zeng</a>",
          "description": "Bayesian optimization is a promising methodology for analog circuit\nsynthesis. However, the sequential nature of the Bayesian optimization\nframework significantly limits its ability to fully utilize real-world\ncomputational resources. In this paper, we propose an efficient parallelizable\nBayesian optimization algorithm via Multi-objective ACquisition function\nEnsemble (MACE) to further accelerate the optimization procedure. By sampling\nquery points from the Pareto front of the probability of improvement (PI),\nexpected improvement (EI) and lower confidence bound (LCB), we combine the\nbenefits of state-of-the-art acquisition functions to achieve a delicate\ntradeoff between exploration and exploitation for the unconstrained\noptimization problem. Based on this batch design, we further adjust the\nalgorithm for the constrained optimization problem. By dividing the\noptimization procedure into two stages and first focusing on finding an initial\nfeasible point, we manage to gain more information about the valid region and\ncan better avoid sampling around the infeasible area. After achieving the first\nfeasible point, we favor the feasible region by adopting a specially designed\npenalization term to the acquisition function ensemble. The experimental\nresults quantitatively demonstrate that our proposed algorithm can reduce the\noverall simulation time by up to 74 times compared to differential evolution\n(DE) for the unconstrained optimization problem when the batch size is 15. For\nthe constrained optimization problem, our proposed algorithm can speed up the\noptimization process by up to 15 times compared to the weighted expected\nimprovement based Bayesian optimization (WEIBO) approach, when the batch size\nis 15.",
          "link": "http://arxiv.org/abs/2106.15412",
          "publishedOn": "2021-06-30T02:01:03.412Z",
          "wordCount": 701,
          "title": "An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble. (arXiv:2106.15412v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kao_C/0/1/0/all/0/1\">Chia-Hsiang Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>",
          "description": "Model-agnostic meta-learning (MAML) is one of the most popular and\nwidely-adopted meta-learning algorithms nowadays, which achieves remarkable\nsuccess in various learning problems. Yet, with the unique design of nested\ninner-loop and outer-loop updates which respectively govern the task-specific\nand meta-model-centric learning, the underlying learning objective of MAML\nstill remains implicit and thus impedes a more straightforward understanding of\nit. In this paper, we provide a new perspective to the working mechanism of\nMAML and discover that: MAML is analogous to a meta-learner using a supervised\ncontrastive objective function, where the query features are pulled towards the\nsupport features of the same class and against those of different classes, in\nwhich such contrastiveness is experimentally verified via an analysis based on\nthe cosine similarity. Moreover, our analysis reveals that the vanilla MAML\nalgorithm has an undesirable interference term originating from the random\ninitialization and the cross-task interaction. We therefore propose a simple\nbut effective technique, zeroing trick, to alleviate such interference, where\nthe extensive experiments are then conducted on both miniImagenet and Omniglot\ndatasets to demonstrate the consistent improvement brought by our proposed\ntechnique thus well validating its effectiveness.",
          "link": "http://arxiv.org/abs/2106.15367",
          "publishedOn": "2021-06-30T02:01:03.406Z",
          "wordCount": 621,
          "title": "MAML is a Noisy Contrastive Learner. (arXiv:2106.15367v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miani_M/0/1/0/all/0/1\">Marco Miani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parton_M/0/1/0/all/0/1\">Maurizio Parton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romito_M/0/1/0/all/0/1\">Marco Romito</a>",
          "description": "Having access to an exploring restart distribution (the so-called wide\ncoverage assumption) is critical with policy gradient methods. This is due to\nthe fact that, while the objective function is insensitive to updates in\nunlikely states, the agent may still need improvements in those states in order\nto reach a nearly optimal payoff. For this reason, wide coverage is used in\nsome form when analyzing theoretical properties of practical policy gradient\nmethods. However, this assumption can be unfeasible in certain environments,\nfor instance when learning is online, or when restarts are possible only from a\nfixed initial state. In these cases, classical policy gradient algorithms can\nhave very poor convergence properties and sample efficiency. In this paper, we\ndevelop Curious Explorer, a novel and simple iterative state space exploration\nstrategy that can be used with any starting distribution $\\rho$. Curious\nExplorer starts from $\\rho$, then using intrinsic rewards assigned to the set\nof poorly visited states produces a sequence of policies, each one more\nexploratory than the previous one in an informed way, and finally outputs a\nrestart model $\\mu$ based on the state visitation distribution of the\nexploratory policies. Curious Explorer is provable, in the sense that we\nprovide theoretical upper bounds on how often an optimal policy visits poorly\nvisited states. These bounds can be used to prove PAC convergence and sample\nefficiency results when a PAC optimizer is plugged in Curious Explorer. This\nallows to achieve global convergence and sample efficiency results without any\ncoverage assumption for REINFORCE, and potentially for any other policy\ngradient method ensuring PAC convergence with wide coverage. Finally, we plug\n(the output of) Curious Explorer into REINFORCE and TRPO, and show empirically\nthat it can improve performance in MDPs with challenging exploration.",
          "link": "http://arxiv.org/abs/2106.15503",
          "publishedOn": "2021-06-30T02:01:03.400Z",
          "wordCount": 726,
          "title": "Curious Explorer: a provable exploration strategy in Policy Learning. (arXiv:2106.15503v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Infante_G/0/1/0/all/0/1\">Guillermo Infante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsso_A/0/1/0/all/0/1\">Anders Jonsso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1\">Vicen&#xe7; G&#xf3;mez</a>",
          "description": "In this work we present a novel approach to hierarchical reinforcement\nlearning for linearly-solvable Markov decision processes. Our approach assumes\nthat the state space is partitioned, and the subtasks consist in moving between\nthe partitions. We represent value functions on several levels of abstraction,\nand use the compositionality of subtasks to estimate the optimal values of the\nstates in each partition. The policy is implicitly defined on these optimal\nvalue estimates, rather than being decomposed among the subtasks. As a\nconsequence, our approach can learn the globally optimal policy, and does not\nsuffer from the non-stationarity of high-level decisions. If several partitions\nhave equivalent dynamics, the subtasks of those partitions can be shared. If\nthe set of boundary states is smaller than the entire state space, our approach\ncan have significantly smaller sample complexity than that of a flat learner,\nand we validate this empirically in several experiments.",
          "link": "http://arxiv.org/abs/2106.15380",
          "publishedOn": "2021-06-30T02:01:03.384Z",
          "wordCount": 583,
          "title": "Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes. (arXiv:2106.15380v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hafiz_A/0/1/0/all/0/1\">Abdul Mueed Hafiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Rouf Ul Alam Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parah_S/0/1/0/all/0/1\">Shabir Ahmad Parah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassaballah_M/0/1/0/all/0/1\">M. Hassaballah</a>",
          "description": "3D model generation from single 2D RGB images is a challenging and actively\nresearched computer vision task. Various techniques using conventional network\narchitectures have been proposed for the same. However, the body of research\nwork is limited and there are various issues like using inefficient 3D\nrepresentation formats, weak 3D model generation backbones, inability to\ngenerate dense point clouds, dependence of post-processing for generation of\ndense point clouds, and dependence on silhouettes in RGB images. In this paper,\na novel 2D RGB image to point cloud conversion technique is proposed, which\nimproves the state of art in the field due to its efficient, robust and simple\nmodel by using the concept of parallelization in network architecture. It not\nonly uses the efficient and rich 3D representation of point clouds, but also\nuses a novel and robust point cloud generation backbone in order to address the\nprevalent issues. This involves using a single-encoder multiple-decoder deep\nnetwork architecture wherein each decoder generates certain fixed viewpoints.\nThis is followed by fusing all the viewpoints to generate a dense point cloud.\nVarious experiments are conducted on the technique and its performance is\ncompared with those of other state of the art techniques and impressive gains\nin performance are demonstrated. Code is available at\nhttps://github.com/mueedhafiz1982/",
          "link": "http://arxiv.org/abs/2106.15325",
          "publishedOn": "2021-06-30T02:01:03.377Z",
          "wordCount": 672,
          "title": "SE-MD: A Single-encoder multiple-decoder deep network for point cloud generation from 2D images. (arXiv:2106.15325v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Akshay Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yerramilli_S/0/1/0/all/0/1\">Suraj Yerramilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apley_D/0/1/0/all/0/1\">Daniel Apley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Ping Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>",
          "description": "Scientific and engineering problems often require the use of artificial\nintelligence to aid understanding and the search for promising designs. While\nGaussian processes (GP) stand out as easy-to-use and interpretable learners,\nthey have difficulties in accommodating big datasets, categorical inputs, and\nmultiple responses, which has become a common challenge for a growing number of\ndata-driven design applications. In this paper, we propose a GP model that\nutilizes latent variables and functions obtained through variational inference\nto address the aforementioned challenges simultaneously. The method is built\nupon the latent variable Gaussian process (LVGP) model where categorical\nfactors are mapped into a continuous latent space to enable GP modeling of\nmixed-variable datasets. By extending variational inference to LVGP models, the\nlarge training dataset is replaced by a small set of inducing points to address\nthe scalability issue. Output response vectors are represented by a linear\ncombination of independent latent functions, forming a flexible kernel\nstructure to handle multiple responses that might have distinct behaviors.\nComparative studies demonstrate that the proposed method scales well for large\ndatasets with over 10^4 data points, while outperforming state-of-the-art\nmachine learning methods without requiring much hyperparameter tuning. In\naddition, an interpretable latent space is obtained to draw insights into the\neffect of categorical factors, such as those associated with building blocks of\narchitectures and element choices in metamaterial and materials design. Our\napproach is demonstrated for machine learning of ternary oxide materials and\ntopology optimization of a multiscale compliant mechanism with aperiodic\nmicrostructures and multiple materials.",
          "link": "http://arxiv.org/abs/2106.15356",
          "publishedOn": "2021-06-30T02:01:03.372Z",
          "wordCount": 703,
          "title": "Scalable Gaussian Processes for Data-Driven Design using Big Data with Categorical Factors. (arXiv:2106.15356v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maniar_T/0/1/0/all/0/1\">Tabish Maniar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akkinepally_A/0/1/0/all/0/1\">Alekhya Akkinepally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anantha Sharma</a>",
          "description": "The use of machine learning algorithms to model user behavior and drive\nbusiness decisions has become increasingly commonplace, specifically providing\nintelligent recommendations to automated decision making. This has led to an\nincrease in the use of customers personal data to analyze customer behavior and\npredict their interests in a companys products. Increased use of this customer\npersonal data can lead to better models but also to the potential of customer\ndata being leaked, reverse engineered, and mishandled. In this paper, we assess\ndifferential privacy as a solution to address these privacy problems by\nbuilding privacy protections into the data engineering and model training\nstages of predictive model development. Our interest is a pragmatic\nimplementation in an operational environment, which necessitates a general\npurpose differentially private modeling framework, and we evaluate one such\ntool from LeapYear as applied to the Credit Risk modeling domain. Credit Risk\nModel is a major modeling methodology in banking and finance where user data is\nanalyzed to determine the total Expected Loss to the bank. We examine the\napplication of differential privacy on the credit risk model and evaluate the\nperformance of a Differentially Private Model with a Non Differentially Private\nModel. Credit Risk Model is a major modeling methodology in banking and finance\nwhere users data is analyzed to determine the total Expected Loss to the bank.\nIn this paper, we explore the application of differential privacy on the credit\nrisk model and evaluate the performance of a Non Differentially Private Model\nwith Differentially Private Model.",
          "link": "http://arxiv.org/abs/2106.15343",
          "publishedOn": "2021-06-30T02:01:03.366Z",
          "wordCount": 697,
          "title": "Differential Privacy for Credit Risk Model. (arXiv:2106.15343v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zehni_M/0/1/0/all/0/1\">Mona Zehni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shaona Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_K/0/1/0/all/0/1\">Krishna Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Sethu Raman</a>",
          "description": "Inverse rendering is the problem of decomposing an image into its intrinsic\ncomponents, i.e. albedo, normal and lighting. To solve this ill-posed problem\nfrom single image, state-of-the-art methods in shape from shading mostly resort\nto supervised training on all the components on either synthetic or real\ndatasets. Here, we propose a new self-supervised training paradigm that 1)\nreduces the need for full supervision on the decomposition task and 2) takes\ninto account the relighting task. We introduce new self-supervised loss terms\nthat leverage the consistencies between multi-lit images (images of the same\nscene under different illuminations). Our approach is applicable to multi-lit\ndatasets. We apply our training approach in two settings: 1) train on a mixture\nof synthetic and real data, 2) train on real datasets with limited supervision.\nWe show-case the effectiveness of our training paradigm on both intrinsic\ndecomposition and relighting and demonstrate how the model struggles in both\ntasks without the self-supervised loss terms in limited supervision settings.\nWe provide results of comprehensive experiments on SfSNet, CelebA and Photoface\ndatasets and verify the performance of our approach on images in the wild.",
          "link": "http://arxiv.org/abs/2106.15305",
          "publishedOn": "2021-06-30T02:01:03.361Z",
          "wordCount": 625,
          "title": "Joint Learning of Portrait Intrinsic Decomposition and Relighting. (arXiv:2106.15305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maniatis_P/0/1/0/all/0/1\">Petros Maniatis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rishabh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Max Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>",
          "description": "Spreadsheet formula prediction has been an important program synthesis\nproblem with many real-world applications. Previous works typically utilize\ninput-output examples as the specification for spreadsheet formula synthesis,\nwhere each input-output pair simulates a separate row in the spreadsheet.\nHowever, this formulation does not fully capture the rich context in real-world\nspreadsheets. First, spreadsheet data entries are organized as tables, thus\nrows and columns are not necessarily independent from each other. In addition,\nmany spreadsheet tables include headers, which provide high-level descriptions\nof the cell data. However, previous synthesis approaches do not consider\nheaders as part of the specification. In this work, we present the first\napproach for synthesizing spreadsheet formulas from tabular context, which\nincludes both headers and semi-structured tabular data. In particular, we\npropose SpreadsheetCoder, a BERT-based model architecture to represent the\ntabular context in both row-based and column-based formats. We train our model\non a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder\nachieves top-1 prediction accuracy of 42.51%, which is a considerable\nimprovement over baselines that do not employ rich tabular context. Compared to\nthe rule-based system, SpreadsheetCoder assists 82% more users in composing\nformulas on Google Sheets.",
          "link": "http://arxiv.org/abs/2106.15339",
          "publishedOn": "2021-06-30T02:01:03.346Z",
          "wordCount": 636,
          "title": "SpreadsheetCoder: Formula Prediction from Semi-structured Context. (arXiv:2106.15339v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15365",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Graafland_C/0/1/0/all/0/1\">Catharina Elisabeth Graafland</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gutierrez_J/0/1/0/all/0/1\">Jos&#xe9; Manuel Guti&#xe9;rrez</a>",
          "description": "Gene expression datasets consist of thousand of genes with relatively small\nsamplesizes (i.e. are large-$p$-small-$n$). Moreover, dependencies of various\norders co-exist in the datasets. In the Undirected probabilistic Graphical\nModel (UGM) framework the Glasso algorithm has been proposed to deal with high\ndimensional micro-array datasets forcing sparsity. Also, modifications of the\ndefault Glasso algorithm are developed to overcome the problem of complex\ninteraction structure. In this work we advocate the use of a simple score-based\nHill Climbing algorithm (HC) that learns Gaussian Bayesian Networks (BNs)\nleaning on Directed Acyclic Graphs (DAGs). We compare HC with Glasso and its\nmodifications in the UGM framework on their capability to reconstruct GRNs from\nmicro-array data belonging to the Escherichia Coli genome. We benefit from the\nanalytical properties of the Joint Probability Density (JPD) function on which\nboth directed and undirected PGMs build to convert DAGs to UGMs.\n\nWe conclude that dependencies in complex data are learned best by the HC\nalgorithm, presenting them most accurately and efficiently, simultaneously\nmodelling strong local and weaker but significant global connections coexisting\nin the gene expression dataset. The HC algorithm adapts intrinsically to the\ncomplex dependency structure of the dataset, without forcing a specific\nstructure in advance. On the contrary, Glasso and modifications model\nunnecessary dependencies at the expense of the probabilistic information in the\nnetwork and of a structural bias in the JPD function that can only be relieved\nincluding many parameters.",
          "link": "http://arxiv.org/abs/2106.15365",
          "publishedOn": "2021-06-30T02:01:03.340Z",
          "wordCount": 696,
          "title": "Learning complex dependency structure of gene regulatory networks from high dimensional micro-array data with Gaussian Bayesian networks. (arXiv:2106.15365v1 [q-bio.MN])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1\">Caglar Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>",
          "description": "Approaches based on refinement operators have been successfully applied to\nclass expression learning on RDF knowledge graphs. These approaches often need\nto explore a large number of concepts to find adequate hypotheses. This need\narguably stems from current approaches relying on myopic heuristic functions to\nguide their search through an infinite concept space. In turn, deep\nreinforcement learning provides effective means to address myopia by estimating\nhow much discounted cumulated future reward states promise. In this work, we\nleverage deep reinforcement learning to accelerate the learning of concepts in\n$\\mathcal{ALC}$ by proposing DRILL -- a novel class expression learning\napproach that uses a convolutional deep Q-learning model to steer its search.\nBy virtue of its architecture, DRILL is able to compute the expected discounted\ncumulated future reward of more than $10^3$ class expressions in a second on\nstandard hardware. We evaluate DRILL on four benchmark datasets against\nstate-of-the-art approaches. Our results suggest that DRILL converges to goal\nstates at least 2.7$\\times$ faster than state-of-the-art models on all\nbenchmark datasets. We provide an open-source implementation of our approach,\nincluding training and evaluation scripts as well as pre-trained models.",
          "link": "http://arxiv.org/abs/2106.15373",
          "publishedOn": "2021-06-30T02:01:03.334Z",
          "wordCount": 626,
          "title": "DRILL-- Deep Reinforcement Learning for Refinement Operators in $\\mathcal{ALC}$. (arXiv:2106.15373v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Sayan Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanawal_M/0/1/0/all/0/1\">Manjesh K. Hanawal</a>",
          "description": "Critical role of Internet of Things (IoT) in various domains like smart city,\nhealthcare, supply chain and transportation has made them the target of\nmalicious attacks. Past works in this area focused on centralized Intrusion\nDetection System (IDS), assuming the existence of a central entity to perform\ndata analysis and identify threats. However, such IDS may not always be\nfeasible, mainly due to spread of data across multiple sources and gathering at\ncentral node can be costly. Also, the earlier works primarily focused on\nimproving True Positive Rate (TPR) and ignored the False Positive Rate (FPR),\nwhich is also essential to avoid unnecessary downtime of the systems. In this\npaper, we first present an architecture for IDS based on hybrid ensemble model,\nnamed PHEC, which gives improved performance compared to state-of-the-art\narchitectures. We then adapt this model to a federated learning framework that\nperforms local training and aggregates only the model parameters. Next, we\npropose Noise-Tolerant PHEC in centralized and federated settings to address\nthe label-noise problem. The proposed idea uses classifiers using weighted\nconvex surrogate loss functions. Natural robustness of KNN classifier towards\nnoisy data is also used in the proposed architecture. Experimental results on\nfour benchmark datasets drawn from various security attacks show that our model\nachieves high TPR while keeping FPR low on noisy and clean data. Further, they\nalso demonstrate that the hybrid ensemble models achieve performance in\nfederated settings close to that of the centralized settings.",
          "link": "http://arxiv.org/abs/2106.15349",
          "publishedOn": "2021-06-30T02:01:03.329Z",
          "wordCount": 691,
          "title": "Federated Learning for Intrusion Detection in IoT Security: A Hybrid Ensemble Approach. (arXiv:2106.15349v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuanqi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1\">Quan Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>",
          "description": "Pseudo-normality synthesis, which computationally generates a pseudo-normal\nimage from an abnormal one (e.g., with lesions), is critical in many\nperspectives, from lesion detection, data augmentation to clinical surgery\nsuggestion. However, it is challenging to generate high-quality pseudo-normal\nimages in the absence of the lesion information. Thus, expensive lesion\nsegmentation data have been introduced to provide lesion information for the\ngenerative models and improve the quality of the synthetic images. In this\npaper, we aim to alleviate the need of a large amount of lesion segmentation\ndata when generating pseudo-normal images. We propose a Semi-supervised Medical\nImage generative LEarning network (SMILE) which not only utilizes limited\nmedical images with segmentation masks, but also leverages massive medical\nimages without segmentation masks to generate realistic pseudo-normal images.\nExtensive experiments show that our model outperforms the best state-of-the-art\nmodel by up to 6% for data augmentation task and 3% in generating high-quality\nimages. Moreover, the proposed semi-supervised learning achieves comparable\nmedical image synthesis quality with supervised learning model, using only 50\nof segmentation data.",
          "link": "http://arxiv.org/abs/2106.15345",
          "publishedOn": "2021-06-30T02:01:03.324Z",
          "wordCount": 623,
          "title": "Where is the disease? Semi-supervised pseudo-normality synthesis from an abnormal image. (arXiv:2106.15345v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasata_D/0/1/0/all/0/1\">Daniel Va&#x161;ata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halama_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Halama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedjungova_M/0/1/0/all/0/1\">Magda Friedjungov&#xe1;</a>",
          "description": "Image inpainting is one of the important tasks in computer vision which\nfocuses on the reconstruction of missing regions in an image. The aim of this\npaper is to introduce an image inpainting model based on Wasserstein Generative\nAdversarial Imputation Network. The generator network of the model uses\nbuilding blocks of convolutional layers with different dilation rates, together\nwith skip connections that help the model reproduce fine details of the output.\nThis combination yields a universal imputation model that is able to handle\nvarious scenarios of missingness with sufficient quality. To show this\nexperimentally, the model is simultaneously trained to deal with three\nscenarios given by missing pixels at random, missing various smaller square\nregions, and one missing square placed in the center of the image. It turns out\nthat our model achieves high-quality inpainting results on all scenarios.\nPerformance is evaluated using peak signal-to-noise ratio and structural\nsimilarity index on two real-world benchmark datasets, CelebA faces and Paris\nStreetView. The results of our model are compared to biharmonic imputation and\nto some of the other state-of-the-art image inpainting methods.",
          "link": "http://arxiv.org/abs/2106.15341",
          "publishedOn": "2021-06-30T02:01:03.309Z",
          "wordCount": 634,
          "title": "Image Inpainting Using Wasserstein Generative Adversarial Imputation Network. (arXiv:2106.15341v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1\">Mingen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tholoniat_P/0/1/0/all/0/1\">Pierre Tholoniat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cidon_A/0/1/0/all/0/1\">Asaf Cidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geambasu_R/0/1/0/all/0/1\">Roxana Geambasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecuyer_M/0/1/0/all/0/1\">Mathias L&#xe9;cuyer</a>",
          "description": "Machine learning (ML) models trained on personal data have been shown to leak\ninformation about users. Differential privacy (DP) enables model training with\na guaranteed bound on this leakage. Each new model trained with DP increases\nthe bound on data leakage and can be seen as consuming part of a global privacy\nbudget that should not be exceeded. This budget is a scarce resource that must\nbe carefully managed to maximize the number of successfully trained models.\n\nWe describe PrivateKube, an extension to the popular Kubernetes datacenter\norchestrator that adds privacy as a new type of resource to be managed\nalongside other traditional compute resources, such as CPU, GPU, and memory.\nThe abstractions we design for the privacy resource mirror those defined by\nKubernetes for traditional resources, but there are also major differences. For\nexample, traditional compute resources are replenishable while privacy is not:\na CPU can be regained after a model finishes execution while privacy budget\ncannot. This distinction forces a re-design of the scheduler. We present DPF\n(Dominant Private Block Fairness) -- a variant of the popular Dominant Resource\nFairness (DRF) algorithm -- that is geared toward the non-replenishable privacy\nresource but enjoys similar theoretical properties as DRF.\n\nWe evaluate PrivateKube and DPF on microbenchmarks and an ML workload on\nAmazon Reviews data. Compared to existing baselines, DPF allows training more\nmodels under the same global privacy guarantee. This is especially true for DPF\nover R\\'enyi DP, a highly composable form of DP.",
          "link": "http://arxiv.org/abs/2106.15335",
          "publishedOn": "2021-06-30T02:01:03.304Z",
          "wordCount": 702,
          "title": "Privacy Budget Scheduling. (arXiv:2106.15335v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gabbur_P/0/1/0/all/0/1\">Prasad Gabbur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilkhu_M/0/1/0/all/0/1\">Manjot Bilkhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Movellan_J/0/1/0/all/0/1\">Javier Movellan</a>",
          "description": "We provide a probabilistic interpretation of attention and show that the\nstandard dot-product attention in transformers is a special case of Maximum A\nPosteriori (MAP) inference. The proposed approach suggests the use of\nExpectation Maximization algorithms for online adaptation of key and value\nmodel parameters. This approach is useful for cases in which external agents,\ne.g., annotators, provide inference-time information about the correct values\nof some tokens, e.g, the semantic category of some pixels, and we need for this\nnew information to propagate to other tokens in a principled manner. We\nillustrate the approach on an interactive semantic segmentation task in which\nannotators and models collaborate online to improve annotation efficiency.\nUsing standard benchmarks, we observe that key adaptation boosts model\nperformance ($\\sim10\\%$ mIoU) in the low feedback regime and value propagation\nimproves model responsiveness in the high feedback regime. A PyTorch layer\nimplementation of our probabilistic attention model will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2106.15338",
          "publishedOn": "2021-06-30T02:01:03.298Z",
          "wordCount": 602,
          "title": "Probabilistic Attention for Interactive Segmentation. (arXiv:2106.15338v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15358",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoqiang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1\">Subhroshekhar Ghosh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scarlett_J/0/1/0/all/0/1\">Jonathan Scarlett</a>",
          "description": "Compressive phase retrieval is a popular variant of the standard compressive\nsensing problem, in which the measurements only contain magnitude information.\nIn this paper, motivated by recent advances in deep generative models, we\nprovide recovery guarantees with order-optimal sample complexity bounds for\nphase retrieval with generative priors. We first show that when using i.i.d.\nGaussian measurements and an $L$-Lipschitz continuous generative model with\nbounded $k$-dimensional inputs, roughly $O(k \\log L)$ samples suffice to\nguarantee that the signal is close to any vector that minimizes an\namplitude-based empirical loss function. Attaining this sample complexity with\na practical algorithm remains a difficult challenge, and a popular spectral\ninitialization method has been observed to pose a major bottleneck. To\npartially address this, we further show that roughly $O(k \\log L)$ samples\nensure sufficient closeness between the signal and any {\\em globally optimal}\nsolution to an optimization problem designed for spectral initialization\n(though finding such a solution may still be challenging). We adapt this result\nto sparse phase retrieval, and show that $O(s \\log n)$ samples are sufficient\nfor a similar guarantee when the underlying signal is $s$-sparse and\n$n$-dimensional, matching an information-theoretic lower bound. While our\nguarantees do not directly correspond to a practical algorithm, we propose a\npractical spectral initialization method motivated by our findings, and\nexperimentally observe significant performance gains over various existing\nspectral initialization methods of sparse phase retrieval.",
          "link": "http://arxiv.org/abs/2106.15358",
          "publishedOn": "2021-06-30T02:01:03.291Z",
          "wordCount": 670,
          "title": "Towards Sample-Optimal Compressive Phase Retrieval with Sparse and Generative Priors. (arXiv:2106.15358v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15379",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1\">Benyamin Ghojogh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1\">Mark Crowley</a>",
          "description": "This is a tutorial and survey paper on unification of spectral dimensionality\nreduction methods, kernel learning by Semidefinite Programming (SDP), Maximum\nVariance Unfolding (MVU) or Semidefinite Embedding (SDE), and its variants. We\nfirst explain how the spectral dimensionality reduction methods can be unified\nas kernel Principal Component Analysis (PCA) with different kernels. This\nunification can be interpreted as eigenfunction learning or representation of\nkernel in terms of distance matrix. Then, since the spectral methods are\nunified as kernel PCA, we say let us learn the best kernel for unfolding the\nmanifold of data to its maximum variance. We first briefly introduce kernel\nlearning by SDP for the transduction task. Then, we explain MVU in detail.\nVarious versions of supervised MVU using nearest neighbors graph, by class-wise\nunfolding, by Fisher criterion, and by colored MVU are explained. We also\nexplain out-of-sample extension of MVU using eigenfunctions and kernel mapping.\nFinally, we introduce other variants of MVU including action respecting\nembedding, relaxed MVU, and landmark MVU for big data.",
          "link": "http://arxiv.org/abs/2106.15379",
          "publishedOn": "2021-06-30T02:01:03.281Z",
          "wordCount": 644,
          "title": "Unified Framework for Spectral Dimensionality Reduction, Maximum Variance Unfolding, and Kernel Learning By Semidefinite Programming: Tutorial and Survey. (arXiv:2106.15379v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_W/0/1/0/all/0/1\">Wenbin Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1\">Hannah Danner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>",
          "description": "Social media offer plenty of information to perform market research in order\nto meet the requirements of customers. One way how this research is conducted\nis that a domain expert gathers and categorizes user-generated content into a\ncomplex and fine-grained class structure. In many of such cases, little data\nmeets complex annotations. It is not yet fully understood how this can be\nleveraged successfully for classification. We examine the classification\naccuracy of expert labels when used with a) many fine-grained classes and b)\nfew abstract classes. For scenario b) we compare abstract class labels given by\nthe domain expert as baseline and by automatic hierarchical clustering. We\ncompare this to another baseline where the entire class structure is given by a\ncompletely unsupervised clustering approach. By doing so, this work can serve\nas an example of how complex expert annotations are potentially beneficial and\ncan be utilized in the most optimal way for opinion mining in highly specific\ndomains. By exploring across a range of techniques and experiments, we find\nthat automated class abstraction approaches in particular the unsupervised\napproach performs remarkably well against domain expert baseline on text\nclassification tasks. This has the potential to inspire opinion mining\napplications in order to support market researchers in practice and to inspire\nfine-grained automated content analysis on a large scale.",
          "link": "http://arxiv.org/abs/2106.15498",
          "publishedOn": "2021-06-30T02:01:03.265Z",
          "wordCount": 657,
          "title": "Classification of Consumer Belief Statements From Social Media. (arXiv:2106.15498v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hallyburton_R/0/1/0/all/0/1\">R. Spencer Hallyburton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yupei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajic_M/0/1/0/all/0/1\">Miroslav Pajic</a>",
          "description": "To enable safe and reliable decision-making, autonomous vehicles (AVs) feed\nsensor data to perception algorithms to understand the environment. Sensor\nfusion, and particularly semantic fusion, with multi-frame tracking is becoming\nincreasingly popular for detecting 3D objects. Recently, it was shown that\nLiDAR-based perception built on deep neural networks is vulnerable to LiDAR\nspoofing attacks. Thus, in this work, we perform the first analysis of\ncamera-LiDAR fusion under spoofing attacks and the first security analysis of\nsemantic fusion in any AV context. We find first that fusion is more successful\nthan existing defenses at guarding against naive spoofing. However, we then\ndefine the frustum attack as a new class of attacks on AVs and find that\nsemantic camera-LiDAR fusion exhibits widespread vulnerability to frustum\nattacks with between 70% and 90% success against target models. Importantly,\nthe attacker needs less than 20 random spoof points on average for successful\nattacks - an order of magnitude less than established maximum capability.\nFinally, we are the first to analyze the longitudinal impact of perception\nattacks by showing the impact of multi-frame attacks.",
          "link": "http://arxiv.org/abs/2106.07098",
          "publishedOn": "2021-06-30T02:01:02.870Z",
          "wordCount": 644,
          "title": "Security Analysis of Camera-LiDAR Semantic-Level Fusion Against Black-Box Attacks on Autonomous Vehicles. (arXiv:2106.07098v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14630",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Raskutti_G/0/1/0/all/0/1\">Garvesh Raskutti</a>",
          "description": "Network estimation from multi-variate point process or time series data is a\nproblem of fundamental importance. Prior work has focused on parametric\napproaches that require a known parametric model, which makes estimation\nprocedures less robust to model mis-specification, non-linearities and\nheterogeneities. In this paper, we develop a semi-parametric approach based on\nthe monotone single-index multi-variate autoregressive model (SIMAM) which\naddresses these challenges. We provide theoretical guarantees for dependent\ndata and an alternating projected gradient descent algorithm. Significantly we\ndo not explicitly assume mixing conditions on the process (although we do\nrequire conditions analogous to restricted strong convexity) and we achieve\nrates of the form $O(T^{-\\frac{1}{3}} \\sqrt{s\\log(TM)})$ (optimal in the\nindependent design case) where $s$ is the threshold for the maximum in-degree\nof the network that indicates the sparsity level, $M$ is the number of actors\nand $T$ is the number of time points. In addition, we demonstrate the superior\nperformance both on simulated data and two real data examples where our SIMAM\napproach out-performs state-of-the-art parametric methods both in terms of\nprediction and network estimation.",
          "link": "http://arxiv.org/abs/2106.14630",
          "publishedOn": "2021-06-30T02:01:02.865Z",
          "wordCount": 634,
          "title": "Improved Prediction and Network Estimation Using the Monotone Single Index Multi-variate Autoregressive Model. (arXiv:2106.14630v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05648",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Larsson_J/0/1/0/all/0/1\">Johan Larsson</a>",
          "description": "The lasso is a popular method to induce shrinkage and sparsity in the\nsolution vector (coefficients) of regression problems, particularly when there\nare many predictors relative to the number of observations. Solving the lasso\nin this high-dimensional setting can, however, be computationally demanding.\nFortunately, this demand can be alleviated via the use of screening rules that\ndiscard predictors prior to fitting the model, leading to a reduced problem to\nbe solved. In this paper, we present a new screening strategy: look-ahead\nscreening. Our method uses safe screening rules to find a range of penalty\nvalues for which a given predictor cannot enter the model, thereby screening\npredictors along the remainder of the path. In experiments we show that these\nlook-ahead screening rules outperform the active warm-start version of the Gap\nSafe rules.",
          "link": "http://arxiv.org/abs/2105.05648",
          "publishedOn": "2021-06-30T02:01:02.858Z",
          "wordCount": 592,
          "title": "Look-Ahead Screening Rules for the Lasso. (arXiv:2105.05648v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06251",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Akiyama_S/0/1/0/all/0/1\">Shunta Akiyama</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1\">Taiji Suzuki</a>",
          "description": "Deep learning empirically achieves high performance in many applications, but\nits training dynamics has not been fully understood theoretically. In this\npaper, we explore theoretical analysis on training two-layer ReLU neural\nnetworks in a teacher-student regression model, in which a student network\nlearns an unknown teacher network through its outputs. We show that with a\nspecific regularization and sufficient over-parameterization, the student\nnetwork can identify the parameters of the teacher network with high\nprobability via gradient descent with a norm dependent stepsize even though the\nobjective function is highly non-convex. The key theoretical tool is the\nmeasure representation of the neural networks and a novel application of a dual\ncertificate argument for sparse estimation on a measure space. We analyze the\nglobal minima and global convergence property in the measure space.",
          "link": "http://arxiv.org/abs/2106.06251",
          "publishedOn": "2021-06-30T02:01:02.808Z",
          "wordCount": 593,
          "title": "On Learnability via Gradient Method for Two-Layer ReLU Neural Networks in Teacher-Student Setting. (arXiv:2106.06251v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.13100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zombori_Z/0/1/0/all/0/1\">Zsolt Zombori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csiszarik_A/0/1/0/all/0/1\">Adri&#xe1;n Csisz&#xe1;rik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaliszyk_C/0/1/0/all/0/1\">Cezary Kaliszyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1\">Josef Urban</a>",
          "description": "We present a reinforcement learning (RL) based guidance system for automated\ntheorem proving geared towards Finding Longer Proofs (FLoP). Unlike most\nlearning based approaches, we focus on generalising from very little training\ndata and achieving near complete confidence. We use several simple, structured\ndatasets with very long proofs to show that FLoP can successfully generalise a\nsingle training proof to a large class of related problems. On these\nbenchmarks, FLoP is competitive with strong theorem provers despite using very\nlimited search, due to its ability to solve problems that are prohibitively\nlong for other systems.",
          "link": "http://arxiv.org/abs/1905.13100",
          "publishedOn": "2021-06-30T02:01:02.717Z",
          "wordCount": 569,
          "title": "Towards Finding Longer Proofs. (arXiv:1905.13100v2 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qin Yang</a>",
          "description": "Distributed artificial intelligence (DAI) studies artificial intelligence\nentities working together to reason, plan, solve problems, organize behaviors\nand strategies, make collective decisions and learn. This Ph.D. research\nproposes a principled Multi-Agent Systems (MAS) cooperation framework,\nSelf-Adaptive Swarm System (SASS), to bridge the fourth level automation gap\nbetween perception, communication, planning, execution, decision-making, and\nlearning.",
          "link": "http://arxiv.org/abs/2106.04679",
          "publishedOn": "2021-06-30T02:01:02.712Z",
          "wordCount": 517,
          "title": "Self-Adaptive Swarm System (SASS). (arXiv:2106.04679v2 [cs.MA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhong Guo</a>",
          "description": "The generalization gap in reinforcement learning (RL) has been a significant\nobstacle that prevents the RL agent from learning general skills and adapting\nto varying environments. Increasing the generalization capacity of the RL\nsystems can significantly improve their performance on real-world working\nenvironments. In this work, we propose a novel policy-aware adversarial data\naugmentation method to augment the standard policy learning method with\nautomatically generated trajectory data. Different from the commonly used\nobservation transformation based data augmentations, our proposed method\nadversarially generates new trajectory data based on the policy gradient\nobjective and aims to more effectively increase the RL agent's generalization\nability with the policy-aware data augmentation. Moreover, we further deploy a\nmixup step to integrate the original and generated data to enhance the\ngeneralization capacity while mitigating the over-deviation of the adversarial\ndata. We conduct experiments on a number of RL tasks to investigate the\ngeneralization performance of the proposed method by comparing it with the\nstandard baselines and the state-of-the-art mixreg approach. The results show\nour method can generalize well with limited training diversity, and achieve the\nstate-of-the-art generalization test performance.",
          "link": "http://arxiv.org/abs/2106.15587",
          "publishedOn": "2021-06-30T02:01:02.707Z",
          "wordCount": 611,
          "title": "Generalization of Reinforcement Learning with Policy-Aware Adversarial Data Augmentation. (arXiv:2106.15587v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
          "link": "http://arxiv.org/abs/2106.15561",
          "publishedOn": "2021-06-30T02:01:02.702Z",
          "wordCount": 607,
          "title": "A Survey on Neural Speech Synthesis. (arXiv:2106.15561v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2008.13578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenghong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zigeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shanglin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jinbo Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekaran_S/0/1/0/all/0/1\">Sanguthevar Rajasekaran</a>",
          "description": "The large model size, high computational operations, and vulnerability\nagainst membership inference attack (MIA) have impeded deep learning or deep\nneural networks (DNNs) popularity, especially on mobile devices. To address the\nchallenge, we envision that the weight pruning technique will help DNNs against\nMIA while reducing model storage and computational operation. In this work, we\npropose a pruning algorithm, and we show that the proposed algorithm can find a\nsubnetwork that can prevent privacy leakage from MIA and achieves competitive\naccuracy with the original DNNs. We also verify our theoretical insights with\nexperiments. Our experimental results illustrate that the attack accuracy using\nmodel compression is up to 13.6% and 10% lower than that of the baseline and\nMin-Max game, accordingly.",
          "link": "http://arxiv.org/abs/2008.13578",
          "publishedOn": "2021-06-30T02:01:02.697Z",
          "wordCount": 620,
          "title": "Against Membership Inference Attack: Pruning is All You Need. (arXiv:2008.13578v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13826",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Raffy_P/0/1/0/all/0/1\">Philippe Raffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pambrun_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Pambrun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Ashish Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubois_D/0/1/0/all/0/1\">David Dubois</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patti_J/0/1/0/all/0/1\">Jay Waldron Patti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cairns_R/0/1/0/all/0/1\">Robyn Alexandra Cairns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Young_R/0/1/0/all/0/1\">Ryan Young</a>",
          "description": "Standardized body region labelling of individual images provides data that\ncan improve human and computer use of medical images. A CNN-based classifier\nwas developed to identify body regions in CT and MRI. 17 CT (18 MRI) body\nregions covering the entire human body were defined for the classification\ntask. Three retrospective databases were built for the AI model training,\nvalidation, and testing, with a balanced distribution of studies per body\nregion. The test databases originated from a different healthcare network.\nAccuracy, recall and precision of the classifier was evaluated for patient age,\npatient gender, institution, scanner manufacturer, contrast, slice thickness,\nMRI sequence, and CT kernel. The data included a retrospective cohort of 2,934\nanonymized CT cases (training: 1,804 studies, validation: 602 studies, test:\n528 studies) and 3,185 anonymized MRI cases (training: 1,911 studies,\nvalidation: 636 studies, test: 638 studies). 27 institutions from primary care\nhospitals, community hospitals and imaging centers contributed to the test\ndatasets. The data included cases of all genders in equal proportions and\nsubjects aged from a few months old to +90 years old. An image-level prediction\naccuracy of 91.9% (90.2 - 92.1) for CT, and 94.2% (92.0 - 95.6) for MRI was\nachieved. The classification results were robust across all body regions and\nconfounding factors. Due to limited data, performance results for subjects\nunder 10 years-old could not be reliably evaluated. We show that deep learning\nmodels can classify CT and MRI images by body region including lower and upper\nextremities with high accuracy.",
          "link": "http://arxiv.org/abs/2104.13826",
          "publishedOn": "2021-06-30T02:01:02.682Z",
          "wordCount": 732,
          "title": "Deep Learning Body Region Classification of MRI and CT examinations. (arXiv:2104.13826v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Birhane_A/0/1/0/all/0/1\">Abeba Birhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalluri_P/0/1/0/all/0/1\">Pratyusha Kalluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1\">Dallas Card</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agnew_W/0/1/0/all/0/1\">William Agnew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dotan_R/0/1/0/all/0/1\">Ravit Dotan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_M/0/1/0/all/0/1\">Michelle Bao</a>",
          "description": "Machine learning (ML) currently exerts an outsized influence on the world,\nincreasingly affecting communities and institutional practices. It is therefore\ncritical that we question vague conceptions of the field as value-neutral or\nuniversally beneficial, and investigate what specific values the field is\nadvancing. In this paper, we present a rigorous examination of the values of\nthe field by quantitatively and qualitatively analyzing 100 highly cited ML\npapers published at premier ML conferences, ICML and NeurIPS. We annotate key\nfeatures of papers which reveal their values: how they justify their choice of\nproject, which aspects they uplift, their consideration of potential negative\nconsequences, and their institutional affiliations and funding sources. We find\nthat societal needs are typically very loosely connected to the choice of\nproject, if mentioned at all, and that consideration of negative consequences\nis extremely rare. We identify 67 values that are uplifted in machine learning\nresearch, and, of these, we find that papers most frequently justify and assess\nthemselves based on performance, generalization, efficiency, researcher\nunderstanding, novelty, and building on previous work. We present extensive\ntextual evidence and analysis of how these values are operationalized. Notably,\nwe find that each of these top values is currently being defined and applied\nwith assumptions and implications generally supporting the centralization of\npower. Finally, we find increasingly close ties between these highly cited\npapers and tech companies and elite universities.",
          "link": "http://arxiv.org/abs/2106.15590",
          "publishedOn": "2021-06-30T02:01:02.676Z",
          "wordCount": 678,
          "title": "The Values Encoded in Machine Learning Research. (arXiv:2106.15590v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1\">Nathan Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1\">Durga Sivasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dani_A/0/1/0/all/0/1\">Apurva Dani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "With the goal of making deep learning more label-efficient, a growing number\nof papers have been studying active learning (AL) for deep models. However,\nthere are a number of issues in the prevalent experimental settings, mainly\nstemming from a lack of unified implementation and benchmarking. Issues in the\ncurrent literature include sometimes contradictory observations on the\nperformance of different AL algorithms, unintended exclusion of important\ngeneralization approaches such as data augmentation and SGD for optimization, a\nlack of study of evaluation facets like the labeling efficiency of AL, and\nlittle or no clarity on the scenarios in which AL outperforms random sampling\n(RS). In this work, we present a unified re-implementation of state-of-the-art\nAL algorithms in the context of image classification, and we carefully study\nthese issues as facets of effective evaluation. On the positive side, we show\nthat AL techniques are 2x to 4x more label-efficient compared to RS with the\nuse of data augmentation. Surprisingly, when data augmentation is included,\nthere is no longer a consistent gain in using BADGE, a state-of-the-art\napproach, over simple uncertainty sampling. We then do a careful analysis of\nhow existing approaches perform with varying amounts of redundancy and number\nof examples per class. Finally, we provide several insights for AL\npractitioners to consider in future work, such as the effect of the AL batch\nsize, the effect of initialization, the importance of retraining a new model at\nevery round, and other insights.",
          "link": "http://arxiv.org/abs/2106.15324",
          "publishedOn": "2021-06-30T02:01:02.671Z",
          "wordCount": 726,
          "title": "Effective Evaluation of Deep Active Learning on Image Classification Tasks. (arXiv:2106.15324v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_F/0/1/0/all/0/1\">Fan Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1\">Hamed Haddadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katevas_K/0/1/0/all/0/1\">Kleomenis Katevas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_E/0/1/0/all/0/1\">Eduard Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perino_D/0/1/0/all/0/1\">Diego Perino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kourtellis_N/0/1/0/all/0/1\">Nicolas Kourtellis</a>",
          "description": "We propose and implement a Privacy-preserving Federated Learning ($PPFL$)\nframework for mobile systems to limit privacy leakages in federated learning.\nLeveraging the widespread presence of Trusted Execution Environments (TEEs) in\nhigh-end and mobile devices, we utilize TEEs on clients for local training, and\non servers for secure aggregation, so that model/gradient updates are hidden\nfrom adversaries. Challenged by the limited memory size of current TEEs, we\nleverage greedy layer-wise training to train each model's layer inside the\ntrusted area until its convergence. The performance evaluation of our\nimplementation shows that $PPFL$ can significantly improve privacy while\nincurring small system overheads at the client-side. In particular, $PPFL$ can\nsuccessfully defend the trained model against data reconstruction, property\ninference, and membership inference attacks. Furthermore, it can achieve\ncomparable model utility with fewer communication rounds (0.54$\\times$) and a\nsimilar amount of network traffic (1.002$\\times$) compared to the standard\nfederated learning of a complete model. This is achieved while only introducing\nup to ~15% CPU time, ~18% memory usage, and ~21% energy consumption overhead in\n$PPFL$'s client-side.",
          "link": "http://arxiv.org/abs/2104.14380",
          "publishedOn": "2021-06-30T02:01:02.635Z",
          "wordCount": 658,
          "title": "PPFL: Privacy-preserving Federated Learning with Trusted Execution Environments. (arXiv:2104.14380v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Li Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazidi_A/0/1/0/all/0/1\">Anis Yazidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1\">Morten Goodwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelstad_P/0/1/0/all/0/1\">Paal Engelstad</a>",
          "description": "We propose a novel algorithm named Expert Q-learning. Expert Q-learning was\ninspired by Dueling Q-learning and aimed at incorporating the ideas from\nsemi-supervised learning into reinforcement learning through splitting Q-values\ninto state values and action advantages. Different from Generative Adversarial\nImitation Learning and Deep Q-Learning from Demonstrations, the offline expert\nwe have used only predicts the value of a state from {-1, 0, 1}, indicating\nwhether this is a bad, neutral or good state. An expert network was designed in\naddition to the Q-network, which updates each time following the regular\noffline minibatch update whenever the expert example buffer is not empty. The\nQ-network plays the role of the advantage function only during the update. Our\nalgorithm also keeps asynchronous copies of the Q-network and expert network,\npredicting the target values using the same manner as of Double Q-learning.\n\nWe compared on the game of Othello our algorithm with the state-of-the-art\nQ-learning algorithm, which was a combination of Double Q-learning and Dueling\nQ-learning. The results showed that Expert Q-learning was indeed useful and\nmore resistant to the overestimation bias of Q-learning. The baseline\nQ-learning algorithm exhibited unstable and suboptimal behavior, especially\nwhen playing against a stochastic player, whereas Expert Q-learning\ndemonstrated more robust performance with higher scores. Expert Q-learning\nwithout using examples has also gained better results than the baseline\nalgorithm when trained and tested against a fixed player. On the other hand,\nExpert Q-learning without examples cannot win against the baseline Q-learning\nalgorithm in direct game competitions despite the fact that it has also shown\nthe strength of reducing the overestimation bias.",
          "link": "http://arxiv.org/abs/2106.14642",
          "publishedOn": "2021-06-30T02:01:02.629Z",
          "wordCount": 723,
          "title": "Expert Q-learning: Deep Q-learning With State Values From Expert Examples. (arXiv:2106.14642v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Din_Z/0/1/0/all/0/1\">Zainul Abi Din</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_H/0/1/0/all/0/1\">Hari Venugopalan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Henry Lin</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Wushensky_A/0/1/0/all/0/1\">Adam Wushensky</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Steven Liu</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+King_S/0/1/0/all/0/1\">Samuel T. King</a> (1 and 2) ((1) University of California, Davis, (2) Bouncer Technologies)",
          "description": "App builders commonly use security challenges, a form of step-up\nauthentication, to add security to their apps. However, the ethical\nimplications of this type of architecture has not been studied previously. In\nthis paper, we present a large-scale measurement study of running an existing\nanti-fraud security challenge, Boxer, in real apps running on mobile devices.\nWe find that although Boxer does work well overall, it is unable to scan\neffectively on devices that run its machine learning models at less than one\nframe per second (FPS), blocking users who use inexpensive devices. With the\ninsights from our study, we design Daredevil, anew anti-fraud system for\nscanning payment cards that work swell across the broad range of performance\ncharacteristics and hardware configurations found on modern mobile devices.\nDaredevil reduces the number of devices that run at less than one FPS by an\norder of magnitude compared to Boxer, providing a more equitable system for\nfighting fraud. In total, we collect data from 5,085,444 real devices spread\nacross 496 real apps running production software and interacting with real\nusers.",
          "link": "http://arxiv.org/abs/2106.14861",
          "publishedOn": "2021-06-30T02:01:02.616Z",
          "wordCount": 667,
          "title": "Doing good by fighting fraud: Ethical anti-fraud systems for mobile payments. (arXiv:2106.14861v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06075",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Barazandeh_B/0/1/0/all/0/1\">Babak Barazandeh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_T/0/1/0/all/0/1\">Tianjian Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Michailidis_G/0/1/0/all/0/1\">George Michailidis</a>",
          "description": "Min-max saddle point games have recently been intensely studied, due to their\nwide range of applications, including training Generative Adversarial Networks\n(GANs). However, most of the recent efforts for solving them are limited to\nspecial regimes such as convex-concave games. Further, it is customarily\nassumed that the underlying optimization problem is solved either by a single\nmachine or in the case of multiple machines connected in centralized fashion,\nwherein each one communicates with a central node. The latter approach becomes\nchallenging, when the underlying communications network has low bandwidth. In\naddition, privacy considerations may dictate that certain nodes can communicate\nwith a subset of other nodes. Hence, it is of interest to develop methods that\nsolve min-max games in a decentralized manner. To that end, we develop a\ndecentralized adaptive momentum (ADAM)-type algorithm for solving min-max\noptimization problem under the condition that the objective function satisfies\na Minty Variational Inequality condition, which is a generalization to\nconvex-concave case. The proposed method overcomes shortcomings of recent\nnon-adaptive gradient-based decentralized algorithms for min-max optimization\nproblems that do not perform well in practice and require careful tuning. In\nthis paper, we obtain non-asymptotic rates of convergence of the proposed\nalgorithm (coined DADAM$^3$) for finding a (stochastic) first-order Nash\nequilibrium point and subsequently evaluate its performance on training GANs.\nThe extensive empirical evaluation shows that DADAM$^3$ outperforms recently\ndeveloped methods, including decentralized optimistic stochastic gradient for\nsolving such min-max problems.",
          "link": "http://arxiv.org/abs/2106.06075",
          "publishedOn": "2021-06-30T02:01:02.611Z",
          "wordCount": 702,
          "title": "A Decentralized Adaptive Momentum Method for Solving a Class of Min-Max Optimization Problems. (arXiv:2106.06075v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanbei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hepp_T/0/1/0/all/0/1\">Tobias Hepp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>",
          "description": "Image-to-image translation plays a vital role in tackling various medical\nimaging tasks such as attenuation correction, motion correction, undersampled\nreconstruction, and denoising. Generative adversarial networks have been shown\nto achieve the state-of-the-art in generating high fidelity images for these\ntasks. However, the state-of-the-art GAN-based frameworks do not estimate the\nuncertainty in the predictions made by the network that is essential for making\ninformed medical decisions and subsequent revision by medical experts and has\nrecently been shown to improve the performance and interpretability of the\nmodel. In this work, we propose an uncertainty-guided progressive learning\nscheme for image-to-image translation. By incorporating aleatoric uncertainty\nas attention maps for GANs trained in a progressive manner, we generate images\nof increasing fidelity progressively. We demonstrate the efficacy of our model\non three challenging medical image translation tasks, including PET to CT\ntranslation, undersampled MRI reconstruction, and MRI motion artefact\ncorrection. Our model generalizes well in three different tasks and improves\nperformance over state of the art under full-supervision and weak-supervision\nwith limited data. Code is released here:\nhttps://github.com/ExplainableML/UncerGuidedI2I",
          "link": "http://arxiv.org/abs/2106.15542",
          "publishedOn": "2021-06-30T02:01:02.599Z",
          "wordCount": 634,
          "title": "Uncertainty-Guided Progressive GANs for Medical Image Translation. (arXiv:2106.15542v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huangjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "Contrastive learning (CL) is effective in learning data representations\nwithout label supervision, where the encoder needs to contrast each positive\nsample over multiple negative samples via a one-vs-many softmax cross-entropy\nloss. However, conventional CL is sensitive to how many negative samples are\nincluded and how they are selected. Proposed in this paper is a doubly CL\nstrategy that contrasts positive samples and negative ones within themselves\nseparately. We realize this strategy with contrastive attraction and\ncontrastive repulsion (CACR) makes the query not only exert a greater force to\nattract more distant positive samples but also do so to repel closer negative\nsamples. Theoretical analysis reveals the connection between CACR and CL from\nthe perspectives of both positive attraction and negative repulsion and shows\nthe benefits in both efficiency and robustness brought by separately\ncontrasting within the sampled positive and negative pairs. Extensive\nlarge-scale experiments on standard vision tasks show that CACR not only\nconsistently outperforms existing CL methods on benchmark datasets in\nrepresentation learning, but also provides interpretable contrastive weights,\ndemonstrating the efficacy of the proposed doubly contrastive strategy.",
          "link": "http://arxiv.org/abs/2105.03746",
          "publishedOn": "2021-06-30T02:01:02.593Z",
          "wordCount": 664,
          "title": "Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_M/0/1/0/all/0/1\">Monami Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1\">Rudrasis Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouza_J/0/1/0/all/0/1\">Jose Bouza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemuri_B/0/1/0/all/0/1\">Baba C. Vemuri</a>",
          "description": "Convolutional neural networks have been highly successful in image-based\nlearning tasks due to their translation equivariance property. Recent work has\ngeneralized the traditional convolutional layer of a convolutional neural\nnetwork to non-Euclidean spaces and shown group equivariance of the generalized\nconvolution operation. In this paper, we present a novel higher order Volterra\nconvolutional neural network (VolterraNet) for data defined as samples of\nfunctions on Riemannian homogeneous spaces. Analagous to the result for\ntraditional convolutions, we prove that the Volterra functional convolutions\nare equivariant to the action of the isometry group admitted by the Riemannian\nhomogeneous spaces, and under some restrictions, any non-linear equivariant\nfunction can be expressed as our homogeneous space Volterra convolution,\ngeneralizing the non-linear shift equivariant characterization of Volterra\nexpansions in Euclidean space. We also prove that second order functional\nconvolution operations can be represented as cascaded convolutions which leads\nto an efficient implementation. Beyond this, we also propose a dilated\nVolterraNet model. These advances lead to large parameter reductions relative\nto baseline non-Euclidean CNNs. To demonstrate the efficacy of the VolterraNet\nperformance, we present several real data experiments involving classification\ntasks on spherical-MNIST, atomic energy, Shrec17 data sets, and group testing\non diffusion MRI data. Performance comparisons to the state-of-the-art are also\npresented.",
          "link": "http://arxiv.org/abs/2106.15301",
          "publishedOn": "2021-06-30T02:01:02.563Z",
          "wordCount": 665,
          "title": "VolterraNet: A higher order convolutional network with group equivariance for homogeneous manifolds. (arXiv:2106.15301v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "This framework for human behavior monitoring aims to take a holistic approach\nto study, track, monitor, and analyze human behavior during activities of daily\nliving (ADLs). The framework consists of two novel functionalities. First, it\ncan perform the semantic analysis of user interactions on the diverse\ncontextual parameters during ADLs to identify a list of distinct behavioral\npatterns associated with different complex activities. Second, it consists of\nan intelligent decision-making algorithm that can analyze these behavioral\npatterns and their relationships with the dynamic contextual and spatial\nfeatures of the environment to detect any anomalies in user behavior that could\nconstitute an emergency. These functionalities of this interdisciplinary\nframework were developed by integrating the latest advancements and\ntechnologies in human-computer interaction, machine learning, Internet of\nThings, pattern recognition, and ubiquitous computing. The framework was\nevaluated on a dataset of ADLs, and the performance accuracies of these two\nfunctionalities were found to be 76.71% and 83.87%, respectively. The presented\nand discussed results uphold the relevance and immense potential of this\nframework to contribute towards improving the quality of life and assisted\nliving of the aging population in the future of Internet of Things (IoT)-based\nubiquitous living environments, e.g., smart homes.",
          "link": "http://arxiv.org/abs/2106.15609",
          "publishedOn": "2021-06-30T02:01:02.549Z",
          "wordCount": 658,
          "title": "An Ambient Intelligence-Based Human Behavior Monitoring Framework for Ubiquitous Environments. (arXiv:2106.15609v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saunshi_N/0/1/0/all/0/1\">Nikunj Saunshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arushi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>",
          "description": "An effective approach in meta-learning is to utilize multiple \"train tasks\"\nto learn a good initialization for model parameters that can help solve unseen\n\"test tasks\" with very few samples by fine-tuning from this initialization.\nAlthough successful in practice, theoretical understanding of such methods is\nlimited. This work studies an important aspect of these methods: splitting the\ndata from each task into train (support) and validation (query) sets during\nmeta-training. Inspired by recent work (Raghu et al., 2020), we view such\nmeta-learning methods through the lens of representation learning and argue\nthat the train-validation split encourages the learned representation to be\nlow-rank without compromising on expressivity, as opposed to the non-splitting\nvariant that encourages high-rank representations. Since sample efficiency\nbenefits from low-rankness, the splitting strategy will require very few\nsamples to solve unseen test tasks. We present theoretical results that\nformalize this idea for linear representation learning on a subspace\nmeta-learning instance, and experimentally verify this practical benefit of\nsplitting in simulations and on standard meta-learning benchmarks.",
          "link": "http://arxiv.org/abs/2106.15615",
          "publishedOn": "2021-06-30T02:01:02.544Z",
          "wordCount": 613,
          "title": "A Representation Learning Perspective on the Importance of Train-Validation Splitting in Meta-Learning. (arXiv:2106.15615v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kahu_S/0/1/0/all/0/1\">Sampanna Yashwant Kahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingram_W/0/1/0/all/0/1\">William A. Ingram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1\">Edward A. Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>",
          "description": "We focus on electronic theses and dissertations (ETDs), aiming to improve\naccess and expand their utility, since more than 6 million are publicly\navailable, and they constitute an important corpus to aid research and\neducation across disciplines. The corpus is growing as new born-digital\ndocuments are included, and since millions of older theses and dissertations\nhave been converted to digital form to be disseminated electronically in\ninstitutional repositories. In ETDs, as with other scholarly works, figures and\ntables can communicate a large amount of information in a concise way. Although\nmethods have been proposed for extracting figures and tables from born-digital\nPDFs, they do not work well with scanned ETDs. Considering this problem, our\nassessment of state-of-the-art figure extraction systems is that the reason\nthey do not function well on scanned PDFs is that they have only been trained\non born-digital documents. To address this limitation, we present ScanBank, a\nnew dataset containing 10 thousand scanned page images, manually labeled by\nhumans as to the presence of the 3.3 thousand figures or tables found therein.\nWe use this dataset to train a deep neural network model based on YOLOv5 to\naccurately extract figures and tables from scanned ETDs. We pose and answer\nimportant research questions aimed at finding better methods for figure\nextraction from scanned documents. One of those concerns the value for\ntraining, of data augmentation techniques applied to born-digital documents\nwhich are used to train models better suited for figure extraction from scanned\ndocuments. To the best of our knowledge, ScanBank is the first manually\nannotated dataset for figure and table extraction for scanned ETDs. A\nYOLOv5-based model, trained on ScanBank, outperforms existing comparable\nopen-source and freely available baseline methods by a considerable margin.",
          "link": "http://arxiv.org/abs/2106.15320",
          "publishedOn": "2021-06-30T02:01:02.539Z",
          "wordCount": 756,
          "title": "ScanBank: A Benchmark Dataset for Figure Extraction from Scanned Electronic Theses and Dissertations. (arXiv:2106.15320v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boucaud_L/0/1/0/all/0/1\">Laurent Boucaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloise_D/0/1/0/all/0/1\">Daniel Aloise</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1\">Nicolas Saunier</a>",
          "description": "We consider the problem of predicting the future path of a pedestrian using\nits motion history and the motion history of the surrounding pedestrians,\ncalled social information. Since the seminal paper on Social-LSTM,\ndeep-learning has become the main tool used to model the impact of social\ninteractions on a pedestrian's motion. The demonstration that these models can\nlearn social interactions relies on an ablative study of these models. The\nmodels are compared with and without their social interactions module on two\nstandard metrics, the Average Displacement Error and Final Displacement Error.\nYet, these complex models were recently outperformed by a simple\nconstant-velocity approach. This questions if they actually allow to model\nsocial interactions as well as the validity of the proof. In this paper, we\nfocus on the deep-learning models with a soft-attention mechanism for social\ninteraction modeling and study whether they use social information at\nprediction time. We conduct two experiments across four state-of-the-art\napproaches on the ETH and UCY datasets, which were also used in previous work.\nFirst, the models are trained by replacing the social information with random\nnoise and compared to model trained with actual social information. Second, we\nuse a gating mechanism along with a $L_0$ penalty, allowing models to shut down\ntheir inner components. The models consistently learn to prune their\nsoft-attention mechanism. For both experiments, neither the course of the\nconvergence nor the prediction performance were altered. This demonstrates that\nthe soft-attention mechanism and therefore the social information are ignored\nby the models.",
          "link": "http://arxiv.org/abs/2106.15321",
          "publishedOn": "2021-06-30T02:01:02.478Z",
          "wordCount": 726,
          "title": "Soft Attention: Does it Actually Help to Learn Social Interactions in Pedestrian Trajectory Prediction?. (arXiv:2106.15321v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruijters_D/0/1/0/all/0/1\">Daniel Ruijters</a>",
          "description": "Minimally invasive image guided treatment procedures often employ advanced\nimage processing algorithms. The recent developments of artificial intelligence\nalgorithms harbor potential to further enhance this domain. In this article we\nexplore several application areas within the minimally invasive treatment space\nand discuss the deployment of artificial intelligence within these areas.",
          "link": "http://arxiv.org/abs/2106.15306",
          "publishedOn": "2021-06-30T02:01:02.472Z",
          "wordCount": 496,
          "title": "Artificial Intelligence in Minimally Invasive Interventional Treatment. (arXiv:2106.15306v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Camero_A/0/1/0/all/0/1\">Andr&#xe9;s Camero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutouh_J/0/1/0/all/0/1\">Jamal Toutouh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alba_E/0/1/0/all/0/1\">Enrique Alba</a>",
          "description": "This article introduces Random Error Sampling-based Neuroevolution (RESN), a\nnovel automatic method to optimize recurrent neural network architectures. RESN\ncombines an evolutionary algorithm with a training-free evaluation approach.\nThe results show that RESN achieves state-of-the-art error performance while\nreducing by half the computational time.",
          "link": "http://arxiv.org/abs/2106.15295",
          "publishedOn": "2021-06-30T02:01:02.458Z",
          "wordCount": 480,
          "title": "Reliable and Fast Recurrent Neural Network Architecture Optimization. (arXiv:2106.15295v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15190",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1\">Thi Ngoc Tho Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watcharasupat_K/0/1/0/all/0/1\">Karn Watcharasupat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Khanh Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jones_D/0/1/0/all/0/1\">Douglas L. Jones</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1\">Woon Seng Gan</a>",
          "description": "Sound event localization and detection consists of two subtasks which are\nsound event detection and direction-of-arrival estimation. While sound event\ndetection mainly relies on time-frequency patterns to distinguish different\nsound classes, direction-of-arrival estimation uses magnitude or phase\ndifferences between microphones to estimate source directions. Therefore, it is\noften difficult to jointly train these two subtasks simultaneously. We propose\na novel feature called spatial cue-augmented log-spectrogram (SALSA) with exact\ntime-frequency mapping between the signal power and the source\ndirection-of-arrival. The feature includes multichannel log-spectrograms\nstacked along with the estimated direct-to-reverberant ratio and a normalized\nversion of the principal eigenvector of the spatial covariance matrix at each\ntime-frequency bin on the spectrograms. Experimental results on the DCASE 2021\ndataset for sound event localization and detection with directional\ninterference showed that the deep learning-based models trained on this new\nfeature outperformed the DCASE challenge baseline by a large margin. We\ncombined several models with slightly different architectures that were trained\non the new feature to further improve the system performances for the DCASE\nsound event localization and detection challenge.",
          "link": "http://arxiv.org/abs/2106.15190",
          "publishedOn": "2021-06-30T02:01:02.453Z",
          "wordCount": 658,
          "title": "DCASE 2021 Task 3: Spectrotemporally-aligned Features for Polyphonic Sound Event Localization and Detection. (arXiv:2106.15190v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereg_D/0/1/0/all/0/1\">Deborah Pereg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Israel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassiliou_A/0/1/0/all/0/1\">Anthony A. Vassiliou</a>",
          "description": "In sparse coding, we attempt to extract features of input vectors, assuming\nthat the data is inherently structured as a sparse superposition of basic\nbuilding blocks. Similarly, neural networks perform a given task by learning\nfeatures of the training data set. Recently both data-driven and model-driven\nfeature extracting methods have become extremely popular and have achieved\nremarkable results. Nevertheless, practical implementations are often too slow\nto be employed in real-life scenarios, especially for real-time applications.\nWe propose a speed-up upgraded version of the classic iterative thresholding\nalgorithm, that produces a good approximation of the convolutional sparse code\nwithin 2-5 iterations. The speed advantage is gained mostly from the\nobservation that most solvers are slowed down by inefficient global\nthresholding. The main idea is to normalize each data point by the local\nreceptive field energy, before applying a threshold. This way, the natural\ninclination towards strong feature expressions is suppressed, so that one can\nrely on a global threshold that can be easily approximated, or learned during\ntraining. The proposed algorithm can be employed with a known predetermined\ndictionary, or with a trained dictionary. The trained version is implemented as\na neural net designed as the unfolding of the proposed solver. The performance\nof the proposed solution is demonstrated via the seismic inversion problem in\nboth synthetic and real data scenarios. We also provide theoretical guarantees\nfor a stable support recovery. Namely, we prove that under certain conditions\nthe true support is perfectly recovered within the first iteration.",
          "link": "http://arxiv.org/abs/2106.15296",
          "publishedOn": "2021-06-30T02:01:02.448Z",
          "wordCount": 697,
          "title": "Convolutional Sparse Coding Fast Approximation with Application to Seismic Reflectivity Estimation. (arXiv:2106.15296v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15298",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fan_Y/0/1/0/all/0/1\">Yangxin Fan</a>",
          "description": "We believe that \"all men are created equal\". With the rise of the police\nshootings reported by media, more people in the U.S. think that police use\nexcessive force during law enforcement, especially to a specific group of\npeople. We want to apply multidimensional statistical analysis to reveal more\nfacts than the monotone mainstream media. Our paper has three parts. First, we\nproposed a new method to quantify fatal police shooting news reporting\ndeviation of mainstream media, which includes CNN, FOX, ABC, and NBC. Second,\nwe analyzed the most comprehensive US fatal police shooting dataset from\nWashington Post. We used FP-growth to reveal the frequent patterns and DBSCAN\nclustering to find fatal shooting hotspots. We brought multi-attributes (social\neconomics, demographics, political tendency, education, gun ownership rate,\npolice training hours, etc.) to reveal connections under the iceberg. We found\nthat the police shooting rate of a state depends on many variables. The top\nfour most relevant attributes were state joined year, state land area, gun\nownership rate, and violent crime rate. Third, we proposed four regression\nmodels to predict police shooting rates at the state level. The best model\nKstar could predict the fatal police shooting rate with about 88.53%\ncorrelation coefficient. We also proposed classification models, including\nGradient Boosting Machine, Multi-class Classifier, Logistic Regression, and\nNaive Bayes Classifier, to predict the race of fatal police shooting victims.\nOur classification models show no significant evidence to conclude that racial\ndiscrimination happened during fatal police shootings recorded by the WP\ndataset.",
          "link": "http://arxiv.org/abs/2106.15298",
          "publishedOn": "2021-06-30T02:01:02.443Z",
          "wordCount": 688,
          "title": "US Fatal Police Shooting Analysis and Prediction. (arXiv:2106.15298v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Royston_S/0/1/0/all/0/1\">Sam Royston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenberg_B/0/1/0/all/0/1\">Ben Greenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavasoli_O/0/1/0/all/0/1\">Omeed Tavasoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotton_C/0/1/0/all/0/1\">Courtenay Cotton</a>",
          "description": "Voter eligibility in United States elections is determined by a patchwork of\nstate databases containing information about which citizens are eligible to\nvote. Administrators at the state and local level are faced with the\nexceedingly difficult task of ensuring that each of their jurisdictions is\nproperly managed, while also monitoring for improper modifications to the\ndatabase. Monitoring changes to Voter Registration Files (VRFs) is crucial,\ngiven that a malicious actor wishing to disrupt the democratic process in the\nUS would be well-advised to manipulate the contents of these files in order to\nachieve their goals. In 2020, we saw election officials perform admirably when\nfaced with administering one of the most contentious elections in US history,\nbut much work remains to secure and monitor the election systems Americans rely\non. Using data created by comparing snapshots taken of VRFs over time, we\npresent a set of methods that make use of machine learning to ease the burden\non analysts and administrators in protecting voter rolls. We first evaluate the\neffectiveness of multiple unsupervised anomaly detection methods in detecting\nVRF modifications by modeling anomalous changes as sparse additive noise. In\nthis setting we determine that statistical models comparing administrative\ndistricts within a short time span and non-negative matrix factorization are\nmost effective for surfacing anomalous events for review. These methods were\ndeployed during 2019-2020 in our organization's monitoring system and were used\nin collaboration with the office of the Iowa Secretary of State. Additionally,\nwe propose a newly deployed model which uses historical and demographic\nmetadata to label the likely root cause of database modifications. We hope to\nuse this model to predict which modifications have known causes and therefore\nbetter identify potentially anomalous modifications.",
          "link": "http://arxiv.org/abs/2106.15285",
          "publishedOn": "2021-06-30T02:01:02.426Z",
          "wordCount": 732,
          "title": "Anomaly Detection and Automated Labeling for Voter Registration File Changes. (arXiv:2106.15285v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirsten_L/0/1/0/all/0/1\">Lucas N. Kirsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccoli_R/0/1/0/all/0/1\">Ricardo Piccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribani_R/0/1/0/all/0/1\">Ricardo Ribani</a>",
          "description": "This work evaluates six state-of-the-art deep neural network (DNN)\narchitectures applied to the problem of enhancing camera-captured document\nimages. The results from each network were evaluated both qualitatively and\nquantitatively using Image Quality Assessment (IQA) metrics, and also compared\nwith an existing approach based on traditional computer vision techniques. The\nbest performing architectures generally produced good enhancement compared to\nthe existing algorithm, showing that it is possible to use DNNs for document\nimage enhancement. Furthermore, the best performing architectures could work as\na baseline for future investigations on document enhancement using deep\nlearning techniques. The main contributions of this paper are: a baseline of\ndeep learning techniques that can be further improved to provide better\nresults, and a evaluation methodology using IQA metrics for quantitatively\ncomparing the produced images from the neural networks to a ground truth.",
          "link": "http://arxiv.org/abs/2106.15286",
          "publishedOn": "2021-06-30T02:01:02.421Z",
          "wordCount": 597,
          "title": "Evaluating Deep Neural Networks for Image Document Enhancement. (arXiv:2106.15286v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soares_D/0/1/0/all/0/1\">Daniel de Barros Soares</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Andrieux_F/0/1/0/all/0/1\">Fran&#xe7;ois Andrieux</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hell_B/0/1/0/all/0/1\">Bastien Hell</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lenhardt_J/0/1/0/all/0/1\">Julien Lenhardt</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Badosa_J/0/1/0/all/0/1\">Jordi Badosa</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Gavoille_S/0/1/0/all/0/1\">Sylvain Gavoille</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gaiffas_S/0/1/0/all/0/1\">St&#xe9;phane Gaiffas</a> (1, 4 and 5), <a href=\"http://arxiv.org/find/cs/1/au:+Bacry_E/0/1/0/all/0/1\">Emmanuel Bacry</a> (1 and 6), ((1) namR, Paris, France, (2) ENSTA Paris, France, (3) LMD, Ecole polytechnique, IP Paris, Palaiseau, France, (4) LPSM, Universit&#xe9; de Paris, France, (5) DMA, Ecole normale sup&#xe9;rieure, Paris, France, (6) CEREMADE, Universit&#xe9; Paris Dauphine, Paris, France)",
          "description": "Estimating the amount of electricity that can be produced by rooftop\nphotovoltaic systems is a time-consuming process that requires on-site\nmeasurements, a difficult task to achieve on a large scale. In this paper, we\npresent an approach to estimate the solar potential of rooftops based on their\nlocation and architectural characteristics, as well as the amount of solar\nradiation they receive annually. Our technique uses computer vision to achieve\nsemantic segmentation of roof sections and roof objects on the one hand, and a\nmachine learning model based on structured building features to predict roof\npitch on the other hand. We then compute the azimuth and maximum number of\nsolar panels that can be installed on a rooftop with geometric approaches.\nFinally, we compute precise shading masks and combine them with solar\nirradiation data that enables us to estimate the yearly solar potential of a\nrooftop.",
          "link": "http://arxiv.org/abs/2106.15268",
          "publishedOn": "2021-06-30T02:01:02.416Z",
          "wordCount": 655,
          "title": "Predicting the Solar Potential of Rooftops using Image Segmentation and Structured Data. (arXiv:2106.15268v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geeho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>",
          "description": "Visual recognition tasks are often limited to dealing with a small subset of\nclasses simply because the labels for the remaining classes are unavailable. We\nare interested in identifying novel concepts in a dataset through\nrepresentation learning based on the examples in both labeled and unlabeled\nclasses, and extending the horizon of recognition to both known and novel\nclasses. To address this challenging task, we propose a combinatorial learning\napproach, which naturally clusters the examples in unseen classes using the\ncompositional knowledge given by multiple supervised meta-classifiers on\nheterogeneous label spaces. We also introduce a metric learning strategy to\nestimate pairwise pseudo-labels for improving representations of unlabeled\nexamples, which preserves semantic relations across known and novel classes\neffectively. The proposed algorithm discovers novel concepts via a joint\noptimization of enhancing the discrimitiveness of unseen classes as well as\nlearning the representations of known classes generalizable to novel ones. Our\nextensive experiments demonstrate remarkable performance gains by the proposed\napproach in multiple image retrieval and novel class discovery benchmarks.",
          "link": "http://arxiv.org/abs/2106.15278",
          "publishedOn": "2021-06-30T02:01:02.411Z",
          "wordCount": 609,
          "title": "Open-Set Representation Learning through Combinatorial Embedding. (arXiv:2106.15278v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zahirnia_K/0/1/0/all/0/1\">Kiarash Zahirnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakhuja_A/0/1/0/all/0/1\">Ankita Sakhuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulte_O/0/1/0/all/0/1\">Oliver Schulte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadaf_P/0/1/0/all/0/1\">Parmis Nadaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "Recent work on graph generative models has made remarkable progress towards\ngenerating increasingly realistic graphs, as measured by global graph features\nsuch as degree distribution, density, and clustering coefficients. Deep\ngenerative models have also made significant advances through better modelling\nof the local correlations in the graph topology, which have been very useful\nfor predicting unobserved graph components, such as the existence of a link or\nthe class of a node, from nearby observed graph components. A complete\nscientific understanding of graph data should address both global and local\nstructure. In this paper, we propose a joint model for both as complementary\nobjectives in a graph VAE framework. Global structure is captured by\nincorporating graph kernels in a probabilistic model whose loss function is\nclosely related to the maximum mean discrepancy(MMD) between the global\nstructures of the reconstructed and the input graphs. The ELBO objective\nderived from the model regularizes a standard local link reconstruction term\nwith an MMD term. Our experiments demonstrate a significant improvement in the\nrealism of the generated graph structures, typically by 1-2 orders of magnitude\nof graph structure metrics, compared to leading graph VAEand GAN models. Local\nlink reconstruction improves as well in many cases.",
          "link": "http://arxiv.org/abs/2106.15239",
          "publishedOn": "2021-06-30T02:01:02.405Z",
          "wordCount": 633,
          "title": "Generating the Graph Gestalt: Kernel-Regularized Graph Representation Learning. (arXiv:2106.15239v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1\">Simone Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1\">Ehsan Nowroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orazi_G/0/1/0/all/0/1\">Gabriele Orazi</a>",
          "description": "The last-generation video conferencing software allows users to utilize a\nvirtual background to conceal their personal environment due to privacy\nconcerns, especially in official meetings with other employers. On the other\nhand, users maybe want to fool people in the meeting by considering the virtual\nbackground to conceal where they are. In this case, developing tools to\nunderstand the virtual background utilize for fooling people in meeting plays\nan important role. Besides, such detectors must prove robust against different\nkinds of attacks since a malicious user can fool the detector by applying a set\nof adversarial editing steps on the video to conceal any revealing footprint.\nIn this paper, we study the feasibility of an efficient tool to detect whether\na videoconferencing user background is real. In particular, we provide the\nfirst tool which computes pixel co-occurrences matrices and uses them to search\nfor inconsistencies among spectral and spatial bands. Our experiments confirm\nthat cross co-occurrences matrices improve the robustness of the detector\nagainst different kinds of attacks. This work's performance is especially\nnoteworthy with regard to color SPAM features. Moreover, the performance\nespecially is significant with regard to robustness versus post-processing,\nlike geometric transformations, filtering, contrast enhancement, and JPEG\ncompression with different quality factors.",
          "link": "http://arxiv.org/abs/2106.15130",
          "publishedOn": "2021-06-30T02:01:02.400Z",
          "wordCount": 669,
          "title": "Do Not Deceive Your Employer with a Virtual Background: A Video Conferencing Manipulation-Detection System. (arXiv:2106.15130v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Michael Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1\">Sidhant Kaushik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>",
          "description": "Many transfer problems require re-using previously optimal decisions for\nsolving new tasks, which suggests the need for learning algorithms that can\nmodify the mechanisms for choosing certain actions independently of those for\nchoosing others. However, there is currently no formalism nor theory for how to\nachieve this kind of modular credit assignment. To answer this question, we\ndefine modular credit assignment as a constraint on minimizing the algorithmic\nmutual information among feedback signals for different decisions. We introduce\nwhat we call the modularity criterion for testing whether a learning algorithm\nsatisfies this constraint by performing causal analysis on the algorithm\nitself. We generalize the recently proposed societal decision-making framework\nas a more granular formalism than the Markov decision process to prove that for\ndecision sequences that do not contain cycles, certain single-step temporal\ndifference action-value methods meet this criterion while all policy-gradient\nmethods do not. Empirical evidence suggests that such action-value methods are\nmore sample efficient than policy-gradient methods on transfer problems that\nrequire only sparse changes to a sequence of previously optimal decisions.",
          "link": "http://arxiv.org/abs/2106.14993",
          "publishedOn": "2021-06-30T02:01:02.386Z",
          "wordCount": 647,
          "title": "Modularity in Reinforcement Learning via Algorithmic Independence in Credit Assignment. (arXiv:2106.14993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulits_P/0/1/0/all/0/1\">Peter Kulits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_J/0/1/0/all/0/1\">Jake Wall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedetti_A/0/1/0/all/0/1\">Anka Bedetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henley_M/0/1/0/all/0/1\">Michelle Henley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>",
          "description": "African elephants are vital to their ecosystems, but their populations are\nthreatened by a rise in human-elephant conflict and poaching. Monitoring\npopulation dynamics is essential in conservation efforts; however, tracking\nelephants is a difficult task, usually relying on the invasive and sometimes\ndangerous placement of GPS collars. Although there have been many recent\nsuccesses in the use of computer vision techniques for automated identification\nof other species, identification of elephants is extremely difficult and\ntypically requires expertise as well as familiarity with elephants in the\npopulation. We have built and deployed a web-based platform and database for\nhuman-in-the-loop re-identification of elephants combining manual attribute\nlabeling and state-of-the-art computer vision algorithms, known as\nElephantBook. Our system is currently in use at the Mara Elephant Project,\nhelping monitor the protected and at-risk population of elephants in the\nGreater Maasai Mara ecosystem. ElephantBook makes elephant re-identification\nusable by non-experts and scalable for use by multiple conservation NGOs.",
          "link": "http://arxiv.org/abs/2106.15083",
          "publishedOn": "2021-06-30T02:01:02.381Z",
          "wordCount": 595,
          "title": "ElephantBook: A Semi-Automated Human-in-the-Loop System for Elephant Re-Identification. (arXiv:2106.15083v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1\">Caglar Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussallem_D/0/1/0/all/0/1\">Diego Moussallem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heindorf_S/0/1/0/all/0/1\">Stefan Heindorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>",
          "description": "Knowledge graph embedding research has mainly focused on the two smallest\nnormed division algebras, $\\mathbb{R}$ and $\\mathbb{C}$. Recent results suggest\nthat trilinear products of quaternion-valued embeddings can be a more effective\nmeans to tackle link prediction. In addition, models based on convolutions on\nreal-valued embeddings often yield state-of-the-art results for link\nprediction. In this paper, we investigate a composition of convolution\noperations with hypercomplex multiplications. We propose the four approaches\nQMult, OMult, ConvQ and ConvO to tackle the link prediction problem. QMult and\nOMult can be considered as quaternion and octonion extensions of previous\nstate-of-the-art approaches, including DistMult and ComplEx. ConvQ and ConvO\nbuild upon QMult and OMult by including convolution operations in a way\ninspired by the residual learning framework. We evaluated our approaches on\nseven link prediction datasets including WN18RR, FB15K-237 and YAGO3-10.\nExperimental results suggest that the benefits of learning hypercomplex-valued\nvector representations become more apparent as the size and complexity of the\nknowledge graph grows. ConvO outperforms state-of-the-art approaches on\nFB15K-237 in MRR, Hit@1 and Hit@3, while QMult, OMult, ConvQ and ConvO\noutperform state-of-the-approaches on YAGO3-10 in all metrics. Results also\nsuggest that link prediction performances can be further improved via\nprediction averaging. To foster reproducible research, we provide an\nopen-source implementation of approaches, including training and evaluation\nscripts as well as pretrained models.",
          "link": "http://arxiv.org/abs/2106.15230",
          "publishedOn": "2021-06-30T02:01:02.376Z",
          "wordCount": 648,
          "title": "Convolutional Hypercomplex Embeddings for Link Prediction. (arXiv:2106.15230v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1\">P.S. Sastry</a>",
          "description": "Deep Neural Networks (DNNs) have been shown to be susceptible to memorization\nor overfitting in the presence of noisily labelled data. For the problem of\nrobust learning under such noisy data, several algorithms have been proposed. A\nprominent class of algorithms rely on sample selection strategies, motivated by\ncurriculum learning. For example, many algorithms use the `small loss trick'\nwherein a fraction of samples with loss values below a certain threshold are\nselected for training. These algorithms are sensitive to such thresholds, and\nit is difficult to fix or learn these thresholds. Often, these algorithms also\nrequire information such as label noise rates which are typically unavailable\nin practice. In this paper, we propose a data-dependent, adaptive sample\nselection strategy that relies only on batch statistics of a given mini-batch\nto provide robustness against label noise. The algorithm does not have any\nadditional hyperparameters for sample selection, does not need any information\non noise rates, and does not need access to separate data with clean labels. We\nempirically demonstrate the effectiveness of our algorithm on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.15292",
          "publishedOn": "2021-06-30T02:01:02.361Z",
          "wordCount": 620,
          "title": "Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sherman_U/0/1/0/all/0/1\">Uri Sherman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1\">Yishay Mansour</a>",
          "description": "We study online convex optimization in the random order model, recently\nproposed by \\citet{garber2020online}, where the loss functions may be chosen by\nan adversary, but are then presented to the online algorithm in a uniformly\nrandom order. Focusing on the scenario where the cumulative loss function is\n(strongly) convex, yet individual loss functions are smooth but might be\nnon-convex, we give algorithms that achieve the optimal bounds and\nsignificantly outperform the results of \\citet{garber2020online}, completely\nremoving the dimension dependence and improving their scaling with respect to\nthe strong convexity parameter. Our analysis relies on novel connections\nbetween algorithmic stability and generalization for sampling\nwithout-replacement analogous to those studied in the with-replacement\ni.i.d.~setting, as well as on a refined average stability analysis of\nstochastic gradient descent.",
          "link": "http://arxiv.org/abs/2106.15207",
          "publishedOn": "2021-06-30T02:01:02.356Z",
          "wordCount": 563,
          "title": "Optimal Rates for Random Order Online Optimization. (arXiv:2106.15207v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marmin_A/0/1/0/all/0/1\">Arthur Marmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulart_J/0/1/0/all/0/1\">Jos&#xe9; Henrique de Morais Goulart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1\">C&#xe9;dric F&#xe9;votte</a>",
          "description": "This article proposes new multiplicative updates for nonnegative matrix\nfactorization (NMF) with the $\\beta$-divergence objective function. Our new\nupdates are derived from a joint majorization-minimization (MM) scheme, in\nwhich an auxiliary function (a tight upper bound of the objective function) is\nbuilt for the two factors jointly and minimized at each iteration. This is in\ncontrast with the classic approach in which the factors are optimized\nalternately and a MM scheme is applied to each factor individually. Like the\nclassic approach, our joint MM algorithm also results in multiplicative updates\nthat are simple to implement. They however yield a significant drop of\ncomputation time (for equally good solutions), in particular for some\n$\\beta$-divergences of important applicative interest, such as the squared\nEuclidean distance and the Kullback-Leibler or Itakura-Saito divergences. We\nreport experimental results using diverse datasets: face images, audio\nspectrograms, hyperspectral data and song play counts. Depending on the value\nof $\\beta$ and on the dataset, our joint MM approach yields a CPU time\nreduction of about $10\\%$ to $78\\%$ in comparison to the classic alternating\nscheme.",
          "link": "http://arxiv.org/abs/2106.15214",
          "publishedOn": "2021-06-30T02:01:02.350Z",
          "wordCount": 615,
          "title": "Joint Majorization-Minimization for Nonnegative Matrix Factorization with the $\\beta$-divergence. (arXiv:2106.15214v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Radstok_W/0/1/0/all/0/1\">Wessel Radstok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chekol_M/0/1/0/all/0/1\">Mel Chekol</a>",
          "description": "The inclusion of temporal scopes of facts in knowledge graph embedding (KGE)\npresents significant opportunities for improving the resulting embeddings, and\nconsequently for increased performance in downstream applications. Yet, little\nresearch effort has focussed on this area and much of the carried out research\nreports only marginally improved results compared to models trained without\ntemporal scopes (static models). Furthermore, rather than leveraging existing\nwork on static models, they introduce new models specific to temporal knowledge\ngraphs. We propose a novel perspective that takes advantage of the power of\nexisting static embedding models by focussing effort on manipulating the data\ninstead. Our method, SpliMe, draws inspiration from the field of signal\nprocessing and early work in graph embedding. We show that SpliMe competes with\nor outperforms the current state of the art in temporal KGE. Additionally, we\nuncover issues with the procedure currently used to assess the performance of\nstatic models on temporal graphs and introduce two ways to counteract them.",
          "link": "http://arxiv.org/abs/2106.15223",
          "publishedOn": "2021-06-30T02:01:02.345Z",
          "wordCount": 598,
          "title": "Leveraging Static Models for Link Prediction in Temporal Knowledge Graphs. (arXiv:2106.15223v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1\">Stylianos I. Venieris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panopoulos_I/0/1/0/all/0/1\">Ioannis Panopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1\">Ilias Leontiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venieris_I/0/1/0/all/0/1\">Iakovos S. Venieris</a>",
          "description": "The unprecedented performance of deep neural networks (DNNs) has led to large\nstrides in various Artificial Intelligence (AI) inference tasks, such as object\nand speech recognition. Nevertheless, deploying such AI models across commodity\ndevices faces significant challenges: large computational cost, multiple\nperformance objectives, hardware heterogeneity and a common need for high\naccuracy, together pose critical problems to the deployment of DNNs across the\nvarious embedded and mobile devices in the wild. As such, we have yet to\nwitness the mainstream usage of state-of-the-art deep learning algorithms\nacross consumer devices. In this paper, we provide preliminary answers to this\npotentially game-changing question by presenting an array of design techniques\nfor efficient AI systems. We start by examining the major roadblocks when\ntargeting both programmable processors and custom accelerators. Then, we\npresent diverse methods for achieving real-time performance following a\ncross-stack approach. These span model-, system- and hardware-level techniques,\nand their combination. Our findings provide illustrative examples of AI systems\nthat do not overburden mobile hardware, while also indicating how they can\nimprove inference accuracy. Moreover, we showcase how custom ASIC- and\nFPGA-based accelerators can be an enabling factor for next-generation AI\napplications, such as multi-DNN systems. Collectively, these results highlight\nthe critical need for further exploration as to how the various cross-stack\nsolutions can be best combined in order to bring the latest advances in deep\nlearning close to users, in a robust and efficient manner.",
          "link": "http://arxiv.org/abs/2106.15021",
          "publishedOn": "2021-06-30T02:01:02.340Z",
          "wordCount": 707,
          "title": "How to Reach Real-Time AI on Consumer Devices? Solutions for Programmable and Custom Architectures. (arXiv:2106.15021v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yongchan Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kunwoong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yongdai Kim</a>",
          "description": "In many classification problems, collecting massive clean-annotated data is\nnot easy, and thus a lot of researches have been done to handle data with noisy\nlabels. Most recent state-of-art solutions for noisy label problems are built\non the small-loss strategy which exploits the memorization effect. While it is\na powerful tool, the memorization effect has several drawbacks. The\nperformances are sensitive to the choice of a training epoch required for\nutilizing the memorization effect. In addition, when the labels are heavily\ncontaminated or imbalanced, the memorization effect may not occur in which case\nthe methods based on the small-loss strategy fail to identify clean labeled\ndata. We introduce a new method called INN(Integration with the Nearest\nNeighborhoods) to refine clean labeled data from training data with noisy\nlabels. The proposed method is based on a new discovery that a prediction\npattern at neighbor regions of clean labeled data is consistently different\nfrom that of noisy labeled data regardless of training epochs. The INN method\nrequires more computation but is much stable and powerful than the small-loss\nstrategy. By carrying out various experiments, we demonstrate that the INN\nmethod resolves the shortcomings in the memorization effect successfully and\nthus is helpful to construct more accurate deep prediction models with training\ndata with noisy labels.",
          "link": "http://arxiv.org/abs/2106.15185",
          "publishedOn": "2021-06-30T02:01:02.334Z",
          "wordCount": 663,
          "title": "INN: A Method Identifying Clean-annotated Samples via Consistency Effect in Deep Neural Networks. (arXiv:2106.15185v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spooner_T/0/1/0/all/0/1\">Thomas Spooner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dervovic_D/0/1/0/all/0/1\">Danial Dervovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1\">Jason Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shepard_J/0/1/0/all/0/1\">Jon Shepard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1\">Daniele Magazzeni</a>",
          "description": "We present a new method for counterfactual explanations (CFEs) based on\nBayesian optimisation that applies to both classification and regression\nmodels. Our method is a globally convergent search algorithm with support for\narbitrary regression models and constraints like feature sparsity and\nactionable recourse, and furthermore can answer multiple counterfactual\nquestions in parallel while learning from previous queries. We formulate CFE\nsearch for regression models in a rigorous mathematical framework using\ndifferentiable potentials, which resolves robustness issues in threshold-based\nobjectives. We prove that in this framework, (a) verifying the existence of\ncounterfactuals is NP-complete; and (b) that finding instances using such\npotentials is CLS-complete. We describe a unified algorithm for CFEs using a\nspecialised acquisition function that composes both expected improvement and an\nexponential-polynomial (EP) family with desirable properties. Our evaluation on\nreal-world benchmark domains demonstrate high sample-efficiency and precision.",
          "link": "http://arxiv.org/abs/2106.15212",
          "publishedOn": "2021-06-30T02:01:02.329Z",
          "wordCount": 584,
          "title": "Counterfactual Explanations for Arbitrary Regression Models. (arXiv:2106.15212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14956",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Turan_B/0/1/0/all/0/1\">Berkay Turan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Uribe_C/0/1/0/all/0/1\">Cesar A. Uribe</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wai_H/0/1/0/all/0/1\">Hoi-To Wai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Alizadeh_M/0/1/0/all/0/1\">Mahnoosh Alizadeh</a>",
          "description": "In this paper, we propose a first-order distributed optimization algorithm\nthat is provably robust to Byzantine failures-arbitrary and potentially\nadversarial behavior, where all the participating agents are prone to failure.\nWe model each agent's state over time as a two-state Markov chain that\nindicates Byzantine or trustworthy behaviors at different time instants. We set\nno restrictions on the maximum number of Byzantine agents at any given time. We\ndesign our method based on three layers of defense: 1) Temporal gradient\naveraging, 2) robust aggregation, and 3) gradient normalization. We study two\nsettings for stochastic optimization, namely Sample Average Approximation and\nStochastic Approximation, and prove that for strongly convex and smooth\nnon-convex cost functions, our algorithm achieves order-optimal statistical\nerror and convergence rates.",
          "link": "http://arxiv.org/abs/2106.14956",
          "publishedOn": "2021-06-30T02:01:02.324Z",
          "wordCount": 578,
          "title": "Robust Distributed Optimization With Randomly Corrupted Gradients. (arXiv:2106.14956v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15159",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Jayasundara_S/0/1/0/all/0/1\">Shyaman Jayasundara</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lokuge_S/0/1/0/all/0/1\">Sandali Lokuge</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ihalagedara_P/0/1/0/all/0/1\">Puwasuru Ihalagedara</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Herath_D/0/1/0/all/0/1\">Damayanthi Herath</a>",
          "description": "MicroRNAs (miRNAs) are endogenous small non-coding RNAs that play an\nimportant role in post-transcriptional gene regulation. However, the\nexperimental determination of miRNA sequence and structure is both expensive\nand time-consuming. Therefore, computational and machine learning-based\napproaches have been adopted to predict novel microRNAs. With the involvement\nof data science and machine learning in biology, multiple research studies have\nbeen conducted to find microRNAs with different computational methods and\ndifferent miRNA features. Multiple approaches are discussed in detail\nconsidering the learning algorithm/s used, features considered, dataset/s used\nand the criteria used in evaluations. This systematic review focuses on the\nmachine learning methods developed for miRNA identification in plants. This\nwill help researchers to gain a detailed idea about past studies and identify\nnovel paths that solve drawbacks occurred in past studies. Our findings\nhighlight the need for plant-specific computational methods for miRNA\nidentification.",
          "link": "http://arxiv.org/abs/2106.15159",
          "publishedOn": "2021-06-30T02:01:02.302Z",
          "wordCount": 577,
          "title": "Machine learning for plant microRNA prediction: A systematic review. (arXiv:2106.15159v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Attention Mechanism is a widely used method for improving the performance of\nconvolutional neural networks (CNNs) on computer vision tasks. Despite its\npervasiveness, we have a poor understanding of what its effectiveness stems\nfrom. It is popularly believed that its effectiveness stems from the visual\nattention explanation, advocating focusing on the important part of input data\nrather than ingesting the entire input. In this paper, we find that there is\nonly a weak consistency between the attention weights of features and their\nimportance. Instead, we verify the crucial role of feature map multiplication\nin attention mechanism and uncover a fundamental impact of feature map\nmultiplication on the learned landscapes of CNNs: with the high order\nnon-linearity brought by the feature map multiplication, it played a\nregularization role on CNNs, which made them learn smoother and more stable\nlandscapes near real samples compared to vanilla CNNs. This smoothness and\nstability induce a more predictive and stable behavior in-between real samples,\nand make CNNs generate better. Moreover, motivated by the proposed\neffectiveness of feature map multiplication, we design feature map\nmultiplication network (FMMNet) by simply replacing the feature map addition in\nResNet with feature map multiplication. FMMNet outperforms ResNet on various\ndatasets, and this indicates that feature map multiplication plays a vital role\nin improving the performance even without finely designed attention mechanism\nin existing methods.",
          "link": "http://arxiv.org/abs/2106.15067",
          "publishedOn": "2021-06-30T02:01:02.295Z",
          "wordCount": 662,
          "title": "Towards Understanding the Effectiveness of Attention Mechanism. (arXiv:2106.15067v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pucci_R/0/1/0/all/0/1\">Rita Pucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinel_N/0/1/0/all/0/1\">Niki Martinel</a>",
          "description": "Automatic image colourisation is the computer vision research path that\nstudies how to colourise greyscale images (for restoration). Deep learning\ntechniques improved image colourisation yielding astonishing results. These\ndiffer by various factors, such as structural differences, input types, user\nassistance, etc. Most of them, base the architectural structure on\nconvolutional layers with no emphasis on layers specialised in object features\nextraction. We introduce a novel downsampling upsampling architecture named\nTUCaN (Tiny UCapsNet) that exploits the collaboration of convolutional layers\nand capsule layers to obtain a neat colourisation of entities present in every\nsingle image. This is obtained by enforcing collaboration among such layers by\nskip and residual connections. We pose the problem as a per pixel colour\nclassification task that identifies colours as a bin in a quantized space. To\ntrain the network, in contrast with the standard end to end learning method, we\npropose the progressive learning scheme to extract the context of objects by\nonly manipulating the learning process without changing the model. In this\nscheme, the upsampling starts from the reconstruction of low resolution images\nand progressively grows to high resolution images throughout the training\nphase. Experimental results on three benchmark datasets show that our approach\nwith ImageNet10k dataset outperforms existing methods on standard quality\nmetrics and achieves state of the art performances on image colourisation. We\nperformed a user study to quantify the perceptual realism of the colourisation\nresults demonstrating: that progressive learning let the TUCaN achieve better\ncolours than the end to end scheme; and pointing out the limitations of the\nexisting evaluation metrics.",
          "link": "http://arxiv.org/abs/2106.15176",
          "publishedOn": "2021-06-30T02:01:02.285Z",
          "wordCount": 691,
          "title": "TUCaN: Progressively Teaching Colourisation to Capsules. (arXiv:2106.15176v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_F/0/1/0/all/0/1\">Fengli Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Unsupervised domain adaptation aims to transfer knowledge from a labeled\nsource domain to an unlabeled target domain. Previous methods focus on learning\ndomain-invariant features to decrease the discrepancy between the feature\ndistributions as well as minimizing the source error and have made remarkable\nprogress. However, a recently proposed theory reveals that such a strategy is\nnot sufficient for a successful domain adaptation. It shows that besides a\nsmall source error, both the discrepancy between the feature distributions and\nthe discrepancy between the labeling functions should be small across domains.\nThe discrepancy between the labeling functions is essentially the cross-domain\nerrors which are ignored by existing methods. To overcome this issue, in this\npaper, a novel method is proposed to integrate all the objectives into a\nunified optimization framework. Moreover, the incorrect pseudo labels widely\nused in previous methods can lead to error accumulation during learning. To\nalleviate this problem, the pseudo labels are obtained by utilizing structural\ninformation of the target domain besides source classifier and we propose a\ncurriculum learning based strategy to select the target samples with more\naccurate pseudo-labels during training. Comprehensive experiments are\nconducted, and the results validate that our approach outperforms\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.15057",
          "publishedOn": "2021-06-30T02:01:02.277Z",
          "wordCount": 632,
          "title": "Cross-domain error minimization for unsupervised domain adaptation. (arXiv:2106.15057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15123",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bak_T/0/1/0/all/0/1\">Taejun Bak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Jae-Sung Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_H/0/1/0/all/0/1\">Hanbin Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Ik Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1\">Hoon-Young Cho</a>",
          "description": "Methods for modeling and controlling prosody with acoustic features have been\nproposed for neural text-to-speech (TTS) models. Prosodic speech can be\ngenerated by conditioning acoustic features. However, synthesized speech with a\nlarge pitch-shift scale suffers from audio quality degradation, and speaker\ncharacteristics deformation. To address this problem, we propose a feed-forward\nTransformer based TTS model that is designed based on the source-filter theory.\nThis model, called FastPitchFormant, has a unique structure that handles text\nand acoustic features in parallel. With modeling each feature separately, the\ntendency that the model learns the relationship between two features can be\nmitigated.",
          "link": "http://arxiv.org/abs/2106.15123",
          "publishedOn": "2021-06-30T02:01:02.270Z",
          "wordCount": 554,
          "title": "FastPitchFormant: Source-filter based Decomposed Modeling for Speech Synthesis. (arXiv:2106.15123v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bjork_S/0/1/0/all/0/1\">Sara Bj&#xf6;rk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anfinsen_S/0/1/0/all/0/1\">Stian Normann Anfinsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naesset_E/0/1/0/all/0/1\">Erik N&#xe6;sset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gobakken_T/0/1/0/all/0/1\">Terje Gobakken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahabu_E/0/1/0/all/0/1\">Eliakimu Zahabu</a>",
          "description": "This paper studies construction of above-ground biomass (AGB) prediction maps\nfrom synthetic aperture radar (SAR) intensity images. The purpose is to improve\ntraditional regression models based on SAR intensity, trained with a limited\namount of AGB in situ measurements. Although it is costly to collect, data from\nairborne laser scanning (ALS) sensors are highly correlated with AGB.\nTherefore, we propose using AGB predictions based on ALS data as surrogate\nresponse variables for SAR data in a sequential modelling fashion. This\nincreases the amount of training data dramatically. To model the regression\nfunction between SAR intensity and ALS-predicted AGB we propose to utilise a\nconditional generative adversarial network (cGAN), i.e. the Pix2Pix\nconvolutional neural network. This enables the recreation of existing ALS-based\nAGB prediction maps. The generated synthesised ALS-based AGB predictions are\nevaluated qualitatively and quantitatively against ALS-based AGB predictions\nretrieved from a traditional non-sequential regression model trained in the\nsame area. Results show that the proposed architecture manages to capture\ncharacteristics of the actual data. This suggests that the use of ALS-guided\ngenerative models is a promising avenue for AGB prediction from SAR intensity.\nFurther research on this area has the potential of providing both large-scale\nand low-cost predictions of AGB.",
          "link": "http://arxiv.org/abs/2106.15020",
          "publishedOn": "2021-06-30T02:01:02.247Z",
          "wordCount": 658,
          "title": "Constructing Forest Biomass Prediction Maps from Radar Backscatter by Sequential Regression with a Conditional Generative Adversarial Network. (arXiv:2106.15020v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alloulah_M/0/1/0/all/0/1\">Mohammed Alloulah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Maximilian Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isopoussu_A/0/1/0/all/0/1\">Anton Isopoussu</a>",
          "description": "Autonomous navigation in uninstrumented and unprepared environments is a\nfundamental demand for next generation indoor and outdoor location-based\nservices. To bring about such ambition, a suite of collaborative sensing\nmodalities is required in order to sustain performance irrespective of\nchallenging dynamic conditions. Of the many modalities on offer, inertial\ntracking plays a key role under momentary unfavourable operational conditions\nowing to its independence of the surrounding environment. However, inertial\ntracking has traditionally (i) suffered from excessive error growth and (ii)\nrequired extensive and cumbersome tuning. Both of these issues have limited the\nappeal and utility of inertial tracking. In this paper, we present DIT: a novel\nDeep learning Inertial Tracking system that overcomes prior limitations;\nnamely, by (i) significantly reducing tracking drift and (ii) seamlessly\nconstructing robust and generalisable learned models. DIT describes two core\ncontributions: (i) DIT employs a robotic platform augmented with a mechanical\nslider subsystem that automatically samples inertial signal variabilities\narising from different sensor mounting geometries. We use the platform to\ncurate in-house a 7.2 million sample dataset covering an aggregate distance of\n21 kilometres split into 11 indexed sensor mounting geometries. (ii) DIT uses\ndeep learning, optimal transport, and domain adaptation (DA) to create a model\nwhich is robust to variabilities in sensor mounting geometry. The overall\nsystem synthesises high-performance and generalisable inertial navigation\nmodels in an end-to-end, robotic-learning fashion. In our evaluation, DIT\noutperforms an industrial-grade sensor fusion baseline by 10x (90th percentile)\nand a state-of-the-art adversarial DA technique by > 2.5x in performance (90th\npercentile) and >10x in training time.",
          "link": "http://arxiv.org/abs/2106.15178",
          "publishedOn": "2021-06-30T02:01:02.234Z",
          "wordCount": 690,
          "title": "Towards Generalisable Deep Inertial Tracking via Geometry-Aware Learning. (arXiv:2106.15178v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14999",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mummadi_C/0/1/0/all/0/1\">Chaithanya Kumar Mummadi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hutmacher_R/0/1/0/all/0/1\">Robin Hutmacher</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rambach_K/0/1/0/all/0/1\">Kilian Rambach</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Levinkov_E/0/1/0/all/0/1\">Evgeny Levinkov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Metzen_J/0/1/0/all/0/1\">Jan Hendrik Metzen</a>",
          "description": "Deep neural networks often exhibit poor performance on data that is unlikely\nunder the train-time data distribution, for instance data affected by\ncorruptions. Previous works demonstrate that test-time adaptation to data\nshift, for instance using entropy minimization, effectively improves\nperformance on such shifted distributions. This paper focuses on the fully\ntest-time adaptation setting, where only unlabeled data from the target\ndistribution is required. This allows adapting arbitrary pretrained networks.\nSpecifically, we propose a novel loss that improves test-time adaptation by\naddressing both premature convergence and instability of entropy minimization.\nThis is achieved by replacing the entropy by a non-saturating surrogate and\nadding a diversity regularizer based on batch-wise entropy maximization that\nprevents convergence to trivial collapsed solutions. Moreover, we propose to\nprepend an input transformation module to the network that can partially undo\ntest-time distribution shifts. Surprisingly, this preprocessing can be learned\nsolely using the fully test-time adaptation loss in an end-to-end fashion\nwithout any target domain labels or source domain data. We show that our\napproach outperforms previous work in improving the robustness of publicly\navailable pretrained image classifiers to common corruptions on such\nchallenging benchmarks as ImageNet-C.",
          "link": "http://arxiv.org/abs/2106.14999",
          "publishedOn": "2021-06-30T02:01:02.221Z",
          "wordCount": 642,
          "title": "Test-Time Adaptation to Distribution Shift by Confidence Maximization and Input Transformation. (arXiv:2106.14999v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15216",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Su_L/0/1/0/all/0/1\">Lili Su</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1\">Jiaming Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_P/0/1/0/all/0/1\">Pengkun Yang</a>",
          "description": "Federated Learning (FL) is a promising framework that has great potentials in\nprivacy preservation and in lowering the computation load at the cloud. FedAvg\nand FedProx are two widely adopted algorithms. However, recent work raised\nconcerns on these two methods: (1) their fixed points do not correspond to the\nstationary points of the original optimization problem, and (2) the common\nmodel found might not generalize well locally.\n\nIn this paper, we alleviate these concerns. Towards this, we adopt the\nstatistical learning perspective yet allow the distributions to be\nheterogeneous and the local data to be unbalanced. We show, in the general\nkernel regression setting, that both FedAvg and FedProx converge to the\nminimax-optimal error rates. Moreover, when the kernel function has a finite\nrank, the convergence is exponentially fast. Our results further analytically\nquantify the impact of the model heterogeneity and characterize the federation\ngain - the reduction of the estimation error for a worker to join the federated\nlearning compared to the best local estimator. To the best of our knowledge, we\nare the first to show the achievability of minimax error rates under FedAvg and\nFedProx, and the first to characterize the gains in joining FL. Numerical\nexperiments further corroborate our theoretical findings on the statistical\noptimality of FedAvg and FedProx and the federation gains.",
          "link": "http://arxiv.org/abs/2106.15216",
          "publishedOn": "2021-06-30T02:01:02.215Z",
          "wordCount": 664,
          "title": "Achieving Statistical Optimality of Federated Learning: Beyond Stationary Points. (arXiv:2106.15216v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15153",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jinhyeok Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Jae-Sung Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bak_T/0/1/0/all/0/1\">Taejun Bak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1\">Youngik Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1\">Hoon-Young Cho</a>",
          "description": "Recent advances in neural multi-speaker text-to-speech (TTS) models have\nenabled the generation of reasonably good speech quality with a single model\nand made it possible to synthesize the speech of a speaker with limited\ntraining data. Fine-tuning to the target speaker data with the multi-speaker\nmodel can achieve better quality, however, there still exists a gap compared to\nthe real speech sample and the model depends on the speaker. In this work, we\npropose GANSpeech, which is a high-fidelity multi-speaker TTS model that adopts\nthe adversarial training method to a non-autoregressive multi-speaker TTS\nmodel. In addition, we propose simple but efficient automatic scaling methods\nfor feature matching loss used in adversarial training. In the subjective\nlistening tests, GANSpeech significantly outperformed the baseline\nmulti-speaker FastSpeech and FastSpeech2 models, and showed a better MOS score\nthan the speaker-specific fine-tuned FastSpeech2.",
          "link": "http://arxiv.org/abs/2106.15153",
          "publishedOn": "2021-06-30T02:01:02.210Z",
          "wordCount": 595,
          "title": "GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis. (arXiv:2106.15153v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bryniarski_O/0/1/0/all/0/1\">Oliver Bryniarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hingun_N/0/1/0/all/0/1\">Nabeel Hingun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pachuca_P/0/1/0/all/0/1\">Pedro Pachuca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_V/0/1/0/all/0/1\">Vincent Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>",
          "description": "Evading adversarial example detection defenses requires finding adversarial\nexamples that must simultaneously (a) be misclassified by the model and (b) be\ndetected as non-adversarial. We find that existing attacks that attempt to\nsatisfy multiple simultaneous constraints often over-optimize against one\nconstraint at the cost of satisfying another. We introduce Orthogonal Projected\nGradient Descent, an improved attack technique to generate adversarial examples\nthat avoids this problem by orthogonalizing the gradients when running standard\ngradient-based attacks. We use our technique to evade four state-of-the-art\ndetection defenses, reducing their accuracy to 0% while maintaining a 0%\ndetection rate.",
          "link": "http://arxiv.org/abs/2106.15023",
          "publishedOn": "2021-06-30T02:01:02.175Z",
          "wordCount": 536,
          "title": "Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent. (arXiv:2106.15023v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rex Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazio_S/0/1/0/all/0/1\">Sarina A Fazio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanle Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramli_A/0/1/0/all/0/1\">Albara Ah Ramli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1\">Jason Yeates Adams</a>",
          "description": "With the development of the Internet of Things(IoT) and Artificial\nIntelligence(AI) technologies, human activity recognition has enabled various\napplications, such as smart homes and assisted living. In this paper, we target\na new healthcare application of human activity recognition, early mobility\nrecognition for Intensive Care Unit(ICU) patients. Early mobility is essential\nfor ICU patients who suffer from long-time immobilization. Our system includes\naccelerometer-based data collection from ICU patients and an AI model to\nrecognize patients' early mobility. To improve the model accuracy and\nstability, we identify features that are insensitive to sensor orientations and\npropose a segment voting process that leverages a majority voting strategy to\nrecognize each segment's activity. Our results show that our system improves\nmodel accuracy from 77.78\\% to 81.86\\% and reduces the model instability\n(standard deviation) from 16.69\\% to 6.92\\%, compared to the same AI model\nwithout our feature engineering and segment voting process.",
          "link": "http://arxiv.org/abs/2106.15017",
          "publishedOn": "2021-06-30T02:01:02.164Z",
          "wordCount": 593,
          "title": "Early Mobility Recognition for Intensive Care Unit Patients Using Accelerometers. (arXiv:2106.15017v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Rakhami_M/0/1/0/all/0/1\">Mabrook S. Al-Rakhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gumaei1_A/0/1/0/all/0/1\">Abdu Gumaei1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altaf_M/0/1/0/all/0/1\">Meteb Altaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1\">Mohammad Mehedi Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhamees_B/0/1/0/all/0/1\">Bader Fahad Alkhamees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_K/0/1/0/all/0/1\">Khan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortino_G/0/1/0/all/0/1\">Giancarlo Fortino</a>",
          "description": "Fall prevalence is high among elderly people, which is challenging due to the\nsevere consequences of falling. This is why rapid assistance is a critical\ntask. Ambient assisted living (AAL) uses recent technologies such as 5G\nnetworks and the internet of medical things (IoMT) to address this research\narea. Edge computing can reduce the cost of cloud communication, including high\nlatency and bandwidth use, by moving conventional healthcare services and\napplications closer to end-users. Artificial intelligence (AI) techniques such\nas deep learning (DL) have been used recently for automatic fall detection, as\nwell as supporting healthcare services. However, DL requires a vast amount of\ndata and substantial processing power to improve its performance for the IoMT\nlinked to the traditional edge computing environment. This research proposes an\neffective fall detection framework based on DL algorithms and mobile edge\ncomputing (MEC) within 5G wireless networks, the aim being to empower\nIoMT-based healthcare applications. We also propose the use of a deep gated\nrecurrent unit (DGRU) neural network to improve the accuracy of existing\nDL-based fall detection methods. DGRU has the advantage of dealing with\ntime-series IoMT data, and it can reduce the number of parameters and avoid the\nvanishing gradient problem. The experimental results on two public datasets\nshow that the DGRU model of the proposed framework achieves higher accuracy\nrates compared to the current related works on the same datasets.",
          "link": "http://arxiv.org/abs/2106.15049",
          "publishedOn": "2021-06-30T02:01:02.149Z",
          "wordCount": 680,
          "title": "FallDeF5: A Fall Detection Framework Using 5G-based Deep Gated Recurrent Unit Networks. (arXiv:2106.15049v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15002",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1\">Jonathan W. Siegel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1\">Jinchao Xu</a>",
          "description": "We consider the variation space corresponding to a dictionary of functions in\n$L^2(\\Omega)$ and present the basic theory of approximation in these spaces.\nSpecifically, we compare the definition based on integral representations with\nthe definition in terms of convex hulls. We show that in many cases, including\nthe dictionaries corresponding to shallow ReLU$^k$ networks and a dictionary of\ndecaying Fourier modes, that the two definitions coincide. We also give a\npartial characterization of the variation space for shallow ReLU$^k$ networks\nand show that the variation space with respect to the dictionary of decaying\nFourier modes corresponds to the Barron spectral space.",
          "link": "http://arxiv.org/abs/2106.15002",
          "publishedOn": "2021-06-30T02:01:02.109Z",
          "wordCount": 546,
          "title": "Characterization of the Variation Spaces Corresponding to Shallow Neural Networks. (arXiv:2106.15002v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14976",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1\">Yifei Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_H/0/1/0/all/0/1\">Hao-Hsuan Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhou Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jere_S/0/1/0/all/0/1\">Shashank Jere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1\">Lingjia Liu</a>",
          "description": "Due to the growing volume of data traffic produced by the surge of Internet\nof Things (IoT) devices, the demand for radio spectrum resources is approaching\ntheir limitation defined by Federal Communications Commission (FCC). To this\nend, Dynamic Spectrum Access (DSA) is considered as a promising technology to\nhandle this spectrum scarcity. However, standard DSA techniques often rely on\nanalytical modeling wireless networks, making its application intractable in\nunder-measured network environments. Therefore, utilizing neural networks to\napproximate the network dynamics is an alternative approach. In this article,\nwe introduce a Federated Learning (FL) based framework for the task of DSA,\nwhere FL is a distributive machine learning framework that can reserve the\nprivacy of network terminals under heterogeneous data distributions. We discuss\nthe opportunities, challenges, and opening problems of this framework. To\nevaluate its feasibility, we implement a Multi-Agent Reinforcement Learning\n(MARL)-based FL as a realization associated with its initial evaluation\nresults.",
          "link": "http://arxiv.org/abs/2106.14976",
          "publishedOn": "2021-06-30T02:01:02.101Z",
          "wordCount": 584,
          "title": "Federated Dynamic Spectrum Access. (arXiv:2106.14976v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1\">Alexander W. Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellnhofer_P/0/1/0/all/0/1\">Petr Kellnhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>",
          "description": "Novel view synthesis is a long-standing problem in machine learning and\ncomputer vision. Significant progress has recently been made in developing\nneural scene representations and rendering techniques that synthesize\nphotorealistic images from arbitrary views. These representations, however, are\nextremely slow to train and often also slow to render. Inspired by neural\nvariants of image-based rendering, we develop a new neural rendering approach\nwith the goal of quickly learning a high-quality representation which can also\nbe rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a\nunique combination of a neural shape representation and 2D CNN-based image\nfeature extraction, aggregation, and re-projection. To push representation\nconvergence times down to minutes, we leverage meta learning to learn neural\nshape and image feature priors which accelerate training. The optimized shape\nand image features can then be extracted using traditional graphics techniques\nand rendered in real time. We show that MetaNLR++ achieves similar or better\nnovel view synthesis results in a fraction of the time that competing methods\nrequire.",
          "link": "http://arxiv.org/abs/2106.14942",
          "publishedOn": "2021-06-30T02:01:02.063Z",
          "wordCount": 616,
          "title": "Fast Training of Neural Lumigraph Representations using Meta Learning. (arXiv:2106.14942v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stoger_D/0/1/0/all/0/1\">Dominik St&#xf6;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>",
          "description": "Recently there has been significant theoretical progress on understanding the\nconvergence and generalization of gradient-based methods on nonconvex losses\nwith overparameterized models. Nevertheless, many aspects of optimization and\ngeneralization and in particular the critical role of small random\ninitialization are not fully understood. In this paper, we take a step towards\ndemystifying this role by proving that small random initialization followed by\na few iterations of gradient descent behaves akin to popular spectral methods.\nWe also show that this implicit spectral bias from small random initialization,\nwhich is provably more prominent for overparameterized models, also puts the\ngradient descent iterations on a particular trajectory towards solutions that\nare not only globally optimal but also generalize well. Concretely, we focus on\nthe problem of reconstructing a low-rank matrix from a few measurements via a\nnatural nonconvex formulation. In this setting, we show that the trajectory of\nthe gradient descent iterations from small random initialization can be\napproximately decomposed into three phases: (I) a spectral or alignment phase\nwhere we show that that the iterates have an implicit spectral bias akin to\nspectral initialization allowing us to show that at the end of this phase the\ncolumn space of the iterates and the underlying low-rank matrix are\nsufficiently aligned, (II) a saddle avoidance/refinement phase where we show\nthat the trajectory of the gradient iterates moves away from certain degenerate\nsaddle points, and (III) a local refinement phase where we show that after\navoiding the saddles the iterates converge quickly to the underlying low-rank\nmatrix. Underlying our analysis are insights for the analysis of\noverparameterized nonconvex optimization schemes that may have implications for\ncomputational problems beyond low-rank reconstruction.",
          "link": "http://arxiv.org/abs/2106.15013",
          "publishedOn": "2021-06-30T02:01:02.030Z",
          "wordCount": 743,
          "title": "Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction. (arXiv:2106.15013v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zihao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chilin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaolu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Face recognition is greatly improved by deep convolutional neural networks\n(CNNs). Recently, these face recognition models have been used for identity\nauthentication in security sensitive applications. However, deep CNNs are\nvulnerable to adversarial patches, which are physically realizable and\nstealthy, raising new security concerns on the real-world applications of these\nmodels. In this paper, we evaluate the robustness of face recognition models\nusing adversarial patches based on transferability, where the attacker has\nlimited accessibility to the target models. First, we extend the existing\ntransfer-based attack techniques to generate transferable adversarial patches.\nHowever, we observe that the transferability is sensitive to initialization and\ndegrades when the perturbation magnitude is large, indicating the overfitting\nto the substitute models. Second, we propose to regularize the adversarial\npatches on the low dimensional data manifold. The manifold is represented by\ngenerative models pre-trained on legitimate human face images. Using face-like\nfeatures as adversarial perturbations through optimization on the manifold, we\nshow that the gaps between the responses of substitute models and the target\nmodels dramatically decrease, exhibiting a better transferability. Extensive\ndigital world experiments are conducted to demonstrate the superiority of the\nproposed method in the black-box setting. We apply the proposed method in the\nphysical world as well.",
          "link": "http://arxiv.org/abs/2106.15058",
          "publishedOn": "2021-06-30T02:01:01.998Z",
          "wordCount": 673,
          "title": "Improving Transferability of Adversarial Patches on Face Recognition with Generative Models. (arXiv:2106.15058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15108",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Bae_Y/0/1/0/all/0/1\">Youngkyoung Bae</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Kyum Kim</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Jeong_H/0/1/0/all/0/1\">Hawoong Jeong</a>",
          "description": "Quantifying entropy production (EP) is essential to understand stochastic\nsystems at mesoscopic scales, such as living organisms or biological\nassemblies. However, without tracking the relevant variables, it is challenging\nto figure out where and to what extent EP occurs from recorded time-series\nimage data from experiments. Here, applying a convolutional neural network\n(CNN), a powerful tool for image processing, we develop an estimation method\nfor EP through an unsupervised learning algorithm that calculates only from\nmovies. Together with an attention map of the CNN's last layer, our method can\nnot only quantify stochastic EP but also produce the spatiotemporal pattern of\nthe EP (dissipation map). We show that our method accurately measures the EP\nand creates a dissipation map in two nonequilibrium systems, the bead-spring\nmodel and a network of elastic filaments. We further confirm high performance\neven with noisy, low spatial resolution data, and partially observed\nsituations. Our method will provide a practical way to obtain dissipation maps\nand ultimately contribute to uncovering the nonequilibrium nature of complex\nsystems.",
          "link": "http://arxiv.org/abs/2106.15108",
          "publishedOn": "2021-06-30T02:01:01.974Z",
          "wordCount": 626,
          "title": "Attaining entropy production and dissipation maps from Brownian movies via neural networks. (arXiv:2106.15108v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McDonald_A/0/1/0/all/0/1\">Andrew McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1\">Vaibhav Srivastava</a>",
          "description": "Heterogeneous multi-robot sensing systems are able to characterize physical\nprocesses more comprehensively than homogeneous systems. Access to multiple\nmodalities of sensory data allow such systems to fuse information between\ncomplementary sources and learn richer representations of a phenomenon of\ninterest. Often, these data are correlated but vary in fidelity, i.e., accuracy\n(bias) and precision (noise). Low-fidelity data may be more plentiful, while\nhigh-fidelity data may be more trustworthy. In this paper, we address the\nproblem of multi-robot online estimation and coverage control by combining low-\nand high-fidelity data to learn and cover a sensory function of interest. We\npropose two algorithms for this task of heterogeneous learning and coverage --\nnamely Stochastic Sequencing of Multi-fidelity Learning and Coverage (SMLC) and\nDeterministic Sequencing of Multi-fidelity Learning and Coverage (DMLC) -- and\nprove that they converge asymptotically. In addition, we demonstrate the\nempirical efficacy of SMLC and DMLC through numerical simulations.",
          "link": "http://arxiv.org/abs/2106.14984",
          "publishedOn": "2021-06-30T02:01:01.969Z",
          "wordCount": 603,
          "title": "Online Estimation and Coverage Control with Heterogeneous Sensing Information. (arXiv:2106.14984v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hron_J/0/1/0/all/0/1\">Jiri Hron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1\">Karl Krauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1\">Niki Kilbertus</a>",
          "description": "Thanks to their scalability, two-stage recommenders are used by many of\ntoday's largest online platforms, including YouTube, LinkedIn, and Pinterest.\nThese systems produce recommendations in two steps: (i) multiple nominators --\ntuned for low prediction latency -- preselect a small subset of candidates from\nthe whole item pool; (ii)~a slower but more accurate ranker further narrows\ndown the nominated items, and serves to the user. Despite their popularity, the\nliterature on two-stage recommenders is relatively scarce, and the algorithms\nare often treated as the sum of their parts. Such treatment presupposes that\nthe two-stage performance is explained by the behavior of individual components\nif they were deployed independently. This is not the case: using synthetic and\nreal-world data, we demonstrate that interactions between the ranker and the\nnominators substantially affect the overall performance. Motivated by these\nfindings, we derive a generalization lower bound which shows that careful\nchoice of each nominator's training set is sometimes the only difference\nbetween a poor and an optimal two-stage recommender. Since searching for a good\nchoice manually is difficult, we learn one instead. In particular, using a\nMixture-of-Experts approach, we train the nominators (experts) to specialize on\ndifferent subsets of the item pool. This significantly improves performance.",
          "link": "http://arxiv.org/abs/2106.14979",
          "publishedOn": "2021-06-30T02:01:01.947Z",
          "wordCount": 641,
          "title": "On component interactions in two-stage recommender systems. (arXiv:2106.14979v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14997",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1\">Jonathan W. Siegel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1\">Jinchao Xu</a>",
          "description": "We consider the approximation rates of shallow neural networks with respect\nto the variation norm. Upper bounds on these rates have been established for\nsigmoidal and ReLU activation functions, but it has remained an important open\nproblem whether these rates are sharp. In this article, we provide a solution\nto this problem by proving sharp lower bounds on the approximation rates for\nshallow neural networks, which are obtained by lower bounding the $L^2$-metric\nentropy of the convex hull of the neural network basis functions. In addition,\nour methods also give sharp lower bounds on the Kolmogorov $n$-widths of this\nconvex hull, which show that the variation spaces corresponding to shallow\nneural networks cannot be efficiently approximated by linear methods. These\nlower bounds apply to both sigmoidal activation functions with bounded\nvariation and to activation functions which are a power of the ReLU. Our\nresults also quantify how much stronger the Barron spectral norm is than the\nvariation norm and, combined with previous results, give the asymptotics of the\n$L^\\infty$-metric entropy up to logarithmic factors in the case of the ReLU\nactivation function.",
          "link": "http://arxiv.org/abs/2106.14997",
          "publishedOn": "2021-06-30T02:01:01.941Z",
          "wordCount": 639,
          "title": "Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks. (arXiv:2106.14997v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wever_F/0/1/0/all/0/1\">Fiorella Wever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_T/0/1/0/all/0/1\">T. Anderson Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_V/0/1/0/all/0/1\">Victor Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Symul_L/0/1/0/all/0/1\">Laura Symul</a>",
          "description": "High levels of sparsity and strong class imbalance are ubiquitous challenges\nthat are often presented simultaneously in real-world time series data. While\nmost methods tackle each problem separately, our proposed approach handles both\nin conjunction, while imposing fewer assumptions on the data. In this work, we\npropose leveraging a self-supervised learning method, specifically\nAutoregressive Predictive Coding (APC), to learn relevant hidden\nrepresentations of time series data in the context of both missing data and\nclass imbalance. We apply APC using either a GRU or GRU-D encoder on two\nreal-world datasets, and show that applying one-step-ahead prediction with APC\nimproves the classification results in all settings. In fact, by applying GRU-D\n- APC, we achieve state-of-the-art AUPRC results on the Physionet benchmark.",
          "link": "http://arxiv.org/abs/2106.15577",
          "publishedOn": "2021-06-30T02:01:01.929Z",
          "wordCount": 597,
          "title": "As easy as APC: Leveraging self-supervised learning in the context of time series classification with varying levels of sparsity and severe class imbalance. (arXiv:2106.15577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_C/0/1/0/all/0/1\">Carrie Lu Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jian Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianming Yang</a>",
          "description": "Deep learning models for human activity recognition (HAR) based on sensor\ndata have been heavily studied recently. However, the generalization ability of\ndeep models on complex real-world HAR data is limited by the availability of\nhigh-quality labeled activity data, which are hard to obtain. In this paper, we\ndesign a similarity embedding neural network that maps input sensor signals\nonto real vectors through carefully designed convolutional and LSTM layers. The\nembedding network is trained with a pairwise similarity loss, encouraging the\nclustering of samples from the same class in the embedded real space, and can\nbe effectively trained on a small dataset and even on a noisy dataset with\nmislabeled samples. Based on the learned embeddings, we further propose both\nnonparametric and parametric approaches for activity recognition. Extensive\nevaluation based on two public datasets has shown that the proposed similarity\nembedding network significantly outperforms state-of-the-art deep models on HAR\nclassification tasks, is robust to mislabeled samples in the training set, and\ncan also be used to effectively denoise a noisy dataset.",
          "link": "http://arxiv.org/abs/2106.15283",
          "publishedOn": "2021-06-30T02:01:01.847Z",
          "wordCount": 626,
          "title": "Similarity Embedding Networks for Robust Human Activity Recognition. (arXiv:2106.15283v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dogaru_R/0/1/0/all/0/1\">Radu Dogaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogaru_I/0/1/0/all/0/1\">Ioana Dogaru</a>",
          "description": "Light binary convolutional neural networks (LB-CNN) are particularly useful\nwhen implemented in low-energy computing platforms as required in many\nindustrial applications. Herein, a framework for optimizing compact LB-CNN is\nintroduced and its effectiveness is evaluated. The framework is freely\navailable and may run on free-access cloud platforms, thus requiring no major\ninvestments. The optimized model is saved in the standardized .h5 format and\ncan be used as input to specialized tools for further deployment into specific\ntechnologies, thus enabling the rapid development of various intelligent image\nsensors. The main ingredient in accelerating the optimization of our model,\nparticularly the selection of binary convolution kernels, is the Chainer/Cupy\nmachine learning library offering significant speed-ups for training the output\nlayer as an extreme-learning machine. Additional training of the output layer\nusing Keras/Tensorflow is included, as it allows an increase in accuracy.\nResults for widely used datasets including MNIST, GTSRB, ORL, VGG show very\ngood compromise between accuracy and complexity. Particularly, for face\nrecognition problems a carefully optimized LB-CNN model provides up to 100%\naccuracies. Such TinyML solutions are well suited for industrial applications\nrequiring image recognition with low energy consumption.",
          "link": "http://arxiv.org/abs/2106.15350",
          "publishedOn": "2021-06-30T02:01:01.731Z",
          "wordCount": 657,
          "title": "LB-CNN: An Open Source Framework for Fast Training of Light Binary Convolutional Neural Networks using Chainer and Cupy. (arXiv:2106.15350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aliyev_N/0/1/0/all/0/1\">Namig Aliyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezer_O/0/1/0/all/0/1\">Oguzhan Sezer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzel_M/0/1/0/all/0/1\">Mehmet Turan Guzel</a>",
          "description": "Autonomous systems require identifying the environment and it has a long way\nto go before putting it safely into practice. In autonomous driving systems,\nthe detection of obstacles and traffic lights are of importance as well as lane\ntracking. In this study, an autonomous driving system is developed and tested\nin the experimental environment designed for this purpose. In this system, a\nmodel vehicle having a camera is used to trace the lanes and avoid obstacles to\nexperimentally study autonomous driving behavior. Convolutional Neural Network\nmodels were trained for Lane tracking. For the vehicle to avoid obstacles,\ncorner detection, optical flow, focus of expansion, time to collision, balance\ncalculation, and decision mechanism were created, respectively.",
          "link": "http://arxiv.org/abs/2106.15274",
          "publishedOn": "2021-06-30T02:01:01.706Z",
          "wordCount": 573,
          "title": "Autonomous Driving Implementation in an Experimental Environment. (arXiv:2106.15274v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1\">Kevin Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Han-Wei Shen</a>",
          "description": "In the past decades, many graph drawing techniques have been proposed for\ngenerating aesthetically pleasing graph layouts. However, it remains a\nchallenging task since different layout methods tend to highlight different\ncharacteristics of the graphs. Recently, studies on deep learning based graph\ndrawing algorithm have emerged but they are often not generalizable to\narbitrary graphs without re-training. In this paper, we propose a Convolutional\nGraph Neural Network based deep learning framework, DeepGD, which can draw\narbitrary graphs once trained. It attempts to generate layouts by compromising\namong multiple pre-specified aesthetics considering a good graph layout usually\ncomplies with multiple aesthetics simultaneously. In order to balance the\ntrade-off, we propose two adaptive training strategies which adjust the weight\nfactor of each aesthetic dynamically during training. The quantitative and\nqualitative assessment of DeepGD demonstrates that it is capable of drawing\narbitrary graphs effectively, while being flexible at accommodating different\naesthetic criteria.",
          "link": "http://arxiv.org/abs/2106.15347",
          "publishedOn": "2021-06-30T02:01:01.700Z",
          "wordCount": 582,
          "title": "DeepGD: A Deep Learning Framework for Graph Drawing Using GNN. (arXiv:2106.15347v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>",
          "description": "We show that cascaded diffusion models are capable of generating high\nfidelity images on the class-conditional ImageNet generation challenge, without\nany assistance from auxiliary image classifiers to boost sample quality. A\ncascaded diffusion model comprises a pipeline of multiple diffusion models that\ngenerate images of increasing resolution, beginning with a standard diffusion\nmodel at the lowest resolution, followed by one or more super-resolution\ndiffusion models that successively upsample the image and add higher resolution\ndetails. We find that the sample quality of a cascading pipeline relies\ncrucially on conditioning augmentation, our proposed method of data\naugmentation of the lower resolution conditioning inputs to the\nsuper-resolution models. Our experiments show that conditioning augmentation\nprevents compounding error during sampling in a cascaded model, helping us to\ntrain cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at\n128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep.",
          "link": "http://arxiv.org/abs/2106.15282",
          "publishedOn": "2021-06-30T02:01:01.686Z",
          "wordCount": 593,
          "title": "Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "In recent years, the growth of Machine Learning algorithms in a variety of\ndifferent applications has raised numerous studies on the applicability of\nthese algorithms in real scenarios. Among all, one of the hardest scenarios,\ndue to its physical requirements, is the aerospace one. In this context, the\nauthors of this work aim to propose a first prototype and a study of\nfeasibility for an AI model to be 'loaded' on board. As a case study, the\nauthors decided to investigate the detection of volcanic eruptions as a method\nto swiftly produce alerts. Two Convolutional Neural Networks have been proposed\nand created, also showing how to correctly implement them on real hardware and\nhow the complexity of a CNN can be adapted to fit computational requirements.",
          "link": "http://arxiv.org/abs/2106.15281",
          "publishedOn": "2021-06-30T02:01:01.671Z",
          "wordCount": 597,
          "title": "On-board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kashi_M/0/1/0/all/0/1\">Mohammad Amin Kashi</a>",
          "description": "Depth perception is fundamental for robots to understand the surrounding\nenvironment. As the view of cognitive neuroscience, visual depth perception\nmethods are divided into three categories, namely binocular, active, and\npictorial. The first two categories have been studied for decades in detail.\nHowever, research for the exploration of the third category is still in its\ninfancy and has got momentum by the advent of deep learning methods in recent\nyears. In cognitive neuroscience, it is known that pictorial depth perception\nmechanisms are dependent on the perception of seen objects. Inspired by this\nfact, in this thesis, we investigated the relation of perception of objects and\ndepth estimation convolutional neural networks. For this purpose, we developed\nnew network structures based on a simple depth estimation network that only\nused a single image at its input. Our proposed structures use both an image and\na semantic label of the image as their input. We used semantic labels as the\noutput of object perception. The obtained results of performance comparison\nbetween the developed network and original network showed that our novel\nstructures can improve the performance of depth estimation by 52\\% of relative\nerror of distance in the examined cases. Most of the experimental studies were\ncarried out on synthetic datasets that were generated by game engines to\nisolate the performance comparison from the effect of inaccurate depth and\nsemantic labels of non-synthetic datasets. It is shown that particular\nsynthetic datasets may be used for training of depth networks in cases that an\nappropriate dataset is not available. Furthermore, we showed that in these\ncases, usage of semantic labels improves the robustness of the network against\ndomain shift from synthetic training data to non-synthetic test data.",
          "link": "http://arxiv.org/abs/2106.15257",
          "publishedOn": "2021-06-30T02:01:01.665Z",
          "wordCount": 748,
          "title": "Predicting Depth from Semantic Segmentation using Game Engine Dataset. (arXiv:2106.15257v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1\">Hannaneh Barahouei Pasandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1\">Tamer Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1\">Hadi Amirpour</a>",
          "description": "Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency\nhave promised to increase data throughput by allowing concurrent communication\nbetween one Access Point and multiple users. However, we are still a long way\nfrom enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry\napplications such as video streaming in practical WiFi network settings due to\nheterogeneous channel conditions and devices, unreliable transmissions, and\nlack of useful feedback exchange among the lower and upper layers'\nrequirements. This paper introduces MuViS, a novel dual-phase optimization\nframework that proposes a Quality of Experience (QoE) aware MU-MIMO\noptimization for multi-user video streaming over IEEE 802.11ac. MuViS first\nemploys reinforcement learning to optimize the MU-MIMO user group and mode\nselection for users based on their PHY/MAC layer characteristics. The video\nbitrate is then optimized based on the user's mode (Multi-User (MU) or\nSingle-User (SU)). We present our design and its evaluation on smartphones and\nlaptops using 802.11ac WiFi. Our experimental results in various indoor\nenvironments and configurations show a scalable framework that can support a\nlarge number of users with streaming at high video rates and satisfying QoE\nrequirements.",
          "link": "http://arxiv.org/abs/2106.15262",
          "publishedOn": "2021-06-30T02:01:01.641Z",
          "wordCount": 632,
          "title": "MuViS: Online MU-MIMO Grouping for Multi-User Applications Over Commodity WiFi. (arXiv:2106.15262v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Heinrich Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>",
          "description": "Self-supervised contrastive representation learning has proved incredibly\nsuccessful in the vision and natural language domains, enabling\nstate-of-the-art performance with orders of magnitude less labeled data.\nHowever, such methods are domain-specific and little has been done to leverage\nthis technique on real-world tabular datasets. We propose SCARF, a simple,\nwidely-applicable technique for contrastive learning, where views are formed by\ncorrupting a random subset of features. When applied to pre-train deep neural\nnetworks on the 69 real-world, tabular classification datasets from the\nOpenML-CC18 benchmark, SCARF not only improves classification accuracy in the\nfully-supervised setting but does so also in the presence of label noise and in\nthe semi-supervised setting where only a fraction of the available training\ndata is labeled. We show that SCARF complements existing strategies and\noutperforms alternatives like autoencoders. We conduct comprehensive ablations,\ndetailing the importance of a range of factors.",
          "link": "http://arxiv.org/abs/2106.15147",
          "publishedOn": "2021-06-30T02:01:01.606Z",
          "wordCount": 575,
          "title": "SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption. (arXiv:2106.15147v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_A/0/1/0/all/0/1\">Ananth Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathioudakis_M/0/1/0/all/0/1\">Michael Mathioudakis</a>",
          "description": "Machine unlearning is the task of updating machine learning (ML) models after\na subset of the training data they were trained on is deleted. Methods for the\ntask are desired to combine effectiveness and efficiency, i.e., they should\neffectively \"unlearn\" deleted data, but in a way that does not require\nexcessive computation effort (e.g., a full retraining) for a small amount of\ndeletions. Such a combination is typically achieved by tolerating some amount\nof approximation in the unlearning. In addition, laws and regulations in the\nspirit of \"the right to be forgotten\" have given rise to requirements for\ncertifiability, i.e., the ability to demonstrate that the deleted data has\nindeed been unlearned by the ML model.\n\nIn this paper, we present an experimental study of the three state-of-the-art\napproximate unlearning methods for linear models and demonstrate the trade-offs\nbetween efficiency, effectiveness and certifiability offered by each method. In\nimplementing the study, we extend some of the existing works and describe a\ncommon ML pipeline to compare and evaluate the unlearning methods on six\nreal-world datasets and a variety of settings. We provide insights into the\neffect of the quantity and distribution of the deleted data on ML models and\nthe performance of each unlearning method in different settings. We also\npropose a practical online strategy to determine when the accumulated error\nfrom approximate unlearning is large enough to warrant a full retrain of the ML\nmodel.",
          "link": "http://arxiv.org/abs/2106.15093",
          "publishedOn": "2021-06-30T02:01:01.591Z",
          "wordCount": 661,
          "title": "Certifiable Machine Unlearning for Linear Models. (arXiv:2106.15093v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1\">Vladimir Braverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1\">Avinatan Hassidim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1\">Yossi Matias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schain_M/0/1/0/all/0/1\">Mariano Schain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1\">Sandeep Silwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Samson Zhou</a>",
          "description": "In this paper, we introduce adversarially robust streaming algorithms for\ncentral machine learning and algorithmic tasks, such as regression and\nclustering, as well as their more general counterparts, subspace embedding,\nlow-rank approximation, and coreset construction. For regression and other\nnumerical linear algebra related tasks, we consider the row arrival streaming\nmodel. Our results are based on a simple, but powerful, observation that many\nimportance sampling-based algorithms give rise to adversarial robustness which\nis in contrast to sketching based algorithms, which are very prevalent in the\nstreaming literature but suffer from adversarial attacks. In addition, we show\nthat the well-known merge and reduce paradigm in streaming is adversarially\nrobust. Since the merge and reduce paradigm allows coreset constructions in the\nstreaming setting, we thus obtain robust algorithms for $k$-means, $k$-median,\n$k$-center, Bregman clustering, projective clustering, principal component\nanalysis (PCA) and non-negative matrix factorization. To the best of our\nknowledge, these are the first adversarially robust results for these problems\nyet require no new algorithmic implementations. Finally, we empirically confirm\nthe robustness of our algorithms on various adversarial attacks and demonstrate\nthat by contrast, some common existing algorithms are not robust.\n\n(Abstract shortened to meet arXiv limits)",
          "link": "http://arxiv.org/abs/2106.14952",
          "publishedOn": "2021-06-30T02:01:01.573Z",
          "wordCount": 634,
          "title": "Adversarial Robustness of Streaming Algorithms through Importance Sampling. (arXiv:2106.14952v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangzhe Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "Molecular graph generation is a fundamental but challenging task in various\napplications such as drug discovery and material science, which requires\ngenerating valid molecules with desired properties. Auto-regressive models,\nwhich usually construct graphs following sequential actions of adding nodes and\nedges at the atom-level, have made rapid progress in recent years. However,\nthese atom-level models ignore high-frequency subgraphs that not only capture\nthe regularities of atomic combination in molecules but also are often related\nto desired chemical properties. In this paper, we propose a method to\nautomatically discover such common substructures, which we call {\\em graph\npieces}, from given molecular graphs. Based on graph pieces, we leverage a\nvariational autoencoder to generate molecules in two phases: piece-level graph\ngeneration followed by bond completion. Experiments show that our graph piece\nvariational autoencoder achieves better performance over state-of-the-art\nbaselines on property optimization and constrained property optimization tasks\nwith higher computational efficiency.",
          "link": "http://arxiv.org/abs/2106.15098",
          "publishedOn": "2021-06-30T02:01:01.567Z",
          "wordCount": 589,
          "title": "GraphPiece: Efficiently Generating High-Quality Molecular Graph with Substructures. (arXiv:2106.15098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blanco_Mulero_D/0/1/0/all/0/1\">David Blanco-Mulero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1\">Markus Heinonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1\">Ville Kyrki</a>",
          "description": "Graph Gaussian Processes (GGPs) provide a data-efficient solution on graph\nstructured domains. Existing approaches have focused on static structures,\nwhereas many real graph data represent a dynamic structure, limiting the\napplications of GGPs. To overcome this we propose evolving-Graph Gaussian\nProcesses (e-GGPs). The proposed method is capable of learning the transition\nfunction of graph vertices over time with a neighbourhood kernel to model the\nconnectivity and interaction changes between vertices. We assess the\nperformance of our method on time-series regression problems where graphs\nevolve over time. We demonstrate the benefits of e-GGPs over static graph\nGaussian Process approaches.",
          "link": "http://arxiv.org/abs/2106.15127",
          "publishedOn": "2021-06-30T02:01:01.552Z",
          "wordCount": 533,
          "title": "Evolving-Graph Gaussian Processes. (arXiv:2106.15127v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14947",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fabian_Z/0/1/0/all/0/1\">Zalan Fabian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heckel_R/0/1/0/all/0/1\">Reinhard Heckel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>",
          "description": "Deep neural networks have emerged as very successful tools for image\nrestoration and reconstruction tasks. These networks are often trained\nend-to-end to directly reconstruct an image from a noisy or corrupted\nmeasurement of that image. To achieve state-of-the-art performance, training on\nlarge and diverse sets of images is considered critical. However, it is often\ndifficult and/or expensive to collect large amounts of training images.\nInspired by the success of Data Augmentation (DA) for classification problems,\nin this paper, we propose a pipeline for data augmentation for accelerated MRI\nreconstruction and study its effectiveness at reducing the required training\ndata in a variety of settings. Our DA pipeline, MRAugment, is specifically\ndesigned to utilize the invariances present in medical imaging measurements as\nnaive DA strategies that neglect the physics of the problem fail. Through\nextensive studies on multiple datasets we demonstrate that in the low-data\nregime DA prevents overfitting and can match or even surpass the state of the\nart while using significantly fewer training data, whereas in the high-data\nregime it has diminishing returns. Furthermore, our findings show that DA can\nimprove the robustness of the model against various shifts in the test\ndistribution.",
          "link": "http://arxiv.org/abs/2106.14947",
          "publishedOn": "2021-06-30T02:01:01.540Z",
          "wordCount": 663,
          "title": "Data augmentation for deep learning based accelerated MRI reconstruction with limited data. (arXiv:2106.14947v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shihong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huishuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Balancing exploration and exploitation (EE) is a fundamental problem in\ncontex-tual bandit. One powerful principle for EE trade-off isOptimism in Face\nof Uncer-tainty(OFU), in which the agent takes the action according to an upper\nconfidencebound (UCB) of reward. OFU has achieved (near-)optimal regret bound\nfor lin-ear/kernel contextual bandits. However, it is in general unknown how to\nderiveefficient and effective EE trade-off methods for non-linearcomplex tasks,\nsuchas contextual bandit with deep neural network as the reward function. In\nthispaper, we propose a novel OFU algorithm namedregularized OFU(ROFU). InROFU,\nwe measure the uncertainty of the reward by a differentiable function\nandcompute the upper confidence bound by solving a regularized optimization\nprob-lem. We prove that, for multi-armed bandit, kernel contextual bandit and\nneuraltangent kernel bandit, ROFU achieves (near-)optimal regret bounds with\ncertainuncertainty measure, which theoretically justifies its effectiveness on\nEE trade-off.Importantly, ROFU admits a very efficient implementation with\ngradient-basedoptimizer, which easily extends to general deep neural network\nmodels beyondneural tangent kernel, in sharp contrast with previous OFU\nmethods. The em-pirical evaluation demonstrates that ROFU works extremelywell\nfor contextualbandits under various settings.",
          "link": "http://arxiv.org/abs/2106.15128",
          "publishedOn": "2021-06-30T02:01:01.532Z",
          "wordCount": 619,
          "title": "Regularized OFU: an Efficient UCB Estimator forNon-linear Contextual Bandit. (arXiv:2106.15128v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15133",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Iwata_T/0/1/0/all/0/1\">Tomoharu Iwata</a>",
          "description": "We propose a method that meta-learns a knowledge on matrix factorization from\nvarious matrices, and uses the knowledge for factorizing unseen matrices. The\nproposed method uses a neural network that takes a matrix as input, and\ngenerates prior distributions of factorized matrices of the given matrix. The\nneural network is meta-learned such that the expected imputation error is\nminimized when the factorized matrices are adapted to each matrix by a maximum\na posteriori (MAP) estimation. We use a gradient descent method for the MAP\nestimation, which enables us to backpropagate the expected imputation error\nthrough the gradient descent steps for updating neural network parameters since\neach gradient descent step is written in a closed form and is differentiable.\nThe proposed method can meta-learn from matrices even when their rows and\ncolumns are not shared, and their sizes are different from each other. In our\nexperiments with three user-item rating datasets, we demonstrate that our\nproposed method can impute the missing values from a limited number of\nobservations in unseen matrices after being trained with different matrices.",
          "link": "http://arxiv.org/abs/2106.15133",
          "publishedOn": "2021-06-30T02:01:01.527Z",
          "wordCount": 610,
          "title": "Meta-learning for Matrix Factorization without Shared Rows or Columns. (arXiv:2106.15133v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hailong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Renshuai Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yufei Ge</a>",
          "description": "Learning from multiple annotators aims to induce a high-quality classifier\nfrom training instances, where each of them is associated with a set of\npossibly noisy labels provided by multiple annotators under the influence of\ntheir varying abilities and own biases. In modeling the probability transition\nprocess from latent true labels to observed labels, most existing methods adopt\nclass-level confusion matrices of annotators that observed labels do not depend\non the instance features, just determined by the true labels. It may limit the\nperformance that the classifier can achieve. In this work, we propose the noise\ntransition matrix, which incorporates the influence of instance features on\nannotators' performance based on confusion matrices. Furthermore, we propose a\nsimple yet effective learning framework, which consists of a classifier module\nand a noise transition matrix module in a unified neural network architecture.\nExperimental results demonstrate the superiority of our method in comparison\nwith state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.15146",
          "publishedOn": "2021-06-30T02:01:01.505Z",
          "wordCount": 584,
          "title": "Learning from Multiple Annotators by Incorporating Instance Features. (arXiv:2106.15146v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>",
          "description": "As communication systems are foreseen to enable new services such as joint\ncommunication and sensing and utilize parts of the sub-THz spectrum, the design\nof novel waveforms that can support these emerging applications becomes\nincreasingly challenging. We present in this work an end-to-end learning\napproach to design waveforms through joint learning of pulse shaping and\nconstellation geometry, together with a neural network (NN)-based receiver.\nOptimization is performed to maximize an achievable information rate, while\nsatisfying constraints on out-of-band emission and power envelope. Our results\nshow that the proposed approach enables up to orders of magnitude smaller\nadjacent channel leakage ratios (ACLRs) with peak-to-average power ratios\n(PAPRs) competitive with traditional filters, without significant loss of\ninformation rate on an additive white Gaussian noise (AWGN) channel, and no\nadditional complexity at the transmitter.",
          "link": "http://arxiv.org/abs/2106.15158",
          "publishedOn": "2021-06-30T02:01:01.493Z",
          "wordCount": 576,
          "title": "End-to-end Waveform Learning Through Joint Optimization of Pulse and Constellation Shaping. (arXiv:2106.15158v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamalov_F/0/1/0/all/0/1\">Firuz Kamalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussa_S/0/1/0/all/0/1\">Sherif Moussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zgheib_R/0/1/0/all/0/1\">Rita Zgheib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mashaal_O/0/1/0/all/0/1\">Omar Mashaal</a>",
          "description": "In this paper, we analyze existing feature selection methods to identify the\nkey elements of network traffic data that allow intrusion detection. In\naddition, we propose a new feature selection method that addresses the\nchallenge of considering continuous input features and discrete target values.\nWe show that the proposed method performs well against the benchmark selection\nmethods. We use our findings to develop a highly effective machine\nlearning-based detection systems that achieves 99.9% accuracy in distinguishing\nbetween DDoS and benign signals. We believe that our results can be useful to\nexperts who are interested in designing and building automated intrusion\ndetection systems.",
          "link": "http://arxiv.org/abs/2106.14941",
          "publishedOn": "2021-06-30T02:01:01.439Z",
          "wordCount": 552,
          "title": "Feature selection for intrusion detection systems. (arXiv:2106.14941v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farsang_M/0/1/0/all/0/1\">M&#xf3;nika Farsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szegletes_L/0/1/0/all/0/1\">Luca Szegletes</a>",
          "description": "Proximal Policy Optimization (PPO) is among the most widely used algorithms\nin reinforcement learning, which achieves state-of-the-art performance in many\nchallenging problems. The keys to its success are the reliable policy updates\nthrough the clipping mechanism and the multiple epochs of minibatch updates.\nThe aim of this research is to give new simple but effective alternatives to\nthe former. For this, we propose linearly and exponentially decaying clipping\nrange approaches throughout the training. With these, we would like to provide\nhigher exploration at the beginning and stronger restrictions at the end of the\nlearning phase. We investigate their performance in several classical control\nand locomotive robotic environments. During the analysis, we found that they\ninfluence the achieved rewards and are effective alternatives to the constant\nclipping method in many reinforcement learning tasks.",
          "link": "http://arxiv.org/abs/2102.10456",
          "publishedOn": "2021-06-29T01:55:20.694Z",
          "wordCount": 600,
          "title": "Decaying Clipping Range in Proximal Policy Optimization. (arXiv:2102.10456v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farsang_M/0/1/0/all/0/1\">M&#xf3;nika Farsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szegletes_L/0/1/0/all/0/1\">Luca Szegletes</a>",
          "description": "An in-depth understanding of the particular environment is crucial in\nreinforcement learning (RL). To address this challenge, the decision-making\nprocess of a mobile collaborative robotic assistant modeled by the Markov\ndecision process (MDP) framework is studied in this paper. The optimal\nstate-action combinations of the MDP are calculated with the non-linear Bellman\noptimality equations. This system of equations can be solved with relative ease\nby the computational power of Wolfram Mathematica, where the obtained optimal\naction-values point to the optimal policy. Unlike other RL algorithms, this\nmethodology does not approximate the optimal behavior, it gives the exact,\nexplicit solution, which provides a strong foundation for our study. With this,\nwe offer new insights into understanding the action selection mechanisms in RL\nby presenting various small modifications on the very same schema that lead to\ndifferent optimal policies.",
          "link": "http://arxiv.org/abs/2102.10447",
          "publishedOn": "2021-06-29T01:55:20.689Z",
          "wordCount": 615,
          "title": "Importance of Environment Design in Reinforcement Learning: A Study of a Robotic Environment. (arXiv:2102.10447v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1\">Micha&#x142; Derezi&#x144;ski</a>",
          "description": "Consider a regression problem where the learner is given a large collection\nof $d$-dimensional data points, but can only query a small subset of the\nreal-valued labels. How many queries are needed to obtain a $1+\\epsilon$\nrelative error approximation of the optimum? While this problem has been\nextensively studied for least squares regression, little is known for other\nlosses. An important example is least absolute deviation regression ($\\ell_1$\nregression) which enjoys superior robustness to outliers compared to least\nsquares. We develop a new framework for analyzing importance sampling methods\nin regression problems, which enables us to show that the query complexity of\nleast absolute deviation regression is $\\Theta(d/\\epsilon^2)$ up to logarithmic\nfactors. We further extend our techniques to show the first bounds on the query\ncomplexity for any $\\ell_p$ loss with $p\\in(1,2)$. As a key novelty in our\nanalysis, we introduce the notion of robust uniform convergence, which is a new\napproximation guarantee for the empirical loss. While it is inspired by uniform\nconvergence in statistical learning, our approach additionally incorporates a\ncorrection term to avoid unnecessary variance due to outliers. This can be\nviewed as a new connection between statistical learning theory and variance\nreduction techniques in stochastic optimization, which should be of independent\ninterest.",
          "link": "http://arxiv.org/abs/2102.02322",
          "publishedOn": "2021-06-29T01:55:20.671Z",
          "wordCount": 672,
          "title": "Query Complexity of Least Absolute Deviation Regression via Robust Uniform Convergence. (arXiv:2102.02322v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "Machine learning analysis of longitudinal neuroimaging data is typically\nbased on supervised learning, which requires a large number of ground-truth\nlabels to be informative. As ground-truth labels are often missing or expensive\nto obtain in neuroscience, we avoid them in our analysis by combing factor\ndisentanglement with self-supervised learning to identify changes and\nconsistencies across the multiple MRIs acquired of each individual over time.\nSpecifically, we propose a new definition of disentanglement by formulating a\nmultivariate mapping between factors (e.g., brain age) associated with an MRI\nand a latent image representation. Then, factors that evolve across\nacquisitions of longitudinal sequences are disentangled from that mapping by\nself-supervised learning in such a way that changes in a single factor induce\nchange along one direction in the representation space. We implement this\nmodel, named Longitudinal Self-Supervised Learning (LSSL), via a standard\nautoencoding structure with a cosine loss to disentangle brain age from the\nimage representation. We apply LSSL to two longitudinal neuroimaging studies to\nhighlight its strength in extracting the brain-age information from MRI and\nrevealing informative characteristics associated with neurodegenerative and\nneuropsychological disorders. Moreover, the representations learned by LSSL\nfacilitate supervised classification by recording faster convergence and higher\n(or similar) prediction accuracy compared to several other representation\nlearning techniques.",
          "link": "http://arxiv.org/abs/2006.06930",
          "publishedOn": "2021-06-29T01:55:20.665Z",
          "wordCount": 670,
          "title": "Longitudinal Self-Supervised Learning. (arXiv:2006.06930v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lobacheva_E/0/1/0/all/0/1\">Ekaterina Lobacheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodryan_M/0/1/0/all/0/1\">Maxim Kodryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Ensembles of deep neural networks are known to achieve state-of-the-art\nperformance in uncertainty estimation and lead to accuracy improvement. In this\nwork, we focus on a classification problem and investigate the behavior of both\nnon-calibrated and calibrated negative log-likelihood (CNLL) of a deep ensemble\nas a function of the ensemble size and the member network size. We indicate the\nconditions under which CNLL follows a power law w.r.t. ensemble size or member\nnetwork size, and analyze the dynamics of the parameters of the discovered\npower laws. Our important practical finding is that one large network may\nperform worse than an ensemble of several medium-size networks with the same\ntotal number of parameters (we call this ensemble a memory split). Using the\ndetected power law-like dependencies, we can predict (1) the possible gain from\nthe ensembling of networks with given structure, (2) the optimal memory split\ngiven a memory budget, based on a relatively small number of trained networks.\n\nWe describe the memory split advantage effect in more details in\narXiv:2005.07292",
          "link": "http://arxiv.org/abs/2007.08483",
          "publishedOn": "2021-06-29T01:55:20.659Z",
          "wordCount": 648,
          "title": "On Power Laws in Deep Ensembles. (arXiv:2007.08483v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08405",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shlezinger_N/0/1/0/all/0/1\">Nir Shlezinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whang_J/0/1/0/all/0/1\">Jay Whang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>",
          "description": "Signal processing, communications, and control have traditionally relied on\nclassical statistical modeling techniques. Such model-based methods utilize\nmathematical formulations that represent the underlying physics, prior\ninformation and additional domain knowledge. Simple classical models are useful\nbut sensitive to inaccuracies and may lead to poor performance when real\nsystems display complex or dynamic behavior. On the other hand, purely\ndata-driven approaches that are model-agnostic are becoming increasingly\npopular as datasets become abundant and the power of modern deep learning\npipelines increases. Deep neural networks (DNNs) use generic architectures\nwhich learn to operate from data, and demonstrate excellent performance,\nespecially for supervised problems. However, DNNs typically require massive\namounts of data and immense computational resources, limiting their\napplicability for some signal processing scenarios. We are interested in hybrid\ntechniques that combine principled mathematical models with data-driven systems\nto benefit from the advantages of both approaches. Such model-based deep\nlearning methods exploit both partial domain knowledge, via mathematical\nstructures designed for specific problems, as well as learning from limited\ndata. In this article we survey the leading approaches for studying and\ndesigning model-based deep learning systems. We divide hybrid\nmodel-based/data-driven systems into categories based on their inference\nmechanism. We provide a comprehensive review of the leading approaches for\ncombining model-based algorithms with deep learning in a systematic manner,\nalong with concrete guidelines and detailed signal processing oriented examples\nfrom recent literature. Our aim is to facilitate the design and study of future\nsystems on the intersection of signal processing and machine learning that\nincorporate the advantages of both domains.",
          "link": "http://arxiv.org/abs/2012.08405",
          "publishedOn": "2021-06-29T01:55:20.653Z",
          "wordCount": 703,
          "title": "Model-Based Deep Learning. (arXiv:2012.08405v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jinheon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minki Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Graph neural networks have been widely used on modeling graph data, achieving\nimpressive results on node classification and link prediction tasks. Yet,\nobtaining an accurate representation for a graph further requires a pooling\nfunction that maps a set of node representations into a compact form. A simple\nsum or average over all node representations considers all node features\nequally without consideration of their task relevance, and any structural\ndependencies among them. Recently proposed hierarchical graph pooling methods,\non the other hand, may yield the same representation for two different graphs\nthat are distinguished by the Weisfeiler-Lehman test, as they suboptimally\npreserve information from the node features. To tackle these limitations of\nexisting graph pooling methods, we first formulate the graph pooling problem as\na multiset encoding problem with auxiliary information about the graph\nstructure, and propose a Graph Multiset Transformer (GMT) which is a multi-head\nattention based global pooling layer that captures the interaction between\nnodes according to their structural dependencies. We show that GMT satisfies\nboth injectiveness and permutation invariance, such that it is at most as\npowerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods\ncan be easily extended to the previous node clustering approaches for\nhierarchical graph pooling. Our experimental results show that GMT\nsignificantly outperforms state-of-the-art graph pooling methods on graph\nclassification benchmarks with high memory and time efficiency, and obtains\neven larger performance gain on graph reconstruction and generation tasks.",
          "link": "http://arxiv.org/abs/2102.11533",
          "publishedOn": "2021-06-29T01:55:20.646Z",
          "wordCount": 716,
          "title": "Accurate Learning of Graph Representations with Graph Multiset Pooling. (arXiv:2102.11533v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14352",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Khamaru_K/0/1/0/all/0/1\">Koulik Khamaru</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xia_E/0/1/0/all/0/1\">Eric Xia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wainwright_M/0/1/0/all/0/1\">Martin J. Wainwright</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Various algorithms in reinforcement learning exhibit dramatic variability in\ntheir convergence rates and ultimate accuracy as a function of the problem\nstructure. Such instance-specific behavior is not captured by existing global\nminimax bounds, which are worst-case in nature. We analyze the problem of\nestimating optimal $Q$-value functions for a discounted Markov decision process\nwith discrete states and actions and identify an instance-dependent functional\nthat controls the difficulty of estimation in the $\\ell_\\infty$-norm. Using a\nlocal minimax framework, we show that this functional arises in lower bounds on\nthe accuracy on any estimation procedure. In the other direction, we establish\nthe sharpness of our lower bounds, up to factors logarithmic in the state and\naction spaces, by analyzing a variance-reduced version of $Q$-learning. Our\ntheory provides a precise way of distinguishing \"easy\" problems from \"hard\"\nones in the context of $Q$-learning, as illustrated by an ensemble with a\ncontinuum of difficulty.",
          "link": "http://arxiv.org/abs/2106.14352",
          "publishedOn": "2021-06-29T01:55:20.627Z",
          "wordCount": 587,
          "title": "Instance-optimality in optimal value estimation: Adaptivity via variance-reduced Q-learning. (arXiv:2106.14352v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yunsheng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Ziheng Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "In this work, we focus on large graph similarity computation problem and\npropose a novel \"embedding-coarsening-matching\" learning framework, which\noutperforms state-of-the-art methods in this task and has significant\nimprovement in time efficiency. Graph similarity computation for metrics such\nas Graph Edit Distance (GED) is typically NP-hard, and existing\nheuristics-based algorithms usually achieves a unsatisfactory trade-off between\naccuracy and efficiency. Recently the development of deep learning techniques\nprovides a promising solution for this problem by a data-driven approach which\ntrains a network to encode graphs to their own feature vectors and computes\nsimilarity based on feature vectors. These deep-learning methods can be\nclassified to two categories, embedding models and matching models. Embedding\nmodels such as GCN-Mean and GCN-Max, which directly map graphs to respective\nfeature vectors, run faster but the performance is usually poor due to the lack\nof interactions across graphs. Matching models such as GMN, whose encoding\nprocess involves interaction across the two graphs, are more accurate but\ninteraction between whole graphs brings a significant increase in time\nconsumption (at least quadratic time complexity over number of nodes). Inspired\nby large biological molecular identification where the whole molecular is first\nmapped to functional groups and then identified based on these functional\ngroups, our \"embedding-coarsening-matching\" learning framework first embeds and\ncoarsens large graphs to coarsened graphs with denser local topology and then\nmatching mechanism is deployed on the coarsened graphs for the final similarity\nscores. Detailed experiments have been conducted and the results demonstrate\nthe efficiency and effectiveness of our proposed framework.",
          "link": "http://arxiv.org/abs/2005.07115",
          "publishedOn": "2021-06-29T01:55:20.620Z",
          "wordCount": 761,
          "title": "Hierarchical Large-scale Graph Similarity Computation via Graph Coarsening and Matching. (arXiv:2005.07115v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1810.01256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guanxiong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>",
          "description": "Deep neural networks (DNNs) are powerful tools in learning sophisticated but\nfixed mapping rules between inputs and outputs, thereby limiting their\napplication in more complex and dynamic situations in which the mapping rules\nare not kept the same but changing according to different contexts. To lift\nsuch limits, we developed a novel approach involving a learning algorithm,\ncalled orthogonal weights modification (OWM), with the addition of a\ncontext-dependent processing (CDP) module. We demonstrated that with OWM to\novercome the problem of catastrophic forgetting, and the CDP module to learn\nhow to reuse a feature representation and a classifier for different contexts,\na single network can acquire numerous context-dependent mapping rules in an\nonline and continual manner, with as few as $\\sim$10 samples to learn each.\nThis should enable highly compact systems to gradually learn myriad\nregularities of the real world and eventually behave appropriately within it.",
          "link": "http://arxiv.org/abs/1810.01256",
          "publishedOn": "2021-06-29T01:55:20.614Z",
          "wordCount": 634,
          "title": "Continual Learning of Context-dependent Processing in Neural Networks. (arXiv:1810.01256v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rawal_K/0/1/0/all/0/1\">Kaivalya Rawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "As predictive models are increasingly being deployed to make a variety of\nconsequential decisions, there is a growing emphasis on designing algorithms\nthat can provide recourse to affected individuals. Existing recourse algorithms\nfunction under the assumption that the underlying predictive model does not\nchange. However, models are regularly updated in practice for several reasons\nincluding data distribution shifts. In this work, we make the first attempt at\nunderstanding how model updates resulting from data distribution shifts impact\nthe algorithmic recourses generated by state-of-the-art algorithms. We carry\nout a rigorous theoretical and empirical analysis to address the above\nquestion. Our theoretical results establish a lower bound on the probability of\nrecourse invalidation due to model shifts, and show the existence of a tradeoff\nbetween this invalidation probability and typical notions of \"cost\" minimized\nby modern recourse generation algorithms. We experiment with multiple synthetic\nand real world datasets, capturing different kinds of distribution shifts\nincluding temporal shifts, geospatial shifts, and shifts due to data\ncorrection. These experiments demonstrate that model updation due to all the\naforementioned distribution shifts can potentially invalidate recourses\ngenerated by state-of-the-art algorithms. Our findings thus not only expose\npreviously unknown flaws in the current recourse generation paradigm, but also\npave the way for fundamentally rethinking the design and development of\nrecourse generation algorithms.",
          "link": "http://arxiv.org/abs/2012.11788",
          "publishedOn": "2021-06-29T01:55:20.599Z",
          "wordCount": 692,
          "title": "Algorithmic Recourse in the Wild: Understanding the Impact of Data and Model Shifts. (arXiv:2012.11788v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.02513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1\">Uri Stemmer</a>",
          "description": "We design a new algorithm for the Euclidean $k$-means problem that operates\nin the local model of differential privacy. Unlike in the non-private\nliterature, differentially private algorithms for the $k$-means objective incur\nboth additive and multiplicative errors. Our algorithm significantly reduces\nthe additive error while keeping the multiplicative error the same as in\nprevious state-of-the-art results. Specifically, on a database of size $n$, our\nalgorithm guarantees $O(1)$ multiplicative error and $\\approx n^{1/2+a}$\nadditive error for an arbitrarily small constant $a>0$. All previous algorithms\nin the local model had additive error $\\approx n^{2/3+a}$. Our techniques\nextend to $k$-median clustering.\n\nWe show that the additive error we obtain is almost optimal in terms of its\ndependency on the database size $n$. Specifically, we give a simple lower bound\nshowing that every locally-private algorithm for the $k$-means objective must\nhave additive error at least $\\approx\\sqrt{n}$.",
          "link": "http://arxiv.org/abs/1907.02513",
          "publishedOn": "2021-06-29T01:55:20.593Z",
          "wordCount": 601,
          "title": "Locally Private k-Means Clustering. (arXiv:1907.02513v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.02944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "Object recognition in the real-world requires handling long-tailed or even\nopen-ended data. An ideal visual system needs to recognize the populated head\nvisual concepts reliably and meanwhile efficiently learn about emerging new\ntail categories with a few training instances. Class-balanced many-shot\nlearning and few-shot learning tackle one side of this problem, by either\nlearning strong classifiers for head or learning to learn few-shot classifiers\nfor the tail. In this paper, we investigate the problem of generalized few-shot\nlearning (GFSL) -- a model during the deployment is required to learn about\ntail categories with few shots and simultaneously classify the head classes. We\npropose the ClAssifier SynThesis LEarning (CASTLE), a learning framework that\nlearns how to synthesize calibrated few-shot classifiers in addition to the\nmulti-class classifiers of head classes with a shared neural dictionary,\nshedding light upon the inductive GFSL. Furthermore, we propose an adaptive\nversion of CASTLE (ACASTLE) that adapts the head classifiers conditioned on the\nincoming tail training examples, yielding a framework that allows effective\nbackward knowledge transfer. As a consequence, ACASTLE can handle GFSL with\nclasses from heterogeneous domains effectively. CASTLE and ACASTLE demonstrate\nsuperior performances than existing GFSL algorithms and strong baselines on\nMiniImageNet as well as TieredImageNet datasets. More interestingly, they\noutperform previous state-of-the-art methods when evaluated with standard\nfew-shot learning criteria.",
          "link": "http://arxiv.org/abs/1906.02944",
          "publishedOn": "2021-06-29T01:55:20.579Z",
          "wordCount": 722,
          "title": "Learning Adaptive Classifiers Synthesis for Generalized Few-Shot Learning. (arXiv:1906.02944v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.11723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yirong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hengyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Although achieving remarkable progress, it is very difficult to induce a\nsupervised classifier without any labeled data. Unsupervised domain adaptation\nis able to overcome this challenge by transferring knowledge from a labeled\nsource domain to an unlabeled target domain. Transferability and\ndiscriminability are two key criteria for characterizing the superiority of\nfeature representations to enable successful domain adaptation. In this paper,\na novel method called \\textit{learning TransFerable and Discriminative Features\nfor unsupervised domain adaptation} (TFDF) is proposed to optimize these two\nobjectives simultaneously. On the one hand, distribution alignment is performed\nto reduce domain discrepancy and learn more transferable representations.\nInstead of adopting \\textit{Maximum Mean Discrepancy} (MMD) which only captures\nthe first-order statistical information to measure distribution discrepancy, we\nadopt a recently proposed statistic called \\textit{Maximum Mean and Covariance\nDiscrepancy} (MMCD), which can not only capture the first-order statistical\ninformation but also capture the second-order statistical information in the\nreproducing kernel Hilbert space (RKHS). On the other hand, we propose to\nexplore both local discriminative information via manifold regularization and\nglobal discriminative information via minimizing the proposed \\textit{class\nconfusion} objective to learn more discriminative features, respectively. We\nintegrate these two objectives into the \\textit{Structural Risk Minimization}\n(RSM) framework and learn a domain-invariant classifier. Comprehensive\nexperiments are conducted on five real-world datasets and the results verify\nthe effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2003.11723",
          "publishedOn": "2021-06-29T01:55:20.573Z",
          "wordCount": 693,
          "title": "Learning transferable and discriminative features for unsupervised domain adaptation. (arXiv:2003.11723v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guofa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1\">Dongpu Cao</a>",
          "description": "Multimodal learning mimics the reasoning process of the human multi-sensory\nsystem, which is used to perceive the surrounding world. While making a\nprediction, the human brain tends to relate crucial cues from multiple sources\nof information. In this work, we propose a novel multimodal fusion module that\nlearns to emphasize more contributive features across all modalities.\nSpecifically, the proposed Multimodal Split Attention Fusion (MSAF) module\nsplits each modality into channel-wise equal feature blocks and creates a joint\nrepresentation that is used to generate soft attention for each channel across\nthe feature blocks. Further, the MSAF module is designed to be compatible with\nfeatures of various spatial dimensions and sequence lengths, suitable for both\nCNNs and RNNs. Thus, MSAF can be easily added to fuse features of any unimodal\nnetworks and utilize existing pretrained unimodal model weights. To demonstrate\nthe effectiveness of our fusion module, we design three multimodal networks\nwith MSAF for emotion recognition, sentiment analysis, and action recognition\ntasks. Our approach achieves competitive results in each task and outperforms\nother application-specific networks and multimodal fusion benchmarks.",
          "link": "http://arxiv.org/abs/2012.07175",
          "publishedOn": "2021-06-29T01:55:20.566Z",
          "wordCount": 637,
          "title": "MSAF: Multimodal Split Attention Fusion. (arXiv:2012.07175v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1302.6808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geiger_D/0/1/0/all/0/1\">Dan Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>",
          "description": "We describe algorithms for learning Bayesian networks from a combination of\nuser knowledge and statistical data. The algorithms have two components: a\nscoring metric and a search procedure. The scoring metric takes a network\nstructure, statistical data, and a user's prior knowledge, and returns a score\nproportional to the posterior probability of the network structure given the\ndata. The search procedure generates networks for evaluation by the scoring\nmetric. Previous work has concentrated on metrics for domains containing only\ndiscrete variables, under the assumption that data represents a multinomial\nsample. In this paper, we extend this work, developing scoring metrics for\ndomains containing all continuous variables or a mixture of discrete and\ncontinuous variables, under the assumption that continuous data is sampled from\na multivariate normal distribution. Our work extends traditional statistical\napproaches for identifying vanishing regression coefficients in that we\nidentify two important assumptions, called event equivalence and parameter\nmodularity, that when combined allow the construction of prior distributions\nfor multivariate normal parameters from a single prior Bayesian network\nspecified by a user.",
          "link": "http://arxiv.org/abs/1302.6808",
          "publishedOn": "2021-06-29T01:55:20.560Z",
          "wordCount": 653,
          "title": "Learning Gaussian Networks. (arXiv:1302.6808v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14289",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/math/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "We study the asymmetric low-rank factorization problem: \\[\\min_{\\mathbf{U}\n\\in \\mathbb{R}^{m \\times d}, \\mathbf{V} \\in \\mathbb{R}^{n \\times d}}\n\\frac{1}{2}\\|\\mathbf{U}\\mathbf{V}^\\top -\\mathbf{\\Sigma}\\|_F^2\\] where\n$\\mathbf{\\Sigma}$ is a given matrix of size $m \\times n$ and rank $d$. This is\na canonical problem that admits two difficulties in optimization: 1)\nnon-convexity and 2) non-smoothness (due to unbalancedness of $\\mathbf{U}$ and\n$\\mathbf{V}$). This is also a prototype for more complex problems such as\nasymmetric matrix sensing and matrix completion. Despite being non-convex and\nnon-smooth, it has been observed empirically that the randomly initialized\ngradient descent algorithm can solve this problem in polynomial time. Existing\ntheories to explain this phenomenon all require artificial modifications of the\nalgorithm, such as adding noise in each iteration and adding a balancing\nregularizer to balance the $\\mathbf{U}$ and $\\mathbf{V}$.\n\nThis paper presents the first proof that shows randomly initialized gradient\ndescent converges to a global minimum of the asymmetric low-rank factorization\nproblem with a polynomial rate. For the proof, we develop 1) a new\nsymmetrization technique to capture the magnitudes of the symmetry and\nasymmetry, and 2) a quantitative perturbation analysis to approximate matrix\nderivatives. We believe both are useful for other related non-convex problems.",
          "link": "http://arxiv.org/abs/2106.14289",
          "publishedOn": "2021-06-29T01:55:20.554Z",
          "wordCount": 637,
          "title": "Global Convergence of Gradient Descent for Asymmetric Low-Rank Matrix Factorization. (arXiv:2106.14289v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1904.06366",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yifan Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dai_F/0/1/0/all/0/1\">Fan Dai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "We develop methodology for three-dimensional (3D) radial visualization\n(RadViz) of multidimensional datasets. Our tool is called RadViz3D and extends\nthe classical two-dimensional (2D) RadViz that visualizes multivariate data in\nthe 2D plane by mapping every observation to a point inside the unit circle. We\nshow that distributing anchor points uniformly on the 3D unit sphere provides\nthe best visualization with minimal artificial visual correlation for data with\nuncorrelated variables. However, anchor points can be placed exactly\nequi-distant from each other only for the five Platonic solids. We provide\nequi-distant anchor points for these five settings, and approximately\nequi-distant anchor points via a Fibonacci grid for the other cases. Our\nmethodology, implemented in the R package $radviz3d$, makes fully 3D RadViz\npossible and is shown to improve clarity of this nonlinear display technique on\nsimulated and real datasets.",
          "link": "http://arxiv.org/abs/1904.06366",
          "publishedOn": "2021-06-29T01:55:20.539Z",
          "wordCount": 601,
          "title": "Fully Three-dimensional Radial Visualization. (arXiv:1904.06366v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunasekaran_V/0/1/0/all/0/1\">V. Gunasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovi_K/0/1/0/all/0/1\">K.K. Kovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arja_S/0/1/0/all/0/1\">S. Arja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chimata_R/0/1/0/all/0/1\">R. Chimata</a>",
          "description": "Renewable energy forecasting is attaining greater importance due to its\nconstant increase in contribution to the electrical power grids. Solar energy\nis one of the most significant contributors to renewable energy and is\ndependent on solar irradiation. For the effective management of electrical\npower grids, forecasting models that predict solar irradiation, with high\naccuracy, are needed. In the current study, Machine Learning techniques such as\nLinear Regression, Extreme Gradient Boosting and Genetic Algorithm Optimization\nare used to forecast solar irradiation. The data used for training and\nvalidation is recorded from across three different geographical stations in the\nUnited States that are part of the SURFRAD network. A Global Horizontal Index\n(GHI) is predicted for the models built and compared. Genetic Algorithm\nOptimization is applied to XGB to further improve the accuracy of solar\nirradiation prediction.",
          "link": "http://arxiv.org/abs/2106.13956",
          "publishedOn": "2021-06-29T01:55:20.533Z",
          "wordCount": 577,
          "title": "Solar Irradiation Forecasting using Genetic Algorithms. (arXiv:2106.13956v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Robert Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_N/0/1/0/all/0/1\">Nayan Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rohan Jain</a>",
          "description": "Deep learning has proven to be a highly effective problem-solving tool for\nobject detection and image segmentation across various domains such as\nhealthcare and autonomous driving. At the heart of this performance lies neural\narchitecture design which relies heavily on domain knowledge and prior\nexperience on the researchers' behalf. More recently, this process of finding\nthe most optimal architectures, given an initial search space of possible\noperations, was automated by Neural Architecture Search (NAS). In this paper,\nwe evaluate the robustness of one such algorithm known as Efficient NAS (ENAS)\nagainst data agnostic poisoning attacks on the original search space with\ncarefully designed ineffective operations. By evaluating algorithm performance\non the CIFAR-10 dataset, we empirically demonstrate how our novel search space\npoisoning (SSP) approach and multiple-instance poisoning attacks exploit design\nflaws in the ENAS controller to result in inflated prediction error rates for\nchild networks. Our results provide insights into the challenges to surmount in\nusing NAS for more adversarially robust architecture search.",
          "link": "http://arxiv.org/abs/2106.14406",
          "publishedOn": "2021-06-29T01:55:20.498Z",
          "wordCount": 630,
          "title": "Poisoning the Search Space in Neural Architecture Search. (arXiv:2106.14406v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xian_L/0/1/0/all/0/1\">Lu Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_H/0/1/0/all/0/1\">Henry Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topaz_C/0/1/0/all/0/1\">Chad M. Topaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziegelmeier_L/0/1/0/all/0/1\">Lori Ziegelmeier</a>",
          "description": "One approach to understanding complex data is to study its shape through the\nlens of algebraic topology. While the early development of topological data\nanalysis focused primarily on static data, in recent years, theoretical and\napplied studies have turned to data that varies in time. A time-varying\ncollection of metric spaces as formed, for example, by a moving school of fish\nor flock of birds, can contain a vast amount of information. There is often a\nneed to simplify or summarize the dynamic behavior. We provide an introduction\nto topological summaries of time-varying metric spaces including vineyards\n[19], crocker plots [56], and multiparameter rank functions [37]. We then\nintroduce a new tool to summarize time-varying metric spaces: a crocker stack.\nCrocker stacks are convenient for visualization, amenable to machine learning,\nand satisfy a desirable continuity property which we prove. We demonstrate the\nutility of crocker stacks for a parameter identification task involving an\ninfluential model of biological aggregations [58]. Altogether, we aim to bring\nthe broader applied mathematics community up-to-date on topological summaries\nof time-varying metric spaces.",
          "link": "http://arxiv.org/abs/2010.05780",
          "publishedOn": "2021-06-29T01:55:18.547Z",
          "wordCount": 658,
          "title": "Capturing Dynamics of Time-Varying Data via Topology. (arXiv:2010.05780v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowdra_N/0/1/0/all/0/1\">Nidhi Gowdra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1\">Roopak Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDonell_S/0/1/0/all/0/1\">Stephen MacDonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wei Qi Yan</a>",
          "description": "Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and\nResNeXt-56 are severely over-parameterized, necessitating a consequent increase\nin the computational resources required for model training which scales\nexponentially for increments in model depth. In this paper, we propose an\nEntropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust\nand simple, yet effective in resolving the problem of over-parameterization\nwith regards to network depth of CNN model. The EBCLE heuristic employs a\npriori knowledge of the entropic data distribution of input datasets to\ndetermine an upper bound for convolutional network depth, beyond which identity\ntransformations are prevalent offering insignificant contributions for\nenhancing model performance. Restricting depth redundancies by forcing feature\ncompression and abstraction restricts over-parameterization while decreasing\ntraining time by 24.99% - 78.59% without degradation in model performance. We\npresent empirical evidence to emphasize the relative effectiveness of broader,\nyet shallower models trained using the EBCLE heuristic, which maintains or\noutperforms baseline classification accuracies of narrower yet deeper models.\nThe EBCLE heuristic is architecturally agnostic and EBCLE based CNN models\nrestrict depth redundancies resulting in enhanced utilization of the available\ncomputational resources. The proposed EBCLE heuristic is a compelling technique\nfor researchers to analytically justify their HyperParameter (HP) choices for\nCNNs. Empirical validation of the EBCLE heuristic in training CNN models was\nestablished on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10,\nMNIST) and four network architectures (DenseNet, ResNet, ResNeXt and\nEfficientNet B0-B2) with appropriate statistical tests employed to infer any\nconclusive claims presented in this paper.",
          "link": "http://arxiv.org/abs/2106.14190",
          "publishedOn": "2021-06-29T01:55:18.531Z",
          "wordCount": 721,
          "title": "Mitigating severe over-parameterization in deep convolutional neural networks through forced feature abstraction and compression with an entropy-based heuristic. (arXiv:2106.14190v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.00731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Law_H/0/1/0/all/0/1\">Ho Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_G/0/1/0/all/0/1\">Gary P. T. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Ka Chun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1\">Lok Ming Lui</a>",
          "description": "Image registration has been widely studied over the past several decades,\nwith numerous applications in science, engineering and medicine. Most of the\nconventional mathematical models for large deformation image registration rely\non prescribed landmarks, which usually require tedious manual labeling and are\nprone to error. In recent years, there has been a surge of interest in the use\nof machine learning for image registration. In this paper, we develop a novel\nmethod for large deformation image registration by a fusion of quasiconformal\ntheory and convolutional neural network (CNN). More specifically, we propose a\nquasiconformal energy model with a novel fidelity term that incorporates the\nfeatures extracted using a pre-trained CNN, thereby allowing us to obtain\nmeaningful registration results without any guidance of prescribed landmarks.\nMoreover, unlike many prior image registration methods, the bijectivity of our\nmethod is guaranteed by quasiconformal theory. Experimental results are\npresented to demonstrate the effectiveness of the proposed method. More\nbroadly, our work sheds light on how rigorous mathematical theories and\npractical machine learning approaches can be integrated for developing\ncomputational methods with improved performance.",
          "link": "http://arxiv.org/abs/2011.00731",
          "publishedOn": "2021-06-29T01:55:18.517Z",
          "wordCount": 681,
          "title": "Quasiconformal model with CNN features for large deformation image registration. (arXiv:2011.00731v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01807",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Hurwitz_C/0/1/0/all/0/1\">Cole Hurwitz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kudryashova_N/0/1/0/all/0/1\">Nina Kudryashova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Onken_A/0/1/0/all/0/1\">Arno Onken</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hennig_M/0/1/0/all/0/1\">Matthias H. Hennig</a>",
          "description": "Modern recording technologies now enable simultaneous recording from large\nnumbers of neurons. This has driven the development of new statistical models\nfor analyzing and interpreting neural population activity. Here we provide a\nbroad overview of recent developments in this area. We compare and contrast\ndifferent approaches, highlight strengths and limitations, and discuss\nbiological and mechanistic insights that these methods provide.",
          "link": "http://arxiv.org/abs/2102.01807",
          "publishedOn": "2021-06-29T01:55:18.510Z",
          "wordCount": 527,
          "title": "Building population models for large-scale neural recordings: opportunities and pitfalls. (arXiv:2102.01807v3 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Witt_L/0/1/0/all/0/1\">Leon Witt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafar_U/0/1/0/all/0/1\">Usama Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">KuoYeh Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_F/0/1/0/all/0/1\">Felix Sattler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>",
          "description": "The recent advent of various forms of Federated Knowledge Distillation (FD)\npaves the way for a new generation of robust and communication-efficient\nFederated Learning (FL), where mere soft-labels are aggregated, rather than\nwhole gradients of Deep Neural Networks (DNN) as done in previous FL schemes.\nThis security-per-design approach in combination with increasingly performant\nInternet of Things (IoT) and mobile devices opens up a new realm of\npossibilities to utilize private data from industries as well as from\nindividuals as input for artificial intelligence model training. Yet in\nprevious FL systems, lack of trust due to the imbalance of power between\nworkers and a central authority, the assumption of altruistic worker\nparticipation and the inability to correctly measure and compare contributions\nof workers hinder this technology from scaling beyond small groups of already\nentrusted entities towards mass adoption. This work aims to mitigate the\naforementioned issues by introducing a novel decentralized federated learning\nframework where heavily compressed 1-bit soft-labels, resembling 1-hot label\npredictions, are aggregated on a smart contract. In a context where workers'\ncontributions are now easily comparable, we modify the Peer Truth Serum for\nCrowdsourcing mechanism (PTSC) for FD to reward honest participation based on\npeer consistency in an incentive compatible fashion. Due to heavy reductions of\nboth computational complexity and storage, our framework is a fully\non-blockchain FL system that is feasible on simple smart contracts and\ntherefore blockchain agnostic. We experimentally test our new framework and\nvalidate its theoretical properties.",
          "link": "http://arxiv.org/abs/2106.14265",
          "publishedOn": "2021-06-29T01:55:18.492Z",
          "wordCount": 687,
          "title": "Reward-Based 1-bit Compressed Federated Distillation on Blockchain. (arXiv:2106.14265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14323",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Deziderio_N/0/1/0/all/0/1\">Nathalie Deziderio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carvalho_H/0/1/0/all/0/1\">Hugo Tremonte de Carvalho</a>",
          "description": "This work was developed aiming to employ Statistical techniques to the field\nof Music Emotion Recognition, a well-recognized area within the Signal\nProcessing world, but hardly explored from the statistical point of view. Here,\nwe opened several possibilities within the field, applying modern Bayesian\nStatistics techniques and developing efficient algorithms, focusing on the\napplicability of the results obtained. Although the motivation for this project\nwas the development of a emotion-based music recommendation system, its main\ncontribution is a highly adaptable multivariate model that can be useful\ninterpreting any database where there is an interest in applying regularization\nin an efficient manner. Broadly speaking, we will explore what role a sound\ntheoretical statistical analysis can play in the modeling of an algorithm that\nis able to understand a well-known database and what can be gained with this\nkind of approach.",
          "link": "http://arxiv.org/abs/2106.14323",
          "publishedOn": "2021-06-29T01:55:18.485Z",
          "wordCount": 580,
          "title": "Use of Variational Inference in Music Emotion Recognition. (arXiv:2106.14323v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macke_J/0/1/0/all/0/1\">J. Macke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedlar_J/0/1/0/all/0/1\">J. Sedlar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1\">M. Olsak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1\">J. Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">J. Sivic</a>",
          "description": "We describe a purely image-based method for finding geometric constructions\nwith a ruler and compass in the Euclidea geometric game. The method is based on\nadapting the Mask R-CNN state-of-the-art image processing neural architecture\nand adding a tree-based search procedure to it. In a supervised setting, the\nmethod learns to solve all 68 kinds of geometric construction problems from the\nfirst six level packs of Euclidea with an average 92% accuracy. When evaluated\non new kinds of problems, the method can solve 31 of the 68 kinds of Euclidea\nproblems. We believe that this is the first time that a purely image-based\nlearning has been trained to solve geometric construction problems of this\ndifficulty.",
          "link": "http://arxiv.org/abs/2106.14195",
          "publishedOn": "2021-06-29T01:55:18.473Z",
          "wordCount": 576,
          "title": "Learning to solve geometric construction problems from images. (arXiv:2106.14195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaekyeom Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seohong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>",
          "description": "Having the ability to acquire inherent skills from environments without any\nexternal rewards or supervision like humans is an important problem. We propose\na novel unsupervised skill discovery method named Information Bottleneck Option\nLearning (IBOL). On top of the linearization of environments that promotes more\nvarious and distant state transitions, IBOL enables the discovery of diverse\nskills. It provides the abstraction of the skills learned with the information\nbottleneck framework for the options with improved stability and encouraged\ndisentanglement. We empirically demonstrate that IBOL outperforms multiple\nstate-of-the-art unsupervised skill discovery methods on the\ninformation-theoretic evaluations and downstream tasks in MuJoCo environments,\nincluding Ant, HalfCheetah, Hopper and D'Kitty.",
          "link": "http://arxiv.org/abs/2106.14305",
          "publishedOn": "2021-06-29T01:55:18.467Z",
          "wordCount": 550,
          "title": "Unsupervised Skill Discovery with Bottleneck Option Learning. (arXiv:2106.14305v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_H/0/1/0/all/0/1\">Hyuntak Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaeho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Recent breakthroughs in self-supervised learning show that such algorithms\nlearn visual representations that can be transferred better to unseen tasks\nthan joint-training methods relying on task-specific supervision. In this\npaper, we found that the similar holds in the continual learning con-text:\ncontrastively learned representations are more robust against the catastrophic\nforgetting than jointly trained representations. Based on this novel\nobservation, we propose a rehearsal-based continual learning algorithm that\nfocuses on continually learning and maintaining transferable representations.\nMore specifically, the proposed scheme (1) learns representations using the\ncontrastive learning objective, and (2) preserves learned representations using\na self-supervised distillation step. We conduct extensive experimental\nvalidations under popular benchmark image classification datasets, where our\nmethod sets the new state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.14413",
          "publishedOn": "2021-06-29T01:55:18.454Z",
          "wordCount": 550,
          "title": "Co$^2$L: Contrastive Continual Learning. (arXiv:2106.14413v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takhanov_R/0/1/0/all/0/1\">Rustem Takhanov</a>",
          "description": "We present a new way of study of Mercer kernels, by corresponding to a\nspecial kernel $K$ a pseudo-differential operator $p({\\mathbf x}, D)$ such that\n$\\mathcal{F} p({\\mathbf x}, D)^\\dag p({\\mathbf x}, D) \\mathcal{F}^{-1}$ acts on\nsmooth functions in the same way as an integral operator associated with $K$\n(where $\\mathcal{F}$ is the Fourier transform). We show that kernels defined by\npseudo-differential operators are able to approximate uniformly any continuous\nMercer kernel on a compact set.\n\nThe symbol $p({\\mathbf x}, {\\mathbf y})$ encapsulates a lot of useful\ninformation about the structure of the Maximum Mean Discrepancy distance\ndefined by the kernel $K$. We approximate $p({\\mathbf x}, {\\mathbf y})$ with\nthe sum of the first $r$ terms of the Singular Value Decomposition of $p$,\ndenoted by $p_r({\\mathbf x}, {\\mathbf y})$. If ordered singular values of the\nintegral operator associated with $p({\\mathbf x}, {\\mathbf y})$ die down\nrapidly, the MMD distance defined by the new symbol $p_r$ differs from the\ninitial one only slightly. Moreover, the new MMD distance can be interpreted as\nan aggregated result of comparing $r$ local moments of two probability\ndistributions.\n\nThe latter results holds under the condition that right singular vectors of\nthe integral operator associated with $p$ are uniformly bounded. But even if\nthis is not satisfied we can still hold that the Hilbert-Schmidt distance\nbetween $p$ and $p_r$ vanishes. Thus, we report an interesting phenomenon: the\nMMD distance measures the difference of two probability distributions with\nrespect to a certain number of local moments, $r^\\ast$, and this number\n$r^\\ast$ depends on the speed with which singular values of $p$ die down.",
          "link": "http://arxiv.org/abs/2106.14277",
          "publishedOn": "2021-06-29T01:55:18.447Z",
          "wordCount": 690,
          "title": "How many moments does MMD compare?. (arXiv:2106.14277v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.04293",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Tao_T/0/1/0/all/0/1\">Ting Tao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pan_S/0/1/0/all/0/1\">Shaohua Pan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bi_S/0/1/0/all/0/1\">Shujun Bi</a>",
          "description": "This paper is concerned with the squared F(robenius)-norm regularized\nfactorization form for noisy low-rank matrix recovery problems. Under a\nsuitable assumption on the restricted condition number of the Hessian for the\nloss function, we derive an error bound to the true matrix for the non-strict\ncritical points with rank not more than that of the true matrix. Then, for the\nsquared F-norm regularized factorized least squares loss function, under the\nnoisy and full sample setting we establish its KL property of exponent $1/2$ on\nits global minimizer set, and under the noisy and partial sample setting\nachieve this property for a class of critical points. These theoretical\nfindings are also confirmed by solving the squared F-norm regularized\nfactorization problem with an accelerated alternating minimization method.",
          "link": "http://arxiv.org/abs/1911.04293",
          "publishedOn": "2021-06-29T01:55:18.442Z",
          "wordCount": 598,
          "title": "Error bound of critical points and KL property of exponent $1/2$ for squared F-norm regularized factorization. (arXiv:1911.04293v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1\">Yash Bhartia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>",
          "description": "In this article, we present our methodologies for SemEval-2021 Task-4:\nReading Comprehension of Abstract Meaning. Given a fill-in-the-blank-type\nquestion and a corresponding context, the task is to predict the most suitable\nword from a list of 5 options. There are three sub-tasks within this task:\nImperceptibility (subtask-I), Non-Specificity (subtask-II), and Intersection\n(subtask-III). We use encoders of transformers-based models pre-trained on the\nmasked language modelling (MLM) task to build our Fill-in-the-blank (FitB)\nmodels. Moreover, to model imperceptibility, we define certain linguistic\nfeatures, and to model non-specificity, we leverage information from hypernyms\nand hyponyms provided by a lexical database. Specifically, for non-specificity,\nwe try out augmentation techniques, and other statistical techniques. We also\npropose variants, namely Chunk Voting and Max Context, to take care of input\nlength restrictions for BERT, etc. Additionally, we perform a thorough ablation\nstudy, and use Integrated Gradients to explain our predictions on a few\nsamples. Our best submissions achieve accuracies of 75.31% and 77.84%, on the\ntest sets for subtask-I and subtask-II, respectively. For subtask-III, we\nachieve accuracies of 65.64% and 62.27%.",
          "link": "http://arxiv.org/abs/2102.12255",
          "publishedOn": "2021-06-29T01:55:18.392Z",
          "wordCount": 666,
          "title": "LRG at SemEval-2021 Task 4: Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting. (arXiv:2102.12255v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1\">Ashok Cutkosky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>",
          "description": "We consider non-convex stochastic optimization using first-order algorithms\nfor which the gradient estimates may have heavy tails. We show that a\ncombination of gradient clipping, momentum, and normalized gradient descent\nyields convergence to critical points in high-probability with best-known rates\nfor smooth losses when the gradients only have bounded $\\mathfrak{p}$th moments\nfor some $\\mathfrak{p}\\in(1,2]$. We then consider the case of second-order\nsmooth losses, which to our knowledge have not been studied in this setting,\nand again obtain high-probability bounds for any $\\mathfrak{p}$. Moreover, our\nresults hold for arbitrary smooth norms, in contrast to the typical SGD\nanalysis which requires a Hilbert space norm. Further, we show that after a\nsuitable \"burn-in\" period, the objective value will monotonically decrease for\nevery iteration until a critical point is identified, which provides intuition\nbehind the popular practice of learning rate \"warm-up\" and also yields a\nlast-iterate guarantee.",
          "link": "http://arxiv.org/abs/2106.14343",
          "publishedOn": "2021-06-29T01:55:18.379Z",
          "wordCount": 583,
          "title": "High-probability Bounds for Non-Convex Stochastic Optimization with Heavy Tails. (arXiv:2106.14343v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.00162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiotras_P/0/1/0/all/0/1\">Panagiotis Tsiotras</a>",
          "description": "We explore the use of policy approximations to reduce the computational cost\nof learning Nash equilibria in zero-sum stochastic games. We propose a new\nQ-learning type algorithm that uses a sequence of entropy-regularized soft\npolicies to approximate the Nash policy during the Q-function updates. We prove\nthat under certain conditions, by updating the regularized Q-function, the\nalgorithm converges to a Nash equilibrium. We also demonstrate the proposed\nalgorithm's ability to transfer previous training experiences, enabling the\nagents to adapt quickly to new environments. We provide a dynamic\nhyper-parameter scheduling scheme to further expedite convergence. Empirical\nresults applied to a number of stochastic games verify that the proposed\nalgorithm converges to the Nash equilibrium, while exhibiting a major speed-up\nover existing algorithms.",
          "link": "http://arxiv.org/abs/2009.00162",
          "publishedOn": "2021-06-29T01:55:18.373Z",
          "wordCount": 606,
          "title": "Learning Nash Equilibria in Zero-Sum Stochastic Games via Entropy-Regularized Policy Approximation. (arXiv:2009.00162v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_P/0/1/0/all/0/1\">Philip Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajapakse_I/0/1/0/all/0/1\">Indika Rajapakse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hero_A/0/1/0/all/0/1\">Alfred Hero</a>",
          "description": "K-Nearest Neighbor (kNN)-based deep learning methods have been applied to\nmany applications due to their simplicity and geometric interpretability.\nHowever, the robustness of kNN-based classification models has not been\nthoroughly explored and kNN attack strategies are underdeveloped. In this\npaper, we propose an Adversarial Soft kNN (ASK) loss to both design more\neffective kNN attack strategies and to develop better defenses against them.\nOur ASK loss approach has two advantages. First, ASK loss can better\napproximate the kNN's probability of classification error than objectives\nproposed in previous works. Second, the ASK loss is interpretable: it preserves\nthe mutual information between the perturbed input and the kNN of the\nunperturbed input. We use the ASK loss to generate a novel attack method called\nthe ASK-Attack (ASK-Atk), which shows superior attack efficiency and accuracy\ndegradation relative to previous kNN attacks. Based on the ASK-Atk, we then\nderive an ASK-Defense (ASK-Def) method that optimizes the worst-case training\nloss induced by ASK-Atk.",
          "link": "http://arxiv.org/abs/2106.14300",
          "publishedOn": "2021-06-29T01:55:18.367Z",
          "wordCount": 601,
          "title": "ASK: Adversarial Soft k-Nearest Neighbor Attack and Defense. (arXiv:2106.14300v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1907.12727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1\">Adolf Pfefferbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1\">Edith V. Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "With recent advances in deep learning, neuroimaging studies increasingly rely\non convolutional networks (ConvNets) to predict diagnosis based on MR images.\nTo gain a better understanding of how a disease impacts the brain, the studies\nvisualize the salience maps of the ConvNet highlighting voxels within the brain\nmajorly contributing to the prediction. However, these salience maps are\ngenerally confounded, i.e., some salient regions are more predictive of\nconfounding variables (such as age) than the diagnosis. To avoid such\nmisinterpretation, we propose in this paper an approach that aims to visualize\nconfounder-free saliency maps that only highlight voxels predictive of the\ndiagnosis. The approach incorporates univariate statistical tests to identify\nconfounding effects within the intermediate features learned by ConvNet. The\ninfluence from the subset of confounded features is then removed by a novel\npartial back-propagation procedure. We use this two-step approach to visualize\nconfounder-free saliency maps extracted from synthetic and two real datasets.\nThese experiments reveal the potential of our visualization in producing\nunbiased model-interpretation.",
          "link": "http://arxiv.org/abs/1907.12727",
          "publishedOn": "2021-06-29T01:55:18.353Z",
          "wordCount": 638,
          "title": "Confounder-Aware Visualization of ConvNets. (arXiv:1907.12727v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalla_D/0/1/0/all/0/1\">Divya Chandrika Kalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_B/0/1/0/all/0/1\">Bedant Agarwal</a>",
          "description": "Powerful deep learning algorithms open an opportunity for solving non-image\nMachine Learning (ML) problems by transforming these problems to into the image\nrecognition problems. The CPC-R algorithm presented in this chapter converts\nnon-image data into images by visualizing non-image data. Then deep learning\nCNN algorithms solve the learning problems on these images. The design of the\nCPC-R algorithm allows preserving all high-dimensional information in 2-D\nimages. The use of pair values mapping instead of single value mapping used in\nthe alternative approaches allows encoding each n-D point with 2 times fewer\nvisual elements. The attributes of an n-D point are divided into pairs of its\nvalues and each pair is visualized as 2-D points in the same 2-D Cartesian\ncoordinates. Next, grey scale or color intensity values are assigned to each\npair to encode the order of pairs. This is resulted in the heatmap image. The\ncomputational experiments with CPC-R are conducted for different CNN\narchitectures, and methods to optimize the CPC-R images showing that the\ncombined CPC-R and deep learning CNN algorithms are able to solve non-image ML\nproblems reaching high accuracy on the benchmark datasets. This chapter expands\nour prior work by adding more experiments to test accuracy of classification,\nexploring saliency and informativeness of discovered features to test their\ninterpretability, and generalizing the approach.",
          "link": "http://arxiv.org/abs/2106.14350",
          "publishedOn": "2021-06-29T01:55:18.348Z",
          "wordCount": 655,
          "title": "Deep Learning Image Recognition for Non-images. (arXiv:2106.14350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azimi_F/0/1/0/all/0/1\">Fatemeh Azimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1\">Federico Raue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1\">Joern Hees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>",
          "description": "Spatial Transformer Networks (STN) can generate geometric transformations\nwhich modify input images to improve the classifier's performance. In this\nwork, we combine the idea of STN with Reinforcement Learning (RL). To this end,\nwe break the affine transformation down into a sequence of simple and discrete\ntransformations. We formulate the task as a Markovian Decision Process (MDP)\nand use RL to solve this sequential decision-making problem. STN architectures\nlearn the transformation parameters by minimizing the classification error and\nbackpropagating the gradients through a sub-differentiable sampling module. In\nour method, we are not bound to the differentiability of the sampling modules.\nMoreover, we have freedom in designing the objective rather than only\nminimizing the error; e.g., we can directly set the target as maximizing the\naccuracy. We design multiple experiments to verify the effectiveness of our\nmethod using cluttered MNIST and Fashion-MNIST datasets and show that our\nmethod outperforms STN with a proper definition of MDP components.",
          "link": "http://arxiv.org/abs/2106.14295",
          "publishedOn": "2021-06-29T01:55:18.338Z",
          "wordCount": 587,
          "title": "A Reinforcement Learning Approach for Sequential Spatial Transformer Networks. (arXiv:2106.14295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.06365",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Juditsky_A/0/1/0/all/0/1\">Anatoli Juditsky</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kulunchakov_A/0/1/0/all/0/1\">Andrei Kulunchakov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tsyntseus_H/0/1/0/all/0/1\">Hlib Tsyntseus</a>",
          "description": "In this paper, we discuss application of iterative Stochastic Optimization\nroutines to the problem of sparse signal recovery from noisy observation. Using\nStochastic Mirror Descent algorithm as a building block, we develop a\nmultistage procedure for recovery of sparse solutions to Stochastic\nOptimization problem under assumption of smoothness and quadratic minoration on\nthe expected objective. An interesting feature of the proposed algorithm is\nlinear convergence of the approximate solution during the preliminary phase of\nthe routine when the component of stochastic error in the gradient observation\nwhich is due to bad initial approximation of the optimal solution is larger\nthan the \"ideal\" asymptotic error component owing to observation noise \"at the\noptimal solution.\" We also show how one can straightforwardly enhance\nreliability of the corresponding solution by using Median-of-Means like\ntechniques.\n\nWe illustrate the performance of the proposed algorithms in application to\nclassical problems of recovery of sparse and low rank signals in linear\nregression framework. We show, under rather weak assumption on the regressor\nand noise distributions, how they lead to parameter estimates which obey (up to\nfactors which are logarithmic in problem dimension and confidence level) the\nbest known to us accuracy bounds.",
          "link": "http://arxiv.org/abs/2006.06365",
          "publishedOn": "2021-06-29T01:55:18.333Z",
          "wordCount": 643,
          "title": "Sparse recovery by reduced variance stochastic approximation. (arXiv:2006.06365v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.00413",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">Jiancheng Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Q/0/1/0/all/0/1\">Qiang Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>",
          "description": "Two-stage ensemble-based forecasting methods have been studied extensively in\nthe wind power forecasting field. However, deep learning-based wind power\nforecasting studies have not investigated two aspects. In the first stage,\ndifferent learning structures considering multiple inputs and multiple outputs\nhave not been discussed. In the second stage, the model extrapolation issue has\nnot been investigated. Therefore, we develop four deep neural networks for the\nfirst stage to learn data features considering the input-and-output structure.\nWe then explore the model extrapolation issue in the second stage using\ndifferent modeling methods. Considering the overfitting issue, we propose a new\nmoving window-based algorithm using a validation set in the first stage to\nupdate the training data in both stages with two different moving window\nprocesses.Experiments were conducted at three wind farms, and the results\ndemonstrate that the model with single input multiple output structure obtains\nbetter forecasting accuracy compared to existing models. In addition, the ridge\nregression method results in a better ensemble model that can further improve\nforecasting accuracy compared to existing machine learning methods. Finally,\nthe proposed two-stage forecasting algorithm can generate more accurate and\nstable results than existing algorithms.",
          "link": "http://arxiv.org/abs/2006.00413",
          "publishedOn": "2021-06-29T01:55:18.318Z",
          "wordCount": 658,
          "title": "Two-stage framework for short-term wind power forecasting using different feature-learning models. (arXiv:2006.00413v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08239",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>",
          "description": "Interpretability is a critical factor in applying complex deep learning\nmodels to advance the understanding of brain disorders in neuroimaging studies.\nTo interpret the decision process of a trained classifier, existing techniques\ntypically rely on saliency maps to quantify the voxel-wise or feature-level\nimportance for classification through partial derivatives. Despite providing\nsome level of localization, these maps are not human-understandable from the\nneuroscience perspective as they do not inform the specific meaning of the\nalteration linked to the brain disorder. Inspired by the image-to-image\ntranslation scheme, we propose to train simulator networks that can warp a\ngiven image to inject or remove patterns of the disease. These networks are\ntrained such that the classifier produces consistently increased or decreased\nprediction logits for the simulated images. Moreover, we propose to couple all\nthe simulators into a unified model based on conditional convolution. We\napplied our approach to interpreting classifiers trained on a synthetic dataset\nand two neuroimaging datasets to visualize the effect of the Alzheimer's\ndisease and alcohol use disorder. Compared to the saliency maps generated by\nbaseline approaches, our simulations and visualizations based on the Jacobian\ndeterminants of the warping field reveal meaningful and understandable patterns\nrelated to the diseases.",
          "link": "http://arxiv.org/abs/2102.08239",
          "publishedOn": "2021-06-29T01:55:18.305Z",
          "wordCount": 673,
          "title": "Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models. (arXiv:2102.08239v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brennan_M/0/1/0/all/0/1\">Matthew Brennan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bresler_G/0/1/0/all/0/1\">Guy Bresler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopkins_S/0/1/0/all/0/1\">Samuel B. Hopkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramm_T/0/1/0/all/0/1\">Tselil Schramm</a>",
          "description": "Researchers currently use a number of approaches to predict and substantiate\ninformation-computation gaps in high-dimensional statistical estimation\nproblems. A prominent approach is to characterize the limits of restricted\nmodels of computation, which on the one hand yields strong computational lower\nbounds for powerful classes of algorithms and on the other hand helps guide the\ndevelopment of efficient algorithms. In this paper, we study two of the most\npopular restricted computational models, the statistical query framework and\nlow-degree polynomials, in the context of high-dimensional hypothesis testing.\nOur main result is that under mild conditions on the testing problem, the two\nclasses of algorithms are essentially equivalent in power. As corollaries, we\nobtain new statistical query lower bounds for sparse PCA, tensor PCA and\nseveral variants of the planted clique problem.",
          "link": "http://arxiv.org/abs/2009.06107",
          "publishedOn": "2021-06-29T01:55:18.298Z",
          "wordCount": 639,
          "title": "Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent. (arXiv:2009.06107v3 [cs.CC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tranos_D/0/1/0/all/0/1\">Damianos Tranos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proutiere_A/0/1/0/all/0/1\">Alexandre Proutiere</a>",
          "description": "We consider Markov Decision Processes (MDPs) with deterministic transitions\nand study the problem of regret minimization, which is central to the analysis\nand design of optimal learning algorithms. We present logarithmic\nproblem-specific regret lower bounds that explicitly depend on the system\nparameter (in contrast to previous minimax approaches) and thus, truly quantify\nthe fundamental limit of performance achievable by any learning algorithm.\nDeterministic MDPs can be interpreted as graphs and analyzed in terms of their\ncycles, a fact which we leverage in order to identify a class of deterministic\nMDPs whose regret lower bound can be determined numerically. We further\nexemplify this result on a deterministic line search problem, and a\ndeterministic MDP with state-dependent rewards, whose regret lower bounds we\ncan state explicitly. These bounds share similarities with the known\nproblem-specific bound of the multi-armed bandit problem and suggest that\nnavigation on a deterministic MDP need not have an effect on the performance of\na learning algorithm.",
          "link": "http://arxiv.org/abs/2106.14338",
          "publishedOn": "2021-06-29T01:55:18.023Z",
          "wordCount": 585,
          "title": "Regret Analysis in Deterministic Reinforcement Learning. (arXiv:2106.14338v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiawei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Steve Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesan_R/0/1/0/all/0/1\">Rangharajan Venkatesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1\">Brucek Khailany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dally_B/0/1/0/all/0/1\">Bill Dally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>",
          "description": "Training large-scale deep neural networks (DNNs) currently requires a\nsignificant amount of energy, leading to serious environmental impacts. One\npromising approach to reduce the energy costs is representing DNNs with\nlow-precision numbers. While it is common to train DNNs with forward and\nbackward propagation in low-precision, training directly over low-precision\nweights, without keeping a copy of weights in high-precision, still remains to\nbe an unsolved problem. This is due to complex interactions between learning\nalgorithms and low-precision number systems. To address this, we jointly design\na low-precision training framework involving a logarithmic number system (LNS)\nand a multiplicative weight update training method, termed LNS-Madam. LNS has a\nhigh dynamic range even in a low-bitwidth setting, leading to high energy\nefficiency and making it relevant for on-board training in energy-constrained\nedge devices. We design LNS to have the flexibility of choosing different bases\nfor weights and gradients, as they usually require different quantization gaps\nand dynamic ranges during training. By drawing the connection between LNS and\nmultiplicative update, LNS-Madam ensures low quantization error during weight\nupdate, leading to a stable convergence even if the bitwidth is limited.\nCompared to using a fixed-point or floating-point number system and training\nwith popular learning algorithms such as SGD and Adam, our joint design with\nLNS and LNS-Madam optimizer achieves better accuracy while requiring smaller\nbitwidth. Notably, with only 5-bit for gradients, the proposed training\nframework achieves accuracy comparable to full-precision state-of-the-art\nmodels such as ResNet-50 and BERT. After conducting energy estimations by\nanalyzing the math datapath units during training, the results show that our\ndesign achieves over 60x energy reduction compared to FP32 on BERT models.",
          "link": "http://arxiv.org/abs/2106.13914",
          "publishedOn": "2021-06-29T01:55:18.010Z",
          "wordCount": 716,
          "title": "Low-Precision Training in Logarithmic Number System using Multiplicative Weight Update. (arXiv:2106.13914v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chung-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroer_C/0/1/0/all/0/1\">Christian Kroer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>",
          "description": "Regret-based algorithms are highly efficient at finding approximate Nash\nequilibria in sequential games such as poker games. However, most regret-based\nalgorithms, including counterfactual regret minimization (CFR) and its\nvariants, rely on iterate averaging to achieve convergence. Inspired by recent\nadvances on last-iterate convergence of optimistic algorithms in zero-sum\nnormal-form games, we study this phenomenon in sequential games, and provide a\ncomprehensive study of last-iterate convergence for zero-sum extensive-form\ngames with perfect recall (EFGs), using various optimistic regret-minimization\nalgorithms over treeplexes. This includes algorithms using the vanilla entropy\nor squared Euclidean norm regularizers, as well as their dilated versions which\nadmit more efficient implementation. In contrast to CFR, we show that all of\nthese algorithms enjoy last-iterate convergence, with some of them even\nconverging exponentially fast. We also provide experiments to further support\nour theoretical results.",
          "link": "http://arxiv.org/abs/2106.14326",
          "publishedOn": "2021-06-29T01:55:17.976Z",
          "wordCount": 557,
          "title": "Last-iterate Convergence in Extensive-Form Games. (arXiv:2106.14326v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1\">Ivan Skorokhodov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ignatyev_S/0/1/0/all/0/1\">Savva Ignatyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>",
          "description": "In most existing learning systems, images are typically viewed as 2D pixel\narrays. However, in another paradigm gaining popularity, a 2D image is\nrepresented as an implicit neural representation (INR) - an MLP that predicts\nan RGB pixel value given its (x,y) coordinate. In this paper, we propose two\nnovel architectural techniques for building INR-based image decoders:\nfactorized multiplicative modulation and multi-scale INRs, and use them to\nbuild a state-of-the-art continuous image GAN. Previous attempts to adapt INRs\nfor image generation were limited to MNIST-like datasets and do not scale to\ncomplex real-world data. Our proposed INR-GAN architecture improves the\nperformance of continuous image generators by several times, greatly reducing\nthe gap between continuous image GANs and pixel-based ones. Apart from that, we\nexplore several exciting properties of the INR-based decoders, like\nout-of-the-box superresolution, meaningful image-space interpolation,\naccelerated inference of low-resolution images, an ability to extrapolate\noutside of image boundaries, and strong geometric prior. The project page is\nlocated at https://universome.github.io/inr-gan.",
          "link": "http://arxiv.org/abs/2011.12026",
          "publishedOn": "2021-06-29T01:55:17.967Z",
          "wordCount": 629,
          "title": "Adversarial Generation of Continuous Images. (arXiv:2011.12026v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Boris Rubinstein</a>",
          "description": "Neural networks mapping sequences to sequences (seq2seq) lead to significant\nprogress in machine translation and speech recognition. Their traditional\narchitecture includes two recurrent networks (RNs) followed by a linear\npredictor. In this manuscript we perform analysis of a corresponding algorithm\nand show that the parameters of the RNs of the well trained predictive network\nare not independent of each other. Their dependence can be used to\nsignificantly improve the network effectiveness. The traditional seq2seq\nalgorithms require short term memory of a size proportional to the predicted\nsequence length. This requirement is quite difficult to implement in a\nneuroscience context. We present a novel memoryless algorithm for seq2seq\npredictive networks and compare it to the traditional one in the context of\ntime series prediction. We show that the new algorithm is more robust and makes\npredictions with higher accuracy than the traditional one.",
          "link": "http://arxiv.org/abs/2106.14120",
          "publishedOn": "2021-06-29T01:55:17.961Z",
          "wordCount": 578,
          "title": "On a novel training algorithm for sequence-to-sequence predictive recurrent networks. (arXiv:2106.14120v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arseniev_Koehler_A/0/1/0/all/0/1\">Alina Arseniev-Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochran_S/0/1/0/all/0/1\">Susan D. Cochran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mays_V/0/1/0/all/0/1\">Vickie M. Mays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jacob Gates Foster</a>",
          "description": "There is an escalating need for methods to identify latent patterns in text\ndata from many domains. We introduce a new method to identify topics in a\ncorpus and represent documents as topic sequences. Discourse Atom Topic\nModeling draws on advances in theoretical machine learning to integrate topic\nmodeling and word embedding, capitalizing on the distinct capabilities of each.\nWe first identify a set of vectors (\"discourse atoms\") that provide a sparse\nrepresentation of an embedding space. Atom vectors can be interpreted as latent\ntopics: Through a generative model, atoms map onto distributions over words;\none can also infer the topic that generated a sequence of words. We illustrate\nour method with a prominent example of underutilized text: the U.S. National\nViolent Death Reporting System (NVDRS). The NVDRS summarizes violent death\nincidents with structured variables and unstructured narratives. We identify\n225 latent topics in the narratives (e.g., preparation for death and physical\naggression); many of these topics are not captured by existing structured\nvariables. Motivated by known patterns in suicide and homicide by gender, and\nrecent research on gender biases in semantic space, we identify the gender bias\nof our topics (e.g., a topic about pain medication is feminine). We then\ncompare the gender bias of topics to their prevalence in narratives of female\nversus male victims. Results provide a detailed quantitative picture of\nreporting about lethal violence and its gendered nature. Our method offers a\nflexible and broadly applicable approach to model topics in text data.",
          "link": "http://arxiv.org/abs/2106.14365",
          "publishedOn": "2021-06-29T01:55:17.956Z",
          "wordCount": 698,
          "title": "Integrating topic modeling and word embedding to characterize violent deaths. (arXiv:2106.14365v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1\">Frederic Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_V/0/1/0/all/0/1\">Viraj Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1\">Andrej Risteski</a>",
          "description": "Normalizing flows are among the most popular paradigms in generative\nmodeling, especially for images, primarily because we can efficiently evaluate\nthe likelihood of a data point. This is desirable both for evaluating the fit\nof a model, and for ease of training, as maximizing the likelihood can be done\nby gradient descent. However, training normalizing flows comes with\ndifficulties as well: models which produce good samples typically need to be\nextremely deep -- which comes with accompanying vanishing/exploding gradient\nproblems. A very related problem is that they are often poorly conditioned:\nsince they are parametrized as invertible maps from $\\mathbb{R}^d \\to\n\\mathbb{R}^d$, and typical training data like images intuitively is\nlower-dimensional, the learned maps often have Jacobians that are close to\nbeing singular.\n\nIn our paper, we tackle representational aspects around depth and\nconditioning of normalizing flows: both for general invertible architectures,\nand for a particular common architecture, affine couplings. We prove that\n$\\Theta(1)$ affine coupling layers suffice to exactly represent a permutation\nor $1 \\times 1$ convolution, as used in GLOW, showing that representationally\nthe choice of partition is not a bottleneck for depth. We also show that\nshallow affine coupling networks are universal approximators in Wasserstein\ndistance if ill-conditioning is allowed, and experimentally investigate related\nphenomena involving padding. Finally, we show a depth lower bound for general\nflow architectures with few neurons per layer and bounded Lipschitz constant.",
          "link": "http://arxiv.org/abs/2010.01155",
          "publishedOn": "2021-06-29T01:55:17.950Z",
          "wordCount": 698,
          "title": "Representational aspects of depth and conditioning in normalizing flows. (arXiv:2010.01155v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meirman_T/0/1/0/all/0/1\">Tomer Meirman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stern_R/0/1/0/all/0/1\">Roni Stern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1\">Gilad Katz</a>",
          "description": "In data systems, activities or events are continuously collected in the field\nto trace their proper executions. Logging, which means recording sequences of\nevents, can be used for analyzing system failures and malfunctions, and\nidentifying the causes and locations of such issues. In our research we focus\non creating an Anomaly detection models for system logs. The task of anomaly\ndetection is identifying unexpected events in dataset, which differ from the\nnormal behavior. Anomaly detection models also assist in data systems analysis\ntasks.\n\nModern systems may produce such a large amount of events monitoring every\nindividual event is not feasible. In such cases, the events are often\naggregated over a fixed period of time, reporting the number of times every\nevent has occurred in that time period. This aggregation facilitates scaling,\nbut requires a different approach for anomaly detection. In this research, we\npresent a thorough analysis of the aggregated data and the relationships\nbetween aggregated events. Based on the initial phase of our research we\npresent graphs representations of our aggregated dataset, which represent the\ndifferent relationships between aggregated instances in the same context.\n\nUsing the graph representation, we propose Multiple-graphs autoencoder MGAE,\na novel convolutional graphs-autoencoder model which exploits the relationships\nof the aggregated instances in our unique dataset. MGAE outperforms standard\ngraph-autoencoder models and the different experiments. With our novel MGAE we\npresent 60% decrease in reconstruction error in comparison to standard graph\nautoencoder, which is expressed in reconstructing high-degree relationships.",
          "link": "http://arxiv.org/abs/2101.04053",
          "publishedOn": "2021-06-29T01:55:17.935Z",
          "wordCount": 738,
          "title": "Anomaly Detection for Aggregated Data Using Multi-Graph Autoencoder. (arXiv:2101.04053v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14045",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ning_N/0/1/0/all/0/1\">Ning Ning</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Qiu_J/0/1/0/all/0/1\">Jinwen Qiu</a>",
          "description": "The multivariate Bayesian structural time series (MBSTS) model\n\\citep{qiu2018multivariate,Jammalamadaka2019Predicting} as a generalized\nversion of many structural time series models, deals with inference and\nprediction for multiple correlated time series, where one also has the choice\nof using a different candidate pool of contemporaneous predictors for each\ntarget series. The MBSTS model has wide applications and is ideal for feature\nselection, time series forecasting, nowcasting, inferring causal impact, and\nothers. This paper demonstrates how to use the R package \\pkg{mbsts} for MBSTS\nmodeling, establishing a bridge between user-friendly and developer-friendly\nfunctions in package and the corresponding methodology. A simulated dataset and\nobject-oriented functions in the \\pkg{mbsts} package are explained in the way\nthat enables users to flexibly add or deduct some components, as well as to\nsimplify or complicate some settings.",
          "link": "http://arxiv.org/abs/2106.14045",
          "publishedOn": "2021-06-29T01:55:17.930Z",
          "wordCount": 574,
          "title": "The mbsts package: Multivariate Bayesian Structural Time Series Models in R. (arXiv:2106.14045v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meyer_A/0/1/0/all/0/1\">Angela Meyer</a>",
          "description": "The trend towards larger wind turbines and remote locations of wind farms\nfuels the demand for automated condition monitoring strategies that can reduce\nthe operating cost and avoid unplanned downtime. Normal behaviour modelling has\nbeen introduced to detect anomalous deviations from normal operation based on\nthe turbine's SCADA data. A growing number of machine learning models of the\nnormal behaviour of turbine subsystems are being developed by wind farm\nmanagers to this end. However, these models need to be kept track of, be\nmaintained and require frequent updates. This research explores multi-target\nmodels as a new approach to capturing a wind turbine's normal behaviour. We\npresent an overview of multi-target regression methods, motivate their\napplication and benefits in wind turbine condition monitoring, and assess their\nperformance in a wind farm case study. We find that multi-target models are\nadvantageous in comparison to single-target modelling in that they can reduce\nthe cost and effort of practical condition monitoring without compromising on\nthe accuracy. We also outline some areas of future research.",
          "link": "http://arxiv.org/abs/2012.03074",
          "publishedOn": "2021-06-29T01:55:17.925Z",
          "wordCount": 637,
          "title": "Multi-target normal behaviour models for wind farm condition monitoring. (arXiv:2012.03074v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16955",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Cieplinski_T/0/1/0/all/0/1\">Tobiasz Cieplinski</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Danel_T/0/1/0/all/0/1\">Tomasz Danel</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Podlewska_S/0/1/0/all/0/1\">Sabina Podlewska</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jastrzebski_S/0/1/0/all/0/1\">Stanislaw Jastrzebski</a>",
          "description": "Designing compounds with desired properties is a key element of the drug\ndiscovery process. However, measuring progress in the field has been\nchallenging due to the lack of realistic retrospective benchmarks, and the\nlarge cost of prospective validation. To close this gap, we propose a benchmark\nbased on docking, a popular computational method for assessing molecule binding\nto a protein. Concretely, the goal is to generate drug-like molecules that are\nscored highly by SMINA, a popular docking software. We observe that popular\ngraph-based generative models fail to generate molecules with a high docking\nscore when trained using a realistically sized training set. This suggests a\nlimitation of the current incarnation of models for de novo drug design.\nFinally, we propose a simplified version of the benchmark based on a simpler\nscoring function, and show that the tested models are able to partially solve\nit. We release the benchmark as an easy to use package available at\nhttps://github.com/cieplinski-tobiasz/smina-docking-benchmark. We hope that our\nbenchmark will serve as a stepping stone towards the goal of automatically\ngenerating promising drug candidates.",
          "link": "http://arxiv.org/abs/2006.16955",
          "publishedOn": "2021-06-29T01:55:17.919Z",
          "wordCount": 660,
          "title": "We Should at Least Be Able to Design Molecules That Dock Well. (arXiv:2006.16955v4 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valle_Perez_G/0/1/0/all/0/1\">Guillermo Valle-P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beskow_J/0/1/0/all/0/1\">Jonas Beskow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzapfel_A/0/1/0/all/0/1\">Andr&#xe9; Holzapfel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexanderson_S/0/1/0/all/0/1\">Simon Alexanderson</a>",
          "description": "Dance requires skillful composition of complex movements that follow\nrhythmic, tonal and timbral features of music. Formally, generating dance\nconditioned on a piece of music can be expressed as a problem of modelling a\nhigh-dimensional continuous motion signal, conditioned on an audio signal. In\nthis work we make two contributions to tackle this problem. First, we present a\nnovel probabilistic autoregressive architecture that models the distribution\nover future poses with a normalizing flow conditioned on previous poses as well\nas music context, using a multimodal transformer encoder. Second, we introduce\nthe currently largest 3D dance-motion dataset, obtained with a variety of\nmotion-capture technologies, and including both professional and casual\ndancers. Using this dataset, we compare our new model against two baselines,\nvia objective metrics and a user study, and show that both the ability to model\na probability distribution, as well as being able to attend over a large motion\nand music context are necessary to produce interesting, diverse, and realistic\ndance that matches the music.",
          "link": "http://arxiv.org/abs/2106.13871",
          "publishedOn": "2021-06-29T01:55:17.913Z",
          "wordCount": 612,
          "title": "Transflower: probabilistic autoregressive dance generation with multimodal attention. (arXiv:2106.13871v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03432",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Noack_M/0/1/0/all/0/1\">Marcus M. Noack</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sethian_J/0/1/0/all/0/1\">James A. Sethian</a>",
          "description": "Gaussian process regression is a widely-applied method for function\napproximation and uncertainty quantification. The technique has gained\npopularity recently in the machine learning community due to its robustness and\ninterpretability. The mathematical methods we discuss in this paper are an\nextension of the Gaussian-process framework. We are proposing advanced kernel\ndesigns that only allow for functions with certain desirable characteristics to\nbe elements of the reproducing kernel Hilbert space (RKHS) that underlies all\nkernel methods and serves as the sample space for Gaussian process regression.\nThese desirable characteristics reflect the underlying physics; two obvious\nexamples are symmetry and periodicity constraints. In addition, non-stationary\nkernel designs can be defined in the same framework to yield flexible\nmulti-task Gaussian processes. We will show the impact of advanced kernel\ndesigns on Gaussian processes using several synthetic and two scientific data\nsets. The results show that including domain knowledge, communicated through\nadvanced kernel designs, has a significant impact on the accuracy and relevance\nof the function approximation.",
          "link": "http://arxiv.org/abs/2102.03432",
          "publishedOn": "2021-06-29T01:55:17.898Z",
          "wordCount": 616,
          "title": "Advanced Stationary and Non-Stationary Kernel Designs for Domain-Aware Gaussian Processes. (arXiv:2102.03432v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abadi_M/0/1/0/all/0/1\">Martin Abadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plotkin_G/0/1/0/all/0/1\">Gordon Plotkin</a>",
          "description": "Describing systems in terms of choices and their resulting costs and rewards\noffers the promise of freeing algorithm designers and programmers from\nspecifying how those choices should be made; in implementations, the choices\ncan be realized by optimization techniques and,increasingly, by\nmachine-learning methods. We study this approach from a programming-language\nperspective. We define two small languages that support decision-making\nabstractions: one with choices and rewards, and the other additionally with\nprobabilities. We give both operational and denotational semantics.\n\nIn the case of the second language we consider three denotational semantics,\nwith varying degrees of correlation between possible program values and\nexpected rewards. The operational semantics combine the usual semantics of\nstandard constructs with optimization over spaces of possible execution\nstrategies. The denotational semantics, which are compositional rely on the\nselection monad, to handle choice, augmented with an auxiliary monad to handle\nother effects, such as rewards or probability.\n\nWe establish adequacy theorems that the two semantics coincide in all cases.\nWe also prove full abstraction at base types, with varying notions of\nobservation in the probabilistic case corresponding to the various degrees of\ncorrelation. We present axioms for choice combined with rewards and\nprobability, establishing completeness at base types for the case of rewards\nwithout probability.",
          "link": "http://arxiv.org/abs/2007.08926",
          "publishedOn": "2021-06-29T01:55:17.892Z",
          "wordCount": 701,
          "title": "Smart Choices and the Selection Monad. (arXiv:2007.08926v5 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yi Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Abhishek Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1\">Richard S. Sutton</a>",
          "description": "We introduce learning and planning algorithms for average-reward MDPs,\nincluding 1) the first general proven-convergent off-policy model-free control\nalgorithm without reference states, 2) the first proven-convergent off-policy\nmodel-free prediction algorithm, and 3) the first off-policy learning algorithm\nthat converges to the actual value function rather than to the value function\nplus an offset. All of our algorithms are based on using the\ntemporal-difference error rather than the conventional error when updating the\nestimate of the average reward. Our proof techniques are a slight\ngeneralization of those by Abounadi, Bertsekas, and Borkar (2001). In\nexperiments with an Access-Control Queuing Task, we show some of the\ndifficulties that can arise when using methods that rely on reference states\nand argue that our new algorithms can be significantly easier to use.",
          "link": "http://arxiv.org/abs/2006.16318",
          "publishedOn": "2021-06-29T01:55:17.886Z",
          "wordCount": 605,
          "title": "Learning and Planning in Average-Reward Markov Decision Processes. (arXiv:2006.16318v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08925",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_P/0/1/0/all/0/1\">Puyu Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lei_Y/0/1/0/all/0/1\">Yunwen Lei</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ying_Y/0/1/0/all/0/1\">Yiming Ying</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1\">Hai Zhang</a>",
          "description": "In this paper, we are concerned with differentially private {stochastic\ngradient descent (SGD)} algorithms in the setting of stochastic convex\noptimization (SCO). Most of the existing work requires the loss to be Lipschitz\ncontinuous and strongly smooth, and the model parameter to be uniformly\nbounded. However, these assumptions are restrictive as many popular losses\nviolate these conditions including the hinge loss for SVM, the absolute loss in\nrobust regression, and even the least square loss in an unbounded domain. We\nsignificantly relax these restrictive assumptions and establish privacy and\ngeneralization (utility) guarantees for private SGD algorithms using output and\ngradient perturbations associated with non-smooth convex losses. Specifically,\nthe loss function is relaxed to have an $\\alpha$-H\\\"{o}lder continuous gradient\n(referred to as $\\alpha$-H\\\"{o}lder smoothness) which instantiates the\nLipschitz continuity ($\\alpha=0$) and the strong smoothness ($\\alpha=1$). We\nprove that noisy SGD with $\\alpha$-H\\\"older smooth losses using gradient\nperturbation can guarantee $(\\epsilon,\\delta)$-differential privacy (DP) and\nattain optimal excess population risk\n$\\mathcal{O}\\Big(\\frac{\\sqrt{d\\log(1/\\delta)}}{n\\epsilon}+\\frac{1}{\\sqrt{n}}\\Big)$,\nup to logarithmic terms, with the gradient complexity $ \\mathcal{O}(\nn^{2-\\alpha\\over 1+\\alpha}+ n).$ This shows an important trade-off between\n$\\alpha$-H\\\"older smoothness of the loss and the computational complexity for\nprivate SGD with statistically optimal performance. In particular, our results\nindicate that $\\alpha$-H\\\"older smoothness with $\\alpha\\ge {1/2}$ is sufficient\nto guarantee $(\\epsilon,\\delta)$-DP of noisy SGD algorithms while achieving\noptimal excess risk with the linear gradient complexity $\\mathcal{O}(n).$",
          "link": "http://arxiv.org/abs/2101.08925",
          "publishedOn": "2021-06-29T01:55:17.881Z",
          "wordCount": 680,
          "title": "Differentially Private SGD with Non-Smooth Losses. (arXiv:2101.08925v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1\">Vitaly Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1\">Tijana Zrnic</a>",
          "description": "We consider a sequential setting in which a single dataset of individuals is\nused to perform adaptively-chosen analyses, while ensuring that the\ndifferential privacy loss of each participant does not exceed a pre-specified\nprivacy budget. The standard approach to this problem relies on bounding a\nworst-case estimate of the privacy loss over all individuals and all possible\nvalues of their data, for every single analysis. Yet, in many scenarios this\napproach is overly conservative, especially for \"typical\" data points which\nincur little privacy loss by participation in most of the analyses. In this\nwork, we give a method for tighter privacy loss accounting based on the value\nof a personalized privacy loss estimate for each individual in each analysis.\nTo implement the accounting method we design a filter for R\\'enyi differential\nprivacy. A filter is a tool that ensures that the privacy parameter of a\ncomposed sequence of algorithms with adaptively-chosen privacy parameters does\nnot exceed a pre-specified budget. Our filter is simpler and tighter than the\nknown filter for $(\\epsilon,\\delta)$-differential privacy by Rogers et al. We\napply our results to the analysis of noisy gradient descent and show that\npersonalized accounting can be practical, easy to implement, and can only make\nthe privacy-utility tradeoff tighter.",
          "link": "http://arxiv.org/abs/2008.11193",
          "publishedOn": "2021-06-29T01:55:17.873Z",
          "wordCount": 677,
          "title": "Individual Privacy Accounting via a Renyi Filter. (arXiv:2008.11193v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13823",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Shangnan_Z/0/1/0/all/0/1\">Zhou Shangnan</a>",
          "description": "Quantum machine learning is an emerging field at the intersection of machine\nlearning and quantum computing. A central quantity for the theoretical\nfoundation of quantum machine learning is the quantum cross entropy. In this\npaper, we present one operational interpretation of this quantity, that the\nquantum cross entropy is the compression rate for sub-optimal quantum source\ncoding. To do so, we give a simple, universal quantum data compression\nprotocol, which is developed based on quantum generalization of variable-length\ncoding, as well as quantum strong typicality.",
          "link": "http://arxiv.org/abs/2106.13823",
          "publishedOn": "2021-06-29T01:55:17.858Z",
          "wordCount": 532,
          "title": "Quantum Data Compression and Quantum Cross Entropy. (arXiv:2106.13823v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>",
          "description": "In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have primarily focused on processing text for\nclassification and small-scale databases. This paper describes a novel\nmultimodal deep learning architecture, called TechDoc, for technical document\nclassification, which utilizes both natural language and descriptive images to\ntrain hierarchical classifiers. The architecture synthesizes convolutional\nneural networks and recurrent neural networks through an integrated training\nprocess. We applied the architecture to a large multimodal technical document\ndatabase and trained the model for classifying documents based on the\nhierarchical International Patent Classification system. Our results show that\nthe trained neural network presents a greater classification accuracy than\nthose using a single modality and several earlier text classification methods.\nThe trained model can potentially be scaled to millions of real-world technical\ndocuments with both text and figures, which is useful for data and knowledge\nmanagement in large technology companies and organizations.",
          "link": "http://arxiv.org/abs/2106.14269",
          "publishedOn": "2021-06-29T01:55:17.851Z",
          "wordCount": 625,
          "title": "Deep Learning for Technical Document Classification. (arXiv:2106.14269v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10510",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Nasim_M/0/1/0/all/0/1\">M Quamer Nasim</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Maiti_T/0/1/0/all/0/1\">Tannistha Maiti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Srivastava_A/0/1/0/all/0/1\">Ayush Srivastava</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Singh_T/0/1/0/all/0/1\">Tarry Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>",
          "description": "Deep neural networks (DNNs) can learn accurately from large quantities of\nlabeled input data, but DNNs sometimes fail to generalize to test data sampled\nfrom different input distributions. Unsupervised Deep Domain Adaptation (DDA)\nproves useful when no input labels are available, and distribution shifts are\nobserved in the target domain (TD). Experiments are performed on seismic images\nof the F3 block 3D dataset from offshore Netherlands (source domain; SD) and\nPenobscot 3D survey data from Canada (target domain; TD). Three geological\nclasses from SD and TD that have similar reflection patterns are considered. In\nthe present study, an improved deep neural network architecture named\nEarthAdaptNet (EAN) is proposed to semantically segment the seismic images. We\nspecifically use a transposed residual unit to replace the traditional dilated\nconvolution in the decoder block. The EAN achieved a pixel-level accuracy >84%\nand an accuracy of ~70% for the minority classes, showing improved performance\ncompared to existing architectures. In addition, we introduced the CORAL\n(Correlation Alignment) method to the EAN to create an unsupervised deep domain\nadaptation network (EAN-DDA) for the classification of seismic reflections\nfromF3 and Penobscot. Maximum class accuracy achieved was ~99% for class 2 of\nPenobscot with >50% overall accuracy. Taken together, EAN-DDA has the potential\nto classify target domain seismic facies classes with high accuracy.",
          "link": "http://arxiv.org/abs/2011.10510",
          "publishedOn": "2021-06-29T01:55:17.845Z",
          "wordCount": 701,
          "title": "Seismic Facies Analysis: A Deep Domain Adaptation Approach. (arXiv:2011.10510v2 [physics.geo-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shaojie Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>",
          "description": "Deep equilibrium networks (DEQs) are a new class of models that eschews\ntraditional depth in favor of finding the fixed point of a single nonlinear\nlayer. These models have been shown to achieve performance competitive with the\nstate-of-the-art deep networks while using significantly less memory. Yet they\nare also slower, brittle to architectural choices, and introduce potential\ninstability to the model. In this paper, we propose a regularization scheme for\nDEQ models that explicitly regularizes the Jacobian of the fixed-point update\nequations to stabilize the learning of equilibrium models. We show that this\nregularization adds only minimal computational cost, significantly stabilizes\nthe fixed-point convergence in both forward and backward passes, and scales\nwell to high-dimensional, realistic domains (e.g., WikiText-103 language\nmodeling and ImageNet classification). Using this method, we demonstrate, for\nthe first time, an implicit-depth model that runs with approximately the same\nspeed and level of performance as popular conventional deep networks such as\nResNet-101, while still maintaining the constant memory footprint and\narchitectural simplicity of DEQs. Code is available at\nhttps://github.com/locuslab/deq .",
          "link": "http://arxiv.org/abs/2106.14342",
          "publishedOn": "2021-06-29T01:55:17.829Z",
          "wordCount": 610,
          "title": "Stabilizing Equilibrium Models by Jacobian Regularization. (arXiv:2106.14342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuezhang_L/0/1/0/all/0/1\">Liu Yuezhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "It is well known that artificial neural networks are vulnerable to\nadversarial examples, in which great efforts have been made to improve the\nrobustness. However, such examples are usually imperceptible to humans, and\nthus their effect on biological neural circuits is largely unknown. This paper\nwill investigate the adversarial robustness in a simulated cerebellum, a\nwell-studied supervised learning system in computational neuroscience.\nSpecifically, we propose to study three unique characteristics revealed in the\ncerebellum: (i) network width; (ii) long-term depression on the parallel\nfiber-Purkinje cell synapses; (iii) sparse connectivity in the granule layer,\nand hypothesize that they will be beneficial for improving robustness. To the\nbest of our knowledge, this is the first attempt to examine the adversarial\nrobustness in simulated cerebellum models.\n\nThe results are negative in the experimental phase -- no significant\nimprovements in robustness are discovered from the proposed three mechanisms.\nConsequently, the cerebellum is expected to be vulnerable to adversarial\nexamples as the deep neural networks under batch training. Neuroscientists are\nencouraged to fool the biological system in experiments with adversarial\nattacks.",
          "link": "http://arxiv.org/abs/2012.02976",
          "publishedOn": "2021-06-29T01:55:17.807Z",
          "wordCount": 645,
          "title": "Evaluating adversarial robustness in simulated cerebellum. (arXiv:2012.02976v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashfahani_A/0/1/0/all/0/1\">Andri Ashfahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1\">Mahardhika Pratama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lughofer_E/0/1/0/all/0/1\">Edwin Lughofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_E/0/1/0/all/0/1\">Edward Yapp Kien Yee</a>",
          "description": "The common practice of quality monitoring in industry relies on manual\ninspection well-known to be slow, error-prone and operator-dependent. This\nissue raises strong demand for automated real-time quality monitoring developed\nfrom data-driven approaches thus alleviating from operator dependence and\nadapting to various process uncertainties. Nonetheless, current approaches do\nnot take into account the streaming nature of sensory information while relying\nheavily on hand-crafted features making them application-specific. This paper\nproposes the online quality monitoring methodology developed from recently\ndeveloped deep learning algorithms for data streams, Neural Networks with\nDynamically Evolved Capacity (NADINE), namely NADINE++. It features the\nintegration of 1-D and 2-D convolutional layers to extract natural features of\ntime-series and visual data streams captured from sensors and cameras of the\ninjection molding machines from our own project. Real-time experiments have\nbeen conducted where the online quality monitoring task is simulated on the fly\nunder the prequential test-then-train fashion - the prominent data stream\nevaluation protocol. Comparison with the state-of-the-art techniques clearly\nexhibits the advantage of NADINE++ with 4.68\\% improvement on average for the\nquality monitoring task in streaming environments. To support the reproducible\nresearch initiative, codes, results of NADINE++ along with supplementary\nmaterials and injection molding dataset are made available in\n\\url{https://github.com/ContinualAL/NADINE-IJCNN2021}.",
          "link": "http://arxiv.org/abs/2106.13955",
          "publishedOn": "2021-06-29T01:55:17.801Z",
          "wordCount": 659,
          "title": "Autonomous Deep Quality Monitoring in Streaming Environments. (arXiv:2106.13955v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vakilian_A/0/1/0/all/0/1\">Ali Vakilian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalciner_M/0/1/0/all/0/1\">Mustafa Yal&#xe7;&#x131;ner</a>",
          "description": "We consider the $k$-clustering problem with $\\ell_p$-norm cost, which\nincludes $k$-median, $k$-means and $k$-center cost functions, under an\nindividual notion of fairness proposed by Jung et al. [2020]: given a set of\npoints $P$ of size $n$, a set of $k$ centers induces a fair clustering if for\nevery point $v\\in P$, $v$ can find a center among its $n/k$ closest neighbors.\nRecently, Mahabadi and Vakilian [2020] showed how to get a\n$(p^{O(p)},7)$-bicriteria approximation for the problem of fair $k$-clustering\nwith $\\ell_p$-norm cost: every point finds a center within distance at most $7$\ntimes its distance to its $(n/k)$-th closest neighbor and the $\\ell_p$-norm\ncost of the solution is at most $p^{O(p)}$ times the cost of an optimal fair\nsolution. In this work, for any $\\varepsilon>0$, we present an improved $(16^p\n+\\varepsilon,3)$-bicriteria approximation for the fair $k$-clustering with\n$\\ell_p$-norm cost. To achieve our guarantees, we extend the framework of\n[Charikar et al., 2002, Swamy, 2016] and devise a $16^p$-approximation\nalgorithm for the facility location with $\\ell_p$-norm cost under matroid\nconstraint which might be of an independent interest. Besides, our approach\nsuggests a reduction from our individually fair clustering to a clustering with\na group fairness requirement proposed by Kleindessner et al. [2019], which is\nessentially the median matroid problem [Krishnaswamy et al., 2011].",
          "link": "http://arxiv.org/abs/2106.14043",
          "publishedOn": "2021-06-29T01:55:17.792Z",
          "wordCount": 655,
          "title": "Improved Approximation Algorithms for Individually Fair Clustering. (arXiv:2106.14043v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Que_Z/0/1/0/all/0/1\">Zhiqiang Que</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Erwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marikar_U/0/1/0/all/0/1\">Umar Marikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_E/0/1/0/all/0/1\">Eric Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngadiuba_J/0/1/0/all/0/1\">Jennifer Ngadiuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_H/0/1/0/all/0/1\">Hamza Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borzyszkowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Borzyszkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aarrestad_T/0/1/0/all/0/1\">Thea Aarrestad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loncar_V/0/1/0/all/0/1\">Vladimir Loncar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summers_S/0/1/0/all/0/1\">Sioni Summers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_P/0/1/0/all/0/1\">Peter Y Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luk_W/0/1/0/all/0/1\">Wayne Luk</a>",
          "description": "This paper presents novel reconfigurable architectures for reducing the\nlatency of recurrent neural networks (RNNs) that are used for detecting\ngravitational waves. Gravitational interferometers such as the LIGO detectors\ncapture cosmic events such as black hole mergers which happen at unknown times\nand of varying durations, producing time-series data. We have developed a new\narchitecture capable of accelerating RNN inference for analyzing time-series\ndata from LIGO detectors. This architecture is based on optimizing the\ninitiation intervals (II) in a multi-layer LSTM (Long Short-Term Memory)\nnetwork, by identifying appropriate reuse factors for each layer. A\ncustomizable template for this architecture has been designed, which enables\nthe generation of low-latency FPGA designs with efficient resource utilization\nusing high-level synthesis tools. The proposed approach has been evaluated\nbased on two LSTM models, targeting a ZYNQ 7045 FPGA and a U250 FPGA.\nExperimental results show that with balanced II, the number of DSPs can be\nreduced up to 42% while achieving the same IIs. When compared to other\nFPGA-based LSTM designs, our design can achieve about 4.92 to 12.4 times lower\nlatency.",
          "link": "http://arxiv.org/abs/2106.14089",
          "publishedOn": "2021-06-29T01:55:17.786Z",
          "wordCount": 655,
          "title": "Accelerating Recurrent Neural Networks for Gravitational Wave Experiments. (arXiv:2106.14089v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1\">Joao Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibert_X/0/1/0/all/0/1\">Xavier Gibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianqiao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dar-Shyang Lee</a>",
          "description": "Learning guarantees often rely on assumptions of i.i.d. data, which will\nlikely be violated in practice once predictors are deployed to perform\nreal-world tasks. Domain adaptation approaches thus appeared as a useful\nframework yielding extra flexibility in that distinct train and test data\ndistributions are supported, provided that other assumptions are satisfied such\nas covariate shift, which expects the conditional distributions over labels to\nbe independent of the underlying data distribution. Several approaches were\nintroduced in order to induce generalization across varying train and test data\nsources, and those often rely on the general idea of domain-invariance, in such\na way that the data-generating distributions are to be disregarded by the\nprediction model. In this contribution, we tackle the problem of generalizing\nacross data sources by approaching it from the opposite direction: we consider\na conditional modeling approach in which predictions, in addition to being\ndependent on the input data, use information relative to the underlying\ndata-generating distribution. For instance, the model has an explicit mechanism\nto adapt to changing environments and/or new data sources. We argue that such\nan approach is more generally applicable than current domain adaptation methods\nsince it does not require extra assumptions such as covariate shift and further\nyields simpler training algorithms that avoid a common source of training\ninstabilities caused by minimax formulations, often employed in\ndomain-invariant methods.",
          "link": "http://arxiv.org/abs/2106.13899",
          "publishedOn": "2021-06-29T01:55:17.780Z",
          "wordCount": 673,
          "title": "Domain Conditional Predictors for Domain Adaptation. (arXiv:2106.13899v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Chenzhuang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tiejun Li</a>",
          "description": "In vision-based reinforcement learning (RL) tasks, it is prevalent to assign\nthe auxiliary task with a surrogate self-supervised loss so as to obtain more\nsemantic representations and improve sample efficiency. However, abundant\ninformation in self-supervised auxiliary tasks has been disregarded, since the\nrepresentation learning part and the decision-making part are separated. To\nsufficiently utilize information in the auxiliary task, we present a simple yet\neffective idea to employ self-supervised loss as an intrinsic reward, called\nIntrinsically Motivated Self-Supervised learning in Reinforcement learning\n(IM-SSR). We formally show that the self-supervised loss can be decomposed as\nexploration for novel states and robustness improvement from nuisance\nelimination. IM-SSR can be effortlessly plugged into any reinforcement learning\nwith self-supervised auxiliary objectives with nearly no additional cost.\nCombined with IM-SSR, the previous underlying algorithms achieve salient\nimprovements on both sample efficiency and generalization in various\nvision-based robotics tasks from the DeepMind Control Suite, especially when\nthe reward signal is sparse.",
          "link": "http://arxiv.org/abs/2106.13970",
          "publishedOn": "2021-06-29T01:55:17.762Z",
          "wordCount": 587,
          "title": "Intrinsically Motivated Self-supervised Learning in Reinforcement Learning. (arXiv:2106.13970v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_C/0/1/0/all/0/1\">Chengping Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianxun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>",
          "description": "Partial differential equations (PDEs) play a fundamental role in modeling and\nsimulating problems across a wide range of disciplines. Recent advances in deep\nlearning have shown the great potential of physics-informed neural networks\n(PINNs) to solve PDEs as a basis for data-driven modeling and inverse analysis.\nHowever, the majority of existing PINN methods, based on fully-connected NNs,\npose intrinsic limitations to low-dimensional spatiotemporal parameterizations.\nMoreover, since the initial/boundary conditions (I/BCs) are softly imposed via\npenalty, the solution quality heavily relies on hyperparameter tuning. To this\nend, we propose the novel physics-informed convolutional-recurrent learning\narchitectures (PhyCRNet and PhyCRNet-s) for solving PDEs without any labeled\ndata. Specifically, an encoder-decoder convolutional long short-term memory\nnetwork is proposed for low-dimensional spatial feature extraction and temporal\nevolution learning. The loss function is defined as the aggregated discretized\nPDE residuals, while the I/BCs are hard-encoded in the network to ensure\nforcible satisfaction (e.g., periodic boundary padding). The networks are\nfurther enhanced by autoregressive and residual connections that explicitly\nsimulate time marching. The performance of our proposed methods has been\nassessed by solving three nonlinear PDEs (e.g., 2D Burgers' equations, the\n$\\lambda$-$\\omega$ and FitzHugh Nagumo reaction-diffusion equations), and\ncompared against the start-of-the-art baseline algorithms. The numerical\nresults demonstrate the superiority of our proposed methodology in the context\nof solution accuracy, extrapolability and generalizability.",
          "link": "http://arxiv.org/abs/2106.14103",
          "publishedOn": "2021-06-29T01:55:17.755Z",
          "wordCount": 662,
          "title": "PhyCRNet: Physics-informed Convolutional-Recurrent Network for Solving Spatiotemporal PDEs. (arXiv:2106.14103v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David W. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1\">Gertjan J. Burghouts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "This work considers predicting the relational structure of a hypergraph for a\ngiven set of vertices, as common for applications in particle physics,\nbiological systems and other complex combinatorial problems. A problem arises\nfrom the number of possible multi-way relationships, or hyperedges, scaling in\n$\\mathcal{O}(2^n)$ for a set of $n$ elements. Simply storing an indicator\ntensor for all relationships is already intractable for moderately sized $n$,\nprompting previous approaches to restrict the number of vertices a hyperedge\nconnects. Instead, we propose a recurrent hypergraph neural network that\npredicts the incidence matrix by iteratively refining an initial guess of the\nsolution. We leverage the property that most hypergraphs of interest are\nsparsely connected and reduce the memory requirement to $\\mathcal{O}(nk)$,\nwhere $k$ is the maximum number of positive edges, i.e., edges that actually\nexist. In order to counteract the linearly growing memory cost from training a\nlengthening sequence of refinement steps, we further propose an algorithm that\napplies backpropagation through time on randomly sampled subsequences. We\nempirically show that our method can match an increase in the intrinsic\ncomplexity without a performance decrease and demonstrate superior performance\ncompared to state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.13919",
          "publishedOn": "2021-06-29T01:55:17.749Z",
          "wordCount": 612,
          "title": "Recurrently Predicting Hypergraphs. (arXiv:2106.13919v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S.M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-06-29T01:55:17.742Z",
          "wordCount": 608,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>",
          "description": "Explainable machine learning models primarily justify predicted labels using\neither extractive rationales (i.e., subsets of input features) or free-text\nnatural language explanations (NLEs) as abstractive justifications. While NLEs\ncan be more comprehensive than extractive rationales, machine-generated NLEs\nhave been shown to sometimes lack commonsense knowledge. Here, we show that\ncommonsense knowledge can act as a bridge between extractive rationales and\nNLEs, rendering both types of explanations better. More precisely, we introduce\na unified framework, called RExC (Rationale-Inspired Explanations with\nCommonsense), that (1) extracts rationales as a set of features responsible for\nmachine predictions, (2) expands the extractive rationales using available\ncommonsense resources, and (3) uses the expanded knowledge to generate natural\nlanguage explanations. Our framework surpasses by a large margin the previous\nstate-of-the-art in generating NLEs across five tasks in both natural language\nprocessing and vision-language understanding, with human annotators\nconsistently rating the explanations generated by RExC to be more\ncomprehensive, grounded in commonsense, and overall preferred compared to\nprevious state-of-the-art models. Moreover, our work shows that\ncommonsense-grounded explanations can enhance both task performance and\nrationales extraction capabilities.",
          "link": "http://arxiv.org/abs/2106.13876",
          "publishedOn": "2021-06-29T01:55:17.726Z",
          "wordCount": 615,
          "title": "Rationale-Inspired Natural Language Explanations with Commonsense. (arXiv:2106.13876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atigh_M/0/1/0/all/0/1\">Mina Ghadimi Atigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_Ressel_M/0/1/0/all/0/1\">Martin Keller-Ressel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>",
          "description": "Hyperbolic space has become a popular choice of manifold for representation\nlearning of arbitrary data, from tree-like structures and text to graphs.\nBuilding on the success of deep learning with prototypes in Euclidean and\nhyperspherical spaces, a few recent works have proposed hyperbolic prototypes\nfor classification. Such approaches enable effective learning in\nlow-dimensional output spaces and can exploit hierarchical relations amongst\nclasses, but require privileged information about class labels to position the\nhyperbolic prototypes. In this work, we propose Hyperbolic Busemann Learning.\nThe main idea behind our approach is to position prototypes on the ideal\nboundary of the Poincare ball, which does not require prior label knowledge. To\nbe able to compute proximities to ideal prototypes, we introduce the penalised\nBusemann loss. We provide theory supporting the use of ideal prototypes and the\nproposed loss by proving its equivalence to logistic regression in the\none-dimensional case. Empirically, we show that our approach provides a natural\ninterpretation of classification confidence, while outperforming recent\nhyperspherical and hyperbolic prototype approaches.",
          "link": "http://arxiv.org/abs/2106.14472",
          "publishedOn": "2021-06-29T01:55:17.720Z",
          "wordCount": 593,
          "title": "Hyperbolic Busemann Learning with Ideal Prototypes. (arXiv:2106.14472v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14144",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Shichao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yangyang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ONeill_Z/0/1/0/all/0/1\">Zheng O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "As people spend up to 87% of their time indoors, intelligent Heating,\nVentilation, and Air Conditioning (HVAC) systems in buildings are essential for\nmaintaining occupant comfort and reducing energy consumption. Those HVAC\nsystems in modern smart buildings rely on real-time sensor readings, which in\npractice often suffer from various faults and could also be vulnerable to\nmalicious attacks. Such faulty sensor inputs may lead to the violation of\nindoor environment requirements (e.g., temperature, humidity, etc.) and the\nincrease of energy consumption. While many model-based approaches have been\nproposed in the literature for building HVAC control, it is costly to develop\naccurate physical models for ensuring their performance and even more\nchallenging to address the impact of sensor faults. In this work, we present a\nnovel learning-based framework for sensor fault-tolerant HVAC control, which\nincludes three deep learning based components for 1) generating temperature\nproposals with the consideration of possible sensor faults, 2) selecting one of\nthe proposals based on the assessment of their accuracy, and 3) applying\nreinforcement learning with the selected temperature proposal. Moreover, to\naddress the challenge of training data insufficiency in building-related tasks,\nwe propose a model-assisted learning method leveraging an abstract model of\nbuilding physical dynamics. Through extensive numerical experiments, we\ndemonstrate that the proposed fault-tolerant HVAC control framework can\nsignificantly reduce building temperature violations under a variety of sensor\nfault patterns while maintaining energy efficiency.",
          "link": "http://arxiv.org/abs/2106.14144",
          "publishedOn": "2021-06-29T01:55:17.687Z",
          "wordCount": 675,
          "title": "Model-assisted Learning-based Framework for Sensor Fault-Tolerant Building HVAC Control. (arXiv:2106.14144v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Dian Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>",
          "description": "In this letter, we propose a multi-task over-theair federated learning\n(MOAFL) framework, where multiple learning tasks share edge devices for data\ncollection and learning models under the coordination of a edge server (ES).\nSpecially, the model updates for all the tasks are transmitted and\nsuperpositioned concurrently over a non-orthogonal uplink channel via\nover-the-air computation, and the aggregation results of all the tasks are\nreconstructed at the ES through an extended version of the turbo compressed\nsensing algorithm. Both the convergence analysis and numerical results\ndemonstrate that the MOAFL framework can significantly reduce the uplink\nbandwidth consumption of multiple tasks without causing substantial learning\nperformance degradation.",
          "link": "http://arxiv.org/abs/2106.14229",
          "publishedOn": "2021-06-29T01:55:17.679Z",
          "wordCount": 543,
          "title": "Multi-task Over-the-Air Federated Learning: A Non-Orthogonal Transmission Approach. (arXiv:2106.14229v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matheny_M/0/1/0/all/0/1\">Michael Matheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M. Phillips</a>",
          "description": "Consider the geometric range space $(X, \\mathcal{H}_d)$ where $X \\subset\n\\mathbb{R}^d$ and $\\mathcal{H}_d$ is the set of ranges defined by\n$d$-dimensional halfspaces. In this setting we consider that $X$ is the\ndisjoint union of a red and blue set. For each halfspace $h \\in \\mathcal{H}_d$\ndefine a function $\\Phi(h)$ that measures the \"difference\" between the fraction\nof red and fraction of blue points which fall in the range $h$. In this context\nthe maximum discrepancy problem is to find the $h^* = \\arg \\max_{h \\in (X,\n\\mathcal{H}_d)} \\Phi(h)$. We aim to instead find an $\\hat{h}$ such that\n$\\Phi(h^*) - \\Phi(\\hat{h}) \\le \\varepsilon$. This is the central problem in\nlinear classification for machine learning, in spatial scan statistics for\nspatial anomaly detection, and shows up in many other areas. We provide a\nsolution for this problem in $O(|X| + (1/\\varepsilon^d) \\log^4\n(1/\\varepsilon))$ time, which improves polynomially over the previous best\nsolutions. For $d=2$ we show that this is nearly tight through conditional\nlower bounds. For different classes of $\\Phi$ we can either provide a\n$\\Omega(|X|^{3/2 - o(1)})$ time lower bound for the exact solution with a\nreduction to APSP, or an $\\Omega(|X| + 1/\\varepsilon^{2-o(1)})$ lower bound for\nthe approximate solution with a reduction to 3SUM.\n\nA key technical result is a $\\varepsilon$-approximate halfspace range\ncounting data structure of size $O(1/\\varepsilon^d)$ with $O(\\log\n(1/\\varepsilon))$ query time, which we can build in $O(|X| + (1/\\varepsilon^d)\n\\log^4 (1/\\varepsilon))$ time.",
          "link": "http://arxiv.org/abs/2106.13851",
          "publishedOn": "2021-06-29T01:55:17.672Z",
          "wordCount": 658,
          "title": "Approximate Maximum Halfspace Discrepancy. (arXiv:2106.13851v1 [cs.CG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>",
          "description": "Contextual Bandits find important use cases in various real-life scenarios\nsuch as online advertising, recommendation systems, healthcare, etc. However,\nmost of the algorithms use flat feature vectors to represent context whereas,\nin the real world, there is a varying number of objects and relations among\nthem to model in the context. For example, in a music recommendation system,\nthe user context contains what music they listen to, which artists create this\nmusic, the artist albums, etc. Adding richer relational context representations\nalso introduces a much larger context space making exploration-exploitation\nharder. To improve the efficiency of exploration-exploitation knowledge about\nthe context can be infused to guide the exploration-exploitation strategy.\nRelational context representations allow a natural way for humans to specify\nknowledge owing to their descriptive nature. We propose an adaptation of\nKnowledge Infused Policy Gradients to the Contextual Bandit setting and a novel\nKnowledge Infused Policy Gradients Upper Confidence Bound algorithm and perform\nan experimental analysis of a simulated music recommendation dataset and\nvarious real-life datasets where expert knowledge can drastically reduce the\ntotal regret and where it cannot.",
          "link": "http://arxiv.org/abs/2106.13895",
          "publishedOn": "2021-06-29T01:55:17.649Z",
          "wordCount": 629,
          "title": "Knowledge Infused Policy Gradients with Upper Confidence Bound for Relational Bandits. (arXiv:2106.13895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Szot_A/0/1/0/all/0/1\">Andrew Szot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clegg_A/0/1/0/all/0/1\">Alex Clegg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Undersander_E/0/1/0/all/0/1\">Eric Undersander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1\">Erik Wijmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yili Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_J/0/1/0/all/0/1\">John Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maestre_N/0/1/0/all/0/1\">Noah Maestre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1\">Mustafa Mukadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Chaplot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1\">Oleksandr Maksymets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrus_V/0/1/0/all/0/1\">Vladimir Vondrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharur_S/0/1/0/all/0/1\">Sameer Dharur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_F/0/1/0/all/0/1\">Franziska Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1\">Wojciech Galuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Angel Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1\">Manolis Savva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>",
          "description": "We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual\nrobots in interactive 3D environments and complex physics-enabled scenarios. We\nmake comprehensive contributions to all levels of the embodied AI stack - data,\nsimulation, and benchmark tasks. Specifically, we present: (i) ReplicaCAD: an\nartist-authored, annotated, reconfigurable 3D dataset of apartments (matching\nreal spaces) with articulated objects (e.g. cabinets and drawers that can\nopen/close); (ii) H2.0: a high-performance physics-enabled 3D simulator with\nspeeds exceeding 25,000 simulation steps per second (850x real-time) on an\n8-GPU node, representing 100x speed-ups over prior work; and, (iii) Home\nAssistant Benchmark (HAB): a suite of common tasks for assistive robots (tidy\nthe house, prepare groceries, set the table) that test a range of mobile\nmanipulation capabilities. These large-scale engineering contributions allow us\nto systematically compare deep reinforcement learning (RL) at scale and\nclassical sense-plan-act (SPA) pipelines in long-horizon structured tasks, with\nan emphasis on generalization to new objects, receptacles, and layouts. We find\nthat (1) flat RL policies struggle on HAB compared to hierarchical ones; (2) a\nhierarchy with independent skills suffers from 'hand-off problems', and (3) SPA\npipelines are more brittle than RL policies.",
          "link": "http://arxiv.org/abs/2106.14405",
          "publishedOn": "2021-06-29T01:55:17.630Z",
          "wordCount": 659,
          "title": "Habitat 2.0: Training Home Assistants to Rearrange their Habitat. (arXiv:2106.14405v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morad_S/0/1/0/all/0/1\">Steven D. Morad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_S/0/1/0/all/0/1\">Stephan Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prorok_A/0/1/0/all/0/1\">Amanda Prorok</a>",
          "description": "Solving partially-observable Markov decision processes (POMDPs) is critical\nwhen applying deep reinforcement learning (DRL) to real-world robotics\nproblems, where agents have an incomplete view of the world. We present graph\nconvolutional memory (GCM) for solving POMDPs using deep reinforcement\nlearning. Unlike recurrent neural networks (RNNs) or transformers, GCM embeds\ndomain-specific priors into the memory recall process via a knowledge graph. By\nencapsulating priors in the graph, GCM adapts to specific tasks but remains\napplicable to any DRL task. Using graph convolutions, GCM extracts hierarchical\ngraph features, analogous to image features in a convolutional neural network\n(CNN). We show GCM outperforms long short-term memory (LSTM), gated\ntransformers for reinforcement learning (GTrXL), and differentiable neural\ncomputers (DNCs) on control, long-term non-sequential recall, and 3D navigation\ntasks while using significantly fewer parameters.",
          "link": "http://arxiv.org/abs/2106.14117",
          "publishedOn": "2021-06-29T01:55:17.545Z",
          "wordCount": 562,
          "title": "Graph Convolutional Memory for Deep Reinforcement Learning. (arXiv:2106.14117v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li-Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Ruiyuan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaozhe Hu</a>",
          "description": "Polynomial functions have plenty of useful analytical properties, but they\nare rarely used as learning models because their function class is considered\nto be restricted. This work shows that when trained properly polynomial\nfunctions can be strong learning models. Particularly this work constructs\npolynomial feedforward neural networks using the product activation, a new\nactivation function constructed from multiplications. The new neural network is\na polynomial function and provides accurate control of its polynomial order. It\ncan be trained by standard training techniques such as batch normalization and\ndropout. This new feedforward network covers several previous polynomial models\nas special cases. Compared with common feedforward neural networks, the\npolynomial feedforward network has closed-form calculations of a few\ninteresting quantities, which are very useful in Bayesian learning. In a series\nof regression and classification tasks in the empirical study, the proposed\nmodel outperforms previous polynomial models.",
          "link": "http://arxiv.org/abs/2106.13834",
          "publishedOn": "2021-06-29T01:55:17.504Z",
          "wordCount": 590,
          "title": "Ladder Polynomial Neural Networks. (arXiv:2106.13834v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Besbes_O/0/1/0/all/0/1\">Omar Besbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_Y/0/1/0/all/0/1\">Yuri Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobel_I/0/1/0/all/0/1\">Ilan Lobel</a>",
          "description": "We study the problems of offline and online contextual optimization with\nfeedback information, where instead of observing the loss, we observe,\nafter-the-fact, the optimal action an oracle with full knowledge of the\nobjective function would have taken. We aim to minimize regret, which is\ndefined as the difference between our losses and the ones incurred by an\nall-knowing oracle. In the offline setting, the decision-maker has information\navailable from past periods and needs to make one decision, while in the online\nsetting, the decision-maker optimizes decisions dynamically over time based a\nnew set of feasible actions and contextual functions in each period. For the\noffline setting, we characterize the optimal minimax policy, establishing the\nperformance that can be achieved as a function of the underlying geometry of\nthe information induced by the data. In the online setting, we leverage this\ngeometric characterization to optimize the cumulative regret. We develop an\nalgorithm that yields the first regret bound for this problem that is\nlogarithmic in the time horizon.",
          "link": "http://arxiv.org/abs/2106.14015",
          "publishedOn": "2021-06-29T01:55:17.498Z",
          "wordCount": 603,
          "title": "Contextual Inverse Optimization: Offline and Online Learning. (arXiv:2106.14015v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finn_T/0/1/0/all/0/1\">Tobias Sebastian Finn</a>",
          "description": "Ensemble data from Earth system models has to be calibrated and\npost-processed. I propose a novel member-by-member post-processing approach\nwith neural networks. I bridge ideas from ensemble data assimilation with\nself-attention, resulting into the self-attentive ensemble transformer. Here,\ninteractions between ensemble members are represented as additive and dynamic\nself-attentive part. As proof-of-concept, global ECMWF ensemble forecasts are\nregressed to 2-metre-temperature fields from the ERA5 reanalysis. I demonstrate\nthat the ensemble transformer can calibrate the ensemble spread and extract\nadditional information from the ensemble. Furthermore, the ensemble transformer\ndirectly outputs multivariate and spatially-coherent ensemble members.\nTherefore, self-attention and the transformer technique can be a missing piece\nfor a member-by-member post-processing of ensemble data with neural networks.",
          "link": "http://arxiv.org/abs/2106.13924",
          "publishedOn": "2021-06-29T01:55:17.378Z",
          "wordCount": 581,
          "title": "Self-Attentive Ensemble Transformer: Representing Ensemble Interactions in Neural Networks for Earth System Models. (arXiv:2106.13924v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chang-E Ren</a>",
          "description": "Broad learning system (BLS) has been proposed for a few years. It\ndemonstrates an effective learning capability for many classification and\nregression problems. However, BLS and its improved versions are mainly used to\ndeal with unsupervised, supervised and semi-supervised learning problems in a\nsingle domain. As far as we know, a little attention is paid to the\ncross-domain learning ability of BLS. Therefore, we introduce BLS into the\nfield of transfer learning and propose a novel algorithm called domain\nadaptation broad learning system based on locally linear embedding (DABLS-LLE).\nThe proposed algorithm can learn a robust classification model by using a small\npart of labeled data from the target domain and all labeled data from the\nsource domain. The proposed algorithm inherits the computational efficiency and\nlearning capability of BLS. Experiments on benchmark dataset\n(Office-Caltech-10) verify the effectiveness of our approach. The results show\nthat our approach can get better classification accuracy with less running time\nthan many existing transfer learning approaches. It shows that our approach can\nbring a new superiority for BLS.",
          "link": "http://arxiv.org/abs/2106.14367",
          "publishedOn": "2021-06-29T01:55:17.311Z",
          "wordCount": 603,
          "title": "Domain Adaptation Broad Learning System Based on Locally Linear Embedding. (arXiv:2106.14367v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junwen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yeye He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Surajit Chaudhuri</a>",
          "description": "Recent work has made significant progress in helping users to automate single\ndata preparation steps, such as string-transformations and table-manipulation\noperators (e.g., Join, GroupBy, Pivot, etc.). We in this work propose to\nautomate multiple such steps end-to-end, by synthesizing complex data pipelines\nwith both string transformations and table-manipulation operators. We propose a\nnovel \"by-target\" paradigm that allows users to easily specify the desired\npipeline, which is a significant departure from the traditional by-example\nparadigm. Using by-target, users would provide input tables (e.g., csv or json\nfiles), and point us to a \"target table\" (e.g., an existing database table or\nBI dashboard) to demonstrate how the output from the desired pipeline would\nschematically \"look like\". While the problem is seemingly underspecified, our\nunique insight is that implicit table constraints such as FDs and keys can be\nexploited to significantly constrain the space to make the problem tractable.\nWe develop an Auto-Pipeline system that learns to synthesize pipelines using\nreinforcement learning and search. Experiments on large numbers of real\npipelines crawled from GitHub suggest that Auto-Pipeline can successfully\nsynthesize 60-70% of these complex pipelines (up to 10 steps) in 10-20 seconds\non average.",
          "link": "http://arxiv.org/abs/2106.13861",
          "publishedOn": "2021-06-29T01:55:17.305Z",
          "wordCount": 624,
          "title": "AutoPipeline: Synthesize Data Pipelines By-Target Using Reinforcement Learning and Search. (arXiv:2106.13861v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jothimurugan_K/0/1/0/all/0/1\">Kishor Jothimurugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Suguman Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alur_R/0/1/0/all/0/1\">Rajeev Alur</a>",
          "description": "We study the problem of learning control policies for complex tasks given by\nlogical specifications. Recent approaches automatically generate a reward\nfunction from a given specification and use a suitable reinforcement learning\nalgorithm to learn a policy that maximizes the expected reward. These\napproaches, however, scale poorly to complex tasks that require high-level\nplanning. In this work, we develop a compositional learning approach, called\nDiRL, that interleaves high-level planning and reinforcement learning. First,\nDiRL encodes the specification as an abstract graph; intuitively, vertices and\nedges of the graph correspond to regions of the state space and simpler\nsub-tasks, respectively. Our approach then incorporates reinforcement learning\nto learn neural network policies for each edge (sub-task) within a\nDijkstra-style planning algorithm to compute a high-level plan in the graph. An\nevaluation of the proposed approach on a set of challenging control benchmarks\nwith continuous state and action spaces demonstrates that it outperforms\nstate-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2106.13906",
          "publishedOn": "2021-06-29T01:55:16.958Z",
          "wordCount": 582,
          "title": "Compositional Reinforcement Learning from Logical Specifications. (arXiv:2106.13906v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13959",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterjee_A/0/1/0/all/0/1\">Avishek Chatterjee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mazumder_S/0/1/0/all/0/1\">Satyaki Mazumder</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Das_K/0/1/0/all/0/1\">Koel Das</a>",
          "description": "In recent times, functional data analysis (FDA) has been successfully applied\nin the field of high dimensional data classification. In this paper, we present\na novel classification framework using functional data and classwise Principal\nComponent Analysis (PCA). Our proposed method can be used in high dimensional\ntime series data which typically suffers from small sample size problem. Our\nmethod extracts a piece wise linear functional feature space and is\nparticularly suitable for hard classification problems.The proposed framework\nconverts time series data into functional data and uses classwise functional\nPCA for feature extraction followed by classification using a Bayesian linear\nclassifier. We demonstrate the efficacy of our proposed method by applying it\nto both synthetic data sets and real time series data from diverse fields\nincluding but not limited to neuroscience, food science, medical sciences and\nchemometrics.",
          "link": "http://arxiv.org/abs/2106.13959",
          "publishedOn": "2021-06-29T01:55:16.935Z",
          "wordCount": 577,
          "title": "Functional Classwise Principal Component Analysis: A Novel Classification Framework. (arXiv:2106.13959v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1\">Ivan Y. Tyukin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higham_D/0/1/0/all/0/1\">Desmond J. Higham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woldegeorgis_E/0/1/0/all/0/1\">Eliyas Woldegeorgis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1\">Alexander N. Gorban</a>",
          "description": "We develop and study new adversarial perturbations that enable an attacker to\ngain control over decisions in generic Artificial Intelligence (AI) systems\nincluding deep learning neural networks. In contrast to adversarial data\nmodification, the attack mechanism we consider here involves alterations to the\nAI system itself. Such a stealth attack could be conducted by a mischievous,\ncorrupt or disgruntled member of a software development team. It could also be\nmade by those wishing to exploit a \"democratization of AI\" agenda, where\nnetwork architectures and trained parameter sets are shared publicly. Building\non work by [Tyukin et al., International Joint Conference on Neural Networks,\n2020], we develop a range of new implementable attack strategies with\naccompanying analysis, showing that with high probability a stealth attack can\nbe made transparent, in the sense that system performance is unchanged on a\nfixed validation set which is unknown to the attacker, while evoking any\ndesired output on a trigger input of interest. The attacker only needs to have\nestimates of the size of the validation set and the spread of the AI's relevant\nlatent space. In the case of deep learning neural networks, we show that a one\nneuron attack is possible - a modification to the weights and bias associated\nwith a single neuron - revealing a vulnerability arising from\nover-parameterization. We illustrate these concepts in a realistic setting.\nGuided by the theory and computational results, we also propose strategies to\nguard against stealth attacks.",
          "link": "http://arxiv.org/abs/2106.13997",
          "publishedOn": "2021-06-29T01:55:16.889Z",
          "wordCount": 688,
          "title": "The Feasibility and Inevitability of Stealth Attacks. (arXiv:2106.13997v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kou_W/0/1/0/all/0/1\">Wenjun Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlson_D/0/1/0/all/0/1\">Dustin A. Carlson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumann_A/0/1/0/all/0/1\">Alexandra J. Baumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnan_E/0/1/0/all/0/1\">Erica N. Donnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schauer_J/0/1/0/all/0/1\">Jacob M. Schauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemadi_M/0/1/0/all/0/1\">Mozziyar Etemadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandolfino_J/0/1/0/all/0/1\">John E. Pandolfino</a>",
          "description": "High-resolution manometry (HRM) is the primary procedure used to diagnose\nesophageal motility disorders. Its interpretation and classification includes\nan initial evaluation of swallow-level outcomes and then derivation of a\nstudy-level diagnosis based on Chicago Classification (CC), using a tree-like\nalgorithm. This diagnostic approach on motility disordered using HRM was\nmirrored using a multi-stage modeling framework developed using a combination\nof various machine learning approaches. Specifically, the framework includes\ndeep-learning models at the swallow-level stage and feature-based machine\nlearning models at the study-level stage. In the swallow-level stage, three\nmodels based on convolutional neural networks (CNNs) were developed to predict\nswallow type, swallow pressurization, and integrated relaxation pressure (IRP).\nAt the study-level stage, model selection from families of the\nexpert-knowledge-based rule models, xgboost models and artificial neural\nnetwork(ANN) models were conducted, with the latter two model designed and\naugmented with motivation from the export knowledge. A simple model-agnostic\nstrategy of model balancing motivated by Bayesian principles was utilized,\nwhich gave rise to model averaging weighted by precision scores. The averaged\n(blended) models and individual models were compared and evaluated, of which\nthe best performance on test dataset is 0.81 in top-1 prediction, 0.92 in top-2\npredictions. This is the first artificial-intelligence-style model to\nautomatically predict CC diagnosis of HRM study from raw multi-swallow data.\nMoreover, the proposed modeling framework could be easily extended to\nmulti-modal tasks, such as diagnosis of esophageal patients based on clinical\ndata from both HRM and functional luminal imaging probe panometry (FLIP).",
          "link": "http://arxiv.org/abs/2106.13869",
          "publishedOn": "2021-06-29T01:55:16.802Z",
          "wordCount": 693,
          "title": "A multi-stage machine learning model on diagnosis of esophageal manometry. (arXiv:2106.13869v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13867",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1\">Jiameng Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "We propose POLAR, a \\textbf{pol}ynomial \\textbf{ar}ithmetic framework that\nleverages polynomial overapproximations with interval remainders for\nbounded-time reachability analysis of neural network-controlled systems\n(NNCSs). Compared with existing arithmetic approaches that use standard Taylor\nmodels, our framework uses a novel approach to iteratively overapproximate the\nneuron output ranges layer-by-layer with a combination of Bernstein polynomial\ninterpolation for continuous activation functions and Taylor model arithmetic\nfor the other operations. This approach can overcome the main drawback in the\nstandard Taylor model arithmetic, i.e. its inability to handle functions that\ncannot be well approximated by Taylor polynomials, and significantly improve\nthe accuracy and efficiency of reachable states computation for NNCSs. To\nfurther tighten the overapproximation, our method keeps the Taylor model\nremainders symbolic under the linear mappings when estimating the output range\nof a neural network. We show that POLAR can be seamlessly integrated with\nexisting Taylor model flowpipe construction techniques, and demonstrate that\nPOLAR significantly outperforms the current state-of-the-art techniques on a\nsuite of benchmarks.",
          "link": "http://arxiv.org/abs/2106.13867",
          "publishedOn": "2021-06-29T01:55:16.795Z",
          "wordCount": 608,
          "title": "POLAR: A Polynomial Arithmetic Framework for Verifying Neural-Network Controlled Systems. (arXiv:2106.13867v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14384",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kang_Y/0/1/0/all/0/1\">Yihuang Kang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chiu_Y/0/1/0/all/0/1\">Yi-Wen Chiu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lin_M/0/1/0/all/0/1\">Ming-Yen Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Su_F/0/1/0/all/0/1\">Fang-yi Su</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Huang_S/0/1/0/all/0/1\">Sheng-Tai Huang</a>",
          "description": "Machine Learning (ML) and its applications have been transforming our lives\nbut it is also creating issues related to the development of fair, accountable,\ntransparent, and ethical Artificial Intelligence. As the ML models are not\nfully comprehensible yet, it is obvious that we still need humans to be part of\nalgorithmic decision-making processes. In this paper, we consider a ML\nframework that may accelerate model learning and improve its interpretability\nby incorporating human experts into the model learning loop. We propose a novel\nhuman-in-the-loop ML framework aimed at dealing with learning problems that the\ncost of data annotation is high and the lack of appropriate data to model the\nassociation between the target tasks and the input features. With an\napplication to precision dosing, our experimental results show that the\napproach can learn interpretable rules from data and may potentially lower\nexperts' workload by replacing data annotation with rule representation\nediting. The approach may also help remove algorithmic bias by introducing\nexperts' feedback into the iterative model learning process.",
          "link": "http://arxiv.org/abs/2106.14384",
          "publishedOn": "2021-06-29T01:55:16.783Z",
          "wordCount": 605,
          "title": "Towards Model-informed Precision Dosing with Expert-in-the-loop Machine Learning. (arXiv:2106.14384v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yi Xiang Marcus Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_P/0/1/0/all/0/1\">Penny Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1\">Ngai-Man Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>",
          "description": "Few-shot classifiers excel under limited training samples, making them useful\nin applications with sparsely user-provided labels. Their unique relative\nprediction setup offers opportunities for novel attacks, such as targeting\nsupport sets required to categorise unseen test samples, which are not\navailable in other machine learning setups. In this work, we propose a\ndetection strategy to identify adversarial support sets, aimed at destroying\nthe understanding of a few-shot classifier for a certain class. We achieve this\nby introducing the concept of self-similarity of a support set and by employing\nfiltering of supports. Our method is attack-agnostic, and we are the first to\nexplore adversarial detection for support sets of few-shot classifiers to the\nbest of our knowledge. Our evaluation of the miniImagenet (MI) and CUB datasets\nexhibits good attack detection performance despite conceptual simplicity,\nshowing high AUROC scores. We show that self-similarity and filtering for\nadversarial detection can be paired with other filtering functions,\nconstituting a generalisable concept.",
          "link": "http://arxiv.org/abs/2012.06330",
          "publishedOn": "2021-06-29T01:55:16.777Z",
          "wordCount": 651,
          "title": "Detection of Adversarial Supports in Few-shot Classifiers Using Self-Similarity and Filtering. (arXiv:2012.06330v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Modhe_N/0/1/0/all/0/1\">Nirbhay Modhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_H/0/1/0/all/0/1\">Harish Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>",
          "description": "Model-based Reinforcement Learning (MBRL) algorithms have been traditionally\ndesigned with the goal of learning accurate dynamics of the environment. This\nintroduces a mismatch between the objectives of model-learning and the overall\nlearning problem of finding an optimal policy. Value-aware model learning, an\nalternative model-learning paradigm to maximum likelihood, proposes to inform\nmodel-learning through the value function of the learnt policy. While this\nparadigm is theoretically sound, it does not scale beyond toy settings. In this\nwork, we propose a novel value-aware objective that is an upper bound on the\nabsolute performance difference of a policy across two models. Further, we\npropose a general purpose algorithm that modifies the standard MBRL pipeline --\nenabling learning with value aware objectives. Our proposed objective, in\nconjunction with this algorithm, is the first successful instantiation of\nvalue-aware MBRL on challenging continuous control environments, outperforming\nprevious value-aware objectives and with competitive performance w.r.t.\nMLE-based MBRL approaches.",
          "link": "http://arxiv.org/abs/2106.14080",
          "publishedOn": "2021-06-29T01:55:16.755Z",
          "wordCount": 585,
          "title": "Model-Advantage Optimization for Model-Based Reinforcement Learning. (arXiv:2106.14080v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noormandipour_M/0/1/0/all/0/1\">Mohammadreza Noormandipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanchen Wang</a>",
          "description": "In this work, we propose a parameterised quantum circuit learning approach to\npoint set matching problem. In contrast to previous annealing-based methods, we\npropose a quantum circuit-based framework whose parameters are optimised via\ndescending the gradients w.r.t a kernel-based loss function. We formulate the\nshape matching problem into a distribution learning task; that is, to learn the\ndistribution of the optimal transformation parameters. We show that this\nframework is able to find multiple optimal solutions for symmetric shapes and\nis more accurate, scalable and robust than the previous annealing-based method.\nCode, data and pre-trained weights are available at the project page:\n\\href{https://hansen7.github.io/qKC}{https://hansen7.github.io/qKC}",
          "link": "http://arxiv.org/abs/2102.06697",
          "publishedOn": "2021-06-29T01:55:16.741Z",
          "wordCount": 579,
          "title": "Matching Point Sets with Quantum Circuit Learning. (arXiv:2102.06697v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13814",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Campos_E/0/1/0/all/0/1\">E. Campos</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Rabinovich_D/0/1/0/all/0/1\">D. Rabinovich</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Akshay_V/0/1/0/all/0/1\">V. Akshay</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Biamonte_J/0/1/0/all/0/1\">J. Biamonte</a>",
          "description": "Quantum Approximate Optimisation (QAOA) is the most studied gate based\nvariational quantum algorithm today. We train QAOA one layer at a time to\nmaximize overlap with an $n$ qubit target state. Doing so we discovered that\nsuch training always saturates -- called \\textit{training saturation} -- at\nsome depth $p^*$, meaning that past a certain depth, overlap can not be\nimproved by adding subsequent layers. We formulate necessary conditions for\nsaturation. Numerically, we find layerwise QAOA reaches its maximum overlap at\ndepth $p^*=n$. The addition of coherent dephasing errors to training removes\nsaturation, recovering robustness to layerwise training. This study sheds new\nlight on the performance limitations and prospects of QAOA.",
          "link": "http://arxiv.org/abs/2106.13814",
          "publishedOn": "2021-06-29T01:55:16.734Z",
          "wordCount": 554,
          "title": "Training Saturation in Layerwise Quantum Approximate Optimisation. (arXiv:2106.13814v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fanuel_M/0/1/0/all/0/1\">Micha&#xeb;l Fanuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bardenet_R/0/1/0/all/0/1\">R&#xe9;mi Bardenet</a>",
          "description": "Determinantal Point Process (DPPs) are statistical models for repulsive point\npatterns. Both sampling and inference are tractable for DPPs, a rare feature\namong models with negative dependence that explains their popularity in machine\nlearning and spatial statistics. Parametric and nonparametric inference methods\nhave been proposed in the finite case, i.e. when the point patterns live in a\nfinite ground set. In the continuous case, only parametric methods have been\ninvestigated, while nonparametric maximum likelihood for DPPs -- an\noptimization problem over trace-class operators -- has remained an open\nquestion. In this paper, we show that a restricted version of this maximum\nlikelihood (MLE) problem falls within the scope of a recent representer theorem\nfor nonnegative functions in an RKHS. This leads to a finite-dimensional\nproblem, with strong statistical ties to the original MLE. Moreover, we\npropose, analyze, and demonstrate a fixed point algorithm to solve this\nfinite-dimensional problem. Finally, we also provide a controlled estimate of\nthe correlation kernel of the DPP, thus providing more interpretability.",
          "link": "http://arxiv.org/abs/2106.14210",
          "publishedOn": "2021-06-29T01:55:16.725Z",
          "wordCount": 600,
          "title": "Nonparametric estimation of continuous DPPs with kernel methods. (arXiv:2106.14210v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fubing_M/0/1/0/all/0/1\">Mao Fubing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiwei_W/0/1/0/all/0/1\">Weng Weiwei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1\">Mahardhika Pratama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_E/0/1/0/all/0/1\">Edward Yapp Kien Yee</a>",
          "description": "Learning from streaming tasks leads a model to catastrophically erase unique\nexperiences it absorbs from previous episodes. While regularization techniques\nsuch as LWF, SI, EWC have proven themselves as an effective avenue to overcome\nthis issue by constraining important parameters of old tasks from changing when\naccepting new concepts, these approaches do not exploit common information of\neach task which can be shared to existing neurons. As a result, they do not\nscale well to large-scale problems since the parameter importance variables\nquickly explode. An Inter-Task Synaptic Mapping (ISYANA) is proposed here to\nunderpin knowledge retention for continual learning. ISYANA combines\ntask-to-neuron relationship as well as concept-to-concept relationship such\nthat it prevents a neuron to embrace distinct concepts while merely accepting\nrelevant concept. Numerical study in the benchmark continual learning problems\nhas been carried out followed by comparison against prominent continual\nlearning algorithms. ISYANA exhibits competitive performance compared to state\nof the arts. Codes of ISYANA is made available in\n\\url{https://github.com/ContinualAL/ISYANAKBS}.",
          "link": "http://arxiv.org/abs/2106.13954",
          "publishedOn": "2021-06-29T01:55:16.709Z",
          "wordCount": 611,
          "title": "Continual Learning via Inter-Task Synaptic Mapping. (arXiv:2106.13954v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dogga_P/0/1/0/all/0/1\">Pradeep Dogga</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Sivaraman_A/0/1/0/all/0/1\">Anirudh Sivaraman</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Saini_S/0/1/0/all/0/1\">Shiv Kumar Saini</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Varghese_G/0/1/0/all/0/1\">George Varghese</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Netravali_R/0/1/0/all/0/1\">Ravi Netravali</a> (2) ((1) UCLA, (2) Princeton University, (3) NYU, (4) Adobe Research, India)",
          "description": "A major difficulty in debugging distributed systems lies in manually\ndetermining which of the many available debugging tools to use and how to query\nits logs. Our own study of a production debugging workflow confirms the\nmagnitude of this burden. This paper explores whether a machine-learning model\ncan assist developers in distributed systems debugging. We present Revelio, a\ndebugging assistant which takes user reports and system logs as input, and\noutputs debugging queries that developers can use to find a bug's root cause.\nThe key challenges lie in (1) combining inputs of different types (e.g.,\nnatural language reports and quantitative logs) and (2) generalizing to unseen\nfaults. Revelio addresses these by employing deep neural networks to uniformly\nembed diverse input sources and potential queries into a high-dimensional\nvector space. In addition, it exploits observations from production systems to\nfactorize query generation into two computationally and statistically simpler\nlearning tasks. To evaluate Revelio, we built a testbed with multiple\ndistributed applications and debugging tools. By injecting faults and training\non logs and reports from 800 Mechanical Turkers, we show that Revelio includes\nthe most helpful query in its predicted list of top-3 relevant queries 96% of\nthe time. Our developer study confirms the utility of Revelio.",
          "link": "http://arxiv.org/abs/2106.14347",
          "publishedOn": "2021-06-29T01:55:16.703Z",
          "wordCount": 665,
          "title": "Revelio: ML-Generated Debugging Queries for Distributed Systems. (arXiv:2106.14347v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1\">Stephanie Tsuei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1\">Aditya Golatkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a method to estimate the uncertainty of the outcome of an image\nclassifier on a given input datum. Deep neural networks commonly used for image\nclassification are deterministic maps from an input image to an output class.\nAs such, their outcome on a given datum involves no uncertainty, so we must\nspecify what variability we are referring to when defining, measuring and\ninterpreting \"confidence.\" To this end, we introduce the Wellington Posterior,\nwhich is the distribution of outcomes that would have been obtained in response\nto data that could have been generated by the same scene that produced the\ngiven image. Since there are infinitely many scenes that could have generated\nthe given image, the Wellington Posterior requires induction from scenes other\nthan the one portrayed. We explore alternate methods using data augmentation,\nensembling, and model linearization. Additional alternatives include generative\nadversarial networks, conditional prior networks, and supervised single-view\nreconstruction. We test these alternatives against the empirical posterior\nobtained by inferring the class of temporally adjacent frames in a video. These\ndevelopments are only a small step towards assessing the reliability of deep\nnetwork classifiers in a manner that is compatible with safety-critical\napplications.",
          "link": "http://arxiv.org/abs/2106.13870",
          "publishedOn": "2021-06-29T01:55:16.691Z",
          "wordCount": 649,
          "title": "Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers. (arXiv:2106.13870v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14320",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hajimohammadi_Z/0/1/0/all/0/1\">Zeinab Hajimohammadi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Parand_K/0/1/0/all/0/1\">Kourosh Parand</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>",
          "description": "Various phenomena in biology, physics, and engineering are modeled by\ndifferential equations. These differential equations including partial\ndifferential equations and ordinary differential equations can be converted and\nrepresented as integral equations. In particular, Volterra Fredholm Hammerstein\nintegral equations are the main type of these integral equations and\nresearchers are interested in investigating and solving these equations. In\nthis paper, we propose Legendre Deep Neural Network (LDNN) for solving\nnonlinear Volterra Fredholm Hammerstein integral equations (VFHIEs). LDNN\nutilizes Legendre orthogonal polynomials as activation functions of the Deep\nstructure. We present how LDNN can be used to solve nonlinear VFHIEs. We show\nusing the Gaussian quadrature collocation method in combination with LDNN\nresults in a novel numerical solution for nonlinear VFHIEs. Several examples\nare given to verify the performance and accuracy of LDNN.",
          "link": "http://arxiv.org/abs/2106.14320",
          "publishedOn": "2021-06-29T01:55:16.685Z",
          "wordCount": 582,
          "title": "Legendre Deep Neural Network (LDNN) and its application for approximation of nonlinear Volterra Fredholm Hammerstein integral equations. (arXiv:2106.14320v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2012.04830",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1\">Jiansheng Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Z/0/1/0/all/0/1\">Zunjie Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1\">Risa Higashita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>",
          "description": "Cataract is one of the leading causes of reversible visual impairment and\nblindness globally. Over the years, researchers have achieved significant\nprogress in developing state-of-the-art artificial intelligence techniques for\nautomatic cataract classification and grading, helping clinicians prevent and\ntreat cataract in time. This paper provides a comprehensive survey of recent\nadvances in machine learning for cataract classification and grading based on\nophthalmic images. We summarize existing literature from two research\ndirections: conventional machine learning techniques and deep learning\ntechniques. This paper also provides insights into existing works of both\nmerits and limitations. In addition, we discuss several challenges of automatic\ncataract classification and grading based on machine learning techniques and\npresent possible solutions to these challenges for future research.",
          "link": "http://arxiv.org/abs/2012.04830",
          "publishedOn": "2021-06-29T01:55:16.671Z",
          "wordCount": 612,
          "title": "Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1902.03717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honnorat_N/0/1/0/all/0/1\">Nicolas Honnorat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "Variation Autoencoder (VAE) has become a powerful tool in modeling the\nnon-linear generative process of data from a low-dimensional latent space.\nRecently, several studies have proposed to use VAE for unsupervised clustering\nby using mixture models to capture the multi-modal structure of latent\nrepresentations. This strategy, however, is ineffective when there are outlier\ndata samples whose latent representations are meaningless, yet contaminating\nthe estimation of key major clusters in the latent space. This exact problem\narises in the context of resting-state fMRI (rs-fMRI) analysis, where\nclustering major functional connectivity patterns is often hindered by heavy\nnoise of rs-fMRI and many minor clusters (rare connectivity patterns) of no\ninterest to analysis. In this paper we propose a novel generative process, in\nwhich we use a Gaussian-mixture to model a few major clusters in the data, and\nuse a non-informative uniform distribution to capture the remaining data. We\nembed this truncated Gaussian-Mixture model in a Variational AutoEncoder\nframework to obtain a general joint clustering and outlier detection approach,\ncalled tGM-VAE. We demonstrated the applicability of tGM-VAE on the MNIST\ndataset and further validated it in the context of rs-fMRI connectivity\nanalysis.",
          "link": "http://arxiv.org/abs/1902.03717",
          "publishedOn": "2021-06-29T01:55:16.665Z",
          "wordCount": 656,
          "title": "Truncated Gaussian-Mixture Variational AutoEncoder. (arXiv:1902.03717v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06073",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1\">Johannes O. Royset</a>",
          "description": "A basic requirement for a mathematical model is often that its solution\n(output) shouldn't change much if the model's parameters (input) are perturbed.\nThis is important because the exact values of parameters may not be known and\none would like to avoid being mislead by an output obtained using incorrect\nvalues. Thus, it's rarely enough to address an application by formulating a\nmodel, solving the resulting optimization problem and presenting the solution\nas the answer. One would need to confirm that the model is suitable, i.e.,\n\"good,\" and this can, at least in part, be achieved by considering a family of\noptimization problems constructed by perturbing parameters of concern. The\nresulting sensitivity analysis uncovers troubling situations with unstable\nsolutions, which we referred to as \"bad\" models, and indicates better model\nformulations. Embedding an actual problem of interest within a family of\nproblems is also a primary path to optimality conditions as well as\ncomputationally attractive, alternative problems, which under ideal\ncircumstances, and when properly tuned, may even furnish the minimum value of\nthe actual problem. The tuning of these alternative problems turns out to be\nintimately tied to finding multipliers in optimality conditions and thus\nemerges as a main component of several optimization algorithms. In fact, the\ntuning amounts to solving certain dual optimization problems. In this tutorial,\nwe'll discuss the opportunities and insights afforded by this broad\nperspective.",
          "link": "http://arxiv.org/abs/2105.06073",
          "publishedOn": "2021-06-29T01:55:16.660Z",
          "wordCount": 678,
          "title": "Good and Bad Optimization Models: Insights from Rockafellians. (arXiv:2105.06073v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00351",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chung_M/0/1/0/all/0/1\">Moo K. Chung</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ombao_H/0/1/0/all/0/1\">Hernando Ombao</a>",
          "description": "Topological data analysis, including persistent homology, has undergone\nsignificant development in recent years. However, one outstanding challenge is\nto build a coherent statistical inference procedure on persistent diagrams. The\npaired dependent data structure, which are the births and deaths in persistent\ndiagrams, adds complexity to statistical inference. In this paper, we present a\nnew lattice path representation for persistent diagrams. A new exact\nstatistical inference procedure is developed for lattice paths via\ncombinatorial enumerations. The proposed lattice path method is applied to\nstudy the topological characterization of the protein structures of the\nCOVID-19 virus. We demonstrate that there are topological changes during the\nconformational change of spike proteins, a necessary step in infecting host\ncells.",
          "link": "http://arxiv.org/abs/2105.00351",
          "publishedOn": "2021-06-29T01:55:16.652Z",
          "wordCount": 638,
          "title": "Lattice Paths for Persistent Diagrams with Application to COVID-19 Virus Spike Proteins. (arXiv:2105.00351v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luca_M/0/1/0/all/0/1\">Massimiliano Luca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlacchi_G/0/1/0/all/0/1\">Gianni Barlacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1\">Bruno Lepri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappalardo_L/0/1/0/all/0/1\">Luca Pappalardo</a>",
          "description": "The study of human mobility is crucial due to its impact on several aspects\nof our society, such as disease spreading, urban planning, well-being,\npollution, and more. The proliferation of digital mobility data, such as phone\nrecords, GPS traces, and social media posts, combined with the predictive power\nof artificial intelligence, triggered the application of deep learning to human\nmobility. Existing surveys focus on single tasks, data sources, mechanistic or\ntraditional machine learning approaches, while a comprehensive description of\ndeep learning solutions is missing. This survey provides a taxonomy of mobility\ntasks, a discussion on the challenges related to each task and how deep\nlearning may overcome the limitations of traditional models, a description of\nthe most relevant solutions to the mobility tasks described above and the\nrelevant challenges for the future. Our survey is a guide to the leading deep\nlearning solutions to next-location prediction, crowd flow prediction,\ntrajectory generation, and flow generation. At the same time, it helps deep\nlearning scientists and practitioners understand the fundamental concepts and\nthe open challenges of the study of human mobility.",
          "link": "http://arxiv.org/abs/2012.02825",
          "publishedOn": "2021-06-29T01:55:16.623Z",
          "wordCount": 654,
          "title": "A Survey on Deep Learning for Human Mobility. (arXiv:2012.02825v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guangmeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yi Zhao</a>",
          "description": "In multi-party collaborative learning, the parameter server sends a global\nmodel to each data holder for local training and then aggregates committed\nmodels globally to achieve privacy protection. However, both the dragger issue\nof synchronous collaborative learning and the staleness issue of asynchronous\ncollaborative learning make collaborative learning inefficient in real-world\nheterogeneous environments. We propose a novel and efficient collaborative\nlearning framework named AdaptCL, which generates an adaptive sub-model\ndynamically from the global base model for each data holder, without any prior\ninformation about worker capability. All workers (data holders) achieve\napproximately identical update time as the fastest worker by equipping them\nwith capability-adapted pruned models. Thus the training process can be\ndramatically accelerated. Besides, we tailor the efficient pruned rate learning\nalgorithm and pruning approach for AdaptCL. Meanwhile, AdaptCL provides a\nmechanism for handling the trade-off between accuracy and time overhead and can\nbe combined with other techniques to accelerate training further. Empirical\nresults show that AdaptCL introduces little computing and communication\noverhead. AdaptCL achieves time savings of more than 41\\% on average and\nimproves accuracy in a low heterogeneous environment. In a highly heterogeneous\nenvironment, AdaptCL achieves a training speedup of 6.2x with a slight loss of\naccuracy.",
          "link": "http://arxiv.org/abs/2106.14126",
          "publishedOn": "2021-06-29T01:55:16.616Z",
          "wordCount": 641,
          "title": "AdaptCL: Efficient Collaborative Learning with Dynamic and Adaptive Pruning. (arXiv:2106.14126v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moharrer_A/0/1/0/all/0/1\">Armin Moharrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_K/0/1/0/all/0/1\">Khashayar Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_E/0/1/0/all/0/1\">Edmund Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1\">Stratis Ioannidis</a>",
          "description": "The mean squared error loss is widely used in many applications, including\nauto-encoders, multi-target regression, and matrix factorization, to name a\nfew. Despite computational advantages due to its differentiability, it is not\nrobust to outliers. In contrast, l_p norms are known to be robust, but cannot\nbe optimized via, e.g., stochastic gradient descent, as they are\nnon-differentiable. We propose an algorithm inspired by so-called model-based\noptimization (MBO) [35, 36], which replaces a non-convex objective with a\nconvex model function and alternates between optimizing the model function and\nupdating the solution. We apply this to robust regression, proposing SADM, a\nstochastic variant of the Online Alternating Direction Method of Multipliers\n(OADM) [50] to solve the inner optimization in MBO. We show that SADM converges\nwith the rate O(log T/T). Finally, we demonstrate experimentally (a) the\nrobustness of l_p norms to outliers and (b) the efficiency of our proposed\nmodel-based algorithms in comparison with gradient methods on autoencoders and\nmulti-target regression.",
          "link": "http://arxiv.org/abs/2106.10759",
          "publishedOn": "2021-06-29T01:55:16.608Z",
          "wordCount": 617,
          "title": "Robust Regression via Model Based Methods. (arXiv:2106.10759v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14473",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ryck_T/0/1/0/all/0/1\">Tim De Ryck</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1\">Siddhartha Mishra</a>",
          "description": "Physics informed neural networks approximate solutions of PDEs by minimizing\npointwise residuals. We derive rigorous bounds on the error, incurred by PINNs\nin approximating the solutions of a large class of linear parabolic PDEs,\nnamely Kolmogorov equations that include the heat equation and Black-Scholes\nequation of option pricing, as examples. We construct neural networks, whose\nPINN residual (generalization error) can be made as small as desired. We also\nprove that the total $L^2$-error can be bounded by the generalization error,\nwhich in turn is bounded in terms of the training error, provided that a\nsufficient number of randomly chosen training (collocation) points is used.\nMoreover, we prove that the size of the PINNs and the number of training\nsamples only grow polynomially with the underlying dimension, enabling PINNs to\novercome the curse of dimensionality in this context. These results enable us\nto provide a comprehensive error analysis for PINNs in approximating Kolmogorov\nPDEs.",
          "link": "http://arxiv.org/abs/2106.14473",
          "publishedOn": "2021-06-29T01:55:16.603Z",
          "wordCount": 600,
          "title": "Error analysis for physics informed neural networks (PINNs) approximating Kolmogorov PDEs. (arXiv:2106.14473v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vicuna_M/0/1/0/all/0/1\">Marc Vicuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khannouz_M/0/1/0/all/0/1\">Martin Khannouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiar_G/0/1/0/all/0/1\">Gregory Kiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatelain_Y/0/1/0/all/0/1\">Yohan Chatelain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glatard_T/0/1/0/all/0/1\">Tristan Glatard</a>",
          "description": "Mondrian Forests are a powerful data stream classification method, but their\nlarge memory footprint makes them ill-suited for low-resource platforms such as\nconnected objects. We explored using reduced-precision floating-point\nrepresentations to lower memory consumption and evaluated its effect on\nclassification performance. We applied the Mondrian Forest implementation\nprovided by OrpailleCC, a C++ collection of data stream algorithms, to two\ncanonical datasets in human activity recognition: Recofit and Banos \\emph{et\nal}. Results show that the precision of floating-point values used by tree\nnodes can be reduced from 64 bits to 8 bits with no significant difference in\nF1 score. In some cases, reduced precision was shown to improve classification\nperformance, presumably due to its regularization effect. We conclude that\nnumerical precision is a relevant hyperparameter in the Mondrian Forest, and\nthat commonly-used double precision values may not be necessary for optimal\nperformance. Future work will evaluate the generalizability of these findings\nto other data stream classifiers.",
          "link": "http://arxiv.org/abs/2106.14340",
          "publishedOn": "2021-06-29T01:55:16.597Z",
          "wordCount": 611,
          "title": "Reducing numerical precision preserves classification accuracy in Mondrian Forests. (arXiv:2106.14340v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14207",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1\">Amith Khandakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1\">Muhammad E. H. Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reaz_M/0/1/0/all/0/1\">Mamun Bin Ibne Reaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sawal Hamid Md Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_M/0/1/0/all/0/1\">Md Anwarul Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1\">Tawsifur Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alfkey_R/0/1/0/all/0/1\">Rashad Alfkey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakar_A/0/1/0/all/0/1\">Ahmad Ashrif A. Bakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_R/0/1/0/all/0/1\">Rayaz A. Malik</a>",
          "description": "Diabetes foot ulceration (DFU) and amputation are a cause of significant\nmorbidity. The prevention of DFU may be achieved by the identification of\npatients at risk of DFU and the institution of preventative measures through\neducation and offloading. Several studies have reported that thermogram images\nmay help to detect an increase in plantar temperature prior to DFU. However,\nthe distribution of plantar temperature may be heterogeneous, making it\ndifficult to quantify and utilize to predict outcomes. We have compared a\nmachine learning-based scoring technique with feature selection and\noptimization techniques and learning classifiers to several state-of-the-art\nConvolutional Neural Networks (CNNs) on foot thermogram images and propose a\nrobust solution to identify the diabetic foot. A comparatively shallow CNN\nmodel, MobilenetV2 achieved an F1 score of ~95% for a two-feet thermogram\nimage-based classification and the AdaBoost Classifier used 10 features and\nachieved an F1 score of 97 %. A comparison of the inference time for the\nbest-performing networks confirmed that the proposed algorithm can be deployed\nas a smartphone application to allow the user to monitor the progression of the\nDFU in a home setting.",
          "link": "http://arxiv.org/abs/2106.14207",
          "publishedOn": "2021-06-29T01:55:16.567Z",
          "wordCount": 670,
          "title": "A Machine Learning Model for Early Detection of Diabetic Foot using Thermogram Images. (arXiv:2106.14207v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jun Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mohammad Al Hasan</a>",
          "description": "Supervised learning, while deployed in real-life scenarios, often encounters\ninstances of unknown classes. Conventional algorithms for training a supervised\nlearning model do not provide an option to detect such instances, so they\nmiss-classify such instances with 100% probability. Open Set Recognition (OSR)\nand Non-Exhaustive Learning (NEL) are potential solutions to overcome this\nproblem. Most existing methods of OSR first classify members of existing\nclasses and then identify instances of new classes. However, many of the\nexisting methods of OSR only makes a binary decision, i.e., they only identify\nthe existence of the unknown class. Hence, such methods cannot distinguish test\ninstances belonging to incremental unseen classes. On the other hand, the\nmajority of NEL methods often make a parametric assumption over the data\ndistribution, which either fail to return good results, due to the reason that\nreal-life complex datasets may not follow a well-known data distribution. In\nthis paper, we propose a new online non-exhaustive learning model, namely,\nNon-Exhaustive Gaussian Mixture Generative Adversarial Networks (NE-GM-GAN) to\naddress these issues. Our proposed model synthesizes Gaussian mixture based\nlatent representation over a deep generative model, such as GAN, for\nincremental detection of instances of emerging classes in the test data.\nExtensive experimental results on several benchmark datasets show that\nNE-GM-GAN significantly outperforms the state-of-the-art methods in detecting\ninstances of novel classes in streaming data.",
          "link": "http://arxiv.org/abs/2106.14344",
          "publishedOn": "2021-06-29T01:55:16.551Z",
          "wordCount": 653,
          "title": "Non-Exhaustive Learning Using Gaussian Mixture Generative Adversarial Networks. (arXiv:2106.14344v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10358",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shoji_T/0/1/0/all/0/1\">Taku Shoji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshida_N/0/1/0/all/0/1\">Noboru Yoshida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanaka_T/0/1/0/all/0/1\">Toshihisa Tanaka</a>",
          "description": "Electroencephalography (EEG) is essential for the diagnosis of epilepsy, but\nit requires expertise and experience to identify abnormalities. It is thus\ncrucial to develop automated models for the detection of abnormalities in EEGs\nrelated to epilepsy. This paper describes the development of a novel class of\ncompact convolutional neural networks (CNNs) for detecting abnormal patterns\nand electrodes in EEGs for epilepsy. The designed model is inspired by a CNN\ndeveloped for brain-computer interfacing called multichannel EEGNet (mEEGNet).\nUnlike the EEGNet, the proposed model, mEEGNet, has the same number of\nelectrode inputs and outputs to detect abnormal patterns. The mEEGNet was\nevaluated with a clinical dataset consisting of 29 cases of juvenile and\nchildhood absence epilepsy labeled by a clinical expert. The labels were given\nto paroxysmal discharges visually observed in both ictal (seizure) and\ninterictal (nonseizure) durations. Results showed that the mEEGNet detected\nabnormalities with the area under the curve, F1-values, and sensitivity\nequivalent to or higher than those of existing CNNs. Moreover, the number of\nparameters is much smaller than other CNN models. To our knowledge, the dataset\nof absence epilepsy validated with machine learning through this research is\nthe largest in the literature.",
          "link": "http://arxiv.org/abs/2105.10358",
          "publishedOn": "2021-06-29T01:55:16.545Z",
          "wordCount": 666,
          "title": "Automated Detection of Abnormalities from an EEG Recording of Epilepsy Patients With a Compact Convolutional Neural Network. (arXiv:2105.10358v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.12301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ito_R/0/1/0/all/0/1\">Rei Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsukada_M/0/1/0/all/0/1\">Mineto Tsukada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1\">Hiroki Matsutani</a>",
          "description": "Most edge AI focuses on prediction tasks on resource-limited edge devices\nwhile the training is done at server machines. However, retraining or\ncustomizing a model is required at edge devices as the model is becoming\noutdated due to environmental changes over time. To follow such a concept\ndrift, a neural-network based on-device learning approach is recently proposed,\nso that edge devices train incoming data at runtime to update their model. In\nthis case, since a training is done at distributed edge devices, the issue is\nthat only a limited amount of training data can be used for each edge device.\nTo address this issue, one approach is a cooperative learning or federated\nlearning, where edge devices exchange their trained results and update their\nmodel by using those collected from the other devices. In this paper, as an\non-device learning algorithm, we focus on OS-ELM (Online Sequential Extreme\nLearning Machine) to sequentially train a model based on recent samples and\ncombine it with autoencoder for anomaly detection. We extend it for an\non-device federated learning so that edge devices can exchange their trained\nresults and update their model by using those collected from the other edge\ndevices. This cooperative model update is one-shot while it can be repeatedly\napplied to synchronize their model. Our approach is evaluated with anomaly\ndetection tasks generated from a driving dataset of cars, a human activity\ndataset, and MNIST dataset. The results demonstrate that the proposed on-device\nfederated learning can produce a merged model by integrating trained results\nfrom multiple edge devices as accurately as traditional backpropagation based\nneural networks and a traditional federated learning approach with lower\ncomputation or communication cost.",
          "link": "http://arxiv.org/abs/2002.12301",
          "publishedOn": "2021-06-29T01:55:16.531Z",
          "wordCount": 786,
          "title": "An On-Device Federated Learning Approach for Cooperative Model Update between Edge Devices. (arXiv:2002.12301v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jiajing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wenxuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yangkang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yaxin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>",
          "description": "Graph neural networks (GNNs) constitute a class of deep learning methods for\ngraph data. They have wide applications in chemistry and biology, such as\nmolecular property prediction, reaction prediction and drug-target interaction\nprediction. Despite the interest, GNN-based modeling is challenging as it\nrequires graph data pre-processing and modeling in addition to programming and\ndeep learning. Here we present DGL-LifeSci, an open-source package for deep\nlearning on graphs in life science. DGL-LifeSci is a python toolkit based on\nRDKit, PyTorch and Deep Graph Library (DGL). DGL-LifeSci allows GNN-based\nmodeling on custom datasets for molecular property prediction, reaction\nprediction and molecule generation. With its command-line interfaces, users can\nperform modeling without any background in programming and deep learning. We\ntest the command-line interfaces using standard benchmarks MoleculeNet, USPTO,\nand ZINC. Compared with previous implementations, DGL-LifeSci achieves a speed\nup by up to 6x. For modeling flexibility, DGL-LifeSci provides well-optimized\nmodules for various stages of the modeling pipeline. In addition, DGL-LifeSci\nprovides pre-trained models for reproducing the test experiment results and\napplying models without training. The code is distributed under an Apache-2.0\nLicense and is freely accessible at https://github.com/awslabs/dgl-lifesci.",
          "link": "http://arxiv.org/abs/2106.14232",
          "publishedOn": "2021-06-29T01:55:16.516Z",
          "wordCount": 637,
          "title": "DGL-LifeSci: An Open-Source Toolkit for Deep Learning on Graphs in Life Science. (arXiv:2106.14232v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1\">Rohan Ramanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salomatin_K/0/1/0/all/0/1\">Konstantin Salomatin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1\">Jeffrey D. Gee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talanine_K/0/1/0/all/0/1\">Kirill Talanine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalal_O/0/1/0/all/0/1\">Onkar Dalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polatkan_G/0/1/0/all/0/1\">Gungor Polatkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smoot_S/0/1/0/all/0/1\">Sara Smoot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Deepak Kumar</a>",
          "description": "One of the most well-established applications of machine learning is in\ndeciding what content to show website visitors. When observation data comes\nfrom high-velocity, user-generated data streams, machine learning methods\nperform a balancing act between model complexity, training time, and\ncomputational costs. Furthermore, when model freshness is critical, the\ntraining of models becomes time-constrained. Parallelized batch offline\ntraining, although horizontally scalable, is often not time-considerate or\ncost-effective. In this paper, we propose Lambda Learner, a new framework for\ntraining models by incremental updates in response to mini-batches from data\nstreams. We show that the resulting model of our framework closely estimates a\nperiodically updated model trained on offline data and outperforms it when\nmodel updates are time-sensitive. We provide theoretical proof that the\nincremental learning updates improve the loss-function over a stale batch\nmodel. We present a large-scale deployment on the sponsored content platform\nfor a large social network, serving hundreds of millions of users across\ndifferent channels (e.g., desktop, mobile). We address challenges and\ncomplexities from both algorithms and infrastructure perspectives, and\nillustrate the system details for computation, storage, and streaming\nproduction of training data.",
          "link": "http://arxiv.org/abs/2010.05154",
          "publishedOn": "2021-06-29T01:55:16.476Z",
          "wordCount": 680,
          "title": "Lambda Learner: Fast Incremental Learning on Data Streams. (arXiv:2010.05154v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Dropout is a powerful and widely used technique to regularize the training of\ndeep neural networks. In this paper, we introduce a simple regularization\nstrategy upon dropout in model training, namely R-Drop, which forces the output\ndistributions of different sub models generated by dropout to be consistent\nwith each other. Specifically, for each training sample, R-Drop minimizes the\nbidirectional KL-divergence between the output distributions of two sub models\nsampled by dropout. Theoretical analysis reveals that R-Drop reduces the\nfreedom of the model parameters and complements dropout. Experiments on\n$\\bf{5}$ widely used deep learning tasks ($\\bf{18}$ datasets in total),\nincluding neural machine translation, abstractive summarization, language\nunderstanding, language modeling, and image classification, show that R-Drop is\nuniversally effective. In particular, it yields substantial improvements when\napplied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large,\nand BART, and achieves state-of-the-art (SOTA) performances with the vanilla\nTransformer model on WMT14 English$\\to$German translation ($\\bf{30.91}$ BLEU)\nand WMT14 English$\\to$French translation ($\\bf{43.95}$ BLEU), even surpassing\nmodels trained with extra large-scale data and expert-designed advanced\nvariants of Transformer models. Our code is available at\nGitHub{\\url{https://github.com/dropreg/R-Drop}}.",
          "link": "http://arxiv.org/abs/2106.14448",
          "publishedOn": "2021-06-29T01:55:16.470Z",
          "wordCount": 616,
          "title": "R-Drop: Regularized Dropout for Neural Networks. (arXiv:2106.14448v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aounon Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Randomized smoothing has been successfully applied to classification tasks on\nhigh-dimensional inputs, such as images, to obtain models that are provably\nrobust against adversarial perturbations of the input. We extend this technique\nto produce provable robustness for functions that map inputs into an arbitrary\nmetric space rather than discrete classes. Such functions are used in many\nmachine learning problems like image reconstruction, dimensionality reduction,\nfacial recognition, etc. Our robustness certificates guarantee that the change\nin the output of the smoothed model as measured by the distance metric remains\nsmall for any norm-bounded perturbation of the input. We can certify robustness\nunder a variety of different output metrics, such as total variation distance,\nJaccard distance, perceptual metrics, etc. In our experiments, we apply our\nprocedure to create certifiably robust models with disparate output spaces --\nfrom sets to images -- and show that it yields meaningful certificates without\nsignificantly degrading the performance of the base model. The code for our\nexperiments is available at: https://github.com/aounon/center-smoothing.",
          "link": "http://arxiv.org/abs/2102.09701",
          "publishedOn": "2021-06-29T01:55:16.459Z",
          "wordCount": 620,
          "title": "Center Smoothing: Provable Robustness for Functions with Metric-Space Outputs. (arXiv:2102.09701v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joon Sik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1\">Gregory Plumb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>",
          "description": "Saliency methods are a popular class of feature attribution tools that aim to\ncapture a model's predictive reasoning by identifying \"important\" pixels in an\ninput image. However, the development and adoption of saliency methods are\ncurrently hindered by the lack of access to underlying model reasoning, which\nprevents accurate method evaluation. In this work, we design a synthetic\nevaluation framework, SMERF, that allows us to perform ground-truth-based\nevaluation of saliency methods while controlling the underlying complexity of\nmodel reasoning. Experimental evaluations via SMERF reveal significant\nlimitations in existing saliency methods, especially given the relative\nsimplicity of SMERF's synthetic evaluation tasks. Moreover, the SMERF\nbenchmarking suite represents a useful tool in the development of new saliency\nmethods to potentially overcome these limitations.",
          "link": "http://arxiv.org/abs/2105.06506",
          "publishedOn": "2021-06-29T01:55:16.381Z",
          "wordCount": 573,
          "title": "Sanity Simulations for Saliency Methods. (arXiv:2105.06506v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>",
          "description": "Recent developments in Text Style Transfer have led this field to be more\nhighlighted than ever. The task of transferring an input's style to another is\naccompanied by plenty of challenges (e.g., fluency and content preservation)\nthat need to be taken care of. In this research, we introduce PGST, a novel\npolyglot text style transfer approach in the gender domain, composed of\ndifferent constitutive elements. In contrast to prior studies, it is feasible\nto apply a style transfer method in multiple languages by fulfilling our\nmethod's predefined elements. We have proceeded with a pre-trained word\nembedding for token replacement purposes, a character-based token classifier\nfor gender exchange purposes, and a beam search algorithm for extracting the\nmost fluent combination. Since different approaches are introduced in our\nresearch, we determine a trade-off value for evaluating different models'\nsuccess in faking our gender identification model with transferred text. To\ndemonstrate our method's multilingual applicability, we applied our method on\nboth English and Persian corpora and ended up defeating our proposed gender\nidentification model by 45.6% and 39.2%, respectively. While this research's\nfocus is not limited to a specific language, our obtained evaluation results\nare highly competitive in an analogy among English state of the art methods.",
          "link": "http://arxiv.org/abs/2009.01040",
          "publishedOn": "2021-06-29T01:55:16.365Z",
          "wordCount": 673,
          "title": "PGST: a Polyglot Gender Style Transfer method. (arXiv:2009.01040v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bento_J/0/1/0/all/0/1\">Jo&#xe3;o Bento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleiro_P/0/1/0/all/0/1\">Pedro Saleiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_A/0/1/0/all/0/1\">Andr&#xe9; F. Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1\">M&#xe1;rio A.T. Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1\">Pedro Bizarro</a>",
          "description": "Although recurrent neural networks (RNNs) are state-of-the-art in numerous\nsequential decision-making tasks, there has been little research on explaining\ntheir predictions. In this work, we present TimeSHAP, a model-agnostic\nrecurrent explainer that builds upon KernelSHAP and extends it to the\nsequential domain. TimeSHAP computes feature-, timestep-, and cell-level\nattributions. As sequences may be arbitrarily long, we further propose a\npruning method that is shown to dramatically decrease both its computational\ncost and the variance of its attributions. We use TimeSHAP to explain the\npredictions of a real-world bank account takeover fraud detection RNN model,\nand draw key insights from its explanations: i) the model identifies important\nfeatures and events aligned with what fraud analysts consider cues for account\ntakeover; ii) positive predicted sequences can be pruned to only 10% of the\noriginal length, as older events have residual attribution values; iii) the\nmost recent input event of positive predictions only contributes on average to\n41% of the model's score; iv) notably high attribution to client's age,\nsuggesting a potential discriminatory reasoning, later confirmed as higher\nfalse positive rates for older clients.",
          "link": "http://arxiv.org/abs/2012.00073",
          "publishedOn": "2021-06-29T01:55:16.359Z",
          "wordCount": 653,
          "title": "TimeSHAP: Explaining Recurrent Models through Sequence Perturbations. (arXiv:2012.00073v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gasse_M/0/1/0/all/0/1\">Maxime Gasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasset_D/0/1/0/all/0/1\">Damien Grasset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaudron_G/0/1/0/all/0/1\">Guillaume Gaudron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Learning efficiently a causal model of the environment is a key challenge of\nmodel-based RL agents operating in POMDPs. We consider here a scenario where\nthe learning agent has the ability to collect online experiences through direct\ninteractions with the environment (interventional data), but has also access to\na large collection of offline experiences, obtained by observing another agent\ninteracting with the environment (observational data). A key ingredient, that\nmakes this situation non-trivial, is that we allow the observed agent to\ninteract with the environment based on hidden information, which is not\nobserved by the learning agent. We then ask the following questions: can the\nonline and offline experiences be safely combined for learning a causal model ?\nAnd can we expect the offline experiences to improve the agent's performances ?\nTo answer these questions, we import ideas from the well-established causal\nframework of do-calculus, and we express model-based reinforcement learning as\na causal inference problem. Then, we propose a general yet simple methodology\nfor leveraging offline data during learning. In a nutshell, the method relies\non learning a latent-based causal transition model that explains both the\ninterventional and observational regimes, and then using the recovered latent\nvariable to infer the standard POMDP transition model via deconfounding. We\nprove our method is correct and efficient in the sense that it attains better\ngeneralization guarantees due to the offline data (in the asymptotic case), and\nwe illustrate its effectiveness empirically on synthetic toy problems. Our\ncontribution aims at bridging the gap between the fields of reinforcement\nlearning and causality.",
          "link": "http://arxiv.org/abs/2106.14421",
          "publishedOn": "2021-06-29T01:55:16.353Z",
          "wordCount": 687,
          "title": "Causal Reinforcement Learning using Observational and Interventional Data. (arXiv:2106.14421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yonghao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>",
          "description": "Most sequential recommendation models capture the features of consecutive\nitems in a user-item interaction history. Though effective, their\nrepresentation expressiveness is still hindered by the sparse learning signals.\nAs a result, the sequential recommender is prone to make inconsistent\npredictions. In this paper, we propose a model, \\textbf{SSI}, to improve\nsequential recommendation consistency with Self-Supervised Imitation.\nPrecisely, we extract the consistency knowledge by utilizing three\nself-supervised pre-training tasks, where temporal consistency and persona\nconsistency capture user-interaction dynamics in terms of the chronological\norder and persona sensitivities, respectively. Furthermore, to provide the\nmodel with a global perspective, global session consistency is introduced by\nmaximizing the mutual information among global and local interaction sequences.\nFinally, to comprehensively take advantage of all three independent aspects of\nconsistency-enhanced knowledge, we establish an integrated imitation learning\nframework. The consistency knowledge is effectively internalized and\ntransferred to the student model by imitating the conventional prediction logit\nas well as the consistency-enhanced item representations. In addition, the\nflexible self-supervised imitation framework can also benefit other student\nrecommenders. Experiments on four real-world datasets show that SSI effectively\noutperforms the state-of-the-art sequential recommendation methods.",
          "link": "http://arxiv.org/abs/2106.14031",
          "publishedOn": "2021-06-29T01:55:16.347Z",
          "wordCount": 623,
          "title": "Improving Sequential Recommendation Consistency with Self-Supervised Imitation. (arXiv:2106.14031v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14257",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Gupta_K/0/1/0/all/0/1\">Kanhaiya Gupta</a>",
          "description": "In recent years, artificial neural networks (ANNs) have won numerous contests\nin pattern recognition and machine learning. ANNS have been applied to problems\nranging from speech recognition to prediction of protein secondary structure,\nclassification of cancers, and gene prediction. Here, we intend to maximize the\nchances of finding the Higgs boson decays to two $\\tau$ leptons in the pseudo\ndataset using a Machine Learning technique to classify the recorded events as\nsignal or background.",
          "link": "http://arxiv.org/abs/2106.14257",
          "publishedOn": "2021-06-29T01:55:16.341Z",
          "wordCount": 528,
          "title": "Use of Machine Learning Technique to maximize the signal over background for $H \\rightarrow \\tau \\tau$. (arXiv:2106.14257v1 [physics.data-an])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dandi_Y/0/1/0/all/0/1\">Yatin Dandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barba_L/0/1/0/all/0/1\">Luis Barba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "A major obstacle to achieving global convergence in distributed and federated\nlearning is the misalignment of gradients across clients, or mini-batches due\nto heterogeneity and stochasticity of the distributed data. One way to\nalleviate this problem is to encourage the alignment of gradients across\ndifferent clients throughout training. Our analysis reveals that this goal can\nbe accomplished by utilizing the right optimization method that replicates the\nimplicit regularization effect of SGD, leading to gradient alignment as well as\nimprovements in test accuracies. Since the existence of this regularization in\nSGD completely relies on the sequential use of different mini-batches during\ntraining, it is inherently absent when training with large mini-batches. To\nobtain the generalization benefits of this regularization while increasing\nparallelism, we propose a novel GradAlign algorithm that induces the same\nimplicit regularization while allowing the use of arbitrarily large batches in\neach update. We experimentally validate the benefit of our algorithm in\ndifferent distributed and federated learning settings.",
          "link": "http://arxiv.org/abs/2106.13897",
          "publishedOn": "2021-06-29T01:55:16.327Z",
          "wordCount": 592,
          "title": "Implicit Gradient Alignment in Distributed and Federated Learning. (arXiv:2106.13897v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buyl_M/0/1/0/all/0/1\">Maarten Buyl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_T/0/1/0/all/0/1\">Tijl De Bie</a>",
          "description": "Learning and reasoning over graphs is increasingly done by means of\nprobabilistic models, e.g. exponential random graph models, graph embedding\nmodels, and graph neural networks. When graphs are modeling relations between\npeople, however, they will inevitably reflect biases, prejudices, and other\nforms of inequity and inequality. An important challenge is thus to design\naccurate graph modeling approaches while guaranteeing fairness according to the\nspecific notion of fairness that the problem requires. Yet, past work on the\ntopic remains scarce, is limited to debiasing specific graph modeling methods,\nand often aims to ensure fairness in an indirect manner.\n\nWe propose a generic approach applicable to most probabilistic graph modeling\napproaches. Specifically, we first define the class of fair graph models\ncorresponding to a chosen set of fairness criteria. Given this, we propose a\nfairness regularizer defined as the KL-divergence between the graph model and\nits I-projection onto the set of fair models. We demonstrate that using this\nfairness regularizer in combination with existing graph modeling approaches\nefficiently trades-off fairness with accuracy, whereas the state-of-the-art\nmodels can only make this trade-off for the fairness criterion that they were\nspecifically designed for.",
          "link": "http://arxiv.org/abs/2103.01846",
          "publishedOn": "2021-06-29T01:55:16.321Z",
          "wordCount": 655,
          "title": "The KL-Divergence between a Graph Model and its Fair I-Projection as a Fairness Regularizer. (arXiv:2103.01846v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "Polygonal meshes are ubiquitous, but have only played a relatively minor role\nin the deep learning revolution. State-of-the-art neural generative models for\n3D shapes learn implicit functions and generate meshes via expensive\niso-surfacing. We overcome these challenges by employing a classical spatial\ndata structure from computer graphics, Binary Space Partitioning (BSP), to\nfacilitate 3D learning. The core operation of BSP involves recursive\nsubdivision of 3D space to obtain convex sets. By exploiting this property, we\ndevise BSP-Net, a network that learns to represent a 3D shape via convex\ndecomposition without supervision. The network is trained to reconstruct a\nshape using a set of convexes obtained from a BSP-tree built over a set of\nplanes, where the planes and convexes are both defined by learned network\nweights. BSP-Net directly outputs polygonal meshes from the inferred convexes.\nThe generated meshes are watertight, compact (i.e., low-poly), and well suited\nto represent sharp geometry. We show that the reconstruction quality by BSP-Net\nis competitive with those from state-of-the-art methods while using much fewer\nprimitives. We also explore variations to BSP-Net including using a more\ngeneric decoder for reconstruction, more general primitives than planes, as\nwell as training a generative model with variational auto-encoders. Code is\navailable at https://github.com/czq142857/BSP-NET-original.",
          "link": "http://arxiv.org/abs/2106.14274",
          "publishedOn": "2021-06-29T01:55:16.313Z",
          "wordCount": 663,
          "title": "Learning Mesh Representations via Binary Space Partitioning Tree Networks. (arXiv:2106.14274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02800",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1\">Konstantina Sampani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1\">Mengjia Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1\">Shengze Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1\">Yixiang Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">He Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer K. Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Em Karniadakis</a>",
          "description": "Microaneurysms (MAs) are one of the earliest signs of diabetic retinopathy\n(DR), a frequent complication of diabetes that can lead to visual impairment\nand blindness. Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides\nreal-time retinal images with resolution down to 2 $\\mu m$ and thus allows\ndetection of the morphologies of individual MAs, a potential marker that might\ndictate MA pathology and affect the progression of DR. In contrast to the\nnumerous automatic models developed for assessing the number of MAs on fundus\nphotographs, currently there is no high throughput image protocol available for\nautomatic analysis of AOSLO photographs. To address this urgency, we introduce\nAOSLO-net, a deep neural network framework with customized training policies to\nautomatically segment MAs from AOSLO images. We evaluate the performance of\nAOSLO-net using 87 DR AOSLO images and our results demonstrate that the\nproposed model outperforms the state-of-the-art segmentation model both in\naccuracy and cost and enables correct MA morphological classification.",
          "link": "http://arxiv.org/abs/2106.02800",
          "publishedOn": "2021-06-29T01:55:16.308Z",
          "wordCount": 651,
          "title": "AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1\">Colby Banbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1\">Vijay Janapa Reddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torelli_P/0/1/0/all/0/1\">Peter Torelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holleman_J/0/1/0/all/0/1\">Jeremy Holleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeffries_N/0/1/0/all/0/1\">Nat Jeffries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiraly_C/0/1/0/all/0/1\">Csaba Kiraly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montino_P/0/1/0/all/0/1\">Pietro Montino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanter_D/0/1/0/all/0/1\">David Kanter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sebastian Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pau_D/0/1/0/all/0/1\">Danilo Pau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1\">Urmish Thakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torrini_A/0/1/0/all/0/1\">Antonio Torrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1\">Peter Warden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordaro_J/0/1/0/all/0/1\">Jay Cordaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guglielmo_G/0/1/0/all/0/1\">Giuseppe Di Guglielmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1\">Javier Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibellini_S/0/1/0/all/0/1\">Stephen Gibellini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1\">Videet Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Honson Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Nhan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenxu_N/0/1/0/all/0/1\">Niu Wenxu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuesong_X/0/1/0/all/0/1\">Xu Xuesong</a>",
          "description": "Advancements in ultra-low-power tiny machine learning (TinyML) systems\npromise to unlock an entirely new class of smart applications. However,\ncontinued progress is limited by the lack of a widely accepted and easily\nreproducible benchmark for these systems. To meet this need, we present MLPerf\nTiny, the first industry-standard benchmark suite for ultra-low-power tiny\nmachine learning systems. The benchmark suite is the collaborative effort of\nmore than 50 organizations from industry and academia and reflects the needs of\nthe community. MLPerf Tiny measures the accuracy, latency, and energy of\nmachine learning inference to properly evaluate the tradeoffs between systems.\nAdditionally, MLPerf Tiny implements a modular design that enables benchmark\nsubmitters to show the benefits of their product, regardless of where it falls\non the ML deployment stack, in a fair and reproducible manner. The suite\nfeatures four benchmarks: keyword spotting, visual wake words, image\nclassification, and anomaly detection.",
          "link": "http://arxiv.org/abs/2106.07597",
          "publishedOn": "2021-06-29T01:55:16.297Z",
          "wordCount": 629,
          "title": "MLPerf Tiny Benchmark. (arXiv:2106.07597v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.07272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1\">Aleksandrs Slivkins</a>",
          "description": "Multi-armed bandits a simple but very powerful framework for algorithms that\nmake decisions over time under uncertainty. An enormous body of work has\naccumulated over the years, covered in several books and surveys. This book\nprovides a more introductory, textbook-like treatment of the subject. Each\nchapter tackles a particular line of work, providing a self-contained,\nteachable technical introduction and a brief review of the further\ndevelopments; many of the chapters conclude with exercises.\n\nThe book is structured as follows. The first four chapters are on IID\nrewards, from the basic model to impossibility results to Bayesian priors to\nLipschitz rewards. The next three chapters cover adversarial rewards, from the\nfull-feedback version to adversarial bandits to extensions with linear rewards\nand combinatorially structured actions. Chapter 8 is on contextual bandits, a\nmiddle ground between IID and adversarial bandits in which the change in reward\ndistributions is completely explained by observable contexts. The last three\nchapters cover connections to economics, from learning in repeated games to\nbandits with supply/budget constraints to exploration in the presence of\nincentives. The appendix provides sufficient background on concentration and\nKL-divergence.\n\nThe chapters on \"bandits with similarity information\", \"bandits with\nknapsacks\" and \"bandits and agents\" can also be consumed as standalone surveys\non the respective topics.",
          "link": "http://arxiv.org/abs/1904.07272",
          "publishedOn": "2021-06-29T01:55:16.281Z",
          "wordCount": 748,
          "title": "Introduction to Multi-Armed Bandits. (arXiv:1904.07272v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14186",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ferla_M/0/1/0/all/0/1\">Michele La Ferla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Montebello_M/0/1/0/all/0/1\">Matthew Montebello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seychell_D/0/1/0/all/0/1\">Dylan Seychell</a>",
          "description": "During the last decade or so, there has been an insurgence in the deep\nlearning community to solve health-related issues, particularly breast cancer.\nFollowing the Camelyon-16 challenge in 2016, several researchers have dedicated\ntheir time to build Convolutional Neural Networks (CNNs) to help radiologists\nand other clinicians diagnose breast cancer. In particular, there has been an\nemphasis on Ductal Carcinoma in Situ (DCIS); the clinical term for early-stage\nbreast cancer. Large companies have given their fair share of research into\nthis subject, among these Google Deepmind who developed a model in 2020 that\nhas proven to be better than radiologists themselves to diagnose breast cancer\ncorrectly.\n\nWe found that among the issues which exist, there is a need for an\nexplanatory system that goes through the hidden layers of a CNN to highlight\nthose pixels that contributed to the classification of a mammogram. We then\nchose an open-source, reasonably successful project developed by Prof. Shen,\nusing the CBIS-DDSM image database to run our experiments on. It was later\nimproved using the Resnet-50 and VGG-16 patch-classifiers, analytically\ncomparing the outcome of both. The results showed that the Resnet-50 one\nconverged earlier in the experiments.\n\nFollowing the research by Montavon and Binder, we used the DeepTaylor\nLayer-wise Relevance Propagation (LRP) model to highlight those pixels and\nregions within a mammogram which contribute most to its classification. This is\nrepresented as a map of those pixels in the original image, which contribute to\nthe diagnosis and the extent to which they contribute to the final\nclassification. The most significant advantage of this algorithm is that it\nperforms exceptionally well with the Resnet-50 patch classifier architecture.",
          "link": "http://arxiv.org/abs/2106.14186",
          "publishedOn": "2021-06-29T01:55:16.275Z",
          "wordCount": 739,
          "title": "An XAI Approach to Deep Learning Models in the Detection of Ductal Carcinoma in Situ. (arXiv:2106.14186v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1\">Yash Bhartia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suthaharan_S/0/1/0/all/0/1\">Shan Suthaharan</a>",
          "description": "Toxicity detection of text has been a popular NLP task in the recent years.\nIn SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic\nspans within passages. Most state-of-the-art span detection approaches employ\nvarious techniques, each of which can be broadly classified into Token\nClassification or Span Prediction approaches. In our paper, we explore simple\nversions of both of these approaches and their performance on the task.\nSpecifically, we use BERT-based models -- BERT, RoBERTa, and SpanBERT for both\napproaches. We also combine these approaches and modify them to bring\nimprovements for Toxic Spans prediction. To this end, we investigate results on\nfour hybrid approaches -- Multi-Span, Span+Token, LSTM-CRF, and a combination\nof predicted offsets using union/intersection. Additionally, we perform a\nthorough ablative analysis and analyze our observed results. Our best\nsubmission -- a combination of SpanBERT Span Predictor and RoBERTa Token\nClassifier predictions -- achieves an F1 score of 0.6753 on the test set. Our\nbest post-eval F1 score is 0.6895 on intersection of predicted offsets from\ntop-3 RoBERTa Token Classification checkpoints. These approaches improve the\nperformance by 3% on average than those of the shared baseline models -- RNNSL\nand SpaCy NER.",
          "link": "http://arxiv.org/abs/2102.12254",
          "publishedOn": "2021-06-29T01:55:16.264Z",
          "wordCount": 685,
          "title": "NLRG at SemEval-2021 Task 5: Toxic Spans Detection Leveraging BERT-based Token Classification and Span Prediction Techniques. (arXiv:2102.12254v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_W/0/1/0/all/0/1\">Waddah Saeed</a>",
          "description": "Short Message Service (SMS) is a very popular service used for communication\nby mobile users. However, this popular service can be abused by executing\nillegal activities and influencing security risks. Nowadays, many automatic\nmachine learning (AutoML) tools exist which can help domain experts and lay\nusers to build high-quality ML models with little or no machine learning\nknowledge. In this work, a classification performance comparison was conducted\nbetween three automatic ML tools for SMS spam message filtering. These tools\nare mljar-supervised AutoML, H2O AutoML, and Tree-based Pipeline Optimization\nTool (TPOT) AutoML. Experimental results showed that ensemble models achieved\nthe best classification performance. The Stacked Ensemble model, which was\nbuilt using H2O AutoML, achieved the best performance in terms of Log Loss\n(0.8370), true positive (1088/1116), and true negative (281/287) metrics. There\nis a 19.05\\% improvement in Log Loss with respect to TPOT AutoML and 5.56\\%\nimprovement with respect to mljar-supervised AutoML. The satisfactory filtering\nperformance achieved with AutoML tools provides a potential application for\nAutoML tools to automatically determine the best ML model that can perform best\nfor SMS spam message filtering.",
          "link": "http://arxiv.org/abs/2106.08671",
          "publishedOn": "2021-06-29T01:55:16.250Z",
          "wordCount": 633,
          "title": "Comparison of Automated Machine Learning Tools for SMS Spam Message Filtering. (arXiv:2106.08671v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07566",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fanyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Haotian Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1\">Cheng Shen</a>",
          "description": "Attention mechanism has shown enormous potential for single image\nsuper-resolution (SISR). However, existing works only proposed some attention\nmechanism for a specific network. A universal attention mechanism for SISR,\nwhich could further improve the performance of networks without attention and\nprovide a baseline for networks with attention, is still lacking. To fit this\ngap, we propose a lightweight and efficient Balanced Attention Mechanism (BAM),\nwhich consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial\nAttention Module (MSAM) in parallel. The information extraction mechanism of\nACAM and MSAM effectively filters redundant information, making the overall\nstructure of BAM very lightweight. Owing to the parallel structure, during the\ngradient backpropagation process of BAM, ACAM and MSAM not only conduct\nself-optimization, but also mutual optimization so as to generate more balanced\nattention information. To verify the effectiveness and robustness of BAM, we\napplied it to 12 state-ofthe-art SISR networks. The results on 4 benchmark\ndatasets demonstrate that BAM can efficiently improve the networks'\nperformance, and for those with attention, the substitution with BAM further\nreduces the amount of parameters and increase the inference speed. Moreover,\nablation experiments were conducted to prove the minimalism of BAM.",
          "link": "http://arxiv.org/abs/2104.07566",
          "publishedOn": "2021-06-29T01:55:16.244Z",
          "wordCount": 670,
          "title": "BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1\">Luis Felipe M.O. Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_E/0/1/0/all/0/1\">Eduardo Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1\">Sergio Colcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1\">Ruy Luiz Milidi&#xfa;</a>",
          "description": "Non-Intrusive Load Monitoring (NILM) is a computational technique to estimate\nthe power loads' appliance-by-appliance from the whole consumption measured by\na single meter. In this paper, we propose a conditional density estimation\nmodel, based on deep neural networks, that joins a Conditional Variational\nAutoencoder with a Conditional Invertible Normalizing Flow model to estimate\nthe individual appliance's power demand. The resulting model is called Prior\nFlow Variational Autoencoder or, for simplicity PFVAE. Thus, instead of having\none model per appliance, the resulting model is responsible for estimating the\npower demand, appliance-by-appliance, at once. We train and evaluate our\nproposed model in a publicly available dataset composed of power demand\nmeasures from a poultry feed factory located in Brazil. The proposed model's\nquality is evaluated by comparing the obtained normalized disaggregation error\n(NDE) and signal aggregated error (SAE) with the previous work values on the\nsame dataset. Our proposal achieves highly competitive results, and for six of\nthe eight machines belonging to the dataset, we observe consistent improvements\nthat go from 28% up to 81% in NDE and from 27% up to 86% in SAE.",
          "link": "http://arxiv.org/abs/2011.14870",
          "publishedOn": "2021-06-29T01:55:16.238Z",
          "wordCount": 663,
          "title": "Prior Flow Variational Autoencoder: A density estimation model for Non-Intrusive Load Monitoring. (arXiv:2011.14870v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-06-29T01:55:16.232Z",
          "wordCount": 628,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandak_S/0/1/0/all/0/1\">Siddharth Chandak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1\">Vivek S. Borkar</a>",
          "description": "Using a martingale concentration inequality, concentration bounds `from time\n$n_0$ on' are derived for stochastic approximation algorithms with contractive\nmaps and both martingale difference and Markov noises. These are applied to\nreinforcement learning algorithms, in particular to asynchronous Q-learning and\nTD(0).",
          "link": "http://arxiv.org/abs/2106.14308",
          "publishedOn": "2021-06-29T01:55:16.227Z",
          "wordCount": 481,
          "title": "Concentration of Contractive Stochastic Approximation and Reinforcement Learning. (arXiv:2106.14308v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muzellec_B/0/1/0/all/0/1\">Boris Muzellec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>",
          "description": "Kernel mean embeddings are a popular tool that consists in representing\nprobability measures by their infinite-dimensional mean embeddings in a\nreproducing kernel Hilbert space. When the kernel is characteristic, mean\nembeddings can be used to define a distance between probability measures, known\nas the maximum mean discrepancy (MMD). A well-known advantage of mean\nembeddings and MMD is their low computational cost and low sample complexity.\nHowever, kernel mean embeddings have had limited applications to problems that\nconsist in optimizing distributions, due to the difficulty of characterizing\nwhich Hilbert space vectors correspond to a probability distribution. In this\nnote, we propose to leverage the kernel sums-of-squares parameterization of\npositive functions of Marteau-Ferey et al. [2020] to fit distributions in the\nMMD geometry. First, we show that when the kernel is characteristic,\ndistributions with a kernel sum-of-squares density are dense. Then, we provide\nalgorithms to optimize such distributions in the finite-sample setting, which\nwe illustrate in a density fitting numerical experiment.",
          "link": "http://arxiv.org/abs/2106.09994",
          "publishedOn": "2021-06-29T01:55:16.221Z",
          "wordCount": 610,
          "title": "A Note on Optimizing Distributions using Kernel Mean Embeddings. (arXiv:2106.09994v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10293",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Sharir_O/0/1/0/all/0/1\">Or Sharir</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1\">Giuseppe Carleo</a>",
          "description": "We establish a direct connection between general tensor networks and deep\nfeed-forward artificial neural networks. The core of our results is the\nconstruction of neural-network layers that efficiently perform tensor\ncontractions, and that use commonly adopted non-linear activation functions.\nThe resulting deep networks feature a number of edges that closely matches the\ncontraction complexity of the tensor networks to be approximated. In the\ncontext of many-body quantum states, this result establishes that\nneural-network states have strictly the same or higher expressive power than\npractically usable variational tensor networks. As an example, we show that all\nmatrix product states can be efficiently written as neural-network states with\na number of edges polynomial in the bond dimension and depth logarithmic in the\nsystem size. The opposite instead does not hold true, and our results imply\nthat there exist quantum states that are not efficiently expressible in terms\nof matrix product states or practically usable PEPS, but that are instead\nefficiently expressible with neural network states.",
          "link": "http://arxiv.org/abs/2103.10293",
          "publishedOn": "2021-06-29T01:55:16.210Z",
          "wordCount": 625,
          "title": "Neural tensor contractions and the expressive power of deep neural quantum states. (arXiv:2103.10293v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashivan_P/0/1/0/all/0/1\">Pouya Bashivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayat_R/0/1/0/all/0/1\">Reza Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1\">Adam Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kartik Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faramarzi_M/0/1/0/all/0/1\">Mojtaba Faramarzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laleh_T/0/1/0/all/0/1\">Touraj Laleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richards_B/0/1/0/all/0/1\">Blake Aaron Richards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1\">Irina Rish</a>",
          "description": "Neural networks are known to be vulnerable to adversarial attacks -- slight\nbut carefully constructed perturbations of the inputs which can drastically\nimpair the network's performance. Many defense methods have been proposed for\nimproving robustness of deep networks by training them on adversarially\nperturbed inputs. However, these models often remain vulnerable to new types of\nattacks not seen during training, and even to slightly stronger versions of\npreviously seen attacks. In this work, we propose a novel approach to\nadversarial robustness, which builds upon the insights from the domain\nadaptation field. Our method, called Adversarial Feature Desensitization (AFD),\naims at learning features that are invariant towards adversarial perturbations\nof the inputs. This is achieved through a game where we learn features that are\nboth predictive and robust (insensitive to adversarial attacks), i.e. cannot be\nused to discriminate between natural and adversarial data. Empirical results on\nseveral benchmarks demonstrate the effectiveness of the proposed approach\nagainst a wide range of attack types and attack strengths.",
          "link": "http://arxiv.org/abs/2006.04621",
          "publishedOn": "2021-06-29T01:55:16.143Z",
          "wordCount": 630,
          "title": "Adversarial Feature Desensitization. (arXiv:2006.04621v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17236",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+He_Z/0/1/0/all/0/1\">Zichang He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>",
          "description": "Fabrication process variations can significantly influence the performance\nand yield of nano-scale electronic and photonic circuits. Stochastic spectral\nmethods have achieved great success in quantifying the impact of process\nvariations, but they suffer from the curse of dimensionality. Recently,\nlow-rank tensor methods have been developed to mitigate this issue, but two\nfundamental challenges remain open: how to automatically determine the tensor\nrank and how to adaptively pick the informative simulation samples. This paper\nproposes a novel tensor regression method to address these two challenges. We\nuse a $\\ell_{q}/ \\ell_{2}$ group-sparsity regularization to determine the\ntensor rank. The resulting optimization problem can be efficiently solved via\nan alternating minimization solver. We also propose a two-stage adaptive\nsampling method to reduce the simulation cost. Our method considers both\nexploration and exploitation via the estimated Voronoi cell volume and\nnonlinearity measurement respectively. The proposed model is verified with\nsynthetic and some realistic circuit benchmarks, on which our method can well\ncapture the uncertainty caused by 19 to 100 random variables with only 100 to\n600 simulation samples.",
          "link": "http://arxiv.org/abs/2103.17236",
          "publishedOn": "2021-06-29T01:55:16.137Z",
          "wordCount": 646,
          "title": "High-Dimensional Uncertainty Quantification via Tensor Regression with Rank Determination and Adaptive Sampling. (arXiv:2103.17236v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1\">Priyam Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1\">Tiasa Singha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muftuoglu_Z/0/1/0/all/0/1\">Zumrut Muftuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>",
          "description": "Natural Language Processing (NLP) techniques can be applied to help with the\ndiagnosis of medical conditions such as depression, using a collection of a\nperson's utterances. Depression is a serious medical illness that can have\nadverse effects on how one feels, thinks, and acts, which can lead to emotional\nand physical problems. Due to the sensitive nature of such data, privacy\nmeasures need to be taken for handling and training models with such data. In\nthis work, we study the effects that the application of Differential Privacy\n(DP) has, in both a centralized and a Federated Learning (FL) setup, on\ntraining contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).\nWe offer insights on how to privately train NLP models and what architectures\nand setups provide more desirable privacy utility trade-offs. We envisage this\nwork to be used in future healthcare and mental health studies to keep medical\nhistory private. Therefore, we provide an open-source implementation of this\nwork.",
          "link": "http://arxiv.org/abs/2106.13973",
          "publishedOn": "2021-06-29T01:55:16.131Z",
          "wordCount": 612,
          "title": "Benchmarking Differential Privacy and Federated Learning for BERT Models. (arXiv:2106.13973v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00771",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Mandralis_I/0/1/0/all/0/1\">Ioannis Mandralis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Weber_P/0/1/0/all/0/1\">Pascal Weber</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Novati_G/0/1/0/all/0/1\">Guido Novati</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Koumoutsakos_P/0/1/0/all/0/1\">Petros Koumoutsakos</a>",
          "description": "Swimming organisms can escape their predators by creating and harnessing\nunsteady flow fields through their body motions. Stochastic optimization and\nflow simulations have identified escape patterns that are consistent with those\nobserved in natural larval swimmers. However, these patterns have been limited\nby the specification of a particular cost function and depend on a prescribed\nfunctional form of the body motion. Here, we deploy reinforcement learning to\ndiscover swimmer escape patterns for larval fish under energy constraints. The\nidentified patterns include the C-start mechanism, in addition to more\nenergetically efficient escapes. We find that maximizing distance with limited\nenergy requires swimming via short bursts of accelerating motion interlinked\nwith phases of gliding. The present, data efficient, reinforcement learning\nalgorithm results in an array of patterns that reveal practical flow\noptimization principles for efficient swimming and the methodology can be\ntransferred to the control of aquatic robotic devices operating under energy\nconstraints.",
          "link": "http://arxiv.org/abs/2105.00771",
          "publishedOn": "2021-06-29T01:55:16.126Z",
          "wordCount": 613,
          "title": "Learning swimming escape patterns for larval fish under energy constraints. (arXiv:2105.00771v2 [physics.flu-dyn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jinshuo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>",
          "description": "In this rejoinder, we aim to address two broad issues that cover most\ncomments made in the discussion. First, we discuss some theoretical aspects of\nour work and comment on how this work might impact the theoretical foundation\nof privacy-preserving data analysis. Taking a practical viewpoint, we next\ndiscuss how f-differential privacy (f-DP) and Gaussian differential privacy\n(GDP) can make a difference in a range of applications.",
          "link": "http://arxiv.org/abs/2104.01987",
          "publishedOn": "2021-06-29T01:55:16.098Z",
          "wordCount": 555,
          "title": "Rejoinder: Gaussian Differential Privacy. (arXiv:2104.01987v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1\">Thomas Bartz-Beielstein</a>",
          "description": "A surrogate model based hyperparameter tuning approach for deep learning is\npresented. This article demonstrates how the architecture-level parameters\n(hyperparameters) of deep learning models that were implemented in\nKeras/tensorflow can be optimized. The implementation of the tuning procedure\nis 100% accessible from R, the software environment for statistical computing.\nWith a few lines of code, existing R packages (tfruns and SPOT) can be combined\nto perform hyperparameter tuning. An elementary hyperparameter tuning task\n(neural network and the MNIST data) is used to exemplify this approach",
          "link": "http://arxiv.org/abs/2105.14625",
          "publishedOn": "2021-06-29T01:55:16.092Z",
          "wordCount": 552,
          "title": "Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT. (arXiv:2105.14625v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06958",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sameni_R/0/1/0/all/0/1\">Reza Sameni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jutten_C/0/1/0/all/0/1\">Christian Jutten</a>",
          "description": "The extraction of nonstationary signals from blind and semi-blind\nmultivariate observations is a recurrent problem. Numerous algorithms have been\ndeveloped for this problem, which are based on the exact or approximate joint\ndiagonalization of second or higher order cumulant matrices/tensors of\nmultichannel data. While a great body of research has been dedicated to joint\ndiagonalization algorithms, the selection of the diagonalized matrix/tensor set\nremains highly problem-specific. Herein, various methods for nonstationarity\nidentification are reviewed and a new general framework based on hypothesis\ntesting is proposed, which results in a classification/clustering perspective\nto semi-blind source separation of nonstationary components. The proposed\nmethod is applied to noninvasive fetal ECG extraction, as case study.",
          "link": "http://arxiv.org/abs/2105.06958",
          "publishedOn": "2021-06-29T01:55:16.086Z",
          "wordCount": 566,
          "title": "A Hypothesis Testing Approach to Nonstationary Source Separation. (arXiv:2105.06958v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Setlur_A/0/1/0/all/0/1\">Amrith Setlur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_O/0/1/0/all/0/1\">Oscar Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1\">Virginia Smith</a>",
          "description": "We categorize meta-learning evaluation into two settings:\n$\\textit{in-distribution}$ [ID], in which the train and test tasks are sampled\n$\\textit{iid}$ from the same underlying task distribution, and\n$\\textit{out-of-distribution}$ [OOD], in which they are not. While most\nmeta-learning theory and some FSL applications follow the ID setting, we\nidentify that most existing few-shot classification benchmarks instead reflect\nOOD evaluation, as they use disjoint sets of train (base) and test (novel)\nclasses for task generation. This discrepancy is problematic because -- as we\nshow on numerous benchmarks -- meta-learning methods that perform better on\nexisting OOD datasets may perform significantly worse in the ID setting. In\naddition, in the OOD setting, even though current FSL benchmarks seem\nbefitting, our study highlights concerns in 1) reliably performing model\nselection for a given meta-learning method, and 2) consistently comparing the\nperformance of different methods. To address these concerns, we provide\nsuggestions on how to construct FSL benchmarks to allow for ID evaluation as\nwell as more reliable OOD evaluation. Our work aims to inform the meta-learning\ncommunity about the importance and distinction of ID vs. OOD evaluation, as\nwell as the subtleties of OOD evaluation with current benchmarks.",
          "link": "http://arxiv.org/abs/2102.11503",
          "publishedOn": "2021-06-29T01:55:16.080Z",
          "wordCount": 652,
          "title": "Two Sides of Meta-Learning Evaluation: In vs. Out of Distribution. (arXiv:2102.11503v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13865",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Peng_H/0/1/0/all/0/1\">Hsuan-Tung Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lederman_J/0/1/0/all/0/1\">Joshua Lederman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lima_T/0/1/0/all/0/1\">Thomas Ferreira de Lima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chaoran Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shastri_B/0/1/0/all/0/1\">Bhavin Shastri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rosenbluth_D/0/1/0/all/0/1\">David Rosenbluth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prucnal_P/0/1/0/all/0/1\">Paul Prucnal</a>",
          "description": "Machine learning (ML) methods are ubiquitous in wireless communication\nsystems and have proven powerful for applications including radio-frequency\n(RF) fingerprinting, automatic modulation classification, and cognitive radio.\nHowever, the large size of ML models can make them difficult to implement on\nedge devices for latency-sensitive downstream tasks. In wireless communication\nsystems, ML data processing at a sub-millisecond scale will enable real-time\nnetwork monitoring to improve security and prevent infiltration. In addition,\ncompact and integratable hardware platforms which can implement ML models at\nthe chip scale will find much broader application to wireless communication\nnetworks. Toward real-time wireless signal classification at the edge, we\npropose a novel compact deep network that consists of a\nphotonic-hardware-inspired recurrent neural network model in combination with a\nsimplified convolutional classifier, and we demonstrate its application to the\nidentification of RF emitters by their random transmissions. With the proposed\nmodel, we achieve 96.32% classification accuracy over a set of 30 identical\nZigBee devices when using 50 times fewer training parameters than an existing\nstate-of-the-art CNN classifier. Thanks to the large reduction in network size,\nwe demonstrate real-time RF fingerprinting with 0.219 ms latency using a\nsmall-scale FPGA board, the PYNQ-Z1.",
          "link": "http://arxiv.org/abs/2106.13865",
          "publishedOn": "2021-06-29T01:55:16.064Z",
          "wordCount": 658,
          "title": "A Photonic-Circuits-Inspired Compact Network: Toward Real-Time Wireless Signal Classification at the Edge. (arXiv:2106.13865v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walton_N/0/1/0/all/0/1\">Neil Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kuang Xu</a>",
          "description": "We review the role of information and learning in the stability and\noptimization of queueing systems. In recent years, techniques from supervised\nlearning, bandit learning and reinforcement learning have been applied to\nqueueing systems supported by increasing role of information in decision\nmaking. We present observations and new results that help rationalize the\napplication of these areas to queueing systems.\n\nWe prove that the MaxWeight and BackPressure policies are an application of\nBlackwell's Approachability Theorem. This connects queueing theoretic results\nwith adversarial learning. We then discuss the requirements of statistical\nlearning for service parameter estimation. As an example, we show how queue\nsize regret can be bounded when applying a perceptron algorithm to classify\nservice. Next, we discuss the role of state information in improved decision\nmaking. Here we contrast the roles of epistemic information (information on\nuncertain parameters) and aleatoric information (information on an uncertain\nstate). Finally we review recent advances in the theory of reinforcement\nlearning and queueing, as well as, provide discussion on current research\nchallenges.",
          "link": "http://arxiv.org/abs/2105.08769",
          "publishedOn": "2021-06-29T01:55:16.051Z",
          "wordCount": 635,
          "title": "Learning and Information in Stochastic Networks and Queues. (arXiv:2105.08769v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tack_J/0/1/0/all/0/1\">Jihoon Tack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sihyun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jongheon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Adversarial training (AT) is currently one of the most successful methods to\nobtain the adversarial robustness of deep neural networks. However, the\nphenomenon of robust overfitting, i.e., the robustness starts to decrease\nsignificantly during AT, has been problematic, not only making practitioners\nconsider a bag of tricks for a successful training, e.g., early stopping, but\nalso incurring a significant generalization gap in the robustness. In this\npaper, we propose an effective regularization technique that prevents robust\noverfitting by optimizing an auxiliary 'consistency' regularization loss during\nAT. Specifically, it forces the predictive distributions after attacking from\ntwo different augmentations of the same instance to be similar with each other.\nOur experimental results demonstrate that such a simple regularization\ntechnique brings significant improvements in the test robust accuracy of a wide\nrange of AT methods. More remarkably, we also show that our method could\nsignificantly help the model to generalize its robustness against unseen\nadversaries, e.g., other types or larger perturbations compared to those used\nduring training. Code is available at\nhttps://github.com/alinlab/consistency-adversarial.",
          "link": "http://arxiv.org/abs/2103.04623",
          "publishedOn": "2021-06-29T01:55:16.022Z",
          "wordCount": 637,
          "title": "Consistency Regularization for Adversarial Robustness. (arXiv:2103.04623v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oberst_M/0/1/0/all/0/1\">Michael Oberst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thams_N/0/1/0/all/0/1\">Nikolaj Thams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1\">Jonas Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>",
          "description": "We propose a method for learning linear models whose predictive performance\nis robust to causal interventions on unobserved variables, when noisy proxies\nof those variables are available. Our approach takes the form of a\nregularization term that trades off between in-distribution performance and\nrobustness to interventions. Under the assumption of a linear structural causal\nmodel, we show that a single proxy can be used to create estimators that are\nprediction optimal under interventions of bounded strength. This strength\ndepends on the magnitude of the measurement noise in the proxy, which is, in\ngeneral, not identifiable. In the case of two proxy variables, we propose a\nmodified estimator that is prediction optimal under interventions up to a known\nstrength. We further show how to extend these estimators to scenarios where\nadditional information about the \"test time\" intervention is available during\ntraining. We evaluate our theoretical findings in synthetic experiments and\nusing real data of hourly pollution levels across several cities in China.",
          "link": "http://arxiv.org/abs/2103.02477",
          "publishedOn": "2021-06-29T01:55:16.003Z",
          "wordCount": 628,
          "title": "Regularizing towards Causal Invariance: Linear Models with Proxies. (arXiv:2103.02477v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dimakopoulou_M/0/1/0/all/0/1\">Maria Dimakopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhimei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengyuan Zhou</a>",
          "description": "During online decision making in Multi-Armed Bandits (MAB), one needs to\nconduct inference on the true mean reward of each arm based on data collected\nso far at each step. However, since the arms are adaptively selected--thereby\nyielding non-iid data--conducting inference accurately is not straightforward.\nIn particular, sample averaging, which is used in the family of UCB and\nThompson sampling (TS) algorithms, does not provide a good choice as it suffers\nfrom bias and a lack of good statistical properties (e.g. asymptotic\nnormality). Our thesis in this paper is that more sophisticated inference\nschemes that take into account the adaptive nature of the sequentially\ncollected data can unlock further performance gains, even though both UCB and\nTS type algorithms are optimal in the worst case. In particular, we propose a\nvariant of TS-style algorithms--which we call doubly adaptive TS--that\nleverages recent advances in causal inference and adaptively reweights the\nterms of a doubly robust estimator on the true mean reward of each arm. Through\n20 synthetic domain experiments and a semi-synthetic experiment based on data\nfrom an A/B test of a web service, we demonstrate that using an adaptive\ninferential scheme (while still retaining the exploration efficacy of TS)\nprovides clear benefits in online decision making: the proposed DATS algorithm\nhas superior empirical performance to existing baselines (UCB and TS) in terms\nof regret and sample complexity in identifying the best arm. In addition, we\nalso provide a finite-time regret bound of doubly adaptive TS that matches (up\nto log factors) those of UCB and TS algorithms, thereby establishing that its\nimproved practical benefits do not come at the expense of worst-case\nsuboptimality.",
          "link": "http://arxiv.org/abs/2102.13202",
          "publishedOn": "2021-06-29T01:55:15.996Z",
          "wordCount": 733,
          "title": "Online Multi-Armed Bandits with Adaptive Inference. (arXiv:2102.13202v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yuning You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Self-supervised learning on graph-structured data has drawn recent interest\nfor learning generalizable, transferable and robust representations from\nunlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged\nwith promising representation learning performance. Unfortunately, unlike its\ncounterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data\naugmentations, which have to be manually picked per dataset, by either rules of\nthumb or trial-and-errors, owing to the diverse nature of graph data. That\nsignificantly limits the more general applicability of GraphCL. Aiming to fill\nin this crucial gap, this paper proposes a unified bi-level optimization\nframework to automatically, adaptively and dynamically select data\naugmentations when performing GraphCL on specific graph data. The general\nframework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as\nmin-max optimization. The selections of augmentations made by JOAO are shown to\nbe in general aligned with previous \"best practices\" observed from handcrafted\ntuning: yet now being automated, more flexible and versatile. Moreover, we\npropose a new augmentation-aware projection head mechanism, which will route\noutput features through different projection heads corresponding to different\naugmentations chosen at each training step. Extensive experiments demonstrate\nthat JOAO performs on par with or sometimes better than the state-of-the-art\ncompetitors including GraphCL, on multiple graph datasets of various scales and\ntypes, yet without resorting to any laborious dataset-specific tuning on\naugmentation selection. We release the code at\nhttps://github.com/Shen-Lab/GraphCL_Automated.",
          "link": "http://arxiv.org/abs/2106.07594",
          "publishedOn": "2021-06-29T01:55:15.970Z",
          "wordCount": 676,
          "title": "Graph Contrastive Learning Automated. (arXiv:2106.07594v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02876",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Schell_A/0/1/0/all/0/1\">Alexander Schell</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oberhauser_H/0/1/0/all/0/1\">Harald Oberhauser</a>",
          "description": "We study the classical problem of recovering a multidimensional source\nprocess from observations of nonlinear mixtures of this process. Assuming\nstatistical independence of the coordinate processes of the source, we show\nthat this recovery is possible for many popular models of stochastic processes\n(up to order and monotone scaling of their coordinates) if the mixture is given\nby a sufficiently differentiable, invertible function. Key to our approach is\nthe combination of tools from stochastic analysis and recent contrastive\nlearning approaches to nonlinear ICA. This yields a scalable method with widely\napplicable theoretical guarantees for which our experiments indicate good\nperformance.",
          "link": "http://arxiv.org/abs/2102.02876",
          "publishedOn": "2021-06-29T01:55:15.939Z",
          "wordCount": 561,
          "title": "Nonlinear Independent Component Analysis for Continuous-Time Signals. (arXiv:2102.02876v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1\">Sai Rajeswar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_C/0/1/0/all/0/1\">Cyril Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1\">Nitin Surya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1\">Florian Golemo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinheiro_P/0/1/0/all/0/1\">Pedro O. Pinheiro</a>",
          "description": "Robots in many real-world settings have access to force/torque sensors in\ntheir gripper and tactile sensing is often necessary in tasks that involve\ncontact-rich motion. In this work, we leverage surprise from mismatches in\ntouch feedback to guide exploration in hard sparse-reward reinforcement\nlearning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible\nobjects interactions are supposed to \"feel\" like. We encourage exploration by\nrewarding interactions where the expectation and the experience don't match. In\nour proposed method, an initial task-independent exploration phase is followed\nby an on-task learning phase, in which the original interactions are relabeled\nwith on-task rewards. We test our approach on a range of touch-intensive robot\narm tasks (e.g. pushing objects, opening doors), which we also release as part\nof this work. Across multiple experiments in a simulated setting, we\ndemonstrate that our method is able to learn these difficult tasks through\nsparse reward and curiosity alone. We compare our cross-modal approach to\nsingle-modality (touch- or vision-only) approaches as well as other\ncuriosity-based methods and find that our method performs better and is more\nsample-efficient.",
          "link": "http://arxiv.org/abs/2104.00442",
          "publishedOn": "2021-06-29T01:55:15.934Z",
          "wordCount": 655,
          "title": "Touch-based Curiosity for Sparse-Reward Tasks. (arXiv:2104.00442v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mengying Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guizhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuanchao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinliang Wu</a>",
          "description": "In representation learning on the graph-structured data, under heterophily\n(or low homophily), many popular GNNs may fail to capture long-range\ndependencies, which leads to their performance degradation. To solve the\nabove-mentioned issue, we propose a graph convolutional networks with structure\nlearning (GCN-SL), and furthermore, the proposed approach can be applied to\nnode classification. The proposed GCN-SL contains two improvements:\ncorresponding to node features and edges, respectively. In the aspect of node\nfeatures, we propose an efficient-spectral-clustering (ESC) and an ESC with\nanchors (ESC-ANCH) algorithms to efficiently aggregate feature representations\nfrom all similar nodes. In the aspect of edges, we build a re-connected\nadjacency matrix by using a special data preprocessing technique and similarity\nlearning, and the re-connected adjacency matrix can be optimized directly along\nwith GCN-SL parameters. Considering that the original adjacency matrix may\nprovide misleading information for aggregation in GCN, especially the graphs\nbeing with a low level of homophily. The proposed GCN-SL can aggregate feature\nrepresentations from nearby nodes via re-connected adjacency matrix and is\napplied to graphs with various levels of homophily. Experimental results on a\nwide range of benchmark datasets illustrate that the proposed GCN-SL\noutperforms the stateof-the-art GNN counterparts.",
          "link": "http://arxiv.org/abs/2105.13795",
          "publishedOn": "2021-06-29T01:55:15.919Z",
          "wordCount": 657,
          "title": "GCN-SL: Graph Convolutional Networks with Structure Learning for Graphs under Heterophily. (arXiv:2105.13795v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rajesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">WenYong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_J/0/1/0/all/0/1\">Jay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakria/0/1/0/all/0/1\">Zakria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Ting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_W/0/1/0/all/0/1\">Waqar Ali</a>",
          "description": "The widespread significance of Android IoT devices is due to its flexibility\nand hardware support features which revolutionized the digital world by\nintroducing exciting applications almost in all walks of daily life, such as\nhealthcare, smart cities, smart environments, safety, remote sensing, and many\nmore. Such versatile applicability gives incentive for more malware attacks. In\nthis paper, we propose a framework which continuously aggregates multiple user\ntrained models on non-overlapping data into single model. Specifically for\nmalware detection task, (i) we propose a novel user (local) neural network\n(LNN) which trains on local distribution and (ii) then to assure the model\nauthenticity and quality, we propose a novel smart contract which enable\naggregation process over blokchain platform. The LNN model analyzes various\nstatic and dynamic features of both malware and benign whereas the smart\ncontract verifies the malicious applications both for uploading and downloading\nprocesses in the network using stored aggregated features of local models. In\nthis way, the proposed model not only improves malware detection accuracy using\ndecentralized model network but also model efficacy with blockchain. We\nevaluate our approach with three state-of-the-art models and performed deep\nanalyses of extracted features of the relative model.",
          "link": "http://arxiv.org/abs/2102.13376",
          "publishedOn": "2021-06-29T01:55:15.913Z",
          "wordCount": 671,
          "title": "Collective Intelligence: Decentralized Learning for Android Malware Detection in IoT with Blockchain. (arXiv:2102.13376v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoneyama_R/0/1/0/all/0/1\">Reo Yoneyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Chiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "We propose a unified approach to data-driven source-filter modeling using a\nsingle neural network for developing a neural vocoder capable of generating\nhigh-quality synthetic speech waveforms while retaining flexibility of the\nsource-filter model to control their voice characteristics. Our proposed\nnetwork called unified source-filter generative adversarial networks (uSFGAN)\nis developed by factorizing quasi-periodic parallel WaveGAN (QPPWG), one of the\nneural vocoders based on a single neural network, into a source excitation\ngeneration network and a vocal tract resonance filtering network by\nadditionally implementing a regularization loss. Moreover, inspired by neural\nsource filter (NSF), only a sinusoidal waveform is additionally used as the\nsimplest clue to generate a periodic source excitation waveform while\nminimizing the effect of approximations in the source filter model. The\nexperimental results demonstrate that uSFGAN outperforms conventional neural\nvocoders, such as QPPWG and NSF in both speech quality and pitch\ncontrollability.",
          "link": "http://arxiv.org/abs/2104.04668",
          "publishedOn": "2021-06-29T01:55:15.879Z",
          "wordCount": 632,
          "title": "Unified Source-Filter GAN: Unified Source-filter Network Based On Factorization of Quasi-Periodic Parallel WaveGAN. (arXiv:2104.04668v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaofeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Principal Component Analysis (PCA) has been widely used for dimensionality\nreduction and feature extraction. Robust PCA (RPCA), under different robust\ndistance metrics, such as l1-norm and l2, p-norm, can deal with noise or\noutliers to some extent. However, real-world data may display structures that\ncan not be fully captured by these simple functions. In addition, existing\nmethods treat complex and simple samples equally. By contrast, a learning\npattern typically adopted by human beings is to learn from simple to complex\nand less to more. Based on this principle, we propose a novel method called\nSelf-paced PCA (SPCA) to further reduce the effect of noise and outliers.\nNotably, the complexity of each sample is calculated at the beginning of each\niteration in order to integrate samples from simple to more complex into\ntraining. Based on an alternating optimization, SPCA finds an optimal\nprojection matrix and filters out outliers iteratively. Theoretical analysis is\npresented to show the rationality of SPCA. Extensive experiments on popular\ndata sets demonstrate that the proposed method can improve the state of-the-art\nresults considerably.",
          "link": "http://arxiv.org/abs/2106.13880",
          "publishedOn": "2021-06-29T01:55:15.726Z",
          "wordCount": 617,
          "title": "Self-paced Principal Component Analysis. (arXiv:2106.13880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eldele_E/0/1/0/all/0/1\">Emadeldeen Eldele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragab_M/0/1/0/all/0/1\">Mohamed Ragab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee Keong Kwoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "Learning decent representations from unlabeled time-series data with temporal\ndynamics is a very challenging task. In this paper, we propose an unsupervised\nTime-Series representation learning framework via Temporal and Contextual\nContrasting (TS-TCC), to learn time-series representation from unlabeled data.\nFirst, the raw time-series data are transformed into two different yet\ncorrelated views by using weak and strong augmentations. Second, we propose a\nnovel temporal contrasting module to learn robust temporal representations by\ndesigning a tough cross-view prediction task. Last, to further learn\ndiscriminative representations, we propose a contextual contrasting module\nbuilt upon the contexts from the temporal contrasting module. It attempts to\nmaximize the similarity among different contexts of the same sample while\nminimizing similarity among contexts of different samples. Experiments have\nbeen carried out on three real-world time-series datasets. The results manifest\nthat training a linear classifier on top of the features learned by our\nproposed TS-TCC performs comparably with the supervised training. Additionally,\nour proposed TS-TCC shows high efficiency in few-labeled data and transfer\nlearning scenarios. The code is publicly available at\nhttps://github.com/emadeldeen24/TS-TCC.",
          "link": "http://arxiv.org/abs/2106.14112",
          "publishedOn": "2021-06-29T01:55:15.720Z",
          "wordCount": 628,
          "title": "Time-Series Representation Learning via Temporal and Contextual Contrasting. (arXiv:2106.14112v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1\">Wolfgang Maass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storey_V/0/1/0/all/0/1\">Veda C. Storey</a>",
          "description": "Both conceptual modeling and machine learning have long been recognized as\nimportant areas of research. With the increasing emphasis on digitizing and\nprocessing large amounts of data for business and other applications, it would\nbe helpful to consider how these areas of research can complement each other.\nTo understand how they can be paired, we provide an overview of machine\nlearning foundations and development cycle. We then examine how conceptual\nmodeling can be applied to machine learning and propose a framework for\nincorporating conceptual modeling into data science projects. The framework is\nillustrated by applying it to a healthcare application. For the inverse\npairing, machine learning can impact conceptual modeling through text and rule\nmining, as well as knowledge graphs. The pairing of conceptual modeling and\nmachine learning in this this way should help lay the foundations for future\nresearch.",
          "link": "http://arxiv.org/abs/2106.14251",
          "publishedOn": "2021-06-29T01:55:15.703Z",
          "wordCount": 579,
          "title": "Pairing Conceptual Modeling with Machine Learning. (arXiv:2106.14251v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14465",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1\">Sk Imran Hossain</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Herve_J/0/1/0/all/0/1\">Jocelyn de Go&#xeb;r de Herve</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1\">Md Shahriar Hassan</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Martineau_D/0/1/0/all/0/1\">Delphine Martineau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petrosyan_E/0/1/0/all/0/1\">Evelina Petrosyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corbain_V/0/1/0/all/0/1\">Violaine Corbain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beytout_J/0/1/0/all/0/1\">Jean Beytout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lebert_I/0/1/0/all/0/1\">Isabelle Lebert</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Baux_E/0/1/0/all/0/1\">Elisabeth Baux</a> (CHRU Nancy), <a href=\"http://arxiv.org/find/eess/1/au:+Cazorla_C/0/1/0/all/0/1\">C&#xe9;line Cazorla</a> (CHU de Saint-Etienne), <a href=\"http://arxiv.org/find/eess/1/au:+Eldin_C/0/1/0/all/0/1\">Carole Eldin</a> (IHU M&#xe9;diterran&#xe9;e Infection), <a href=\"http://arxiv.org/find/eess/1/au:+Hansmann_Y/0/1/0/all/0/1\">Yves Hansmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patrat_Delon_S/0/1/0/all/0/1\">Solene Patrat-Delon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prazuck_T/0/1/0/all/0/1\">Thierry Prazuck</a> (CHR), <a href=\"http://arxiv.org/find/eess/1/au:+Raffetin_A/0/1/0/all/0/1\">Alice Raffetin</a> (CHIV), <a href=\"http://arxiv.org/find/eess/1/au:+Tattevin_P/0/1/0/all/0/1\">Pierre Tattevin</a> (CHU Rennes), <a href=\"http://arxiv.org/find/eess/1/au:+VourcH_G/0/1/0/all/0/1\">Gwena&#xeb;l Vourc&#x27;H</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Lesens_O/0/1/0/all/0/1\">Olivier Lesens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguifo_E/0/1/0/all/0/1\">Engelbert Nguifo</a> (LIMOS)",
          "description": "Lyme disease is one of the most common infectious vector-borne diseases in\nthe world. In the early stage, the disease manifests itself in most cases with\nerythema migrans (EM) skin lesions. Better diagnosis of these early forms would\nallow improving the prognosis by preventing the transition to a severe late\nform thanks to appropriate antibiotic therapy. Recent studies show that\nconvolutional neural networks (CNNs) perform very well to identify skin lesions\nfrom the image but, there is not much work for Lyme disease prediction from EM\nlesion images. The main objective of this study is to extensively analyze the\neffectiveness of CNNs for diagnosing Lyme disease from images and to find out\nthe best CNN architecture for the purpose. There is no publicly available EM\nimage dataset for Lyme dis",
          "link": "http://arxiv.org/abs/2106.14465",
          "publishedOn": "2021-06-29T01:55:15.690Z",
          "wordCount": 857,
          "title": "Benchmarking convolutional neural networks for diagnosing Lyme disease from images. (arXiv:2106.14465v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.",
          "link": "http://arxiv.org/abs/2106.14463",
          "publishedOn": "2021-06-29T01:55:15.682Z",
          "wordCount": 674,
          "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Comandur_B/0/1/0/all/0/1\">Bharath Comandur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kak_A/0/1/0/all/0/1\">Avinash C. Kak</a>",
          "description": "We present a novel multi-view training framework and CNN architecture for\ncombining information from multiple overlapping satellite images and noisy\ntraining labels derived from OpenStreetMap (OSM) to semantically label\nbuildings and roads across large geographic regions (100 km$^2$). Our approach\nto multi-view semantic segmentation yields a 4-7% improvement in the per-class\nIoU scores compared to the traditional approaches that use the views\nindependently of one another. A unique (and, perhaps, surprising) property of\nour system is that modifications that are added to the tail-end of the CNN for\nlearning from the multi-view data can be discarded at the time of inference\nwith a relatively small penalty in the overall performance. This implies that\nthe benefits of training using multiple views are absorbed by all the layers of\nthe network. Additionally, our approach only adds a small overhead in terms of\nthe GPU-memory consumption even when training with as many as 32 views per\nscene. The system we present is end-to-end automated, which facilitates\ncomparing the classifiers trained directly on true orthophotos vis-a-vis first\ntraining them on the off-nadir images and subsequently translating the\npredicted labels to geographical coordinates. With no human supervision, our\nIoU scores for the buildings and roads classes are 0.8 and 0.64 respectively\nwhich are better than state-of-the-art approaches that use OSM labels and that\nare not completely automated.",
          "link": "http://arxiv.org/abs/2008.10271",
          "publishedOn": "2021-06-29T01:55:15.675Z",
          "wordCount": 774,
          "title": "Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images and Noisy OSM Training Labels. (arXiv:2008.10271v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1\">Kuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>",
          "description": "The learning efficiency and generalization ability of an intelligent agent\ncan be greatly improved by utilizing a useful set of skills. However, the\ndesign of robot skills can often be intractable in real-world applications due\nto the prohibitive amount of effort and expertise that it requires. In this\nwork, we introduce Skill Learning In Diversified Environments (SLIDE), a method\nto discover generalizable skills via automated generation of a diverse set of\ntasks. As opposed to prior work on unsupervised discovery of skills which\nincentivizes the skills to produce different outcomes in the same environment,\nour method pairs each skill with a unique task produced by a trainable task\ngenerator. To encourage generalizable skills to emerge, our method trains each\nskill to specialize in the paired task and maximizes the diversity of the\ngenerated tasks. A task discriminator defined on the robot behaviors in the\ngenerated tasks is jointly trained to estimate the evidence lower bound of the\ndiversity objective. The learned skills can then be composed in a hierarchical\nreinforcement learning algorithm to solve unseen target tasks. We demonstrate\nthat the proposed method can effectively learn a variety of robot skills in two\ntabletop manipulation domains. Our results suggest that the learned skills can\neffectively improve the robot's performance in various unseen target tasks\ncompared to existing reinforcement learning and skill learning methods.",
          "link": "http://arxiv.org/abs/2106.13935",
          "publishedOn": "2021-06-29T01:55:15.661Z",
          "wordCount": 664,
          "title": "Discovering Generalizable Skills via Automated Generation of Diverse Tasks. (arXiv:2106.13935v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2011.00810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1\">Dimitris Fotakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalavasis_A/0/1/0/all/0/1\">Alkis Kalavasis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavropoulos_K/0/1/0/all/0/1\">Konstantinos Stavropoulos</a>",
          "description": "We consider the problem of learning the true ordering of a set of\nalternatives from largely incomplete and noisy rankings. We introduce a natural\ngeneralization of both the classical Mallows model of ranking distributions and\nthe extensively studied model of noisy pairwise comparisons. Our selective\nMallows model outputs a noisy ranking on any given subset of alternatives,\nbased on an underlying Mallows distribution. Assuming a sequence of subsets\nwhere each pair of alternatives appears frequently enough, we obtain strong\nasymptotically tight upper and lower bounds on the sample complexity of\nlearning the underlying complete ranking and the (identities and the) ranking\nof the top-k alternatives from selective Mallows rankings. Moreover, building\non the work of (Braverman and Mossel, 2009), we show how to efficiently compute\nthe maximum likelihood complete ranking from selective Mallows rankings.",
          "link": "http://arxiv.org/abs/2011.00810",
          "publishedOn": "2021-06-29T01:55:15.655Z",
          "wordCount": 637,
          "title": "Aggregating Incomplete and Noisy Rankings. (arXiv:2011.00810v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>",
          "description": "Emerging from low-level vision theory, steerable filters found their\ncounterpart in deep learning. Earlier works used the steering theorems and\npresented convolutional networks equivariant to rigid transformations. In our\nwork, we propose a steerable feed-forward learning-based approach that consists\nof spherical decision surfaces and operates on point clouds. Due to the\ninherent geometric 3D structure of our theory, we derive a 3D steerability\nconstraint for its atomic parts, the hypersphere neurons. Exploiting the\nrotational equivariance, we show how the model parameters are fully steerable\nat inference time. The proposed spherical filter banks enable to make\nequivariant and, after online optimization, invariant class predictions for\nknown synthetic point sets in unknown orientations.",
          "link": "http://arxiv.org/abs/2106.13863",
          "publishedOn": "2021-06-29T01:55:15.649Z",
          "wordCount": 543,
          "title": "Fully Steerable 3D Spherical Neurons. (arXiv:2106.13863v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.15421",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Saha_A/0/1/0/all/0/1\">Arkajyoti Saha</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Basu_S/0/1/0/all/0/1\">Sumanta Basu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Datta_A/0/1/0/all/0/1\">Abhirup Datta</a>",
          "description": "Random forest (RF) is one of the most popular methods for estimating\nregression functions. The local nature of the RF algorithm, based on intra-node\nmeans and variances, is ideal when errors are i.i.d. For dependent error\nprocesses like time series and spatial settings where data in all the nodes\nwill be correlated, operating locally ignores this dependence. Also, RF will\ninvolve resampling of correlated data, violating the principles of bootstrap.\nTheoretically, consistency of RF has been established for i.i.d. errors, but\nlittle is known about the case of dependent errors.\n\nWe propose RF-GLS, a novel extension of RF for dependent error processes in\nthe same way Generalized Least Squares (GLS) fundamentally extends Ordinary\nLeast Squares (OLS) for linear models under dependence. The key to this\nextension is the equivalent representation of the local decision-making in a\nregression tree as a global OLS optimization which is then replaced with a GLS\nloss to create a GLS-style regression tree. This also synergistically addresses\nthe resampling issue, as the use of GLS loss amounts to resampling uncorrelated\ncontrasts (pre-whitened data) instead of the correlated data. For spatial\nsettings, RF-GLS can be used in conjunction with Gaussian Process correlated\nerrors to generate kriging predictions at new locations. RF becomes a special\ncase of RF-GLS with an identity working covariance matrix.\n\nWe establish consistency of RF-GLS under beta- (absolutely regular) mixing\nerror processes and show that this general result subsumes important cases like\nautoregressive time series and spatial Matern Gaussian Processes. As a\nbyproduct, we also establish consistency of RF for beta-mixing processes, which\nto our knowledge, is the first such result for RF under dependence.\n\nWe empirically demonstrate the improvement achieved by RF-GLS over RF for\nboth estimation and prediction under dependence.",
          "link": "http://arxiv.org/abs/2007.15421",
          "publishedOn": "2021-06-29T01:55:15.643Z",
          "wordCount": 740,
          "title": "Random Forests for dependent data. (arXiv:2007.15421v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.02373",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1\">Yanbin Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yamada_M/0/1/0/all/0/1\">Makoto Yamada</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tsai_Y/0/1/0/all/0/1\">Yao-Hung Hubert Tsai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Le_T/0/1/0/all/0/1\">Tam Le</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Estimating mutual information is an important statistics and machine learning\nproblem. To estimate the mutual information from data, a common practice is\npreparing a set of paired samples $\\{(\\mathbf{x}_i,\\mathbf{y}_i)\\}_{i=1}^n\n\\stackrel{\\mathrm{i.i.d.}}{\\sim} p(\\mathbf{x},\\mathbf{y})$. However, in many\nsituations, it is difficult to obtain a large number of data pairs. To address\nthis problem, we propose the semi-supervised Squared-loss Mutual Information\n(SMI) estimation method using a small number of paired samples and the\navailable unpaired ones. We first represent SMI through the density ratio\nfunction, where the expectation is approximated by the samples from marginals\nand its assignment parameters. The objective is formulated using the optimal\ntransport problem and quadratic programming. Then, we introduce the\nLeast-Squares Mutual Information with Sinkhorn (LSMI-Sinkhorn) algorithm for\nefficient optimization. Through experiments, we first demonstrate that the\nproposed method can estimate the SMI without a large number of paired samples.\nThen, we show the effectiveness of the proposed LSMI-Sinkhorn algorithm on\nvarious types of machine learning problems such as image matching and photo\nalbum summarization. Code can be found at\nhttps://github.com/csyanbin/LSMI-Sinkhorn.",
          "link": "http://arxiv.org/abs/1909.02373",
          "publishedOn": "2021-06-29T01:55:15.637Z",
          "wordCount": 642,
          "title": "LSMI-Sinkhorn: Semi-supervised Mutual Information Estimation with Optimal Transport. (arXiv:1909.02373v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ariu_K/0/1/0/all/0/1\">Kaito Ariu</a>",
          "description": "We study the best-arm identification problem with fixed confidence when\ncontextual (covariate) information is available in stochastic bandits. Although\nwe can use contextual information in each round, we are interested in the\nmarginalized mean reward over the contextual distribution. Our goal is to\nidentify the best arm with a minimal number of samplings under a given value of\nthe error rate. We show the instance-specific sample complexity lower bounds\nfor the problem. Then, we propose a context-aware version of the\n\"Track-and-Stop\" strategy, wherein the proportion of the arm draws tracks the\nset of optimal allocations and prove that the expected number of arm draws\nmatches the lower bound asymptotically. We demonstrate that contextual\ninformation can be used to improve the efficiency of the identification of the\nbest marginalized mean reward compared with the results of Garivier & Kaufmann\n(2016). We experimentally confirm that context information contributes to\nfaster best-arm identification.",
          "link": "http://arxiv.org/abs/2106.14077",
          "publishedOn": "2021-06-29T01:55:15.620Z",
          "wordCount": 596,
          "title": "The Role of Contextual Information in Best Arm Identification. (arXiv:2106.14077v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Ye Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_V/0/1/0/all/0/1\">Vincent Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Songfu Cai</a>",
          "description": "Sparse coding is a class of unsupervised methods for learning a sparse\nrepresentation of the input data in the form of a linear combination of a\ndictionary and a sparse code. This learning framework has led to\nstate-of-the-art results in various image and video processing tasks. However,\nclassical methods learn the dictionary and the sparse code based on alternative\noptimizations, usually without theoretical guarantees for either optimality or\nconvergence due to non-convexity of the problem. Recent works on sparse coding\nwith a complete dictionary provide strong theoretical guarantees thanks to the\ndevelopment of the non-convex optimization. However, initial non-convex\napproaches learn the dictionary in the sparse coding problem sequentially in an\natom-by-atom manner, which leads to a long execution time. More recent works\nseek to directly learn the entire dictionary at once, which substantially\nreduces the execution time. However, the associated recovery performance is\ndegraded with a finite number of data samples. In this paper, we propose an\nefficient sparse coding scheme with a two-stage optimization. The proposed\nscheme leverages the global and local Riemannian geometry of the two-stage\noptimization problem and facilitates fast implementation for superb dictionary\nrecovery performance by a finite number of samples without atom-by-atom\ncalculation. We further prove that, with high probability, the proposed scheme\ncan exactly recover any atom in the target dictionary with a finite number of\nsamples if it is adopted to recover one atom of the dictionary. An application\non wireless sensor data compression is also proposed. Experiments on both\nsynthetic and real-world data verify the efficiency and effectiveness of the\nproposed scheme.",
          "link": "http://arxiv.org/abs/2104.10314",
          "publishedOn": "2021-06-29T01:55:15.614Z",
          "wordCount": 735,
          "title": "Efficient Sparse Coding using Hierarchical Riemannian Pursuit. (arXiv:2104.10314v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1\">Mathias Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1\">Lucas Liebenwein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschaikowski_M/0/1/0/all/0/1\">Max Tschaikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teschl_G/0/1/0/all/0/1\">Gerald Teschl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "Continuous-depth neural models, where the derivative of the model's hidden\nstate is defined by a neural network, have enabled strong sequential data\nprocessing capabilities. However, these models rely on advanced numerical\ndifferential equation (DE) solvers resulting in a significant overhead both in\nterms of computational cost and model complexity. In this paper, we present a\nnew family of models, termed Closed-form Continuous-depth (CfC) networks, that\nare simple to describe and at least one order of magnitude faster while\nexhibiting equally strong modeling abilities compared to their ODE-based\ncounterparts. The models are hereby derived from the analytical closed-form\nsolution of an expressive subset of time-continuous models, thus alleviating\nthe need for complex DE solvers all together. In our experimental evaluations,\nwe demonstrate that CfC networks outperform advanced, recurrent models over a\ndiverse set of time-series prediction tasks, including those with long-term\ndependencies and irregularly sampled data. We believe our findings open new\nopportunities to train and deploy rich, continuous neural models in\nresource-constrained settings, which demand both performance and efficiency.",
          "link": "http://arxiv.org/abs/2106.13898",
          "publishedOn": "2021-06-29T01:55:15.608Z",
          "wordCount": 615,
          "title": "Closed-form Continuous-Depth Models. (arXiv:2106.13898v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14122",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1\">Lang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salmon_J/0/1/0/all/0/1\">Joseph Salmon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "The widespread use of machine learning algorithms calls for automatic change\ndetection algorithms to monitor their behavior over time. As a machine learning\nalgorithm learns from a continuous, possibly evolving, stream of data, it is\ndesirable and often critical to supplement it with a companion change detection\nalgorithm to facilitate its monitoring and control. We present a generic\nscore-based change detection method that can detect a change in any number of\ncomponents of a machine learning model trained via empirical risk minimization.\nThis proposed statistical hypothesis test can be readily implemented for such\nmodels designed within a differentiable programming framework. We establish the\nconsistency of the hypothesis test and show how to calibrate it to achieve a\nprescribed false alarm rate. We illustrate the versatility of the approach on\nsynthetic and real data.",
          "link": "http://arxiv.org/abs/2106.14122",
          "publishedOn": "2021-06-29T01:55:15.603Z",
          "wordCount": 563,
          "title": "Score-Based Change Detection for Gradient-Based Learning Machines. (arXiv:2106.14122v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Le Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jingyu Xin</a>",
          "description": "With rapidly evolving internet technologies and emerging tools, sports\nrelated videos generated online are increasing at an unprecedentedly fast pace.\nTo automate sports video editing/highlight generation process, a key task is to\nprecisely recognize and locate the events in the long untrimmed videos. In this\ntech report, we present a two-stage paradigm to detect what and when events\nhappen in soccer broadcast videos. Specifically, we fine-tune multiple action\nrecognition models on soccer data to extract high-level semantic features, and\ndesign a transformer based temporal detection module to locate the target\nevents. This approach achieved the state-of-the-art performance in both two\ntasks, i.e., action spotting and replay grounding, in the SoccerNet-v2\nChallenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features\nare released at https://github.com/baidu-research/vidpress-sports. By sharing\nthese features with the broader community, we hope to accelerate the research\ninto soccer video understanding.",
          "link": "http://arxiv.org/abs/2106.14447",
          "publishedOn": "2021-06-29T01:55:15.597Z",
          "wordCount": 611,
          "title": "Feature Combination Meets Attention: Baidu Soccer Embeddings and Transformer based Temporal Detection. (arXiv:2106.14447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14324",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Weimin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhadra_S/0/1/0/all/0/1\">Sayantan Bhadra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brooks_F/0/1/0/all/0/1\">Frank J. Brooks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>",
          "description": "In order to objectively assess new medical imaging technologies via\ncomputer-simulations, it is important to account for all sources of variability\nthat contribute to image data. One important source of variability that can\nsignificantly limit observer performance is associated with the variability in\nthe ensemble of objects to-be-imaged. This source of variability can be\ndescribed by stochastic object models (SOMs), which are generative models that\ncan be employed to sample from a distribution of to-be-virtually-imaged\nobjects. It is generally desirable to establish SOMs from experimental imaging\nmeasurements acquired by use of a well-characterized imaging system, but this\ntask has remained challenging. Deep generative neural networks, such as\ngenerative adversarial networks (GANs) hold potential for such tasks. To\nestablish SOMs from imaging measurements, an AmbientGAN has been proposed that\naugments a GAN with a measurement operator. However, the original AmbientGAN\ncould not immediately benefit from modern training procedures and GAN\narchitectures, which limited its ability to be applied to realistically sized\nmedical image data. To circumvent this, in this work, a modified AmbientGAN\ntraining strategy is proposed that is suitable for modern progressive or\nmulti-resolution training approaches such as employed in the Progressive\nGrowing of GANs and Style-based GANs. AmbientGANs established by use of the\nproposed training procedure are systematically validated in a controlled way by\nuse of computer-simulated measurement data corresponding to a stylized imaging\nsystem. Finally, emulated single-coil experimental magnetic resonance imaging\ndata are employed to demonstrate the methods under less stylized conditions.",
          "link": "http://arxiv.org/abs/2106.14324",
          "publishedOn": "2021-06-29T01:55:15.581Z",
          "wordCount": 728,
          "title": "Learning stochastic object models from medical imaging measurements by use of advanced AmbientGANs. (arXiv:2106.14324v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muravev_N/0/1/0/all/0/1\">Nikita Muravev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1\">Aleksandr Petiushko</a>",
          "description": "We propose a novel approach of randomized smoothing over multiplicative\nparameters. Using this method we construct certifiably robust classifiers with\nrespect to a gamma-correction perturbation and compare the result with\nclassifiers obtained via Gaussian smoothing. To the best of our knowledge it is\nthe first work concerning certified robustness against the multiplicative\ngamma-correction transformation.",
          "link": "http://arxiv.org/abs/2106.14432",
          "publishedOn": "2021-06-29T01:55:15.575Z",
          "wordCount": 480,
          "title": "Certified Robustness via Randomized Smoothing over Multiplicative Parameters. (arXiv:2106.14432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14178",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quanziang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Renzhen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "Location information is proven to benefit the deep learning models on\ncapturing the manifold structure of target objects, and accordingly boosts the\naccuracy of medical image segmentation. However, most existing methods encode\nthe location information in an implicit way, e.g. the distance transform maps,\nwhich describe the relative distance from each pixel to the contour boundary,\nfor the network to learn. These implicit approaches do not fully exploit the\nposition information (i.e. absolute location) of targets. In this paper, we\npropose a novel loss function, namely residual moment (RM) loss, to explicitly\nembed the location information of segmentation targets during the training of\ndeep learning networks. Particularly, motivated by image moments, the\nsegmentation prediction map and ground-truth map are weighted by coordinate\ninformation. Then our RM loss encourages the networks to maintain the\nconsistency between the two weighted maps, which promotes the segmentation\nnetworks to easily locate the targets and extract manifold-structure-related\nfeatures. We validate the proposed RM loss by conducting extensive experiments\non two publicly available datasets, i.e., 2D optic cup and disk segmentation\nand 3D left atrial segmentation. The experimental results demonstrate the\neffectiveness of our RM loss, which significantly boosts the accuracy of\nsegmentation networks.",
          "link": "http://arxiv.org/abs/2106.14178",
          "publishedOn": "2021-06-29T01:55:15.569Z",
          "wordCount": 648,
          "title": "Residual Moment Loss for Medical Image Segmentation. (arXiv:2106.14178v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.05505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Youngduck Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngnam Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jineon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Dongmin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hangyeol Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_Y/0/1/0/all/0/1\">Yugeun Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seewoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jonghun Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_C/0/1/0/all/0/1\">Chan Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byungsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1\">Jaewe Heo</a>",
          "description": "Like many other domains in Artificial Intelligence (AI), there are specific\ntasks in the field of AI in Education (AIEd) for which labels are scarce and\nexpensive, such as predicting exam score or review correctness. A common way of\ncircumventing label-scarce problems is pre-training a model to learn\nrepresentations of the contents of learning items. However, such methods fail\nto utilize the full range of student interaction data available and do not\nmodel student learning behavior. To this end, we propose Assessment Modeling, a\nclass of fundamental pre-training tasks for general interactive educational\nsystems. An assessment is a feature of student-system interactions which can\nserve as a pedagogical evaluation. Examples include the correctness and\ntimeliness of a student's answer. Assessment Modeling is the prediction of\nassessments conditioned on the surrounding context of interactions. Although it\nis natural to pre-train on interactive features available in large amounts,\nlimiting the prediction targets to assessments focuses the tasks' relevance to\nthe label-scarce educational problems and reduces less-relevant noise. While\nthe effectiveness of different combinations of assessments is open for\nexploration, we suggest Assessment Modeling as a first-order guiding principle\nfor selecting proper pre-training tasks for label-scarce educational problems.",
          "link": "http://arxiv.org/abs/2002.05505",
          "publishedOn": "2021-06-29T01:55:15.563Z",
          "wordCount": 717,
          "title": "Assessment Modeling: Fundamental Pre-training Tasks for Interactive Educational Systems. (arXiv:2002.05505v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>",
          "description": "Ensemble methods are generally regarded to be better than a single model if\nthe base learners are deemed to be \"accurate\" and \"diverse.\" Here we\ninvestigate a semi-supervised ensemble learning strategy to produce\ngeneralizable blind image quality assessment models. We train a multi-head\nconvolutional network for quality prediction by maximizing the accuracy of the\nensemble (as well as the base learners) on labeled data, and the disagreement\n(i.e., diversity) among them on unlabeled data, both implemented by the\nfidelity loss. We conduct extensive experiments to demonstrate the advantages\nof employing unlabeled data for BIQA, especially in model generalization and\nfailure identification.",
          "link": "http://arxiv.org/abs/2106.14008",
          "publishedOn": "2021-06-29T01:55:15.557Z",
          "wordCount": 552,
          "title": "Semi-Supervised Deep Ensembles for Blind Image Quality Assessment. (arXiv:2106.14008v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11037",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cai_H/0/1/0/all/0/1\">HanQin Cai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hamm_K/0/1/0/all/0/1\">Keaton Hamm</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_L/0/1/0/all/0/1\">Longxiu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Needell_D/0/1/0/all/0/1\">Deanna Needell</a>",
          "description": "Low rank tensor approximation is a fundamental tool in modern machine\nlearning and data science. In this paper, we study the characterization,\nperturbation analysis, and an efficient sampling strategy for two primary\ntensor CUR approximations, namely Chidori and Fiber CUR. We characterize exact\ntensor CUR decompositions for low multilinear rank tensors. We also present\ntheoretical error bounds of the tensor CUR approximations when (adversarial or\nGaussian) noise appears. Moreover, we show that low cost uniform sampling is\nsufficient for tensor CUR approximations if the tensor has an incoherent\nstructure. Empirical performance evaluations, with both synthetic and\nreal-world datasets, establish the speed advantage of the tensor CUR\napproximations over other state-of-the-art low multilinear rank tensor\napproximations.",
          "link": "http://arxiv.org/abs/2103.11037",
          "publishedOn": "2021-06-29T01:55:15.540Z",
          "wordCount": 590,
          "title": "Mode-wise Tensor Decompositions: Multi-dimensional Generalizations of CUR Decompositions. (arXiv:2103.11037v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1\">Andrea Cossu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1\">Claudio Gallicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>",
          "description": "Continual Learning (CL) refers to a learning setup where data is non\nstationary and the model has to learn without forgetting existing knowledge.\nThe study of CL for sequential patterns revolves around trained recurrent\nnetworks. In this work, instead, we introduce CL in the context of Echo State\nNetworks (ESNs), where the recurrent component is kept fixed. We provide the\nfirst evaluation of catastrophic forgetting in ESNs and we highlight the\nbenefits in using CL strategies which are not applicable to trained recurrent\nmodels. Our results confirm the ESN as a promising model for CL and open to its\nuse in streaming scenarios.",
          "link": "http://arxiv.org/abs/2105.07674",
          "publishedOn": "2021-06-29T01:55:15.533Z",
          "wordCount": 572,
          "title": "Continual Learning with Echo State Networks. (arXiv:2105.07674v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolpert_D/0/1/0/all/0/1\">David H. Wolpert</a>",
          "description": "The important recent book by G. Schurz appreciates that the no-free-lunch\ntheorems (NFL) have major implications for the problem of (meta) induction.\nHere I review the NFL theorems, emphasizing that they do not only concern the\ncase where there is a uniform prior -- they prove that there are \"as many\npriors\" (loosely speaking) for which any induction algorithm $A$\nout-generalizes some induction algorithm $B$ as vice-versa. Importantly though,\nin addition to the NFL theorems, there are many \\textit{free lunch} theorems.\nIn particular, the NFL theorems can only be used to compare the\n\\textit{marginal} expected performance of an induction algorithm $A$ with the\nmarginal expected performance of an induction algorithm $B$. There is a rich\nset of free lunches which instead concern the statistical correlations among\nthe generalization errors of induction algorithms. As I describe, the\nmeta-induction algorithms that Schurz advocate as a \"solution to Hume's\nproblem\" are just an example of such a free lunch based on correlations among\nthe generalization errors of induction algorithms. I end by pointing out that\nthe prior that Schurz advocates, which is uniform over bit frequencies rather\nthan bit patterns, is contradicted by thousands of experiments in statistical\nphysics and by the great success of the maximum entropy procedure in inductive\ninference.",
          "link": "http://arxiv.org/abs/2103.11956",
          "publishedOn": "2021-06-29T01:55:15.528Z",
          "wordCount": 664,
          "title": "The Implications of the No-Free-Lunch Theorems for Meta-induction. (arXiv:2103.11956v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1\">Eli Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianhao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovic</a>",
          "description": "In many important graph data processing applications the acquired information\nincludes both node features and observations of the graph topology. Graph\nneural networks (GNNs) are designed to exploit both sources of evidence but\nthey do not optimally trade-off their utility and integrate them in a manner\nthat is also universal. Here, universality refers to independence on homophily\nor heterophily graph assumptions. We address these issues by introducing a new\nGeneralized PageRank (GPR) GNN architecture that adaptively learns the GPR\nweights so as to jointly optimize node feature and topological information\nextraction, regardless of the extent to which the node labels are homophilic or\nheterophilic. Learned GPR weights automatically adjust to the node label\npattern, irrelevant on the type of initialization, and thereby guarantee\nexcellent learning performance for label patterns that are usually hard to\nhandle. Furthermore, they allow one to avoid feature over-smoothing, a process\nwhich renders feature information nondiscriminative, without requiring the\nnetwork to be shallow. Our accompanying theoretical analysis of the GPR-GNN\nmethod is facilitated by novel synthetic benchmark datasets generated by the\nso-called contextual stochastic block model. We also compare the performance of\nour GNN architecture with that of several state-of-the-art GNNs on the problem\nof node-classification, using well-known benchmark homophilic and heterophilic\ndatasets. The results demonstrate that GPR-GNN offers significant performance\nimprovement compared to existing techniques on both synthetic and benchmark\ndata.",
          "link": "http://arxiv.org/abs/2006.07988",
          "publishedOn": "2021-06-29T01:55:15.522Z",
          "wordCount": 716,
          "title": "Adaptive Universal Generalized PageRank Graph Neural Network. (arXiv:2006.07988v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12909",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Narita_Y/0/1/0/all/0/1\">Yusuke Narita</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Yata_K/0/1/0/all/0/1\">Kohei Yata</a>",
          "description": "Algorithms produce a growing portion of decisions and recommendations both in\npolicy and business. Such algorithmic decisions are natural experiments\n(conditionally quasi-randomly assigned instruments) since the algorithms make\ndecisions based only on observable input variables. We use this observation to\ndevelop a treatment-effect estimator for a class of stochastic and\ndeterministic decision-making algorithms. Our estimator is shown to be\nconsistent and asymptotically normal for well-defined causal effects. A key\nspecial case of our estimator is a multidimensional regression discontinuity\ndesign. We apply our estimator to evaluate the effect of the Coronavirus Aid,\nRelief, and Economic Security (CARES) Act, where more than \\$175 billion worth\nof relief funding is allocated to hospitals via an algorithmic rule. Our\nestimates suggest that the relief funding has little effect on COVID-19-related\nhospital activity levels. Naive OLS and IV estimates exhibit substantial\nselection bias.",
          "link": "http://arxiv.org/abs/2104.12909",
          "publishedOn": "2021-06-29T01:55:15.515Z",
          "wordCount": 642,
          "title": "Algorithm is Experiment: Machine Learning, Market Design, and Policy Eligibility Rules. (arXiv:2104.12909v2 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andresel_M/0/1/0/all/0/1\">Medina Andresel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domokos_C/0/1/0/all/0/1\">Csaba Domokos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stepanova_D/0/1/0/all/0/1\">Daria Stepanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trung-Kien Tran</a>",
          "description": "Recently, low-dimensional vector space representations of knowledge graphs\n(KGs) have been applied to find answers to conjunctive queries (CQs) over\nincomplete KGs. However, the current methods only focus on inductive reasoning,\ni.e. answering CQs by predicting facts based on patterns learned from the data,\nand lack the ability of deductive reasoning by applying external domain\nknowledge. Such (expert or commonsense) domain knowledge is an invaluable\nresource which can be used to advance machine intelligence. To address this\nshortcoming, we introduce a neural-symbolic method for ontology-mediated CQ\nanswering over incomplete KGs that operates in the embedding space. More\nspecifically, we propose various data augmentation strategies to generate\ntraining queries using query-rewriting based methods and then exploit a novel\nloss function for training the model. The experimental results demonstrate the\neffectiveness of our training strategies and the new loss function, i.e., our\nmethod significantly outperforms the baseline in the settings that require both\ninductive and deductive reasoning.",
          "link": "http://arxiv.org/abs/2106.14052",
          "publishedOn": "2021-06-29T01:55:15.493Z",
          "wordCount": 589,
          "title": "A Neural-symbolic Approach for Ontology-mediated Query Answering. (arXiv:2106.14052v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_M/0/1/0/all/0/1\">Muvazima Mansoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Srikanth Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinath_R/0/1/0/all/0/1\">Ramamoorthy Srinath</a>",
          "description": "In this paper, we propose an architecture to solve a novel problem statement\nthat has stemmed more so in recent times with an increase in demand for virtual\ncontent delivery due to the COVID-19 pandemic. All educational institutions,\nworkplaces, research centers, etc. are trying to bridge the gap of\ncommunication during these socially distanced times with the use of online\ncontent delivery. The trend now is to create presentations, and then\nsubsequently deliver the same using various virtual meeting platforms. The time\nbeing spent in such creation of presentations and delivering is what we try to\nreduce and eliminate through this paper which aims to use Machine Learning (ML)\nalgorithms and Natural Language Processing (NLP) modules to automate the\nprocess of creating a slides-based presentation from a document, and then use\nstate-of-the-art voice cloning models to deliver the content in the desired\nauthor's voice. We consider a structured document such as a research paper to\nbe the content that has to be presented. The research paper is first summarized\nusing BERT summarization techniques and condensed into bullet points that go\ninto the slides. Tacotron inspired architecture with Encoder, Synthesizer, and\na Generative Adversarial Network (GAN) based vocoder, is used to convey the\ncontents of the slides in the author's voice (or any customized voice). Almost\nall learning has now been shifted to online mode, and professionals are now\nworking from the comfort of their homes. Due to the current situation, teachers\nand professionals have shifted to presentations to help them in imparting\ninformation. In this paper, we aim to reduce the considerable amount of time\nthat is taken in creating a presentation by automating this process and\nsubsequently delivering this presentation in a customized voice, using a\ncontent delivery mechanism that can clone any voice using a short audio clip.",
          "link": "http://arxiv.org/abs/2106.14213",
          "publishedOn": "2021-06-29T01:55:15.485Z",
          "wordCount": 783,
          "title": "AI based Presentation Creator With Customized Audio Content Delivery. (arXiv:2106.14213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>",
          "description": "Label Smoothing (LS) improves model generalization through penalizing models\nfrom generating overconfident output distributions. For each training sample\nthe LS strategy smooths the one-hot encoded training signal by distributing its\ndistribution mass over the non-ground truth classes. We extend this technique\nby considering example pairs, coined PLS. PLS first creates midpoint samples by\naveraging random sample pairs and then learns a smoothing distribution during\ntraining for each of these midpoint samples, resulting in midpoints with high\nuncertainty labels for training. We empirically show that PLS significantly\noutperforms LS, achieving up to 30% of relative classification error reduction.\nWe also visualize that PLS produces very low winning softmax scores for both in\nand out of distribution samples.",
          "link": "http://arxiv.org/abs/2106.13913",
          "publishedOn": "2021-06-29T01:55:15.473Z",
          "wordCount": 566,
          "title": "Midpoint Regularization: from High Uncertainty Training to Conservative Classification. (arXiv:2106.13913v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalgaonkar_P/0/1/0/all/0/1\">Priyank Kalgaonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sharkawy_M/0/1/0/all/0/1\">Mohamed El-Sharkawy</a>",
          "description": "In this paper, we demonstrate the implementation of our ultra-efficient deep\nconvolutional neural network architecture: CondenseNeXt on NXP BlueBox, an\nautonomous driving development platform developed for self-driving vehicles. We\nshow that CondenseNeXt is remarkably efficient in terms of FLOPs, designed for\nARM-based embedded computing platforms with limited computational resources and\ncan perform image classification without the need of a CUDA enabled GPU.\nCondenseNeXt utilizes the state-of-the-art depthwise separable convolution and\nmodel compression techniques to achieve a remarkable computational efficiency.\nExtensive analyses are conducted on CIFAR-10, CIFAR-100 and ImageNet datasets\nto verify the performance of CondenseNeXt Convolutional Neural Network (CNN)\narchitecture. It achieves state-of-the-art image classification performance on\nthree benchmark datasets including CIFAR-10 (4.79% top-1 error), CIFAR-100\n(21.98% top-1 error) and ImageNet (7.91% single model, single crop top-5\nerror). CondenseNeXt achieves final trained model size improvement of 2.9+ MB\nand up to 59.98% reduction in forward FLOPs compared to CondenseNet and can\nperform image classification on ARM-Based computing platforms without needing a\nCUDA enabled GPU support, with outstanding efficiency.",
          "link": "http://arxiv.org/abs/2106.14102",
          "publishedOn": "2021-06-29T01:55:15.467Z",
          "wordCount": 620,
          "title": "Image Classification with CondenseNeXt for ARM-Based Computing Platforms. (arXiv:2106.14102v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valipour_M/0/1/0/all/0/1\">Mojtaba Valipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_B/0/1/0/all/0/1\">Bowen You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panju_M/0/1/0/all/0/1\">Maysum Panju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>",
          "description": "Symbolic regression is the task of identifying a mathematical expression that\nbest fits a provided dataset of input and output values. Due to the richness of\nthe space of mathematical expressions, symbolic regression is generally a\nchallenging problem. While conventional approaches based on genetic evolution\nalgorithms have been used for decades, deep learning-based methods are\nrelatively new and an active research area. In this work, we present\nSymbolicGPT, a novel transformer-based language model for symbolic regression.\nThis model exploits the advantages of probabilistic language models like GPT,\nincluding strength in performance and flexibility. Through comprehensive\nexperiments, we show that our model performs strongly compared to competing\nmodels with respect to the accuracy, running time, and data efficiency.",
          "link": "http://arxiv.org/abs/2106.14131",
          "publishedOn": "2021-06-29T01:55:15.451Z",
          "wordCount": 560,
          "title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression. (arXiv:2106.14131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14238",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wilson_J/0/1/0/all/0/1\">James D. Wilson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1\">Jihui Lee</a>",
          "description": "We consider the problem of interpretable network representation learning for\nsamples of network-valued data. We propose the Principal Component Analysis for\nNetworks (PCAN) algorithm to identify statistically meaningful low-dimensional\nrepresentations of a network sample via subgraph count statistics. The PCAN\nprocedure provides an interpretable framework for which one can readily\nvisualize, explore, and formulate predictive models for network samples. We\nfurthermore introduce a fast sampling-based algorithm, sPCAN, which is\nsignificantly more computationally efficient than its counterpart, but still\nenjoys advantages of interpretability. We investigate the relationship between\nthese two methods and analyze their large-sample properties under the common\nregime where the sample of networks is a collection of kernel-based random\ngraphs. We show that under this regime, the embeddings of the sPCAN method\nenjoy a central limit theorem and moreover that the population level embeddings\nof PCAN and sPCAN are equivalent. We assess PCAN's ability to visualize,\ncluster, and classify observations in network samples arising in nature,\nincluding functional connectivity network samples and dynamic networks\ndescribing the political co-voting habits of the U.S. Senate. Our analyses\nreveal that our proposed algorithm provides informative and discriminatory\nfeatures describing the networks in each sample. The PCAN and sPCAN methods\nbuild on the current literature of network representation learning and set the\nstage for a new line of research in interpretable learning on network-valued\ndata. Publicly available software for the PCAN and sPCAN methods are available\nat https://www.github.com/jihuilee/.",
          "link": "http://arxiv.org/abs/2106.14238",
          "publishedOn": "2021-06-29T01:55:15.445Z",
          "wordCount": 678,
          "title": "Interpretable Network Representation Learning with Principal Component Analysis. (arXiv:2106.14238v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1\">Sana Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Saeid Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1\">Raziyeh Zall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1\">Mohammad Reza Kangavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1\">Sara Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>",
          "description": "Multimodal sentiment analysis benefits various applications such as\nhuman-computer interaction and recommendation systems. It aims to infer the\nusers' bipolar ideas using visual, textual, and acoustic signals. Although\nresearchers affirm the association between cognitive cues and emotional\nmanifestations, most of the current multimodal approaches in sentiment analysis\ndisregard user-specific aspects. To tackle this issue, we devise a novel method\nto perform multimodal sentiment prediction using cognitive cues, such as\npersonality. Our framework constructs an adaptive tree by hierarchically\ndividing users and trains the LSTM-based submodels, utilizing an\nattention-based fusion to transfer cognitive-oriented knowledge within the\ntree. Subsequently, the framework consumes the conclusive agglomerative\nknowledge from the adaptive tree to predict final sentiments. We also devise a\ndynamic dropout method to facilitate data sharing between neighboring nodes,\nreducing data sparsity. The empirical results on real-world datasets determine\nthat our proposed model for sentiment prediction can surpass trending rivals.\nMoreover, compared to other ensemble approaches, the proposed transfer-based\nalgorithm can better utilize the latent cognitive cues and foster the\nprediction outcomes. Based on the given extrinsic and intrinsic analysis\nresults, we note that compared to other theoretical-based techniques, the\nproposed hierarchical clustering approach can better group the users within the\nadaptive tree.",
          "link": "http://arxiv.org/abs/2106.14174",
          "publishedOn": "2021-06-29T01:55:15.437Z",
          "wordCount": 664,
          "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. (arXiv:2106.14174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baptista_A/0/1/0/all/0/1\">Andr&#xe9; Baptista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baghoussi_Y/0/1/0/all/0/1\">Yassine Baghoussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1\">Carlos Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendes_Moreira_J/0/1/0/all/0/1\">Jo&#xe3;o Mendes-Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arantes_M/0/1/0/all/0/1\">Miguel Arantes</a>",
          "description": "Forecasting accuracy is reliant on the quality of available past data. Data\ndisruptions can adversely affect the quality of the generated model (e.g.\nunexpected events such as out-of-stock products when forecasting demand). We\naddress this problem by pastcasting: predicting how data should have been in\nthe past to explain the future better. We propose Pastprop-LSTM, a data-centric\nbackpropagation algorithm that assigns part of the responsibility for errors to\nthe training data and changes it accordingly. We test three variants of\nPastprop-LSTM on forecasting competition datasets, M4 and M5, plus the Numenta\nAnomaly Benchmark. Empirical evaluation indicates that the proposed method can\nimprove forecasting accuracy, especially when the prediction errors of standard\nLSTM are high. It also demonstrates the potential of the algorithm on datasets\ncontaining anomalies.",
          "link": "http://arxiv.org/abs/2106.13881",
          "publishedOn": "2021-06-29T01:55:15.430Z",
          "wordCount": 561,
          "title": "Pastprop-RNN: improved predictions of the future by correcting the past. (arXiv:2106.13881v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alimi_R/0/1/0/all/0/1\">Roger Alimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_E/0/1/0/all/0/1\">Elad Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_E/0/1/0/all/0/1\">Eyal Weiss</a>",
          "description": "Modern magnetic sensor arrays conventionally utilize state of the art low\npower magnetometers such as parallel and orthogonal fluxgates. Low power\nfluxgates tend to have large Barkhausen jumps that appear as a dc jump in the\nfluxgate output. This phenomenon deteriorates the signal fidelity and\neffectively increases the internal sensor noise. Even if sensors that are more\nprone to dc jumps can be screened during production, the conventional noise\nmeasurement does not always catch the dc jump because of its sparsity.\nMoreover, dc jumps persist in almost all the sensor cores although at a slower\nbut still intolerable rate. Even if dc jumps can be easily detected in a\nshielded environment, when deployed in presence of natural noise and clutter,\nit can be hard to positively detect them. This work fills this gap and presents\nalgorithms that distinguish dc jumps embedded in natural magnetic field data.\nTo improve robustness to noise, we developed two machine learning algorithms\nthat employ temporal and statistical physical-based features of a pre-acquired\nand well-known experimental data set. The first algorithm employs a support\nvector machine classifier, while the second is based on a neural network\narchitecture. We compare these new approaches to a more classical kernel-based\nmethod. To that purpose, the receiver operating characteristic curve is\ngenerated, which allows diagnosis ability of the different classifiers by\ncomparing their performances across various operation points. The accuracy of\nthe machine learning-based algorithms over the classic method is highly\nemphasized. In addition, high generalization and robustness of the neural\nnetwork can be concluded, based on the rapid convergence of the corresponding\nreceiver operating characteristic curves.",
          "link": "http://arxiv.org/abs/2106.14148",
          "publishedOn": "2021-06-29T01:55:15.424Z",
          "wordCount": 733,
          "title": "Machine Learning Detection Algorithm for Large Barkhausen Jumps in Cluttered Environment. (arXiv:2106.14148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>",
          "description": "In this paper, we propose a spectral-spatial graph reasoning network (SSGRN)\nfor hyperspectral image (HSI) classification. Concretely, this network contains\ntwo parts that separately named spatial graph reasoning subnetwork (SAGRN) and\nspectral graph reasoning subnetwork (SEGRN) to capture the spatial and spectral\ngraph contexts, respectively. Different from the previous approaches\nimplementing superpixel segmentation on the original image or attempting to\nobtain the category features under the guide of label image, we perform the\nsuperpixel segmentation on intermediate features of the network to adaptively\nproduce the homogeneous regions to get the effective descriptors. Then, we\nadopt a similar idea in spectral part that reasonably aggregating the channels\nto generate spectral descriptors for spectral graph contexts capturing. All\ngraph reasoning procedures in SAGRN and SEGRN are achieved through graph\nconvolution. To guarantee the global perception ability of the proposed\nmethods, all adjacent matrices in graph reasoning are obtained with the help of\nnon-local self-attention mechanism. At last, by combining the extracted spatial\nand spectral graph contexts, we obtain the SSGRN to achieve a high accuracy\nclassification. Extensive quantitative and qualitative experiments on three\npublic HSI benchmarks demonstrate the competitiveness of the proposed methods\ncompared with other state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2106.13952",
          "publishedOn": "2021-06-29T01:55:15.400Z",
          "wordCount": 641,
          "title": "Spectral-Spatial Graph Reasoning Network for Hyperspectral Image Classification. (arXiv:2106.13952v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Alvaro Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_S/0/1/0/all/0/1\">Simon M. Lucas</a>",
          "description": "A large part of the interest in model-based reinforcement learning derives\nfrom the potential utility to acquire a forward model capable of strategic long\nterm decision making. Assuming that an agent succeeds in learning a useful\npredictive model, it still requires a mechanism to harness it to generate and\nselect among competing simulated plans. In this paper, we explore this theme\ncombining evolutionary algorithmic planning techniques with models learned via\ndeep learning and variational inference. We demonstrate the approach with an\nagent that reliably performs online planning in a set of visual navigation\ntasks.",
          "link": "http://arxiv.org/abs/2106.13911",
          "publishedOn": "2021-06-29T01:55:15.394Z",
          "wordCount": 548,
          "title": "Predictive Control Using Learned State Space Models via Rolling Horizon Evolution. (arXiv:2106.13911v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.03040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianfeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuowei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijun Zhang</a>",
          "description": "This paper establishes the optimal approximation error characterization of\ndeep ReLU networks for smooth functions in terms of both width and depth\nsimultaneously. To that end, we first prove that multivariate polynomials can\nbe approximated by deep ReLU networks of width $\\mathcal{O}(N)$ and depth\n$\\mathcal{O}(L)$ with an approximation error $\\mathcal{O}(N^{-L})$. Through\nlocal Taylor expansions and their deep ReLU network approximations, we show\nthat deep ReLU networks of width $\\mathcal{O}(N\\ln N)$ and depth\n$\\mathcal{O}(L\\ln L)$ can approximate $f\\in C^s([0,1]^d)$ with a nearly optimal\napproximation error $\\mathcal{O}(\\|f\\|_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})$. Our\nestimate is non-asymptotic in the sense that it is valid for arbitrary width\nand depth specified by $N\\in\\mathbb{N}^+$ and $L\\in\\mathbb{N}^+$, respectively.",
          "link": "http://arxiv.org/abs/2001.03040",
          "publishedOn": "2021-06-28T01:57:57.979Z",
          "wordCount": 600,
          "title": "Deep Network Approximation for Smooth Functions. (arXiv:2001.03040v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.13365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mehdi Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1\">Max Berrendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoyt_C/0/1/0/all/0/1\">Charles Tapley Hoyt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vermue_L/0/1/0/all/0/1\">Laurent Vermue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1\">Mikhail Galkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifzadeh_S/0/1/0/all/0/1\">Sahand Sharifzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1\">Asja Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>",
          "description": "The heterogeneity in recently published knowledge graph embedding models'\nimplementations, training, and evaluation has made fair and thorough\ncomparisons difficult. In order to assess the reproducibility of previously\npublished results, we re-implemented and evaluated 21 interaction models in the\nPyKEEN software package. Here, we outline which results could be reproduced\nwith their reported hyper-parameters, which could only be reproduced with\nalternate hyper-parameters, and which could not be reproduced at all as well as\nprovide insight as to why this might be the case.\n\nWe then performed a large-scale benchmarking on four datasets with several\nthousands of experiments and 24,804 GPU hours of computation time. We present\ninsights gained as to best practices, best configurations for each model, and\nwhere improvements could be made over previously published best configurations.\nOur results highlight that the combination of model architecture, training\napproach, loss function, and the explicit modeling of inverse relations is\ncrucial for a model's performances, and not only determined by the model\narchitecture. We provide evidence that several architectures can obtain results\ncompetitive to the state-of-the-art when configured carefully. We have made all\ncode, experimental configurations, results, and analyses that lead to our\ninterpretations available at https://github.com/pykeen/pykeen and\nhttps://github.com/pykeen/benchmarking",
          "link": "http://arxiv.org/abs/2006.13365",
          "publishedOn": "2021-06-28T01:57:57.973Z",
          "wordCount": 719,
          "title": "Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework. (arXiv:2006.13365v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farid_A/0/1/0/all/0/1\">Alec Farid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veer_S/0/1/0/all/0/1\">Sushant Veer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1\">Anirudha Majumdar</a>",
          "description": "Our goal is to perform out-of-distribution (OOD) detection, i.e., to detect\nwhen a robot is operating in environments that are drawn from a different\ndistribution than the environments used to train the robot. We leverage\nProbably Approximately Correct (PAC)-Bayes theory in order to train a policy\nwith a guaranteed bound on performance on the training distribution. Our key\nidea for OOD detection then relies on the following intuition: violation of the\nperformance bound on test environments provides evidence that the robot is\noperating OOD. We formalize this via statistical techniques based on p-values\nand concentration inequalities. The resulting approach (i) provides guaranteed\nconfidence bounds on OOD detection, and (ii) is task-driven and sensitive only\nto changes that impact the robot's performance. We demonstrate our approach on\na simulated example of grasping objects with unfamiliar poses or shapes. We\nalso present both simulation and hardware experiments for a drone performing\nvision-based obstacle avoidance in unfamiliar environments (including wind\ndisturbances and different obstacle densities). Our examples demonstrate that\nwe can perform task-driven OOD detection within just a handful of trials.\nComparisons with baselines also demonstrate the advantages of our approach in\nterms of providing statistical guarantees and being insensitive to\ntask-irrelevant distribution shifts.",
          "link": "http://arxiv.org/abs/2106.13703",
          "publishedOn": "2021-06-28T01:57:57.965Z",
          "wordCount": 637,
          "title": "Task-Driven Out-of-Distribution Detection with Statistical Guarantees for Robot Learning. (arXiv:2106.13703v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13781",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_T/0/1/0/all/0/1\">Tianyi Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1\">Yuejiao Sun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yin_W/0/1/0/all/0/1\">Wotao Yin</a>",
          "description": "Stochastic nested optimization, including stochastic compositional, min-max\nand bilevel optimization, is gaining popularity in many machine learning\napplications. While the three problems share the nested structure, existing\nworks often treat them separately, and thus develop problem-specific algorithms\nand their analyses. Among various exciting developments, simple SGD-type\nupdates (potentially on multiple variables) are still prevalent in solving this\nclass of nested problems, but they are believed to have slower convergence rate\ncompared to that of the non-nested problems. This paper unifies several\nSGD-type updates for stochastic nested problems into a single SGD approach that\nwe term ALternating Stochastic gradient dEscenT (ALSET) method. By leveraging\nthe hidden smoothness of the problem, this paper presents a tighter analysis of\nALSET for stochastic nested problems. Under the new analysis, to achieve an\n$\\epsilon$-stationary point of the nested problem, it requires ${\\cal\nO}(\\epsilon^{-2})$ samples. Under certain regularity conditions, applying our\nresults to stochastic compositional, min-max and reinforcement learning\nproblems either improves or matches the best-known sample complexity in the\nrespective cases. Our results explain why simple SGD-type algorithms in\nstochastic nested problems all work very well in practice without the need for\nfurther modifications.",
          "link": "http://arxiv.org/abs/2106.13781",
          "publishedOn": "2021-06-28T01:57:57.960Z",
          "wordCount": 638,
          "title": "Tighter Analysis of Alternating Stochastic Gradient Method for Stochastic Nested Problems. (arXiv:2106.13781v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2003.12319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andreoli_L/0/1/0/all/0/1\">Louis Andreoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porte_X/0/1/0/all/0/1\">Xavier Porte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chretien_S/0/1/0/all/0/1\">St&#xe9;phane Chr&#xe9;tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacquot_M/0/1/0/all/0/1\">Maxime Jacquot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larger_L/0/1/0/all/0/1\">Laurent Larger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunner_D/0/1/0/all/0/1\">Daniel Brunner</a>",
          "description": "A high efficiency hardware integration of neural networks benefits from\nrealizing nonlinearity, network connectivity and learning fully in a physical\nsubstrate. Multiple systems have recently implemented some or all of these\noperations, yet the focus was placed on addressing technological challenges.\nFundamental questions regarding learning in hardware neural networks remain\nlargely unexplored. Noise in particular is unavoidable in such architectures,\nand here we investigate its interaction with a learning algorithm using an\nopto-electronic recurrent neural network. We find that noise strongly modifies\nthe system's path during convergence, and surprisingly fully decorrelates the\nfinal readout weight matrices. This highlights the importance of understanding\narchitecture, noise and learning algorithm as interacting players, and\ntherefore identifies the need for mathematical tools for noisy, analogue system\noptimization.",
          "link": "http://arxiv.org/abs/2003.12319",
          "publishedOn": "2021-06-28T01:57:57.953Z",
          "wordCount": 613,
          "title": "Boolean learning under noise-perturbations in hardware neural networks. (arXiv:2003.12319v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiding Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagarajan_V/0/1/0/all/0/1\">Vaishnavh Nagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_C/0/1/0/all/0/1\">Christina Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>",
          "description": "We empirically show that the test error of deep networks can be estimated by\nsimply training the same architecture on the same training set but with a\ndifferent run of Stochastic Gradient Descent (SGD), and measuring the\ndisagreement rate between the two networks on unlabeled test data. This builds\non -- and is a stronger version of -- the observation in Nakkiran & Bansal '20,\nwhich requires the second run to be on an altogether fresh training set. We\nfurther theoretically show that this peculiar phenomenon arises from the\n\\emph{well-calibrated} nature of \\emph{ensembles} of SGD-trained models. This\nfinding not only provides a simple empirical measure to directly predict the\ntest error using unlabeled test data, but also establishes a new conceptual\nconnection between generalization and calibration.",
          "link": "http://arxiv.org/abs/2106.13799",
          "publishedOn": "2021-06-28T01:57:57.932Z",
          "wordCount": 562,
          "title": "Assessing Generalization of SGD via Disagreement. (arXiv:2106.13799v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13755",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Angiuli_A/0/1/0/all/0/1\">Andrea Angiuli</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fouque_J/0/1/0/all/0/1\">Jean-Pierre Fouque</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lauriere_M/0/1/0/all/0/1\">Mathieu Lauriere</a>",
          "description": "Mean field games (MFG) and mean field control problems (MFC) are frameworks\nto study Nash equilibria or social optima in games with a continuum of agents.\nThese problems can be used to approximate competitive or cooperative games with\na large finite number of agents and have found a broad range of applications,\nin particular in economics. In recent years, the question of learning in MFG\nand MFC has garnered interest, both as a way to compute solutions and as a way\nto model how large populations of learners converge to an equilibrium. Of\nparticular interest is the setting where the agents do not know the model,\nwhich leads to the development of reinforcement learning (RL) methods. After\nreviewing the literature on this topic, we present a two timescale approach\nwith RL for MFG and MFC, which relies on a unified Q-learning algorithm. The\nmain novelty of this method is to simultaneously update an action-value\nfunction and a distribution but with different rates, in a model-free fashion.\nDepending on the ratio of the two learning rates, the algorithm learns either\nthe MFG or the MFC solution. To illustrate this method, we apply it to a mean\nfield problem of accumulated consumption in finite horizon with HARA utility\nfunction, and to a trader's optimal liquidation problem.",
          "link": "http://arxiv.org/abs/2106.13755",
          "publishedOn": "2021-06-28T01:57:57.924Z",
          "wordCount": 652,
          "title": "Reinforcement Learning for Mean Field Games, with Applications to Economics. (arXiv:2106.13755v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kavouras_I/0/1/0/all/0/1\">Ioannis Kavouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaselimia_M/0/1/0/all/0/1\">Maria Kaselimia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardis_E/0/1/0/all/0/1\">Emmanuel Sardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>",
          "description": "In this work we investigate the short-term variations in air quality\nemissions, attributed to the prevention measures, applied in different cities,\nto mitigate the COVID-19 spread. In particular, we emphasize on the\nconcentration effects regarding specific pollutant gases, such as carbon\nmonoxide (CO), ozone (O3), nitrogen dioxide (NO2) and sulphur dioxide (SO2).\nThe assessment of the impact of lockdown on air quality focused on four\nEuropean Cities (Athens, Gladsaxe, Lodz and Rome). Available data on pollutant\nfactors were obtained using global satellite observations. The level of the\nemployed prevention measures is employed using the Oxford COVID-19 Government\nResponse Tracker. The second part of the analysis employed a variety of machine\nlearning tools, utilized for estimating the concentration of each pollutant,\ntwo days ahead. The results showed that a weak to moderate correlation exists\nbetween the corresponding measures and the pollutant factors and that it is\npossible to create models which can predict the behaviour of the pollutant\ngases under daily human activities.",
          "link": "http://arxiv.org/abs/2106.13750",
          "publishedOn": "2021-06-28T01:57:57.919Z",
          "wordCount": 648,
          "title": "Assessing the Lockdown Effects on Air Quality during COVID-19 Era. (arXiv:2106.13750v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Errica_F/0/1/0/all/0/1\">Federico Errica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1\">Alessio Micheli</a>",
          "description": "We introduce the Graph Mixture Density Networks, a new family of machine\nlearning models that can fit multimodal output distributions conditioned on\ngraphs of arbitrary topology. By combining ideas from mixture models and graph\nrepresentation learning, we address a broader class of challenging conditional\ndensity estimation problems that rely on structured data. In this respect, we\nevaluate our method on a new benchmark application that leverages random graphs\nfor stochastic epidemic simulations. We show a significant improvement in the\nlikelihood of epidemic outcomes when taking into account both multimodality and\nstructure. The empirical analysis is complemented by two real-world regression\ntasks showing the effectiveness of our approach in modeling the output\nprediction uncertainty. Graph Mixture Density Networks open appealing research\nopportunities in the study of structure-dependent phenomena that exhibit\nnon-trivial conditional output distributions.",
          "link": "http://arxiv.org/abs/2012.03085",
          "publishedOn": "2021-06-28T01:57:57.911Z",
          "wordCount": 613,
          "title": "Graph Mixture Density Networks. (arXiv:2012.03085v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13727",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Fuhg_J/0/1/0/all/0/1\">Jan Niklas Fuhg</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fau_A/0/1/0/all/0/1\">Am&#xe9;lie Fau</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bouklas_N/0/1/0/all/0/1\">Nikolaos Bouklas</a>",
          "description": "Temporally and spatially dependent uncertain parameters are regularly\nencountered in engineering applications. Commonly these uncertainties are\naccounted for using random fields and processes which require knowledge about\nthe appearing probability distributions functions which is not readily\navailable. In these cases non-probabilistic approaches such as interval\nanalysis and fuzzy set theory are helpful uncertainty measures. Partial\ndifferential equations involving fuzzy and interval fields are traditionally\nsolved using the finite element method where the input fields are sampled using\nsome basis function expansion methods. This approach however is problematic, as\nit is reliant on knowledge about the spatial correlation fields. In this work\nwe utilize physics-informed neural networks (PINNs) to solve interval and fuzzy\npartial differential equations. The resulting network structures termed\ninterval physics-informed neural networks (iPINNs) and fuzzy physics-informed\nneural networks (fPINNs) show promising results for obtaining bounded solutions\nof equations involving spatially uncertain parameter fields. In contrast to\nfinite element approaches, no correlation length specification of the input\nfields as well as no averaging via Monte-Carlo simulations are necessary. In\nfact, information about the input interval fields is obtained directly as a\nbyproduct of the presented solution scheme. Furthermore, all major advantages\nof PINNs are retained, i.e. meshfree nature of the scheme, and ease of inverse\nproblem set-up.",
          "link": "http://arxiv.org/abs/2106.13727",
          "publishedOn": "2021-06-28T01:57:57.905Z",
          "wordCount": 662,
          "title": "Interval and fuzzy physics-informed neural networks for uncertain fields. (arXiv:2106.13727v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chengshuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Cong Shen</a>",
          "description": "We study a new stochastic multi-player multi-armed bandits (MP-MAB) problem,\nwhere the reward distribution changes if a collision occurs on the arm.\nExisting literature always assumes a zero reward for involved players if\ncollision happens, but for applications such as cognitive radio, the more\nrealistic scenario is that collision reduces the mean reward but not\nnecessarily to zero. We focus on the more practical no-sensing setting where\nplayers do not perceive collisions directly, and propose the Error-Correction\nCollision Communication (EC3) algorithm that models implicit communication as a\nreliable communication over noisy channel problem, for which random coding\nerror exponent is used to establish the optimal regret that no communication\nprotocol can beat. Finally, optimizing the tradeoff between code length and\ndecoding error rate leads to a regret that approaches the centralized MP-MAB\nregret, which represents a natural lower bound. Experiments with practical\nerror-correction codes on both synthetic and real-world datasets demonstrate\nthe superiority of EC3. In particular, the results show that the choice of\ncoding schemes has a profound impact on the regret performance.",
          "link": "http://arxiv.org/abs/2106.13669",
          "publishedOn": "2021-06-28T01:57:57.891Z",
          "wordCount": 625,
          "title": "Multi-player Multi-armed Bandits with Collision-Dependent Reward Distributions. (arXiv:2106.13669v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korine_R/0/1/0/all/0/1\">Ron Korine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendler_D/0/1/0/all/0/1\">Danny Hendler</a>",
          "description": "Numerous metamorphic and polymorphic malicious variants are generated\nautomatically on a daily basis by mutation engines that transform the code of a\nmalicious program while retaining its functionality, in order to evade\nsignature-based detection. These automatic processes have greatly increased the\nnumber of malware variants, deeming their fully-manual analysis impossible.\nMalware classification is the task of determining to which family a new\nmalicious variant belongs. Variants of the same malware family show similar\nbehavioral patterns. Thus, classifying newly discovered malicious programs and\napplications helps assess the risks they pose. Moreover, malware classification\nfacilitates determining which of the newly discovered variants should undergo\nmanual analysis by a security expert, in order to determine whether they belong\nto a new family (e.g., one whose members exploit a zero-day vulnerability) or\nare simply the result of a concept drift within a known malicious family. This\nmotivated intense research in recent years on devising high-accuracy automatic\ntools for malware classification. In this work, we present DAEMON - a novel\ndataset-agnostic malware classifier. A key property of DAEMON is that the type\nof features it uses and the manner in which they are mined facilitate\nunderstanding the distinctive behavior of malware families, making its\nclassification decisions explainable. We've optimized DAEMON using a\nlarge-scale dataset of x86 binaries, belonging to a mix of several malware\nfamilies targeting computers running Windows. We then re-trained it and applied\nit, without any algorithmic change, feature re-engineering or parameter tuning,\nto two other large-scale datasets of malicious Android applications consisting\nof numerous malware families. DAEMON obtained highly accurate classification\nresults on all datasets, establishing that it is also platform-agnostic.",
          "link": "http://arxiv.org/abs/2008.01855",
          "publishedOn": "2021-06-28T01:57:57.885Z",
          "wordCount": 729,
          "title": "DAEMON: Dataset-Agnostic Explainable Malware Classification Using Multi-Stage Feature Mining. (arXiv:2008.01855v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05888",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chakrabarty_A/0/1/0/all/0/1\">Ankush Chakrabarty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benosman_M/0/1/0/all/0/1\">Mouhacine Benosman</a>",
          "description": "Data generated from dynamical systems with unknown dynamics enable the\nlearning of state observers that are: robust to modeling error, computationally\ntractable to design, and capable of operating with guaranteed performance. In\nthis paper, a modular design methodology is formulated, that consists of three\ndesign phases: (i) an initial robust observer design that enables one to learn\nthe dynamics without allowing the state estimation error to diverge (hence,\nsafe); (ii) a learning phase wherein the unmodeled components are estimated\nusing Bayesian optimization and Gaussian processes; and, (iii) a re-design\nphase that leverages the learned dynamics to improve convergence rate of the\nstate estimation error. The potential of our proposed learning-based observer\nis demonstrated on a benchmark nonlinear system. Additionally, certificates of\nguaranteed estimation performance are provided.",
          "link": "http://arxiv.org/abs/2005.05888",
          "publishedOn": "2021-06-28T01:57:57.879Z",
          "wordCount": 595,
          "title": "Safe Learning-based Observers for Unknown Nonlinear Systems using Bayesian Optimization. (arXiv:2005.05888v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>",
          "description": "Recent advances in image synthesis enables one to translate images by\nlearning the mapping between a source domain and a target domain. Existing\nmethods tend to learn the distributions by training a model on a variety of\ndatasets, with results evaluated largely in a subjective manner. Relatively few\nworks in this area, however, study the potential use of semantic image\ntranslation methods for image recognition tasks. In this paper, we explore the\nuse of Single Image Texture Translation (SITT) for data augmentation. We first\npropose a lightweight model for translating texture to images based on a single\ninput of source texture, allowing for fast training and testing. Based on SITT,\nwe then explore the use of augmented data in long-tailed and few-shot image\nclassification tasks. We find the proposed method is capable of translating\ninput data into a target domain, leading to consistent improved image\nrecognition performance. Finally, we examine how SITT and related image\ntranslation methods can provide a basis for a data-efficient, augmentation\nengineering approach to model training.",
          "link": "http://arxiv.org/abs/2106.13804",
          "publishedOn": "2021-06-28T01:57:57.872Z",
          "wordCount": 612,
          "title": "Single Image Texture Translation for Data Augmentation. (arXiv:2106.13804v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1\">Carolin Lawrence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1\">Mathias Niepert</a>",
          "description": "Genetic mutations can cause disease by disrupting normal gene function.\nIdentifying the disease-causing mutations from millions of genetic variants\nwithin an individual patient is a challenging problem. Computational methods\nwhich can prioritize disease-causing mutations have, therefore, enormous\napplications. It is well-known that genes function through a complex regulatory\nnetwork. However, existing variant effect prediction models only consider a\nvariant in isolation. In contrast, we propose VEGN, which models variant effect\nprediction using a graph neural network (GNN) that operates on a heterogeneous\ngraph with genes and variants. The graph is created by assigning variants to\ngenes and connecting genes with an gene-gene interaction network. In this\ncontext, we explore an approach where a gene-gene graph is given and another\nwhere VEGN learns the gene-gene graph and therefore operates both on given and\nlearnt edges. The graph neural network is trained to aggregate information\nbetween genes, and between genes and variants. Variants can exchange\ninformation via the genes they connect to. This approach improves the\nperformance of existing state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.13642",
          "publishedOn": "2021-06-28T01:57:57.866Z",
          "wordCount": 618,
          "title": "VEGN: Variant Effect Prediction with Graph Neural Networks. (arXiv:2106.13642v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wright_L/0/1/0/all/0/1\">Less Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeure_N/0/1/0/all/0/1\">Nestor Demeure</a>",
          "description": "As optimizers are critical to the performances of neural networks, every year\na large number of papers innovating on the subject are published. However,\nwhile most of these publications provide incremental improvements to existing\nalgorithms, they tend to be presented as new optimizers rather than composable\nalgorithms. Thus, many worthwhile improvements are rarely seen out of their\ninitial publication. Taking advantage of this untapped potential, we introduce\nRanger21, a new optimizer which combines AdamW with eight components, carefully\nselected after reviewing and testing ideas from the literature. We found that\nthe resulting optimizer provides significantly improved validation accuracy and\ntraining speed, smoother training curves, and is even able to train a ResNet50\non ImageNet2012 without Batch Normalization layers. A problem on which AdamW\nstays systematically stuck in a bad initial state.",
          "link": "http://arxiv.org/abs/2106.13731",
          "publishedOn": "2021-06-28T01:57:57.858Z",
          "wordCount": 564,
          "title": "Ranger21: a synergistic deep learning optimizer. (arXiv:2106.13731v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stiasny_J/0/1/0/all/0/1\">Jochen Stiasny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misyris_G/0/1/0/all/0/1\">Georgios S. Misyris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzivasileiadis_S/0/1/0/all/0/1\">Spyros Chatzivasileiadis</a>",
          "description": "Solving the ordinary differential equations that govern the power system is\nan indispensable part in transient stability analysis. However, the\ntraditionally applied methods either carry a significant computational burden,\nrequire model simplifications, or use overly conservative surrogate models.\nNeural networks can circumvent these limitations but are faced with high\ndemands on the used datasets. Furthermore, they are agnostic to the underlying\ngoverning equations. Physics-informed neural network tackle this problem and we\nexplore their advantages and challenges in this paper. We illustrate the\nfindings on the Kundur two-area system and highlight possible pathways forward\nin developing this method further.",
          "link": "http://arxiv.org/abs/2106.13638",
          "publishedOn": "2021-06-28T01:57:57.842Z",
          "wordCount": 531,
          "title": "Transient Stability Analysis with Physics-Informed Neural Networks. (arXiv:2106.13638v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Daniel T. Chang</a>",
          "description": "Bayesian neural networks utilize probabilistic layers that capture\nuncertainty over weights and activations, and are trained using Bayesian\ninference. Since these probabilistic layers are designed to be drop-in\nreplacement of their deterministic counter parts, Bayesian neural networks\nprovide a direct and natural way to extend conventional deep neural networks to\nsupport probabilistic deep learning. However, it is nontrivial to understand,\ndesign and train Bayesian neural networks due to their complexities. We discuss\nthe essentials of Bayesian neural networks including duality (deep neural\nnetworks, probabilistic models), approximate Bayesian inference, Bayesian\npriors, Bayesian posteriors, and deep variational learning. We use TensorFlow\nProbability APIs and code examples for illustration. The main problem with\nBayesian neural networks is that the architecture of deep neural networks makes\nit quite redundant, and costly, to account for uncertainty for a large number\nof successive layers. Hybrid Bayesian neural networks, which use few\nprobabilistic layers judicially positioned in the networks, provide a practical\nsolution.",
          "link": "http://arxiv.org/abs/2106.13594",
          "publishedOn": "2021-06-28T01:57:57.835Z",
          "wordCount": 577,
          "title": "Bayesian Neural Networks: Essentials. (arXiv:2106.13594v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaeili_B/0/1/0/all/0/1\">Babak Esmaeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wick_M/0/1/0/all/0/1\">Michael Wick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tristan_J/0/1/0/all/0/1\">Jean-Baptiste Tristan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>",
          "description": "In this paper, we propose conjugate energy-based models (CEBMs), a new class\nof energy-based models that define a joint density over data and latent\nvariables. The joint density of a CEBM decomposes into an intractable\ndistribution over data and a tractable posterior over latent variables. CEBMs\nhave similar use cases as variational autoencoders, in the sense that they\nlearn an unsupervised mapping from data to latent variables. However, these\nmodels omit a generator network, which allows them to learn more flexible\nnotions of similarity between data points. Our experiments demonstrate that\nconjugate EBMs achieve competitive results in terms of image modelling,\npredictive power of latent space, and out-of-domain detection on a variety of\ndatasets.",
          "link": "http://arxiv.org/abs/2106.13798",
          "publishedOn": "2021-06-28T01:57:57.828Z",
          "wordCount": 543,
          "title": "Conjugate Energy-Based Models. (arXiv:2106.13798v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scieur_D/0/1/0/all/0/1\">Damien Scieur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngsung Kim</a>",
          "description": "This paper considers classification problems with hierarchically organized\nclasses. We force the classifier (hyperplane) of each class to belong to a\nsphere manifold, whose center is the classifier of its super-class. Then,\nindividual sphere manifolds are connected based on their hierarchical\nrelations. Our technique replaces the last layer of a neural network by\ncombining a spherical fully-connected layer with a hierarchical layer. This\nregularization is shown to improve the performance of widely used deep neural\nnetwork architectures (ResNet and DenseNet) on publicly available datasets\n(CIFAR100, CUB200, Stanford dogs, Stanford cars, and Tiny-ImageNet).",
          "link": "http://arxiv.org/abs/2106.13549",
          "publishedOn": "2021-06-28T01:57:57.814Z",
          "wordCount": 524,
          "title": "Connecting Sphere Manifolds Hierarchically for Regularization. (arXiv:2106.13549v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1\">Mingyi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Steven Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jinfeng Yi</a>",
          "description": "Providing privacy protection has been one of the primary motivations of\nFederated Learning (FL). Recently, there has been a line of work on\nincorporating the formal privacy notion of differential privacy with FL. To\nguarantee the client-level differential privacy in FL algorithms, the clients'\ntransmitted model updates have to be clipped before adding privacy noise. Such\nclipping operation is substantially different from its counterpart of gradient\nclipping in the centralized differentially private SGD and has not been\nwell-understood. In this paper, we first empirically demonstrate that the\nclipped FedAvg can perform surprisingly well even with substantial data\nheterogeneity when training neural networks, which is partly because the\nclients' updates become similar for several popular deep architectures. Based\non this key observation, we provide the convergence analysis of a differential\nprivate (DP) FedAvg algorithm and highlight the relationship between clipping\nbias and the distribution of the clients' updates. To the best of our\nknowledge, this is the first work that rigorously investigates theoretical and\nempirical issues regarding the clipping operation in FL algorithms.",
          "link": "http://arxiv.org/abs/2106.13673",
          "publishedOn": "2021-06-28T01:57:57.807Z",
          "wordCount": 620,
          "title": "Understanding Clipping for Federated Learning: Convergence and Client-Level Differential Privacy. (arXiv:2106.13673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Israel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1\">Baruch Berdugo</a>",
          "description": "In this paper, we propose a residual echo suppression method using a UNet\nneural network that directly maps the outputs of a linear acoustic echo\ncanceler to the desired signal in the spectral domain. This system embeds a\ndesign parameter that allows a tunable tradeoff between the desired-signal\ndistortion and residual echo suppression in double-talk scenarios. The system\nemploys 136 thousand parameters, and requires 1.6 Giga floating-point\noperations per second and 10 Mega-bytes of memory. The implementation satisfies\nboth the timing requirements of the AEC challenge and the computational and\nmemory limitations of on-device applications. Experiments are conducted with\n161~h of data from the AEC challenge database and from real independent\nrecordings. We demonstrate the performance of the proposed system in real-life\nconditions and compare it with two competing methods regarding echo suppression\nand desired-signal distortion, generalization to various environments, and\nrobustness to high echo levels.",
          "link": "http://arxiv.org/abs/2106.13531",
          "publishedOn": "2021-06-28T01:57:57.800Z",
          "wordCount": 608,
          "title": "Deep Residual Echo Suppression with A Tunable Tradeoff Between Signal Distortion and Echo Suppression. (arXiv:2106.13531v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kates_B/0/1/0/all/0/1\">Brandon Kates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mentch_J/0/1/0/all/0/1\">Jeff Mentch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharkar_A/0/1/0/all/0/1\">Anant Kharkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "This work improves the quality of automated machine learning (AutoML) systems\nby using dataset and function descriptions while significantly decreasing\ncomputation time from minutes to milliseconds by using a zero-shot approach.\nGiven a new dataset and a well-defined machine learning task, humans begin by\nreading a description of the dataset and documentation for the algorithms to be\nused. This work is the first to use these textual descriptions, which we call\nprivileged information, for AutoML. We use a pre-trained Transformer model to\nprocess the privileged text and demonstrate that using this information\nimproves AutoML performance. Thus, our approach leverages the progress of\nunsupervised representation learning in natural language processing to provide\na significant boost to AutoML. We demonstrate that using only textual\ndescriptions of the data and functions achieves reasonable classification\nperformance, and adding textual descriptions to data meta-features improves\nclassification across tabular datasets. To achieve zero-shot AutoML we train a\ngraph neural network with these description embeddings and the data\nmeta-features. Each node represents a training dataset, which we use to predict\nthe best machine learning pipeline for a new test dataset in a zero-shot\nfashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a\nsupervised learning task and dataset. In contrast, most AutoML systems require\ntens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces\nrunning and prediction times from minutes to milliseconds, consistently across\ndatasets. By speeding up AutoML by orders of magnitude this work demonstrates\nreal-time AutoML.",
          "link": "http://arxiv.org/abs/2106.13743",
          "publishedOn": "2021-06-28T01:57:57.792Z",
          "wordCount": 677,
          "title": "Privileged Zero-Shot AutoML. (arXiv:2106.13743v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1\">Francesco Farina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1\">Emma Slade</a>",
          "description": "We introduce a novel architecture for graph networks which is equivariant to\nany transformation in the coordinate embeddings that preserves the distance\nbetween neighbouring nodes. In particular, it is equivariant to the Euclidean\nand conformal orthogonal groups in $n$-dimensions. Thanks to its equivariance\nproperties, the proposed model is extremely more data efficient with respect to\nclassical graph architectures and also intrinsically equipped with a better\ninductive bias. We show that, learning on a minimal amount of data, the\narchitecture we propose can perfectly generalise to unseen data in a synthetic\nproblem, while much more training data are required from a standard model to\nreach comparable performance.",
          "link": "http://arxiv.org/abs/2106.13786",
          "publishedOn": "2021-06-28T01:57:57.784Z",
          "wordCount": 554,
          "title": "Data efficiency in graph networks through equivariance. (arXiv:2106.13786v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jinjin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiguo Gong</a>",
          "description": "The abundant sequential documents such as online archival, social media and\nnews feeds are streamingly updated, where each chunk of documents is\nincorporated with smoothly evolving yet dependent topics. Such digital texts\nhave attracted extensive research on dynamic topic modeling to infer hidden\nevolving topics and their temporal dependencies. However, most of the existing\napproaches focus on single-topic-thread evolution and ignore the fact that a\ncurrent topic may be coupled with multiple relevant prior topics. In addition,\nthese approaches also incur the intractable inference problem when inferring\nlatent parameters, resulting in a high computational cost and performance\ndegradation. In this work, we assume that a current topic evolves from all\nprior topics with corresponding coupling weights, forming the\nmulti-topic-thread evolution. Our method models the dependencies between\nevolving topics and thoroughly encodes their complex multi-couplings across\ntime steps. To conquer the intractable inference challenge, a new solution with\na set of novel data augmentation techniques is proposed, which successfully\ndiscomposes the multi-couplings between evolving topics. A fully conjugate\nmodel is thus obtained to guarantee the effectiveness and efficiency of the\ninference technique. A novel Gibbs sampler with a backward-forward filter\nalgorithm efficiently learns latent timeevolving parameters in a closed-form.\nIn addition, the latent Indian Buffet Process (IBP) compound distribution is\nexploited to automatically infer the overall topic number and customize the\nsparse topic proportions for each sequential document without bias. The\nproposed method is evaluated on both synthetic and real-world datasets against\nthe competitive baselines, demonstrating its superiority over the baselines in\nterms of the low per-word perplexity, high coherent topics, and better document\ntime prediction.",
          "link": "http://arxiv.org/abs/2106.13732",
          "publishedOn": "2021-06-28T01:57:57.771Z",
          "wordCount": 695,
          "title": "Recurrent Coupled Topic Modeling over Sequential Documents. (arXiv:2106.13732v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Rui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>",
          "description": "Building classifiers on multiple domains is a practical problem in the real\nlife. Instead of building classifiers one by one, multi-domain learning (MDL)\nsimultaneously builds classifiers on multiple domains. MDL utilizes the\ninformation shared among the domains to improve the performance. As a\nsupervised learning problem, the labeling effort is still high in MDL problems.\nUsually, this high labeling cost issue could be relieved by using active\nlearning. Thus, it is natural to utilize active learning to reduce the labeling\neffort in MDL, and we refer this setting as multi-domain active learning\n(MDAL). However, there are only few works which are built on this setting. And\nwhen the researches have to face this problem, there is no off-the-shelf\nsolutions. Under this circumstance, combining the current multi-domain learning\nmodels and single-domain active learning strategies might be a preliminary\nsolution for MDAL problem. To find out the potential of this preliminary\nsolution, a comparative study over 5 models and 4 selection strategies is made\nin this paper. To the best of our knowledge, this is the first work provides\nthe formal definition of MDAL. Besides, this is the first comparative work for\nMDAL problem. From the results, the Multinomial Adversarial Networks (MAN)\nmodel with a simple best vs second best (BvSB) uncertainty strategy shows its\nsuperiority in most cases. We take this combination as our off-the-shelf\nrecommendation for the MDAL problem.",
          "link": "http://arxiv.org/abs/2106.13516",
          "publishedOn": "2021-06-28T01:57:57.760Z",
          "wordCount": 657,
          "title": "Multi-Domain Active Learning: A Comparative Study. (arXiv:2106.13516v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13327",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Nandy_A/0/1/0/all/0/1\">Aditya Nandy</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Duan_C/0/1/0/all/0/1\">Chenru Duan</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kulik_H/0/1/0/all/0/1\">Heather J. Kulik</a>",
          "description": "Although the tailored metal active sites and porous architectures of MOFs\nhold great promise for engineering challenges ranging from gas separations to\ncatalysis, a lack of understanding of how to improve their stability limits\ntheir use in practice. To overcome this limitation, we extract thousands of\npublished reports of the key aspects of MOF stability necessary for their\npractical application: the ability to withstand high temperatures without\ndegrading and the capacity to be activated by removal of solvent molecules.\nFrom nearly 4,000 manuscripts, we use natural language processing and automated\nimage analysis to obtain over 2,000 solvent-removal stability measures and\n3,000 thermal degradation temperatures. We analyze the relationships between\nstability properties and the chemical and geometric structures in this set to\nidentify limits of prior heuristics derived from smaller sets of MOFs. By\ntraining predictive machine learning (ML, i.e., Gaussian process and artificial\nneural network) models to encode the structure-property relationships with\ngraph- and pore-structure-based representations, we are able to make\npredictions of stability orders of magnitude faster than conventional\nphysics-based modeling or experiment. Interpretation of important features in\nML models provides insights that we use to identify strategies to engineer\nincreased stability into typically unstable 3d-containing MOFs that are\nfrequently targeted for catalytic applications. We expect our approach to\naccelerate the time to discovery of stable, practical MOF materials for a wide\nrange of applications.",
          "link": "http://arxiv.org/abs/2106.13327",
          "publishedOn": "2021-06-28T01:57:57.470Z",
          "wordCount": 681,
          "title": "Using Machine Learning and Data Mining to Leverage Community Knowledge for the Engineering of Stable Metal-Organic Frameworks. (arXiv:2106.13327v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zexuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barucca_P/0/1/0/all/0/1\">Paolo Barucca</a>",
          "description": "Time series forecasting based on deep architectures has been gaining\npopularity in recent years due to their ability to model complex non-linear\ntemporal dynamics. The recurrent neural network is one such model capable of\nhandling variable-length input and output. In this paper, we leverage recent\nadvances in deep generative models and the concept of state space models to\npropose a stochastic adaptation of the recurrent neural network for\nmultistep-ahead time series forecasting, which is trained with stochastic\ngradient variational Bayes. In our model design, the transition function of the\nrecurrent neural network, which determines the evolution of the hidden states,\nis stochastic rather than deterministic as in a regular recurrent neural\nnetwork; this is achieved by incorporating a latent random variable into the\ntransition process which captures the stochasticity of the temporal dynamics.\nOur model preserves the architectural workings of a recurrent neural network\nfor which all relevant information is encapsulated in its hidden states, and\nthis flexibility allows our model to be easily integrated into any deep\narchitecture for sequential modelling. We test our model on a wide range of\ndatasets from finance to healthcare; results show that the stochastic recurrent\nneural network consistently outperforms its deterministic counterpart.",
          "link": "http://arxiv.org/abs/2104.12311",
          "publishedOn": "2021-06-28T01:57:57.449Z",
          "wordCount": 665,
          "title": "Stochastic Recurrent Neural Network for Multistep Time Series Forecasting. (arXiv:2104.12311v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutt_F/0/1/0/all/0/1\">Florina Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Subhajit Das</a>",
          "description": "Twitter is a useful resource to analyze peoples' opinions on various topics.\nOften these topics are correlated or associated with locations from where these\nTweet posts are made. For example, restaurant owners may need to know where\ntheir target customers eat with respect to the sentiment of the posts made\nrelated to food, policy planners may need to analyze citizens' opinion on\nrelevant issues such as crime, safety, congestion, etc. with respect to\nspecific parts of the city, or county or state. As promising as this is, less\nthan $1\\%$ of the crawled Tweet posts come with geolocation tags. That makes\naccurate prediction of Tweet posts for the non geo-tagged tweets very critical\nto analyze data in various domains. In this research, we utilized millions of\nTwitter posts and end-users domain expertise to build a set of deep neural\nnetwork models using natural language processing (NLP) techniques, that\npredicts the geolocation of non geo-tagged Tweet posts at various level of\ngranularities such as neighborhood, zipcode, and longitude with latitudes. With\nmultiple neural architecture experiments, and a collaborative human-machine\nworkflow design, our ongoing work on geolocation detection shows promising\nresults that empower end-users to correlate relationship between variables of\nchoice with the location information.",
          "link": "http://arxiv.org/abs/2106.13411",
          "publishedOn": "2021-06-28T01:57:57.431Z",
          "wordCount": 646,
          "title": "Fine-grained Geolocation Prediction of Tweets with Human Machine Collaboration. (arXiv:2106.13411v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Young D. Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1\">Jagmohan Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1\">Cecilia Mascolo</a>",
          "description": "Various incremental learning (IL) approaches have been proposed to help deep\nlearning models learn new tasks/classes continuously without forgetting what\nwas learned previously (i.e., avoid catastrophic forgetting). With the growing\nnumber of deployed audio sensing applications that need to dynamically\nincorporate new tasks and changing input distribution from users, the ability\nof IL on-device becomes essential for both efficiency and user privacy.\n\nHowever, prior works suffer from high computational costs and storage demands\nwhich hinders the deployment of IL on-device. In this work, to overcome these\nlimitations, we develop an end-to-end and on-device IL framework, FastICARL,\nthat incorporates an exemplar-based IL and quantization in the context of\naudio-based applications. We first employ k-nearest-neighbor to reduce the\nlatency of IL. Then, we jointly utilize a quantization technique to decrease\nthe storage requirements of IL. We implement FastICARL on two types of mobile\ndevices and demonstrate that FastICARL remarkably decreases the IL time up to\n78-92% and the storage requirements by 2-4 times without sacrificing its\nperformance. FastICARL enables complete on-device IL, ensuring user privacy as\nthe user data does not need to leave the device.",
          "link": "http://arxiv.org/abs/2106.07268",
          "publishedOn": "2021-06-28T01:57:57.407Z",
          "wordCount": 661,
          "title": "FastICARL: Fast Incremental Classifier and Representation Learning with Efficient Budget Allocation in Audio Sensing Applications. (arXiv:2106.07268v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.15083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gaglione_J/0/1/0/all/0/1\">Jean-Rapha&#xeb;l Gaglione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1\">Daniel Neider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rajarshi Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>",
          "description": "We address the problem of inferring descriptions of system behavior using\nLinear Temporal Logic (LTL) from a finite set of positive and negative\nexamples. Most of the existing approaches for solving such a task rely on\npredefined templates for guiding the structure of the inferred formula. The\napproaches that can infer arbitrary LTL formulas, on the other hand, are not\nrobust to noise in the data. To alleviate such limitations, we devise two\nalgorithms for inferring concise LTL formulas even in the presence of noise.\nOur first algorithm infers minimal LTL formulas by reducing the inference\nproblem to a problem in maximum satisfiability and then using off-the-shelf\nMaxSAT solvers to find a solution. To the best of our knowledge, we are the\nfirst to incorporate the usage of MaxSAT solvers for inferring formulas in LTL.\nOur second learning algorithm relies on the first algorithm to derive a\ndecision tree over LTL formulas based on a decision tree learning algorithm. We\nhave implemented both our algorithms and verified that our algorithms are\nefficient in extracting concise LTL descriptions even in the presence of noise.",
          "link": "http://arxiv.org/abs/2104.15083",
          "publishedOn": "2021-06-28T01:57:57.400Z",
          "wordCount": 660,
          "title": "Learning Linear Temporal Properties from Noisy Data: A MaxSAT Approach. (arXiv:2104.15083v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zniyed_Y/0/1/0/all/0/1\">Yassine Zniyed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usevich_K/0/1/0/all/0/1\">Konstantin Usevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_S/0/1/0/all/0/1\">Sebastian Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brie_D/0/1/0/all/0/1\">David Brie</a>",
          "description": "Activation functions (AFs) are an important part of the design of neural\nnetworks (NNs), and their choice plays a predominant role in the performance of\na NN. In this work, we are particularly interested in the estimation of\nflexible activation functions using tensor-based solutions, where the AFs are\nexpressed as a weighted sum of predefined basis functions. To do so, we propose\na new learning algorithm which solves a constrained coupled matrix-tensor\nfactorization (CMTF) problem. This technique fuses the first and zeroth order\ninformation of the NN, where the first-order information is contained in a\nJacobian tensor, following a constrained canonical polyadic decomposition\n(CPD). The proposed algorithm can handle different decomposition bases. The\ngoal of this method is to compress large pretrained NN models, by replacing\nsubnetworks, {\\em i.e.,} one or multiple layers of the original network, by a\nnew flexible layer. The approach is applied to a pretrained convolutional\nneural network (CNN) used for character classification.",
          "link": "http://arxiv.org/abs/2106.13542",
          "publishedOn": "2021-06-28T01:57:57.385Z",
          "wordCount": 601,
          "title": "Tensor-based framework for training flexible neural networks. (arXiv:2106.13542v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01839",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1\">R&#xe9;mi Gribonval</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chatalic_A/0/1/0/all/0/1\">Antoine Chatalic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1\">Nicolas Keriven</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schellekens_V/0/1/0/all/0/1\">Vincent Schellekens</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jacques_L/0/1/0/all/0/1\">Laurent Jacques</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schniter_P/0/1/0/all/0/1\">Philip Schniter</a>",
          "description": "This article considers \"compressive learning,\" an approach to large-scale\nmachine learning where datasets are massively compressed before learning (e.g.,\nclustering, classification, or regression) is performed. In particular, a\n\"sketch\" is first constructed by computing carefully chosen nonlinear random\nfeatures (e.g., random Fourier features) and averaging them over the whole\ndataset. Parameters are then learned from the sketch, without access to the\noriginal dataset. This article surveys the current state-of-the-art in\ncompressive learning, including the main concepts and algorithms, their\nconnections with established signal-processing methods, existing theoretical\nguarantees -- on both information preservation and privacy preservation, and\nimportant open problems.",
          "link": "http://arxiv.org/abs/2008.01839",
          "publishedOn": "2021-06-28T01:57:57.360Z",
          "wordCount": 567,
          "title": "Sketching Datasets for Large-Scale Learning (long version). (arXiv:2008.01839v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyejin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seiyun Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_K/0/1/0/all/0/1\">Kwang-Sung Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ok_J/0/1/0/all/0/1\">Jungseul Ok</a>",
          "description": "Structured stochastic multi-armed bandits provide accelerated regret rates\nover the standard unstructured bandit problems. Most structured bandits,\nhowever, assume the knowledge of the structural parameter such as Lipschitz\ncontinuity, which is often not available. To cope with the latent structural\nparameter, we consider a transfer learning setting in which an agent must learn\nto transfer the structural information from the prior tasks to the next task,\nwhich is inspired by practical problems such as rate adaptation in wireless\nlink. We propose a novel framework to provably and accurately estimate the\nLipschitz constant based on previous tasks and fully exploit it for the new\ntask at hand. We analyze the efficiency of the proposed framework in two folds:\n(i) the sample complexity of our estimator matches with the\ninformation-theoretic fundamental limit; and (ii) our regret bound on the new\ntask is close to that of the oracle algorithm with the full knowledge of the\nLipschitz constant under mild assumptions. Our analysis reveals a set of useful\ninsights on transfer learning for latent Lipschitzconstants such as the\nfundamental challenge a learner faces. Our numerical evaluations confirm our\ntheoretical findings and show the superiority of the proposed framework\ncompared to baselines.",
          "link": "http://arxiv.org/abs/2102.02472",
          "publishedOn": "2021-06-28T01:57:57.354Z",
          "wordCount": 662,
          "title": "Transfer Learning in Bandits with Latent Continuity. (arXiv:2102.02472v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abels_A/0/1/0/all/0/1\">Axel Abels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenaerts_T/0/1/0/all/0/1\">Tom Lenaerts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trianni_V/0/1/0/all/0/1\">Vito Trianni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1\">Ann Now&#xe9;</a>",
          "description": "Quite some real-world problems can be formulated as decision-making problems\nwherein one must repeatedly make an appropriate choice from a set of\nalternatives. Expert judgements, whether human or artificial, can help in\ntaking correct decisions, especially when exploration of alternative solutions\nis costly. As expert opinions might deviate, the problem of finding the right\nalternative can be approached as a collective decision making problem (CDM).\nCurrent state-of-the-art approaches to solve CDM are limited by the quality of\nthe best expert in the group, and perform poorly if experts are not qualified\nor if they are overly biased, thus potentially derailing the decision-making\nprocess. In this paper, we propose a new algorithmic approach based on\ncontextual multi-armed bandit problems (CMAB) to identify and counteract such\nbiased expertises. We explore homogeneous, heterogeneous and polarised expert\ngroups and show that this approach is able to effectively exploit the\ncollective expertise, irrespective of whether the provided advice is directly\nconducive to good performance, outperforming state-of-the-art methods,\nespecially when the quality of the provided expertise degrades. Our novel\nCMAB-inspired approach achieves a higher final performance and does so while\nconverging more rapidly than previous adaptive algorithms, especially when\nheterogeneous expertise is readily available.",
          "link": "http://arxiv.org/abs/2106.13539",
          "publishedOn": "2021-06-28T01:57:57.348Z",
          "wordCount": 630,
          "title": "Dealing with Expert Bias in Collective Decision-Making. (arXiv:2106.13539v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santos_R/0/1/0/all/0/1\">Rodrigo dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nilizadeh_S/0/1/0/all/0/1\">Shirin Nilizadeh</a>",
          "description": "Audio Event Detection (AED) Systems capture audio from the environment and\nemploy some deep learning algorithms for detecting the presence of a specific\nsound of interest. In this paper, we evaluate deep learning-based AED systems\nagainst evasion attacks through adversarial examples. We run multiple security\ncritical AED tasks, implemented as CNNs classifiers, and then generate audio\nadversarial examples using two different types of noise, namely background and\nwhite noise, that can be used by the adversary to evade detection. We also\nexamine the robustness of existing third-party AED capable devices, such as\nNest devices manufactured by Google, which run their own black-box deep\nlearning models.\n\nWe show that an adversary can focus on audio adversarial inputs to cause AED\nsystems to misclassify, similarly to what has been previously done by works\nfocusing on adversarial examples from the image domain. We then, seek to\nimprove classifiers' robustness through countermeasures to the attacks. We\nemploy adversarial training and a custom denoising technique. We show that\nthese countermeasures, when applied to audio input, can be successful, either\nin isolation or in combination, generating relevant increases of nearly fifty\npercent in the performance of the classifiers when these are under attack.",
          "link": "http://arxiv.org/abs/2106.07428",
          "publishedOn": "2021-06-28T01:57:57.340Z",
          "wordCount": 656,
          "title": "Audio Attacks and Defenses against AED Systems -- A Practical Study. (arXiv:2106.07428v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_T/0/1/0/all/0/1\">Tianle Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zongliang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkhodary_K/0/1/0/all/0/1\">Khalil I. Elkhodary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xu Guo</a>",
          "description": "In this paper, a mechanistic data-driven approach is proposed to accelerate\nstructural topology optimization, employing an in-house developed finite\nelement convolutional neural network (FE-CNN). Our approach can be divided into\ntwo stages: offline training, and online optimization. During offline training,\na mapping function is built between high and low resolution representations of\na given design domain. The mapping is expressed by a FE-CNN, which targets a\ncommon objective function value (e.g., structural compliance) across design\ndomains of differing resolutions. During online optimization, an arbitrary\ndesign domain of high resolution is reduced to low resolution through the\ntrained mapping function. The original high-resolution domain is thus designed\nby computations performed on only the low-resolution version, followed by an\ninverse mapping back to the high-resolution domain. Numerical examples\ndemonstrate that this approach can accelerate optimization by up to an order of\nmagnitude in computational time. Our proposed approach therefore shows great\npotential to overcome the curse-of-dimensionality incurred by density-based\nstructural topology optimization. The limitation of our present approach is\nalso discussed.",
          "link": "http://arxiv.org/abs/2106.13652",
          "publishedOn": "2021-06-28T01:57:57.334Z",
          "wordCount": 622,
          "title": "A mechanistic-based data-driven approach to accelerate structural topology optimization through finite element convolutional neural network (FE-CNN). (arXiv:2106.13652v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13683",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Tang_P/0/1/0/all/0/1\">Peipei Tang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_C/0/1/0/all/0/1\">Chengjing Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>",
          "description": "In this paper, we introduce a proximal-proximal majorization-minimization\n(PPMM) algorithm for nonconvex tuning-free robust regression problems. The\nbasic idea is to apply the proximal majorization-minimization algorithm to\nsolve the nonconvex problem with the inner subproblems solved by a sparse\nsemismooth Newton (SSN) method based proximal point algorithm (PPA). We must\nemphasize that the main difficulty in the design of the algorithm lies in how\nto overcome the singular difficulty of the inner subproblem. Furthermore, we\nalso prove that the PPMM algorithm converges to a d-stationary point. Due to\nthe Kurdyka-Lojasiewicz (KL) property of the problem, we present the\nconvergence rate of the PPMM algorithm. Numerical experiments demonstrate that\nour proposed algorithm outperforms the existing state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2106.13683",
          "publishedOn": "2021-06-28T01:57:57.317Z",
          "wordCount": 572,
          "title": "A proximal-proximal majorization-minimization algorithm for nonconvex tuning-free robust regression problems. (arXiv:2106.13683v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1\">Susheel Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Cong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neville_J/0/1/0/all/0/1\">Jennifer Neville</a>",
          "description": "Self-supervised learning of graph neural networks (GNN) is in great need\nbecause of the widespread label scarcity issue in real-world graph/network\ndata. Graph contrastive learning (GCL), by training GNNs to maximize the\ncorrespondence between the representations of the same graph in its different\naugmented forms, may yield robust and transferable GNNs even without using\nlabels. However, GNNs trained by traditional GCL often risk capturing redundant\ngraph features and thus may be brittle and provide sub-par performance in\ndownstream tasks. Here, we propose a novel principle, termed adversarial-GCL\n(AD-GCL), which enables GNNs to avoid capturing redundant information during\nthe training by optimizing adversarial graph augmentation strategies used in\nGCL. We pair AD-GCL with theoretical explanations and design a practical\ninstantiation based on trainable edge-dropping graph augmentation. We\nexperimentally validate AD-GCL by comparing with the state-of-the-art GCL\nmethods and achieve performance gains of up-to $14\\%$ in unsupervised, $6\\%$ in\ntransfer, and $3\\%$ in semi-supervised learning settings overall with 18\ndifferent benchmark datasets for the tasks of molecule property regression and\nclassification, and social network classification.",
          "link": "http://arxiv.org/abs/2106.05819",
          "publishedOn": "2021-06-28T01:57:57.310Z",
          "wordCount": 641,
          "title": "Adversarial Graph Augmentation to Improve Graph Contrastive Learning. (arXiv:2106.05819v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13790",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dhulipala_S/0/1/0/all/0/1\">S. L. N. Dhulipala</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shields_M/0/1/0/all/0/1\">M. D. Shields</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spencer_B/0/1/0/all/0/1\">B. W. Spencer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bolisetti_C/0/1/0/all/0/1\">C. Bolisetti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Slaughter_A/0/1/0/all/0/1\">A. E. Slaughter</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Laboure_V/0/1/0/all/0/1\">V. M. Laboure</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chakroborty_P/0/1/0/all/0/1\">P. Chakroborty</a>",
          "description": "While multifidelity modeling provides a cost-effective way to conduct\nuncertainty quantification with computationally expensive models, much greater\nefficiency can be achieved by adaptively deciding the number of required\nhigh-fidelity (HF) simulations, depending on the type and complexity of the\nproblem and the desired accuracy in the results. We propose a framework for\nactive learning with multifidelity modeling emphasizing the efficient\nestimation of rare events. Our framework works by fusing a low-fidelity (LF)\nprediction with an HF-inferred correction, filtering the corrected LF\nprediction to decide whether to call the high-fidelity model, and for enhanced\nsubsequent accuracy, adapting the correction for the LF prediction after every\nHF model call. The framework does not make any assumptions as to the LF model\ntype or its correlations with the HF model. In addition, for improved\nrobustness when estimating smaller failure probabilities, we propose using\ndynamic active learning functions that decide when to call the HF model. We\ndemonstrate our framework using several academic case studies and two finite\nelement (FE) model case studies: estimating Navier-Stokes velocities using the\nStokes approximation and estimating stresses in a transversely isotropic model\nsubjected to displacements via a coarsely meshed isotropic model. Across these\ncase studies, not only did the proposed framework estimate the failure\nprobabilities accurately, but compared with either Monte Carlo or a standard\nvariance reduction method, it also required only a small fraction of the calls\nto the HF model.",
          "link": "http://arxiv.org/abs/2106.13790",
          "publishedOn": "2021-06-28T01:57:57.302Z",
          "wordCount": 686,
          "title": "Active Learning with Multifidelity Modeling for Efficient Rare Event Simulation. (arXiv:2106.13790v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13559",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1\">Gabriel Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Esteve_A/0/1/0/all/0/1\">Anna Esteve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1\">Adri&#xe1;n Colomer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramos_D/0/1/0/all/0/1\">David Ramos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>",
          "description": "Recently, bladder cancer has been significantly increased in terms of\nincidence and mortality. Currently, two subtypes are known based on tumour\ngrowth: non-muscle invasive (NMIBC) and muscle-invasive bladder cancer (MIBC).\nIn this work, we focus on the MIBC subtype because it is of the worst prognosis\nand can spread to adjacent organs. We present a self-learning framework to\ngrade bladder cancer from histological images stained via immunohistochemical\ntechniques. Specifically, we propose a novel Deep Convolutional Embedded\nAttention Clustering (DCEAC) which allows classifying histological patches into\ndifferent severity levels of the disease, according to the patterns established\nin the literature. The proposed DCEAC model follows a two-step fully\nunsupervised learning methodology to discern between non-tumour, mild and\ninfiltrative patterns from high-resolution samples of 512x512 pixels. Our\nsystem outperforms previous clustering-based methods by including a\nconvolutional attention module, which allows refining the features of the\nlatent space before the classification stage. The proposed network exceeds\nstate-of-the-art approaches by 2-3% across different metrics, achieving a final\naverage accuracy of 0.9034 in a multi-class scenario. Furthermore, the reported\nclass activation maps evidence that our model is able to learn by itself the\nsame patterns that clinicians consider relevant, without incurring prior\nannotation steps. This fact supposes a breakthrough in muscle-invasive bladder\ncancer grading which bridges the gap with respect to train the model on\nlabelled data.",
          "link": "http://arxiv.org/abs/2106.13559",
          "publishedOn": "2021-06-28T01:57:57.294Z",
          "wordCount": 686,
          "title": "A Novel Self-Learning Framework for Bladder Cancer Grading Using Histopathological Images. (arXiv:2106.13559v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12133",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hien_L/0/1/0/all/0/1\">Le Thi Khanh Hien</a>, <a href=\"http://arxiv.org/find/math/1/au:+Phan_D/0/1/0/all/0/1\">Duy Nhat Phan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gillis_N/0/1/0/all/0/1\">Nicolas Gillis</a>",
          "description": "In this paper, we introduce TITAN, a novel inerTIal block majorizaTion\nminimizAtioN framework for non-smooth non-convex optimization problems. To the\nbest of our knowledge, TITAN is the first framework of block-coordinate update\nmethod that relies on the majorization-minimization framework while embedding\ninertial force to each step of the block updates. The inertial force is\nobtained via an extrapolation operator that subsumes heavy-ball and\nNesterov-type accelerations for block proximal gradient methods as special\ncases. By choosing various surrogate functions, such as proximal, Lipschitz\ngradient, Bregman, quadratic, and composite surrogate functions, and by varying\nthe extrapolation operator, TITAN produces a rich set of inertial\nblock-coordinate update methods. We study sub-sequential convergence as well as\nglobal convergence for the generated sequence of TITAN. We illustrate the\neffectiveness of TITAN on two important machine learning problems, namely\nsparse non-negative matrix factorization and matrix completion.",
          "link": "http://arxiv.org/abs/2010.12133",
          "publishedOn": "2021-06-28T01:57:57.286Z",
          "wordCount": 607,
          "title": "An Inertial Block Majorization Minimization Framework for Nonsmooth Nonconvex Optimization. (arXiv:2010.12133v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06631",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parmentier_A/0/1/0/all/0/1\">Axel Parmentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_T/0/1/0/all/0/1\">Thibaut Vidal</a>",
          "description": "Counterfactual explanations are usually generated through heuristics that are\nsensitive to the search's initial conditions. The absence of guarantees of\nperformance and robustness hinders trustworthiness. In this paper, we take a\ndisciplined approach towards counterfactual explanations for tree ensembles. We\nadvocate for a model-based search aiming at \"optimal\" explanations and propose\nefficient mixed-integer programming approaches. We show that isolation forests\ncan be modeled within our framework to focus the search on plausible\nexplanations with a low outlier score. We provide comprehensive coverage of\nadditional constraints that model important objectives, heterogeneous data\ntypes, structural constraints on the feature space, along with resource and\nactionability restrictions. Our experimental analyses demonstrate that the\nproposed search approach requires a computational effort that is orders of\nmagnitude smaller than previous mathematical programming algorithms. It scales\nup to large data sets and tree ensembles, where it provides, within seconds,\nsystematic explanations grounded on well-defined models solved to optimality.",
          "link": "http://arxiv.org/abs/2106.06631",
          "publishedOn": "2021-06-28T01:57:57.258Z",
          "wordCount": 630,
          "title": "Optimal Counterfactual Explanations in Tree Ensembles. (arXiv:2106.06631v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rommel_C/0/1/0/all/0/1\">C&#xe9;dric Rommel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1\">Thomas Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gramfort_A/0/1/0/all/0/1\">Alexandre Gramfort</a>",
          "description": "Data augmentation is a key element of deep learning pipelines, as it informs\nthe network during training about transformations of the input data that keep\nthe label unchanged. Manually finding adequate augmentation methods and\nparameters for a given pipeline is however rapidly cumbersome. In particular,\nwhile intuition can guide this decision for images, the design and choice of\naugmentation policies remains unclear for more complex types of data, such as\nneuroscience signals. Moreover, label independent strategies might not be\nsuitable for such structured data and class-dependent augmentations might be\nnecessary. This idea has been surprisingly unexplored in the literature, while\nit is quite intuitive: changing the color of a car image does not change the\nobject class to be predicted, but doing the same to the picture of an orange\ndoes. This paper aims to increase the generalization power added through\nclass-wise data augmentation. Yet, as seeking transformations depending on the\nclass largely increases the complexity of the task, using gradient-free\noptimization techniques as done by most existing automatic approaches becomes\nintractable for real-world datasets. For this reason we propose to use\ndifferentiable data augmentation amenable to gradient-based learning. EEG\nsignals are a perfect example of data for which good augmentation policies are\nmostly unknown. In this work, we demonstrate the relevance of our approach on\nthe clinically relevant sleep staging classification task, for which we also\npropose differentiable transformations.",
          "link": "http://arxiv.org/abs/2106.13695",
          "publishedOn": "2021-06-28T01:57:57.252Z",
          "wordCount": 660,
          "title": "CADDA: Class-wise Automatic Differentiable Data Augmentation for EEG Signals. (arXiv:2106.13695v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13493",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kabeli_O/0/1/0/all/0/1\">Ori Kabeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenyu Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1\">Buye Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>",
          "description": "Deep neural networks have recently shown great success in the task of blind\nsource separation, both under monaural and binaural settings. Although these\nmethods were shown to produce high-quality separations, they were mainly\napplied under offline settings, in which the model has access to the full input\nsignal while separating the signal. In this study, we convert a non-causal\nstate-of-the-art separation model into a causal and real-time model and\nevaluate its performance under both online and offline settings. We compare the\nperformance of the proposed model to several baseline methods under anechoic,\nnoisy, and noisy-reverberant recording conditions while exploring both monaural\nand binaural inputs and outputs. Our findings shed light on the relative\ndifference between causal and non-causal models when performing separation. Our\nstateful implementation for online separation leads to a minor drop in\nperformance compared to the offline model; 0.8dB for monaural inputs and 0.3dB\nfor binaural inputs while reaching a real-time factor of 0.65. Samples can be\nfound under the following link:\nhttps://kwanum.github.io/sagrnnc-stream-results/.",
          "link": "http://arxiv.org/abs/2106.13493",
          "publishedOn": "2021-06-28T01:57:57.243Z",
          "wordCount": 618,
          "title": "Online Self-Attentive Gated RNNs for Real-Time Speaker Separation. (arXiv:2106.13493v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2011.08485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yao-Yuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashtchian_C/0/1/0/all/0/1\">Cyrus Rashtchian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1\">Kamalika Chaudhuri</a>",
          "description": "Adversarial robustness has emerged as a desirable property for neural\nnetworks. Prior work shows that robust networks perform well in some\nout-of-distribution generalization tasks, such as transfer learning and outlier\ndetection. We uncover a different kind of out-of-distribution generalization\nproperty of such networks, and find that they also do well in a task that we\ncall nearest category generalization (NCG) - given an out-of-distribution\ninput, they tend to predict the same label as that of the closest training\nexample. We empirically show that this happens even when the\nout-of-distribution inputs lie outside the robustness radius of the training\ndata, which suggests that these networks may generalize better along unseen\ndirections on the natural image manifold than arbitrary unseen directions. We\nexamine how performance changes when we change the robustness regions during\ntraining. We then design experiments to investigate the connection between\nout-of-distribution detection and nearest category generalization. Taken\ntogether, our work provides evidence that robust neural networks may resemble\nnearest neighbor classifiers in their behavior on out-of-distribution data. The\ncode is available at\nhttps://github.com/yangarbiter/nearest-category-generalization",
          "link": "http://arxiv.org/abs/2011.08485",
          "publishedOn": "2021-06-28T01:57:57.235Z",
          "wordCount": 645,
          "title": "Robustness and Generalization to Nearest Categories. (arXiv:2011.08485v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murray_D/0/1/0/all/0/1\">Dakota Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jisung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kojaku_S/0/1/0/all/0/1\">Sadamori Kojaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costas_R/0/1/0/all/0/1\">Rodrigo Costas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1\">Woo-Sung Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milojevic_S/0/1/0/all/0/1\">Sta&#x161;a Milojevi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1\">Yong-Yeol Ahn</a>",
          "description": "Human mobility drives major societal phenomena including epidemics,\neconomies, and innovation. Historically, mobility was constrained by geographic\ndistance, however, in the globalizing world, language, culture, and history are\nincreasingly important. We propose using the neural embedding model word2vec\nfor studying mobility and capturing its complexity. Word2ec is shown to be\nmathematically equivalent to the gravity model of mobility, and using three\nhuman trajectory datasets, we demonstrate that it encodes nuanced relationships\nbetween locations into a vector-space, providing a measure of effective\ndistance that outperforms baselines. Focusing on the case of scientific\nmobility, we show that embeddings uncover cultural, linguistic, and\nhierarchical relationships at multiple levels of granularity. Connecting neural\nembeddings to the gravity model opens up new avenues for the study of mobility.",
          "link": "http://arxiv.org/abs/2012.02785",
          "publishedOn": "2021-06-28T01:57:57.225Z",
          "wordCount": 616,
          "title": "Unsupervised embedding of trajectories captures the latent structure of mobility. (arXiv:2012.02785v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhi-Hua Zhou</a>",
          "description": "We study the problem of Online Convex Optimization (OCO) with memory, which\nallows loss functions to depend on past decisions and thus captures temporal\neffects of learning problems. In this paper, we introduce dynamic policy regret\nas the performance measure to design algorithms robust to non-stationary\nenvironments, which competes algorithms' decisions with a sequence of changing\ncomparators. We propose a novel algorithm for OCO with memory that provably\nenjoys an optimal dynamic policy regret. The key technical challenge is how to\ncontrol the switching cost, the cumulative movements of player's decisions,\nwhich is neatly addressed by a novel decomposition of dynamic policy regret and\nan appropriate meta-expert structure. Furthermore, we apply the results to the\nproblem of online non-stochastic control, i.e., controlling a linear dynamical\nsystem with adversarial disturbance and convex loss functions. We derive a\nnovel gradient-based controller with dynamic policy regret guarantees, which is\nthe first controller competitive to a sequence of changing policies.",
          "link": "http://arxiv.org/abs/2102.03758",
          "publishedOn": "2021-06-28T01:57:57.207Z",
          "wordCount": 617,
          "title": "Non-stationary Online Learning with Memory and Non-stochastic Control. (arXiv:2102.03758v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander G. Ororbia</a>",
          "description": "In this article, we propose a novel form of unsupervised learning, continual\ncompetitive memory (CCM), as well as a computational framework to unify related\nneural models that operate under the principles of competition. The resulting\nneural system is shown to offer an effective approach for combating\ncatastrophic forgetting in online continual classification problems. We\ndemonstrate that the proposed CCM system not only outperforms other competitive\nlearning neural models but also yields performance that is competitive with\nseveral modern, state-of-the-art lifelong learning approaches on benchmarks\nsuch as Split MNIST and Split NotMNIST. CCM yields a promising path forward for\nacquiring representations that are robust to interference from data streams,\nespecially when the task is unknown to the model and must be inferred without\nexternal guidance.",
          "link": "http://arxiv.org/abs/2106.13300",
          "publishedOn": "2021-06-28T01:57:57.199Z",
          "wordCount": 555,
          "title": "Continual Competitive Memory: A Neural System for Online Task-Free Lifelong Learning. (arXiv:2106.13300v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.15082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiamang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>",
          "description": "Mixture-of-Experts (MoE) models can achieve promising results with outrageous\nlarge amount of parameters but constant computation cost, and thus it has\nbecome a trend in model scaling. Still it is a mystery how MoE layers bring\nquality gains by leveraging the parameters with sparse activation. In this\nwork, we investigate several key factors in sparse expert models. We observe\nthat load imbalance may not be a significant problem affecting model quality,\ncontrary to the perspectives of recent studies, while the number of sparsely\nactivated experts $k$ and expert capacity $C$ in top-$k$ routing can\nsignificantly make a difference in this context. Furthermore, we take a step\nforward to propose a simple method called expert prototyping that splits\nexperts into different prototypes and applies $k$ top-$1$ routing. This\nstrategy improves the model quality but maintains constant computational costs,\nand our further exploration on extremely large-scale models reflects that it is\nmore effective in training larger models. We push the model scale to over $1$\ntrillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in\ncomparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model\nachieves substantial speedup in convergence over the same-size baseline.",
          "link": "http://arxiv.org/abs/2105.15082",
          "publishedOn": "2021-06-28T01:57:57.192Z",
          "wordCount": 689,
          "title": "Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1\">Giovanni Trappolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosmo_L/0/1/0/all/0/1\">Luca Cosmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1\">Luca Moschella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1\">Riccardo Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>",
          "description": "In this paper, we propose a transformer-based procedure for the efficient\nregistration of non-rigid 3D point clouds. The proposed approach is data-driven\nand adopts for the first time the transformer architecture in the registration\ntask. Our method is general and applies to different settings. Given a fixed\ntemplate with some desired properties (e.g. skinning weights or other animation\ncues), we can register raw acquired data to it, thereby transferring all the\ntemplate properties to the input geometry. Alternatively, given a pair of\nshapes, our method can register the first onto the second (or vice-versa),\nobtaining a high-quality dense correspondence between the two. In both\ncontexts, the quality of our results enables us to target real applications\nsuch as texture transfer and shape interpolation. Furthermore, we also show\nthat including an estimation of the underlying density of the surface eases the\nlearning process. By exploiting the potential of this architecture, we can\ntrain our model requiring only a sparse set of ground truth correspondences\n($10\\sim20\\%$ of the total points). The proposed model and the analysis that we\nperform pave the way for future exploration of transformer-based architectures\nfor registration and matching applications. Qualitative and quantitative\nevaluations demonstrate that our pipeline outperforms state-of-the-art methods\nfor deformable and unordered 3D data registration on different datasets and\nscenarios.",
          "link": "http://arxiv.org/abs/2106.13679",
          "publishedOn": "2021-06-28T01:57:57.183Z",
          "wordCount": 657,
          "title": "Shape registration in the time of transformers. (arXiv:2106.13679v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1\">Minjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_j/0/1/0/all/0/1\">jinhong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joonseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1\">Hyunjung Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongwuk Lee</a>",
          "description": "Session-based recommendation aims at predicting the next item given a\nsequence of previous items consumed in the session, e.g., on e-commerce or\nmultimedia streaming services. Specifically, session data exhibits some unique\ncharacteristics, i.e., session consistency and sequential dependency over items\nwithin the session, repeated item consumption, and session timeliness. In this\npaper, we propose simple-yet-effective linear models for considering the\nholistic aspects of the sessions. The comprehensive nature of our models helps\nimprove the quality of session-based recommendation. More importantly, it\nprovides a generalized framework for reflecting different perspectives of\nsession data. Furthermore, since our models can be solved by closed-form\nsolutions, they are highly scalable. Experimental results demonstrate that the\nproposed linear models show competitive or state-of-the-art performance in\nvarious metrics on several real-world datasets.",
          "link": "http://arxiv.org/abs/2103.16104",
          "publishedOn": "2021-06-28T01:57:57.173Z",
          "wordCount": 599,
          "title": "Session-aware Linear Item-Item Models for Session-based Recommendation. (arXiv:2103.16104v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12525",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1\">Varun A. Kelkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>",
          "description": "Obtaining a useful estimate of an object from highly incomplete imaging\nmeasurements remains a holy grail of imaging science. Deep learning methods\nhave shown promise in learning object priors or constraints to improve the\nconditioning of an ill-posed imaging inverse problem. In this study, a\nframework for estimating an object of interest that is semantically related to\na known prior image, is proposed. An optimization problem is formulated in the\ndisentangled latent space of a style-based generative model, and semantically\nmeaningful constraints are imposed using the disentangled latent representation\nof the prior image. Stable recovery from incomplete measurements with the help\nof a prior image is theoretically analyzed. Numerical experiments demonstrating\nthe superior performance of our approach as compared to related methods are\npresented.",
          "link": "http://arxiv.org/abs/2102.12525",
          "publishedOn": "2021-06-28T01:57:57.154Z",
          "wordCount": 603,
          "title": "Prior Image-Constrained Reconstruction using Style-Based Generative Models. (arXiv:2102.12525v2 [eess.IV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13092",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Caragea_A/0/1/0/all/0/1\">A. Caragea</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lee_D/0/1/0/all/0/1\">D.G. Lee</a>, <a href=\"http://arxiv.org/find/math/1/au:+Maly_J/0/1/0/all/0/1\">J. Maly</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pfander_G/0/1/0/all/0/1\">G. Pfander</a>, <a href=\"http://arxiv.org/find/math/1/au:+Voigtlaender_F/0/1/0/all/0/1\">F. Voigtlaender</a>",
          "description": "Until recently, applications of neural networks in machine learning have\nalmost exclusively relied on real-valued networks. It was recently observed,\nhowever, that complex-valued neural networks (CVNNs) exhibit superior\nperformance in applications in which the input is naturally complex-valued,\nsuch as MRI fingerprinting. While the mathematical theory of real-valued\nnetworks has, by now, reached some level of maturity, this is far from true for\ncomplex-valued networks. In this paper, we analyze the expressivity of\ncomplex-valued networks by providing explicit quantitative error bounds for\napproximating $C^n$ functions on compact subsets of $\\mathbb{C}^d$ by\ncomplex-valued neural networks that employ the modReLU activation function,\ngiven by $\\sigma(z) = \\mathrm{ReLU}(|z| - 1) \\, \\mathrm{sgn} (z)$, which is one\nof the most popular complex activation functions used in practice. We show that\nthe derived approximation rates are optimal (up to log factors) in the class of\nmodReLU networks with weights of moderate growth.",
          "link": "http://arxiv.org/abs/2102.13092",
          "publishedOn": "2021-06-28T01:57:57.147Z",
          "wordCount": 603,
          "title": "Quantitative approximation results for complex-valued neural networks. (arXiv:2102.13092v2 [math.FA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07006",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kim_J/0/1/0/all/0/1\">Junhyung Lyle Kim</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kollias_G/0/1/0/all/0/1\">George Kollias</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kalev_A/0/1/0/all/0/1\">Amir Kalev</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wei_K/0/1/0/all/0/1\">Ken X. Wei</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "We propose a new quantum state reconstruction method that combines ideas from\ncompressed sensing, non-convex optimization, and acceleration methods. The\nalgorithm, called Momentum-Inspired Factored Gradient Descent (\\texttt{MiFGD}),\nextends the applicability of quantum tomography for larger systems. Despite\nbeing a non-convex method, \\texttt{MiFGD} converges \\emph{provably} to the true\ndensity matrix at a linear rate, in the absence of experimental and statistical\nnoise, and under common assumptions. With this manuscript, we present the\nmethod, prove its convergence property and provide Frobenius norm bound\nguarantees with respect to the true density matrix. From a practical point of\nview, we benchmark the algorithm performance with respect to other existing\nmethods, in both synthetic and real experiments performed on an IBM's quantum\nprocessing unit. We find that the proposed algorithm performs orders of\nmagnitude faster than state of the art approaches, with the same or better\naccuracy. In both synthetic and real experiments, we observed accurate and\nrobust reconstruction, despite experimental and statistical noise in the\ntomographic data. Finally, we provide a ready-to-use code for state tomography\nof multi-qubit systems.",
          "link": "http://arxiv.org/abs/2104.07006",
          "publishedOn": "2021-06-28T01:57:57.141Z",
          "wordCount": 656,
          "title": "Fast quantum state reconstruction via accelerated non-convex programming. (arXiv:2104.07006v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Durasov_N/0/1/0/all/0/1\">Nikita Durasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1\">Pierre Baque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>",
          "description": "Deep neural networks have amply demonstrated their prowess but estimating the\nreliability of their predictions remains challenging. Deep Ensembles are widely\nconsidered as being one of the best methods for generating uncertainty\nestimates but are very expensive to train and evaluate. MC-Dropout is another\npopular alternative, which is less expensive, but also less reliable. Our\ncentral intuition is that there is a continuous spectrum of ensemble-like\nmodels of which MC-Dropout and Deep Ensembles are extreme examples. The first\nuses an effectively infinite number of highly correlated models while the\nsecond relies on a finite number of independent models.\n\nTo combine the benefits of both, we introduce Masksembles. Instead of\nrandomly dropping parts of the network as in MC-dropout, Masksemble relies on a\nfixed number of binary masks, which are parameterized in a way that allows to\nchange correlations between individual models. Namely, by controlling the\noverlap between the masks and their density one can choose the optimal\nconfiguration for the task at hand. This leads to a simple and easy to\nimplement method with performance on par with Ensembles at a fraction of the\ncost. We experimentally validate Masksembles on two widely used datasets,\nCIFAR10 and ImageNet.",
          "link": "http://arxiv.org/abs/2012.08334",
          "publishedOn": "2021-06-28T01:57:57.134Z",
          "wordCount": 672,
          "title": "Masksembles for Uncertainty Estimation. (arXiv:2012.08334v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haijin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Caomingzhe Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junhua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fushuan Wen</a>",
          "description": "Non-intrusive load monitoring (NILM) is essential for understanding\ncustomer's power consumption patterns and may find wide applications like\ncarbon emission reduction and energy conservation. The training of NILM models\nrequires massive load data containing different types of appliances. However,\ninadequate load data and the risk of power consumer privacy breaches may be\nencountered by local data owners during the NILM model training. To prevent\nsuch potential risks, a novel NILM method named Fed-NILM which is based on\nFederated Learning (FL) is proposed in this paper. In Fed-NILM, local model\nparameters instead of local load data are shared among multiple data owners.\nThe global model is obtained by weighted averaging the parameters. Experiments\nbased on two measured load datasets are conducted to explore the generalization\nability of Fed-NILM. Besides, a comparison of Fed-NILM with locally-trained\nNILMs and the centrally-trained NILM is conducted. The experimental results\nshow that Fed-NILM has superior performance in scalability and convergence.\nFed-NILM outperforms locally-trained NILMs operated by local data owners and\napproximates the centrally-trained NILM which is trained on the entire load\ndataset without privacy protection. The proposed Fed-NILM significantly\nimproves the co-modeling capabilities of local data owners while protecting\npower consumers' privacy.",
          "link": "http://arxiv.org/abs/2105.11085",
          "publishedOn": "2021-06-28T01:57:57.126Z",
          "wordCount": 664,
          "title": "Fed-NILM: A Federated Learning-based Non-Intrusive Load Monitoring Method for Privacy-Protection. (arXiv:2105.11085v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12756",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Beck_E/0/1/0/all/0/1\">Edgar Beck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bockelmann_C/0/1/0/all/0/1\">Carsten Bockelmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dekorsy_A/0/1/0/all/0/1\">Armin Dekorsy</a>",
          "description": "Following the great success of Machine Learning (ML), especially Deep Neural\nNetworks (DNNs), in many research domains in 2010s, several ML-based approaches\nwere proposed for detection in large inverse linear problems, e.g., massive\nMIMO systems. The main motivation behind is that the complexity of Maximum\nA-Posteriori (MAP) detection grows exponentially with system dimensions.\nInstead of using DNNs, essentially being a black-box, we take a slightly\ndifferent approach and introduce a probabilistic Continuous relaxation of\ndisCrete variables to MAP detection. Enabling close approximation and\ncontinuous optimization, we derive an iterative detection algorithm: Concrete\nMAP Detection (CMD). Furthermore, extending CMD by the idea of deep unfolding\ninto CMDNet, we allow for (online) optimization of a small number of parameters\nto different working points while limiting complexity. In contrast to recent\nDNN-based approaches, we select the optimization criterion and output of CMDNet\nbased on information theory and are thus able to learn approximate\nprobabilities of the individual optimal detector. This is crucial for soft\ndecoding in today's communication systems. Numerical simulation results in MIMO\nsystems reveal CMDNet to feature a promising accuracy complexity trade-off\ncompared to State of the Art. Notably, we demonstrate CMDNet's soft outputs to\nbe reliable for decoders.",
          "link": "http://arxiv.org/abs/2102.12756",
          "publishedOn": "2021-06-28T01:57:57.120Z",
          "wordCount": 683,
          "title": "Learning a Probabilistic Relaxation of Discrete Variables for Soft Detection with Low Complexity: CMDNet. (arXiv:2102.12756v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05397",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Stankewitz_B/0/1/0/all/0/1\">Bernhard Stankewitz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mucke_N/0/1/0/all/0/1\">Nicole M&#xfc;cke</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1\">Lorenzo Rosasco</a>",
          "description": "Optimization was recently shown to control the inductive bias in a learning\nprocess, a property referred to as implicit, or iterative regularization. The\nestimator obtained iteratively minimizing the training error can generalise\nwell with no need of further penalties or constraints. In this paper, we\ninvestigate this phenomenon in the context of linear models with smooth loss\nfunctions. In particular, we investigate and propose a proof technique\ncombining ideas from inexact optimization and probability theory, specifically\ngradient concentration. The proof is easy to follow and allows to obtain sharp\nlearning bounds. More generally, it highlights a way to develop optimization\nresults into learning guarantees.",
          "link": "http://arxiv.org/abs/2106.05397",
          "publishedOn": "2021-06-28T01:57:57.100Z",
          "wordCount": 559,
          "title": "From inexact optimization to learning via gradient concentration. (arXiv:2106.05397v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Segal_S/0/1/0/all/0/1\">Shahar Segal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinkas_B/0/1/0/all/0/1\">Benny Pinkas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baum_C/0/1/0/all/0/1\">Carsten Baum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_C/0/1/0/all/0/1\">Chaya Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshet_J/0/1/0/all/0/1\">Joseph Keshet</a>",
          "description": "We present a framework that allows to certify the fairness degree of a model\nbased on an interactive and privacy-preserving test. The framework verifies any\ntrained model, regardless of its training process and architecture. Thus, it\nallows us to evaluate any deep learning model on multiple fairness definitions\nempirically. We tackle two scenarios, where either the test data is privately\navailable only to the tester or is publicly known in advance, even to the model\ncreator. We investigate the soundness of the proposed approach using\ntheoretical analysis and present statistical guarantees for the interactive\ntest. Finally, we provide a cryptographic technique to automate fairness\ntesting and certified inference with only black-box access to the model at hand\nwhile hiding the participants' sensitive data.",
          "link": "http://arxiv.org/abs/2009.01534",
          "publishedOn": "2021-06-28T01:57:57.083Z",
          "wordCount": 617,
          "title": "Fairness in the Eyes of the Data: Certifying Machine-Learning Models. (arXiv:2009.01534v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Appleby_G/0/1/0/all/0/1\">Gabriel Appleby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espadoto_M/0/1/0/all/0/1\">Mateus Espadoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goree_S/0/1/0/all/0/1\">Samuel Goree</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telea_A/0/1/0/all/0/1\">Alexandru Telea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_E/0/1/0/all/0/1\">Erik W Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1\">Remco Chang</a>",
          "description": "Projection algorithms such as t-SNE or UMAP are useful for the visualization\nof high dimensional data, but depend on hyperparameters which must be tuned\ncarefully. Unfortunately, iteratively recomputing projections to find the\noptimal hyperparameter value is computationally intensive and unintuitive due\nto the stochastic nature of these methods. In this paper we propose HyperNP, a\nscalable method that allows for real-time interactive hyperparameter\nexploration of projection methods by training neural network approximations.\nHyperNP can be trained on a fraction of the total data instances and\nhyperparameter configurations and can compute projections for new data and\nhyperparameters at interactive speeds. HyperNP is compact in size and fast to\ncompute, thus allowing it to be embedded in lightweight visualization systems\nsuch as web browsers. We evaluate the performance of the HyperNP across three\ndatasets in terms of performance and speed. The results suggest that HyperNP is\naccurate, scalable, interactive, and appropriate for use in real-world\nsettings.",
          "link": "http://arxiv.org/abs/2106.13777",
          "publishedOn": "2021-06-28T01:57:57.070Z",
          "wordCount": 595,
          "title": "HyperNP: Interactive Visual Exploration of Multidimensional Projection Hyperparameters. (arXiv:2106.13777v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.02470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sutter_T/0/1/0/all/0/1\">Thomas M. Sutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daunhawer_I/0/1/0/all/0/1\">Imant Daunhawer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1\">Julia E. Vogt</a>",
          "description": "Multiple data types naturally co-occur when describing real-world phenomena\nand learning from them is a long-standing goal in machine learning research.\nHowever, existing self-supervised generative models approximating an ELBO are\nnot able to fulfill all desired requirements of multimodal models: their\nposterior approximation functions lead to a trade-off between the semantic\ncoherence and the ability to learn the joint data distribution. We propose a\nnew, generalized ELBO formulation for multimodal data that overcomes these\nlimitations. The new objective encompasses two previous methods as special\ncases and combines their benefits without compromises. In extensive\nexperiments, we demonstrate the advantage of the proposed method compared to\nstate-of-the-art models in self-supervised, generative learning tasks.",
          "link": "http://arxiv.org/abs/2105.02470",
          "publishedOn": "2021-06-28T01:57:57.063Z",
          "wordCount": 566,
          "title": "Generalized Multimodal ELBO. (arXiv:2105.02470v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cunnington_D/0/1/0/all/0/1\">Daniel Cunnington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1\">Alessandra Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Mark Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1\">Jorge Lobo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_L/0/1/0/all/0/1\">Lance Kaplan</a>",
          "description": "Inductive Logic Programming (ILP) systems learn generalised, interpretable\nrules in a data-efficient manner utilising existing background knowledge.\nHowever, current ILP systems require training examples to be specified in a\nstructured logical format. Neural networks learn from unstructured data,\nalthough their learned models may be difficult to interpret and are vulnerable\nto data perturbations at run-time. This paper introduces a hybrid\nneural-symbolic learning framework, called NSL, that learns interpretable rules\nfrom labelled unstructured data. NSL combines pre-trained neural networks for\nfeature extraction with FastLAS, a state-of-the-art ILP system for rule\nlearning under the answer set semantics. Features extracted by the neural\ncomponents define the structured context of labelled examples and the\nconfidence of the neural predictions determines the level of noise of the\nexamples. Using the scoring function of FastLAS, NSL searches for short,\ninterpretable rules that generalise over such noisy examples. We evaluate our\nframework on propositional and first-order classification tasks using the MNIST\ndataset as raw data. Specifically, we demonstrate that NSL is able to learn\nrobust rules from perturbed MNIST data and achieve comparable or superior\naccuracy when compared to neural network and random forest baselines whilst\nbeing more general and interpretable.",
          "link": "http://arxiv.org/abs/2012.05023",
          "publishedOn": "2021-06-28T01:57:57.025Z",
          "wordCount": 667,
          "title": "NSL: Hybrid Interpretable Learning From Noisy Raw Data. (arXiv:2012.05023v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Millidge_B/0/1/0/all/0/1\">Beren Millidge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschantz_A/0/1/0/all/0/1\">Alexander Tschantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Anil Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buckley_C/0/1/0/all/0/1\">Christopher Buckley</a>",
          "description": "The exploration-exploitation trade-off is central to the description of\nadaptive behaviour in fields ranging from machine learning, to biology, to\neconomics. While many approaches have been taken, one approach to solving this\ntrade-off has been to equip or propose that agents possess an intrinsic\n'exploratory drive' which is often implemented in terms of maximizing the\nagents information gain about the world -- an approach which has been widely\nstudied in machine learning and cognitive science. In this paper we\nmathematically investigate the nature and meaning of such approaches and\ndemonstrate that this combination of utility maximizing and information-seeking\nbehaviour arises from the minimization of an entirely difference class of\nobjectives we call divergence objectives. We propose a dichotomy in the\nobjective functions underlying adaptive behaviour between \\emph{evidence}\nobjectives, which correspond to well-known reward or utility maximizing\nobjectives in the literature, and \\emph{divergence} objectives which instead\nseek to minimize the divergence between the agent's expected and desired\nfutures, and argue that this new class of divergence objectives could form the\nmathematical foundation for a much richer understanding of the exploratory\ncomponents of adaptive and intelligent action, beyond simply greedy utility\nmaximization.",
          "link": "http://arxiv.org/abs/2103.06859",
          "publishedOn": "2021-06-28T01:57:57.018Z",
          "wordCount": 694,
          "title": "Understanding the Origin of Information-Seeking Exploration in Probabilistic Objectives for Control. (arXiv:2103.06859v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13682",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guan_Z/0/1/0/all/0/1\">Zoe Guan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Parmigiani_G/0/1/0/all/0/1\">Giovanni Parmigiani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Braun_D/0/1/0/all/0/1\">Danielle Braun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Trippa_L/0/1/0/all/0/1\">Lorenzo Trippa</a>",
          "description": "Family history is a major risk factor for many types of cancer. Mendelian\nrisk prediction models translate family histories into cancer risk predictions\nbased on knowledge of cancer susceptibility genes. These models are widely used\nin clinical practice to help identify high-risk individuals. Mendelian models\nleverage the entire family history, but they rely on many assumptions about\ncancer susceptibility genes that are either unrealistic or challenging to\nvalidate due to low mutation prevalence. Training more flexible models, such as\nneural networks, on large databases of pedigrees can potentially lead to\naccuracy gains. In this paper, we develop a framework to apply neural networks\nto family history data and investigate their ability to learn inherited\nsusceptibility to cancer. While there is an extensive literature on neural\nnetworks and their state-of-the-art performance in many tasks, there is little\nwork applying them to family history data. We propose adaptations of\nfully-connected neural networks and convolutional neural networks to pedigrees.\nIn data simulated under Mendelian inheritance, we demonstrate that our proposed\nneural network models are able to achieve nearly optimal prediction\nperformance. Moreover, when the observed family history includes misreported\ncancer diagnoses, neural networks are able to outperform the Mendelian BRCAPRO\nmodel embedding the correct inheritance laws. Using a large dataset of over\n200,000 family histories, the Risk Service cohort, we train prediction models\nfor future risk of breast cancer. We validate the models using data from the\nCancer Genetics Network.",
          "link": "http://arxiv.org/abs/2106.13682",
          "publishedOn": "2021-06-28T01:57:56.949Z",
          "wordCount": 669,
          "title": "Prediction of Hereditary Cancers Using Neural Networks. (arXiv:2106.13682v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2007.11752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Ting-Wu Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1\">Ari S. Morcos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1\">Diana Marculescu</a>",
          "description": "Slimmable neural networks provide a flexible trade-off front between\nprediction error and computational requirement (such as the number of\nfloating-point operations or FLOPs) with the same storage requirement as a\nsingle model. They are useful for reducing maintenance overhead for deploying\nmodels to devices with different memory constraints and are useful for\noptimizing the efficiency of a system with many CNNs. However, existing\nslimmable network approaches either do not optimize layer-wise widths or\noptimize the shared-weights and layer-wise widths independently, thereby\nleaving significant room for improvement by joint width and weight\noptimization. In this work, we propose a general framework to enable joint\noptimization for both width configurations and weights of slimmable networks.\nOur framework subsumes conventional and NAS-based slimmable methods as special\ncases and provides flexibility to improve over existing methods. From a\npractical standpoint, we propose Joslim, an algorithm that jointly optimizes\nboth the widths and weights for slimmable nets, which outperforms existing\nmethods for optimizing slimmable networks across various networks, datasets,\nand objectives. Quantitatively, improvements up to 1.7% and 8% in top-1\naccuracy on the ImageNet dataset can be attained for MobileNetV2 considering\nFLOPs and memory footprint, respectively. Our results highlight the potential\nof optimizing the channel counts for different layers jointly with the weights\nfor slimmable networks. Code available at https://github.com/cmu-enyac/Joslim.",
          "link": "http://arxiv.org/abs/2007.11752",
          "publishedOn": "2021-06-28T01:57:56.934Z",
          "wordCount": 730,
          "title": "Joslim: Joint Widths and Weights Optimization for Slimmable Neural Networks. (arXiv:2007.11752v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loveland_D/0/1/0/all/0/1\">Donald Loveland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shusen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiszpanski_A/0/1/0/all/0/1\">Anna Hiszpanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yong Han</a>",
          "description": "Graph neural network (GNN) explanations have largely been facilitated through\npost-hoc introspection. While this has been deemed successful, many post-hoc\nexplanation methods have been shown to fail in capturing a model's learned\nrepresentation. Due to this problem, it is worthwhile to consider how one might\ntrain a model so that it is more amenable to post-hoc analysis. Given the\nsuccess of adversarial training in the computer vision domain to train models\nwith more reliable representations, we propose a similar training paradigm for\nGNNs and analyze the respective impact on a model's explanations. In instances\nwithout ground truth labels, we also determine how well an explanation method\nis utilizing a model's learned representation through a new metric and\ndemonstrate adversarial training can help better extract domain-relevant\ninsights in chemistry.",
          "link": "http://arxiv.org/abs/2106.13427",
          "publishedOn": "2021-06-28T01:57:56.911Z",
          "wordCount": 577,
          "title": "Reliable Graph Neural Network Explanations Through Adversarial Training. (arXiv:2106.13427v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1\">Siu Ming Yiu</a>",
          "description": "Graphs have been widely used in data mining and machine learning due to their\nunique representation of real-world objects and their interactions. As graphs\nare getting bigger and bigger nowadays, it is common to see their subgraphs\nseparately collected and stored in multiple local systems. Therefore, it is\nnatural to consider the subgraph federated learning setting, where each local\nsystem holding a small subgraph that may be biased from the distribution of the\nwhole graph. Hence, the subgraph federated learning aims to collaboratively\ntrain a powerful and generalizable graph mining model without directly sharing\ntheir graph data. In this work, towards the novel yet realistic setting of\nsubgraph federated learning, we propose two major techniques: (1) FedSage,\nwhich trains a GraphSage model based on FedAvg to integrate node features, link\nstructures, and task labels on multiple local subgraphs; (2) FedSage+, which\ntrains a missing neighbor generator along FedSage to deal with missing links\nacross local subgraphs. Empirical results on four real-world graph datasets\nwith synthesized subgraph federated learning settings demonstrate the\neffectiveness and efficiency of our proposed techniques. At the same time,\nconsistent theoretical implications are made towards their generalization\nability on the global graphs.",
          "link": "http://arxiv.org/abs/2106.13430",
          "publishedOn": "2021-06-28T01:57:56.893Z",
          "wordCount": 632,
          "title": "Subgraph Federated Learning with Missing Neighbor Generation. (arXiv:2106.13430v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Singh Chaplot</a>",
          "description": "Breakthroughs in machine learning in the last decade have led to `digital\nintelligence', i.e. machine learning models capable of learning from vast\namounts of labeled data to perform several digital tasks such as speech\nrecognition, face recognition, machine translation and so on. The goal of this\nthesis is to make progress towards designing algorithms capable of `physical\nintelligence', i.e. building intelligent autonomous navigation agents capable\nof learning to perform complex navigation tasks in the physical world involving\nvisual perception, natural language understanding, reasoning, planning, and\nsequential decision making. Despite several advances in classical navigation\nmethods in the last few decades, current navigation agents struggle at\nlong-term semantic navigation tasks. In the first part of the thesis, we\ndiscuss our work on short-term navigation using end-to-end reinforcement\nlearning to tackle challenges such as obstacle avoidance, semantic perception,\nlanguage grounding, and reasoning. In the second part, we present a new class\nof navigation methods based on modular learning and structured explicit map\nrepresentations, which leverage the strengths of both classical and end-to-end\nlearning methods, to tackle long-term navigation tasks. We show that these\nmethods are able to effectively tackle challenges such as localization,\nmapping, long-term planning, exploration and learning semantic priors. These\nmodular learning methods are capable of long-term spatial and semantic\nunderstanding and achieve state-of-the-art results on various navigation tasks.",
          "link": "http://arxiv.org/abs/2106.13415",
          "publishedOn": "2021-06-28T01:57:56.885Z",
          "wordCount": 671,
          "title": "Building Intelligent Autonomous Navigation Agents. (arXiv:2106.13415v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhifeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1\">Kamalika Chaudhuri</a>",
          "description": "Normalizing flows are a class of flexible deep generative models that offer\neasy likelihood computation. Despite their empirical success, there is little\ntheoretical understanding of their expressiveness. In this work, we study\nresidual flows, a class of normalizing flows composed of Lipschitz residual\nblocks. We prove residual flows are universal approximators in maximum mean\ndiscrepancy. We provide upper bounds on the number of residual blocks to\nachieve approximation under different assumptions.",
          "link": "http://arxiv.org/abs/2103.05793",
          "publishedOn": "2021-06-28T01:57:56.876Z",
          "wordCount": 532,
          "title": "Universal Approximation of Residual Flows in Maximum Mean Discrepancy. (arXiv:2103.05793v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tuan Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>",
          "description": "Inspired by a new coded computation algorithm for invertible functions, we\npropose Coded-InvNet a new approach to design resilient prediction serving\nsystems that can gracefully handle stragglers or node failures. Coded-InvNet\nleverages recent findings in the deep learning literature such as invertible\nneural networks, Manifold Mixup, and domain translation algorithms, identifying\ninteresting research directions that span across machine learning and systems.\nOur experimental results show that Coded-InvNet can outperform existing\napproaches, especially when the compute resource overhead is as low as 10%. For\ninstance, without knowing which of the ten workers is going to fail, our\nalgorithm can design a backup task so that it can correctly recover the missing\nprediction result with an accuracy of 85.9%, significantly outperforming the\nprevious SOTA by 32.5%.",
          "link": "http://arxiv.org/abs/2106.06445",
          "publishedOn": "2021-06-28T01:57:56.870Z",
          "wordCount": 569,
          "title": "Coded-InvNet for Resilient Prediction Serving Systems. (arXiv:2106.06445v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03326",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mondelli_M/0/1/0/all/0/1\">Marco Mondelli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thrampoulidis_C/0/1/0/all/0/1\">Christos Thrampoulidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Venkataramanan_R/0/1/0/all/0/1\">Ramji Venkataramanan</a>",
          "description": "We study the problem of recovering an unknown signal $\\boldsymbol x$ given\nmeasurements obtained from a generalized linear model with a Gaussian sensing\nmatrix. Two popular solutions are based on a linear estimator $\\hat{\\boldsymbol\nx}^{\\rm L}$ and a spectral estimator $\\hat{\\boldsymbol x}^{\\rm s}$. The former\nis a data-dependent linear combination of the columns of the measurement\nmatrix, and its analysis is quite simple. The latter is the principal\neigenvector of a data-dependent matrix, and a recent line of work has studied\nits performance. In this paper, we show how to optimally combine\n$\\hat{\\boldsymbol x}^{\\rm L}$ and $\\hat{\\boldsymbol x}^{\\rm s}$. At the heart\nof our analysis is the exact characterization of the joint empirical\ndistribution of $(\\boldsymbol x, \\hat{\\boldsymbol x}^{\\rm L}, \\hat{\\boldsymbol\nx}^{\\rm s})$ in the high-dimensional limit. This allows us to compute the\nBayes-optimal combination of $\\hat{\\boldsymbol x}^{\\rm L}$ and\n$\\hat{\\boldsymbol x}^{\\rm s}$, given the limiting distribution of the signal\n$\\boldsymbol x$. When the distribution of the signal is Gaussian, then the\nBayes-optimal combination has the form $\\theta\\hat{\\boldsymbol x}^{\\rm\nL}+\\hat{\\boldsymbol x}^{\\rm s}$ and we derive the optimal combination\ncoefficient. In order to establish the limiting distribution of $(\\boldsymbol\nx, \\hat{\\boldsymbol x}^{\\rm L}, \\hat{\\boldsymbol x}^{\\rm s})$, we design and\nanalyze an Approximate Message Passing (AMP) algorithm whose iterates give\n$\\hat{\\boldsymbol x}^{\\rm L}$ and approach $\\hat{\\boldsymbol x}^{\\rm s}$.\nNumerical simulations demonstrate the improvement of the proposed combination\nwith respect to the two methods considered separately.",
          "link": "http://arxiv.org/abs/2008.03326",
          "publishedOn": "2021-06-28T01:57:56.851Z",
          "wordCount": 713,
          "title": "Optimal Combination of Linear and Spectral Estimators for Generalized Linear Models. (arXiv:2008.03326v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfke_D/0/1/0/all/0/1\">Dominik Alfke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gondos_M/0/1/0/all/0/1\">Miriam Gondos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peroche_L/0/1/0/all/0/1\">Lucile Peroche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoll_M/0/1/0/all/0/1\">Martin Stoll</a>",
          "description": "Time series data play an important role in many applications and their\nanalysis reveals crucial information for understanding the underlying\nprocesses. Among the many time series learning tasks of great importance, we\nhere focus on semi-supervised learning based on a graph representation of the\ndata. Two main aspects are involved in this task. A suitable distance measure\nto evaluate the similarities between time series, and a learning method to make\npredictions based on these distances. However, the relationship between the two\naspects has never been studied systematically in the context of graph-based\nlearning. We describe four different distance measures, including (Soft) DTW\nand MPDist, a distance measure based on the Matrix Profile, as well as four\nsuccessful semi-supervised learning methods, including the graph Allen--Cahn\nmethod and a Graph Convolutional Neural Network. We then compare the\nperformance of the algorithms on binary classification data sets. In our\nfindings we compare the chosen graph-based methods using all distance measures\nand observe that the results vary strongly with respect to the accuracy. As\npredicted by the ``no free lunch'' theorem, no clear best combination to employ\nin all cases is found. Our study provides a reproducible framework for future\nwork in the direction of semi-supervised learning for time series with a focus\non graph representations.",
          "link": "http://arxiv.org/abs/2104.08153",
          "publishedOn": "2021-06-28T01:57:56.841Z",
          "wordCount": 683,
          "title": "An Empirical Study of Graph-Based Approaches for Semi-Supervised Time Series Classification. (arXiv:2104.08153v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Luwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "Although many techniques have been applied to matrix factorization (MF), they\nmay not fully exploit the feature structure. In this paper, we incorporate the\ngrouping effect into MF and propose a novel method called Robust Matrix\nFactorization with Grouping effect (GRMF). The grouping effect is a\ngeneralization of the sparsity effect, which conducts denoising by clustering\nsimilar values around multiple centers instead of just around 0. Compared with\nexisting algorithms, the proposed GRMF can automatically learn the grouping\nstructure and sparsity in MF without prior knowledge, by introducing a\nnaturally adjustable non-convex regularization to achieve simultaneous sparsity\nand grouping effect. Specifically, GRMF uses an efficient alternating\nminimization framework to perform MF, in which the original non-convex problem\nis first converted into a convex problem through Difference-of-Convex (DC)\nprogramming, and then solved by Alternating Direction Method of Multipliers\n(ADMM). In addition, GRMF can be easily extended to the Non-negative Matrix\nFactorization (NMF) settings. Extensive experiments have been conducted using\nreal-world data sets with outliers and contaminated noise, where the\nexperimental results show that GRMF has promoted performance and robustness,\ncompared to five benchmark algorithms.",
          "link": "http://arxiv.org/abs/2106.13681",
          "publishedOn": "2021-06-28T01:57:56.814Z",
          "wordCount": 626,
          "title": "Robust Matrix Factorization with Grouping Effect. (arXiv:2106.13681v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shokry_A/0/1/0/all/0/1\">Ahmed Shokry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torki_M/0/1/0/all/0/1\">Marwan Torki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youssef_M/0/1/0/all/0/1\">Moustafa Youssef</a>",
          "description": "Recent years have witnessed fast growth in outdoor location-based services.\nWhile GPS is considered a ubiquitous localization system, it is not supported\nby low-end phones, requires direct line of sight to the satellites, and can\ndrain the phone battery quickly.\n\nIn this paper, we propose DeepLoc: a deep learning-based outdoor localization\nsystem that obtains GPS-like localization accuracy without its limitations. In\nparticular, DeepLoc leverages the ubiquitous cellular signals received from the\ndifferent cell towers heard by the mobile device as hints to localize it. To do\nthat, crowd-sensed geo-tagged received signal strength information coming from\ndifferent cell towers is used to train a deep model that is used to infer the\nuser's position. As part of DeepLoc design, we introduce modules to address a\nnumber of practical challenges including scaling the data collection to large\nareas, handling the inherent noise in the cellular signal and geo-tagged data,\nas well as providing enough data that is required for deep learning models with\nlow-overhead.\n\nWe implemented DeepLoc on different Android devices. Evaluation results in\nrealistic urban and rural environments show that DeepLoc can achieve a median\nlocalization accuracy within 18.8m in urban areas and within 15.7m in rural\nareas. This accuracy outperforms the state-of-the-art cellular-based systems by\nmore than 470% and comes with 330% savings in power compared to the GPS. This\nhighlights the promise of DeepLoc as a ubiquitous accurate and low-overhead\nlocalization system.",
          "link": "http://arxiv.org/abs/2106.13632",
          "publishedOn": "2021-06-28T01:57:56.808Z",
          "wordCount": 690,
          "title": "DeepLoc: A Ubiquitous Accurate and Low-Overhead Outdoor Cellular Localization System. (arXiv:2106.13632v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Ting-Kuei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1\">Fernando Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1\">Brian M. Sadler</a>",
          "description": "In this paper, we present a perception-action-communication loop design using\nVision-based Graph Aggregation and Inference (VGAI). This multi-agent\ndecentralized learning-to-control framework maps raw visual observations to\nagent actions, aided by local communication among neighboring agents. Our\nframework is implemented by a cascade of a convolutional and a graph neural\nnetwork (CNN / GNN), addressing agent-level visual perception and feature\nlearning, as well as swarm-level communication, local information aggregation\nand agent action inference, respectively. By jointly training the CNN and GNN,\nimage features and communication messages are learned in conjunction to better\naddress the specific task. We use imitation learning to train the VGAI\ncontroller in an offline phase, relying on a centralized expert controller.\nThis results in a learned VGAI controller that can be deployed in a distributed\nmanner for online execution. Additionally, the controller exhibits good scaling\nproperties, with training in smaller teams and application in larger teams.\nThrough a multi-agent flocking application, we demonstrate that VGAI yields\nperformance comparable to or better than other decentralized controllers, using\nonly the visual input modality and without accessing precise location or motion\nstate information.",
          "link": "http://arxiv.org/abs/2106.13358",
          "publishedOn": "2021-06-28T01:57:56.797Z",
          "wordCount": 639,
          "title": "Scalable Perception-Action-Communication Loops with Convolutional and Graph Neural Networks. (arXiv:2106.13358v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sachan_R/0/1/0/all/0/1\">Rohit Kumar Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Rachit Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1\">Sandeep Kumar Shukla</a>",
          "description": "The rise in the adoption of blockchain technology has led to increased\nillegal activities by cyber-criminals costing billions of dollars. Many machine\nlearning algorithms are applied to detect such illegal behavior. These\nalgorithms are often trained on the transaction behavior and, in some cases,\ntrained on the vulnerabilities that exist in the system. In our approach, we\nstudy the feasibility of using metadata such as Domain Name (DN) associated\nwith the account in the blockchain and identify whether an account should be\ntagged malicious or not. Here, we leverage the temporal aspects attached to the\nDNs. Our results identify 144930 DNs that show malicious behavior, and out of\nthese, 54114 DNs show persistent malicious behavior over time. Nonetheless,\nnone of these identified malicious DNs were reported in new officially tagged\nmalicious blockchain DNs.",
          "link": "http://arxiv.org/abs/2106.13420",
          "publishedOn": "2021-06-28T01:57:56.777Z",
          "wordCount": 581,
          "title": "Identifying malicious accounts in Blockchains using Domain Names and associated temporal properties. (arXiv:2106.13420v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zouzias_A/0/1/0/all/0/1\">Anastasios Zouzias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalaitzidis_K/0/1/0/all/0/1\">Kleovoulos Kalaitzidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grot_B/0/1/0/all/0/1\">Boris Grot</a>",
          "description": "Recent years have seen stagnating improvements to branch predictor (BP)\nefficacy and a dearth of fresh ideas in branch predictor design, calling for\nfresh thinking in this area. This paper argues that looking at BP from the\nviewpoint of Reinforcement Learning (RL) facilitates systematic reasoning\nabout, and exploration of, BP designs. We describe how to apply the RL\nformulation to branch predictors, show that existing predictors can be\nsuccinctly expressed in this formulation, and study two RL-based variants of\nconventional BPs.",
          "link": "http://arxiv.org/abs/2106.13429",
          "publishedOn": "2021-06-28T01:57:56.767Z",
          "wordCount": 533,
          "title": "Branch Prediction as a Reinforcement Learning Problem: Why, How and Case Studies. (arXiv:2106.13429v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_H/0/1/0/all/0/1\">Hannes Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1\">Geoff Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1\">Phil Bachman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tachet_R/0/1/0/all/0/1\">Remi Tachet</a>",
          "description": "Recent contrastive representation learning methods rely on estimating mutual\ninformation (MI) between multiple views of an underlying context. E.g., we can\nderive multiple views of a given image by applying data augmentation, or we can\nsplit a sequence into views comprising the past and future of some step in the\nsequence. Contrastive lower bounds on MI are easy to optimize, but have a\nstrong underestimation bias when estimating large amounts of MI. We propose\ndecomposing the full MI estimation problem into a sum of smaller estimation\nproblems by splitting one of the views into progressively more informed\nsubviews and by applying the chain rule on MI between the decomposed views.\nThis expression contains a sum of unconditional and conditional MI terms, each\nmeasuring modest chunks of the total MI, which facilitates approximation via\ncontrastive bounds. To maximize the sum, we formulate a contrastive lower bound\non the conditional MI which can be approximated efficiently. We refer to our\ngeneral approach as Decomposed Estimation of Mutual Information (DEMI). We show\nthat DEMI can capture a larger amount of MI than standard non-decomposed\ncontrastive bounds in a synthetic setting, and learns better representations in\na vision domain and for dialogue generation.",
          "link": "http://arxiv.org/abs/2106.13401",
          "publishedOn": "2021-06-28T01:57:56.759Z",
          "wordCount": 640,
          "title": "Decomposed Mutual Information Estimation for Contrastive Representation Learning. (arXiv:2106.13401v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.06214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhe Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bizhao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yijin Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guojie Luo</a>",
          "description": "In recent years, Graph Neural Networks (GNNs) appear to be state-of-the-art\nalgorithms for analyzing non-euclidean graph data. By applying deep-learning to\nextract high-level representations from graph structures, GNNs achieve\nextraordinary accuracy and great generalization ability in various tasks.\nHowever, with the ever-increasing graph sizes, more and more complicated GNN\nlayers, and higher feature dimensions, the computational complexity of GNNs\ngrows exponentially. How to inference GNNs in real time has become a\nchallenging problem, especially for some resource-limited edge-computing\nplatforms.\n\nTo tackle this challenge, we propose BlockGNN, a software-hardware co-design\napproach to realize efficient GNN acceleration. At the algorithm level, we\npropose to leverage block-circulant weight matrices to greatly reduce the\ncomplexity of various GNN models. At the hardware design level, we propose a\npipelined CirCore architecture, which supports efficient block-circulant\nmatrices computation. Basing on CirCore, we present a novel BlockGNN\naccelerator to compute various GNNs with low latency. Moreover, to determine\nthe optimal configurations for diverse deployed tasks, we also introduce a\nperformance and resource model that helps choose the optimal hardware\nparameters automatically. Comprehensive experiments on the ZC706 FPGA platform\ndemonstrate that on various GNN tasks, BlockGNN achieves up to $8.3\\times$\nspeedup compared to the baseline HyGCN architecture and $111.9\\times$ energy\nreduction compared to the Intel Xeon CPU platform.",
          "link": "http://arxiv.org/abs/2104.06214",
          "publishedOn": "2021-06-28T01:57:56.745Z",
          "wordCount": 682,
          "title": "BlockGNN: Towards Efficient GNN Acceleration Using Block-Circulant Weight Matrices. (arXiv:2104.06214v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Israel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1\">Baruch Berdugo</a>",
          "description": "State-of-the-art deep-learning-based voice activity detectors (VADs) are\noften trained with anechoic data. However, real acoustic environments are\ngenerally reverberant, which causes the performance to significantly\ndeteriorate. To mitigate this mismatch between training data and real data, we\nsimulate an augmented training set that contains nearly five million\nutterances. This extension comprises of anechoic utterances and their\nreverberant modifications, generated by convolutions of the anechoic utterances\nwith a variety of room impulse responses (RIRs). We consider five different\nmodels to generate RIRs, and five different VADs that are trained with the\naugmented training set. We test all trained systems in three different real\nreverberant environments. Experimental results show $20\\%$ increase on average\nin accuracy, precision and recall for all detectors and response models,\ncompared to anechoic training. Furthermore, one of the RIR models consistently\nyields better performance than the other models, for all the tested VADs.\nAdditionally, one of the VADs consistently outperformed the other VADs in all\nexperiments.",
          "link": "http://arxiv.org/abs/2106.13511",
          "publishedOn": "2021-06-28T01:57:56.726Z",
          "wordCount": 621,
          "title": "Evaluation of Deep-Learning-Based Voice Activity Detectors and Room Impulse Response Models in Reverberant Environments. (arXiv:2106.13511v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rai_K/0/1/0/all/0/1\">Khushwant Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hojatpanah_F/0/1/0/all/0/1\">Farnam Hojatpanah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajaei_F/0/1/0/all/0/1\">Firouz Badrkhani Ajaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grolinger_K/0/1/0/all/0/1\">Katarina Grolinger</a>",
          "description": "High-impedance faults (HIF) are difficult to detect because of their low\ncurrent amplitude and highly diverse characteristics. In recent years, machine\nlearning (ML) has been gaining popularity in HIF detection because ML\ntechniques learn patterns from data and successfully detect HIFs. However, as\nthese methods are based on supervised learning, they fail to reliably detect\nany scenario, fault or non-fault, not present in the training data.\nConsequently, this paper takes advantage of unsupervised learning and proposes\na convolutional autoencoder framework for HIF detection (CAE-HIFD). Contrary to\nthe conventional autoencoders that learn from normal behavior, the\nconvolutional autoencoder (CAE) in CAE-HIFD learns only from the HIF signals\neliminating the need for presence of diverse non-HIF scenarios in the CAE\ntraining. CAE distinguishes HIFs from non-HIF operating conditions by employing\ncross-correlation. To discriminate HIFs from transient disturbances such as\ncapacitor or load switching, CAE-HIFD uses kurtosis, a statistical measure of\nthe probability distribution shape. The performance evaluation studies\nconducted using the IEEE 13-node test feeder indicate that the CAE-HIFD\nreliably detects HIFs, outperforms the state-of-the-art HIF detection\ntechniques, and is robust against noise.",
          "link": "http://arxiv.org/abs/2106.13276",
          "publishedOn": "2021-06-28T01:57:56.679Z",
          "wordCount": 624,
          "title": "Deep Learning for High-Impedance Fault Detection: Convolutional Autoencoders. (arXiv:2106.13276v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mojto_M/0/1/0/all/0/1\">Martin Mojto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubusky_K/0/1/0/all/0/1\">Karol &#x13d;ubu&#x161;k&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fikar_M/0/1/0/all/0/1\">Miroslav Fikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulen_R/0/1/0/all/0/1\">Radoslav Paulen</a>",
          "description": "Inferential (or soft) sensors are used in industry to infer the values of\nimprecisely and rarely measured (or completely unmeasured) variables from\nvariables measured online (e.g., pressures, temperatures). The main challenge,\nakin to classical model overfitting, in designing an effective inferential\nsensor is the selection of a correct structure of the sensor. The sensor\nstructure is represented by the number of inputs to the sensor, which\ncorrespond to the variables measured online and their (simple) combinations.\nThis work is focused on the design of inferential sensors for product\ncomposition of an industrial distillation column in two oil refinery units, a\nFluid Catalytic Cracking unit and a Vacuum Gasoil Hydrogenation unit. As the\nfirst design step, we use several well-known data pre-treatment (gross error\ndetection) methods and compare the ability of these approaches to indicate\nsystematic errors and outliers in the available industrial data. We then study\neffectiveness of various methods for design of the inferential sensors taking\ninto account the complexity and accuracy of the resulting model. The\neffectiveness analysis indicates that the improvements achieved over the\ncurrent inferential sensors are up to 19 %.",
          "link": "http://arxiv.org/abs/2106.13503",
          "publishedOn": "2021-06-28T01:57:56.673Z",
          "wordCount": 620,
          "title": "Data-based Design of Inferential Sensors for Petrochemical Industry. (arXiv:2106.13503v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1910.09734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Na Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yuan-Hai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huajun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu-Ting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Ling-Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiu_N/0/1/0/all/0/1\">Naihua Xiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Nai-Yang Deng</a>",
          "description": "Considering the classification problem, we summarize the nonparallel support\nvector machines with the nonparallel hyperplanes to two types of frameworks.\nThe first type constructs the hyperplanes separately. It solves a series of\nsmall optimization problems to obtain a series of hyperplanes, but is hard to\nmeasure the loss of each sample. The other type constructs all the hyperplanes\nsimultaneously, and it solves one big optimization problem with the ascertained\nloss of each sample. We give the characteristics of each framework and compare\nthem carefully. In addition, based on the second framework, we construct a\nmax-min distance-based nonparallel support vector machine for multiclass\nclassification problem, called NSVM. It constructs hyperplanes with large\ndistance margin by solving an optimization problem. Experimental results on\nbenchmark data sets show the advantages of our NSVM.",
          "link": "http://arxiv.org/abs/1910.09734",
          "publishedOn": "2021-06-28T01:57:56.661Z",
          "wordCount": 616,
          "title": "Single and Union Non-parallel Support Vector Machine Frameworks. (arXiv:1910.09734v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McGovern_H/0/1/0/all/0/1\">Hope McGovern</a>",
          "description": "It is well-documented that word embeddings trained on large public corpora\nconsistently exhibit known human social biases. Although many methods for\ndebiasing exist, almost all fixate on completely eliminating biased information\nfrom the embeddings and often diminish training set size in the process. In\nthis paper, we present a simple yet effective method for debiasing GloVe word\nembeddings (Pennington et al., 2014) which works by incorporating explicit\ninformation about training set bias rather than removing biased data outright.\nOur method runs quickly and efficiently with the help of a fast bias gradient\napproximation method from Brunet et al. (2019). As our approach is akin to the\nnotion of 'source criticism' in the humanities, we term our method\nSource-Critical GloVe (SC-GloVe). We show that SC-GloVe reduces the effect size\non Word Embedding Association Test (WEAT) sets without sacrificing training\ndata or TOP-1 performance.",
          "link": "http://arxiv.org/abs/2106.13382",
          "publishedOn": "2021-06-28T01:57:56.648Z",
          "wordCount": 569,
          "title": "A Source-Criticism Debiasing Method for GloVe Embeddings. (arXiv:2106.13382v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Israel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1\">Baruch Berdugo</a>",
          "description": "We propose a nonlinear acoustic echo cancellation system, which aims to model\nthe echo path from the far-end signal to the near-end microphone in two parts.\nInspired by the physical behavior of modern hands-free devices, we first\nintroduce a novel neural network architecture that is specifically designed to\nmodel the nonlinear distortions these devices induce between receiving and\nplaying the far-end signal. To account for variations between devices, we\nconstruct this network with trainable memory length and nonlinear activation\nfunctions that are not parameterized in advance, but are rather optimized\nduring the training stage using the training data. Second, the network is\nsucceeded by a standard adaptive linear filter that constantly tracks the echo\npath between the loudspeaker output and the microphone. During training, the\nnetwork and filter are jointly optimized to learn the network parameters. This\nsystem requires 17 thousand parameters that consume 500 Million floating-point\noperations per second and 40 Kilo-bytes of memory. It also satisfies hands-free\ncommunication timing requirements on a standard neural processor, which renders\nit adequate for embedding on hands-free communication devices. Using 280 hours\nof real and synthetic data, experiments show advantageous performance compared\nto competing methods.",
          "link": "http://arxiv.org/abs/2106.13754",
          "publishedOn": "2021-06-28T01:57:56.620Z",
          "wordCount": 633,
          "title": "Nonlinear Acoustic Echo Cancellation with Deep Learning. (arXiv:2106.13754v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13724",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Rezaie_M/0/1/0/all/0/1\">Mehdi Rezaie</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Ross_A/0/1/0/all/0/1\">Ashley J. Ross</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Seo_H/0/1/0/all/0/1\">Hee-Jong Seo</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Mueller_E/0/1/0/all/0/1\">Eva-Maria Mueller</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Percival_W/0/1/0/all/0/1\">Will J. Percival</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Merz_G/0/1/0/all/0/1\">Grant Merz</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Katebi_R/0/1/0/all/0/1\">Reza Katebi</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bunescu_R/0/1/0/all/0/1\">Razvan C. Bunescu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bautista_J/0/1/0/all/0/1\">Julian Bautista</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Brownstein_J/0/1/0/all/0/1\">Joel R. Brownstein</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Burtin_E/0/1/0/all/0/1\">Etienne Burtin</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dawson_K/0/1/0/all/0/1\">Kyle Dawson</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gil_Marin_H/0/1/0/all/0/1\">H&#xe9;ctor Gil-Mar&#xed;n</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hou_J/0/1/0/all/0/1\">Jiamin Hou</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lyke_E/0/1/0/all/0/1\">Eleanor B. Lyke</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Macorra_A/0/1/0/all/0/1\">Axel de la Macorra</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Rossi_G/0/1/0/all/0/1\">Graziano Rossi</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Schneider_D/0/1/0/all/0/1\">Donald P. Schneider</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Zarrouk_P/0/1/0/all/0/1\">Pauline Zarrouk</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Zhao_G/0/1/0/all/0/1\">Gong-Bo Zhao</a>",
          "description": "We investigate the large-scale clustering of the final spectroscopic sample\nof quasars from the recently completed extended Baryon Oscillation\nSpectroscopic Survey (eBOSS). The sample contains $343708$ objects in the\nredshift range $0.8<z<2.2$ and $72667$ objects with redshifts $2.2<z<3.5$,\ncovering an effective area of $4699~{\\rm deg}^{2}$. We develop a neural\nnetwork-based approach to mitigate spurious fluctuations in the density field\ncaused by spatial variations in the quality of the imaging data used to select\ntargets for follow-up spectroscopy. Simulations are used with the same angular\nand radial distributions as the real data to estimate covariance matrices,\nperform error analyses, and assess residual systematic uncertainties. We\nmeasure the mean density contrast and cross-correlations of the eBOSS quasars\nagainst maps of potential sources of imaging systematics to address algorithm\neffectiveness, finding that the neural network-based approach outperforms\nstandard linear regression. Stellar density is one of the most important\nsources of spurious fluctuations, and a new template constructed using data\nfrom the Gaia spacecraft provides the best match to the observed quasar\nclustering. The end-product from this work is a new value-added quasar\ncatalogue with the improved weights to correct for nonlinear imaging systematic\neffects, which will be made public. Our quasar catalogue is used to measure the\nlocal-type primordial non-Gaussianity in our companion paper, Mueller et al. in\npreparation.",
          "link": "http://arxiv.org/abs/2106.13724",
          "publishedOn": "2021-06-28T01:57:56.611Z",
          "wordCount": 755,
          "title": "Primordial non-Gaussianity from the Completed SDSS-IV extended Baryon Oscillation Spectroscopic Survey I: Catalogue Preparation and Systematic Mitigation. (arXiv:2106.13724v1 [astro-ph.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1\">Spencer Frei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "Although the optimization objectives for learning neural networks are highly\nnon-convex, gradient-based methods have been wildly successful at learning\nneural networks in practice. This juxtaposition has led to a number of recent\nstudies on provable guarantees for neural networks trained by gradient descent.\nUnfortunately, the techniques in these works are often highly specific to the\nproblem studied in each setting, relying on different assumptions on the\ndistribution, optimization parameters, and network architectures, making it\ndifficult to generalize across different settings. In this work, we propose a\nunified non-convex optimization framework for the analysis of neural network\ntraining. We introduce the notions of proxy convexity and proxy\nPolyak-Lojasiewicz (PL) inequalities, which are satisfied if the original\nobjective function induces a proxy objective function that is implicitly\nminimized when using gradient methods. We show that stochastic gradient descent\n(SGD) on objectives satisfying proxy convexity or the proxy PL inequality leads\nto efficient guarantees for proxy objective functions. We further show that\nmany existing guarantees for neural networks trained by gradient descent can be\nunified through proxy convexity and proxy PL inequalities.",
          "link": "http://arxiv.org/abs/2106.13792",
          "publishedOn": "2021-06-28T01:57:56.601Z",
          "wordCount": 633,
          "title": "Proxy Convexity: A Unified Framework for the Analysis of Neural Networks Trained by Gradient Descent. (arXiv:2106.13792v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1\">Baruch Berdugo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Israel Cohen</a>",
          "description": "We address voice activity detection in acoustic environments of transients\nand stationary noises, which often occur in real life scenarios. We exploit\nunique spatial patterns of speech and non-speech audio frames by independently\nlearning their underlying geometric structure. This process is done through a\ndeep encoder-decoder based neural network architecture. This structure involves\nan encoder that maps spectral features with temporal information to their\nlow-dimensional representations, which are generated by applying the diffusion\nmaps method. The encoder feeds a decoder that maps the embedded data back into\nthe high-dimensional space. A deep neural network, which is trained to separate\nspeech from non-speech frames, is obtained by concatenating the decoder to the\nencoder, resembling the known Diffusion nets architecture. Experimental results\nshow enhanced performance compared to competing voice activity detection\nmethods. The improvement is achieved in both accuracy, robustness and\ngeneralization ability. Our model performs in a real-time manner and can be\nintegrated into audio-based communication systems. We also present a batch\nalgorithm which obtains an even higher accuracy for off-line applications.",
          "link": "http://arxiv.org/abs/2106.13763",
          "publishedOn": "2021-06-28T01:57:56.595Z",
          "wordCount": 639,
          "title": "Voice Activity Detection for Transient Noisy Environment Based on Diffusion Nets. (arXiv:2106.13763v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asi_H/0/1/0/all/0/1\">Hilal Asi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1\">John Duchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_A/0/1/0/all/0/1\">Alireza Fallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javidbakht_O/0/1/0/all/0/1\">Omid Javidbakht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1\">Kunal Talwar</a>",
          "description": "We study adaptive methods for differentially private convex optimization,\nproposing and analyzing differentially private variants of a Stochastic\nGradient Descent (SGD) algorithm with adaptive stepsizes, as well as the\nAdaGrad algorithm. We provide upper bounds on the regret of both algorithms and\nshow that the bounds are (worst-case) optimal. As a consequence of our\ndevelopment, we show that our private versions of AdaGrad outperform adaptive\nSGD, which in turn outperforms traditional SGD in scenarios with non-isotropic\ngradients where (non-private) Adagrad provably outperforms SGD. The major\nchallenge is that the isotropic noise typically added for privacy dominates the\nsignal in gradient geometry for high-dimensional problems; approaches to this\nthat effectively optimize over lower-dimensional subspaces simply ignore the\nactual problems that varying gradient geometries introduce. In contrast, we\nstudy non-isotropic clipping and noise addition, developing a principled\ntheoretical approach; the consequent procedures also enjoy significantly\nstronger empirical performance than prior approaches.",
          "link": "http://arxiv.org/abs/2106.13756",
          "publishedOn": "2021-06-28T01:57:56.585Z",
          "wordCount": 607,
          "title": "Private Adaptive Gradient Methods for Convex Optimization. (arXiv:2106.13756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McNeil_M/0/1/0/all/0/1\">Maxwell McNeil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogdanov_P/0/1/0/all/0/1\">Petko Bogdanov</a>",
          "description": "Temporal graph signals are multivariate time series with individual\ncomponents associated with nodes of a fixed graph structure. Data of this kind\narises in many domains including activity of social network users, sensor\nnetwork readings over time, and time course gene expression within the\ninteraction network of a model organism. Traditional matrix decomposition\nmethods applied to such data fall short of exploiting structural regularities\nencoded in the underlying graph and also in the temporal patterns of the\nsignal. How can we take into account such structure to obtain a succinct and\ninterpretable representation of temporal graph signals?\n\nWe propose a general, dictionary-based framework for temporal graph signal\ndecomposition (TGSD). The key idea is to learn a low-rank, joint encoding of\nthe data via a combination of graph and time dictionaries. We propose a highly\nscalable decomposition algorithm for both complete and incomplete data, and\ndemonstrate its advantage for matrix decomposition, imputation of missing\nvalues, temporal interpolation, clustering, period estimation, and rank\nestimation in synthetic and real-world data ranging from traffic patterns to\nsocial media activity. Our framework achieves 28% reduction in RMSE compared to\nbaselines for temporal interpolation when as many as 75% of the observations\nare missing. It scales best among baselines taking under 20 seconds on 3.5\nmillion data points and produces the most parsimonious models. To the best of\nour knowledge, TGSD is the first framework to jointly model graph signals by\ntemporal and graph dictionaries.",
          "link": "http://arxiv.org/abs/2106.13517",
          "publishedOn": "2021-06-28T01:57:56.574Z",
          "wordCount": 699,
          "title": "Temporal Graph Signal Decomposition. (arXiv:2106.13517v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhicheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chenglei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Sidan Du</a>",
          "description": "Regularization plays a vital role in machine learning optimization. One novel\nregularization method called flooding makes the training loss fluctuate around\nthe flooding level. It intends to make the model continue to random walk until\nit comes to a flat loss landscape to enhance generalization. However, the\nhyper-parameter flooding level of the flooding method fails to be selected\nproperly and uniformly. We propose a novel method called Jitter to improve it.\nJitter is essentially a kind of random loss function. Before training, we\nrandomly sample the Jitter Point from a specific probability distribution. The\nflooding level should be replaced by Jitter point to obtain a new target\nfunction and train the model accordingly. As Jitter point acting as a random\nfactor, we actually add some randomness to the loss function, which is\nconsistent with the fact that there exists innumerable random behaviors in the\nlearning process of the machine learning model and is supposed to make the\nmodel more robust. In addition, Jitter performs random walk randomly which\ndivides the loss curve into small intervals and then flipping them over,\nideally making the loss curve much flatter and enhancing generalization\nability. Moreover, Jitter can be a domain-, task-, and model-independent\nregularization method and train the model effectively after the training error\nreduces to zero. Our experimental results show that Jitter method can improve\nmodel performance more significantly than the previous flooding method and make\nthe test loss curve descend twice.",
          "link": "http://arxiv.org/abs/2106.13749",
          "publishedOn": "2021-06-28T01:57:56.567Z",
          "wordCount": 665,
          "title": "Jitter: Random Jittering Loss Function. (arXiv:2106.13749v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toutouh_J/0/1/0/all/0/1\">Jamal Toutouh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemberg_E/0/1/0/all/0/1\">Erik Hemberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OReilly_U/0/1/0/all/0/1\">Una-May O&#x27;Reilly</a>",
          "description": "Generative adversary networks (GANs) suffer from training pathologies such as\ninstability and mode collapse, which mainly arise from a lack of diversity in\ntheir adversarial interactions. Co-evolutionary GAN (CoE-GAN) training\nalgorithms have shown to be resilient to these pathologies. This article\nintroduces Mustangs, a spatially distributed CoE-GAN, which fosters diversity\nby using different loss functions during the training. Experimental analysis on\nMNIST and CelebA demonstrated that Mustangs trains statistically more accurate\ngenerators.",
          "link": "http://arxiv.org/abs/2106.13590",
          "publishedOn": "2021-06-28T01:57:56.471Z",
          "wordCount": 536,
          "title": "Fostering Diversity in Spatial Evolutionary Generative Adversarial Networks. (arXiv:2106.13590v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13746",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Miao_N/0/1/0/all/0/1\">Ning Miao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mathieu_E/0/1/0/all/0/1\">Emile Mathieu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>",
          "description": "We introduce a simple and effective method for learning VAEs with\ncontrollable inductive biases by using an intermediary set of latent variables.\nThis allows us to overcome the limitations of the standard Gaussian prior\nassumption. In particular, it allows us to impose desired properties like\nsparsity or clustering on learned representations, and incorporate prior\ninformation into the learned model. Our approach, which we refer to as the\nIntermediary Latent Space VAE (InteL-VAE), is based around controlling the\nstochasticity of the encoding process with the intermediary latent variables,\nbefore deterministically mapping them forward to our target latent\nrepresentation, from which reconstruction is performed. This allows us to\nmaintain all the advantages of the traditional VAE framework, while\nincorporating desired prior information, inductive biases, and even topological\ninformation through the latent mapping. We show that this, in turn, allows\nInteL-VAEs to learn both better generative models and representations.",
          "link": "http://arxiv.org/abs/2106.13746",
          "publishedOn": "2021-06-28T01:57:56.452Z",
          "wordCount": 586,
          "title": "InteL-VAEs: Adding Inductive Biases to Variational Auto-Encoders via Intermediary Latents. (arXiv:2106.13746v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1\">David Dehaene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1\">R&#xe9;my Brossard</a>",
          "description": "We propose a theoretical approach towards the training numerical stability of\nVariational AutoEncoders (VAE). Our work is motivated by recent studies\nempowering VAEs to reach state of the art generative results on complex image\ndatasets. These very deep VAE architectures, as well as VAEs using more complex\noutput distributions, highlight a tendency to haphazardly produce high training\ngradients as well as NaN losses. The empirical fixes proposed to train them\ndespite their limitations are neither fully theoretically grounded nor\ngenerally sufficient in practice. Building on this, we localize the source of\nthe problem at the interface between the model's neural networks and their\noutput probabilistic distributions. We explain a common source of instability\nstemming from an incautious formulation of the encoded Normal distribution's\nvariance, and apply the same approach on other, less obvious sources. We show\nthat by implementing small changes to the way we parameterize the Normal\ndistributions on which they rely, VAEs can securely be trained.",
          "link": "http://arxiv.org/abs/2106.13739",
          "publishedOn": "2021-06-28T01:57:56.430Z",
          "wordCount": 583,
          "title": "Re-parameterizing VAEs for stability. (arXiv:2106.13739v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13689",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wahab_N/0/1/0/all/0/1\">Noorul Wahab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miligy_I/0/1/0/all/0/1\">Islam M Miligy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dodd_K/0/1/0/all/0/1\">Katherine Dodd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahota_H/0/1/0/all/0/1\">Harvir Sahota</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toss_M/0/1/0/all/0/1\">Michael Toss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1\">Wenqi Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bilal_M/0/1/0/all/0/1\">Mohsin Bilal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1\">Simon Graham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Young Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hadjigeorghiou_G/0/1/0/all/0/1\">Giorgos Hadjigeorghiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhalerao_A/0/1/0/all/0/1\">Abhir Bhalerao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lashen_A/0/1/0/all/0/1\">Ayat Lashen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ibrahim_A/0/1/0/all/0/1\">Asmaa Ibrahim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katayama_A/0/1/0/all/0/1\">Ayaka Katayama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebili_H/0/1/0/all/0/1\">Henry O Ebili</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parkin_M/0/1/0/all/0/1\">Matthew Parkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sorell_T/0/1/0/all/0/1\">Tom Sorell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hero_E/0/1/0/all/0/1\">Emily Hero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldaly_H/0/1/0/all/0/1\">Hesham Eldaly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsang_Y/0/1/0/all/0/1\">Yee Wah Tsang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Kishore Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Snead_D/0/1/0/all/0/1\">David Snead</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rakha_E/0/1/0/all/0/1\">Emad Rakha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz Minhas</a>",
          "description": "Recent advances in whole slide imaging (WSI) technology have led to the\ndevelopment of a myriad of computer vision and artificial intelligence (AI)\nbased diagnostic, prognostic, and predictive algorithms. Computational\nPathology (CPath) offers an integrated solution to utilize information embedded\nin pathology WSIs beyond what we obtain through visual assessment. For\nautomated analysis of WSIs and validation of machine learning (ML) models,\nannotations at the slide, tissue and cellular levels are required. The\nannotation of important visual constructs in pathology images is an important\ncomponent of CPath projects. Improper annotations can result in algorithms\nwhich are hard to interpret and can potentially produce inaccurate and\ninconsistent results. Despite the crucial role of annotations in CPath\nprojects, there are no well-defined guidelines or best practices on how\nannotations should be carried out. In this paper, we address this shortcoming\nby presenting the experience and best practices acquired during the execution\nof a large-scale annotation exercise involving a multidisciplinary team of\npathologists, ML experts and researchers as part of the Pathology image data\nLake for Analytics, Knowledge and Education (PathLAKE) consortium. We present a\nreal-world case study along with examples of different types of annotations,\ndiagnostic algorithm, annotation data dictionary and annotation constructs. The\nanalyses reported in this work highlight best practice recommendations that can\nbe used as annotation guidelines over the lifecycle of a CPath project.",
          "link": "http://arxiv.org/abs/2106.13689",
          "publishedOn": "2021-06-28T01:57:56.412Z",
          "wordCount": 729,
          "title": "Semantic annotation for computational pathology: Multidisciplinary experience and best practice recommendations. (arXiv:2106.13689v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gallouedec_Q/0/1/0/all/0/1\">Quentin Gallou&#xe9;dec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cazin_N/0/1/0/all/0/1\">Nicolas Cazin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dellandrea_E/0/1/0/all/0/1\">Emmanuel Dellandr&#xe9;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>",
          "description": "This technical report presents panda-gym, a set Reinforcement Learning (RL)\nenvironments for the Franka Emika Panda robot integrated with OpenAI Gym. Five\ntasks are included: reach, push, slide, pick & place and stack. They all follow\na Multi-Goal RL framework, allowing to use goal-oriented RL algorithms. To\nfoster open-research, we chose to use the open-source physics engine PyBullet.\nThe implementation chosen for this package allows to define very easily new\ntasks or new robots. This report also presents a baseline of results obtained\nwith state-of-the-art model-free off-policy algorithms. panda-gym is\nopen-source at https://github.com/qgallouedec/panda-gym.",
          "link": "http://arxiv.org/abs/2106.13687",
          "publishedOn": "2021-06-28T01:57:56.281Z",
          "wordCount": 535,
          "title": "Multi-Goal Reinforcement Learning environments for simulated Franka Emika Panda robot. (arXiv:2106.13687v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alguacil_A/0/1/0/all/0/1\">Antonio Alguacil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_G/0/1/0/all/0/1\">Gon&#xe7;alves Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauerheim_M/0/1/0/all/0/1\">Michael Bauerheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_M/0/1/0/all/0/1\">Marc C. Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_S/0/1/0/all/0/1\">St&#xe9;phane Moreau</a>",
          "description": "Accurate modeling of boundary conditions is crucial in computational physics.\nThe ever increasing use of neural networks as surrogates for physics-related\nproblems calls for an improved understanding of boundary condition treatment,\nand its influence on the network accuracy. In this paper, several strategies to\nimpose boundary conditions (namely padding, improved spatial context, and\nexplicit encoding of physical boundaries) are investigated in the context of\nfully convolutional networks applied to recurrent tasks. These strategies are\nevaluated on two spatio-temporal evolving problems modeled by partial\ndifferential equations: the 2D propagation of acoustic waves (hyperbolic PDE)\nand the heat equation (parabolic PDE). Results reveal a high sensitivity of\nboth accuracy and stability on the boundary implementation in such recurrent\ntasks. It is then demonstrated that the choice of the optimal padding strategy\nis directly linked to the data semantics. Furthermore, the inclusion of\nadditional input spatial context or explicit physics-based rules allows a\nbetter handling of boundaries in particular for large number of recurrences,\nresulting in more robust and stable neural networks, while facilitating the\ndesign and versatility of such networks.",
          "link": "http://arxiv.org/abs/2106.11160",
          "publishedOn": "2021-06-28T01:57:56.274Z",
          "wordCount": 651,
          "title": "Effects of boundary conditions in fully convolutional networks for learning spatio-temporal dynamics. (arXiv:2106.11160v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13551",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1\">Gabriel Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amor_R/0/1/0/all/0/1\">Roc&#xed;o del Amor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1\">Adri&#xe1;n Colomer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verdu_Monedero_R/0/1/0/all/0/1\">Rafael Verd&#xfa;-Monedero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morales_Sanchez_J/0/1/0/all/0/1\">Juan Morales-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>",
          "description": "Glaucoma is one of the leading causes of blindness worldwide and Optical\nCoherence Tomography (OCT) is the quintessential imaging technique for its\ndetection. Unlike most of the state-of-the-art studies focused on glaucoma\ndetection, in this paper, we propose, for the first time, a novel framework for\nglaucoma grading using raw circumpapillary B-scans. In particular, we set out a\nnew OCT-based hybrid network which combines hand-driven and deep learning\nalgorithms. An OCT-specific descriptor is proposed to extract hand-crafted\nfeatures related to the retinal nerve fibre layer (RNFL). In parallel, an\ninnovative CNN is developed using skip-connections to include tailored residual\nand attention modules to refine the automatic features of the latent space. The\nproposed architecture is used as a backbone to conduct a novel few-shot\nlearning based on static and dynamic prototypical networks. The k-shot paradigm\nis redefined giving rise to a supervised end-to-end system which provides\nsubstantial improvements discriminating between healthy, early and advanced\nglaucoma samples. The training and evaluation processes of the dynamic\nprototypical network are addressed from two fused databases acquired via\nHeidelberg Spectralis system. Validation and testing results reach a\ncategorical accuracy of 0.9459 and 0.8788 for glaucoma grading, respectively.\nBesides, the high performance reported by the proposed model for glaucoma\ndetection deserves a special mention. The findings from the class activation\nmaps are directly in line with the clinicians' opinion since the heatmaps\npointed out the RNFL as the most relevant structure for glaucoma diagnosis.",
          "link": "http://arxiv.org/abs/2106.13551",
          "publishedOn": "2021-06-28T01:57:56.261Z",
          "wordCount": 708,
          "title": "Circumpapillary OCT-Focused Hybrid Learning for Glaucoma Grading Using Tailored Prototypical Neural Networks. (arXiv:2106.13551v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13318",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Brossard_R/0/1/0/all/0/1\">R&#xe9;my Brossard</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Frigo_O/0/1/0/all/0/1\">Oriel Frigo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dehaene_D/0/1/0/all/0/1\">David Dehaene</a>",
          "description": "Deep learning based molecular graph generation and optimization has recently\nbeen attracting attention due to its great potential for de novo drug design.\nOn the one hand, recent models are able to efficiently learn a given graph\ndistribution, and many approaches have proven very effective to produce a\nmolecule that maximizes a given score. On the other hand, it was shown by\nprevious studies that generated optimized molecules are often unrealistic, even\nwith the inclusion of mechanics to enforce similarity to a dataset of real drug\nmolecules. In this work we use a hybrid approach, where the dataset\ndistribution is learned using an autoregressive model while the score\noptimization is done using the Metropolis algorithm, biased toward the learned\ndistribution. We show that the resulting method, that we call learned realism\nsampling (LRS), produces empirically more realistic molecules and outperforms\nall recent baselines in the task of molecule optimization with similarity\nconstraints.",
          "link": "http://arxiv.org/abs/2106.13318",
          "publishedOn": "2021-06-28T01:57:56.255Z",
          "wordCount": 602,
          "title": "Realistic molecule optimization on a learned graph manifold. (arXiv:2106.13318v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1\">Spencer Frei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Difan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We consider a binary classification problem when the data comes from a\nmixture of two isotropic distributions satisfying concentration and\nanti-concentration properties enjoyed by log-concave distributions among\nothers. We show that there exists a universal constant $C_{\\mathrm{err}}>0$\nsuch that if a pseudolabeler $\\boldsymbol{\\beta}_{\\mathrm{pl}}$ can achieve\nclassification error at most $C_{\\mathrm{err}}$, then for any $\\varepsilon>0$,\nan iterative self-training algorithm initialized at $\\boldsymbol{\\beta}_0 :=\n\\boldsymbol{\\beta}_{\\mathrm{pl}}$ using pseudolabels $\\hat y =\n\\mathrm{sgn}(\\langle \\boldsymbol{\\beta}_t, \\mathbf{x}\\rangle)$ and using at\nmost $\\tilde O(d/\\varepsilon^2)$ unlabeled examples suffices to learn the\nBayes-optimal classifier up to $\\varepsilon$ error, where $d$ is the ambient\ndimension. That is, self-training converts weak learners to strong learners\nusing only unlabeled examples. We additionally show that by running gradient\ndescent on the logistic loss one can obtain a pseudolabeler\n$\\boldsymbol{\\beta}_{\\mathrm{pl}}$ with classification error $C_{\\mathrm{err}}$\nusing only $O(d)$ labeled examples (i.e., independent of $\\varepsilon$).\nTogether our results imply that mixture models can be learned to within\n$\\varepsilon$ of the Bayes-optimal accuracy using at most $O(d)$ labeled\nexamples and $\\tilde O(d/\\varepsilon^2)$ unlabeled examples by way of a\nsemi-supervised self-training algorithm.",
          "link": "http://arxiv.org/abs/2106.13805",
          "publishedOn": "2021-06-28T01:57:56.249Z",
          "wordCount": 622,
          "title": "Self-training Converts Weak Learners to Strong Learners in Mixture Models. (arXiv:2106.13805v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oesterling_A/0/1/0/all/0/1\">Alex Oesterling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_A/0/1/0/all/0/1\">Angikar Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haoyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_R/0/1/0/all/0/1\">Rui Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baig_Y/0/1/0/all/0/1\">Yasa Baig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semenova_L/0/1/0/all/0/1\">Lesia Semenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1\">Cynthia Rudin</a>",
          "description": "We present our entry into the 2021 3C Shared Task Citation Context\nClassification based on Purpose competition. The goal of the competition is to\nclassify a citation in a scientific article based on its purpose. This task is\nimportant because it could potentially lead to more comprehensive ways of\nsummarizing the purpose and uses of scientific articles, but it is also\ndifficult, mainly due to the limited amount of available training data in which\nthe purposes of each citation have been hand-labeled, along with the\nsubjectivity of these labels. Our entry in the competition is a multi-task\nmodel that combines multiple modules designed to handle the problem from\ndifferent perspectives, including hand-generated linguistic features, TF-IDF\nfeatures, and an LSTM-with-attention model. We also provide an ablation study\nand feature analysis whose insights could lead to future work.",
          "link": "http://arxiv.org/abs/2106.13275",
          "publishedOn": "2021-06-28T01:57:56.242Z",
          "wordCount": 579,
          "title": "Multitask Learning for Citation Purpose Classification. (arXiv:2106.13275v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Abdul Rafae Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varsanyi_P/0/1/0/all/0/1\">Peter Varsanyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pabreja_R/0/1/0/all/0/1\">Rachit Pabreja</a>",
          "description": "While predictive policing has become increasingly common in assisting with\ndecisions in the criminal justice system, the use of these results is still\ncontroversial. Some software based on deep learning lacks accuracy (e.g., in\nF-1), and many decision processes are not transparent causing doubt about\ndecision bias, such as perceived racial, age, and gender disparities. This\npaper addresses bias issues with post-hoc explanations to provide a trustable\nprediction of whether a person will receive future criminal charges given one's\nprevious criminal records by learning temporal behavior patterns over twenty\nyears. Bi-LSTM relieves the vanishing gradient problem, and attentional\nmechanisms allows learning and interpretation of feature importance. Our\napproach shows consistent and reliable prediction precision and recall on a\nreal-life dataset. Our analysis of the importance of each input feature shows\nthe critical causal impact on decision-making, suggesting that criminal\nhistories are statistically significant factors, while identifiers, such as\nrace, gender, and age, are not. Finally, our algorithm indicates that a suspect\ntends to gradually rather than suddenly increase crime severity level over\ntime.",
          "link": "http://arxiv.org/abs/2106.13456",
          "publishedOn": "2021-06-28T01:57:56.217Z",
          "wordCount": 610,
          "title": "Deep Interpretable Criminal Charge Prediction and Algorithmic Bias. (arXiv:2106.13456v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09474",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Aylett_Bullock_J/0/1/0/all/0/1\">Joseph Aylett-Bullock</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Badger_S/0/1/0/all/0/1\">Simon Badger</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Moodie_R/0/1/0/all/0/1\">Ryan Moodie</a>",
          "description": "Machine learning technology has the potential to dramatically optimise event\ngeneration and simulations. We continue to investigate the use of neural\nnetworks to approximate matrix elements for high-multiplicity scattering\nprocesses. We focus on the case of loop-induced diphoton production through\ngluon fusion and develop a realistic simulation method that can be applied to\nhadron collider observables. Neural networks are trained using the one-loop\namplitudes implemented in the NJet C++ library and interfaced to the Sherpa\nMonte Carlo event generator where we perform a detailed study for $2\\to3$ and\n$2\\to4$ scattering problems. We also consider how the trained networks perform\nwhen varying the kinematic cuts effecting the phase space and the reliability\nof the neural network simulations.",
          "link": "http://arxiv.org/abs/2106.09474",
          "publishedOn": "2021-06-28T01:57:56.202Z",
          "wordCount": 588,
          "title": "Optimising simulations for diphoton production at hadron colliders using amplitude neural networks. (arXiv:2106.09474v2 [hep-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Sadia Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urner_R/0/1/0/all/0/1\">Ruth Urner</a>",
          "description": "The phenomenon of adversarial examples in deep learning models has caused\nsubstantial concern over their reliability. While many deep neural networks\nhave shown impressive performance in terms of predictive accuracy, it has been\nshown that in many instances an imperceptible perturbation can falsely flip the\nnetwork's prediction. Most research has then focused on developing defenses\nagainst adversarial attacks or learning under a worst-case adversarial loss. In\nthis work, we take a step back and aim to provide a framework for determining\nwhether a model's label change under small perturbation is justified (and when\nit is not). We carefully argue that adversarial robustness should be defined as\na locally adaptive measure complying with the underlying distribution. We then\nsuggest a definition for an adaptive robust loss, derive an empirical version\nof it, and develop a resulting data-augmentation framework. We prove that our\nadaptive data-augmentation maintains consistency of 1-nearest neighbor\nclassification under deterministic labels and provide illustrative empirical\nevaluations.",
          "link": "http://arxiv.org/abs/2106.13326",
          "publishedOn": "2021-06-28T01:57:56.195Z",
          "wordCount": 598,
          "title": "On the (Un-)Avoidability of Adversarial Examples. (arXiv:2106.13326v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1909.07750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajan_R/0/1/0/all/0/1\">Raghu Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1\">Jessica Lizeth Borja Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guttikonda_S/0/1/0/all/0/1\">Suresh Guttikonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1\">Fabio Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biedenkapp_A/0/1/0/all/0/1\">Andr&#xe9; Biedenkapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartz_J/0/1/0/all/0/1\">Jan Ole von Hartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "We present \\emph{MDP Playground}, an efficient testbed for Reinforcement\nLearning (RL) agents with \\textit{orthogonal} dimensions that can be controlled\nindependently to challenge agents in different ways and obtain varying degrees\nof hardness in generated environments. We consider and allow control over a\nwide variety of dimensions, including \\textit{delayed rewards},\n\\textit{rewardable sequences}, \\textit{density of rewards},\n\\textit{stochasticity}, \\textit{image representations}, \\textit{irrelevant\nfeatures}, \\textit{time unit}, \\textit{action range} and more. We define a\nparameterised collection of fast-to-run toy environments in \\textit{OpenAI Gym}\nby varying these dimensions and propose to use these for the initial design and\ndevelopment of agents. We also provide wrappers that inject these dimensions\ninto complex environments from \\textit{Atari} and \\textit{Mujoco} to allow for\nevaluating agent robustness. We further provide various example use-cases and\ninstructions on how to use \\textit{MDP Playground} to design and debug agents.\nWe believe that \\textit{MDP Playground} is a valuable testbed for researchers\ndesigning new, adaptive and intelligent RL agents and those wanting to unit\ntest their agents.",
          "link": "http://arxiv.org/abs/1909.07750",
          "publishedOn": "2021-06-28T01:57:56.178Z",
          "wordCount": 673,
          "title": "MDP Playground: A Design and Debug Testbed for Reinforcement Learning. (arXiv:1909.07750v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05144",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shulby_C/0/1/0/all/0/1\">Christopher Shulby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_F/0/1/0/all/0/1\">Frederico Santos de Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teixeira_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Teixeira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Maria Aluisio</a>",
          "description": "Speech provides a natural way for human-computer interaction. In particular,\nspeech synthesis systems are popular in different applications, such as\npersonal assistants, GPS applications, screen readers and accessibility tools.\nHowever, not all languages are on the same level when in terms of resources and\nsystems for speech synthesis. This work consists of creating publicly available\nresources for Brazilian Portuguese in the form of a novel dataset along with\ndeep learning models for end-to-end speech synthesis. Such dataset has 10.5\nhours from a single speaker, from which a Tacotron 2 model with the RTISI-LA\nvocoder presented the best performance, achieving a 4.03 MOS value. The\nobtained results are comparable to related works covering English language and\nthe state-of-the-art in Portuguese.",
          "link": "http://arxiv.org/abs/2005.05144",
          "publishedOn": "2021-06-28T01:57:56.172Z",
          "wordCount": 620,
          "title": "TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese. (arXiv:2005.05144v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1\">Eli Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianhao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovic</a>",
          "description": "Hypergraphs are used to model higher-order interactions amongst agents and\nthere exist many practically relevant instances of hypergraph datasets. To\nenable efficient processing of hypergraph-structured data, several hypergraph\nneural network platforms have been proposed for learning hypergraph properties\nand structure, with a special focus on node classification. However, almost all\nexisting methods use heuristic propagation rules and offer suboptimal\nperformance on many datasets. We propose AllSet, a new hypergraph neural\nnetwork paradigm that represents a highly general framework for (hyper)graph\nneural networks and for the first time implements hypergraph neural network\nlayers as compositions of two multiset functions that can be efficiently\nlearned for each task and each dataset. Furthermore, AllSet draws on new\nconnections between hypergraph neural networks and recent advances in deep\nlearning of multiset functions. In particular, the proposed architecture\nutilizes Deep Sets and Set Transformer architectures that allow for significant\nmodeling flexibility and offer high expressive power. To evaluate the\nperformance of AllSet, we conduct the most extensive experiments to date\ninvolving ten known benchmarking datasets and three newly curated datasets that\nrepresent significant challenges for hypergraph node classification. The\nresults demonstrate that AllSet has the unique ability to consistently either\nmatch or outperform all other hypergraph neural networks across the tested\ndatasets. Our implementation and dataset will be released upon acceptance.",
          "link": "http://arxiv.org/abs/2106.13264",
          "publishedOn": "2021-06-28T01:57:56.165Z",
          "wordCount": 655,
          "title": "You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks. (arXiv:2106.13264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Han Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Federated learning has emerged as an important paradigm for training machine\nlearning models in different domains. For graph-level tasks such as graph\nclassification, graphs can also be regarded as a special type of data samples,\nwhich can be collected and stored in separate local systems. Similar to other\ndomains, multiple local systems, each holding a small set of graphs, may\nbenefit from collaboratively training a powerful graph mining model, such as\nthe popular graph neural networks (GNNs). To provide more motivation towards\nsuch endeavors, we analyze real-world graphs from different domains to confirm\nthat they indeed share certain graph properties that are statistically\nsignificant compared with random graphs. However, we also find that different\nsets of graphs, even from the same domain or same dataset, are non-IID\nregarding both graph structures and node features. To handle this, we propose a\ngraph clustering federated learning (GCFL) framework that dynamically finds\nclusters of local systems based on the gradients of GNNs, and theoretically\njustify that such clusters can reduce the structure and feature heterogeneity\namong graphs owned by the local systems. Moreover, we observe the gradients of\nGNNs to be rather fluctuating in GCFL which impedes high-quality clustering,\nand design a gradient sequence-based clustering mechanism based on dynamic time\nwarping (GCFL+). Extensive experimental results and in-depth analysis\ndemonstrate the effectiveness of our proposed frameworks.",
          "link": "http://arxiv.org/abs/2106.13423",
          "publishedOn": "2021-06-28T01:57:56.158Z",
          "wordCount": 664,
          "title": "Federated Graph Classification over Non-IID Graphs. (arXiv:2106.13423v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Golowich_N/0/1/0/all/0/1\">Noah Golowich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1\">Roi Livni</a>",
          "description": "We consider the problem of online classification under a privacy constraint.\nIn this setting a learner observes sequentially a stream of labelled examples\n$(x_t, y_t)$, for $1 \\leq t \\leq T$, and returns at each iteration $t$ a\nhypothesis $h_t$ which is used to predict the label of each new example $x_t$.\nThe learner's performance is measured by her regret against a known hypothesis\nclass $\\mathcal{H}$. We require that the algorithm satisfies the following\nprivacy constraint: the sequence $h_1, \\ldots, h_T$ of hypotheses output by the\nalgorithm needs to be an $(\\epsilon, \\delta)$-differentially private function\nof the whole input sequence $(x_1, y_1), \\ldots, (x_T, y_T)$. We provide the\nfirst non-trivial regret bound for the realizable setting. Specifically, we\nshow that if the class $\\mathcal{H}$ has constant Littlestone dimension then,\ngiven an oblivious sequence of labelled examples, there is a private learner\nthat makes in expectation at most $O(\\log T)$ mistakes -- comparable to the\noptimal mistake bound in the non-private case, up to a logarithmic factor.\nMoreover, for general values of the Littlestone dimension $d$, the same mistake\nbound holds but with a doubly-exponential in $d$ factor. A recent line of work\nhas demonstrated a strong connection between classes that are online learnable\nand those that are differentially-private learnable. Our results strengthen\nthis connection and show that an online learning algorithm can in fact be\ndirectly privatized (in the realizable setting). We also discuss an adaptive\nsetting and provide a sublinear regret bound of $O(\\sqrt{T})$.",
          "link": "http://arxiv.org/abs/2106.13513",
          "publishedOn": "2021-06-28T01:57:56.152Z",
          "wordCount": 679,
          "title": "Littlestone Classes are Privately Online Learnable. (arXiv:2106.13513v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nooraiepour_A/0/1/0/all/0/1\">Alireza Nooraiepour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajwa_W/0/1/0/all/0/1\">Waheed U. Bajwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandayam_N/0/1/0/all/0/1\">Narayan B. Mandayam</a>",
          "description": "The fundamental task of classification given a limited number of training\ndata samples is considered for physical systems with known parametric\nstatistical models. The standalone learning-based and statistical model-based\nclassifiers face major challenges towards the fulfillment of the classification\ntask using a small training set. Specifically, classifiers that solely rely on\nthe physics-based statistical models usually suffer from their inability to\nproperly tune the underlying unobservable parameters, which leads to a\nmismatched representation of the system's behaviors. Learning-based\nclassifiers, on the other hand, typically rely on a large number of training\ndata from the underlying physical process, which might not be feasible in most\npractical scenarios. In this paper, a hybrid classification method -- termed\nHyPhyLearn -- is proposed that exploits both the physics-based statistical\nmodels and the learning-based classifiers. The proposed solution is based on\nthe conjecture that HyPhyLearn would alleviate the challenges associated with\nthe individual approaches of learning-based and statistical model-based\nclassifiers by fusing their respective strengths. The proposed hybrid approach\nfirst estimates the unobservable model parameters using the available\n(suboptimal) statistical estimation procedures, and subsequently use the\nphysics-based statistical models to generate synthetic data. Then, the training\ndata samples are incorporated with the synthetic data in a learning-based\nclassifier that is based on domain-adversarial training of neural networks.\nSpecifically, in order to address the mismatch problem, the classifier learns a\nmapping from the training data and the synthetic data to a common feature\nspace. Simultaneously, the classifier is trained to find discriminative\nfeatures within this space in order to fulfill the classification task.",
          "link": "http://arxiv.org/abs/2106.13436",
          "publishedOn": "2021-06-28T01:57:56.131Z",
          "wordCount": 715,
          "title": "A hybrid model-based and learning-based approach for classification using limited number of training samples. (arXiv:2106.13436v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13274",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ivlev_D/0/1/0/all/0/1\">Dmitry Ivlev</a>",
          "description": "Purpose of this research is to forecast the development of sand bodies in\nproductive sediments based on well log data and seismic attributes. The object\nof the study is the productive intervals of Achimov sedimentary complex in the\npart of oil field located in Western Siberia. The research shows a\ntechnological stack of machine learning algorithms, methods for enriching the\nsource data with synthetic ones and algorithms for creating new features. The\nresult was the model of regression relationship between the values of natural\nradioactivity of rocks and seismic wave field attributes with an acceptable\nprediction quality. Acceptable quality of the forecast is confirmed both by\nmodel cross validation, and by the data obtained following the results of new\nwell.",
          "link": "http://arxiv.org/abs/2106.13274",
          "publishedOn": "2021-06-28T01:57:56.124Z",
          "wordCount": 587,
          "title": "Prediction of geophysical properties of rocks on rare well data and attributes of seismic waves by machine learning methods on the example of the Achimov formation. (arXiv:2106.13274v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Rachit Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapliyal_T/0/1/0/all/0/1\">Tanmay Thapliyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1\">Sandeep Kumar Shukla</a>",
          "description": "Smart Contracts (SCs) in Ethereum can automate tasks and provide different\nfunctionalities to a user. Such automation is enabled by the `Turing-complete'\nnature of the programming language (Solidity) in which SCs are written. This\nalso opens up different vulnerabilities and bugs in SCs that malicious actors\nexploit to carry out malicious or illegal activities on the cryptocurrency\nplatform. In this work, we study the correlation between malicious activities\nand the vulnerabilities present in SCs and find that some malicious activities\nare correlated with certain types of vulnerabilities. We then develop and study\nthe feasibility of a scoring mechanism that corresponds to the severity of the\nvulnerabilities present in SCs to determine if it is a relevant feature to\nidentify suspicious SCs. We analyze the utility of severity score towards\ndetection of suspicious SCs using unsupervised machine learning (ML) algorithms\nacross different temporal granularities and identify behavioral changes. In our\nexperiments with on-chain SCs, we were able to find a total of 1094 benign SCs\nacross different granularities which behave similar to malicious SCs, with the\ninclusion of the smart contract vulnerability scores in the feature set.",
          "link": "http://arxiv.org/abs/2106.13422",
          "publishedOn": "2021-06-28T01:57:56.117Z",
          "wordCount": 636,
          "title": "Vulnerability and Transaction behavior based detection of Malicious Smart Contracts. (arXiv:2106.13422v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2007.15779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1\">Michael Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>",
          "description": "Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding & Reasoning\nBenchmark) at https://aka.ms/BLURB.",
          "link": "http://arxiv.org/abs/2007.15779",
          "publishedOn": "2021-06-28T01:57:56.105Z",
          "wordCount": 708,
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13280",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1\">Katie Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_G/0/1/0/all/0/1\">Gregory Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Deep reinforcement learning algorithms require large and diverse datasets in\norder to learn successful policies for perception-based mobile navigation.\nHowever, gathering such datasets with a single robot can be prohibitively\nexpensive. Collecting data with multiple different robotic platforms with\npossibly different dynamics is a more scalable approach to large-scale data\ncollection. But how can deep reinforcement learning algorithms leverage such\nheterogeneous datasets? In this work, we propose a deep reinforcement learning\nalgorithm with hierarchically integrated models (HInt). At training time, HInt\nlearns separate perception and dynamics models, and at test time, HInt\nintegrates the two models in a hierarchical manner and plans actions with the\nintegrated model. This method of planning with hierarchically integrated models\nallows the algorithm to train on datasets gathered by a variety of different\nplatforms, while respecting the physical capabilities of the deployment robot\nat test time. Our mobile navigation experiments show that HInt outperforms\nconventional hierarchical policies and single-source approaches.",
          "link": "http://arxiv.org/abs/2106.13280",
          "publishedOn": "2021-06-28T01:57:56.092Z",
          "wordCount": 587,
          "title": "Multi-Robot Deep Reinforcement Learning for Mobile Navigation. (arXiv:2106.13280v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mingkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Recently, transformers have shown great superiority in solving computer\nvision tasks by modeling images as a sequence of manually-split patches with\nself-attention mechanism. However, current architectures of vision transformers\n(ViTs) are simply inherited from natural language processing (NLP) tasks and\nhave not been sufficiently investigated and optimized. In this paper, we make a\nfurther step by examining the intrinsic structure of transformers for vision\ntasks and propose an architecture search method, dubbed ViTAS, to search for\nthe optimal architecture with similar hardware budgets. Concretely, we design a\nnew effective yet efficient weight sharing paradigm for ViTs, such that\narchitectures with different token embedding, sequence size, number of heads,\nwidth, and depth can be derived from a single super-transformer. Moreover, to\ncater for the variance of distinct architectures, we introduce \\textit{private}\nclass token and self-attention maps in the super-transformer. In addition, to\nadapt the searching for different budgets, we propose to search the sampling\nprobability of identity operation. Experimental results show that our ViTAS\nattains excellent results compared to existing pure transformer architectures.\nFor example, with $1.3$G FLOPs budget, our searched architecture achieves\n$74.7\\%$ top-$1$ accuracy on ImageNet and is $2.5\\%$ superior than the current\nbaseline ViT architecture. Code is available at\n\\url{https://github.com/xiusu/ViTAS}.",
          "link": "http://arxiv.org/abs/2106.13700",
          "publishedOn": "2021-06-28T01:57:56.072Z",
          "wordCount": 646,
          "title": "Vision Transformer Architecture Search. (arXiv:2106.13700v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Worrall_G/0/1/0/all/0/1\">George Worrall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1\">Anand Rangarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Judge_J/0/1/0/all/0/1\">Jasmeet Judge</a>",
          "description": "Advanced machine learning techniques have been used in remote sensing (RS)\napplications such as crop mapping and yield prediction, but remain\nunder-utilized for tracking crop progress. In this study, we demonstrate the\nuse of agronomic knowledge of crop growth drivers in a Long Short-Term\nMemory-based, Domain-guided neural network (DgNN) for in-season crop progress\nestimation. The DgNN uses a branched structure and attention to separate\nindependent crop growth drivers and capture their varying importance throughout\nthe growing season. The DgNN is implemented for corn, using RS data in Iowa for\nthe period 2003-2019, with USDA crop progress reports used as ground truth.\nState-wide DgNN performance shows significant improvement over sequential and\ndense-only NN structures, and a widely-used Hidden Markov Model method. The\nDgNN had a 3.5% higher Nash-Sutfliffe efficiency over all growth stages and 33%\nmore weeks with highest cosine similarity than the other NNs during test years.\nThe DgNN and Sequential NN were more robust during periods of abnormal crop\nprogress, though estimating the Silking-Grainfill transition was difficult for\nall methods. Finally, Uniform Manifold Approximation and Projection\nvisualizations of layer activations showed how LSTM-based NNs separate crop\ngrowth time-series differently from a dense-only structure. Results from this\nstudy exhibit both the viability of NNs in crop growth stage estimation (CGSE)\nand the benefits of using domain knowledge. The DgNN methodology presented here\ncan be extended to provide near-real time CGSE of other crops.",
          "link": "http://arxiv.org/abs/2106.13323",
          "publishedOn": "2021-06-28T01:57:56.059Z",
          "wordCount": 679,
          "title": "Domain-guided Machine Learning for Remotely Sensed In-Season Crop Growth Estimation. (arXiv:2106.13323v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1\">Rui Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekhor_S/0/1/0/all/0/1\">Shlomo Bekhor</a>",
          "description": "This paper derives the generalized extreme value (GEV) model with implicit\navailability/perception (IAP) of alternatives and proposes a variational\nautoencoder (VAE) approach for choice set generation and implicit perception of\nalternatives. Specifically, the cross-nested logit (CNL) model with IAP is\nderived as an example of IAP-GEV models. The VAE approach is adapted to model\nthe choice set generation process, in which the likelihood of perceiving chosen\nalternatives in the choice set is maximized. The VAE approach for route choice\nset generation is exemplified using a real dataset. IAP- CNL model estimated\nhas the best performance in terms of goodness-of-fit and prediction\nperformance, compared to multinomial logit models and conventional choice set\ngeneration methods.",
          "link": "http://arxiv.org/abs/2106.13319",
          "publishedOn": "2021-06-28T01:57:56.042Z",
          "wordCount": 568,
          "title": "A variational autoencoder approach for choice set generation and implicit perception of alternatives in choice modeling. (arXiv:2106.13319v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Shan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masegosa_A/0/1/0/all/0/1\">Andr&#xe9;s R. Masegosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzen_S/0/1/0/all/0/1\">Stephan S. Lorenzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1\">Christian Igel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seldin_Y/0/1/0/all/0/1\">Yevgeny Seldin</a>",
          "description": "We present a new second-order oracle bound for the expected risk of a\nweighted majority vote. The bound is based on a novel parametric form of the\nChebyshev-Cantelli inequality (a.k.a.\\ one-sided Chebyshev's), which is\namenable to efficient minimization. The new form resolves the optimization\nchallenge faced by prior oracle bounds based on the Chebyshev-Cantelli\ninequality, the C-bounds [Germain et al., 2015], and, at the same time, it\nimproves on the oracle bound based on second order Markov's inequality\nintroduced by Masegosa et al. [2020]. We also derive the PAC-Bayes-Bennett\ninequality, which we use for empirical estimation of the oracle bound. The\nPAC-Bayes-Bennett inequality improves on the PAC-Bayes-Bernstein inequality by\nSeldin et al. [2012]. We provide an empirical evaluation demonstrating that the\nnew bounds can improve on the work by Masegosa et al. [2020]. Both the\nparametric form of the Chebyshev-Cantelli inequality and the PAC-Bayes-Bennett\ninequality may be of independent interest for the study of concentration of\nmeasure in other domains.",
          "link": "http://arxiv.org/abs/2106.13624",
          "publishedOn": "2021-06-28T01:57:56.035Z",
          "wordCount": 606,
          "title": "Chebyshev-Cantelli PAC-Bayes-Bennett Inequality for the Weighted Majority Vote. (arXiv:2106.13624v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1\">Qingyang Hong</a>",
          "description": "This paper proposes a multi-task learning network with phoneme-aware and\nchannel-wise attentive learning strategies for text-dependent Speaker\nVerification (SV). In the proposed structure, the frame-level multi-task\nlearning along with the segment-level adversarial learning is adopted for\nspeaker embedding extraction. The phoneme-aware attentive pooling is exploited\non frame-level features in the main network for speaker classifier, with the\ncorresponding posterior probability for the phoneme distribution in the\nauxiliary subnet. Further, the introduction of Squeeze and Excitation\n(SE-block) performs dynamic channel-wise feature recalibration, which improves\nthe representational ability. The proposed method exploits speaker\nidiosyncrasies associated with pass-phrases, and is further improved by the\nphoneme-aware attentive pooling and SE-block from temporal and channel-wise\naspects, respectively. The experiments conducted on RSR2015 Part 1 database\nconfirm that the proposed system achieves outstanding results for textdependent\nSV.",
          "link": "http://arxiv.org/abs/2106.13514",
          "publishedOn": "2021-06-28T01:57:56.011Z",
          "wordCount": 572,
          "title": "Phoneme-aware and Channel-wise Attentive Learning for Text DependentSpeaker Verification. (arXiv:2106.13514v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahinpei_A/0/1/0/all/0/1\">Anita Mahinpei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Justin Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lage_I/0/1/0/all/0/1\">Isaac Lage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1\">Finale Doshi-Velez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weiwei Pan</a>",
          "description": "Machine learning models that incorporate concept learning as an intermediate\nstep in their decision making process can match the performance of black-box\npredictive models while retaining the ability to explain outcomes in human\nunderstandable terms. However, we demonstrate that the concept representations\nlearned by these models encode information beyond the pre-defined concepts, and\nthat natural mitigation strategies do not fully work, rendering the\ninterpretation of the downstream prediction misleading. We describe the\nmechanism underlying the information leakage and suggest recourse for\nmitigating its effects.",
          "link": "http://arxiv.org/abs/2106.13314",
          "publishedOn": "2021-06-28T01:57:56.005Z",
          "wordCount": 519,
          "title": "Promises and Pitfalls of Black-Box Concept Learning Models. (arXiv:2106.13314v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wanqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lizhong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>",
          "description": "Self-Rating Depression Scale (SDS) questionnaire has frequently been used for\nefficient depression preliminary screening. However, the uncontrollable\nself-administered measure can be easily affected by insouciantly or deceptively\nanswering, and producing the different results with the clinician-administered\nHamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,\nfacial expression (FE) and actions play a vital role in clinician-administered\nevaluation, while FE and action are underexplored for self-administered\nevaluations. In this work, we collect a novel dataset of 200 subjects to\nevidence the validity of self-rating questionnaires with their corresponding\nquestion-wise video recording. To automatically interpret depression from the\nSDS evaluation and the paired video, we propose an end-to-end hierarchical\nframework for the long-term variable-length video, which is also conditioned on\nthe questionnaire results and the answering time. Specifically, we resort to a\nhierarchical model which utilizes a 3D CNN for local temporal pattern\nexploration and a redundancy-aware self-attention (RAS) scheme for\nquestion-wise global feature aggregation. Targeting for the redundant long-term\nFE video processing, our RAS is able to effectively exploit the correlations of\neach video clip within a question set to emphasize the discriminative\ninformation and eliminate the redundancy based on feature pair-wise affinity.\nThen, the question-wise video feature is concatenated with the questionnaire\nscores for final depression detection. Our thorough evaluations also show the\nvalidity of fusing SDS evaluation and its video recording, and the superiority\nof our framework to the conventional state-of-the-art temporal modeling\nmethods.",
          "link": "http://arxiv.org/abs/2106.13393",
          "publishedOn": "2021-06-28T01:57:55.995Z",
          "wordCount": 702,
          "title": "Interpreting Depression From Question-wise Long-term Video Recording of SDS Evaluation. (arXiv:2106.13393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1\">Sai Vemprala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyde_N/0/1/0/all/0/1\">Nicholas Gyde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1\">Hadi Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1\">Ashish Kapoor</a>",
          "description": "The ability to perform causal and counterfactual reasoning are central\nproperties of human intelligence. Decision-making systems that can perform\nthese types of reasoning have the potential to be more generalizable and\ninterpretable. Simulations have helped advance the state-of-the-art in this\ndomain, by providing the ability to systematically vary parameters (e.g.,\nconfounders) and generate examples of the outcomes in the case of\ncounterfactual scenarios. However, simulating complex temporal causal events in\nmulti-agent scenarios, such as those that exist in driving and vehicle\nnavigation, is challenging. To help address this, we present a high-fidelity\nsimulation environment that is designed for developing algorithms for causal\ndiscovery and counterfactual reasoning in the safety-critical context. A core\ncomponent of our work is to introduce \\textit{agency}, such that it is simple\nto define and create complex scenarios using high-level definitions. The\nvehicles then operate with agency to complete these objectives, meaning\nlow-level behaviors need only be controlled if necessary. We perform\nexperiments with three state-of-the-art methods to create baselines and\nhighlight the affordances of this environment. Finally, we highlight challenges\nand opportunities for future work.",
          "link": "http://arxiv.org/abs/2106.13364",
          "publishedOn": "2021-06-28T01:57:55.987Z",
          "wordCount": 636,
          "title": "CausalCity: Complex Simulations with Agency for Causal Discovery and Reasoning. (arXiv:2106.13364v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13434",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kovacs_R/0/1/0/all/0/1\">Reka A. Kovacs</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gunluk_O/0/1/0/all/0/1\">Oktay Gunluk</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hauser_R/0/1/0/all/0/1\">Raphael A. Hauser</a>",
          "description": "Binary matrix factorisation is an essential tool for identifying discrete\npatterns in binary data. In this paper we consider the rank-k binary matrix\nfactorisation problem (k-BMF) under Boolean arithmetic: we are given an n x m\nbinary matrix X with possibly missing entries and need to find two binary\nmatrices A and B of dimension n x k and k x m respectively, which minimise the\ndistance between X and the Boolean product of A and B in the squared Frobenius\ndistance. We present a compact and two exponential size integer programs (IPs)\nfor k-BMF and show that the compact IP has a weak LP relaxation, while the\nexponential size LPs have a stronger equivalent LP relaxation. We introduce a\nnew objective function, which differs from the traditional squared Frobenius\nobjective in attributing a weight to zero entries of the input matrix that is\nproportional to the number of times the zero is erroneously covered in a rank-k\nfactorisation. For one of the exponential size IPs we describe a computational\napproach based on column generation. Experimental results on synthetic and real\nword datasets suggest that our integer programming approach is competitive\nagainst available methods for k-BMF and provides accurate low-error\nfactorisations.",
          "link": "http://arxiv.org/abs/2106.13434",
          "publishedOn": "2021-06-28T01:57:55.979Z",
          "wordCount": 643,
          "title": "Binary Matrix Factorisation and Completion via Integer Programming. (arXiv:2106.13434v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jessica Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Sohini Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "In situations where explanations of black-box models may be useful, the\nfairness of the black-box is also often a relevant concern. However, the link\nbetween the fairness of the black-box model and the behavior of explanations\nfor the black-box is unclear. We focus on explanations applied to tabular\ndatasets, suggesting that explanations do not necessarily preserve the fairness\nproperties of the black-box algorithm. In other words, explanation algorithms\ncan ignore or obscure critical relevant properties, creating incorrect or\nmisleading explanations. More broadly, we propose future research directions\nfor evaluating and generating explanations such that they are informative and\nrelevant from a fairness perspective.",
          "link": "http://arxiv.org/abs/2106.13346",
          "publishedOn": "2021-06-28T01:57:55.961Z",
          "wordCount": 558,
          "title": "What will it take to generate fairness-preserving explanations?. (arXiv:2106.13346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>",
          "description": "One-class learning is the classic problem of fitting a model to the data for\nwhich annotations are available only for a single class. In this paper, we\nexplore novel objectives for one-class learning, which we collectively refer to\nas Generalized One-class Discriminative Subspaces (GODS). Our key idea is to\nlearn a pair of complementary classifiers to flexibly bound the one-class data\ndistribution, where the data belongs to the positive half-space of one of the\nclassifiers in the complementary pair and to the negative half-space of the\nother. To avoid redundancy while allowing non-linearity in the classifier\ndecision surfaces, we propose to design each classifier as an orthonormal frame\nand seek to learn these frames via jointly optimizing for two conflicting\nobjectives, namely: i) to minimize the distance between the two frames, and ii)\nto maximize the margin between the frames and the data. The learned orthonormal\nframes will thus characterize a piecewise linear decision surface that allows\nfor efficient inference, while our objectives seek to bound the data within a\nminimal volume that maximizes the decision margin, thereby robustly capturing\nthe data distribution. We explore several variants of our formulation under\ndifferent constraints on the constituent classifiers, including kernelized\nfeature maps. We demonstrate the empirical benefits of our approach via\nexperiments on data from several applications in computer vision, such as\nanomaly detection in video sequences, human poses, and human activities. We\nalso explore the generality and effectiveness of GODS for non-vision tasks via\nexperiments on several UCI datasets, demonstrating state-of-the-art results.",
          "link": "http://arxiv.org/abs/2106.13272",
          "publishedOn": "2021-06-28T01:57:55.949Z",
          "wordCount": 699,
          "title": "Generalized One-Class Learning Using Pairs of Complementary Classifiers. (arXiv:2106.13272v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1\">Robert L. Logan IV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balazevic_I/0/1/0/all/0/1\">Ivana Bala&#x17e;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>",
          "description": "Prompting language models (LMs) with training examples and task descriptions\nhas been seen as critical to recent successes in few-shot learning. In this\nwork, we show that finetuning LMs in the few-shot setting can considerably\nreduce the need for prompt engineering. In fact, one can use null prompts,\nprompts that contain neither task-specific templates nor training examples, and\nachieve competitive accuracy to manually-tuned prompts across a wide range of\ntasks. While finetuning LMs does introduce new parameters for each downstream\ntask, we show that this memory overhead can be substantially reduced:\nfinetuning only the bias terms can achieve comparable or better accuracy than\nstandard finetuning while only updating 0.1% of the parameters. All in all, we\nrecommend finetuning LMs for few-shot learning as it is more accurate, robust\nto different prompts, and can be made nearly as efficient as using frozen LMs.",
          "link": "http://arxiv.org/abs/2106.13353",
          "publishedOn": "2021-06-28T01:57:55.943Z",
          "wordCount": 592,
          "title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models. (arXiv:2106.13353v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suryanarayanan_P/0/1/0/all/0/1\">Parthasarathy Suryanarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Prithwish Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_P/0/1/0/all/0/1\">Piyush Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bore_K/0/1/0/all/0/1\">Kibichii Bore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogallo_W/0/1/0/all/0/1\">William Ogallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rachita Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghalwash_M/0/1/0/all/0/1\">Mohamed Ghalwash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buleje_I/0/1/0/all/0/1\">Italo Buleje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remy_S/0/1/0/all/0/1\">Sekou Remy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahatma_S/0/1/0/all/0/1\">Shilpa Mahatma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_P/0/1/0/all/0/1\">Pablo Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jianying Hu</a>",
          "description": "In this work we introduce Disease Progression Modeling workbench 360 (DPM360)\nopensource clinical informatics framework for collaborative research and\ndelivery of healthcare AI. DPM360, when fully developed, will manage the entire\nmodeling life cycle, from data analysis (e.g., cohort identification) to\nmachine learning algorithm development and prototyping. DPM360 augments the\nadvantages of data model standardization and tooling (OMOP-CDM, Athena, ATLAS)\nprovided by the widely-adopted OHDSI initiative with a powerful machine\nlearning training framework, and a mechanism for rapid prototyping through\nautomatic deployment of models as containerized services to a cloud\nenvironment.",
          "link": "http://arxiv.org/abs/2106.13265",
          "publishedOn": "2021-06-28T01:57:55.919Z",
          "wordCount": 544,
          "title": "Disease Progression Modeling Workbench 360. (arXiv:2106.13265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansourifar_H/0/1/0/all/0/1\">Hadi Mansourifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsagheer_D/0/1/0/all/0/1\">Dana Alsagheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathi_R/0/1/0/all/0/1\">Reza Fathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weidong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>",
          "description": "With high prevalence of offensive language against the minorities in social\nmedia, counter hate speech generation is considered as an automatic way to\ntackle this challenge. The counter hate speeches are supposed to appear as a\nthird voice to educate people and keep the social red lines bold without\nlimiting the freedom of speech principles. The counter hate speech generation\nis based on the optimistic assumption that, any attempt to intervene the hate\nspeeches in social media can play a positive role in this context. Beyond that,\nprevious works ignored to investigate the sequence of comments before and after\ncounter speech. To the best of our knowledge, no attempt has been made to\nmeasure the counter hate speech impact from statistical point of view. In this\npaper, we take the first step in this direction by measuring the counter hate\nspeech impact on the next comments in terms of Google Perspective Scores.\nFurthermore, our experiments show that, counter hate speech can cause negative\nimpacts, a phenomena which is called aggression in social media.",
          "link": "http://arxiv.org/abs/2106.13238",
          "publishedOn": "2021-06-28T01:57:55.907Z",
          "wordCount": 617,
          "title": "Hate Speech Detection in Clubhouse. (arXiv:2106.13238v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashby_M/0/1/0/all/0/1\">Michael Hunter Ashby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilbrey_J/0/1/0/all/0/1\">Jenna A. Bilbrey</a>",
          "description": "We apply a temporal edge prediction model for weighted dynamic graphs to\npredict time-dependent changes in molecular structure. Each molecule is\nrepresented as a complete graph in which each atom is a vertex and all vertex\npairs are connected by an edge weighted by the Euclidean distance between atom\npairs. We ingest a sequence of complete molecular graphs into a dynamic graph\nneural network (GNN) to predict the graph at the next time step. Our dynamic\nGNN predicts atom-to-atom distances with a mean absolute error of 0.017 \\r{A},\nwhich is considered ``chemically accurate'' for molecular simulations. We also\nexplored the transferability of a trained network to new molecular systems and\nfound that finetuning with less than 10% of the total trajectory provides a\nmean absolute error of the same order of magnitude as that when training from\nscratch on the full molecular trajectory.",
          "link": "http://arxiv.org/abs/2106.13277",
          "publishedOn": "2021-06-28T01:57:55.901Z",
          "wordCount": 590,
          "title": "Geometric learning of the conformational dynamics of molecules using dynamic graph neural networks. (arXiv:2106.13277v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spieler_J/0/1/0/all/0/1\">Jonathan Spieler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potyka_N/0/1/0/all/0/1\">Nico Potyka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1\">Steffen Staab</a>",
          "description": "Gradual argumentation frameworks represent arguments and their relationships\nin a weighted graph. Their graphical structure and intuitive semantics makes\nthem a potentially interesting tool for interpretable machine learning. It has\nbeen noted recently that their mechanics are closely related to neural\nnetworks, which allows learning their weights from data by standard deep\nlearning frameworks. As a first proof of concept, we propose a genetic\nalgorithm to simultaneously learn the structure of argumentative classification\nmodels. To obtain a well interpretable model, the fitness function balances\nsparseness and accuracy of the classifier. We discuss our algorithm and present\nfirst experimental results on standard benchmarks from the UCI machine learning\nrepository. Our prototype learns argumentative classification models that are\ncomparable to decision trees in terms of learning performance and\ninterpretability.",
          "link": "http://arxiv.org/abs/2106.13585",
          "publishedOn": "2021-06-28T01:57:55.884Z",
          "wordCount": 558,
          "title": "Learning Gradual Argumentation Frameworks using Genetic Algorithms. (arXiv:2106.13585v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13706",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hagen_A/0/1/0/all/0/1\">Alex Hagen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jackson_S/0/1/0/all/0/1\">Shane Jackson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kahn_J/0/1/0/all/0/1\">James Kahn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Strube_J/0/1/0/all/0/1\">Jan Strube</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Haide_I/0/1/0/all/0/1\">Isabel Haide</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pazdernik_K/0/1/0/all/0/1\">Karl Pazdernik</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hainje_C/0/1/0/all/0/1\">Connor Hainje</a>",
          "description": "Statistical testing is widespread and critical for a variety of scientific\ndisciplines. The advent of machine learning and the increase of computing power\nhas increased the interest in the analysis and statistical testing of\nmultidimensional data. We extend the powerful Kolmogorov-Smirnov two sample\ntest to a high dimensional form in a similar manner to Fasano (Fasano, 1987).\nWe call our result the d-dimensional Kolmogorov-Smirnov test (ddKS) and provide\nthree novel contributions therewith: we develop an analytical equation for the\nsignificance of a given ddKS score, we provide an algorithm for computation of\nddKS on modern computing hardware that is of constant time complexity for small\nsample sizes and dimensions, and we provide two approximate calculations of\nddKS: one that reduces the time complexity to linear at larger sample sizes,\nand another that reduces the time complexity to linear with increasing\ndimension. We perform power analysis of ddKS and its approximations on a corpus\nof datasets and compare to other common high dimensional two sample tests and\ndistances: Hotelling's T^2 test and Kullback-Leibler divergence. Our ddKS test\nperforms well for all datasets, dimensions, and sizes tested, whereas the other\ntests and distances fail to reject the null hypothesis on at least one dataset.\nWe therefore conclude that ddKS is a powerful multidimensional two sample test\nfor general use, and can be calculated in a fast and efficient manner using our\nparallel or approximate methods. Open source implementations of all methods\ndescribed in this work are located at https://github.com/pnnl/ddks.",
          "link": "http://arxiv.org/abs/2106.13706",
          "publishedOn": "2021-06-28T01:57:55.839Z",
          "wordCount": 699,
          "title": "Accelerated Computation of a High Dimensional Kolmogorov-Smirnov Distance. (arXiv:2106.13706v1 [stat.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1\">Clayton Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picchetti_B/0/1/0/all/0/1\">Bianca Picchetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantelic_J/0/1/0/all/0/1\">Jovan Pantelic</a>",
          "description": "Machine learning for building energy prediction has exploded in popularity in\nrecent years, yet understanding its limitations and potential for improvement\nare lacking. The ASHRAE Great Energy Predictor III (GEPIII) Kaggle competition\nwas the largest building energy meter machine learning competition ever held\nwith 4,370 participants who submitted 39,403 predictions. The test data set\nincluded two years of hourly electricity, hot water, chilled water, and steam\nreadings from 2,380 meters in 1,448 buildings at 16 locations. This paper\nanalyzes the various sources and types of residual model error from an\naggregation of the competition's top 50 solutions. This analysis reveals the\nlimitations for machine learning using the standard model inputs of historical\nmeter, weather, and basic building metadata. The types of error are classified\naccording to the amount of time errors occur in each instance, abrupt versus\ngradual behavior, the magnitude of error, and whether the error existed on\nsingle buildings or several buildings at once from a single location. The\nresults show machine learning models have errors within a range of\nacceptability on 79.1% of the test data. Lower magnitude model errors occur in\n16.1% of the test data. These discrepancies can likely be addressed through\nadditional training data sources or innovations in machine learning. Higher\nmagnitude errors occur in 4.8% of the test data and are unlikely to be\naccurately predicted regardless of innovation. There is a diversity of error\nbehavior depending on the energy meter type (electricity prediction models have\nunacceptable error in under 10% of test data, while hot water is over 60%) and\nbuilding use type (public service less than 14%, while technology/science is\njust over 46%).",
          "link": "http://arxiv.org/abs/2106.13475",
          "publishedOn": "2021-06-28T01:57:55.796Z",
          "wordCount": 724,
          "title": "Limitations of machine learning for building energy prediction: ASHRAE Great Energy Predictor III Kaggle competition error analysis. (arXiv:2106.13475v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1\">Rui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1\">Kristofer Bouchard</a>",
          "description": "Many modern time-series datasets contain large numbers of output response\nvariables sampled for prolonged periods of time. For example, in neuroscience,\nthe activities of 100s-1000's of neurons are recorded during behaviors and in\nresponse to sensory stimuli. Multi-output Gaussian process models leverage the\nnonparametric nature of Gaussian processes to capture structure across multiple\noutputs. However, this class of models typically assumes that the correlations\nbetween the output response variables are invariant in the input space.\nStochastic linear mixing models (SLMM) assume the mixture coefficients depend\non input, making them more flexible and effective to capture complex output\ndependence. However, currently, the inference for SLMMs is intractable for\nlarge datasets, making them inapplicable to several modern time-series\nproblems. In this paper, we propose a new regression framework, the orthogonal\nstochastic linear mixing model (OSLMM) that introduces an orthogonal constraint\namongst the mixing coefficients. This constraint reduces the computational\nburden of inference while retaining the capability to handle complex output\ndependence. We provide Markov chain Monte Carlo inference procedures for both\nSLMM and OSLMM and demonstrate superior model scalability and reduced\nprediction error of OSLMM compared with state-of-the-art methods on several\nreal-world applications. In neurophysiology recordings, we use the inferred\nlatent functions for compact visualization of population responses to auditory\nstimuli, and demonstrate superior results compared to a competing method\n(GPFA). Together, these results demonstrate that OSLMM will be useful for the\nanalysis of diverse, large-scale time-series datasets.",
          "link": "http://arxiv.org/abs/2106.13379",
          "publishedOn": "2021-06-28T01:57:55.788Z",
          "wordCount": 676,
          "title": "Bayesian Inference in High-Dimensional Time-Serieswith the Orthogonal Stochastic Linear Mixing Model. (arXiv:2106.13379v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13361",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Penwarden_M/0/1/0/all/0/1\">Michael Penwarden</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhe_S/0/1/0/all/0/1\">Shandian Zhe</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Narayan_A/0/1/0/all/0/1\">Akil Narayan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kirby_R/0/1/0/all/0/1\">Robert M. Kirby</a>",
          "description": "Multifidelity simulation methodologies are often used in an attempt to\njudiciously combine low-fidelity and high-fidelity simulation results in an\naccuracy-increasing, cost-saving way. Candidates for this approach are\nsimulation methodologies for which there are fidelity differences connected\nwith significant computational cost differences. Physics-informed Neural\nNetworks (PINNs) are candidates for these types of approaches due to the\nsignificant difference in training times required when different fidelities\n(expressed in terms of architecture width and depth as well as optimization\ncriteria) are employed. In this paper, we propose a particular multifidelity\napproach applied to PINNs that exploits low-rank structure. We demonstrate that\nwidth, depth, and optimization criteria can be used as parameters related to\nmodel fidelity, and show numerical justification of cost differences in\ntraining due to fidelity parameter choices. We test our multifidelity scheme on\nvarious canonical forward PDE models that have been presented in the emerging\nPINNs literature.",
          "link": "http://arxiv.org/abs/2106.13361",
          "publishedOn": "2021-06-28T01:57:55.781Z",
          "wordCount": 579,
          "title": "Multifidelity Modeling for Physics-Informed Neural Networks (PINNs). (arXiv:2106.13361v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Federated learning (FL) collaboratively aggregates a shared global model\ndepending on multiple local clients, while keeping the training data\ndecentralized in order to preserve data privacy. However, standard FL methods\nignore the noisy client issue, which may harm the overall performance of the\naggregated model. In this paper, we first analyze the noisy client statement,\nand then model noisy clients with different noise distributions (e.g.,\nBernoulli and truncated Gaussian distributions). To learn with noisy clients,\nwe propose a simple yet effective FL framework, named Federated Noisy Client\nLearning (Fed-NCL), which is a plug-and-play algorithm and contains two main\ncomponents: a data quality measurement (DQM) to dynamically quantify the data\nquality of each participating client, and a noise robust aggregation (NRA) to\nadaptively aggregate the local models of each client by jointly considering the\namount of local training data and the data quality of each client. Our Fed-NCL\ncan be easily applied in any standard FL workflow to handle the noisy client\nissue. Experimental results on various datasets demonstrate that our algorithm\nboosts the performances of different state-of-the-art systems with noisy\nclients.",
          "link": "http://arxiv.org/abs/2106.13239",
          "publishedOn": "2021-06-28T01:57:55.744Z",
          "wordCount": 623,
          "title": "Federated Noisy Client Learning. (arXiv:2106.13239v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moya_B/0/1/0/all/0/1\">Beatriz Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1\">Alberto Badias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1\">David Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1\">Francisco Chinesta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1\">Elias Cueto</a>",
          "description": "Physics perception very often faces the problem that only limited data or\npartial measurements on the scene are available. In this work, we propose a\nstrategy to learn the full state of sloshing liquids from measurements of the\nfree surface. Our approach is based on recurrent neural networks (RNN) that\nproject the limited information available to a reduced-order manifold so as to\nnot only reconstruct the unknown information, but also to be capable of\nperforming fluid reasoning about future scenarios in real time. To obtain\nphysically consistent predictions, we train deep neural networks on the\nreduced-order manifold that, through the employ of inductive biases, ensure the\nfulfillment of the principles of thermodynamics. RNNs learn from history the\nrequired hidden information to correlate the limited information with the\nlatent space where the simulation occurs. Finally, a decoder returns data back\nto the high-dimensional manifold, so as to provide the user with insightful\ninformation in the form of augmented reality. This algorithm is connected to a\ncomputer vision system to test the performance of the proposed methodology with\nreal information, resulting in a system capable of understanding and predicting\nfuture states of the observed fluid in real-time.",
          "link": "http://arxiv.org/abs/2106.13301",
          "publishedOn": "2021-06-28T01:57:55.721Z",
          "wordCount": 644,
          "title": "Physics perception in sloshing scenes with guaranteed thermodynamic consistency. (arXiv:2106.13301v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1\">Alexandre Drouin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>",
          "description": "This article introduces byteSteady -- a fast model for classification using\nbyte-level n-gram embeddings. byteSteady assumes that each input comes as a\nsequence of bytes. A representation vector is produced using the averaged\nembedding vectors of byte-level n-grams, with a pre-defined set of n. The\nhashing trick is used to reduce the number of embedding vectors. This input\nrepresentation vector is then fed into a linear classifier. A straightforward\napplication of byteSteady is text classification. We also apply byteSteady to\none type of non-language data -- DNA sequences for gene classification. For\nboth problems we achieved competitive classification results against strong\nbaselines, suggesting that byteSteady can be applied to both language and\nnon-language data. Furthermore, we find that simple compression using Huffman\ncoding does not significantly impact the results, which offers an\naccuracy-speed trade-off previously unexplored in machine learning.",
          "link": "http://arxiv.org/abs/2106.13302",
          "publishedOn": "2021-06-28T01:57:55.663Z",
          "wordCount": 571,
          "title": "byteSteady: Fast Classification Using Byte-Level n-Gram Embeddings. (arXiv:2106.13302v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Juyang Weng</a>",
          "description": "This paper raises a rarely reported practice in Artificial Intelligence (AI)\ncalled Post Selection Using Test Sets (PSUTS). Consequently, the popular\nerror-backprop methodology in deep learning lacks an acceptable generalization\npower. All AI methods fall into two broad schools, connectionist and symbolic.\nThe PSUTS fall into two kinds, machine PSUTS and human PSUTS. The connectionist\nschool received criticisms for its \"scruffiness\" due to a huge number of\nnetwork parameters and now the worse machine PSUTS; but the seemingly \"clean\"\nsymbolic school seems more brittle because of a weaker generalization power\nusing human PSUTS. This paper formally defines what PSUTS is, analyzes why\nerror-backprop methods with random initial weights suffer from severe local\nminima, why PSUTS violates well-established research ethics, and how every\npaper that used PSUTS should have at least transparently reported PSUTS. For\nimproved transparency in future publications, this paper proposes a new\nstandard for performance evaluation of AI, called developmental errors for all\nnetworks trained, along with Three Learning Conditions: (1) an incremental\nlearning architecture, (2) a training experience and (3) a limited amount of\ncomputational resources. Developmental Networks avoid PSUTS and are not\n\"scruffy\" because they drive Emergent Turing Machines and are optimal in the\nsense of maximum-likelihood across lifetime.",
          "link": "http://arxiv.org/abs/2106.13233",
          "publishedOn": "2021-06-28T01:57:55.645Z",
          "wordCount": 669,
          "title": "Post Selections Using Test Sets (PSUTS) and How Developmental Networks Avoid Them. (arXiv:2106.13233v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1\">Gavin Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1\">Marco Gaboardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Adam Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullman_J/0/1/0/all/0/1\">Jonathan Ullman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakynthinou_L/0/1/0/all/0/1\">Lydia Zakynthinou</a>",
          "description": "We present two sample-efficient differentially private mean estimators for\n$d$-dimensional (sub)Gaussian distributions with unknown covariance.\nInformally, given $n \\gtrsim d/\\alpha^2$ samples from such a distribution with\nmean $\\mu$ and covariance $\\Sigma$, our estimators output $\\tilde\\mu$ such that\n$\\| \\tilde\\mu - \\mu \\|_{\\Sigma} \\leq \\alpha$, where $\\| \\cdot \\|_{\\Sigma}$ is\nthe Mahalanobis distance. All previous estimators with the same guarantee\neither require strong a priori bounds on the covariance matrix or require\n$\\Omega(d^{3/2})$ samples.\n\nEach of our estimators is based on a simple, general approach to designing\ndifferentially private mechanisms, but with novel technical steps to make the\nestimator private and sample-efficient. Our first estimator samples a point\nwith approximately maximum Tukey depth using the exponential mechanism, but\nrestricted to the set of points of large Tukey depth. Proving that this\nmechanism is private requires a novel analysis. Our second estimator perturbs\nthe empirical mean of the data set with noise calibrated to the empirical\ncovariance, without releasing the covariance itself. Its sample complexity\nguarantees hold more generally for subgaussian distributions, albeit with a\nslightly worse dependence on the privacy parameter. For both estimators,\ncareful preprocessing of the data is required to satisfy differential privacy.",
          "link": "http://arxiv.org/abs/2106.13329",
          "publishedOn": "2021-06-28T01:57:55.585Z",
          "wordCount": 627,
          "title": "Covariance-Aware Private Mean Estimation Without Private Covariance Estimation. (arXiv:2106.13329v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.04644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingnan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coumans_E/0/1/0/all/0/1\">Erwin Coumans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jie Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1\">Byron Boots</a>",
          "description": "We focus on the problem of developing energy efficient controllers for\nquadrupedal robots. Animals can actively switch gaits at different speeds to\nlower their energy consumption. In this paper, we devise a hierarchical\nlearning framework, in which distinctive locomotion gaits and natural gait\ntransitions emerge automatically with a simple reward of energy minimization.\nWe use reinforcement learning to train a high-level gait policy that specifies\ngait patterns of each foot, while the low-level whole-body controller optimizes\nthe motor commands so that the robot can walk at a desired velocity using that\ngait pattern. We test our learning framework on a quadruped robot and\ndemonstrate automatic gait transitions, from walking to trotting and to\nfly-trotting, as the robot increases its speed. We show that the learned\nhierarchical controller consumes much less energy across a wide range of\nlocomotion speed than baseline controllers.",
          "link": "http://arxiv.org/abs/2104.04644",
          "publishedOn": "2021-06-25T02:00:47.701Z",
          "wordCount": 608,
          "title": "Fast and Efficient Locomotion via Learned Gait Transitions. (arXiv:2104.04644v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hiranandani_G/0/1/0/all/0/1\">Gaurush Hiranandani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_J/0/1/0/all/0/1\">Jatin Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1\">Harikrishna Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_M/0/1/0/all/0/1\">Mahdi Milani Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1\">Oluwasanmi Koyejo</a>",
          "description": "We consider learning to optimize a classification metric defined by a\nblack-box function of the confusion matrix. Such black-box learning settings\nare ubiquitous, for example, when the learner only has query access to the\nmetric of interest, or in noisy-label and domain adaptation applications where\nthe learner must evaluate the metric via performance evaluation using a small\nvalidation sample. Our approach is to adaptively learn example weights on the\ntraining dataset such that the resulting weighted objective best approximates\nthe metric on the validation sample. We show how to model and estimate the\nexample weights and use them to iteratively post-shift a pre-trained class\nprobability estimator to construct a classifier. We also analyze the resulting\nprocedure's statistical properties. Experiments on various label noise, domain\nshift, and fair classification setups confirm that our proposal compares\nfavorably to the state-of-the-art baselines for each application.",
          "link": "http://arxiv.org/abs/2102.09492",
          "publishedOn": "2021-06-25T02:00:47.695Z",
          "wordCount": 629,
          "title": "Optimizing Black-box Metrics with Iterative Example Weighting. (arXiv:2102.09492v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Punnakkal_A/0/1/0/all/0/1\">Abhinanda R. Punnakkal</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1\">Arjun Chandrasekaran</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1\">Nikos Athanasiou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Quiros_Ramirez_A/0/1/0/all/0/1\">Alejandra Quiros-Ramirez</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a> (1) ((1) Max Planck Institute for Intelligent Systems, (2) Universitat Konstanz)",
          "description": "Understanding the semantics of human movement -- the what, how and why of the\nmovement -- is an important problem that requires datasets of human actions\nwith semantic labels. Existing datasets take one of two approaches. Large-scale\nvideo datasets contain many action labels but do not contain ground-truth 3D\nhuman motion. Alternatively, motion-capture (mocap) datasets have precise body\nmotions but are limited to a small number of actions. To address this, we\npresent BABEL, a large dataset with language labels describing the actions\nbeing performed in mocap sequences. BABEL consists of action labels for about\n43 hours of mocap sequences from AMASS. Action labels are at two levels of\nabstraction -- sequence labels describe the overall action in the sequence, and\nframe labels describe all actions in every frame of the sequence. Each frame\nlabel is precisely aligned with the duration of the corresponding action in the\nmocap sequence, and multiple actions can overlap. There are over 28k sequence\nlabels, and 63k frame labels in BABEL, which belong to over 250 unique action\ncategories. Labels from BABEL can be leveraged for tasks like action\nrecognition, temporal action localization, motion synthesis, etc. To\ndemonstrate the value of BABEL as a benchmark, we evaluate the performance of\nmodels on 3D action recognition. We demonstrate that BABEL poses interesting\nlearning challenges that are applicable to real-world scenarios, and can serve\nas a useful benchmark of progress in 3D action recognition. The dataset,\nbaseline method, and evaluation code is made available, and supported for\nacademic research purposes at https://babel.is.tue.mpg.de/.",
          "link": "http://arxiv.org/abs/2106.09696",
          "publishedOn": "2021-06-25T02:00:47.678Z",
          "wordCount": 745,
          "title": "BABEL: Bodies, Action and Behavior with English Labels. (arXiv:2106.09696v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02336",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Song_Z/0/1/0/all/0/1\">Zhixin Song</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_Y/0/1/0/all/0/1\">Youle Wang</a>",
          "description": "Singular value decomposition is central to many problems in engineering and\nscientific fields. Several quantum algorithms have been proposed to determine\nthe singular values and their associated singular vectors of a given matrix.\nAlthough these algorithms are promising, the required quantum subroutines and\nresources are too costly on near-term quantum devices. In this work, we propose\na variational quantum algorithm for singular value decomposition (VQSVD). By\nexploiting the variational principles for singular values and the Ky Fan\nTheorem, we design a novel loss function such that two quantum neural networks\n(or parameterized quantum circuits) could be trained to learn the singular\nvectors and output the corresponding singular values. Furthermore, we conduct\nnumerical simulations of VQSVD for random matrices as well as its applications\nin image compression of handwritten digits. Finally, we discuss the\napplications of our algorithm in recommendation systems and polar\ndecomposition. Our work explores new avenues for quantum information processing\nbeyond the conventional protocols that only works for Hermitian data, and\nreveals the capability of matrix decomposition on near-term quantum devices.",
          "link": "http://arxiv.org/abs/2006.02336",
          "publishedOn": "2021-06-25T02:00:47.602Z",
          "wordCount": 630,
          "title": "Variational Quantum Singular Value Decomposition. (arXiv:2006.02336v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.04883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>",
          "description": "Vector-valued learning, where the output space admits a vector-valued\nstructure, is an important problem that covers a broad family of important\ndomains, e.g. multi-label learning and multi-class classification. Using local\nRademacher complexity and unlabeled data, we derive novel data-dependent excess\nrisk bounds for learning vector-valued functions in both the kernel space and\nlinear space. The derived bounds are much sharper than existing ones, where\nconvergence rates are improved from $\\mathcal{O}(1/\\sqrt{n})$ to\n$\\mathcal{O}(1/\\sqrt{n+u}),$ and $\\mathcal{O}(1/n)$ in special cases. Motivated\nby our theoretical analysis, we propose a unified framework for learning\nvector-valued functions, incorporating both local Rademacher complexity and\nLaplacian regularization. Empirical results on a wide number of benchmark\ndatasets show that the proposed algorithm significantly outperforms baseline\nmethods, which coincides with our theoretical findings.",
          "link": "http://arxiv.org/abs/1909.04883",
          "publishedOn": "2021-06-25T02:00:47.597Z",
          "wordCount": 591,
          "title": "Semi-supervised Vector-valued Learning: From Theory to Algorithm. (arXiv:1909.04883v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chiyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>",
          "description": "As machine learning methods are deployed in real-world settings such as\nhealthcare, legal systems, and social science, it is crucial to recognize how\nthey shape social biases and stereotypes in these sensitive decision-making\nprocesses. Among such real-world deployments are large-scale pretrained\nlanguage models (LMs) that can be potentially dangerous in manifesting\nundesirable representational biases - harmful biases resulting from\nstereotyping that propagate negative generalizations involving gender, race,\nreligion, and other social constructs. As a step towards improving the fairness\nof LMs, we carefully define several sources of representational biases before\nproposing new benchmarks and metrics to measure them. With these tools, we\npropose steps towards mitigating social biases during text generation. Our\nempirical results and human evaluation demonstrate effectiveness in mitigating\nbias while retaining crucial contextual information for high-fidelity text\ngeneration, thereby pushing forward the performance-fairness Pareto frontier.",
          "link": "http://arxiv.org/abs/2106.13219",
          "publishedOn": "2021-06-25T02:00:47.592Z",
          "wordCount": 596,
          "title": "Towards Understanding and Mitigating Social Biases in Language Models. (arXiv:2106.13219v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1810.03730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1\">Christian Walder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1\">Marian-Andrei Rizoiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lexing Xie</a>",
          "description": "In this paper, we develop an efficient nonparametric Bayesian estimation of\nthe kernel function of Hawkes processes. The non-parametric Bayesian approach\nis important because it provides flexible Hawkes kernels and quantifies their\nuncertainty. Our method is based on the cluster representation of Hawkes\nprocesses. Utilizing the stationarity of the Hawkes process, we efficiently\nsample random branching structures and thus, we split the Hawkes process into\nclusters of Poisson processes. We derive two algorithms -- a block Gibbs\nsampler and a maximum a posteriori estimator based on expectation maximization\n-- and we show that our methods have a linear time complexity, both\ntheoretically and empirically. On synthetic data, we show our methods to be\nable to infer flexible Hawkes triggering kernels. On two large-scale Twitter\ndiffusion datasets, we show that our methods outperform the current\nstate-of-the-art in goodness-of-fit and that the time complexity is linear in\nthe size of the dataset. We also observe that on diffusions related to online\nvideos, the learned kernels reflect the perceived longevity for different\ncontent types such as music or pets videos.",
          "link": "http://arxiv.org/abs/1810.03730",
          "publishedOn": "2021-06-25T02:00:47.587Z",
          "wordCount": 657,
          "title": "Efficient Non-parametric Bayesian Hawkes Processes. (arXiv:1810.03730v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cunnington_D/0/1/0/all/0/1\">Daniel Cunnington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Mark Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1\">Alessandra Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1\">Jorge Lobo</a>",
          "description": "Inductive Logic Programming (ILP) aims to learn generalised, interpretable\nhypotheses in a data-efficient manner. However, current ILP systems require\ntraining examples to be specified in a structured logical form. This paper\nintroduces a neural-symbolic learning framework, called Feed-Forward\nNeural-Symbolic Learner (FF-NSL), that integrates state-of-the-art ILP systems\nbased on the Answer Set semantics, with neural networks, in order to learn\ninterpretable hypotheses from labelled unstructured data. FF-NSL uses a\npre-trained neural network to extract symbolic facts from unstructured data and\nan ILP system to learn a hypothesis that performs a downstream classification\ntask. In order to evaluate the applicability of our approach to real-world\napplications, the framework is evaluated on tasks where distributional shifts\nare introduced to unstructured input data, for which pre-trained neural\nnetworks are likely to predict incorrectly and with high confidence.\nExperimental results show that FF-NSL outperforms baseline approaches such as a\nrandom forest and deep neural networks by learning more accurate and\ninterpretable hypotheses with fewer examples.",
          "link": "http://arxiv.org/abs/2106.13103",
          "publishedOn": "2021-06-25T02:00:47.568Z",
          "wordCount": 586,
          "title": "FF-NSL: Feed-Forward Neural-Symbolic Learner. (arXiv:2106.13103v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1\">Jia Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>",
          "description": "The vision community is witnessing a modeling shift from CNNs to\nTransformers, where pure Transformer architectures have attained top accuracy\non the major video recognition benchmarks. These video models are all built on\nTransformer layers that globally connect patches across the spatial and\ntemporal dimensions. In this paper, we instead advocate an inductive bias of\nlocality in video Transformers, which leads to a better speed-accuracy\ntrade-off compared to previous approaches which compute self-attention globally\neven with spatial-temporal factorization. The locality of the proposed video\narchitecture is realized by adapting the Swin Transformer designed for the\nimage domain, while continuing to leverage the power of pre-trained image\nmodels. Our approach achieves state-of-the-art accuracy on a broad range of\nvideo recognition benchmarks, including on action recognition (84.9 top-1\naccuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less\npre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1\naccuracy on Something-Something v2). The code and models will be made publicly\navailable at https://github.com/SwinTransformer/Video-Swin-Transformer.",
          "link": "http://arxiv.org/abs/2106.13230",
          "publishedOn": "2021-06-25T02:00:47.562Z",
          "wordCount": 606,
          "title": "Video Swin Transformer. (arXiv:2106.13230v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramzi_Z/0/1/0/all/0/1\">Zaccharie Ramzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannel_F/0/1/0/all/0/1\">Florian Mannel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shaojie Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Starck_J/0/1/0/all/0/1\">Jean-Luc Starck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciuciu_P/0/1/0/all/0/1\">Philippe Ciuciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1\">Thomas Moreau</a>",
          "description": "In recent years, implicit deep learning has emerged as a method to increase\nthe depth of deep neural networks. While their training is memory-efficient,\nthey are still significantly slower to train than their explicit counterparts.\nIn Deep Equilibrium Models (DEQs), the training is performed as a bi-level\nproblem, and its computational complexity is partially driven by the iterative\ninversion of a huge Jacobian matrix. In this paper, we propose a novel strategy\nto tackle this computational bottleneck from which many bi-level problems\nsuffer. The main idea is to use the quasi-Newton matrices from the forward pass\nto efficiently approximate the inverse Jacobian matrix in the direction needed\nfor the gradient computation. We provide a theorem that motivates using our\nmethod with the original forward algorithms. In addition, by modifying these\nforward algorithms, we further provide theoretical guarantees that our method\nasymptotically estimates the true implicit gradient. We empirically study this\napproach in many settings, ranging from hyperparameter optimization to large\nMultiscale DEQs applied to CIFAR and ImageNet. We show that it reduces the\ncomputational cost of the backward pass by up to two orders of magnitude. All\nthis is achieved while retaining the excellent performance of the original\nmodels in hyperparameter optimization and on CIFAR, and giving encouraging and\ncompetitive results on ImageNet.",
          "link": "http://arxiv.org/abs/2106.00553",
          "publishedOn": "2021-06-25T02:00:47.557Z",
          "wordCount": 682,
          "title": "SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models. (arXiv:2106.00553v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07879",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Subhani_M/0/1/0/all/0/1\">Moeez M. Subhani</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Anjum_A/0/1/0/all/0/1\">Ashiq Anjum</a>",
          "description": "Clinical predictions using clinical data by computational methods are common\nin bioinformatics. However, clinical predictions using information from\ngenomics datasets as well is not a frequently observed phenomenon in research.\nPrecision medicine research requires information from all available datasets to\nprovide intelligent clinical solutions. In this paper, we have attempted to\ncreate a prediction model which uses information from both clinical and\ngenomics datasets. We have demonstrated multiclass disease predictions based on\ncombined clinical and genomics datasets using machine learning methods. We have\ncreated an integrated dataset, using a clinical (ClinVar) and a genomics (gene\nexpression) dataset, and trained it using instance-based learner to predict\nclinical diseases. We have used an innovative but simple way for multiclass\nclassification, where the number of output classes is as high as 75. We have\nused Principal Component Analysis for feature selection. The classifier\npredicted diseases with 73\\% accuracy on the integrated dataset. The results\nwere consistent and competent when compared with other classification models.\nThe results show that genomics information can be reliably included in datasets\nfor clinical predictions and it can prove to be valuable in clinical\ndiagnostics and precision medicine.",
          "link": "http://arxiv.org/abs/2006.07879",
          "publishedOn": "2021-06-25T02:00:47.552Z",
          "wordCount": 653,
          "title": "Multiclass Disease Predictions Based on Integrated Clinical and Genomics Datasets. (arXiv:2006.07879v1 [q-bio.GN] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troshin_S/0/1/0/all/0/1\">Sergey Troshin</a>",
          "description": "Initially developed for natural language processing (NLP), Transformers are\nnow widely used for source code processing, due to the format similarity\nbetween source code and text. In contrast to natural language, source code is\nstrictly structured, i.e., it follows the syntax of the programming language.\nSeveral recent works develop Transformer modifications for capturing syntactic\ninformation in source code. The drawback of these works is that they do not\ncompare to each other and consider different tasks. In this work, we conduct a\nthorough empirical study of the capabilities of Transformers to utilize\nsyntactic information in different tasks. We consider three tasks (code\ncompletion, function naming and bug fixing) and re-implement different\nsyntax-capturing modifications in a unified framework. We show that\nTransformers are able to make meaningful predictions based purely on syntactic\ninformation and underline the best practices of taking the syntactic\ninformation into account for improving the performance of the model.",
          "link": "http://arxiv.org/abs/2010.07987",
          "publishedOn": "2021-06-25T02:00:47.547Z",
          "wordCount": 632,
          "title": "Empirical Study of Transformers for Source Code. (arXiv:2010.07987v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neumeier_M/0/1/0/all/0/1\">Marion Neumeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tollkuhn_A/0/1/0/all/0/1\">Andreas Tollk&#xfc;hn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berberich_T/0/1/0/all/0/1\">Thomas Berberich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botsch_M/0/1/0/all/0/1\">Michael Botsch</a>",
          "description": "This paper introduces the Descriptive Variational Autoencoder (DVAE), an\nunsupervised and end-to-end trainable neural network for predicting vehicle\ntrajectories that provides partial interpretability. The novel approach is\nbased on the architecture and objective of common variational autoencoders. By\nintroducing expert knowledge within the decoder part of the autoencoder, the\nencoder learns to extract latent parameters that provide a graspable meaning in\nhuman terms. Such an interpretable latent space enables the validation by\nexpert defined rule sets. The evaluation of the DVAE is performed using the\npublicly available highD dataset for highway traffic scenarios. In comparison\nto a conventional variational autoencoder with equivalent complexity, the\nproposed model provides a similar prediction accuracy but with the great\nadvantage of having an interpretable latent space. For crucial decision making\nand assessing trustworthiness of a prediction this property is highly\ndesirable.",
          "link": "http://arxiv.org/abs/2103.13726",
          "publishedOn": "2021-06-25T02:00:47.533Z",
          "wordCount": 607,
          "title": "Variational Autoencoder-Based Vehicle Trajectory Prediction with an Interpretable Latent Space. (arXiv:2103.13726v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pogrebnyakov_N/0/1/0/all/0/1\">Nicolai Pogrebnyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaghaghian_S/0/1/0/all/0/1\">Shohreh Shaghaghian</a>",
          "description": "Transfer learning methods, and in particular domain adaptation, help exploit\nlabeled data in one domain to improve the performance of a certain task in\nanother domain. However, it is still not clear what factors affect the success\nof domain adaptation. This paper models adaptation success and selection of the\nmost suitable source domains among several candidates in text similarity. We\nuse descriptive domain information and cross-domain similarity metrics as\npredictive features. While mostly positive, the results also point to some\ndomains where adaptation success was difficult to predict.",
          "link": "http://arxiv.org/abs/2106.04641",
          "publishedOn": "2021-06-25T02:00:47.527Z",
          "wordCount": 543,
          "title": "Predicting the Success of Domain Adaptation in Text Similarity. (arXiv:2106.04641v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lili Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kevin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1\">Aravind Rajeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kimin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1\">Aditya Grover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskin_M/0/1/0/all/0/1\">Michael Laskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1\">Aravind Srinivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>",
          "description": "We introduce a framework that abstracts Reinforcement Learning (RL) as a\nsequence modeling problem. This allows us to draw upon the simplicity and\nscalability of the Transformer architecture, and associated advances in\nlanguage modeling such as GPT-x and BERT. In particular, we present Decision\nTransformer, an architecture that casts the problem of RL as conditional\nsequence modeling. Unlike prior approaches to RL that fit value functions or\ncompute policy gradients, Decision Transformer simply outputs the optimal\nactions by leveraging a causally masked Transformer. By conditioning an\nautoregressive model on the desired return (reward), past states, and actions,\nour Decision Transformer model can generate future actions that achieve the\ndesired return. Despite its simplicity, Decision Transformer matches or exceeds\nthe performance of state-of-the-art model-free offline RL baselines on Atari,\nOpenAI Gym, and Key-to-Door tasks.",
          "link": "http://arxiv.org/abs/2106.01345",
          "publishedOn": "2021-06-25T02:00:47.522Z",
          "wordCount": 603,
          "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling. (arXiv:2106.01345v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hambly_B/0/1/0/all/0/1\">Ben Hambly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huining Yang</a>",
          "description": "We explore reinforcement learning methods for finding the optimal policy in\nthe linear quadratic regulator (LQR) problem. In particular, we consider the\nconvergence of policy gradient methods in the setting of known and unknown\nparameters. We are able to produce a global linear convergence guarantee for\nthis approach in the setting of finite time horizon and stochastic state\ndynamics under weak assumptions. The convergence of a projected policy gradient\nmethod is also established in order to handle problems with constraints. We\nillustrate the performance of the algorithm with two examples. The first\nexample is the optimal liquidation of a holding in an asset. We show results\nfor the case where we assume a model for the underlying dynamics and where we\napply the method to the data directly. The empirical evidence suggests that the\npolicy gradient method can learn the global optimal solution for a larger class\nof stochastic systems containing the LQR framework and that it is more robust\nwith respect to model mis-specification when compared to a model-based\napproach. The second example is an LQR system in a higher dimensional setting\nwith synthetic data.",
          "link": "http://arxiv.org/abs/2011.10300",
          "publishedOn": "2021-06-25T02:00:47.517Z",
          "wordCount": 665,
          "title": "Policy Gradient Methods for the Noisy Linear Quadratic Regulator over a Finite Horizon. (arXiv:2011.10300v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1\">Rachid Guerraoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nirupam Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinot_R/0/1/0/all/0/1\">Rafa&#xeb;l Pinot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouault_S/0/1/0/all/0/1\">S&#xe9;bastien Rouault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stephan_J/0/1/0/all/0/1\">John Stephan</a>",
          "description": "This paper addresses the problem of combining Byzantine resilience with\nprivacy in machine learning (ML). Specifically, we study if a distributed\nimplementation of the renowned Stochastic Gradient Descent (SGD) learning\nalgorithm is feasible with both differential privacy (DP) and\n$(\\alpha,f)$-Byzantine resilience. To the best of our knowledge, this is the\nfirst work to tackle this problem from a theoretical point of view. A key\nfinding of our analyses is that the classical approaches to these two\n(seemingly) orthogonal issues are incompatible. More precisely, we show that a\ndirect composition of these techniques makes the guarantees of the resulting\nSGD algorithm depend unfavourably upon the number of parameters of the ML\nmodel, making the training of large models practically infeasible. We validate\nour theoretical results through numerical experiments on publicly-available\ndatasets; showing that it is impractical to ensure DP and Byzantine resilience\nsimultaneously.",
          "link": "http://arxiv.org/abs/2102.08166",
          "publishedOn": "2021-06-25T02:00:47.512Z",
          "wordCount": 629,
          "title": "Differential Privacy and Byzantine Resilience in SGD: Do They Add Up?. (arXiv:2102.08166v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Delaney_E/0/1/0/all/0/1\">Eoin Delaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greene_D/0/1/0/all/0/1\">Derek Greene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keane_M/0/1/0/all/0/1\">Mark T. Keane</a>",
          "description": "In recent years, there has been a rapidly expanding focus on explaining the\npredictions made by black-box AI systems that handle image and tabular data.\nHowever, considerably less attention has been paid to explaining the\npredictions of opaque AI systems handling time series data. In this paper, we\nadvance a novel model-agnostic, case-based technique -- Native Guide -- that\ngenerates counterfactual explanations for time series classifiers. Given a\nquery time series, $T_{q}$, for which a black-box classification system\npredicts class, $c$, a counterfactual time series explanation shows how $T_{q}$\ncould change, such that the system predicts an alternative class, $c'$. The\nproposed instance-based technique adapts existing counterfactual instances in\nthe case-base by highlighting and modifying discriminative areas of the time\nseries that underlie the classification. Quantitative and qualitative results\nfrom two comparative experiments indicate that Native Guide generates\nplausible, proximal, sparse and diverse explanations that are better than those\nproduced by key benchmark counterfactual methods.",
          "link": "http://arxiv.org/abs/2009.13211",
          "publishedOn": "2021-06-25T02:00:47.498Z",
          "wordCount": 615,
          "title": "Instance-based Counterfactual Explanations for Time Series Classification. (arXiv:2009.13211v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sangwoong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_Y/0/1/0/all/0/1\">Yung-Kyun Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1\">Frank Chongwoo Park</a>",
          "description": "Likelihood is a standard estimate for outlier detection. The specific role of\nthe normalization constraint is to ensure that the out-of-distribution (OOD)\nregime has a small likelihood when samples are learned using maximum\nlikelihood. Because autoencoders do not possess such a process of\nnormalization, they often fail to recognize outliers even when they are\nobviously OOD. We propose the Normalized Autoencoder (NAE), a normalized\nprobabilistic model constructed from an autoencoder. The probability density of\nNAE is defined using the reconstruction error of an autoencoder, which is\ndifferently defined in the conventional energy-based model. In our model,\nnormalization is enforced by suppressing the reconstruction of negative\nsamples, significantly improving the outlier detection performance. Our\nexperimental results confirm the efficacy of NAE, both in detecting outliers\nand in generating in-distribution samples.",
          "link": "http://arxiv.org/abs/2105.05735",
          "publishedOn": "2021-06-25T02:00:47.492Z",
          "wordCount": 611,
          "title": "Autoencoding Under Normalization Constraints. (arXiv:2105.05735v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02609",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Pananjady_A/0/1/0/all/0/1\">Ashwin Pananjady</a>, <a href=\"http://arxiv.org/find/math/1/au:+Samworth_R/0/1/0/all/0/1\">Richard J. Samworth</a>",
          "description": "Motivated by models for multiway comparison data, we consider the problem of\nestimating a coordinate-wise isotonic function on the domain $[0, 1]^d$ from\nnoisy observations collected on a uniform lattice, but where the design points\nhave been permuted along each dimension. While the univariate and bivariate\nversions of this problem have received significant attention, our focus is on\nthe multivariate case $d \\geq 3$. We study both the minimax risk of estimation\n(in empirical $L_2$ loss) and the fundamental limits of adaptation (quantified\nby the adaptivity index) to a family of piecewise constant functions. We\nprovide a computationally efficient Mirsky partition estimator that is minimax\noptimal while also achieving the smallest adaptivity index possible for\npolynomial time procedures. Thus, from a worst-case perspective and in sharp\ncontrast to the bivariate case, the latent permutations in the model do not\nintroduce significant computational difficulties over and above vanilla\nisotonic regression. On the other hand, the fundamental limits of adaptation\nare significantly different with and without unknown permutations: Assuming a\nhardness conjecture from average-case complexity theory, a\nstatistical-computational gap manifests in the former case. In a complementary\ndirection, we show that natural modifications of existing estimators fail to\nsatisfy at least one of the desiderata of optimal worst-case statistical\nperformance, computational efficiency, and fast adaptation. Along the way to\nshowing our results, we improve adaptation results in the special case $d = 2$\nand establish some properties of estimators for vanilla isotonic regression,\nboth of which may be of independent interest.",
          "link": "http://arxiv.org/abs/2009.02609",
          "publishedOn": "2021-06-25T02:00:47.487Z",
          "wordCount": 725,
          "title": "Isotonic regression with unknown permutations: Statistics, computation, and adaptation. (arXiv:2009.02609v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sater_R/0/1/0/all/0/1\">Raed Abdel Sater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1\">A. Ben Hamza</a>",
          "description": "Internet of Things (IoT) sensors in smart buildings are becoming increasingly\nubiquitous, making buildings more livable, energy efficient, and sustainable.\nThese devices sense the environment and generate multivariate temporal data of\nparamount importance for detecting anomalies and improving the prediction of\nenergy usage in smart buildings. However, detecting these anomalies in\ncentralized systems is often plagued by a huge delay in response time. To\novercome this issue, we formulate the anomaly detection problem in a federated\nlearning setting by leveraging the multi-task learning paradigm, which aims at\nsolving multiple tasks simultaneously while taking advantage of the\nsimilarities and differences across tasks. We propose a novel privacy-by-design\nfederated learning model using a stacked long short-time memory (LSTM) model,\nand we demonstrate that it is more than twice as fast during training\nconvergence compared to the centralized LSTM. The effectiveness of our\nfederated learning approach is demonstrated on three real-world datasets\ngenerated by the IoT production system at General Electric Current smart\nbuilding, achieving state-of-the-art performance compared to baseline methods\nin both classification and regression tasks. Our experimental results\ndemonstrate the effectiveness of the proposed framework in reducing the overall\ntraining cost without compromising the prediction performance.",
          "link": "http://arxiv.org/abs/2010.10293",
          "publishedOn": "2021-06-25T02:00:47.480Z",
          "wordCount": 664,
          "title": "A Federated Learning Approach to Anomaly Detection in Smart Buildings. (arXiv:2010.10293v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.01849",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bauch_J/0/1/0/all/0/1\">Jonathan Bauch</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nadler_B/0/1/0/all/0/1\">Boaz Nadler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zilber_P/0/1/0/all/0/1\">Pini Zilber</a>",
          "description": "We present a new, simple and computationally efficient iterative method for\nlow rank matrix completion. Our method is inspired by the class of\nfactorization-type iterative algorithms, but substantially differs from them in\nthe way the problem is cast. Precisely, given a target rank $r$, instead of\noptimizing on the manifold of rank $r$ matrices, we allow our interim estimated\nmatrix to have a specific over-parametrized rank $2r$ structure. Our algorithm,\ndenoted R2RILS for rank $2r$ iterative least squares, has low memory\nrequirements, and at each iteration it solves a computationally cheap sparse\nleast-squares problem. We motivate our algorithm by its theoretical analysis\nfor the simplified case of a rank-1 matrix. Empirically, R2RILS is able to\nrecover ill conditioned low rank matrices from very few observations -- near\nthe information limit, and it is stable to additive noise.",
          "link": "http://arxiv.org/abs/2002.01849",
          "publishedOn": "2021-06-25T02:00:47.466Z",
          "wordCount": 602,
          "title": "Rank $2r$ iterative least squares: efficient recovery of ill-conditioned low rank matrices from few entries. (arXiv:2002.01849v2 [math.OC] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13202",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Park_J/0/1/0/all/0/1\">Ju An Park</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Thomas_K/0/1/0/all/0/1\">Kathryn E. Thomas</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deglint_J/0/1/0/all/0/1\">Jason L. Deglint</a>",
          "description": "Warming oceans due to climate change are leading to increased numbers of\nectoparasitic copepods, also known as sea lice, which can cause significant\necological loss to wild salmon populations and major economic loss to\naquaculture sites. The main transport mechanism driving the spread of sea lice\npopulations are near-surface ocean currents. Present strategies to estimate the\ndistribution of sea lice larvae are computationally complex and limit\nfull-scale analysis. Motivated to address this challenge, we propose SALT: Sea\nlice Adaptive Lattice Tracking approach for efficient estimation of sea lice\ndispersion and distribution in space and time. Specifically, an adaptive\nspatial mesh is generated by merging nodes in the lattice graph of the Ocean\nModel based on local ocean properties, thus enabling highly efficient graph\nrepresentation. SALT demonstrates improved efficiency while maintaining\nconsistent results with the standard method, using near-surface current data\nfor Hardangerfjord, Norway. The proposed SALT technique shows promise for\nenhancing proactive aquaculture management through predictive modelling of sea\nlice infestation pressure maps in a changing climate.",
          "link": "http://arxiv.org/abs/2106.13202",
          "publishedOn": "2021-06-25T02:00:47.455Z",
          "wordCount": 631,
          "title": "SALT: Sea lice Adaptive Lattice Tracking -- An Unsupervised Approach to Generate an Improved Ocean Model. (arXiv:2106.13202v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08393",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sadhu_S/0/1/0/all/0/1\">Samik Sadhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Che-Wei Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mallidi_S/0/1/0/all/0/1\">Sri Harish Mallidi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Minhua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Droppo_J/0/1/0/all/0/1\">Jasha Droppo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maas_R/0/1/0/all/0/1\">Roland Maas</a>",
          "description": "Wav2vec-C introduces a novel representation learning technique combining\nelements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce quantized\nrepresentations from partially masked speech encoding using a contrastive loss\nin a way similar to Wav2vec 2.0. However, the quantization process is\nregularized by an additional consistency network that learns to reconstruct the\ninput features to the wav2vec 2.0 network from the quantized representations in\na way similar to a VQ-VAE model. The proposed self-supervised model is trained\non 10k hours of unlabeled data and subsequently used as the speech encoder in a\nRNN-T ASR model and fine-tuned with 1k hours of labeled data. This work is one\nof only a few studies of self-supervised learning on speech tasks with a large\nvolume of real far-field labeled data. The Wav2vec-C encoded representations\nachieves, on average, twice the error reduction over baseline and a higher\ncodebook utilization in comparison to wav2vec 2.0",
          "link": "http://arxiv.org/abs/2103.08393",
          "publishedOn": "2021-06-25T02:00:47.438Z",
          "wordCount": 630,
          "title": "Wav2vec-C: A Self-supervised Model for Speech Representation Learning. (arXiv:2103.08393v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rebrova_E/0/1/0/all/0/1\">Elizaveta Rebrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yu-Hang Tang</a>",
          "description": "We introduce and investigate matrix approximation by decomposition into a sum\nof radial basis function (RBF) components. An RBF component is a generalization\nof the outer product between a pair of vectors, where an RBF function replaces\nthe scalar multiplication between individual vector elements. Even though the\nRBF functions are positive definite, the summation across components is not\nrestricted to convex combinations and allows us to compute the decomposition\nfor any real matrix that is not necessarily symmetric or positive definite. We\nformulate the problem of seeking such a decomposition as an optimization\nproblem with a nonlinear and non-convex loss function. Several modern versions\nof the gradient descent method, including their scalable stochastic\ncounterparts, are used to solve this problem. We provide extensive empirical\nevidence of the effectiveness of the RBF decomposition and that of the\ngradient-based fitting algorithm. While being conceptually motivated by\nsingular value decomposition (SVD), our proposed nonlinear counterpart\noutperforms SVD by drastically reducing the memory required to approximate a\ndata matrix with the same L2 error for a wide range of matrix types. For\nexample, it leads to 2 to 6 times memory save for Gaussian noise, graph\nadjacency matrices, and kernel matrices. Moreover, this proximity-based\ndecomposition can offer additional interpretability in applications that\ninvolve, e.g., capturing the inner low-dimensional structure of the data,\nretaining graph connectivity structure, and preserving the acutance of images.",
          "link": "http://arxiv.org/abs/2106.02018",
          "publishedOn": "2021-06-25T02:00:47.425Z",
          "wordCount": 676,
          "title": "Nonlinear Matrix Approximation with Radial Basis Function Components. (arXiv:2106.02018v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13188",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ren_M/0/1/0/all/0/1\">Mengwei Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Heejong Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_N/0/1/0/all/0/1\">Neel Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gerig_G/0/1/0/all/0/1\">Guido Gerig</a>",
          "description": "Current deep learning approaches for diffusion MRI modeling circumvent the\nneed for densely-sampled diffusion-weighted images (DWIs) by directly\npredicting microstructural indices from sparsely-sampled DWIs. However, they\nimplicitly make unrealistic assumptions of static $q$-space sampling during\ntraining and reconstruction. Further, such approaches can restrict downstream\nusage of variably sampled DWIs for usages including the estimation of\nmicrostructural indices or tractography. We propose a generative adversarial\ntranslation framework for high-quality DWI synthesis with arbitrary $q$-space\nsampling given commonly acquired structural images (e.g., B0, T1, T2). Our\ntranslation network linearly modulates its internal representations conditioned\non continuous $q$-space information, thus removing the need for fixed sampling\nschemes. Moreover, this approach enables downstream estimation of high-quality\nmicrostructural maps from arbitrarily subsampled DWIs, which may be\nparticularly important in cases with sparsely sampled DWIs. Across several\nrecent methodologies, the proposed approach yields improved DWI synthesis\naccuracy and fidelity with enhanced downstream utility as quantified by the\naccuracy of scalar microstructure indices estimated from the synthesized\nimages. Code is available at\nhttps://github.com/mengweiren/q-space-conditioned-dwi-synthesis.",
          "link": "http://arxiv.org/abs/2106.13188",
          "publishedOn": "2021-06-25T02:00:47.419Z",
          "wordCount": 645,
          "title": "Q-space Conditioned Translation Networks for Directional Synthesis of Diffusion Weighted Images from Multi-modal Structural MRI. (arXiv:2106.13188v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10745",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1\">Adrian Celaya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Actor_J/0/1/0/all/0/1\">Jonas A. Actor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muthusivarajan_R/0/1/0/all/0/1\">Rajarajeswari Muthusivarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gates_E/0/1/0/all/0/1\">Evan Gates</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_C/0/1/0/all/0/1\">Caroline Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schellingerhout_D/0/1/0/all/0/1\">Dawid Schellingerhout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1\">Beatrice Riviere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1\">David Fuentes</a>",
          "description": "Medical imaging deep learning models are often large and complex, requiring\nspecialized hardware to train and evaluate these models. To address such\nissues, we propose the PocketNet paradigm to reduce the size of deep learning\nmodels by throttling the growth of the number of channels in convolutional\nneural networks. We demonstrate that, for a range of segmentation and\nclassification tasks, PocketNet architectures produce results comparable to\nthat of conventional neural networks while reducing the number of parameters by\nmultiple orders of magnitude, using up to 90% less GPU memory, and speeding up\ntraining times by up to 40%, thereby allowing such models to be trained and\ndeployed in resource-constrained settings.",
          "link": "http://arxiv.org/abs/2104.10745",
          "publishedOn": "2021-06-25T02:00:47.411Z",
          "wordCount": 586,
          "title": "PocketNet: A Smaller Neural Network for Medical Image Analysis. (arXiv:2104.10745v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gordillo_C/0/1/0/all/0/1\">Camilo Gordillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergdahl_J/0/1/0/all/0/1\">Joakim Bergdahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tollmar_K/0/1/0/all/0/1\">Konrad Tollmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gisslen_L/0/1/0/all/0/1\">Linus Gissl&#xe9;n</a>",
          "description": "As modern games continue growing both in size and complexity, it has become\nmore challenging to ensure that all the relevant content is tested and that any\npotential issue is properly identified and fixed. Attempting to maximize\ntesting coverage using only human participants, however, results in a tedious\nand hard to orchestrate process which normally slows down the development\ncycle. Complementing playtesting via autonomous agents has shown great promise\naccelerating and simplifying this process. This paper addresses the problem of\nautomatically exploring and testing a given scenario using reinforcement\nlearning agents trained to maximize game state coverage. Each of these agents\nis rewarded based on the novelty of its actions, thus encouraging a curious and\nexploratory behaviour on a complex 3D scenario where previously proposed\nexploration techniques perform poorly. The curious agents are able to learn the\ncomplex navigation mechanics required to reach the different areas around the\nmap, thus providing the necessary data to identify potential issues. Moreover,\nthe paper also explores different visualization strategies and evaluates how to\nmake better use of the collected data to drive design decisions and to\nrecognize possible problems and oversights.",
          "link": "http://arxiv.org/abs/2103.13798",
          "publishedOn": "2021-06-25T02:00:47.406Z",
          "wordCount": 647,
          "title": "Improving Playtesting Coverage via Curiosity Driven Reinforcement Learning Agents. (arXiv:2103.13798v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yifan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Liyao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>",
          "description": "Federated learning is emerging as a machine learning technique that trains a\nmodel across multiple decentralized parties. It is renowned for preserving\nprivacy as the data never leaves the computational devices, and recent\napproaches further enhance its privacy by hiding messages transferred in\nencryption. However, we found that despite the efforts, federated learning\nremains privacy-threatening, due to its interactive nature across different\nparties. In this paper, we analyze the privacy threats in industrial-level\nfederated learning frameworks with secure computation, and reveal such threats\nwidely exist in typical machine learning models such as linear regression,\nlogistic regression and decision tree. For the linear and logistic regression,\nwe show through theoretical analysis that it is possible for the attacker to\ninvert the entire private input of the victim, given very few information. For\nthe decision tree model, we launch an attack to infer the range of victim's\nprivate inputs. All attacks are evaluated on popular federated learning\nframeworks and real-world datasets.",
          "link": "http://arxiv.org/abs/2106.13076",
          "publishedOn": "2021-06-25T02:00:47.389Z",
          "wordCount": 601,
          "title": "Privacy Threats Analysis to Secure Federated Learning. (arXiv:2106.13076v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qiyue Yin</a>",
          "description": "Model-based reinforcement learning (MBRL) is believed to have higher sample\nefficiency compared with model-free reinforcement learning (MFRL). However,\nMBRL is plagued by dynamics bottleneck dilemma. Dynamics bottleneck dilemma is\nthe phenomenon that the performance of the algorithm falls into the local\noptimum instead of increasing when the interaction step with the environment\nincreases, which means more data can not bring better performance. In this\npaper, we find that the trajectory reward estimation error is the main reason\nthat causes dynamics bottleneck dilemma through theoretical analysis. We give\nan upper bound of the trajectory reward estimation error and point out that\nincreasing the agent's exploration ability is the key to reduce trajectory\nreward estimation error, thereby alleviating dynamics bottleneck dilemma.\nMotivated by this, a model-based control method combined with exploration named\nMOdel-based Progressive Entropy-based Exploration (MOPE2) is proposed. We\nconduct experiments on several complex continuous control benchmark tasks. The\nresults verify that MOPE2 can effectively alleviate dynamics bottleneck dilemma\nand have higher sample efficiency than previous MBRL and MFRL algorithms.",
          "link": "http://arxiv.org/abs/2010.12914",
          "publishedOn": "2021-06-25T02:00:47.357Z",
          "wordCount": 649,
          "title": "Planning with Exploration: Addressing Dynamics Bottleneck in Model-based Reinforcement Learning. (arXiv:2010.12914v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>",
          "description": "Structural features are important features in graph datasets. However,\nalthough there are some correlation analysis of features based on covariance,\nthere is no relevant research on exploring structural feature correlation on\ngraphs with graph neural network based models. In this paper, we introduce\ngraph feature to feature (Fea2Fea) prediction pipelines in a low dimensional\nspace to explore some preliminary results on structural feature correlation,\nwhich is based on graph neural network. The results show that there exists high\ncorrelation between some of the structural features. A redundant feature\ncombination with initial node features, which is filtered by graph neural\nnetwork has improved its classification accuracy in some graph datasets. We\ncompare the difference between concatenation methods on connecting embeddings\nbetween features and show that the simplest is the best. We generalize on the\nsynthetic geometric graphs and certify the results on prediction difficulty\nbetween two structural features.",
          "link": "http://arxiv.org/abs/2106.13061",
          "publishedOn": "2021-06-25T02:00:47.351Z",
          "wordCount": 596,
          "title": "Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks. (arXiv:2106.13061v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+George_Y/0/1/0/all/0/1\">Yasmeen George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karunasekera_S/0/1/0/all/0/1\">Shanika Karunasekera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwood_A/0/1/0/all/0/1\">Aaron Harwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kwan Hui Lim</a>",
          "description": "A key challenge in mining social media data streams is to identify events\nwhich are actively discussed by a group of people in a specific local or global\narea. Such events are useful for early warning for accident, protest, election\nor breaking news. However, neither the list of events nor the resolution of\nboth event time and space is fixed or known beforehand. In this work, we\npropose an online spatio-temporal event detection system using social media\nthat is able to detect events at different time and space resolutions. First,\nto address the challenge related to the unknown spatial resolution of events, a\nquad-tree method is exploited in order to split the geographical space into\nmultiscale regions based on the density of social media data. Then, a\nstatistical unsupervised approach is performed that involves Poisson\ndistribution and a smoothing method for highlighting regions with unexpected\ndensity of social posts. Further, event duration is precisely estimated by\nmerging events happening in the same region at consecutive time intervals. A\npost processing stage is introduced to filter out events that are spam, fake or\nwrong. Finally, we incorporate simple semantics by using social media entities\nto assess the integrity, and accuracy of detected events. The proposed method\nis evaluated using different social media datasets: Twitter and Flickr for\ndifferent cities: Melbourne, London, Paris and New York. To verify the\neffectiveness of the proposed method, we compare our results with two baseline\nalgorithms based on fixed split of geographical space and clustering method.\nFor performance evaluation, we manually compute recall and precision. We also\npropose a new quality measure named strength index, which automatically\nmeasures how accurate the reported event is.",
          "link": "http://arxiv.org/abs/2106.13121",
          "publishedOn": "2021-06-25T02:00:47.336Z",
          "wordCount": 729,
          "title": "Real-time Spatio-temporal Event Detection on Geotagged Social Media. (arXiv:2106.13121v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagadeesan_M/0/1/0/all/0/1\">Meena Jagadeesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendler_Dunner_C/0/1/0/all/0/1\">Celestine Mendler-D&#xfc;nner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1\">Moritz Hardt</a>",
          "description": "When reasoning about strategic behavior in a machine learning context it is\ntempting to combine standard microfoundations of rational agents with the\nstatistical decision theory underlying classification. In this work, we argue\nthat a direct combination of these standard ingredients leads to brittle\nsolution concepts of limited descriptive and prescriptive value. First, we show\nthat rational agents with perfect information produce discontinuities in the\naggregate response to a decision rule that we often do not observe empirically.\nSecond, when any positive fraction of agents is not perfectly strategic,\ndesirable stable points -- where the classifier is optimal for the data it\nentails -- cease to exist. Third, optimal decision rules under standard\nmicrofoundations maximize a measure of negative externality known as social\nburden within a broad class of possible assumptions about agent behavior.\n\nRecognizing these limitations we explore alternatives to standard\nmicrofoundations for binary classification. We start by describing a set of\ndesiderata that help navigate the space of possible assumptions about how\nagents respond to a decision rule. In particular, we analyze a natural\nconstraint on feature manipulations, and discuss properties that are sufficient\nto guarantee the robust existence of stable points. Building on these insights,\nwe then propose the noisy response model. Inspired by smoothed analysis and\nempirical observations, noisy response incorporates imperfection in the agent\nresponses, which we show mitigates the limitations of standard\nmicrofoundations. Our model retains analytical tractability, leads to more\nrobust insights about stable points, and imposes a lower social burden at\noptimality.",
          "link": "http://arxiv.org/abs/2106.12705",
          "publishedOn": "2021-06-25T02:00:47.330Z",
          "wordCount": 695,
          "title": "Alternative Microfoundations for Strategic Classification. (arXiv:2106.12705v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>",
          "description": "We analyze the process of creating word embedding feature representations\ndesigned for a learning task when annotated data is scarce, for example, in\ndepressive language detection from Tweets. We start with a rich word embedding\npre-trained from a large general dataset, which is then augmented with\nembeddings learned from a much smaller and more specific domain dataset through\na simple non-linear mapping mechanism. We also experimented with several other\nmore sophisticated methods of such mapping including, several auto-encoder\nbased and custom loss-function based methods that learn embedding\nrepresentations through gradually learning to be close to the words of similar\nsemantics and distant to dissimilar semantics. Our strengthened representations\nbetter capture the semantics of the depression domain, as it combines the\nsemantics learned from the specific domain coupled with word coverage from the\ngeneral language. We also present a comparative performance analyses of our\nword embedding representations with a simple bag-of-words model, well known\nsentiment and psycholinguistic lexicons, and a general pre-trained word\nembedding. When used as feature representations for several different machine\nlearning methods, including deep learning models in a depressive Tweets\nidentification task, we show that our augmented word embedding representations\nachieve a significantly better F1 score than the others, specially when applied\nto a high quality dataset. Also, we present several data ablation tests which\nconfirm the efficacy of our augmentation techniques.",
          "link": "http://arxiv.org/abs/2106.12797",
          "publishedOn": "2021-06-25T02:00:47.325Z",
          "wordCount": 707,
          "title": "A comprehensive empirical analysis on cross-domain semantic enrichment for detection of depressive language. (arXiv:2106.12797v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allmendinger_R/0/1/0/all/0/1\">Richard Allmendinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Ibanez_M/0/1/0/all/0/1\">Manuel L&#xf3;pez-Ib&#xe1;&#xf1;ez</a>",
          "description": "Safe learning and optimization deals with learning and optimization problems\nthat avoid, as much as possible, the evaluation of non-safe input points, which\nare solutions, policies, or strategies that cause an irrecoverable loss (e.g.,\nbreakage of a machine or equipment, or life threat). Although a comprehensive\nsurvey of safe reinforcement learning algorithms was published in 2015, a\nnumber of new algorithms have been proposed thereafter, and related works in\nactive learning and in optimization were not considered. This paper reviews\nthose algorithms from a number of domains including reinforcement learning,\nGaussian process regression and classification, evolutionary algorithms, and\nactive learning. We provide the fundamental concepts on which the reviewed\nalgorithms are based and a characterization of the individual algorithms. We\nconclude by explaining how the algorithms are connected and suggestions for\nfuture research.",
          "link": "http://arxiv.org/abs/2101.09505",
          "publishedOn": "2021-06-25T02:00:47.287Z",
          "wordCount": 672,
          "title": "Safe Learning and Optimization Techniques: Towards a Survey of the State of the Art. (arXiv:2101.09505v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhifeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>",
          "description": "In this work, we propose FastDPM, a unified framework for fast sampling in\ndiffusion probabilistic models. FastDPM generalizes previous methods and gives\nrise to new algorithms with improved sample quality. We systematically\ninvestigate the fast sampling methods under this framework across different\ndomains, on different datasets, and with different amount of conditional\ninformation provided for generation. We find the performance of a particular\nmethod depends on data domains (e.g., image or audio), the trade-off between\nsampling speed and sample quality, and the amount of conditional information.\nWe further provide insights and recipes on the choice of methods for\npractitioners.",
          "link": "http://arxiv.org/abs/2106.00132",
          "publishedOn": "2021-06-25T02:00:47.272Z",
          "wordCount": 544,
          "title": "On Fast Sampling of Diffusion Probabilistic Models. (arXiv:2106.00132v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Da Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huishuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "We propose a reparametrization scheme to address the challenges of applying\ndifferentially private SGD on large neural networks, which are 1) the huge\nmemory cost of storing individual gradients, 2) the added noise suffering\nnotorious dimensional dependence. Specifically, we reparametrize each weight\nmatrix with two \\emph{gradient-carrier} matrices of small dimension and a\n\\emph{residual weight} matrix. We argue that such reparametrization keeps the\nforward/backward process unchanged while enabling us to compute the projected\ngradient without computing the gradient itself. To learn with differential\nprivacy, we design \\emph{reparametrized gradient perturbation (RGP)} that\nperturbs the gradients on gradient-carrier matrices and reconstructs an update\nfor the original weight from the noisy gradients. Importantly, we use\nhistorical updates to find the gradient-carrier matrices, whose optimality is\nrigorously justified under linear regression and empirically verified with deep\nlearning tasks. RGP significantly reduces the memory cost and improves the\nutility. For example, we are the first able to apply differential privacy on\nthe BERT model and achieve an average accuracy of $83.9\\%$ on four downstream\ntasks with $\\epsilon=8$, which is within $5\\%$ loss compared to the non-private\nbaseline but enjoys much lower privacy leakage risk.",
          "link": "http://arxiv.org/abs/2106.09352",
          "publishedOn": "2021-06-25T02:00:47.261Z",
          "wordCount": 661,
          "title": "Large Scale Private Learning via Low-rank Reparametrization. (arXiv:2106.09352v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "In spoken conversational question answering (SCQA), the answer to the\ncorresponding question is generated by retrieving and then analyzing a fixed\nspoken document, including multi-part conversations. Most SCQA systems have\nconsidered only retrieving information from ordered utterances. However, the\nsequential order of dialogue is important to build a robust spoken\nconversational question answering system, and the changes of utterances order\nmay severely result in low-quality and incoherent corpora. To this end, we\nintroduce a self-supervised learning approach, including incoherence\ndiscrimination, insertion detection, and question prediction, to explicitly\ncapture the coreference resolution and dialogue coherence among spoken\ndocuments. Specifically, we design a joint learning framework where the\nauxiliary self-supervised tasks can enable the pre-trained SCQA systems towards\nmore coherent and meaningful spoken dialogue learning. We also utilize the\nproposed self-supervised learning tasks to capture intra-sentence coherence.\nExperimental results demonstrate that our proposed method provides more\ncoherent, meaningful, and appropriate responses, yielding superior performance\ngains compared to the original pre-trained language models. Our method achieves\nstate-of-the-art results on the Spoken-CoQA dataset.",
          "link": "http://arxiv.org/abs/2106.02182",
          "publishedOn": "2021-06-25T02:00:47.230Z",
          "wordCount": 645,
          "title": "Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yunhao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuno_T/0/1/0/all/0/1\">Tadashi Kozuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowland_M/0/1/0/all/0/1\">Mark Rowland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1\">R&#xe9;mi Munos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1\">Michal Valko</a>",
          "description": "Model-agnostic meta-reinforcement learning requires estimating the Hessian\nmatrix of value functions. This is challenging from an implementation\nperspective, as repeatedly differentiating policy gradient estimates may lead\nto biased Hessian estimates. In this work, we provide a unifying framework for\nestimating higher-order derivatives of value functions, based on off-policy\nevaluation. Our framework interprets a number of prior approaches as special\ncases and elucidates the bias and variance trade-off of Hessian estimates. This\nframework also opens the door to a new family of estimates, which can be easily\nimplemented with auto-differentiation libraries, and lead to performance gains\nin practice.",
          "link": "http://arxiv.org/abs/2106.13125",
          "publishedOn": "2021-06-25T02:00:47.219Z",
          "wordCount": 531,
          "title": "Unifying Gradient Estimators for Meta-Reinforcement Learning via Off-Policy Evaluation. (arXiv:2106.13125v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agudelo_Espana_D/0/1/0/all/0/1\">Diego Agudelo-Espa&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemmour_Y/0/1/0/all/0/1\">Yassine Nemmour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jia-Jie Zhu</a>",
          "description": "Random features is a powerful universal function approximator that inherits\nthe theoretical rigor of kernel methods and can scale up to modern learning\ntasks. This paper views uncertain system models as unknown or uncertain smooth\nfunctions in universal reproducing kernel Hilbert spaces. By directly\napproximating the one-step dynamics function using random features with\nuncertain parameters, which are equivalent to a shallow Bayesian neural\nnetwork, we then view the whole dynamical system as a multi-layer neural\nnetwork. Exploiting the structure of Hamiltonian dynamics, we show that finding\nworst-case dynamics realizations using Pontryagin's minimum principle is\nequivalent to performing the Frank-Wolfe algorithm on the deep net. Various\nnumerical experiments on dynamics learning showcase the capacity of our\nmodeling methodology.",
          "link": "http://arxiv.org/abs/2106.13066",
          "publishedOn": "2021-06-25T02:00:47.188Z",
          "wordCount": 565,
          "title": "Shallow Representation is Deep: Learning Uncertainty-aware and Worst-case Random Feature Dynamics. (arXiv:2106.13066v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13109",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Duan_C/0/1/0/all/0/1\">Chenru Duan</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chen_S/0/1/0/all/0/1\">Shuxin Chen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Taylor_M/0/1/0/all/0/1\">Michael G. Taylor</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Liu_F/0/1/0/all/0/1\">Fang Liu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kulik_H/0/1/0/all/0/1\">Heather J. Kulik</a>",
          "description": "Computational virtual high-throughput screening (VHTS) with density\nfunctional theory (DFT) and machine-learning (ML)-acceleration is essential in\nrapid materials discovery. By necessity, efficient DFT-based workflows are\ncarried out with a single density functional approximation (DFA). Nevertheless,\nproperties evaluated with different DFAs can be expected to disagree for the\ncases with challenging electronic structure (e.g., open shell transition metal\ncomplexes, TMCs) for which rapid screening is most needed and accurate\nbenchmarks are often unavailable. To quantify the effect of DFA bias, we\nintroduce an approach to rapidly obtain property predictions from 23\nrepresentative DFAs spanning multiple families and \"rungs\" (e.g., semi-local to\ndouble hybrid) and basis sets on over 2,000 TMCs. Although computed properties\n(e.g., spin-state ordering and frontier orbital gap) naturally differ by DFA,\nhigh linear correlations persist across all DFAs. We train independent ML\nmodels for each DFA and observe convergent trends in feature importance; these\nfeatures thus provide DFA-invariant, universal design rules. We devise a\nstrategy to train ML models informed by all 23 DFAs and use them to predict\nproperties (e.g., spin-splitting energy) of over 182k TMCs. By requiring\nconsensus of the ANN-predicted DFA properties, we improve correspondence of\nthese computational lead compounds with literature-mined, experimental\ncompounds over the single-DFA approach typically employed. Both feature\nanalysis and consensus-based ML provide efficient, alternative paths to\novercome accuracy limitations of practical DFT.",
          "link": "http://arxiv.org/abs/2106.13109",
          "publishedOn": "2021-06-25T02:00:47.183Z",
          "wordCount": 680,
          "title": "Machine learning to tame divergent density functional approximations: a new path to consensus materials design principles. (arXiv:2106.13109v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xueyang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Song Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jingcai Guo</a>",
          "description": "We study the recent emerging personalized federated learning (PFL) that aims\nat dealing with the challenging problem of Non-I.I.D. data in the federated\nlearning (FL) setting. The key difference between PFL and conventional FL lies\nin the training target, of which the personalized models in PFL usually pursue\na trade-off between personalization (i.e., usually from local models) and\ngeneralization (i.e., usually from the global model) on trained models.\nConventional FL methods can hardly meet this target because of their both\nwell-developed global and local models. The prevalent PFL approaches usually\nmaintain a global model to guide the training process of local models and\ntransfer a proper degree of generalization to them. However, the sole global\nmodel can only provide one direction of generalization and may even transfer\nnegative effects to some local models when rich statistical diversity exists\nacross multiple local datasets. Based on our observation, most real or\nsynthetic data distributions usually tend to be clustered to some degree, of\nwhich we argue different directions of generalization can facilitate the PFL.\nIn this paper, we propose a novel concept called clustered generalization to\nhandle the challenge of statistical heterogeneity in FL. Specifically, we\nmaintain multiple global (generalized) models in the server to associate with\nthe corresponding amount of local model clusters in clients, and further\nformulate the PFL as a bi-level optimization problem that can be solved\nefficiently and robustly. We also conduct detailed theoretical analysis and\nprovide the convergence guarantee for the smooth non-convex objectives.\nExperimental results on both synthetic and real datasets show that our approach\nsurpasses the state-of-the-art by a significant margin.",
          "link": "http://arxiv.org/abs/2106.13044",
          "publishedOn": "2021-06-25T02:00:47.172Z",
          "wordCount": 697,
          "title": "Personalized Federated Learning with Clustered Generalization. (arXiv:2106.13044v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1\">Andr&#xe9; Barreto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borsa_D/0/1/0/all/0/1\">Diana Borsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Shaobo Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comanici_G/0/1/0/all/0/1\">Gheorghe Comanici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aygun_E/0/1/0/all/0/1\">Eser Ayg&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamel_P/0/1/0/all/0/1\">Philippe Hamel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toyama_D/0/1/0/all/0/1\">Daniel Toyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hunt_J/0/1/0/all/0/1\">Jonathan Hunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mourad_S/0/1/0/all/0/1\">Shibl Mourad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silver_D/0/1/0/all/0/1\">David Silver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "The ability to combine known skills to create new ones may be crucial in the\nsolution of complex reinforcement learning problems that unfold over extended\nperiods. We argue that a robust way of combining skills is to define and\nmanipulate them in the space of pseudo-rewards (or \"cumulants\"). Based on this\npremise, we propose a framework for combining skills using the formalism of\noptions. We show that every deterministic option can be unambiguously\nrepresented as a cumulant defined in an extended domain. Building on this\ninsight and on previous results on transfer learning, we show how to\napproximate options whose cumulants are linear combinations of the cumulants of\nknown options. This means that, once we have learned options associated with a\nset of cumulants, we can instantaneously synthesise options induced by any\nlinear combination of them, without any learning involved. We describe how this\nframework provides a hierarchical interface to the environment whose abstract\nactions correspond to combinations of basic skills. We demonstrate the\npractical benefits of our approach in a resource management problem and a\nnavigation task involving a quadrupedal simulated robot.",
          "link": "http://arxiv.org/abs/2106.13105",
          "publishedOn": "2021-06-25T02:00:47.167Z",
          "wordCount": 636,
          "title": "The Option Keyboard: Combining Skills in Reinforcement Learning. (arXiv:2106.13105v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1\">Takuhiro Kaneko</a>",
          "description": "Understanding the 3D world from 2D projected natural images is a fundamental\nchallenge in computer vision and graphics. Recently, an unsupervised learning\napproach has garnered considerable attention owing to its advantages in data\ncollection. However, to mitigate training limitations, typical methods need to\nimpose assumptions for viewpoint distribution (e.g., a dataset containing\nvarious viewpoint images) or object shape (e.g., symmetric objects). These\nassumptions often restrict applications; for instance, the application to\nnon-rigid objects or images captured from similar viewpoints (e.g., flower or\nbird images) remains a challenge. To complement these approaches, we propose\naperture rendering generative adversarial networks (AR-GANs), which equip\naperture rendering on top of GANs, and adopt focus cues to learn the depth and\ndepth-of-field (DoF) effect of unlabeled natural images. To address the\nambiguities triggered by unsupervised setting (i.e., ambiguities between smooth\ntexture and out-of-focus blurs, and between foreground and background blurs),\nwe develop DoF mixture learning, which enables the generator to learn real\nimage distribution while generating diverse DoF images. In addition, we devise\na center focus prior to guiding the learning direction. In the experiments, we\ndemonstrate the effectiveness of AR-GANs in various datasets, such as flower,\nbird, and face images, demonstrate their portability by incorporating them into\nother 3D representation learning GANs, and validate their applicability in\nshallow DoF rendering.",
          "link": "http://arxiv.org/abs/2106.13041",
          "publishedOn": "2021-06-25T02:00:47.161Z",
          "wordCount": 689,
          "title": "Unsupervised Learning of Depth and Depth-of-Field Effect from Natural Images with Aperture Rendering Generative Adversarial Networks. (arXiv:2106.13041v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13081",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sadiya_S/0/1/0/all/0/1\">S Sadiya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alhanai_T/0/1/0/all/0/1\">T Alhanai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghassemi_M/0/1/0/all/0/1\">MM Ghassemi</a>",
          "description": "Electroencephalography (EEG) has countless applications across many of\nfields. However, EEG applications are limited by low signal-to-noise ratios.\nMultiple types of artifacts contribute to the noisiness of EEG, and many\ntechniques have been proposed to detect and correct these artifacts. These\ntechniques range from simply detecting and rejecting artifact ridden segments,\nto extracting the noise component from the EEG signal. In this paper we review\na variety of recent and classical techniques for EEG data artifact detection\nand correction with a focus on the last half-decade. We compare the strengths\nand weaknesses of the approaches and conclude with proposed future directions\nfor the field.",
          "link": "http://arxiv.org/abs/2106.13081",
          "publishedOn": "2021-06-25T02:00:47.145Z",
          "wordCount": 563,
          "title": "Artifact Detection and Correction in EEG data: A Review. (arXiv:2106.13081v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hanxi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plawinski_J/0/1/0/all/0/1\">Jason Plawinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramaniam_S/0/1/0/all/0/1\">Sajanth Subramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1\">Amir Jamaludin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1\">Timor Kadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Readie_A/0/1/0/all/0/1\">Aimee Readie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ligozio_G/0/1/0/all/0/1\">Gregory Ligozio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohlssen_D/0/1/0/all/0/1\">David Ohlssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baillie_M/0/1/0/all/0/1\">Mark Baillie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coroller_T/0/1/0/all/0/1\">Thibaud Coroller</a>",
          "description": "Sharing data from clinical studies can facilitate innovative data-driven\nresearch and ultimately lead to better public health. However, sharing\nbiomedical data can put sensitive personal information at risk. This is usually\nsolved by anonymization, which is a slow and expensive process. An alternative\nto anonymization is sharing a synthetic dataset that bears a behaviour similar\nto the real data but preserves privacy. As part of the collaboration between\nNovartis and the Oxford Big Data Institute, we generate a synthetic dataset\nbased on COSENTYX (secukinumab) Ankylosing Spondylitis (AS) clinical study. We\napply an Auxiliary Classifier GAN (ac-GAN) to generate synthetic magnetic\nresonance images (MRIs) of vertebral units (VUs). The images are conditioned on\nthe VU location (cervical, thoracic and lumbar). In this paper, we present a\nmethod for generating a synthetic dataset and conduct an in-depth analysis on\nits properties of along three key metrics: image fidelity, sample diversity and\ndataset privacy.",
          "link": "http://arxiv.org/abs/2106.13199",
          "publishedOn": "2021-06-25T02:00:47.140Z",
          "wordCount": 606,
          "title": "A Deep Learning Approach to Private Data Sharing of Medical Images Using Conditional GANs. (arXiv:2106.13199v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anders_C/0/1/0/all/0/1\">Christopher J. Anders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_D/0/1/0/all/0/1\">David Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Klaus-Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1\">Sebastian Lapuschkin</a>",
          "description": "Deep Neural Networks (DNNs) are known to be strong predictors, but their\nprediction strategies can rarely be understood. With recent advances in\nExplainable Artificial Intelligence, approaches are available to explore the\nreasoning behind those complex models' predictions. One class of approaches are\npost-hoc attribution methods, among which Layer-wise Relevance Propagation\n(LRP) shows high performance. However, the attempt at understanding a DNN's\nreasoning often stops at the attributions obtained for individual samples in\ninput space, leaving the potential for deeper quantitative analyses untouched.\nAs a manual analysis without the right tools is often unnecessarily labor\nintensive, we introduce three software packages targeted at scientists to\nexplore model reasoning using attribution approaches and beyond: (1) Zennit - a\nhighly customizable and intuitive attribution framework implementing LRP and\nrelated approaches in PyTorch, (2) CoRelAy - a framework to easily and quickly\nconstruct quantitative analysis pipelines for dataset-wide analyses of\nexplanations, and (3) ViRelAy - a web-application to interactively explore\ndata, attributions, and analysis results.",
          "link": "http://arxiv.org/abs/2106.13200",
          "publishedOn": "2021-06-25T02:00:47.135Z",
          "wordCount": 613,
          "title": "Software for Dataset-wide XAI: From Local Explanations to Global Insights with Zennit, CoRelAy, and ViRelAy. (arXiv:2106.13200v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12996",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ghosh_S/0/1/0/all/0/1\">Subhro Ghosh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rigollet_P/0/1/0/all/0/1\">Philippe Rigollet</a>",
          "description": "Motivated by cutting-edge applications like cryo-electron microscopy\n(cryo-EM), the Multi-Reference Alignment (MRA) model entails the learning of an\nunknown signal from repeated measurements of its images under the latent action\nof a group of isometries and additive noise of magnitude $\\sigma$. Despite\nsignificant interest, a clear picture for understanding rates of estimation in\nthis model has emerged only recently, particularly in the high-noise regime\n$\\sigma \\gg 1$ that is highly relevant in applications. Recent investigations\nhave revealed a remarkable asymptotic sample complexity of order $\\sigma^6$ for\ncertain signals whose Fourier transforms have full support, in stark contrast\nto the traditional $\\sigma^2$ that arise in regular models. Often prohibitively\nlarge in practice, these results have prompted the investigation of variations\naround the MRA model where better sample complexity may be achieved. In this\npaper, we show that \\emph{sparse} signals exhibit an intermediate $\\sigma^4$\nsample complexity even in the classical MRA model. Our results explore and\nexploit connections of the MRA estimation problem with two classical topics in\napplied mathematics: the \\textit{beltway problem} from combinatorial\noptimization, and \\textit{uniform uncertainty principles} from harmonic\nanalysis.",
          "link": "http://arxiv.org/abs/2106.12996",
          "publishedOn": "2021-06-25T02:00:47.129Z",
          "wordCount": 629,
          "title": "Multi-Reference Alignment for sparse signals, Uniform Uncertainty Principles and the Beltway Problem. (arXiv:2106.12996v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Sun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>",
          "description": "Leveraging the framework of Optimal Transport, we introduce a new family of\ngenerative autoencoders with a learnable prior, called Symmetric Wasserstein\nAutoencoders (SWAEs). We propose to symmetrically match the joint distributions\nof the observed data and the latent representation induced by the encoder and\nthe decoder. The resulting algorithm jointly optimizes the modelling losses in\nboth the data and the latent spaces with the loss in the data space leading to\nthe denoising effect. With the symmetric treatment of the data and the latent\nrepresentation, the algorithm implicitly preserves the local structure of the\ndata in the latent space. To further improve the quality of the latent\nrepresentation, we incorporate a reconstruction loss into the objective, which\nsignificantly benefits both the generation and reconstruction. We empirically\nshow the superior performance of SWAEs over the state-of-the-art generative\nautoencoders in terms of classification, reconstruction, and generation.",
          "link": "http://arxiv.org/abs/2106.13024",
          "publishedOn": "2021-06-25T02:00:47.123Z",
          "wordCount": 575,
          "title": "Symmetric Wasserstein Autoencoders. (arXiv:2106.13024v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morrison_K/0/1/0/all/0/1\">Katelyn Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilby_B/0/1/0/all/0/1\">Benjamin Gilby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipchak_C/0/1/0/all/0/1\">Colton Lipchak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattioli_A/0/1/0/all/0/1\">Adam Mattioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>",
          "description": "Recently, vision transformers and MLP-based models have been developed in\norder to address some of the prevalent weaknesses in convolutional neural\nnetworks. Due to the novelty of transformers being used in this domain along\nwith the self-attention mechanism, it remains unclear to what degree these\narchitectures are robust to corruptions. Despite some works proposing that data\naugmentation remains essential for a model to be robust against corruptions, we\npropose to explore the impact that the architecture has on corruption\nrobustness. We find that vision transformer architectures are inherently more\nrobust to corruptions than the ResNet-50 and MLP-Mixers. We also find that\nvision transformers with 5 times fewer parameters than a ResNet-50 have more\nshape bias. Our code is available to reproduce.",
          "link": "http://arxiv.org/abs/2106.13122",
          "publishedOn": "2021-06-25T02:00:47.107Z",
          "wordCount": 593,
          "title": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers. (arXiv:2106.13122v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13082",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Rosenbaum_R/0/1/0/all/0/1\">Robert Rosenbaum</a>",
          "description": "In this manuscript, I review and extend recent work on the relationship\nbetween predictive coding and backpropagation for training artificial neural\nnetworks on supervised learning tasks. I also discuss some implications of\nthese results for the interpretation of predictive coding and deep neural\nnetworks as models of biological learning and I describe a repository of\nfunctions, Torch2PC, for performing predictive coding with PyTorch neural\nnetwork models.",
          "link": "http://arxiv.org/abs/2106.13082",
          "publishedOn": "2021-06-25T02:00:47.102Z",
          "wordCount": 502,
          "title": "On the relationship between predictive coding and backpropagation. (arXiv:2106.13082v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Terrance Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_A/0/1/0/all/0/1\">Anna Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muszynski_M/0/1/0/all/0/1\">Michal Muszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_R/0/1/0/all/0/1\">Ryo Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1\">Nicholas Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1\">Randy Auerbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brent_D/0/1/0/all/0/1\">David Brent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Mental health conditions remain underdiagnosed even in countries with common\naccess to advanced medical care. The ability to accurately and efficiently\npredict mood from easily collectible data has several important implications\nfor the early detection, intervention, and treatment of mental health\ndisorders. One promising data source to help monitor human behavior is daily\nsmartphone usage. However, care must be taken to summarize behaviors without\nidentifying the user through personal (e.g., personally identifiable\ninformation) or protected (e.g., race, gender) attributes. In this paper, we\nstudy behavioral markers of daily mood using a recent dataset of mobile\nbehaviors from adolescent populations at high risk of suicidal behaviors. Using\ncomputational models, we find that language and multimodal representations of\nmobile typed text (spanning typed characters, words, keystroke timings, and app\nusage) are predictive of daily mood. However, we find that models trained to\npredict mood often also capture private user identities in their intermediate\nrepresentations. To tackle this problem, we evaluate approaches that obfuscate\nuser identity while remaining predictive. By combining multimodal\nrepresentations with privacy-preserving learning, we are able to push forward\nthe performance-privacy frontier.",
          "link": "http://arxiv.org/abs/2106.13213",
          "publishedOn": "2021-06-25T02:00:47.092Z",
          "wordCount": 656,
          "title": "Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data. (arXiv:2106.13213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13194",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bubnova_A/0/1/0/all/0/1\">Anna V. Bubnova</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deeva_I/0/1/0/all/0/1\">Irina Deeva</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1\">Anna V. Kalyuzhnaya</a>",
          "description": "This paper describes a new library for learning Bayesian networks from data\ncontaining discrete and continuous variables (mixed data). In addition to the\nclassical learning methods on discretized data, this library proposes its\nalgorithm that allows structural learning and parameters learning from mixed\ndata without discretization since data discretization leads to information\nloss. This algorithm based on mixed MI score function for structural learning,\nand also linear regression and Gaussian distribution approximation for\nparameters learning. The library also offers two algorithms for enumerating\ngraph structures - the greedy Hill-Climbing algorithm and the evolutionary\nalgorithm. Thus the key capabilities of the proposed library are as follows:\n(1) structural and parameters learning of a Bayesian network on discretized\ndata, (2) structural and parameters learning of a Bayesian network on mixed\ndata using the MI mixed score function and Gaussian approximation, (3)\nlaunching learning algorithms on one of two algorithms for enumerating graph\nstructures - Hill-Climbing and the evolutionary algorithm. Since the need for\nmixed data representation comes from practical necessity, the advantages of our\nimplementations are evaluated in the context of solving approximation and gap\nrecovery problems on synthetic data and real datasets.",
          "link": "http://arxiv.org/abs/2106.13194",
          "publishedOn": "2021-06-25T02:00:47.087Z",
          "wordCount": 627,
          "title": "MIxBN: library for learning Bayesian networks from mixed data. (arXiv:2106.13194v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tirinzoni_A/0/1/0/all/0/1\">Andrea Tirinzoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1\">Matteo Pirotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1\">Alessandro Lazaric</a>",
          "description": "We derive a novel asymptotic problem-dependent lower-bound for regret\nminimization in finite-horizon tabular Markov Decision Processes (MDPs). While,\nsimilar to prior work (e.g., for ergodic MDPs), the lower-bound is the solution\nto an optimization problem, our derivation reveals the need for an additional\nconstraint on the visitation distribution over state-action pairs that\nexplicitly accounts for the dynamics of the MDP. We provide a characterization\nof our lower-bound through a series of examples illustrating how different MDPs\nmay have significantly different complexity. 1) We first consider a \"difficult\"\nMDP instance, where the novel constraint based on the dynamics leads to a\nlarger lower-bound (i.e., a larger regret) compared to the classical analysis.\n2) We then show that our lower-bound recovers results previously derived for\nspecific MDP instances. 3) Finally, we show that, in certain \"simple\" MDPs, the\nlower bound is considerably smaller than in the general case and it does not\nscale with the minimum action gap at all. We show that this last result is\nattainable (up to $poly(H)$ terms, where $H$ is the horizon) by providing a\nregret upper-bound based on policy gaps for an optimistic algorithm.",
          "link": "http://arxiv.org/abs/2106.13013",
          "publishedOn": "2021-06-25T02:00:47.072Z",
          "wordCount": 619,
          "title": "A Fully Problem-Dependent Regret Lower Bound for Finite-Horizon MDPs. (arXiv:2106.13013v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pogodin_R/0/1/0/all/0/1\">Roman Pogodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_Y/0/1/0/all/0/1\">Yash Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1\">Timothy P. Lillicrap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latham_P/0/1/0/all/0/1\">Peter E. Latham</a>",
          "description": "Convolutional networks are ubiquitous in deep learning. They are particularly\nuseful for images, as they reduce the number of parameters, reduce training\ntime, and increase accuracy. However, as a model of the brain they are\nseriously problematic, since they require weight sharing - something real\nneurons simply cannot do. Consequently, while neurons in the brain can be\nlocally connected (one of the features of convolutional networks), they cannot\nbe convolutional. Locally connected but non-convolutional networks, however,\nsignificantly underperform convolutional ones. This is troublesome for studies\nthat use convolutional networks to explain activity in the visual system. Here\nwe study plausible alternatives to weight sharing that aim at the same\nregularization principle, which is to make each neuron within a pool react\nsimilarly to identical inputs. The most natural way to do that is by showing\nthe network multiple translations of the same image, akin to saccades in animal\nvision. However, this approach requires many translations, and doesn't remove\nthe performance gap. We propose instead to add lateral connectivity to a\nlocally connected network, and allow learning via Hebbian plasticity. This\nrequires the network to pause occasionally for a sleep-like phase of \"weight\nsharing\". This method enables locally connected networks to achieve nearly\nconvolutional performance on ImageNet, thus supporting convolutional networks\nas a model of the visual stream.",
          "link": "http://arxiv.org/abs/2106.13031",
          "publishedOn": "2021-06-25T02:00:47.066Z",
          "wordCount": 655,
          "title": "Towards Biologically Plausible Convolutional Networks. (arXiv:2106.13031v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianlong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Simon Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>",
          "description": "The adoption of electronic health records (EHR) has become universal during\nthe past decade, which has afforded in-depth data-based research. By learning\nfrom the large amount of healthcare data, various data-driven models have been\nbuilt to predict future events for different medical tasks, such as auto\ndiagnosis and heart-attack prediction. Although EHR is abundant, the population\nthat satisfies specific criteria for learning population-specific tasks is\nscarce, making it challenging to train data-hungry deep learning models. This\nstudy presents the Claim Pre-Training (Claim-PT) framework, a generic\npre-training model that first trains on the entire pediatric claims dataset,\nfollowed by a discriminative fine-tuning on each population-specific task. The\nsemantic meaning of medical events can be captured in the pre-training stage,\nand the effective knowledge transfer is completed through the task-aware\nfine-tuning stage. The fine-tuning process requires minimal parameter\nmodification without changing the model architecture, which mitigates the data\nscarcity issue and helps train the deep learning model adequately on small\npatient cohorts. We conducted experiments on a real-world claims dataset with\nmore than one million patient records. Experimental results on two downstream\ntasks demonstrated the effectiveness of our method: our general task-agnostic\npre-training framework outperformed tailored task-specific models, achieving\nmore than 10\\% higher in model performance as compared to baselines. In\naddition, our framework showed a great generalizability potential to transfer\nlearned knowledge from one institution to another, paving the way for future\nhealthcare model pre-training across institutions.",
          "link": "http://arxiv.org/abs/2106.13095",
          "publishedOn": "2021-06-25T02:00:47.059Z",
          "wordCount": 675,
          "title": "Pre-training transformer-based framework on large-scale pediatric claims data for downstream population-specific tasks. (arXiv:2106.13095v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Babaeizadeh_M/0/1/0/all/0/1\">Mohammad Babaeizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffar_M/0/1/0/all/0/1\">Mohammad Taghi Saffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erhan_D/0/1/0/all/0/1\">Dumitru Erhan</a>",
          "description": "An agent that is capable of predicting what happens next can perform a\nvariety of tasks through planning with no additional training. Furthermore,\nsuch an agent can internally represent the complex dynamics of the real-world\nand therefore can acquire a representation useful for a variety of visual\nperception tasks. This makes predicting the future frames of a video,\nconditioned on the observed past and potentially future actions, an interesting\ntask which remains exceptionally challenging despite many recent advances.\nExisting video prediction models have shown promising results on simple narrow\nbenchmarks but they generate low quality predictions on real-life datasets with\nmore complicated dynamics or broader domain. There is a growing body of\nevidence that underfitting on the training data is one of the primary causes\nfor the low quality predictions. In this paper, we argue that the inefficient\nuse of parameters in the current video models is the main reason for\nunderfitting. Therefore, we introduce a new architecture, named FitVid, which\nis capable of severe overfitting on the common benchmarks while having similar\nparameter count as the current state-of-the-art models. We analyze the\nconsequences of overfitting, illustrating how it can produce unexpected\noutcomes such as generating high quality output by repeating the training data,\nand how it can be mitigated using existing image augmentation techniques. As a\nresult, FitVid outperforms the current state-of-the-art models across four\ndifferent video prediction benchmarks on four different metrics.",
          "link": "http://arxiv.org/abs/2106.13195",
          "publishedOn": "2021-06-25T02:00:47.052Z",
          "wordCount": 675,
          "title": "FitVid: Overfitting in Pixel-Level Video Prediction. (arXiv:2106.13195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feretzakis_G/0/1/0/all/0/1\">Georgios Feretzakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlis_G/0/1/0/all/0/1\">George Karlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loupelis_E/0/1/0/all/0/1\">Evangelos Loupelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalles_D/0/1/0/all/0/1\">Dimitris Kalles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzikyriakou_R/0/1/0/all/0/1\">Rea Chatzikyriakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trakas_N/0/1/0/all/0/1\">Nikolaos Trakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakou_E/0/1/0/all/0/1\">Eugenia Karakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakagianni_A/0/1/0/all/0/1\">Aikaterini Sakagianni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzelves_L/0/1/0/all/0/1\">Lazaros Tzelves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petropoulou_S/0/1/0/all/0/1\">Stavroula Petropoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tika_A/0/1/0/all/0/1\">Aikaterini Tika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalainas_I/0/1/0/all/0/1\">Ilias Dalainas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaldis_V/0/1/0/all/0/1\">Vasileios Kaldis</a>",
          "description": "Introduction: One of the most important tasks in the Emergency Department\n(ED) is to promptly identify the patients who will benefit from hospital\nadmission. Machine Learning (ML) techniques show promise as diagnostic aids in\nhealthcare. Material and methods: We investigated the following features\nseeking to investigate their performance in predicting hospital admission:\nserum levels of Urea, Creatinine, Lactate Dehydrogenase, Creatine Kinase,\nC-Reactive Protein, Complete Blood Count with differential, Activated Partial\nThromboplastin Time, D Dimer, International Normalized Ratio, age, gender,\ntriage disposition to ED unit and ambulance utilization. A total of 3,204 ED\nvisits were analyzed. Results: The proposed algorithms generated models which\ndemonstrated acceptable performance in predicting hospital admission of ED\npatients. The range of F-measure and ROC Area values of all eight evaluated\nalgorithms were [0.679-0.708] and [0.734-0.774], respectively. Discussion: The\nmain advantages of this tool include easy access, availability, yes/no result,\nand low cost. The clinical implications of our approach might facilitate a\nshift from traditional clinical decision-making to a more sophisticated model.\nConclusion: Developing robust prognostic models with the utilization of common\nbiomarkers is a project that might shape the future of emergency medicine. Our\nfindings warrant confirmation with implementation in pragmatic ED trials.",
          "link": "http://arxiv.org/abs/2106.12921",
          "publishedOn": "2021-06-25T02:00:47.047Z",
          "wordCount": 661,
          "title": "Using machine learning techniques to predict hospital admission at the emergency department. (arXiv:2106.12921v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12961",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Yang_L/0/1/0/all/0/1\">Liping Yang</a>",
          "description": "In recent years, Bitcoin price prediction has attracted the interest of\nresearchers and investors. However, the accuracy of previous studies is not\nwell enough. Machine learning and deep learning methods have been proved to\nhave strong prediction ability in this area. This paper proposed a method\ncombined with Ensemble Empirical Mode Decomposition (EEMD) and a deep learning\nmethod called long short-term memory (LSTM) to research the problem of next-day\nBitcoin price forecast.",
          "link": "http://arxiv.org/abs/2106.12961",
          "publishedOn": "2021-06-25T02:00:47.032Z",
          "wordCount": 503,
          "title": "Next-Day Bitcoin Price Forecast Based on Artificial intelligence Methods. (arXiv:2106.12961v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12954",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jia_C/0/1/0/all/0/1\">Chuanmin Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1\">Ziqing Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>",
          "description": "End-to-end optimization capability offers neural image compression (NIC)\nsuperior lossy compression performance. However, distinct models are required\nto be trained to reach different points in the rate-distortion (R-D) space. In\nthis paper, we consider the problem of R-D characteristic analysis and modeling\nfor NIC. We make efforts to formulate the essential mathematical functions to\ndescribe the R-D behavior of NIC using deep network and statistical modeling.\nThus continuous bit-rate points could be elegantly realized by leveraging such\nmodel via a single trained network. In this regard, we propose a plugin-in\nmodule to learn the relationship between the target bit-rate and the binary\nrepresentation for the latent variable of auto-encoder. Furthermore, we model\nthe rate and distortion characteristic of NIC as a function of the coding\nparameter $\\lambda$ respectively. Our experiments show our proposed method is\neasy to adopt and obtains competitive coding performance with fixed-rate coding\napproaches, which would benefit the practical deployment of NIC. In addition,\nthe proposed model could be applied to NIC rate control with limited bit-rate\nerror using a single network.",
          "link": "http://arxiv.org/abs/2106.12954",
          "publishedOn": "2021-06-25T02:00:47.022Z",
          "wordCount": 630,
          "title": "Rate Distortion Characteristic Modeling for Neural Image Compression. (arXiv:2106.12954v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13035",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zanetti_A/0/1/0/all/0/1\">Andrea Zanetti</a>",
          "description": "Pre-trained language models like Ernie or Bert are currently used in many\napplications. These models come with a set of pre-trained weights typically\nobtained in unsupervised/self-supervised modality on a huge amount of data.\nAfter that, they are fine-tuned on a specific task. Applications then use these\nmodels for inference, and often some additional constraints apply, like low\npower-budget or low latency between input and output. The main avenue to meet\nthese additional requirements for the inference settings, is to use low\nprecision computation (e.g. INT8 rather than FP32), but this comes with a cost\nof deteriorating the functional performance (e.g. accuracy) of the model. Some\napproaches have been developed to tackle the problem and go beyond the\nlimitations of the PTO (Post-Training Quantization), more specifically the QAT\n(Quantization Aware Training, see [4]) is a procedure that interferes with the\ntraining process in order to make it affected (or simply disturbed) by the\nquantization phase during the training itself. Besides QAT, recently\nIntel-Habana Labs have proposed an additional and more direct way to make the\ntraining results more robust to subsequent quantization which uses a\nregularizer, therefore changing the loss function that drives the training\nprocedure. But their proposal does not work out-of-the-box for pre-trained\nmodels like Ernie, for example. In this short paper we show why this is not\nhappening (for the Ernie case) and we propose a very basic way to deal with it,\nsharing as well some initial results (increase in final INT8 accuracy) that\nmight be of interest to practitioners willing to use Ernie in their\napplications, in low precision regime.",
          "link": "http://arxiv.org/abs/2106.13035",
          "publishedOn": "2021-06-25T02:00:47.016Z",
          "wordCount": 701,
          "title": "Quantization Aware Training, ERNIE and Kurtosis Regularizer: a short empirical study. (arXiv:2106.13035v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13055",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lorsung_C/0/1/0/all/0/1\">Cooper Lorsung</a>",
          "description": "Neural Linear Models (NLM) are deep Bayesian models that produce predictive\nuncertainty by learning features from the data and then performing Bayesian\nlinear regression over these features. Despite their popularity, few works have\nfocused on formally evaluating the predictive uncertainties of these models.\nFurthermore, existing works point out the difficulties of encoding domain\nknowledge in models like NLMs, making them unsuitable for applications where\ninterpretability is required. In this work, we show that traditional training\nprocedures for NLMs can drastically underestimate uncertainty in data-scarce\nregions. We identify the underlying reasons for this behavior and propose a\nnovel training method that can both capture useful predictive uncertainties as\nwell as allow for incorporation of domain knowledge.",
          "link": "http://arxiv.org/abs/2106.13055",
          "publishedOn": "2021-06-25T02:00:47.010Z",
          "wordCount": 547,
          "title": "Understanding Uncertainty in Bayesian Deep Learning. (arXiv:2106.13055v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leonardos_S/0/1/0/all/0/1\">Stefanos Leonardos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1\">Georgios Piliouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spendlove_K/0/1/0/all/0/1\">Kelly Spendlove</a>,",
          "description": "The interplay between exploration and exploitation in competitive multi-agent\nlearning is still far from being well understood. Motivated by this, we study\nsmooth Q-learning, a prototypical learning model that explicitly captures the\nbalance between game rewards and exploration costs. We show that Q-learning\nalways converges to the unique quantal-response equilibrium (QRE), the standard\nsolution concept for games under bounded rationality, in weighted zero-sum\npolymatrix games with heterogeneous learning agents using positive exploration\nrates. Complementing recent results about convergence in weighted potential\ngames, we show that fast convergence of Q-learning in competitive settings is\nobtained regardless of the number of agents and without any need for parameter\nfine-tuning. As showcased by our experiments in network zero-sum games, these\ntheoretical results provide the necessary guarantees for an algorithmic\napproach to the currently open problem of equilibrium selection in competitive\nmulti-agent settings.",
          "link": "http://arxiv.org/abs/2106.12928",
          "publishedOn": "2021-06-25T02:00:47.000Z",
          "wordCount": 601,
          "title": "Exploration-Exploitation in Multi-Agent Competition: Convergence with Bounded Rationality. (arXiv:2106.12928v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">James Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbster_M/0/1/0/all/0/1\">Mark Herbster</a>",
          "description": "We address the problem of sequential prediction with expert advice in a\nnon-stationary environment with long-term memory guarantees in the sense of\nBousquet and Warmuth [4]. We give a linear-time algorithm that improves on the\nbest known regret bounds [26]. This algorithm incorporates a relative entropy\nprojection step. This projection is advantageous over previous weight-sharing\napproaches in that weight updates may come with implicit costs as in for\nexample portfolio optimization. We give an algorithm to compute this projection\nstep in linear time, which may be of independent interest.",
          "link": "http://arxiv.org/abs/2106.13021",
          "publishedOn": "2021-06-25T02:00:46.983Z",
          "wordCount": 516,
          "title": "Improved Regret Bounds for Tracking Experts with Memory. (arXiv:2106.13021v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roster_K/0/1/0/all/0/1\">Kirstin Roster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_F/0/1/0/all/0/1\">Francisco A. Rodrigues</a>",
          "description": "Due to a lack of treatments and universal vaccine, early forecasts of Dengue\nare an important tool for disease control. Neural networks are powerful\npredictive models that have made contributions to many areas of public health.\nIn this systematic review, we provide an introduction to the neural networks\nrelevant to Dengue forecasting and review their applications in the literature.\nThe objective is to help inform model design for future work. Following the\nPRISMA guidelines, we conduct a systematic search of studies that use neural\nnetworks to forecast Dengue in human populations. We summarize the relative\nperformance of neural networks and comparator models, model architectures and\nhyper-parameters, as well as choices of input features. Nineteen papers were\nincluded. Most studies implement shallow neural networks using historical\nDengue incidence and meteorological input features. Prediction horizons tend to\nbe short. Building on the strengths of neural networks, most studies use\ngranular observations at the city or sub-national level. Performance of neural\nnetworks relative to comparators such as Support Vector Machines varies across\nstudy contexts. The studies suggest that neural networks can provide good\npredictions of Dengue and should be included in the set of candidate models.\nThe use of convolutional, recurrent, or deep networks is relatively unexplored\nbut offers promising avenues for further research, as does the use of a broader\nset of input features such as social media or mobile phone data.",
          "link": "http://arxiv.org/abs/2106.12905",
          "publishedOn": "2021-06-25T02:00:46.967Z",
          "wordCount": 670,
          "title": "Neural Networks for Dengue Prediction: A Systematic Review. (arXiv:2106.12905v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cairoli_F/0/1/0/all/0/1\">Francesca Cairoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbone_G/0/1/0/all/0/1\">Ginevra Carbone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bortolussi_L/0/1/0/all/0/1\">Luca Bortolussi</a>",
          "description": "Markov Population Models are a widespread formalism used to model the\ndynamics of complex systems, with applications in Systems Biology and many\nother fields. The associated Markov stochastic process in continuous time is\noften analyzed by simulation, which can be costly for large or stiff systems,\nparticularly when a massive number of simulations has to be performed (e.g. in\na multi-scale model). A strategy to reduce computational load is to abstract\nthe population model, replacing it with a simpler stochastic model, faster to\nsimulate. Here we pursue this idea, building on previous works and constructing\na generator capable of producing stochastic trajectories in continuous space\nand discrete time. This generator is learned automatically from simulations of\nthe original model in a Generative Adversarial setting. Compared to previous\nworks, which rely on deep neural networks and Dirichlet processes, we explore\nthe use of state of the art generative models, which are flexible enough to\nlearn a full trajectory rather than a single transition kernel.",
          "link": "http://arxiv.org/abs/2106.12981",
          "publishedOn": "2021-06-25T02:00:46.958Z",
          "wordCount": 593,
          "title": "Abstraction of Markov Population Dynamics via Generative Adversarial Nets. (arXiv:2106.12981v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martins_F/0/1/0/all/0/1\">Felipe B. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1\">Mateus G. Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassani_H/0/1/0/all/0/1\">Hansenclever F. Bassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braga_P/0/1/0/all/0/1\">Pedro H. M. Braga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barros_E/0/1/0/all/0/1\">Edna S. Barros</a>",
          "description": "Reinforcement learning is an active research area with a vast number of\napplications in robotics, and the RoboCup competition is an interesting\nenvironment for studying and evaluating reinforcement learning methods. A known\ndifficulty in applying reinforcement learning to robotics is the high number of\nexperience samples required, being the use of simulated environments for\ntraining the agents followed by transfer learning to real-world (sim-to-real) a\nviable path. This article introduces an open-source simulator for the IEEE Very\nSmall Size Soccer and the Small Size League optimized for reinforcement\nlearning experiments. We also propose a framework for creating OpenAI Gym\nenvironments with a set of benchmarks tasks for evaluating single-agent and\nmulti-agent robot soccer skills. We then demonstrate the learning capabilities\nof two state-of-the-art reinforcement learning methods as well as their\nlimitations in certain scenarios introduced in this framework. We believe this\nwill make it easier for more teams to compete in these categories using\nend-to-end reinforcement learning approaches and further develop this research\narea.",
          "link": "http://arxiv.org/abs/2106.12895",
          "publishedOn": "2021-06-25T02:00:46.948Z",
          "wordCount": 623,
          "title": "rSoccer: A Framework for Studying Reinforcement Learning in Small and Very Small Size Robot Soccer. (arXiv:2106.12895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barumerli_R/0/1/0/all/0/1\">Roberto Barumerli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_D/0/1/0/all/0/1\">Daniele Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geronazzo_M/0/1/0/all/0/1\">Michele Geronazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avanzini_F/0/1/0/all/0/1\">Federico Avanzini</a>",
          "description": "This paper introduces a shoebox room simulator able to systematically\ngenerate synthetic datasets of binaural room impulse responses (BRIRs) given an\narbitrary set of head-related transfer functions (HRTFs). The evaluation of\nmachine hearing algorithms frequently requires BRIR datasets in order to\nsimulate the acoustics of any environment. However, currently available\nsolutions typically consider only HRTFs measured on dummy heads, which poorly\ncharacterize the high variability in spatial sound perception. Our solution\nallows to integrate a room impulse response (RIR) simulator with different HRTF\nsets represented in Spatially Oriented Format for Acoustics (SOFA). The source\ncode and the compiled binaries for different operating systems allow to both\nadvanced and non-expert users to benefit from our toolbox, see\nhttps://github.com/spatialaudiotools/sofamyroom/ .",
          "link": "http://arxiv.org/abs/2106.12992",
          "publishedOn": "2021-06-25T02:00:46.931Z",
          "wordCount": 587,
          "title": "SofaMyRoom: a fast and multiplatform \"shoebox\" room simulator for binaural room impulse response dataset generation. (arXiv:2106.12992v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soni_B/0/1/0/all/0/1\">Badal Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakuria_D/0/1/0/all/0/1\">Debangan Thakuria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nath_N/0/1/0/all/0/1\">Nilutpal Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1\">Navarun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boro_B/0/1/0/all/0/1\">Bhaskarananda Boro</a>",
          "description": "Anime is quite well-received today, especially among the younger generations.\nWith many genres of available shows, more and more people are increasingly\ngetting attracted to this niche section of the entertainment industry. As anime\nhas recently garnered mainstream attention, we have insufficient information\nregarding users' penchant and watching habits. Therefore, it is an uphill task\nto build a recommendation engine for this relatively obscure entertainment\nmedium. In this attempt, we have built a novel hybrid recommendation system\nthat could act both as a recommendation system and as a means of exploring new\nanime genres and titles. We have analyzed the general trends in this field and\nthe users' watching habits for coming up with our efficacious solution. Our\nsolution employs deep autoencoders for the tasks of predicting ratings and\ngenerating embeddings. Following this, we formed clusters using the embeddings\nof the anime titles. These clusters form the search space for anime with\nsimilarities and are used to find anime similar to the ones liked and disliked\nby the user. This method, combined with the predicted ratings, forms the novel\nhybrid filter. In this article, we have demonstrated this idea and compared the\nperformance of our implemented model with the existing state-of-the-art\ntechniques.",
          "link": "http://arxiv.org/abs/2106.12970",
          "publishedOn": "2021-06-25T02:00:46.924Z",
          "wordCount": 644,
          "title": "RikoNet: A Novel Anime Recommendation Engine. (arXiv:2106.12970v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12923",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_J/0/1/0/all/0/1\">Jun-Kun Wang</a>",
          "description": "In the first part of this dissertation research, we develop a modular\nframework that can serve as a recipe for constructing and analyzing iterative\nalgorithms for convex optimization. Specifically, our work casts optimization\nas iteratively playing a two-player zero-sum game. Many existing optimization\nalgorithms including Frank-Wolfe and Nesterov's acceleration methods can be\nrecovered from the game by pitting two online learners with appropriate\nstrategies against each other. Furthermore, the sum of the weighted average\nregrets of the players in the game implies the convergence rate. As a result,\nour approach provides simple alternative proofs to these algorithms. Moreover,\nwe demonstrate that our approach of optimization as iteratively playing a game\nleads to three new fast Frank-Wolfe-like algorithms for some constraint sets,\nwhich further shows that our framework is indeed generic, modular, and\neasy-to-use.\n\nIn the second part, we develop a modular analysis of provable acceleration\nvia Polyak's momentum for certain problems, which include solving the classical\nstrongly quadratic convex problems, training a wide ReLU network under the\nneural tangent kernel regime, and training a deep linear network with an\northogonal initialization. We develop a meta theorem and show that when\napplying Polyak's momentum for these problems, the induced dynamics exhibit a\nform where we can directly apply our meta theorem.\n\nIn the last part of the dissertation, we show another advantage of the use of\nPolyak's momentum -- it facilitates fast saddle point escape in smooth\nnon-convex optimization. This result, together with those of the second part,\nsheds new light on Polyak's momentum in modern non-convex optimization and deep\nlearning.",
          "link": "http://arxiv.org/abs/2106.12923",
          "publishedOn": "2021-06-25T02:00:46.916Z",
          "wordCount": 709,
          "title": "Understanding Modern Techniques in Optimization: Frank-Wolfe, Nesterov's Momentum, and Polyak's Momentum. (arXiv:2106.12923v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lorenzen_S/0/1/0/all/0/1\">Stephan Sloth Lorenzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1\">Christian Igel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_M/0/1/0/all/0/1\">Mads Nielsen</a>",
          "description": "The information bottleneck (IB) principle has been suggested as a way to\nanalyze deep neural networks. The learning dynamics are studied by inspecting\nthe mutual information (MI) between the hidden layers and the input and output.\nNotably, separate fitting and compression phases during training have been\nreported. This led to some controversy including claims that the observations\nare not reproducible and strongly dependent on the type of activation function\nused as well as on the way the MI is estimated. Our study confirms that\ndifferent ways of binning when computing the MI lead to qualitatively different\nresults, either supporting or refusing IB conjectures. To resolve the\ncontroversy, we study the IB principle in settings where MI is non-trivial and\ncan be computed exactly. We monitor the dynamics of quantized neural networks,\nthat is, we discretize the whole deep learning system so that no approximation\nis required when computing the MI. This allows us to quantify the information\nflow without measurement errors. In this setting, we observed a fitting phase\nfor all layers and a compression phase for the output layer in all experiments;\nthe compression in the hidden layers was dependent on the type of activation\nfunction. Our study shows that the initial IB results were not artifacts of\nbinning when computing the MI. However, the critical claim that the compression\nphase may not be observed for some networks also holds true.",
          "link": "http://arxiv.org/abs/2106.12912",
          "publishedOn": "2021-06-25T02:00:46.910Z",
          "wordCount": 662,
          "title": "Information Bottleneck: Exact Analysis of (Quantized) Neural Networks. (arXiv:2106.12912v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Solbiati_A/0/1/0/all/0/1\">Alessandro Solbiati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1\">Kevin Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damaskinos_G/0/1/0/all/0/1\">Georgios Damaskinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1\">Shivani Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_S/0/1/0/all/0/1\">Shubham Modi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cali_J/0/1/0/all/0/1\">Jacques Cali</a>",
          "description": "Topic segmentation of meetings is the task of dividing multi-person meeting\ntranscripts into topic blocks. Supervised approaches to the problem have proven\nintractable due to the difficulties in collecting and accurately annotating\nlarge datasets. In this paper we show how previous unsupervised topic\nsegmentation methods can be improved using pre-trained neural architectures. We\nintroduce an unsupervised approach based on BERT embeddings that achieves a\n15.5% reduction in error rate over existing unsupervised approaches applied to\ntwo popular datasets for meeting transcripts.",
          "link": "http://arxiv.org/abs/2106.12978",
          "publishedOn": "2021-06-25T02:00:46.903Z",
          "wordCount": 519,
          "title": "Unsupervised Topic Segmentation of Meetings with BERT Embeddings. (arXiv:2106.12978v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1\">Qingqing Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guojie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Kunqing Xie</a>",
          "description": "Spatial-temporal forecasting has attracted tremendous attention in a wide\nrange of applications, and traffic flow prediction is a canonical and typical\nexample. The complex and long-range spatial-temporal correlations of traffic\nflow bring it to a most intractable challenge. Existing works typically utilize\nshallow graph convolution networks (GNNs) and temporal extracting modules to\nmodel spatial and temporal dependencies respectively. However, the\nrepresentation ability of such models is limited due to: (1) shallow GNNs are\nincapable to capture long-range spatial correlations, (2) only spatial\nconnections are considered and a mass of semantic connections are ignored,\nwhich are of great importance for a comprehensive understanding of traffic\nnetworks. To this end, we propose Spatial-Temporal Graph Ordinary Differential\nEquation Networks (STGODE). Specifically, we capture spatial-temporal dynamics\nthrough a tensor-based ordinary differential equation (ODE), as a result,\ndeeper networks can be constructed and spatial-temporal features are utilized\nsynchronously. To understand the network more comprehensively, semantical\nadjacency matrix is considered in our model, and a well-design temporal\ndialated convolution structure is used to capture long term temporal\ndependencies. We evaluate our model on multiple real-world traffic datasets and\nsuperior performance is achieved over state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2106.12931",
          "publishedOn": "2021-06-25T02:00:46.897Z",
          "wordCount": 620,
          "title": "Spatial-Temporal Graph ODE Networks for Traffic Flow Forecasting. (arXiv:2106.12931v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rathee_M/0/1/0/all/0/1\">Mandeep Rathee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funke_T/0/1/0/all/0/1\">Thorben Funke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosla_M/0/1/0/all/0/1\">Megha Khosla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>",
          "description": "Graph neural networks (GNNs) have achieved great success on various tasks and\nfields that require relational modeling. GNNs aggregate node features using the\ngraph structure as inductive biases resulting in flexible and powerful models.\nHowever, GNNs remain hard to interpret as the interplay between node features\nand graph structure is only implicitly learned. In this paper, we propose a\nnovel method called Kedge for explicitly sparsifying the underlying graph by\nremoving unnecessary neighbors. Our key idea is based on a tractable method for\nsparsification using the Hard Kumaraswamy distribution that can be used in\nconjugation with any GNN model. Kedge learns edge masks in a modular fashion\ntrained with any GNN allowing for gradient based optimization in an end-to-end\nfashion. We demonstrate through extensive experiments that our model Kedge can\nprune a large proportion of the edges with only a minor effect on the test\naccuracy. Specifically, in the PubMed dataset, Kedge learns to drop more than\n80% of the edges with an accuracy drop of merely 2% showing that graph\nstructure has only a small contribution in comparison to node features.\nFinally, we also show that Kedge effectively counters the over-smoothing\nphenomena in deep GNNs by maintaining good task performance with increasing GNN\nlayers.",
          "link": "http://arxiv.org/abs/2106.12920",
          "publishedOn": "2021-06-25T02:00:46.881Z",
          "wordCount": 641,
          "title": "Learnt Sparsification for Interpretable Graph Neural Networks. (arXiv:2106.12920v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12936",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Abraham_K/0/1/0/all/0/1\">Kweku Abraham</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Naulet_Z/0/1/0/all/0/1\">Zacharie Naulet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gassiat_E/0/1/0/all/0/1\">Elisabeth Gassiat</a>",
          "description": "We study the frontier between learnable and unlearnable hidden Markov models\n(HMMs). HMMs are flexible tools for clustering dependent data coming from\nunknown populations. The model parameters are known to be identifiable as soon\nas the clusters are distinct and the hidden chain is ergodic with a full rank\ntransition matrix. In the limit as any one of these conditions fails, it\nbecomes impossible to identify parameters. For a chain with two hidden states\nwe prove nonasymptotic minimax upper and lower bounds, matching up to\nconstants, which exhibit thresholds at which the parameters become learnable.",
          "link": "http://arxiv.org/abs/2106.12936",
          "publishedOn": "2021-06-25T02:00:46.876Z",
          "wordCount": 537,
          "title": "Fundamental limits for learning hidden Markov model parameters. (arXiv:2106.12936v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xuelong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>",
          "description": "Meta-learning model can quickly adapt to new tasks using few-shot labeled\ndata. However, despite achieving good generalization on few-shot classification\ntasks, it is still challenging to improve the adversarial robustness of the\nmeta-learning model in few-shot learning. Although adversarial training (AT)\nmethods such as Adversarial Query (AQ) can improve the adversarially robust\nperformance of meta-learning models, AT is still computationally expensive\ntraining. On the other hand, meta-learning models trained with AT will drop\nsignificant accuracy on the original clean images. This paper proposed a\nmeta-learning method on the adversarially robust neural network called\nLong-term Cross Adversarial Training (LCAT). LCAT will update meta-learning\nmodel parameters cross along the natural and adversarial sample distribution\ndirection with long-term to improve both adversarial and clean few-shot\nclassification accuracy. Due to cross-adversarial training, LCAT only needs\nhalf of the adversarial training epoch than AQ, resulting in a low adversarial\ntraining computation. Experiment results show that LCAT achieves superior\nperformance both on the clean and adversarial few-shot classification accuracy\nthan SOTA adversarial training methods for meta-learning models.",
          "link": "http://arxiv.org/abs/2106.12900",
          "publishedOn": "2021-06-25T02:00:46.870Z",
          "wordCount": 620,
          "title": "Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (arXiv:2106.12900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raahil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokora_K/0/1/0/all/0/1\">Kamil Pokora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezzerg_A/0/1/0/all/0/1\">Abdelhamid Ezzerg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimkov_V/0/1/0/all/0/1\">Viacheslav Klimkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huybrechts_G/0/1/0/all/0/1\">Goeric Huybrechts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putrycz_B/0/1/0/all/0/1\">Bartosz Putrycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korzekwa_D/0/1/0/all/0/1\">Daniel Korzekwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merritt_T/0/1/0/all/0/1\">Thomas Merritt</a>",
          "description": "Whilst recent neural text-to-speech (TTS) approaches produce high-quality\nspeech, they typically require a large amount of recordings from the target\nspeaker. In previous work, a 3-step method was proposed to generate\nhigh-quality TTS while greatly reducing the amount of data required for\ntraining. However, we have observed a ceiling effect in the level of\nnaturalness achievable for highly expressive voices when using this approach.\nIn this paper, we present a method for building highly expressive TTS voices\nwith as little as 15 minutes of speech data from the target speaker. Compared\nto the current state-of-the-art approach, our proposed improvements close the\ngap to recordings by 23.3% for naturalness of speech and by 16.3% for speaker\nsimilarity. Further, we match the naturalness and speaker similarity of a\nTacotron2-based full-data (~10 hours) model using only 15 minutes of target\nspeaker data, whereas with 30 minutes or more, we significantly outperform it.\nThe following improvements are proposed: 1) changing from an autoregressive,\nattention-based TTS model to a non-autoregressive model replacing attention\nwith an external duration model and 2) an additional Conditional Generative\nAdversarial Network (cGAN) based fine-tuning step.",
          "link": "http://arxiv.org/abs/2106.12896",
          "publishedOn": "2021-06-25T02:00:46.856Z",
          "wordCount": 647,
          "title": "Non-Autoregressive TTS with Explicit Duration Modelling for Low-Resource Highly Expressive Speech. (arXiv:2106.12896v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12974",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Li_S/0/1/0/all/0/1\">Sujie Li</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhang_J/0/1/0/all/0/1\">Jiang Zhang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhang_P/0/1/0/all/0/1\">Pan Zhang</a>",
          "description": "Modeling the joint distribution of high-dimensional data is a central task in\nunsupervised machine learning. In recent years, many interests have been\nattracted to developing learning models based on tensor networks, which have\nadvantages of theoretical understandings of the expressive power using\nentanglement properties, and as a bridge connecting the classical computation\nand the quantum computation. Despite the great potential, however, existing\ntensor-network-based unsupervised models only work as a proof of principle, as\ntheir performances are much worse than the standard models such as the\nrestricted Boltzmann machines and neural networks. In this work, we present the\nAutoregressive Matrix Product States (AMPS), a tensor-network-based model\ncombining the matrix product states from quantum many-body physics and the\nautoregressive models from machine learning. The model enjoys exact calculation\nof normalized probability and unbiased sampling, as well as a clear theoretical\nunderstanding of expressive power. We demonstrate the performance of our model\nusing two applications, the generative modeling on synthetic and real-world\ndata, and the reinforcement learning in statistical physics. Using extensive\nnumerical experiments, we show that the proposed model significantly\noutperforms the existing tensor-network-based models and the restricted\nBoltzmann machines, and is competitive with the state-of-the-art neural network\nmodels.",
          "link": "http://arxiv.org/abs/2106.12974",
          "publishedOn": "2021-06-25T02:00:46.851Z",
          "wordCount": 651,
          "title": "Tensor networks for unsupervised machine learning. (arXiv:2106.12974v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12985",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Fataliyev_K/0/1/0/all/0/1\">Kamaladdin Fataliyev</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chivukula_A/0/1/0/all/0/1\">Aneesh Chivukula</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Prasad_M/0/1/0/all/0/1\">Mukesh Prasad</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>",
          "description": "Stock market movements are influenced by public and private information\nshared through news articles, company reports, and social media discussions.\nAnalyzing these vast sources of data can give market participants an edge to\nmake profit. However, the majority of the studies in the literature are based\non traditional approaches that come short in analyzing unstructured, vast\ntextual data. In this study, we provide a review on the immense amount of\nexisting literature of text-based stock market analysis. We present input data\ntypes and cover main textual data sources and variations. Feature\nrepresentation techniques are then presented. Then, we cover the analysis\ntechniques and create a taxonomy of the main stock market forecast models.\nImportantly, we discuss representative work in each category of the taxonomy,\nanalyzing their respective contributions. Finally, this paper shows the\nfindings on unaddressed open problems and gives suggestions for future work.\nThe aim of this study is to survey the main stock market analysis models, text\nrepresentation techniques for financial market prediction, shortcomings of\nexisting techniques, and propose promising directions for future research.",
          "link": "http://arxiv.org/abs/2106.12985",
          "publishedOn": "2021-06-25T02:00:46.845Z",
          "wordCount": 611,
          "title": "Stock Market Analysis with Text Data: A Review. (arXiv:2106.12985v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertoin_D/0/1/0/all/0/1\">David Bertoin</a> (ISAE-SUPAERO), <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Bolte</a> (UT1, TSE), <a href=\"http://arxiv.org/find/cs/1/au:+Gerchinovitz_S/0/1/0/all/0/1\">S&#xe9;bastien Gerchinovitz</a> (IMT), <a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Edouard Pauwels</a> (CNRS, IRIT)",
          "description": "In theory, the choice of ReLU (0) in [0, 1] for a neural network has a\nnegligible influence both on backpropagation and training. Yet, in the real\nworld, 32 bits default precision combined with the size of deep learning\nproblems makes it a hyperparameter of training methods. We investigate the\nimportance of the value of ReLU (0) for several precision levels (16, 32, 64\nbits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST,\nCIFAR10, SVHN). We observe considerable variations of backpropagation outputs\nwhich occur around half of the time in 32 bits precision. The effect disappears\nwith double precision, while it is systematic at 16 bits. For vanilla SGD\ntraining, the choice ReLU (0) = 0 seems to be the most efficient. We also\nevidence that reconditioning approaches as batch-norm or ADAM tend to buffer\nthe influence of ReLU (0)'s value. Overall, the message we want to convey is\nthat algorithmic differentiation of nonsmooth problems potentially hides\nparameters that could be tuned advantageously.",
          "link": "http://arxiv.org/abs/2106.12915",
          "publishedOn": "2021-06-25T02:00:46.839Z",
          "wordCount": 600,
          "title": "Numerical influence of ReLU'(0) on backpropagation. (arXiv:2106.12915v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hossam_M/0/1/0/all/0/1\">Mahmoud Hossam</a>",
          "description": "Real-time remote sensing applications like search and rescue missions,\nmilitary target detection, environmental monitoring, hazard prevention and\nother time-critical applications require onboard real time processing\ncapabilities or autonomous decision making. Some unmanned remote systems like\nsatellites are physically remote from their operators, and all control of the\nspacecraft and data returned by the spacecraft must be transmitted over a\nwireless radio link. This link may not be available for extended periods when\nthe satellite is out of line of sight of its ground station. Therefore,\nlightweight, small size and low power consumption hardware is essential for\nonboard real time processing systems. With increasing dimensionality, size and\nresolution of recent hyperspectral imaging sensors, additional challenges are\nposed upon remote sensing processing systems and more capable computing\narchitectures are needed. Graphical Processing Units (GPUs) emerged as\npromising architecture for light weight high performance computing that can\naddress these computational requirements for onboard systems. The goal of this\nstudy is to build high performance methods for onboard hyperspectral analysis.\nWe propose accelerated methods for the well-known recursive hierarchical\nsegmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a\nGPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the\nNational Aeronautics and Space Administration (NASA), which is designed to\nprovide rich classification information with several output levels. The\nachieved speedups by parallel solutions compared to CPU sequential\nimplementations are 21x for parallel single GPU and 240x for hybrid multi-node\ncomputer clusters with 16 computing nodes. The energy consumption is reduced to\n74% using a single GPU compared to the equivalent parallel CPU cluster.",
          "link": "http://arxiv.org/abs/2106.12942",
          "publishedOn": "2021-06-25T02:00:46.834Z",
          "wordCount": 715,
          "title": "High Performance Hyperspectral Image Classification using Graphics Processing Units. (arXiv:2106.12942v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12751",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_X/0/1/0/all/0/1\">Xuanqing Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chang_W/0/1/0/all/0/1\">Wei-Cheng Chang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yu_H/0/1/0/all/0/1\">Hsiang-Fu Yu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit S. Dhillon</a>",
          "description": "Partition-based methods are increasingly-used in extreme multi-label\nclassification (XMC) problems due to their scalability to large output spaces\n(e.g., millions or more). However, existing methods partition the large label\nspace into mutually exclusive clusters, which is sub-optimal when labels have\nmulti-modality and rich semantics. For instance, the label \"Apple\" can be the\nfruit or the brand name, which leads to the following research question: can we\ndisentangle these multi-modal labels with non-exclusive clustering tailored for\ndownstream XMC tasks? In this paper, we show that the label assignment problem\nin partition-based XMC can be formulated as an optimization problem, with the\nobjective of maximizing precision rates. This leads to an efficient algorithm\nto form flexible and overlapped label clusters, and a method that can\nalternatively optimizes the cluster assignments and the model parameters for\npartition-based XMC. Experimental results on synthetic and real datasets show\nthat our method can successfully disentangle multi-modal labels, leading to\nstate-of-the-art (SOTA) results on four XMC benchmarks.",
          "link": "http://arxiv.org/abs/2106.12751",
          "publishedOn": "2021-06-25T02:00:46.827Z",
          "wordCount": 595,
          "title": "Label Disentanglement in Partition-based Extreme Multilabel Classification. (arXiv:2106.12751v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wegmann_M/0/1/0/all/0/1\">Marc Wegmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zipperling_D/0/1/0/all/0/1\">Domenique Zipperling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillenbrand_J/0/1/0/all/0/1\">Jonas Hillenbrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1\">J&#xfc;rgen Fleischer</a>",
          "description": "Data analysis plays an indispensable role for value creation in industry.\nCluster analysis in this context is able to explore given datasets with little\nor no prior knowledge and to identify unknown patterns. As (big) data\ncomplexity increases in the dimensions volume, variety, and velocity, this\nbecomes even more important. Many tools for cluster analysis have been\ndeveloped from early on and the variety of different clustering algorithms is\nhuge. As the selection of the right clustering procedure is crucial to the\nresults of the data analysis, users are in need for support on their journey of\nextracting knowledge from raw data. Thus, the objective of this paper lies in\nthe identification of a systematic selection logic for clustering algorithms\nand corresponding validation concepts. The goal is to enable potential users to\nchoose an algorithm that fits best to their needs and the properties of their\nunderlying data clustering problem. Moreover, users are supported in selecting\nthe right validation concepts to make sense of the clustering results. Based on\na comprehensive literature review, this paper provides assessment criteria for\nclustering method evaluation and validation concept selection. The criteria are\napplied to several common algorithms and the selection process of an algorithm\nis supported by the introduction of pseudocode-based routines that consider the\nunderlying data structure.",
          "link": "http://arxiv.org/abs/2106.12792",
          "publishedOn": "2021-06-25T02:00:46.816Z",
          "wordCount": 658,
          "title": "A review of systematic selection of clustering algorithms and their evaluation. (arXiv:2106.12792v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Viehmann_T/0/1/0/all/0/1\">Thomas Viehmann</a>",
          "description": "With the rise of machine learning and deep learning based applications in\npractice, monitoring, i.e. verifying that these operate within specification,\nhas become an important practical problem. An important aspect of this\nmonitoring is to check whether the inputs (or intermediates) have strayed from\nthe distribution they were validated for, which can void the performance\nassurances obtained during testing.\n\nThere are two common approaches for this. The, perhaps, more classical one is\noutlier detection or novelty detection, where, for a single input we ask\nwhether it is an outlier, i.e. exceedingly unlikely to have originated from a\nreference distribution. The second, perhaps more recent approach, is to\nconsider a larger number of inputs and compare its distribution to a reference\ndistribution (e.g. sampled during testing). This is done under the label drift\ndetection.\n\nIn this work, we bridge the gap between outlier detection and drift detection\nthrough comparing a given number of inputs to an automatically chosen part of\nthe reference distribution.",
          "link": "http://arxiv.org/abs/2106.12893",
          "publishedOn": "2021-06-25T02:00:46.789Z",
          "wordCount": 609,
          "title": "Partial Wasserstein and Maximum Mean Discrepancy distances for bridging the gap between outlier detection and drift detection. (arXiv:2106.12893v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Si Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1\">Samy Bengio</a>",
          "description": "Attentional mechanisms are order-invariant. Positional encoding is a crucial\ncomponent to allow attention-based deep model architectures such as Transformer\nto address sequences or images where the position of information matters. In\nthis paper, we propose a novel positional encoding method based on learnable\nFourier features. Instead of hard-coding each position as a token or a vector,\nwe represent each position, which can be multi-dimensional, as a trainable\nencoding based on learnable Fourier feature mapping, modulated with a\nmulti-layer perceptron. The representation is particularly advantageous for a\nspatial multi-dimensional position, e.g., pixel positions on an image, where\n$L_2$ distances or more complex positional relationships need to be captured.\nOur experiments based on several public benchmark tasks show that our learnable\nFourier feature representation for multi-dimensional positional encoding\noutperforms existing methods by both improving the accuracy and allowing faster\nconvergence.",
          "link": "http://arxiv.org/abs/2106.02795",
          "publishedOn": "2021-06-25T02:00:46.778Z",
          "wordCount": 598,
          "title": "Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding. (arXiv:2106.02795v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haixu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiehui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>",
          "description": "Extending the forecasting time is a critical demand for real applications,\nsuch as extreme weather early warning and long-term energy consumption\nplanning. This paper studies the \\textit{long-term forecasting} problem of time\nseries. Prior Transformer-based models adopt various self-attention mechanisms\nto discover the long-range dependencies. However, intricate temporal patterns\nof the long-term future prohibit the model from finding reliable dependencies.\nAlso, Transformers have to adopt the sparse versions of point-wise\nself-attentions for long series efficiency, resulting in the information\nutilization bottleneck. Towards these challenges, we propose Autoformer as a\nnovel decomposition architecture with an Auto-Correlation mechanism. We go\nbeyond the pre-processing convention of series decomposition and renovate it as\na basic inner block of deep models. This design empowers Autoformer with\nprogressive decomposition capacities for complex time series. Further, inspired\nby the stochastic process theory, we design the Auto-Correlation mechanism\nbased on the series periodicity, which conducts the dependencies discovery and\nrepresentation aggregation at the sub-series level. Auto-Correlation\noutperforms self-attention in both efficiency and accuracy. In long-term\nforecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative\nimprovement on six benchmarks, covering five practical applications: energy,\ntraffic, economics, weather and disease.",
          "link": "http://arxiv.org/abs/2106.13008",
          "publishedOn": "2021-06-25T02:00:46.763Z",
          "wordCount": 624,
          "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. (arXiv:2106.13008v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maddox_W/0/1/0/all/0/1\">Wesley J. Maddox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balandat_M/0/1/0/all/0/1\">Maximilian Balandat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakshy_E/0/1/0/all/0/1\">Eytan Bakshy</a>",
          "description": "Bayesian Optimization is a sample-efficient black-box optimization procedure\nthat is typically applied to problems with a small number of independent\nobjectives. However, in practice we often wish to optimize objectives defined\nover many correlated outcomes (or ``tasks\"). For example, scientists may want\nto optimize the coverage of a cell tower network across a dense grid of\nlocations. Similarly, engineers may seek to balance the performance of a robot\nacross dozens of different environments via constrained or robust optimization.\nHowever, the Gaussian Process (GP) models typically used as probabilistic\nsurrogates for multi-task Bayesian Optimization scale poorly with the number of\noutcomes, greatly limiting applicability. We devise an efficient technique for\nexact multi-task GP sampling that combines exploiting Kronecker structure in\nthe covariance matrices with Matheron's identity, allowing us to perform\nBayesian Optimization using exact multi-task GP models with tens of thousands\nof correlated outputs. In doing so, we achieve substantial improvements in\nsample efficiency compared to existing approaches that only model aggregate\nfunctions of the outcomes. We demonstrate how this unlocks a new class of\napplications for Bayesian Optimization across a range of tasks in science and\nengineering, including optimizing interference patterns of an optical\ninterferometer with more than 65,000 outputs.",
          "link": "http://arxiv.org/abs/2106.12997",
          "publishedOn": "2021-06-25T02:00:46.752Z",
          "wordCount": 634,
          "title": "Bayesian Optimization with High-Dimensional Outputs. (arXiv:2106.12997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zengyi Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chuchu Fan</a>",
          "description": "We study constrained reinforcement learning (CRL) from a novel perspective by\nsetting constraints directly on state density functions, rather than the value\nfunctions considered by previous works. State density has a clear physical and\nmathematical interpretation, and is able to express a wide variety of\nconstraints such as resource limits and safety requirements. Density\nconstraints can also avoid the time-consuming process of designing and tuning\ncost functions required by value function-based constraints to encode system\nspecifications. We leverage the duality between density functions and Q\nfunctions to develop an effective algorithm to solve the density constrained RL\nproblem optimally and the constrains are guaranteed to be satisfied. We prove\nthat the proposed algorithm converges to a near-optimal solution with a bounded\nerror even when the policy update is imperfect. We use a set of comprehensive\nexperiments to demonstrate the advantages of our approach over state-of-the-art\nCRL methods, with a wide range of density constrained tasks as well as standard\nCRL benchmarks such as Safety-Gym.",
          "link": "http://arxiv.org/abs/2106.12764",
          "publishedOn": "2021-06-25T02:00:46.744Z",
          "wordCount": 596,
          "title": "Density Constrained Reinforcement Learning. (arXiv:2106.12764v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_B/0/1/0/all/0/1\">Bryan E. Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenz_J/0/1/0/all/0/1\">Juan A. Saenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonnewald_M/0/1/0/all/0/1\">Maike Sonnewald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_D/0/1/0/all/0/1\">Daniel Livescu</a>",
          "description": "The advent of big data has vast potential for discovery in natural phenomena\nranging from climate science to medicine, but overwhelming complexity stymies\ninsight. Existing theory is often not able to succinctly describe salient\nphenomena, and progress has largely relied on ad hoc definitions of dynamical\nregimes to guide and focus exploration. We present a formal definition in which\nthe identification of dynamical regimes is formulated as an optimization\nproblem, and we propose an intelligible objective function. Furthermore, we\npropose an unsupervised learning framework which eliminates the need for a\npriori knowledge and ad hoc definitions; instead, the user need only choose\nappropriate clustering and dimensionality reduction algorithms, and this choice\ncan be guided using our proposed objective function. We illustrate its\napplicability with example problems drawn from ocean dynamics, tumor\nangiogenesis, and turbulent boundary layers. Our method is a step towards\nunbiased data exploration that allows serendipitous discovery within dynamical\nsystems, with the potential to propel the physical sciences forward.",
          "link": "http://arxiv.org/abs/2106.12963",
          "publishedOn": "2021-06-25T02:00:46.739Z",
          "wordCount": 621,
          "title": "Objective discovery of dominant dynamical processes with intelligible machine learning. (arXiv:2106.12963v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dongjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evensen_S/0/1/0/all/0/1\">Sara Evensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1\">&#xc7;a&#x11f;atay Demiralp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1\">Estevam Hruschka</a>",
          "description": "Despite rapid developments in the field of machine learning research,\ncollecting high-quality labels for supervised learning remains a bottleneck for\nmany applications. This difficulty is exacerbated by the fact that\nstate-of-the-art models for NLP tasks are becoming deeper and more complex,\noften increasing the amount of training data required even for fine-tuning.\nWeak supervision methods, including data programming, address this problem and\nreduce the cost of label collection by using noisy label sources for\nsupervision. However, until recently, data programming was only accessible to\nusers who knew how to program. To bridge this gap, the Data Programming by\nDemonstration framework was proposed to facilitate the automatic creation of\nlabeling functions based on a few examples labeled by a domain expert. This\nframework has proven successful for generating high-accuracy labeling models\nfor document classification. In this work, we extend the DPBD framework to\nspan-level annotation tasks, arguably one of the most time-consuming NLP\nlabeling tasks. We built a novel tool, TagRuler, that makes it easy for\nannotators to build span-level labeling functions without programming and\nencourages them to explore trade-offs between different labeling models and\nactive learning strategies. We empirically demonstrated that an annotator could\nachieve a higher F1 score using the proposed tool compared to manual labeling\nfor different span-level annotation tasks.",
          "link": "http://arxiv.org/abs/2106.12767",
          "publishedOn": "2021-06-25T02:00:46.712Z",
          "wordCount": 660,
          "title": "TagRuler: Interactive Tool for Span-Level Data Programming by Demonstration. (arXiv:2106.12767v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cailian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>",
          "description": "In federated learning (FL), model training is distributed over clients and\nlocal models are aggregated by a central server. The performance of uploaded\nmodels in such situations can vary widely due to imbalanced data distributions,\npotential demands on privacy protections, and quality of transmissions. In this\npaper, we aim to minimize FL training delay over wireless channels, constrained\nby overall training performance as well as each client's differential privacy\n(DP) requirement. We solve this problem in the framework of multi-agent\nmulti-armed bandit (MAMAB) to deal with the situation where there are multiple\nclients confornting different unknown transmission environments, e.g., channel\nfading and interferences. Specifically, we first transform the long-term\nconstraints on both training performance and each client's DP into a virtual\nqueue based on the Lyapunov drift technique. Then, we convert the MAMAB to a\nmax-min bipartite matching problem at each communication round, by estimating\nrewards with the upper confidence bound (UCB) approach. More importantly, we\npropose two efficient solutions to this matching problem, i.e., modified\nHungarian algorithm and greedy matching with a better alternative (GMBA), in\nwhich the first one can achieve the optimal solution with a high complexity\nwhile the second one approaches a better trade-off by enabling a verified\nlow-complexity with little performance loss. In addition, we develop an upper\nbound on the expected regret of this MAMAB based FL framework, which shows a\nlinear growth over the logarithm of communication rounds, justifying its\ntheoretical feasibility. Extensive experimental results are conducted to\nvalidate the effectiveness of our proposed algorithms, and the impacts of\nvarious parameters on the FL performance over wireless edge networks are also\ndiscussed.",
          "link": "http://arxiv.org/abs/2106.13039",
          "publishedOn": "2021-06-25T02:00:46.696Z",
          "wordCount": 720,
          "title": "Low-Latency Federated Learning over Wireless Channels with Differential Privacy. (arXiv:2106.13039v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dymond_J/0/1/0/all/0/1\">Jack Dymond</a>",
          "description": "When machine learning models encounter data which is out of the distribution\non which they were trained they have a tendency to behave poorly, most\nprominently over-confidence in erroneous predictions. Such behaviours will have\ndisastrous effects on real-world machine learning systems. In this field\ngraceful degradation refers to the optimisation of model performance as it\nencounters this out-of-distribution data. This work presents a definition and\ndiscussion of graceful degradation and where it can be applied in deployed\nvisual systems. Following this a survey of relevant areas is undertaken,\nnovelly splitting the graceful degradation problem into active and passive\napproaches. In passive approaches, graceful degradation is handled and achieved\nby the model in a self-contained manner, in active approaches the model is\nupdated upon encountering epistemic uncertainties. This work communicates the\nimportance of the problem and aims to prompt the development of machine\nlearning strategies that are aware of graceful degradation.",
          "link": "http://arxiv.org/abs/2106.11119",
          "publishedOn": "2021-06-25T02:00:46.682Z",
          "wordCount": 592,
          "title": "Graceful Degradation and Related Fields. (arXiv:2106.11119v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13086",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuanhao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Badong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshimura_N/0/1/0/all/0/1\">Natsue Yoshimura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koike_Y/0/1/0/all/0/1\">Yasuharu Koike</a>",
          "description": "The Partial Least Square Regression (PLSR) algorithm exhibits exceptional\ncompetence for predicting continuous variables from inter-correlated brain\nrecordings in brain-computer interfaces, which achieved successful prediction\nfrom epidural electrocorticography of macaques to three-dimensional continuous\nhand trajectories recently. Nevertheless, PLSR is in essence formulated based\non the least square criterion, thus, being non-robust with respect to\ncomplicated noises consequently. The aim of the present study is to propose a\nrobust version of PLSR. To this end, the maximum correntropy criterion is\nadopted to structure a new robust variant of PLSR, namely Partial Maximum\nCorrentropy Regression (PMCR). Half-quadratic optimization technique is\nutilized to calculate the robust latent variables. We assess the proposed PMCR\non a synthetic example and the public Neurotycho dataset. Compared with the\nconventional PLSR and the state-of-the-art variant, PMCR realized superior\nprediction competence on three different performance indicators with\ncontaminated training set. The proposed PMCR was demonstrated as an effective\napproach for robust decoding from noisy brain measurements, which could reduce\nthe performance degradation resulting from adverse noises, thus, improving the\ndecoding robustness of brain-computer interfaces.",
          "link": "http://arxiv.org/abs/2106.13086",
          "publishedOn": "2021-06-25T02:00:46.676Z",
          "wordCount": 626,
          "title": "Partial Maximum Correntropy Regression for Robust Trajectory Decoding from Noisy Epidural Electrocorticographic Signals. (arXiv:2106.13086v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2009.04899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jingyuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun-Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimoukha_I/0/1/0/all/0/1\">Imad Jaimoukha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>",
          "description": "In this paper, we aim to address the problem of solving a non-convex\noptimization problem over an intersection of multiple variable sets. This kind\nof problems is typically solved by using an alternating minimization (AM)\nstrategy which splits the overall problem into a set of sub-problems\ncorresponding to each variable, and then iteratively performs minimization over\neach sub-problem using a fixed updating rule. However, due to the intrinsic\nnon-convexity of the overall problem, the optimization can usually be trapped\ninto bad local minimum even when each sub-problem can be globally optimized at\neach iteration. To tackle this problem, we propose a meta-learning based Global\nScope Optimization (GSO) method. It adaptively generates optimizers for\nsub-problems via meta-learners and constantly updates these meta-learners with\nrespect to the global loss information of the overall problem. Therefore, the\nsub-problems are optimized with the objective of minimizing the global loss\nspecifically. We evaluate the proposed model on a number of simulations,\nincluding solving bi-linear inverse problems: matrix completion, and non-linear\nproblems: Gaussian mixture models. The experimental results show that our\nproposed approach outperforms AM-based methods in standard settings, and is\nable to achieve effective optimization in some challenging cases while other\nmethods would typically fail.",
          "link": "http://arxiv.org/abs/2009.04899",
          "publishedOn": "2021-06-25T02:00:46.653Z",
          "wordCount": 691,
          "title": "Meta-learning for Multi-variable Non-convex Optimization Problems: Iterating Non-optimums Makes Optimum Possible. (arXiv:2009.04899v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12639",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Schmucker_R/0/1/0/all/0/1\">Robin Schmucker</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Donini_M/0/1/0/all/0/1\">Michele Donini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zafar_M/0/1/0/all/0/1\">Muhammad Bilal Zafar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salinas_D/0/1/0/all/0/1\">David Salinas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Archambeau_C/0/1/0/all/0/1\">C&#xe9;dric Archambeau</a>",
          "description": "Hyperparameter optimization (HPO) is increasingly used to automatically tune\nthe predictive performance (e.g., accuracy) of machine learning models.\nHowever, in a plethora of real-world applications, accuracy is only one of the\nmultiple -- often conflicting -- performance criteria, necessitating the\nadoption of a multi-objective (MO) perspective. While the literature on MO\noptimization is rich, few prior studies have focused on HPO. In this paper, we\npropose algorithms that extend asynchronous successive halving (ASHA) to the MO\nsetting. Considering multiple evaluation metrics, we assess the performance of\nthese methods on three real world tasks: (i) Neural architecture search, (ii)\nalgorithmic fairness and (iii) language model optimization. Our empirical\nanalysis shows that MO ASHA enables to perform MO HPO at scale. Further, we\nobserve that that taking the entire Pareto front into account for candidate\nselection consistently outperforms multi-fidelity HPO based on MO scalarization\nin terms of wall-clock time. Our algorithms (to be open-sourced) establish new\nbaselines for future research in the area.",
          "link": "http://arxiv.org/abs/2106.12639",
          "publishedOn": "2021-06-25T02:00:46.633Z",
          "wordCount": 590,
          "title": "Multi-objective Asynchronous Successive Halving. (arXiv:2106.12639v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chuning Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagabandi_A/0/1/0/all/0/1\">Anusha Nagabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "The ability to plan into the future while utilizing only raw high-dimensional\nobservations, such as images, can provide autonomous agents with broad\ncapabilities. Visual model-based reinforcement learning (RL) methods that plan\nfuture actions directly have shown impressive results on tasks that require\nonly short-horizon reasoning, however, these methods struggle on temporally\nextended tasks. We argue that it is easier to solve long-horizon tasks by\nplanning sequences of states rather than just actions, as the effects of\nactions greatly compound over time and are harder to optimize. To achieve this,\nwe draw on the idea of collocation, which has shown good results on\nlong-horizon tasks in optimal control literature, and adapt it to the\nimage-based setting by utilizing learned latent state space models. The\nresulting latent collocation method (LatCo) optimizes trajectories of latent\nstates, which improves over previously proposed shooting methods for visual\nmodel-based RL on tasks with sparse rewards and long-term goals. Videos and\ncode at https://orybkin.github.io/latco/.",
          "link": "http://arxiv.org/abs/2106.13229",
          "publishedOn": "2021-06-25T02:00:46.581Z",
          "wordCount": 611,
          "title": "Model-Based Reinforcement Learning via Latent-Space Collocation. (arXiv:2106.13229v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Spoken conversational question answering (SCQA) requires machines to model\ncomplex dialogue flow given the speech utterances and text corpora. Different\nfrom traditional text question answering (QA) tasks, SCQA involves audio signal\nprocessing, passage comprehension, and contextual understanding. However, ASR\nsystems introduce unexpected noisy signals to the transcriptions, which result\nin performance degradation on SCQA. To overcome the problem, we propose CADNet,\na novel contextualized attention-based distillation approach, which applies\nboth cross-attention and self-attention to obtain ASR-robust contextualized\nembedding representations of the passage and dialogue history for performance\nimprovements. We also introduce the spoken conventional knowledge distillation\nframework to distill the ASR-robust knowledge from the estimated probabilities\nof the teacher model to the student. We conduct extensive experiments on the\nSpoken-CoQA dataset and demonstrate that our approach achieves remarkable\nperformance in this task.",
          "link": "http://arxiv.org/abs/2010.11066",
          "publishedOn": "2021-06-25T02:00:46.576Z",
          "wordCount": 631,
          "title": "Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Ming Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>",
          "description": "This work studies the statistical limits of uniform convergence for offline\npolicy evaluation (OPE) problems with model-based methods (for episodic MDP)\nand provides a unified framework towards optimal learning for several\nwell-motivated offline tasks. Uniform OPE\n$\\sup_\\Pi|Q^\\pi-\\hat{Q}^\\pi|<\\epsilon$ is a stronger measure than the\npoint-wise OPE and ensures offline learning when $\\Pi$ contains all policies\n(the global class). In this paper, we establish an $\\Omega(H^2\nS/d_m\\epsilon^2)$ lower bound (over model-based family) for the global uniform\nOPE and our main result establishes an upper bound of\n$\\tilde{O}(H^2/d_m\\epsilon^2)$ for the \\emph{local} uniform convergence that\napplies to all \\emph{near-empirically optimal} policies for the MDPs with\n\\emph{stationary} transition. Here $d_m$ is the minimal marginal state-action\nprobability. Critically, the highlight in achieving the optimal rate\n$\\tilde{O}(H^2/d_m\\epsilon^2)$ is our design of \\emph{singleton absorbing MDP},\nwhich is a new sharp analysis tool that works with the model-based approach. We\ngeneralize such a model-based framework to the new settings: offline\ntask-agnostic and the offline reward-free with optimal complexity\n$\\tilde{O}(H^2\\log(K)/d_m\\epsilon^2)$ ($K$ is the number of tasks) and\n$\\tilde{O}(H^2S/d_m\\epsilon^2)$ respectively. These results provide a unified\nsolution for simultaneously solving different offline RL problems.",
          "link": "http://arxiv.org/abs/2105.06029",
          "publishedOn": "2021-06-25T02:00:46.570Z",
          "wordCount": 665,
          "title": "Optimal Uniform OPE and Model-based Offline Reinforcement Learning in Time-Homogeneous, Reward-Free and Task-Agnostic Settings. (arXiv:2105.06029v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1\">Cameron Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_M/0/1/0/all/0/1\">Michael Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_T/0/1/0/all/0/1\">Tim Klinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1\">George Konidaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1\">Matthew Riemer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tesauro_G/0/1/0/all/0/1\">Gerald Tesauro</a>",
          "description": "The difficulty of deterministic planning increases exponentially with\nsearch-tree depth. Black-box planning presents an even greater challenge, since\nplanners must operate without an explicit model of the domain. Heuristics can\nmake search more efficient, but goal-aware heuristics for black-box planning\nusually rely on goal counting, which is often quite uninformative. In this\nwork, we show how to overcome this limitation by discovering macro-actions that\nmake the goal-count heuristic more accurate. Our approach searches for\nmacro-actions with focused effects (i.e. macros that modify only a small number\nof state variables), which align well with the assumptions made by the\ngoal-count heuristic. Focused macros dramatically improve black-box planning\nefficiency across a wide range of planning domains, sometimes beating even\nstate-of-the-art planners with access to a full domain model.",
          "link": "http://arxiv.org/abs/2004.13242",
          "publishedOn": "2021-06-25T02:00:46.565Z",
          "wordCount": 620,
          "title": "Efficient Black-Box Planning Using Macro-Actions with Focused Effects. (arXiv:2004.13242v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.16243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dobrev_D/0/1/0/all/0/1\">Dimiter Dobrev</a>",
          "description": "We will reduce the task of creating AI to the task of finding an appropriate\nlanguage for description of the world. This will not be a programing language\nbecause programing languages describe only computable functions, while our\nlanguage will describe a somewhat broader class of functions. Another\nspecificity of this language will be that the description will consist of\nseparate modules. This will enable us look for the description of the world\nautomatically such that we discover it module after module. Our approach to the\ncreation of this new language will be to start with a particular world and\nwrite the description of that particular world. The point is that the language\nwhich can describe this particular world will be appropriate for describing any\nworld.",
          "link": "http://arxiv.org/abs/2010.16243",
          "publishedOn": "2021-06-25T02:00:46.547Z",
          "wordCount": 581,
          "title": "Language for Description of Worlds. (arXiv:2010.16243v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1\">Howe Seng Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sin_K/0/1/0/all/0/1\">Kai Ling Sin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kelly Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1\">Nicole Ka Hei Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_X/0/1/0/all/0/1\">Xin Yu Liew</a>",
          "description": "The intention of this research is to study and design an automated\nagriculture commodity price prediction system with novel machine learning\ntechniques. Due to the increasing large amounts historical data of agricultural\ncommodity prices and the need of performing accurate prediction of price\nfluctuations, the solution has largely shifted from statistical methods to\nmachine learning area. However, the selection of proper set from historical\ndata for forecasting still has limited consideration. On the other hand, when\nimplementing machine learning techniques, finding a suitable model with optimal\nparameters for global solution, nonlinearity and avoiding curse of\ndimensionality are still biggest challenges, therefore machine learning\nstrategies study are needed. In this research, we propose a web-based automated\nsystem to predict agriculture commodity price. In the two series experiments,\nfive popular machine learning algorithms, ARIMA, SVR, Prophet, XGBoost and LSTM\nhave been compared with large historical datasets in Malaysia and the most\noptimal algorithm, LSTM model with an average of 0.304 mean-square error has\nbeen selected as the prediction engine of the proposed system.",
          "link": "http://arxiv.org/abs/2106.12747",
          "publishedOn": "2021-06-25T02:00:46.541Z",
          "wordCount": 628,
          "title": "Automated Agriculture Commodity Price Prediction System with Machine Learning Techniques. (arXiv:2106.12747v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12864",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Johann Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_G/0/1/0/all/0/1\">Guangming Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hua_C/0/1/0/all/0/1\">Cong Hua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_M/0/1/0/all/0/1\">Mingtao Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+BasheerBennamoun/0/1/0/all/0/1\">BasheerBennamoun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoyuan Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Juan Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_P/0/1/0/all/0/1\">Peiyi Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mei_L/0/1/0/all/0/1\">Lin Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_S/0/1/0/all/0/1\">Syed Afaq Ali Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>",
          "description": "The astounding success made by artificial intelligence (AI) in healthcare and\nother fields proves that AI can achieve human-like performance. However,\nsuccess always comes with challenges. Deep learning algorithms are\ndata-dependent and require large datasets for training. The lack of data in the\nmedical imaging field creates a bottleneck for the application of deep learning\nto medical image analysis. Medical image acquisition, annotation, and analysis\nare costly, and their usage is constrained by ethical restrictions. They also\nrequire many resources, such as human expertise and funding. That makes it\ndifficult for non-medical researchers to have access to useful and large\nmedical data. Thus, as comprehensive as possible, this paper provides a\ncollection of medical image datasets with their associated challenges for deep\nlearning research. We have collected information of around three hundred\ndatasets and challenges mainly reported between 2013 and 2020 and categorized\nthem into four categories: head & neck, chest & abdomen, pathology & blood, and\n``others''. Our paper has three purposes: 1) to provide a most up to date and\ncomplete list that can be used as a universal reference to easily find the\ndatasets for clinical image analysis, 2) to guide researchers on the\nmethodology to test and evaluate their methods' performance and robustness on\nrelevant datasets, 3) to provide a ``route'' to relevant algorithms for the\nrelevant medical topics, and challenge leaderboards.",
          "link": "http://arxiv.org/abs/2106.12864",
          "publishedOn": "2021-06-25T02:00:46.535Z",
          "wordCount": 708,
          "title": "A Systematic Collection of Medical Image Datasets for Deep Learning. (arXiv:2106.12864v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abid_A/0/1/0/all/0/1\">Abubakar Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>",
          "description": "Understanding and explaining the mistakes made by trained models is critical\nto many machine learning objectives, such as improving robustness, addressing\nconcept drift, and mitigating biases. However, this is often an ad hoc process\nthat involves manually looking at the model's mistakes on many test samples and\nguessing at the underlying reasons for those incorrect predictions. In this\npaper, we propose a systematic approach, conceptual explanation scores (CES),\nthat explains why a classifier makes a mistake on a particular test sample(s)\nin terms of human-understandable concepts (e.g. this zebra is misclassified as\na dog because of faint stripes). We base CES on two prior ideas: counterfactual\nexplanations and concept activation vectors, and validate our approach on\nwell-known pretrained models, showing that it explains the models' mistakes\nmeaningfully. We also train new models with intentional and known spurious\ncorrelations, which CES successfully identifies from a single misclassified\ntest sample. The code for CES is publicly available and can easily be applied\nto new models.",
          "link": "http://arxiv.org/abs/2106.12723",
          "publishedOn": "2021-06-25T02:00:46.529Z",
          "wordCount": 595,
          "title": "Meaningfully Explaining a Model's Mistakes. (arXiv:2106.12723v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duong_T/0/1/0/all/0/1\">Thai Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1\">Nikolay Atanasov</a>",
          "description": "Accurate models of robot dynamics are critical for safe and stable control\nand generalization to novel operational conditions. Hand-designed models,\nhowever, may be insufficiently accurate, even after careful parameter tuning.\nThis motivates the use of machine learning techniques to approximate the robot\ndynamics over a training set of state-control trajectories. The dynamics of\nmany robots, including ground, aerial, and underwater vehicles, are described\nin terms of their SE(3) pose and generalized velocity, and satisfy conservation\nof energy principles. This paper proposes a Hamiltonian formulation over the\nSE(3) manifold of the structure of a neural ordinary differential equation\n(ODE) network to approximate the dynamics of a rigid body. In contrast to a\nblack-box ODE network, our formulation guarantees total energy conservation by\nconstruction. We develop energy shaping and damping injection control for the\nlearned, potentially under-actuated SE(3) Hamiltonian dynamics to enable a\nunified approach for stabilization and trajectory tracking with various\nplatforms, including pendulum, rigid-body, and quadrotor systems.",
          "link": "http://arxiv.org/abs/2106.12782",
          "publishedOn": "2021-06-25T02:00:46.524Z",
          "wordCount": 611,
          "title": "Hamiltonian-based Neural ODE Networks on the SE(3) Manifold For Dynamics Learning and Control. (arXiv:2106.12782v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12819",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1\">Yuxuan Du</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Qian_Y/0/1/0/all/0/1\">Yang Qian</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Variational quantum algorithms (VQAs) have the potential of utilizing\nnear-term quantum machines to gain certain computational advantages over\nclassical methods. Nevertheless, modern VQAs suffer from cumbersome\ncomputational overhead, hampered by the tradition of employing a solitary\nquantum processor to handle large-volume data. As such, to better exert the\nsuperiority of VQAs, it is of great significance to improve their runtime\nefficiency. Here we devise an efficient distributed optimization scheme, called\nQUDIO, to address this issue. Specifically, in QUDIO, a classical central\nserver partitions the learning problem into multiple subproblems and allocate\nthem to multiple local nodes where each of them consists of a quantum processor\nand a classical optimizer. During the training procedure, all local nodes\nproceed parallel optimization and the classical server synchronizes\noptimization information among local nodes timely. In doing so, we prove a\nsublinear convergence rate of QUDIO in terms of the number of global iteration\nunder the ideal scenario, while the system imperfection may incur divergent\noptimization. Numerical results on standard benchmarks demonstrate that QUDIO\ncan surprisingly achieve a superlinear runtime speedup with respect to the\nnumber of local nodes. Our proposal can be readily mixed with other advanced\nVQAs-based techniques to narrow the gap between the state of the art and\napplications with quantum advantage.",
          "link": "http://arxiv.org/abs/2106.12819",
          "publishedOn": "2021-06-25T02:00:46.508Z",
          "wordCount": 640,
          "title": "Accelerating variational quantum algorithms with multiple quantum processors. (arXiv:2106.12819v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lucic</a>",
          "description": "We present a scalable post-processing algorithm for debiasing trained models,\nincluding deep neural networks (DNNs), which we prove to be near-optimal by\nbounding its excess Bayes risk. We empirically validate its advantages on\nstandard benchmark datasets across both classical algorithms as well as modern\nDNN architectures and demonstrate that it outperforms previous post-processing\nmethods while performing on par with in-processing. In addition, we show that\nthe proposed algorithm is particularly effective for models trained at scale\nwhere post-processing is a natural and practical choice.",
          "link": "http://arxiv.org/abs/2106.12887",
          "publishedOn": "2021-06-25T02:00:46.503Z",
          "wordCount": 535,
          "title": "A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models. (arXiv:2106.12887v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huh_D/0/1/0/all/0/1\">Dom Huh</a>",
          "description": "Shared feature spaces for actor-critic methods aims to capture generalized\nlatent representations to be used by the policy and value function with the\nhopes for a more stable and sample-efficient optimization. However, such a\nparadigm present a number of challenges in practice, as parameters generating a\nshared representation must learn off two distinct objectives, resulting in\ncompeting updates and learning perturbations. In this paper, we present a novel\nfeature-sharing framework to address these difficulties by introducing the mix\nand mask mechanisms and the distributional scalarization technique. These\nmechanisms behaves dynamically to couple and decouple connected latent features\nvariably between the policy and value function, while the distributional\nscalarization standardizes the two objectives using a probabilistic standpoint.\nFrom our experimental results, we demonstrate significant performance\nimprovements compared to alternative methods using separate networks and\nnetworks with a shared backbone.",
          "link": "http://arxiv.org/abs/2106.13037",
          "publishedOn": "2021-06-25T02:00:46.498Z",
          "wordCount": 559,
          "title": "Mix and Mask Actor-Critic Methods. (arXiv:2106.13037v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_T/0/1/0/all/0/1\">Tushar Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_U/0/1/0/all/0/1\">Umang Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rupali Patil</a>",
          "description": "Anticipating the quantity of new associated or affirmed cases with novel\ncoronavirus ailment 2019 (COVID-19) is critical in the counteraction and\ncontrol of the COVID-19 flare-up. The new associated cases with COVID-19\ninformation were gathered from 20 January 2020 to 21 July 2020. We filtered out\nthe countries which are converging and used those for training the network. We\nutilized the SARIMAX, Linear regression model to anticipate new suspected\nCOVID-19 cases for the countries which did not converge yet. We predict the\ncurve of non-converged countries with the help of proposed Statistical SARIMAX\nmodel (SSM). We present new information investigation-based forecast results\nthat can assist governments with planning their future activities and help\nclinical administrations to be more ready for what's to come. Our framework can\nforesee peak corona cases with an R-Squared value of 0.986 utilizing linear\nregression and fall of this pandemic at various levels for countries like\nIndia, US, and Brazil. We found that considering more countries for training\ndegrades the prediction process as constraints vary from nation to nation.\nThus, we expect that the outcomes referenced in this work will help individuals\nto better understand the possibilities of this pandemic.",
          "link": "http://arxiv.org/abs/2106.12888",
          "publishedOn": "2021-06-25T02:00:46.493Z",
          "wordCount": 693,
          "title": "COVID-19 cases prediction using regression and novel SSM model for non-converged countries. (arXiv:2106.12888v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jokic_P/0/1/0/all/0/1\">Petar Jokic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azarkhish_E/0/1/0/all/0/1\">Erfan Azarkhish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonetti_A/0/1/0/all/0/1\">Andrea Bonetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_M/0/1/0/all/0/1\">Marc Pons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emery_S/0/1/0/all/0/1\">Stephane Emery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1\">Luca Benini</a>",
          "description": "Implementing embedded neural network processing at the edge requires\nefficient hardware acceleration that couples high computational performance\nwith low power consumption. Driven by the rapid evolution of network\narchitectures and their algorithmic features, accelerator designs are\nconstantly updated and improved. To evaluate and compare hardware design\nchoices, designers can refer to a myriad of accelerator implementations in the\nliterature. Surveys provide an overview of these works but are often limited to\nsystem-level and benchmark-specific performance metrics, making it difficult to\nquantitatively compare the individual effect of each utilized optimization\ntechnique. This complicates the evaluation of optimizations for new accelerator\ndesigns, slowing-down the research progress. This work provides a survey of\nneural network accelerator optimization approaches that have been used in\nrecent works and reports their individual effects on edge processing\nperformance. It presents the list of optimizations and their quantitative\neffects as a construction kit, allowing to assess the design choices for each\nbuilding block separately. Reported optimizations range from up to 10'000x\nmemory savings to 33x energy reductions, providing chip designers an overview\nof design choices for implementing efficient low power neural network\naccelerators.",
          "link": "http://arxiv.org/abs/2106.12810",
          "publishedOn": "2021-06-25T02:00:46.488Z",
          "wordCount": 628,
          "title": "A Construction Kit for Efficient Low Power Neural Network Accelerator Designs. (arXiv:2106.12810v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hengxu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>",
          "description": "Successful quantitative investment usually relies on precise predictions of\nthe future movement of the stock price. Recently, machine learning based\nsolutions have shown their capacity to give more accurate stock prediction and\nbecome indispensable components in modern quantitative investment systems.\nHowever, the i.i.d. assumption behind existing methods is inconsistent with the\nexistence of diverse trading patterns in the stock market, which inevitably\nlimits their ability to achieve better stock prediction performance. In this\npaper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to\nempower existing stock prediction models with the ability to model multiple\nstock trading patterns. Essentially, TRA is a lightweight module that consists\nof a set of independent predictors for learning multiple patterns as well as a\nrouter to dispatch samples to different predictors. Nevertheless, the lack of\nexplicit pattern identifiers makes it quite challenging to train an effective\nTRA-based model. To tackle this challenge, we further design a learning\nalgorithm based on Optimal Transport (OT) to obtain the optimal sample to\npredictor assignment and effectively optimize the router with such assignment\nthrough an auxiliary loss term. Experiments on the real-world stock ranking\ntask show that compared to the state-of-the-art baselines, e.g., Attention LSTM\nand Transformer, the proposed method can improve information coefficient (IC)\nfrom 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used\nin this work are publicly available: https://github.com/microsoft/qlib.",
          "link": "http://arxiv.org/abs/2106.12950",
          "publishedOn": "2021-06-25T02:00:46.472Z",
          "wordCount": 686,
          "title": "Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport. (arXiv:2106.12950v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12929",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Bedrunka_M/0/1/0/all/0/1\">Mario Christopher Bedrunka</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wilde_D/0/1/0/all/0/1\">Dominik Wilde</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kliemank_M/0/1/0/all/0/1\">Martin Kliemank</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Reith_D/0/1/0/all/0/1\">Dirk Reith</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Foysi_H/0/1/0/all/0/1\">Holger Foysi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kramer_A/0/1/0/all/0/1\">Andreas Kr&#xe4;mer</a>",
          "description": "The lattice Boltzmann method (LBM) is an efficient simulation technique for\ncomputational fluid mechanics and beyond. It is based on a simple\nstream-and-collide algorithm on Cartesian grids, which is easily compatible\nwith modern machine learning architectures. While it is becoming increasingly\nclear that deep learning can provide a decisive stimulus for classical\nsimulation techniques, recent studies have not addressed possible connections\nbetween machine learning and LBM. Here, we introduce Lettuce, a PyTorch-based\nLBM code with a threefold aim. Lettuce enables GPU accelerated calculations\nwith minimal source code, facilitates rapid prototyping of LBM models, and\nenables integrating LBM simulations with PyTorch's deep learning and automatic\ndifferentiation facility. As a proof of concept for combining machine learning\nwith the LBM, a neural collision model is developed, trained on a doubly\nperiodic shear layer and then transferred to a different flow, a decaying\nturbulence. We also exemplify the added benefit of PyTorch's automatic\ndifferentiation framework in flow control and optimization. To this end, the\nspectrum of a forced isotropic turbulence is maintained without further\nconstraining the velocity field. The source code is freely available from\nhttps://github.com/lettucecfd/lettuce.",
          "link": "http://arxiv.org/abs/2106.12929",
          "publishedOn": "2021-06-25T02:00:46.467Z",
          "wordCount": 622,
          "title": "Lettuce: PyTorch-based Lattice Boltzmann Framework. (arXiv:2106.12929v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Nishant Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanfeld_P/0/1/0/all/0/1\">Pia Hanfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hecht_M/0/1/0/all/0/1\">Michael Hecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bussmann_M/0/1/0/all/0/1\">Michael Bussmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gumhold_S/0/1/0/all/0/1\">Stefan Gumhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmannn_N/0/1/0/all/0/1\">Nico Hoffmannn</a>",
          "description": "Normalizing flows are prominent deep generative models that provide tractable\nprobability distributions and efficient density estimation. However, they are\nwell known to fail while detecting Out-of-Distribution (OOD) inputs as they\ndirectly encode the local features of the input representations in their latent\nspace. In this paper, we solve this overconfidence issue of normalizing flows\nby demonstrating that flows, if extended by an attention mechanism, can\nreliably detect outliers including adversarial attacks. Our approach does not\nrequire outlier data for training and we showcase the efficiency of our method\nfor OOD detection by reporting state-of-the-art performance in diverse\nexperimental settings. Code available at\nhttps://github.com/ComputationalRadiationPhysics/InFlow .",
          "link": "http://arxiv.org/abs/2106.12894",
          "publishedOn": "2021-06-25T02:00:46.453Z",
          "wordCount": 546,
          "title": "InFlow: Robust outlier detection utilizing Normalizing Flows. (arXiv:2106.12894v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhiheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minxian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_M/0/1/0/all/0/1\">Maria Alejandra Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buyya_R/0/1/0/all/0/1\">Rajkumar Buyya</a>",
          "description": "Containerization is a lightweight application virtualization technology,\nproviding high environmental consistency, operating system distribution\nportability, and resource isolation. Existing mainstream cloud service\nproviders have prevalently adopted container technologies in their distributed\nsystem infrastructures for automated application management. To handle the\nautomation of deployment, maintenance, autoscaling, and networking of\ncontainerized applications, container orchestration is proposed as an essential\nresearch problem. However, the highly dynamic and diverse feature of cloud\nworkloads and environments considerably raises the complexity of orchestration\nmechanisms. Machine learning algorithms are accordingly employed by container\norchestration systems for behavior modelling and prediction of\nmulti-dimensional performance metrics. Such insights could further improve the\nquality of resource provisioning decisions in response to the changing\nworkloads under complex environments. In this paper, we present a comprehensive\nliterature review of existing machine learning-based container orchestration\napproaches. Detailed taxonomies are proposed to classify the current researches\nby their common features. Moreover, the evolution of machine learning-based\ncontainer orchestration technologies from the year 2016 to 2021 has been\ndesigned based on objectives and metrics. A comparative analysis of the\nreviewed techniques is conducted according to the proposed taxonomies, with\nemphasis on their key characteristics. Finally, various open research\nchallenges and potential future directions are highlighted.",
          "link": "http://arxiv.org/abs/2106.12739",
          "publishedOn": "2021-06-25T02:00:46.447Z",
          "wordCount": 652,
          "title": "Machine Learning-based Orchestration of Containers: A Taxonomy and Future Directions. (arXiv:2106.12739v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12758",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Dhadphale_J/0/1/0/all/0/1\">Jayesh Dhadphale</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Unni_V/0/1/0/all/0/1\">Vishnu R. Unni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Saha_A/0/1/0/all/0/1\">Abhishek Saha</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sujith_R/0/1/0/all/0/1\">R. I. Sujith</a>",
          "description": "In reacting flow systems, thermoacoustic instability characterized by high\namplitude pressure fluctuations, is driven by a positive coupling between the\nunsteady heat release rate and the acoustic field of the combustor. When the\nunderlying flow is turbulent, as a control parameter of the system is varied\nand the system approach thermoacoustic instability, the acoustic pressure\noscillations synchronize with heat release rate oscillations. Consequently,\nduring the onset of thermoacoustic instability in turbulent combustors, the\nsystem dynamics transition from chaotic oscillations to periodic oscillations\nvia a state of intermittency. Thermoacoustic systems are traditionally modeled\nby coupling the model for the unsteady heat source and the acoustic subsystem,\neach estimated independently. The response of the unsteady heat source, the\nflame, to acoustic fluctuations are characterized by introducing external\nunsteady forcing. This necessitates a powerful excitation module to obtain the\nnonlinear response of the flame to acoustic perturbations. Instead of\ncharacterizing individual subsystems, we introduce a neural ordinary\ndifferential equation (neural ODE) framework to model the thermoacoustic system\nas a whole. The neural ODE model for the thermoacoustic system uses time series\nof the heat release rate and the pressure fluctuations, measured simultaneously\nwithout introducing any external perturbations, to model their coupled\ninteraction. Further, we use the parameters of neural ODE to define an anomaly\nmeasure that represents the proximity of system dynamics to limit cycle\noscillations and thus provide an early warning signal for the onset of\nthermoacoustic instability.",
          "link": "http://arxiv.org/abs/2106.12758",
          "publishedOn": "2021-06-25T02:00:46.441Z",
          "wordCount": 682,
          "title": "Neural ODE to model and prognose thermoacoustic instability. (arXiv:2106.12758v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirichenko_P/0/1/0/all/0/1\">Polina Kirichenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1\">Mehrdad Farajtabar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">Dushyant Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_N/0/1/0/all/0/1\">Nir Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huiyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>",
          "description": "Learning new tasks continuously without forgetting on a constantly changing\ndata distribution is essential for real-world problems but extremely\nchallenging for modern deep learning. In this work we propose HCL, a Hybrid\ngenerative-discriminative approach to Continual Learning for classification. We\nmodel the distribution of each task and each class with a normalizing flow. The\nflow is used to learn the data distribution, perform classification, identify\ntask changes, and avoid forgetting, all leveraging the invertibility and exact\nlikelihood which are uniquely enabled by the normalizing flow model. We use the\ngenerative capabilities of the flow to avoid catastrophic forgetting through\ngenerative replay and a novel functional regularization technique. For task\nidentification, we use state-of-the-art anomaly detection techniques based on\nmeasuring the typicality of the model's statistics. We demonstrate the strong\nperformance of HCL on a range of continual learning benchmarks such as\nsplit-MNIST, split-CIFAR, and SVHN-MNIST.",
          "link": "http://arxiv.org/abs/2106.12772",
          "publishedOn": "2021-06-25T02:00:46.425Z",
          "wordCount": 589,
          "title": "Task-agnostic Continual Learning with Hybrid Probabilistic Models. (arXiv:2106.12772v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shechner_M/0/1/0/all/0/1\">Moshe Shechner</a>",
          "description": "We study the problem of differentially private clustering under\ninput-stability assumptions. Despite the ever-growing volume of works on\ndifferential privacy in general and differentially private clustering in\nparticular, only three works (Nissim et al. 2007, Wang et al. 2015, Huang et\nal. 2018) looked at the problem of privately clustering \"nice\" k-means\ninstances, all three relying on the sample-and-aggregate framework and all\nthree measuring utility in terms of Wasserstein distance between the true\ncluster centers and the centers returned by the private algorithm. In this work\nwe improve upon this line of works on multiple axes. We present a far simpler\nalgorithm for clustering stable inputs (not relying on the sample-and-aggregate\nframework), and analyze its utility in both the Wasserstein distance and the\nk-means cost. Moreover, our algorithm has straight-forward analogues for \"nice\"\nk-median instances and for the local-model of differential privacy.",
          "link": "http://arxiv.org/abs/2106.12959",
          "publishedOn": "2021-06-25T02:00:46.406Z",
          "wordCount": 603,
          "title": "Differentially Private Algorithms for Clustering with Stability Assumptions. (arXiv:2106.12959v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhadip Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rout_S/0/1/0/all/0/1\">Swapna Sourav Rout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Sudeep Choudhary</a>",
          "description": "Detection of semantic data types is a very crucial task in data science for\nautomated data cleaning, schema matching, data discovery, semantic data type\nnormalization and sensitive data identification. Existing methods include\nregular expression-based or dictionary lookup-based methods that are not robust\nto dirty as well unseen data and are limited to a very less number of semantic\ndata types to predict. Existing Machine Learning methods extract large number\nof engineered features from data and build logistic regression, random forest\nor feedforward neural network for this purpose. In this paper, we introduce\nDCoM, a collection of multi-input NLP-based deep neural networks to detect\nsemantic data types where instead of extracting large number of features from\nthe data, we feed the raw values of columns (or instances) to the model as\ntexts. We train DCoM on 686,765 data columns extracted from VizNet corpus with\n78 different semantic data types. DCoM outperforms other contemporary results\nwith a quite significant margin on the same dataset.",
          "link": "http://arxiv.org/abs/2106.12871",
          "publishedOn": "2021-06-25T02:00:46.389Z",
          "wordCount": 617,
          "title": "DCoM: A Deep Column Mapper for Semantic Data Type Detection. (arXiv:2106.12871v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haowei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1\">Feiwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yanli Shao</a>",
          "description": "The recurrent network architecture is a widely used model in sequence\nmodeling, but its serial dependency hinders the computation parallelization,\nwhich makes the operation inefficient. The same problem was encountered in\nserial adder at the early stage of digital electronics. In this paper, we\ndiscuss the similarities between recurrent neural network (RNN) and serial\nadder. Inspired by carry-lookahead adder, we introduce carry-lookahead module\nto RNN, which makes it possible for RNN to run in parallel. Then, we design the\nmethod of parallel RNN computation, and finally Carry-lookahead RNN (CL-RNN) is\nproposed. CL-RNN takes advantages in parallelism and flexible receptive field.\nThrough a comprehensive set of tests, we verify that CL-RNN can perform better\nthan existing typical RNNs in sequence modeling tasks which are specially\ndesigned for RNNs.",
          "link": "http://arxiv.org/abs/2106.12901",
          "publishedOn": "2021-06-25T02:00:46.382Z",
          "wordCount": 564,
          "title": "Recurrent Neural Network from Adder's Perspective: Carry-lookahead RNN. (arXiv:2106.12901v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lingam_V/0/1/0/all/0/1\">Vijay Lingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragesh_R/0/1/0/all/0/1\">Rahul Ragesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Arun Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellamanickam_S/0/1/0/all/0/1\">Sundararajan Sellamanickam</a>",
          "description": "Graph Neural Networks (GNNs) have shown excellent performance on graphs that\nexhibit strong homophily with respect to the node labels i.e. connected nodes\nhave same labels. However, they perform poorly on heterophilic graphs. Recent\napproaches have typically modified aggregation schemes, designed adaptive graph\nfilters, etc. to address this limitation. In spite of this, the performance on\nheterophilic graphs can still be poor. We propose a simple alternative method\nthat exploits Truncated Singular Value Decomposition (TSVD) of topological\nstructure and node features. Our approach achieves up to ~30% improvement in\nperformance over state-of-the-art methods on heterophilic graphs. This work is\nan early investigation into methods that differ from aggregation based\napproaches. Our experimental results suggest that it might be important to\nexplore other alternatives to aggregation methods for heterophilic setting.",
          "link": "http://arxiv.org/abs/2106.12807",
          "publishedOn": "2021-06-25T02:00:46.362Z",
          "wordCount": 577,
          "title": "Simple Truncated SVD based Model for Node Classification on Heterophilic Graphs. (arXiv:2106.12807v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziyadidegan_S/0/1/0/all/0/1\">Samira Ziyadidegan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_M/0/1/0/all/0/1\">Moein Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesarakli_H/0/1/0/all/0/1\">Homa Pesarakli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javid_A/0/1/0/all/0/1\">Amir Hossein Javid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erraguntla_M/0/1/0/all/0/1\">Madhav Erraguntla</a>",
          "description": "The COVID-19 disease spreads swiftly, and nearly three months after the first\npositive case was confirmed in China, Coronavirus started to spread all over\nthe United States. Some states and counties reported high number of positive\ncases and deaths, while some reported lower COVID-19 related cases and\nmortality. In this paper, the factors that could affect the risk of COVID-19\ninfection and mortality were analyzed in county level. An innovative method by\nusing K-means clustering and several classification models is utilized to\ndetermine the most critical factors. Results showed that mean temperature,\npercent of people below poverty, percent of adults with obesity, air pressure,\npopulation density, wind speed, longitude, and percent of uninsured people were\nthe most significant attributes",
          "link": "http://arxiv.org/abs/2106.12766",
          "publishedOn": "2021-06-25T02:00:46.344Z",
          "wordCount": 627,
          "title": "Factors affecting the COVID-19 risk in the US counties: an innovative approach by combining unsupervised and supervised learning. (arXiv:2106.12766v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zipeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_J/0/1/0/all/0/1\">J&#xfc;rgen Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munzner_T/0/1/0/all/0/1\">Tamara Munzner</a>",
          "description": "Graph neural networks (GNNs) are a class of powerful machine learning tools\nthat model node relations for making predictions of nodes or links. GNN\ndevelopers rely on quantitative metrics of the predictions to evaluate a GNN,\nbut similar to many other neural networks, it is difficult for them to\nunderstand if the GNN truly learns characteristics of a graph as expected. We\npropose an approach to corresponding an input graph to its node embedding (aka\nlatent space), a common component of GNNs that is later used for prediction. We\nabstract the data and tasks, and develop an interactive multi-view interface\ncalled CorGIE to instantiate the abstraction. As the key function in CorGIE, we\npropose the K-hop graph layout to show topological neighbors in hops and their\nclustering structure. To evaluate the functionality and usability of CorGIE, we\npresent how to use CorGIE in two usage scenarios, and conduct a case study with\ntwo GNN experts.",
          "link": "http://arxiv.org/abs/2106.12839",
          "publishedOn": "2021-06-25T02:00:46.338Z",
          "wordCount": 596,
          "title": "Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding. (arXiv:2106.12839v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00543",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1\">Junyu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bedi_A/0/1/0/all/0/1\">Amrit Singh Bedi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1\">Mengdi Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1\">Alec Koppel</a>",
          "description": "We posit a new mechanism for cooperation in multi-agent reinforcement\nlearning (MARL) based upon any nonlinear function of the team's long-term\nstate-action occupancy measure, i.e., a \\emph{general utility}. This subsumes\nthe cumulative return but also allows one to incorporate risk-sensitivity,\nexploration, and priors. % We derive the {\\bf D}ecentralized {\\bf S}hadow\nReward {\\bf A}ctor-{\\bf C}ritic (DSAC) in which agents alternate between policy\nevaluation (critic), weighted averaging with neighbors (information mixing),\nand local gradient updates for their policy parameters (actor). DSAC augments\nthe classic critic step by requiring agents to (i) estimate their local\noccupancy measure in order to (ii) estimate the derivative of the local utility\nwith respect to their occupancy measure, i.e., the \"shadow reward\". DSAC\nconverges to $\\epsilon$-stationarity in $\\mathcal{O}(1/\\epsilon^{2.5})$\n(Theorem \\ref{theorem:final}) or faster $\\mathcal{O}(1/\\epsilon^{2})$\n(Corollary \\ref{corollary:communication}) steps with high probability,\ndepending on the amount of communications. We further establish the\nnon-existence of spurious stationary points for this problem, that is, DSAC\nfinds the globally optimal policy (Corollary \\ref{corollary:global}).\nExperiments demonstrate the merits of goals beyond the cumulative return in\ncooperative MARL.",
          "link": "http://arxiv.org/abs/2106.00543",
          "publishedOn": "2021-06-25T02:00:46.318Z",
          "wordCount": 639,
          "title": "MARL with General Utilities via Decentralized Shadow Reward Actor-Critic. (arXiv:2106.00543v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaofei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jincan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>",
          "description": "Graph neural networks (GNNs) emerged recently as a standard toolkit for\nlearning from data on graphs. Current GNN designing works depend on immense\nhuman expertise to explore different message-passing mechanisms, and require\nmanual enumeration to determine the proper message-passing depth. Inspired by\nthe strong searching capability of neural architecture search (NAS) in CNN,\nthis paper proposes Graph Neural Architecture Search (GNAS) with novel-designed\nsearch space. The GNAS can automatically learn better architecture with the\noptimal depth of message passing on the graph. Specifically, we design Graph\nNeural Architecture Paradigm (GAP) with tree-topology computation procedure and\ntwo types of fine-grained atomic operations (feature filtering and neighbor\naggregation) from message-passing mechanism to construct powerful graph network\nsearch space. Feature filtering performs adaptive feature selection, and\nneighbor aggregation captures structural information and calculates neighbors'\nstatistics. Experiments show that our GNAS can search for better GNNs with\nmultiple message-passing mechanisms and optimal message-passing depth. The\nsearched network achieves remarkable improvement over state-of-the-art manual\ndesigned and search-based GNNs on five large-scale datasets at three classical\ngraph tasks. Codes can be found at https://github.com/phython96/GNAS-MP.",
          "link": "http://arxiv.org/abs/2103.14282",
          "publishedOn": "2021-06-25T02:00:46.312Z",
          "wordCount": 672,
          "title": "Rethinking Graph Neural Architecture Search from Message-passing. (arXiv:2103.14282v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.03376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Persand_K/0/1/0/all/0/1\">Kaveena Persand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Andrew Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1\">David Gregg</a>",
          "description": "The computation and memory needed for Convolutional Neural Network (CNN)\ninference can be reduced by pruning weights from the trained network. Pruning\nis guided by a pruning saliency, which heuristically approximates the change in\nthe loss function associated with the removal of specific weights. Many pruning\nsignals have been proposed, but the performance of each heuristic depends on\nthe particular trained network. This leaves the data scientist with a difficult\nchoice. When using any one saliency metric for the entire pruning process, we\nrun the risk of the metric assumptions being invalidated, leading to poor\ndecisions being made by the metric. Ideally we could combine the best aspects\nof different saliency metrics. However, despite an extensive literature review,\nwe are unable to find any prior work on composing different saliency metrics.\nThe chief difficulty lies in combining the numerical output of different\nsaliency metrics, which are not directly comparable.\n\nWe propose a method to compose several primitive pruning saliencies, to\nexploit the cases where each saliency measure does well. Our experiments show\nthat the composition of saliencies avoids many poor pruning choices identified\nby individual saliencies. In most cases our method finds better selections than\neven the best individual pruning saliency.",
          "link": "http://arxiv.org/abs/2004.03376",
          "publishedOn": "2021-06-25T02:00:46.306Z",
          "wordCount": 690,
          "title": "Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle. (arXiv:2004.03376v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.01956",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gneiting_T/0/1/0/all/0/1\">Tilmann Gneiting</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Walz_E/0/1/0/all/0/1\">Eva-Maria Walz</a>",
          "description": "Throughout science and technology, receiver operating characteristic (ROC)\ncurves and associated area under the curve (AUC) measures constitute powerful\ntools for assessing the predictive abilities of features, markers and tests in\nbinary classification problems. Despite its immense popularity, ROC analysis\nhas been subject to a fundamental restriction, in that it applies to\ndichotomous (yes or no) outcomes only. Here we introduce ROC movies and\nuniversal ROC (UROC) curves that apply to just any linearly ordered outcome,\nalong with an associated coefficient of predictive ability (CPA) measure. CPA\nequals the area under the UROC curve, and admits appealing interpretations in\nterms of probabilities and rank based covariances. For binary outcomes CPA\nequals AUC, and for pairwise distinct outcomes CPA relates linearly to\nSpearman's coefficient, in the same way that the C index relates linearly to\nKendall's coefficient. ROC movies, UROC curves, and CPA nest and generalize the\ntools of classical ROC analysis, and are bound to supersede them in a wealth of\napplications. Their usage is illustrated in data examples from biomedicine and\nmeteorology, where rank based measures yield new insights in the WeatherBench\ncomparison of the predictive performance of convolutional neural networks and\nphysical-numerical models for weather prediction.",
          "link": "http://arxiv.org/abs/1912.01956",
          "publishedOn": "2021-06-25T02:00:46.288Z",
          "wordCount": 671,
          "title": "Receiver operating characteristic (ROC) movies, universal ROC (UROC) curves, and coefficient of predictive ability (CPA). (arXiv:1912.01956v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.01171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lubin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Weili Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongrui Wu</a>",
          "description": "Multiple convolutional neural network (CNN) classifiers have been proposed\nfor electroencephalogram (EEG) based brain-computer interfaces (BCIs). However,\nCNN models have been found vulnerable to universal adversarial perturbations\n(UAPs), which are small and example-independent, yet powerful enough to degrade\nthe performance of a CNN model, when added to a benign example. This paper\nproposes a novel total loss minimization (TLM) approach to generate UAPs for\nEEG-based BCIs. Experimental results demonstrated the effectiveness of TLM on\nthree popular CNN classifiers for both target and non-target attacks. We also\nverified the transferability of UAPs in EEG-based BCI systems. To our\nknowledge, this is the first study on UAPs of CNN classifiers in EEG-based\nBCIs. UAPs are easy to construct, and can attack BCIs in real-time, exposing a\npotentially critical security concern of BCIs.",
          "link": "http://arxiv.org/abs/1912.01171",
          "publishedOn": "2021-06-25T02:00:46.268Z",
          "wordCount": 636,
          "title": "Universal Adversarial Perturbations for CNN Classifiers in EEG-Based BCIs. (arXiv:1912.01171v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.12851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingnan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habetler_T/0/1/0/all/0/1\">Thomas G. Habetler</a>",
          "description": "The rapid development of artificial intelligence and deep learning has\nprovided many opportunities to further enhance the safety, stability, and\naccuracy of industrial Cyber-Physical Systems (CPS). As indispensable\ncomponents to many mission-critical CPS assets and equipment, mechanical\nbearings need to be monitored to identify any trace of abnormal conditions.\nMost of the data-driven approaches applied to bearing fault diagnosis\nup-to-date are trained using a large amount of fault data collected a priori.\nIn many practical applications, however, it can be unsafe and time-consuming to\ncollect sufficient data samples for each fault category, making it challenging\nto train a robust classifier. In this paper, we propose a few-shot learning\nframework for bearing fault diagnosis based on model-agnostic meta-learning\n(MAML), which targets for training an effective fault classifier using limited\ndata. In addition, it can leverage the training data and learn to identify new\nfault scenarios more efficiently. Case studies on the generalization to new\nartificial faults show that the proposed framework achieves an overall accuracy\nup to 25% higher than a Siamese network-based benchmark study. Finally, the\nrobustness and the generalization capability of the proposed framework are\nfurther validated by applying it to identify real bearing damages using data\nfrom artificial damages, which compares favorably against 6 state-of-the-art\nfew-shot learning algorithms using consistent test environments.",
          "link": "http://arxiv.org/abs/2007.12851",
          "publishedOn": "2021-06-25T02:00:46.240Z",
          "wordCount": 700,
          "title": "Few-Shot Bearing Fault Diagnosis Based on Model-Agnostic Meta-Learning. (arXiv:2007.12851v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cadavid_J/0/1/0/all/0/1\">Juan Pablo Usuga Cadavid</a> (LAMIH, ENSAM), <a href=\"http://arxiv.org/find/cs/1/au:+Lamouri_S/0/1/0/all/0/1\">Samir Lamouri</a> (LAMIH, ENSAM), <a href=\"http://arxiv.org/find/cs/1/au:+Grabot_B/0/1/0/all/0/1\">Bernard Grabot</a> (LGP, ENIT), <a href=\"http://arxiv.org/find/cs/1/au:+Fortin_A/0/1/0/all/0/1\">Arnaud Fortin</a>",
          "description": "Proper Production Planning and Control (PPC) is capital to have an edge over\ncompetitors, reduce costs and respect delivery dates. With regard to PPC,\nMachine Learning (ML) provides new opportunities to make intelligent decisions\nbased on data. Therefore, this communication provides an initial systematic\nreview of publications on ML applied in PPC. The research objective of this\nstudy is twofold: firstly, it aims to identify techniques and tools allowing to\napply ML in PPC, and secondly, it reviews the characteristics of Industry 4.0\n(I4.0) in recent research papers. Concerning the second objective, seven\ncharacteristics of I4.0 are used in the analysis framework, from which two of\nthem are proposed by the authors. Additionally, the addressed domains of\nML-aided PPC in scientific literature are identified. Finally, results are\nanalyzed and gaps that may motivate further research are highlighted.",
          "link": "http://arxiv.org/abs/2106.12916",
          "publishedOn": "2021-06-25T02:00:46.235Z",
          "wordCount": 609,
          "title": "L'Apprentissage Automatique dans la planification et le contr{\\^o}le de la production : un {\\'e}tat de l'art. (arXiv:2106.12916v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yixiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yan Song</a>",
          "description": "Since the first coronavirus case was identified in the U.S. on Jan. 21, more\nthan 1 million people in the U.S. have confirmed cases of COVID-19. This\ninfectious respiratory disease has spread rapidly across more than 3000\ncounties and 50 states in the U.S. and have exhibited evolutionary clustering\nand complex triggering patterns. It is essential to understand the complex\nspacetime intertwined propagation of this disease so that accurate prediction\nor smart external intervention can be carried out. In this paper, we model the\npropagation of the COVID-19 as spatio-temporal point processes and propose a\ngenerative and intensity-free model to track the spread of the disease. We\nfurther adopt a generative adversarial imitation learning framework to learn\nthe model parameters. In comparison with the traditional likelihood-based\nlearning methods, this imitation learning framework does not need to prespecify\nan intensity function, which alleviates the model-misspecification. Moreover,\nthe adversarial learning procedure bypasses the difficult-to-evaluate integral\ninvolved in the likelihood evaluation, which makes the model inference more\nscalable with the data and variables. We showcase the dynamic learning\nperformance on the COVID-19 confirmed cases in the U.S. and evaluate the social\ndistancing policy based on the learned generative model.",
          "link": "http://arxiv.org/abs/2106.13097",
          "publishedOn": "2021-06-25T02:00:46.219Z",
          "wordCount": 684,
          "title": "Understanding the Spread of COVID-19 Epidemic: A Spatio-Temporal Point Process View. (arXiv:2106.13097v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13067",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Johnstone_P/0/1/0/all/0/1\">Patrick R. Johnstone</a>, <a href=\"http://arxiv.org/find/math/1/au:+Eckstein_J/0/1/0/all/0/1\">Jonathan Eckstein</a>, <a href=\"http://arxiv.org/find/math/1/au:+Flynn_T/0/1/0/all/0/1\">Thomas Flynn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yoo_S/0/1/0/all/0/1\">Shinjae Yoo</a>",
          "description": "We present a new, stochastic variant of the projective splitting (PS) family\nof algorithms for monotone inclusion problems. It can solve min-max and\nnoncooperative game formulations arising in applications such as robust ML\nwithout the convergence issues associated with gradient descent-ascent, the\ncurrent de facto standard approach in such situations. Our proposal is the\nfirst version of PS able to use stochastic (as opposed to deterministic)\ngradient oracles. It is also the first stochastic method that can solve min-max\ngames while easily handling multiple constraints and nonsmooth regularizers via\nprojection and proximal operators. We close with numerical experiments on a\ndistributionally robust sparse logistic regression problem.",
          "link": "http://arxiv.org/abs/2106.13067",
          "publishedOn": "2021-06-25T02:00:46.208Z",
          "wordCount": 545,
          "title": "Stochastic Projective Splitting: Solving Saddle-Point Problems with Multiple Regularizers. (arXiv:2106.13067v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12933",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zilber_P/0/1/0/all/0/1\">Pini Zilber</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nadler_B/0/1/0/all/0/1\">Boaz Nadler</a>",
          "description": "Low rank matrix recovery problems, including matrix completion and matrix\nsensing, appear in a broad range of applications. In this work we present GNMR\n-- an extremely simple iterative algorithm for low rank matrix recovery, based\non a Gauss-Newton linearization. On the theoretical front, we derive recovery\nguarantees for GNMR in both the matrix sensing and matrix completion settings.\nA key property of GNMR is that it implicitly keeps the factor matrices\napproximately balanced throughout its iterations. On the empirical front, we\nshow that for matrix completion with uniform sampling, GNMR performs better\nthan several popular methods, especially when given very few observations close\nto the information limit.",
          "link": "http://arxiv.org/abs/2106.12933",
          "publishedOn": "2021-06-25T02:00:46.203Z",
          "wordCount": 556,
          "title": "GNMR: A provable one-line algorithm for low rank matrix recovery. (arXiv:2106.12933v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samuel_S/0/1/0/all/0/1\">Sam Zabdiel Sunder Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamakshi_V/0/1/0/all/0/1\">Vidhya Kamakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lodhi_N/0/1/0/all/0/1\">Namrata Lodhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C Krishnan</a>",
          "description": "A particular class of Explainable AI (XAI) methods provide saliency maps to\nhighlight part of the image a Convolutional Neural Network (CNN) model looks at\nto classify the image as a way to explain its working. These methods provide an\nintuitive way for users to understand predictions made by CNNs. Other than\nquantitative computational tests, the vast majority of evidence to highlight\nthat the methods are valuable is anecdotal. Given that humans would be the\nend-users of such methods, we devise three human subject experiments through\nwhich we gauge the effectiveness of these saliency-based explainability\nmethods.",
          "link": "http://arxiv.org/abs/2106.12773",
          "publishedOn": "2021-06-25T02:00:46.198Z",
          "wordCount": 540,
          "title": "Evaluation of Saliency-based Explainability Method. (arXiv:2106.12773v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wickramanayake_S/0/1/0/all/0/1\">Sandareka Wickramanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wynne Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mong Li Lee</a>",
          "description": "Despite the remarkable performance, Deep Neural Networks (DNNs) behave as\nblack-boxes hindering user trust in Artificial Intelligence (AI) systems.\nResearch on opening black-box DNN can be broadly categorized into post-hoc\nmethods and inherently interpretable DNNs. While many surveys have been\nconducted on post-hoc interpretation methods, little effort is devoted to\ninherently interpretable DNNs. This paper provides a review of existing methods\nto develop DNNs with intrinsic interpretability, with a focus on Convolutional\nNeural Networks (CNNs). The aim is to understand the current progress towards\nfully interpretable DNNs that can cater to different interpretation\nrequirements. Finally, we identify gaps in current work and suggest potential\nresearch directions.",
          "link": "http://arxiv.org/abs/2106.13164",
          "publishedOn": "2021-06-25T02:00:46.193Z",
          "wordCount": 563,
          "title": "Towards Fully Interpretable Deep Neural Networks: Are We There Yet?. (arXiv:2106.13164v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandle_S/0/1/0/all/0/1\">Sebastian Br&#xe4;ndle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanussek_M/0/1/0/all/0/1\">Marc Hanussek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blohm_M/0/1/0/all/0/1\">Matthias Blohm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1\">Maximilien Kintz</a>",
          "description": "Automated Machine Learning (AutoML) has gained increasing success on tabular\ndata in recent years. However, processing unstructured data like text is a\nchallenge and not widely supported by open-source AutoML tools. This work\ncompares three manually created text representations and text embeddings\nautomatically created by AutoML tools. Our benchmark includes four popular\nopen-source AutoML tools and eight datasets for text classification purposes.\nThe results show that straightforward text representations perform better than\nAutoML tools with automatically created text embeddings.",
          "link": "http://arxiv.org/abs/2106.12798",
          "publishedOn": "2021-06-25T02:00:46.188Z",
          "wordCount": 526,
          "title": "Evaluation of Representation Models for Text Classification with AutoML Tools. (arXiv:2106.12798v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12930",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu T. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia T. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huynh_T/0/1/0/all/0/1\">Thang Q. Huynh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dao_M/0/1/0/all/0/1\">Minh Dao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vu_V/0/1/0/all/0/1\">Van Vu</a>",
          "description": "Radiographs are used as the most important imaging tool for identifying spine\nanomalies in clinical practice. The evaluation of spinal bone lesions, however,\nis a challenging task for radiologists. This work aims at developing and\nevaluating a deep learning-based framework, named VinDr-SpineXR, for the\nclassification and localization of abnormalities from spine X-rays. First, we\nbuild a large dataset, comprising 10,468 spine X-ray images from 5,000 studies,\neach of which is manually annotated by an experienced radiologist with bounding\nboxes around abnormal findings in 13 categories. Using this dataset, we then\ntrain a deep learning classifier to determine whether a spine scan is abnormal\nand a detector to localize 7 crucial findings amongst the total 13. The\nVinDr-SpineXR is evaluated on a test set of 2,078 images from 1,000 studies,\nwhich is kept separate from the training set. It demonstrates an area under the\nreceiver operating characteristic curve (AUROC) of 88.61% (95% CI 87.19%,\n90.02%) for the image-level classification task and a mean average precision\n(mAP@0.5) of 33.56% for the lesion-level localization task. These results serve\nas a proof of concept and set a baseline for future research in this direction.\nTo encourage advances, the dataset, codes, and trained deep learning models are\nmade publicly available.",
          "link": "http://arxiv.org/abs/2106.12930",
          "publishedOn": "2021-06-25T02:00:46.173Z",
          "wordCount": 702,
          "title": "VinDr-SpineXR: A deep learning framework for spinal lesions detection and classification from radiographs. (arXiv:2106.12930v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keltjens_B/0/1/0/all/0/1\">Benjamin Keltjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijk_T/0/1/0/all/0/1\">Tom van Dijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1\">Guido de Croon</a>",
          "description": "Self-supervised deep learning methods have leveraged stereo images for\ntraining monocular depth estimation. Although these methods show strong results\non outdoor datasets such as KITTI, they do not match performance of supervised\nmethods on indoor environments with camera rotation. Indoor, rotated scenes are\ncommon for less constrained applications and pose problems for two reasons:\nabundance of low texture regions and increased complexity of depth cues for\nimages under rotation. In an effort to extend self-supervised learning to more\ngeneralised environments we propose two additions. First, we propose a novel\nFilled Disparity Loss term that corrects for ambiguity of image reconstruction\nerror loss in textureless regions. Specifically, we interpolate disparity in\nuntextured regions, using the estimated disparity from surrounding textured\nareas, and use L1 loss to correct the original estimation. Our experiments show\nthat depth estimation is substantially improved on low-texture scenes, without\nany loss on textured scenes, when compared to Monodepth by Godard et al.\nSecondly, we show that training with an application's representative rotations,\nin both pitch and roll, is sufficient to significantly improve performance over\nthe entire range of expected rotation. We demonstrate that depth estimation is\nsuccessfully generalised as performance is not lost when evaluated on test sets\nwith no camera rotation. Together these developments enable a broader use of\nself-supervised learning of monocular depth estimation for complex\nenvironments.",
          "link": "http://arxiv.org/abs/2106.12958",
          "publishedOn": "2021-06-25T02:00:46.167Z",
          "wordCount": 664,
          "title": "Self-Supervised Monocular Depth Estimation of Untextured Indoor Rotated Scenes. (arXiv:2106.12958v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xingyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiani Liu</a>",
          "description": "Sketching uses randomized Hash functions for dimensionality reduction and\nacceleration. The existing sketching methods, such as count sketch (CS), tensor\nsketch (TS), and higher-order count sketch (HCS), either suffer from low\naccuracy or slow speed in some tensor based applications. In this paper, the\nproposed fast count sketch (FCS) applies multiple shorter Hash functions based\nCS to the vector form of the input tensor, which is more accurate than TS since\nthe spatial information of the input tensor can be preserved more sufficiently.\nWhen the input tensor admits CANDECOMP/PARAFAC decomposition (CPD), FCS can\naccelerate CS and HCS by using fast Fourier transform, which exhibits a\ncomputational complexity asymptotically identical to TS for low-order tensors.\nThe effectiveness of FCS is validated by CPD, tensor regression network\ncompression, and Kronecker product compression. Experimental results show its\nsuperior performance in terms of approximation accuracy and computational\nefficiency.",
          "link": "http://arxiv.org/abs/2106.13062",
          "publishedOn": "2021-06-25T02:00:46.162Z",
          "wordCount": 568,
          "title": "Efficient Tensor Contraction via Fast Count Sketch. (arXiv:2106.13062v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1\">Vijil Chenthamarakshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1\">Igor Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yang Shen</a>",
          "description": "Designing novel protein sequences for a desired 3D topological fold is a\nfundamental yet non-trivial task in protein engineering. Challenges exist due\nto the complex sequence--fold relationship, as well as the difficulties to\ncapture the diversity of the sequences (therefore structures and functions)\nwithin a fold. To overcome these challenges, we propose Fold2Seq, a novel\ntransformer-based generative framework for designing protein sequences\nconditioned on a specific target fold. To model the complex sequence--structure\nrelationship, Fold2Seq jointly learns a sequence embedding using a transformer\nand a fold embedding from the density of secondary structural elements in 3D\nvoxels. On test sets with single, high-resolution and complete structure inputs\nfor individual folds, our experiments demonstrate improved or comparable\nperformance of Fold2Seq in terms of speed, coverage, and reliability for\nsequence design, when compared to existing state-of-the-art methods that\ninclude data-driven deep generative models and physics-based RosettaDesign. The\nunique advantages of fold-based Fold2Seq, in comparison to a structure-based\ndeep model and RosettaDesign, become more evident on three additional\nreal-world challenges originating from low-quality, incomplete, or ambiguous\ninput structures. Source code and data are available at\nhttps://github.com/IBM/fold2seq.",
          "link": "http://arxiv.org/abs/2106.13058",
          "publishedOn": "2021-06-25T02:00:46.147Z",
          "wordCount": 630,
          "title": "Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design. (arXiv:2106.13058v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1\">Ee Fey Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">ZhiYuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_W/0/1/0/all/0/1\">Wei Xiang Lim</a>",
          "description": "The conventional spatial convolution layers in the Convolutional Neural\nNetworks (CNNs) are computationally expensive at the point where the training\ntime could take days unless the number of layers, the number of training images\nor the size of the training images are reduced. The image size of 256x256\npixels is commonly used for most of the applications of CNN, but this image\nsize is too small for applications like Diabetic Retinopathy (DR)\nclassification where the image details are important for accurate\nclassification. This research proposed Frequency Domain Convolution (FDC) and\nFrequency Domain Pooling (FDP) layers which were built with RFFT, kernel\ninitialization strategy, convolution artifact removal and Channel Independent\nConvolution (CIC) to replace the conventional convolution and pooling layers.\nThe FDC and FDP layers are used to build a Frequency Domain Convolutional\nNeural Network (FDCNN) to accelerate the training of large images for DR\nclassification. The Full FDC layer is an extension of the FDC layer to allow\ndirect use in conventional CNNs, it is also used to modify the VGG16\narchitecture. FDCNN is shown to be at least 54.21% faster and 70.74% more\nmemory efficient compared to an equivalent CNN architecture. The modified VGG16\narchitecture with Full FDC layer is reported to achieve a shorter training time\nand a higher accuracy at 95.63% compared to the original VGG16 architecture for\nDR classification.",
          "link": "http://arxiv.org/abs/2106.12736",
          "publishedOn": "2021-06-25T02:00:46.141Z",
          "wordCount": 680,
          "title": "Frequency Domain Convolutional Neural Network: Accelerated CNN for Large Diabetic Retinopathy Image Classification. (arXiv:2106.12736v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12987",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Satone_V/0/1/0/all/0/1\">Vipul Satone</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Desai_D/0/1/0/all/0/1\">Dhruv Desai</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mehta_D/0/1/0/all/0/1\">Dhagash Mehta</a>",
          "description": "Identifying similar mutual funds with respect to the underlying portfolios\nhas found many applications in financial services ranging from fund recommender\nsystems, competitors analysis, portfolio analytics, marketing and sales, etc.\nThe traditional methods are either qualitative, and hence prone to biases and\noften not reproducible, or, are known not to capture all the nuances\n(non-linearities) among the portfolios from the raw data. We propose a\nradically new approach to identify similar funds based on the weighted\nbipartite network representation of funds and their underlying assets data\nusing a sophisticated machine learning method called Node2Vec which learns an\nembedded low-dimensional representation of the network. We call the embedding\n\\emph{Fund2Vec}. Ours is the first ever study of the weighted bipartite network\nrepresentation of the funds-assets network in its original form that identifies\nstructural similarity among portfolios as opposed to merely portfolio overlaps.",
          "link": "http://arxiv.org/abs/2106.12987",
          "publishedOn": "2021-06-25T02:00:46.123Z",
          "wordCount": 589,
          "title": "Fund2Vec: Mutual Funds Similarity using Graph Learning. (arXiv:2106.12987v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1\">Woosub Jung</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yizhou Feng</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Sabbir Ahmed Khan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Xin_C/0/1/0/all/0/1\">Chunsheng Xin</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Danella Zhao</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Gang Zhou</a> (1) ((1) William &amp; Mary, (2) Old Dominion University)",
          "description": "As the number of IoT devices has increased rapidly, IoT botnets have\nexploited the vulnerabilities of IoT devices. However, it is still challenging\nto detect the initial intrusion on IoT devices prior to massive attacks. Recent\nstudies have utilized power side-channel information to characterize this\nintrusion behavior on IoT devices but still lack real-time detection\napproaches. This study aimed to design an online intrusion detection system\ncalled DeepAuditor for IoT devices via power auditing. To realize the real-time\nsystem, we first proposed a lightweight power auditing device called Power\nAuditor. With the Power Auditor, we developed a Distributed CNN classifier for\nonline inference in our laboratory setting. In order to protect data leakage\nand reduce networking redundancy, we also proposed a privacy-preserved\ninference protocol via Packed Homomorphic Encryption and a sliding window\nprotocol in our system. The classification accuracy and processing time were\nmeasured in our laboratory settings. We also demonstrated that the distributed\nCNN design is secure against any distributed components. Overall, the\nmeasurements were shown to the feasibility of our real-time distributed system\nfor intrusion detection on IoT devices.",
          "link": "http://arxiv.org/abs/2106.12753",
          "publishedOn": "2021-06-25T02:00:46.105Z",
          "wordCount": 649,
          "title": "DeepAuditor: Distributed Online Intrusion Detection System for IoT devices via Power Side-channel Auditing. (arXiv:2106.12753v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Anwesh Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattheakis_M/0/1/0/all/0/1\">Marios Mattheakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapas_P/0/1/0/all/0/1\">Pavlos Protopapas</a>",
          "description": "In certain situations, Neural Networks (NN) are trained upon data that obey\nunderlying physical symmetries. However, it is not guaranteed that NNs will\nobey the underlying symmetry unless embedded in the network structure. In this\nwork, we explore a special kind of symmetry where functions are invariant with\nrespect to involutory linear/affine transformations up to parity $p=\\pm 1$. We\ndevelop mathematical theorems and propose NN architectures that ensure\ninvariance and universal approximation properties. Numerical experiments\nindicate that the proposed models outperform baseline networks while respecting\nthe imposed symmetry. An adaption of our technique to convolutional NN\nclassification tasks for datasets with inherent horizontal/vertical reflection\nsymmetry has also been proposed.",
          "link": "http://arxiv.org/abs/2106.12891",
          "publishedOn": "2021-06-25T02:00:46.099Z",
          "wordCount": 547,
          "title": "Encoding Involutory Invariance in Neural Networks. (arXiv:2106.12891v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guzman_C/0/1/0/all/0/1\">Crist&#xf3;bal Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1\">Nishant A. Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_A/0/1/0/all/0/1\">Ali Mortazavi</a>",
          "description": "Much of the work in online learning focuses on the study of sublinear upper\nbounds on the regret. In this work, we initiate the study of best-case lower\nbounds in online convex optimization, wherein we bound the largest improvement\nan algorithm can obtain relative to the single best action in hindsight. This\nproblem is motivated by the goal of better understanding the adaptivity of a\nlearning algorithm. Another motivation comes from fairness: it is known that\nbest-case lower bounds are instrumental in obtaining algorithms for\ndecision-theoretic online learning (DTOL) that satisfy a notion of group\nfairness. Our contributions are a general method to provide best-case lower\nbounds in Follow The Regularized Leader (FTRL) algorithms with time-varying\nregularizers, which we use to show that best-case lower bounds are of the same\norder as existing upper regret bounds: this includes situations with a fixed\nlearning rate, decreasing learning rates, timeless methods, and adaptive\ngradient methods. In stark contrast, we show that the linearized version of\nFTRL can attain negative linear regret. Finally, in DTOL with two experts and\nbinary predictions, we fully characterize the best-case sequences, which\nprovides a finer understanding of the best-case lower bounds.",
          "link": "http://arxiv.org/abs/2106.12688",
          "publishedOn": "2021-06-25T02:00:46.064Z",
          "wordCount": 622,
          "title": "Best-Case Lower Bounds in Online Learning. (arXiv:2106.12688v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weg_B/0/1/0/all/0/1\">Bram van de Weg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greve_L/0/1/0/all/0/1\">Lars Greve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosic_B/0/1/0/all/0/1\">Bojana Rosic</a>",
          "description": "To incorporate prior knowledge as well as measurement uncertainties in the\ntraditional long short term memory (LSTM) neural networks, an efficient sparse\nBayesian training algorithm is introduced to the network architecture. The\nproposed scheme automatically determines relevant neural connections and adapts\naccordingly, in contrast to the classical LSTM solution. Due to its\nflexibility, the new LSTM scheme is less prone to overfitting, and hence can\napproximate time dependent solutions by use of a smaller data set. On a\nstructural nonlinear finite element application we show that the\nself-regulating framework does not require prior knowledge of a suitable\nnetwork architecture and size, while ensuring satisfying accuracy at reasonable\ncomputational cost.",
          "link": "http://arxiv.org/abs/2106.12694",
          "publishedOn": "2021-06-25T02:00:46.048Z",
          "wordCount": 538,
          "title": "Long short-term relevance learning. (arXiv:2106.12694v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12619",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Lee_K/0/1/0/all/0/1\">Kookjin Lee</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Trask_N/0/1/0/all/0/1\">Nathaniel A. Trask</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Stinis_P/0/1/0/all/0/1\">Panos Stinis</a>",
          "description": "Forecasting of time-series data requires imposition of inductive biases to\nobtain predictive extrapolation, and recent works have imposed\nHamiltonian/Lagrangian form to preserve structure for systems with reversible\ndynamics. In this work we present a novel parameterization of dissipative\nbrackets from metriplectic dynamical systems appropriate for learning\nirreversible dynamics with unknown a priori model form. The process learns\ngeneralized Casimirs for energy and entropy guaranteed to be conserved and\nnondecreasing, respectively. Furthermore, for the case of added thermal noise,\nwe guarantee exact preservation of a fluctuation-dissipation theorem, ensuring\nthermodynamic consistency. We provide benchmarks for dissipative systems\ndemonstrating learned dynamics are more robust and generalize better than\neither \"black-box\" or penalty-based approaches.",
          "link": "http://arxiv.org/abs/2106.12619",
          "publishedOn": "2021-06-25T02:00:46.032Z",
          "wordCount": 545,
          "title": "Machine learning structure preserving brackets for forecasting irreversible processes. (arXiv:2106.12619v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Mengnan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanchu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "Existing bias mitigation methods for DNN models primarily work on learning\ndebiased encoders. This process not only requires a lot of instance-level\nannotations for sensitive attributes, it also does not guarantee that all\nfairness sensitive information has been removed from the encoder. To address\nthese limitations, we explore the following research question: Can we reduce\nthe discrimination of DNN models by only debiasing the classification head,\neven with biased representations as inputs? To this end, we propose a new\nmitigation technique, namely, Representation Neutralization for Fairness (RNF)\nthat achieves fairness by debiasing only the task-specific classification head\nof DNN models. To this end, we leverage samples with the same ground-truth\nlabel but different sensitive attributes, and use their neutralized\nrepresentations to train the classification head of the DNN model. The key idea\nof RNF is to discourage the classification head from capturing spurious\ncorrelation between fairness sensitive information in encoder representations\nwith specific class labels. To address low-resource settings with no access to\nsensitive attribute annotations, we leverage a bias-amplified model to generate\nproxy annotations for sensitive attributes. Experimental results over several\nbenchmark datasets demonstrate our RNF framework to effectively reduce\ndiscrimination of DNN models with minimal degradation in task-specific\nperformance.",
          "link": "http://arxiv.org/abs/2106.12674",
          "publishedOn": "2021-06-25T02:00:45.997Z",
          "wordCount": 641,
          "title": "Fairness via Representation Neutralization. (arXiv:2106.12674v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Luhuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1\">Andrew Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_L/0/1/0/all/0/1\">Lauren Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1\">Geoff Pleiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1\">David Blei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1\">John Cunningham</a>",
          "description": "We examine the general problem of inter-domain Gaussian Processes (GPs):\nproblems where the GP realization and the noisy observations of that\nrealization lie on different domains. When the mapping between those domains is\nlinear, such as integration or differentiation, inference is still closed form.\nHowever, many of the scaling and approximation techniques that our community\nhas developed do not apply to this setting. In this work, we introduce the\nhierarchical inducing point GP (HIP-GP), a scalable inter-domain GP inference\nmethod that enables us to improve the approximation accuracy by increasing the\nnumber of inducing points to the millions. HIP-GP, which relies on inducing\npoints with grid structure and a stationary kernel assumption, is suitable for\nlow-dimensional problems. In developing HIP-GP, we introduce (1) a fast\nwhitening strategy, and (2) a novel preconditioner for conjugate gradients\nwhich can be helpful in general GP settings. Our code is available at https:\n//github.com/cunningham-lab/hipgp.",
          "link": "http://arxiv.org/abs/2103.00393",
          "publishedOn": "2021-06-25T02:00:45.992Z",
          "wordCount": 620,
          "title": "Hierarchical Inducing Point Gaussian Process for Inter-domain Observations. (arXiv:2103.00393v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ainsworth_S/0/1/0/all/0/1\">Samuel Ainsworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowrey_K/0/1/0/all/0/1\">Kendall Lowrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1\">John Thickstun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1\">Siddhartha Srinivasa</a>",
          "description": "We study the estimation of policy gradients for continuous-time systems with\nknown dynamics. By reframing policy learning in continuous-time, we show that\nit is possible construct a more efficient and accurate gradient estimator. The\nstandard back-propagation through time estimator (BPTT) computes exact\ngradients for a crude discretization of the continuous-time system. In\ncontrast, we approximate continuous-time gradients in the original system. With\nthe explicit goal of estimating continuous-time gradients, we are able to\ndiscretize adaptively and construct a more efficient policy gradient estimator\nwhich we call the Continuous-Time Policy Gradient (CTPG). We show that\nreplacing BPTT policy gradients with more efficient CTPG estimates results in\nfaster and more robust learning in a variety of control tasks and simulators.",
          "link": "http://arxiv.org/abs/2012.06684",
          "publishedOn": "2021-06-25T02:00:45.965Z",
          "wordCount": 587,
          "title": "Faster Policy Learning with Continuous-Time Gradients. (arXiv:2012.06684v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anjali Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1\">Shamanth R Nayak K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_B/0/1/0/all/0/1\">Balaji Ganesan</a>",
          "description": "Explainability techniques for Graph Neural Networks still have a long way to\ngo compared to explanations available for both neural and decision decision\ntree-based models trained on tabular data. Using a task that straddles both\ngraphs and tabular data, namely Entity Matching, we comment on key aspects of\nexplainability that are missing in GNN model explanations.",
          "link": "http://arxiv.org/abs/2106.12665",
          "publishedOn": "2021-06-25T02:00:45.959Z",
          "wordCount": 500,
          "title": "Reimagining GNN Explanations with ideas from Tabular Data. (arXiv:2106.12665v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenshuo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1\">Karl Krauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1\">Nikhil Garg</a>",
          "description": "Recommender systems -- and especially matrix factorization-based\ncollaborative filtering algorithms -- play a crucial role in mediating our\naccess to online information. We show that such algorithms induce a particular\nkind of stereotyping: if preferences for a \\textit{set} of items are\nanti-correlated in the general user population, then those items may not be\nrecommended together to a user, regardless of that user's preferences and\nratings history. First, we introduce a notion of \\textit{joint accessibility},\nwhich measures the extent to which a set of items can jointly be accessed by\nusers. We then study joint accessibility under the standard factorization-based\ncollaborative filtering framework, and provide theoretical necessary and\nsufficient conditions when joint accessibility is violated. Moreover, we show\nthat these conditions can easily be violated when the users are represented by\na single feature vector. To improve joint accessibility, we further propose an\nalternative modelling fix, which is designed to capture the diverse multiple\ninterests of each user using a multi-vector representation. We conduct\nextensive experiments on real and simulated datasets, demonstrating the\nstereotyping problem with standard single-vector matrix factorization models.",
          "link": "http://arxiv.org/abs/2106.12622",
          "publishedOn": "2021-06-25T02:00:45.954Z",
          "wordCount": 615,
          "title": "The Stereotyping Problem in Collaboratively Filtered Recommender Systems. (arXiv:2106.12622v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2004.13240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohiuddin_T/0/1/0/all/0/1\">Tasnim Mohiuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>",
          "description": "Transfer learning has yielded state-of-the-art (SoTA) results in many\nsupervised NLP tasks. However, annotated data for every target task in every\ntarget language is rare, especially for low-resource languages. We propose\nUXLA, a novel unsupervised data augmentation framework for zero-resource\ntransfer learning scenarios. In particular, UXLA aims to solve cross-lingual\nadaptation problems from a source language task distribution to an unknown\ntarget language task distribution, assuming no training label in the target\nlanguage. At its core, UXLA performs simultaneous self-training with data\naugmentation and unsupervised sample selection. To show its effectiveness, we\nconduct extensive experiments on three diverse zero-resource cross-lingual\ntransfer tasks. UXLA achieves SoTA results in all the tasks, outperforming the\nbaselines by a good margin. With an in-depth framework dissection, we\ndemonstrate the cumulative contributions of different components to its\nsuccess.",
          "link": "http://arxiv.org/abs/2004.13240",
          "publishedOn": "2021-06-25T02:00:45.948Z",
          "wordCount": 611,
          "title": "UXLA: A Robust Unsupervised Data Augmentation Framework for {Zero-Resource} Cross-Lingual NLP. (arXiv:2004.13240v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1\">Samay Pashine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_R/0/1/0/all/0/1\">Ritik Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushwah_R/0/1/0/all/0/1\">Rishika Kushwah</a>",
          "description": "The reliance of humans over machines has never been so high such that from\nobject classification in photographs to adding sound to silent movies\neverything can be performed with the help of deep learning and machine learning\nalgorithms. Likewise, Handwritten text recognition is one of the significant\nareas of research and development with a streaming number of possibilities that\ncould be attained. Handwriting recognition (HWR), also known as Handwritten\nText Recognition (HTR), is the ability of a computer to receive and interpret\nintelligible handwritten input from sources such as paper documents,\nphotographs, touch-screens and other devices [1]. Apparently, in this paper, we\nhave performed handwritten digit recognition with the help of MNIST datasets\nusing Support Vector Machines (SVM), Multi-Layer Perceptron (MLP) and\nConvolution Neural Network (CNN) models. Our main objective is to compare the\naccuracy of the models stated above along with their execution time to get the\nbest possible model for digit recognition.",
          "link": "http://arxiv.org/abs/2106.12614",
          "publishedOn": "2021-06-25T02:00:45.942Z",
          "wordCount": 620,
          "title": "Handwritten Digit Recognition using Machine and Deep Learning Algorithms. (arXiv:2106.12614v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tianhao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changliu Liu</a>",
          "description": "Although neural networks are widely used, it remains challenging to formally\nverify the safety and robustness of neural networks in real-world applications.\nExisting methods are designed to verify the network before use, which is\nlimited to relatively simple specifications and fixed networks. These methods\nare not ready to be applied to real-world problems with complex and/or\ndynamically changing specifications and networks. To effectively handle\ndynamically changing specifications and networks, the verification needs to be\nperformed online when these changes take place. However, it is still\nchallenging to run existing verification algorithms online. Our key insight is\nthat we can leverage the temporal dependencies of these changes to accelerate\nthe verification process, e.g., by warm starting new online verification using\nprevious verified results. This paper establishes a novel framework for\nscalable online verification to solve real-world verification problems with\ndynamically changing specifications and/or networks, known as domain shift and\nweight shift respectively. We propose three types of techniques (branch\nmanagement, perturbation tolerance analysis, and incremental computation) to\naccelerate the online verification of deep neural networks. Experiment results\nshow that our online verification algorithm is up to two orders of magnitude\nfaster than existing verification algorithms, and thus can scale to real-world\napplications.",
          "link": "http://arxiv.org/abs/2106.12732",
          "publishedOn": "2021-06-25T02:00:45.926Z",
          "wordCount": 636,
          "title": "Online Verification of Deep Neural Networks under Domain or Weight Shift. (arXiv:2106.12732v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1\">Lucas Liebenwein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "Continuous deep learning architectures enable learning of flexible\nprobabilistic models for predictive modeling as neural ordinary differential\nequations (ODEs), and for generative modeling as continuous normalizing flows.\nIn this work, we design a framework to decipher the internal dynamics of these\ncontinuous depth models by pruning their network architectures. Our empirical\nresults suggest that pruning improves generalization for neural ODEs in\ngenerative modeling. Moreover, pruning finds minimal and efficient neural ODE\nrepresentations with up to 98\\% less parameters compared to the original\nnetwork, without loss of accuracy. Finally, we show that by applying pruning we\ncan obtain insightful information about the design of better neural ODEs.We\nhope our results will invigorate further research into the performance-size\ntrade-offs of modern continuous-depth models.",
          "link": "http://arxiv.org/abs/2106.12718",
          "publishedOn": "2021-06-25T02:00:45.921Z",
          "wordCount": 548,
          "title": "Sparse Flows: Pruning Continuous-depth Models. (arXiv:2106.12718v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12594",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Dax_M/0/1/0/all/0/1\">Maximilian Dax</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Green_S/0/1/0/all/0/1\">Stephen R. Green</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Gair_J/0/1/0/all/0/1\">Jonathan Gair</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Macke_J/0/1/0/all/0/1\">Jakob H. Macke</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Buonanno_A/0/1/0/all/0/1\">Alessandra Buonanno</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "We demonstrate unprecedented accuracy for rapid gravitational-wave parameter\nestimation with deep learning. Using neural networks as surrogates for Bayesian\nposterior distributions, we analyze eight gravitational-wave events from the\nfirst LIGO-Virgo Gravitational-Wave Transient Catalog and find very close\nquantitative agreement with standard inference codes, but with inference times\nreduced from O(day) to a minute per event. Our networks are trained using\nsimulated data, including an estimate of the detector-noise characteristics\nnear the event. This encodes the signal and noise models within millions of\nneural-network parameters, and enables inference for any observed data\nconsistent with the training distribution, accounting for noise nonstationarity\nfrom event to event. Our algorithm -- called \"DINGO\" -- sets a new standard in\nfast-and-accurate inference of physical parameters of detected\ngravitational-wave events, which should enable real-time data analysis without\nsacrificing accuracy.",
          "link": "http://arxiv.org/abs/2106.12594",
          "publishedOn": "2021-06-25T02:00:45.915Z",
          "wordCount": 592,
          "title": "Real-time gravitational-wave science with neural posterior estimation. (arXiv:2106.12594v1 [gr-qc])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadzadeh_S/0/1/0/all/0/1\">Saeed Mohammadzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_V/0/1/0/all/0/1\">Vitor H.Nascimento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamare_R/0/1/0/all/0/1\">Rodrigo C. de Lamare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukrer_O/0/1/0/all/0/1\">Osman Kukrer</a>",
          "description": "In this paper, a novel and robust algorithm is proposed for adaptive\nbeamforming based on the idea of reconstructing the autocorrelation sequence\n(ACS) of a random process from a set of measured data. This is obtained from\nthe first column and the first row of the sample covariance matrix (SCM) after\naveraging along its diagonals. Then, the power spectrum of the correlation\nsequence is estimated using the discrete Fourier transform (DFT). The DFT\ncoefficients corresponding to the angles within the noise-plus-interference\nregion are used to reconstruct the noise-plus-interference covariance matrix\n(NPICM), while the desired signal covariance matrix (DSCM) is estimated by\nidentifying and removing the noise-plus-interference component from the SCM. In\nparticular, the spatial power spectrum of the estimated received signal is\nutilized to compute the correlation sequence corresponding to the\nnoise-plus-interference in which the dominant DFT coefficient of the\nnoise-plus-interference is captured. A key advantage of the proposed adaptive\nbeamforming is that only little prior information is required. Specifically, an\nimprecise knowledge of the array geometry and of the angular sectors in which\nthe interferences are located is needed. Simulation results demonstrate that\ncompared with previous reconstruction-based beamformers, the proposed approach\ncan achieve better overall performance in the case of multiple mismatches over\na very large range of input signal-to-noise ratios.",
          "link": "http://arxiv.org/abs/2106.12663",
          "publishedOn": "2021-06-25T02:00:45.909Z",
          "wordCount": 663,
          "title": "Study of Robust Adaptive Beamforming Based on Low-Complexity DFT Spatial Sampling. (arXiv:2106.12663v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12627",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1\">Hsin-Yuan Huang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kueng_R/0/1/0/all/0/1\">Richard Kueng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Torlai_G/0/1/0/all/0/1\">Giacomo Torlai</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Albert_V/0/1/0/all/0/1\">Victor V. Albert</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Preskill_J/0/1/0/all/0/1\">John Preskill</a>",
          "description": "Classical machine learning (ML) provides a potentially powerful approach to\nsolving challenging quantum many-body problems in physics and chemistry.\nHowever, the advantages of ML over more traditional methods have not been\nfirmly established. In this work, we prove that classical ML algorithms can\nefficiently predict ground state properties of gapped Hamiltonians in finite\nspatial dimensions, after learning from data obtained by measuring other\nHamiltonians in the same quantum phase of matter. In contrast, under widely\naccepted complexity theory assumptions, classical algorithms that do not learn\nfrom data cannot achieve the same guarantee. We also prove that classical ML\nalgorithms can efficiently classify a wide range of quantum phases of matter.\nOur arguments are based on the concept of a classical shadow, a succinct\nclassical description of a many-body quantum state that can be constructed in\nfeasible quantum experiments and be used to predict many properties of the\nstate. Extensive numerical experiments corroborate our theoretical results in a\nvariety of scenarios, including Rydberg atom systems, 2D random Heisenberg\nmodels, symmetry-protected topological phases, and topologically ordered\nphases.",
          "link": "http://arxiv.org/abs/2106.12627",
          "publishedOn": "2021-06-25T02:00:45.903Z",
          "wordCount": 625,
          "title": "Provably efficient machine learning for quantum many-body problems. (arXiv:2106.12627v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zaiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1\">Siva Theja Maguluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>",
          "description": "In temporal difference (TD) learning, off-policy sampling is known to be more\npractical than on-policy sampling, and by decoupling learning from data\ncollection, it enables data reuse. It is known that policy evaluation\n(including multi-step off-policy importance sampling) has the interpretation of\nsolving a generalized Bellman equation. In this paper, we derive finite-sample\nbounds for any general off-policy TD-like stochastic approximation algorithm\nthat solves for the fixed-point of this generalized Bellman operator. Our key\nstep is to show that the generalized Bellman operator is simultaneously a\ncontraction mapping with respect to a weighted $\\ell_p$-norm for each $p$ in\n$[1,\\infty)$, with a common contraction factor.\n\nOff-policy TD-learning is known to suffer from high variance due to the\nproduct of importance sampling ratios. A number of algorithms (e.g.\n$Q^\\pi(\\lambda)$, Tree-Backup$(\\lambda)$, Retrace$(\\lambda)$, and $Q$-trace)\nhave been proposed in the literature to address this issue. Our results\nimmediately imply finite-sample bounds of these algorithms. In particular, we\nprovide first-known finite-sample guarantees for $Q^\\pi(\\lambda)$,\nTree-Backup$(\\lambda)$, and Retrace$(\\lambda)$, and improve the best known\nbounds of $Q$-trace in [19]. Moreover, we show the bias-variance trade-offs in\neach of these algorithms.",
          "link": "http://arxiv.org/abs/2106.12729",
          "publishedOn": "2021-06-25T02:00:45.888Z",
          "wordCount": 627,
          "title": "Finite-Sample Analysis of Off-Policy TD-Learning via Generalized Bellman Operators. (arXiv:2106.12729v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1\">Samay Pashine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandiya_S/0/1/0/all/0/1\">Sagar Mandiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Praveen Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_R/0/1/0/all/0/1\">Rashid Sheikh</a>",
          "description": "Deep Learning as a field has been successfully used to solve a plethora of\ncomplex problems, the likes of which we could not have imagined a few decades\nback. But as many benefits as it brings, there are still ways in which it can\nbe used to bring harm to our society. Deep fakes have been proven to be one\nsuch problem, and now more than ever, when any individual can create a fake\nimage or video simply using an application on the smartphone, there need to be\nsome countermeasures, with which we can detect if the image or video is a fake\nor real and dispose of the problem threatening the trustworthiness of online\ninformation. Although the Deep fakes created by neural networks, may seem to be\nas real as a real image or video, it still leaves behind spatial and temporal\ntraces or signatures after moderation, these signatures while being invisible\nto a human eye can be detected with the help of a neural network trained to\nspecialize in Deep fake detection. In this paper, we analyze several such\nstates of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception\nNet) and compare them against each other, to find an optimal solution for\nvarious scenarios like real-time deep fake detection to be deployed in online\nsocial media platforms where the classification should be made as fast as\npossible or for a small news agency where the classification need not be in\nreal-time but requires utmost accuracy.",
          "link": "http://arxiv.org/abs/2106.12605",
          "publishedOn": "2021-06-25T02:00:45.883Z",
          "wordCount": 713,
          "title": "Deep Fake Detection: Survey of Facial Manipulation Detection Solutions. (arXiv:2106.12605v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12704",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Mizota_Y/0/1/0/all/0/1\">Yusuke Mizota</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hamada_N/0/1/0/all/0/1\">Naoki Hamada</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ichiki_S/0/1/0/all/0/1\">Shunsuke Ichiki</a>",
          "description": "A multi-objective optimization problem is $C^r$ weakly simplicial if there\nexists a $C^r$ surjection from a simplex onto the Pareto set/front such that\nthe image of each subsimplex is the Pareto set/front of a subproblem, where\n$0\\leq r\\leq \\infty$. This property is helpful to compute a parametric-surface\napproximation of the entire Pareto set and Pareto front. It is known that all\nunconstrained strongly convex $C^r$ problems are $C^{r-1}$ weakly simplicial\nfor $1\\leq r \\leq \\infty$. In this paper, we show that all unconstrained\nstrongly convex problems are $C^0$ weakly simplicial. The usefulness of this\ntheorem is demonstrated in a sparse modeling application: we reformulate the\nelastic net as a non-differentiable multi-objective strongly convex problem and\napproximate its Pareto set (the set of all trained models with different\nhyper-parameters) and Pareto front (the set of performance metrics of the\ntrained models) by using a B\\'ezier simplex fitting method, which accelerates\nhyper-parameter search.",
          "link": "http://arxiv.org/abs/2106.12704",
          "publishedOn": "2021-06-25T02:00:45.847Z",
          "wordCount": 602,
          "title": "All unconstrained strongly convex problems are weakly simplicial. (arXiv:2106.12704v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anurag Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1\">Seth Cooper</a>",
          "description": "Variational autoencoders (VAEs) have been used in prior works for generating\nand blending levels from different games. To add controllability to these\nmodels, conditional VAEs (CVAEs) were recently shown capable of generating\noutput that can be modified using labels specifying desired content, albeit\nworking with segments of levels and platformers exclusively. We expand these\nworks by using CVAEs for generating whole platformer and dungeon levels, and\nblending levels across these genres. We show that CVAEs can reliably control\ndoor placement in dungeons and progression direction in platformer levels.\nThus, by using appropriate labels, our approach can generate whole dungeons and\nplatformer levels of interconnected rooms and segments respectively as well as\nlevels that blend dungeons and platformers. We demonstrate our approach using\nThe Legend of Zelda, Metroid, Mega Man and Lode Runner.",
          "link": "http://arxiv.org/abs/2106.12692",
          "publishedOn": "2021-06-25T02:00:45.831Z",
          "wordCount": 562,
          "title": "Dungeon and Platformer Level Blending and Generation using Conditional VAEs. (arXiv:2106.12692v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baranchuk_D/0/1/0/all/0/1\">Dmitry Baranchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliev_V/0/1/0/all/0/1\">Vladimir Aliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1\">Artem Babenko</a>",
          "description": "Normalizing flows are a powerful class of generative models demonstrating\nstrong performance in several speech and vision problems. In contrast to other\ngenerative models, normalizing flows have tractable likelihoods and allow for\nstable training. However, they have to be carefully designed to represent\ninvertible functions with efficient Jacobian determinant calculation. In\npractice, these requirements lead to overparameterized and sophisticated\narchitectures that are inferior to alternative feed-forward models in terms of\ninference time and memory consumption. In this work, we investigate whether one\ncan distill knowledge from flow-based models to more efficient alternatives. We\nprovide a positive answer to this question by proposing a simple distillation\napproach and demonstrating its effectiveness on state-of-the-art conditional\nflow-based models for image super-resolution and speech synthesis.",
          "link": "http://arxiv.org/abs/2106.12699",
          "publishedOn": "2021-06-25T02:00:45.816Z",
          "wordCount": 549,
          "title": "Distilling the Knowledge from Normalizing Flows. (arXiv:2106.12699v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Wei-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daniel Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hsiang-Fu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1\">Choon-Hui Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1\">Kai Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolluri_K/0/1/0/all/0/1\">Kedarnath Kolluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shandilya_N/0/1/0/all/0/1\">Nikhil Shandilya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ievgrafov_V/0/1/0/all/0/1\">Vyacheslav Ievgrafov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Japinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit S. Dhillon</a>",
          "description": "We consider the problem of semantic matching in product search: given a\ncustomer query, retrieve all semantically related products from a huge catalog\nof size 100 million, or more. Because of large catalog spaces and real-time\nlatency constraints, semantic matching algorithms not only desire high recall\nbut also need to have low latency. Conventional lexical matching approaches\n(e.g., Okapi-BM25) exploit inverted indices to achieve fast inference time, but\nfail to capture behavioral signals between queries and products. In contrast,\nembedding-based models learn semantic representations from customer behavior\ndata, but the performance is often limited by shallow neural encoders due to\nlatency constraints. Semantic product search can be viewed as an eXtreme\nMulti-label Classification (XMC) problem, where customer queries are input\ninstances and products are output labels. In this paper, we aim to improve\nsemantic product search by using tree-based XMC models where inference time\ncomplexity is logarithmic in the number of products. We consider hierarchical\nlinear models with n-gram features for fast real-time inference.\nQuantitatively, our method maintains a low latency of 1.25 milliseconds per\nquery and achieves a 65% improvement of Recall@100 (60.9% v.s. 36.8%) over a\ncompeting embedding-based DSSM model. Our model is robust to weight pruning\nwith varying thresholds, which can flexibly meet different system requirements\nfor online deployments. Qualitatively, our method can retrieve products that\nare complementary to existing product search system and add diversity to the\nmatch set.",
          "link": "http://arxiv.org/abs/2106.12657",
          "publishedOn": "2021-06-25T02:00:45.800Z",
          "wordCount": 695,
          "title": "Extreme Multi-label Learning for Semantic Matching in Product Search. (arXiv:2106.12657v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianlong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Simon Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>",
          "description": "The claims data, containing medical codes, services information, and incurred\nexpenditure, can be a good resource for estimating an individual's health\ncondition and medical risk level. In this study, we developed Transformer-based\nMultimodal AutoEncoder (TMAE), an unsupervised learning framework that can\nlearn efficient patient representation by encoding meaningful information from\nthe claims data. TMAE is motivated by the practical needs in healthcare to\nstratify patients into different risk levels for improving care delivery and\nmanagement. Compared to previous approaches, TMAE is able to 1) model\ninpatient, outpatient, and medication claims collectively, 2) handle irregular\ntime intervals between medical events, 3) alleviate the sparsity issue of the\nrare medical codes, and 4) incorporate medical expenditure information. We\ntrained TMAE using a real-world pediatric claims dataset containing more than\n600,000 patients and compared its performance with various approaches in two\nclustering tasks. Experimental results demonstrate that TMAE has superior\nperformance compared to all baselines. Multiple downstream applications are\nalso conducted to illustrate the effectiveness of our framework. The promising\nresults confirm that the TMAE framework is scalable to large claims data and is\nable to generate efficient patient embeddings for risk stratification and\nanalysis.",
          "link": "http://arxiv.org/abs/2106.12658",
          "publishedOn": "2021-06-25T02:00:45.777Z",
          "wordCount": 634,
          "title": "Transformer-based unsupervised patient representation learning based on medical claims for risk stratification and analysis. (arXiv:2106.12658v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibayashi_H/0/1/0/all/0/1\">Hikaru Ibayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamaguchi_T/0/1/0/all/0/1\">Takuo Hamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imaizum_M/0/1/0/all/0/1\">Masaaki Imaizum</a>",
          "description": "Toward achieving robust and defensive neural networks, the robustness against\nthe weight parameters perturbations, i.e., sharpness, attracts attention in\nrecent years (Sun et al., 2020). However, sharpness is known to remain a\ncritical issue, \"scale-sensitivity.\" In this paper, we propose a novel\nsharpness measure, Minimum Sharpness. It is known that NNs have a specific\nscale transformation that constitutes equivalent classes where functional\nproperties are completely identical, and at the same time, their sharpness\ncould change unlimitedly. We define our sharpness through a minimization\nproblem over the equivalent NNs being invariant to the scale transformation. We\nalso develop an efficient and exact technique to make the sharpness tractable,\nwhich reduces the heavy computational costs involved with Hessian. In the\nexperiment, we observed that our sharpness has a valid correlation with the\ngeneralization of NNs and runs with less computational cost than existing\nsharpness measures.",
          "link": "http://arxiv.org/abs/2106.12612",
          "publishedOn": "2021-06-25T02:00:45.761Z",
          "wordCount": 587,
          "title": "Minimum sharpness: Scale-invariant parameter-robustness of neural networks. (arXiv:2106.12612v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1\">Simon Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>",
          "description": "State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.",
          "link": "http://arxiv.org/abs/2106.12672",
          "publishedOn": "2021-06-25T02:00:45.755Z",
          "wordCount": 628,
          "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter L. Bartlett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">S&#xe9;bastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherapanamjeri_Y/0/1/0/all/0/1\">Yeshwanth Cherapanamjeri</a>",
          "description": "We consider the phenomenon of adversarial examples in ReLU networks with\nindependent gaussian parameters. For networks of constant depth and with a\nlarge range of widths (for instance, it suffices if the width of each layer is\npolynomial in that of any other layer), small perturbations of input vectors\nlead to large changes of outputs. This generalizes results of Daniely and\nSchacham (2020) for networks of rapidly decreasing width and of Bubeck et al\n(2021) for two-layer networks. The proof shows that adversarial examples arise\nin these networks because the functions that they compute are very close to\nlinear. Bottleneck layers in the network play a key role: the minimal width up\nto some point in the network determines scales and sensitivities of mappings\ncomputed up to that point. The main result is for networks with constant depth,\nbut we also show that some constraint on depth is necessary for a result of\nthis kind, because there are suitably deep networks that, with constant\nprobability, compute a function that is close to constant.",
          "link": "http://arxiv.org/abs/2106.12611",
          "publishedOn": "2021-06-25T02:00:45.750Z",
          "wordCount": 610,
          "title": "Adversarial Examples in Multi-Layer Random ReLU Networks. (arXiv:2106.12611v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1\">Hayden S. Helm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdin_M/0/1/0/all/0/1\">Marah Abdin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedigo_B/0/1/0/all/0/1\">Benjamin D. Pedigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1\">Shweti Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyzinski_V/0/1/0/all/0/1\">Vince Lyzinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Youngser Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Amitabh Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_P/0/1/0/all/0/1\">Piali~Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Christopher M. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>",
          "description": "In modern ranking problems, different and disparate representations of the\nitems to be ranked are often available. It is sensible, then, to try to combine\nthese representations to improve ranking. Indeed, learning to rank via\ncombining representations is both principled and practical for learning a\nranking function for a particular query. In extremely data-scarce settings,\nhowever, the amount of labeled data available for a particular query can lead\nto a highly variable and ineffective ranking function. One way to mitigate the\neffect of the small amount of data is to leverage information from semantically\nsimilar queries. Indeed, as we demonstrate in simulation settings and real data\nexamples, when semantically similar queries are available it is possible to\ngainfully use them when ranking with respect to a particular query. We describe\nand explore this phenomenon in the context of the bias-variance trade off and\napply it to the data-scarce settings of a Bing navigational graph and the\nDrosophila larva connectome.",
          "link": "http://arxiv.org/abs/2106.12621",
          "publishedOn": "2021-06-25T02:00:45.729Z",
          "wordCount": 616,
          "title": "Leveraging semantically similar queries for ranking via combining representations. (arXiv:2106.12621v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bayat_N/0/1/0/all/0/1\">Niloofar Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_W/0/1/0/all/0/1\">Weston Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Derrick Liu</a>",
          "description": "Monitoring network traffic to identify content, services, and applications is\nan active research topic in network traffic control systems. While modern\nfirewalls provide the capability to decrypt packets, this is not appealing for\nprivacy advocates. Hence, identifying any information from encrypted traffic is\na challenging task. Nonetheless, previous work has identified machine learning\nmethods that may enable application and service identification. The process\ninvolves high level feature extraction from network packet data then training a\nrobust machine learning classifier for traffic identification. We propose a\nclassification technique using an ensemble of deep learning architectures on\npacket, payload, and inter-arrival time sequences. To our knowledge, this is\nthe first time such deep learning architectures have been applied to the Server\nName Indication (SNI) classification problem. Our ensemble model beats the\nstate of the art machine learning methods and our up-to-date model can be found\non github: \\url{https://github.com/niloofarbayat/NetworkClassification}",
          "link": "http://arxiv.org/abs/2106.12693",
          "publishedOn": "2021-06-25T02:00:45.666Z",
          "wordCount": 584,
          "title": "Deep Learning for Network Traffic Classification. (arXiv:2106.12693v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uniyal_A/0/1/0/all/0/1\">Archit Uniyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotti_S/0/1/0/all/0/1\">Sasikanth Kotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1\">Patrik Joslin Kenfack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trask_A/0/1/0/all/0/1\">Andrew Trask</a>",
          "description": "Recent advances in differentially private deep learning have demonstrated\nthat application of differential privacy, specifically the DP-SGD algorithm,\nhas a disparate impact on different sub-groups in the population, which leads\nto a significantly high drop-in model utility for sub-populations that are\nunder-represented (minorities), compared to well-represented ones. In this\nwork, we aim to compare PATE, another mechanism for training deep learning\nmodels using differential privacy, with DP-SGD in terms of fairness. We show\nthat PATE does have a disparate impact too, however, it is much less severe\nthan DP-SGD. We draw insights from this observation on what might be promising\ndirections in achieving better fairness-privacy trade-offs.",
          "link": "http://arxiv.org/abs/2106.12576",
          "publishedOn": "2021-06-25T02:00:45.661Z",
          "wordCount": 562,
          "title": "DP-SGD vs PATE: Which Has Less Disparate Impact on Model Accuracy?. (arXiv:2106.12576v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}